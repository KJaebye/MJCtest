[32m[20221213 23:12:32 @logger.py:105][0m Log file set to ./tmp/hopper/stand/20221213_231232/log/hopper_stand-20221213_231232.log
[32m[20221213 23:12:32 @agent_ppo2.py:79][0m [4m[34mCRITICAL[0m Loading model from checkpoint: ./tmp/hopper/stand/20221213_221813/models/iter_2000.p
[32m[20221213 23:12:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2000 --------------------------#
[32m[20221213 23:12:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:12:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:32 @agent_ppo2.py:185][0m |           0.0053 |          45.6056 |          15.2321 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |           0.0006 |          43.8823 |          15.2485 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |          -0.0016 |          42.8974 |          15.2606 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |          -0.0014 |          43.1600 |          15.2678 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |          -0.0025 |          41.8188 |          15.2575 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |          -0.0037 |          41.5571 |          15.2593 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |          -0.0074 |          41.3127 |          15.2538 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |          -0.0039 |          41.3730 |          15.2697 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |          -0.0074 |          40.8205 |          15.2665 |
[32m[20221213 23:12:33 @agent_ppo2.py:185][0m |           0.0042 |          44.2442 |          15.2684 |
[32m[20221213 23:12:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:12:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.81
[32m[20221213 23:12:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.53
[32m[20221213 23:12:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.31
[32m[20221213 23:12:33 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 456.31
[32m[20221213 23:12:33 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 456.31
[32m[20221213 23:12:33 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221213 23:12:33 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 23:12:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2001 --------------------------#
[32m[20221213 23:12:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |           0.0041 |          45.4385 |          15.0558 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |          -0.0003 |          42.3394 |          15.0933 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |           0.0018 |          42.3649 |          15.0612 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |          -0.0069 |          41.2490 |          15.0380 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |          -0.0096 |          40.9924 |          15.0601 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |          -0.0070 |          40.8398 |          15.0529 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |          -0.0071 |          40.5817 |          15.0603 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |          -0.0098 |          40.2853 |          15.0536 |
[32m[20221213 23:12:34 @agent_ppo2.py:185][0m |          -0.0137 |          40.1963 |          15.0423 |
[32m[20221213 23:12:35 @agent_ppo2.py:185][0m |          -0.0096 |          40.0762 |          15.0263 |
[32m[20221213 23:12:35 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:12:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.96
[32m[20221213 23:12:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.58
[32m[20221213 23:12:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.89
[32m[20221213 23:12:35 @agent_ppo2.py:143][0m Total time:       0.05 min
[32m[20221213 23:12:35 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 23:12:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2002 --------------------------#
[32m[20221213 23:12:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:35 @agent_ppo2.py:185][0m |           0.0000 |          53.2746 |          15.1309 |
[32m[20221213 23:12:35 @agent_ppo2.py:185][0m |           0.0030 |          54.4715 |          15.0865 |
[32m[20221213 23:12:35 @agent_ppo2.py:185][0m |          -0.0070 |          51.7003 |          15.1101 |
[32m[20221213 23:12:35 @agent_ppo2.py:185][0m |          -0.0070 |          51.2887 |          15.1268 |
[32m[20221213 23:12:35 @agent_ppo2.py:185][0m |          -0.0105 |          51.0012 |          15.1230 |
[32m[20221213 23:12:36 @agent_ppo2.py:185][0m |          -0.0110 |          50.8069 |          15.1139 |
[32m[20221213 23:12:36 @agent_ppo2.py:185][0m |          -0.0092 |          50.8188 |          15.1253 |
[32m[20221213 23:12:36 @agent_ppo2.py:185][0m |          -0.0109 |          50.5489 |          15.1146 |
[32m[20221213 23:12:36 @agent_ppo2.py:185][0m |          -0.0113 |          50.6012 |          15.1088 |
[32m[20221213 23:12:36 @agent_ppo2.py:185][0m |          -0.0149 |          50.4482 |          15.0934 |
[32m[20221213 23:12:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:12:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.41
[32m[20221213 23:12:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.21
[32m[20221213 23:12:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.01
[32m[20221213 23:12:36 @agent_ppo2.py:143][0m Total time:       0.07 min
[32m[20221213 23:12:36 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 23:12:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2003 --------------------------#
[32m[20221213 23:12:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:36 @agent_ppo2.py:185][0m |           0.0063 |          36.0743 |          15.2976 |
[32m[20221213 23:12:36 @agent_ppo2.py:185][0m |          -0.0092 |          30.8380 |          15.2935 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0033 |          29.2642 |          15.2821 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0001 |          29.3287 |          15.2811 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0095 |          27.8073 |          15.2598 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0020 |          28.3176 |          15.2496 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0092 |          27.0238 |          15.2592 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0083 |          26.8311 |          15.2596 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0171 |          26.4798 |          15.2617 |
[32m[20221213 23:12:37 @agent_ppo2.py:185][0m |          -0.0146 |          26.2714 |          15.2581 |
[32m[20221213 23:12:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:12:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.16
[32m[20221213 23:12:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 342.40
[32m[20221213 23:12:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.15
[32m[20221213 23:12:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 498.15
[32m[20221213 23:12:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 498.15
[32m[20221213 23:12:37 @agent_ppo2.py:143][0m Total time:       0.09 min
[32m[20221213 23:12:37 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 23:12:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2004 --------------------------#
[32m[20221213 23:12:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:12:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:38 @agent_ppo2.py:185][0m |           0.0050 |          24.3467 |          15.1192 |
[32m[20221213 23:12:38 @agent_ppo2.py:185][0m |          -0.0015 |          21.2394 |          15.1066 |
[32m[20221213 23:12:38 @agent_ppo2.py:185][0m |          -0.0081 |          20.0892 |          15.0848 |
[32m[20221213 23:12:38 @agent_ppo2.py:185][0m |          -0.0063 |          19.5405 |          15.0904 |
[32m[20221213 23:12:38 @agent_ppo2.py:185][0m |          -0.0081 |          19.0953 |          15.0912 |
[32m[20221213 23:12:38 @agent_ppo2.py:185][0m |          -0.0120 |          19.0011 |          15.0710 |
[32m[20221213 23:12:39 @agent_ppo2.py:185][0m |          -0.0119 |          18.4618 |          15.0601 |
[32m[20221213 23:12:39 @agent_ppo2.py:185][0m |          -0.0073 |          19.0954 |          15.0620 |
[32m[20221213 23:12:39 @agent_ppo2.py:185][0m |          -0.0106 |          17.9283 |          15.0595 |
[32m[20221213 23:12:39 @agent_ppo2.py:185][0m |          -0.0177 |          17.6031 |          15.0580 |
[32m[20221213 23:12:39 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 23:12:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.11
[32m[20221213 23:12:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.95
[32m[20221213 23:12:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.73
[32m[20221213 23:12:39 @agent_ppo2.py:143][0m Total time:       0.12 min
[32m[20221213 23:12:39 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 23:12:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2005 --------------------------#
[32m[20221213 23:12:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:39 @agent_ppo2.py:185][0m |           0.0074 |          55.8195 |          15.0354 |
[32m[20221213 23:12:39 @agent_ppo2.py:185][0m |           0.0141 |          60.7500 |          15.0317 |
[32m[20221213 23:12:39 @agent_ppo2.py:185][0m |          -0.0028 |          53.3715 |          15.0076 |
[32m[20221213 23:12:40 @agent_ppo2.py:185][0m |          -0.0043 |          52.8192 |          15.0236 |
[32m[20221213 23:12:40 @agent_ppo2.py:185][0m |          -0.0052 |          52.8274 |          15.0003 |
[32m[20221213 23:12:40 @agent_ppo2.py:185][0m |          -0.0062 |          52.5083 |          15.0212 |
[32m[20221213 23:12:40 @agent_ppo2.py:185][0m |           0.0011 |          54.0550 |          15.0097 |
[32m[20221213 23:12:40 @agent_ppo2.py:185][0m |          -0.0014 |          52.8730 |          14.9926 |
[32m[20221213 23:12:40 @agent_ppo2.py:185][0m |          -0.0100 |          52.2988 |          15.0225 |
[32m[20221213 23:12:40 @agent_ppo2.py:185][0m |          -0.0094 |          52.1677 |          15.0223 |
[32m[20221213 23:12:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:12:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.64
[32m[20221213 23:12:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.31
[32m[20221213 23:12:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.76
[32m[20221213 23:12:40 @agent_ppo2.py:143][0m Total time:       0.14 min
[32m[20221213 23:12:40 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 23:12:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2006 --------------------------#
[32m[20221213 23:12:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0005 |          23.3016 |          15.2737 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0018 |          20.2752 |          15.2660 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0087 |          19.5494 |          15.2627 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0085 |          19.2148 |          15.2503 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0059 |          19.8466 |          15.2287 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0115 |          18.6424 |          15.2167 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0102 |          18.4252 |          15.2208 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0135 |          18.4209 |          15.2165 |
[32m[20221213 23:12:41 @agent_ppo2.py:185][0m |          -0.0124 |          18.0719 |          15.2316 |
[32m[20221213 23:12:42 @agent_ppo2.py:185][0m |          -0.0170 |          17.9330 |          15.2015 |
[32m[20221213 23:12:42 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 23:12:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.46
[32m[20221213 23:12:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.66
[32m[20221213 23:12:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.72
[32m[20221213 23:12:42 @agent_ppo2.py:143][0m Total time:       0.16 min
[32m[20221213 23:12:42 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 23:12:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2007 --------------------------#
[32m[20221213 23:12:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:42 @agent_ppo2.py:185][0m |           0.0084 |          62.5411 |          14.9934 |
[32m[20221213 23:12:42 @agent_ppo2.py:185][0m |          -0.0026 |          55.9908 |          14.9756 |
[32m[20221213 23:12:42 @agent_ppo2.py:185][0m |          -0.0015 |          55.0006 |          15.0029 |
[32m[20221213 23:12:42 @agent_ppo2.py:185][0m |          -0.0036 |          54.5106 |          15.0016 |
[32m[20221213 23:12:42 @agent_ppo2.py:185][0m |          -0.0049 |          54.0083 |          14.9982 |
[32m[20221213 23:12:42 @agent_ppo2.py:185][0m |          -0.0059 |          53.6295 |          14.9891 |
[32m[20221213 23:12:43 @agent_ppo2.py:185][0m |          -0.0049 |          53.6055 |          14.9965 |
[32m[20221213 23:12:43 @agent_ppo2.py:185][0m |          -0.0119 |          53.2466 |          14.9952 |
[32m[20221213 23:12:43 @agent_ppo2.py:185][0m |          -0.0117 |          52.8444 |          15.0122 |
[32m[20221213 23:12:43 @agent_ppo2.py:185][0m |          -0.0081 |          53.2912 |          14.9939 |
[32m[20221213 23:12:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:12:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.51
[32m[20221213 23:12:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.62
[32m[20221213 23:12:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.04
[32m[20221213 23:12:43 @agent_ppo2.py:143][0m Total time:       0.18 min
[32m[20221213 23:12:43 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 23:12:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2008 --------------------------#
[32m[20221213 23:12:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:12:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:43 @agent_ppo2.py:185][0m |           0.0021 |          37.8874 |          15.0127 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0032 |          34.2255 |          15.0124 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0038 |          33.2982 |          14.9659 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0034 |          33.0443 |          14.9895 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |           0.0014 |          32.7243 |          14.9571 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0064 |          32.2829 |          14.9836 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0071 |          31.8916 |          14.9901 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0122 |          31.9147 |          14.9580 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0092 |          31.5989 |          14.9579 |
[32m[20221213 23:12:44 @agent_ppo2.py:185][0m |          -0.0112 |          31.5877 |          14.9960 |
[32m[20221213 23:12:44 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 23:12:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.37
[32m[20221213 23:12:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.88
[32m[20221213 23:12:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.32
[32m[20221213 23:12:44 @agent_ppo2.py:143][0m Total time:       0.21 min
[32m[20221213 23:12:44 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 23:12:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2009 --------------------------#
[32m[20221213 23:12:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:12:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0026 |          46.6912 |          15.2936 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0062 |          44.1908 |          15.3131 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0029 |          43.7989 |          15.3281 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0109 |          42.4983 |          15.3242 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0124 |          42.0820 |          15.3499 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0136 |          41.8423 |          15.3413 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0145 |          41.6997 |          15.3491 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0122 |          41.1434 |          15.3640 |
[32m[20221213 23:12:45 @agent_ppo2.py:185][0m |          -0.0164 |          40.9964 |          15.3436 |
[32m[20221213 23:12:46 @agent_ppo2.py:185][0m |          -0.0159 |          40.7700 |          15.3842 |
[32m[20221213 23:12:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:12:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.07
[32m[20221213 23:12:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.32
[32m[20221213 23:12:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.77
[32m[20221213 23:12:46 @agent_ppo2.py:143][0m Total time:       0.23 min
[32m[20221213 23:12:46 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 23:12:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2010 --------------------------#
[32m[20221213 23:12:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:12:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:46 @agent_ppo2.py:185][0m |          -0.0006 |          42.2062 |          15.3059 |
[32m[20221213 23:12:46 @agent_ppo2.py:185][0m |          -0.0031 |          39.6052 |          15.2859 |
[32m[20221213 23:12:46 @agent_ppo2.py:185][0m |          -0.0018 |          39.3202 |          15.2729 |
[32m[20221213 23:12:46 @agent_ppo2.py:185][0m |          -0.0028 |          39.0159 |          15.2413 |
[32m[20221213 23:12:46 @agent_ppo2.py:185][0m |           0.0061 |          42.2680 |          15.2512 |
[32m[20221213 23:12:47 @agent_ppo2.py:185][0m |          -0.0005 |          39.7831 |          15.2528 |
[32m[20221213 23:12:47 @agent_ppo2.py:185][0m |          -0.0107 |          37.3955 |          15.2315 |
[32m[20221213 23:12:47 @agent_ppo2.py:185][0m |          -0.0011 |          39.8071 |          15.2378 |
[32m[20221213 23:12:47 @agent_ppo2.py:185][0m |          -0.0120 |          36.8067 |          15.2179 |
[32m[20221213 23:12:47 @agent_ppo2.py:185][0m |          -0.0118 |          36.5813 |          15.2211 |
[32m[20221213 23:12:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:12:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.52
[32m[20221213 23:12:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.23
[32m[20221213 23:12:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.64
[32m[20221213 23:12:47 @agent_ppo2.py:143][0m Total time:       0.25 min
[32m[20221213 23:12:47 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 23:12:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2011 --------------------------#
[32m[20221213 23:12:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:12:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:47 @agent_ppo2.py:185][0m |          -0.0000 |          29.3790 |          15.1913 |
[32m[20221213 23:12:47 @agent_ppo2.py:185][0m |          -0.0030 |          27.0118 |          15.1676 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0072 |          26.2287 |          15.1867 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0099 |          25.7448 |          15.1726 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0070 |          25.3341 |          15.1663 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0074 |          25.2389 |          15.1525 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0133 |          24.8029 |          15.1412 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0117 |          24.6135 |          15.1359 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0143 |          24.4697 |          15.1134 |
[32m[20221213 23:12:48 @agent_ppo2.py:185][0m |          -0.0110 |          24.2495 |          15.1263 |
[32m[20221213 23:12:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:12:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.91
[32m[20221213 23:12:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.99
[32m[20221213 23:12:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.19
[32m[20221213 23:12:48 @agent_ppo2.py:143][0m Total time:       0.27 min
[32m[20221213 23:12:48 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 23:12:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2012 --------------------------#
[32m[20221213 23:12:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |           0.0042 |          40.9347 |          15.0655 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0033 |          37.3516 |          15.0535 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0052 |          36.1673 |          15.0451 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0090 |          35.3548 |          15.0573 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0029 |          35.9467 |          15.0781 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0136 |          34.1233 |          15.1016 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0102 |          33.6479 |          15.0832 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0146 |          33.2608 |          15.0933 |
[32m[20221213 23:12:49 @agent_ppo2.py:185][0m |          -0.0105 |          33.0463 |          15.1001 |
[32m[20221213 23:12:50 @agent_ppo2.py:185][0m |          -0.0110 |          32.5928 |          15.1133 |
[32m[20221213 23:12:50 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 23:12:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.73
[32m[20221213 23:12:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.63
[32m[20221213 23:12:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.04
[32m[20221213 23:12:50 @agent_ppo2.py:143][0m Total time:       0.30 min
[32m[20221213 23:12:50 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 23:12:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2013 --------------------------#
[32m[20221213 23:12:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:50 @agent_ppo2.py:185][0m |          -0.0001 |          39.6924 |          15.0669 |
[32m[20221213 23:12:50 @agent_ppo2.py:185][0m |          -0.0019 |          38.4476 |          15.0170 |
[32m[20221213 23:12:50 @agent_ppo2.py:185][0m |          -0.0033 |          36.1031 |          15.0144 |
[32m[20221213 23:12:50 @agent_ppo2.py:185][0m |          -0.0035 |          34.7902 |          14.9913 |
[32m[20221213 23:12:50 @agent_ppo2.py:185][0m |          -0.0098 |          34.2792 |          14.9870 |
[32m[20221213 23:12:51 @agent_ppo2.py:185][0m |          -0.0081 |          34.0216 |          14.9846 |
[32m[20221213 23:12:51 @agent_ppo2.py:185][0m |          -0.0133 |          33.7595 |          14.9584 |
[32m[20221213 23:12:51 @agent_ppo2.py:185][0m |          -0.0146 |          33.6347 |          14.9502 |
[32m[20221213 23:12:51 @agent_ppo2.py:185][0m |          -0.0102 |          33.5180 |          14.9753 |
[32m[20221213 23:12:51 @agent_ppo2.py:185][0m |          -0.0109 |          33.3568 |          14.9381 |
[32m[20221213 23:12:51 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 23:12:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.37
[32m[20221213 23:12:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.40
[32m[20221213 23:12:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.44
[32m[20221213 23:12:51 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 507.44
[32m[20221213 23:12:51 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 507.44
[32m[20221213 23:12:51 @agent_ppo2.py:143][0m Total time:       0.32 min
[32m[20221213 23:12:51 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 23:12:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2014 --------------------------#
[32m[20221213 23:12:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:51 @agent_ppo2.py:185][0m |           0.0128 |          40.2806 |          15.0401 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0096 |          32.3566 |          15.0272 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0110 |          31.2665 |          15.0176 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0105 |          31.2877 |          15.0404 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0116 |          30.1755 |          15.0463 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0162 |          29.8851 |          15.0461 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0129 |          29.6452 |          15.0526 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0180 |          29.3410 |          15.0593 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0163 |          29.1254 |          15.0512 |
[32m[20221213 23:12:52 @agent_ppo2.py:185][0m |          -0.0170 |          28.9862 |          15.0493 |
[32m[20221213 23:12:52 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 23:12:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.16
[32m[20221213 23:12:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.98
[32m[20221213 23:12:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.89
[32m[20221213 23:12:52 @agent_ppo2.py:143][0m Total time:       0.34 min
[32m[20221213 23:12:52 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 23:12:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2015 --------------------------#
[32m[20221213 23:12:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:53 @agent_ppo2.py:185][0m |           0.0072 |          46.5937 |          14.9718 |
[32m[20221213 23:12:53 @agent_ppo2.py:185][0m |           0.0006 |          41.7126 |          14.9383 |
[32m[20221213 23:12:53 @agent_ppo2.py:185][0m |          -0.0104 |          40.8723 |          14.9225 |
[32m[20221213 23:12:53 @agent_ppo2.py:185][0m |           0.0044 |          41.9709 |          14.8958 |
[32m[20221213 23:12:53 @agent_ppo2.py:185][0m |          -0.0075 |          40.0963 |          14.8735 |
[32m[20221213 23:12:53 @agent_ppo2.py:185][0m |          -0.0032 |          42.4758 |          14.8750 |
[32m[20221213 23:12:53 @agent_ppo2.py:185][0m |          -0.0100 |          39.6591 |          14.8314 |
[32m[20221213 23:12:54 @agent_ppo2.py:185][0m |          -0.0108 |          39.2434 |          14.8380 |
[32m[20221213 23:12:54 @agent_ppo2.py:185][0m |          -0.0129 |          38.8112 |          14.8452 |
[32m[20221213 23:12:54 @agent_ppo2.py:185][0m |          -0.0090 |          38.9941 |          14.8339 |
[32m[20221213 23:12:54 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 23:12:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.84
[32m[20221213 23:12:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.65
[32m[20221213 23:12:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.03
[32m[20221213 23:12:54 @agent_ppo2.py:143][0m Total time:       0.36 min
[32m[20221213 23:12:54 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 23:12:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2016 --------------------------#
[32m[20221213 23:12:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:54 @agent_ppo2.py:185][0m |          -0.0006 |          37.5343 |          14.9623 |
[32m[20221213 23:12:54 @agent_ppo2.py:185][0m |          -0.0019 |          35.3312 |          14.9635 |
[32m[20221213 23:12:54 @agent_ppo2.py:185][0m |          -0.0047 |          34.4631 |          14.9765 |
[32m[20221213 23:12:55 @agent_ppo2.py:185][0m |          -0.0055 |          33.8593 |          14.9792 |
[32m[20221213 23:12:55 @agent_ppo2.py:185][0m |          -0.0035 |          33.6936 |          14.9961 |
[32m[20221213 23:12:55 @agent_ppo2.py:185][0m |           0.0050 |          38.1236 |          14.9894 |
[32m[20221213 23:12:55 @agent_ppo2.py:185][0m |          -0.0103 |          33.2889 |          15.0189 |
[32m[20221213 23:12:55 @agent_ppo2.py:185][0m |          -0.0080 |          32.7243 |          15.0044 |
[32m[20221213 23:12:55 @agent_ppo2.py:185][0m |          -0.0039 |          33.4017 |          15.0132 |
[32m[20221213 23:12:55 @agent_ppo2.py:185][0m |          -0.0075 |          32.5467 |          15.0373 |
[32m[20221213 23:12:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:12:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.55
[32m[20221213 23:12:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.23
[32m[20221213 23:12:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.07
[32m[20221213 23:12:55 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 23:12:55 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 23:12:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2017 --------------------------#
[32m[20221213 23:12:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0020 |          35.6136 |          15.0442 |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0055 |          31.5863 |          15.0294 |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0079 |          30.4672 |          15.0289 |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0133 |          29.7339 |          15.0439 |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0059 |          29.8009 |          15.0319 |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0105 |          28.7491 |          15.0527 |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0152 |          28.4047 |          15.0417 |
[32m[20221213 23:12:56 @agent_ppo2.py:185][0m |          -0.0185 |          28.2779 |          15.0508 |
[32m[20221213 23:12:57 @agent_ppo2.py:185][0m |          -0.0122 |          28.0663 |          15.0402 |
[32m[20221213 23:12:57 @agent_ppo2.py:185][0m |          -0.0143 |          27.6725 |          15.0344 |
[32m[20221213 23:12:57 @agent_ppo2.py:130][0m Policy update time: 1.39 s
[32m[20221213 23:12:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.31
[32m[20221213 23:12:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.51
[32m[20221213 23:12:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.84
[32m[20221213 23:12:57 @agent_ppo2.py:143][0m Total time:       0.42 min
[32m[20221213 23:12:57 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 23:12:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2018 --------------------------#
[32m[20221213 23:12:57 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 23:12:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |           0.0147 |          44.1393 |          15.0916 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0032 |          36.9385 |          15.0977 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0111 |          35.6900 |          15.0770 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0055 |          34.9710 |          15.0723 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0108 |          34.7147 |          15.0315 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0116 |          34.1950 |          15.0358 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0132 |          33.8114 |          15.0485 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0123 |          33.4166 |          15.0212 |
[32m[20221213 23:12:58 @agent_ppo2.py:185][0m |          -0.0130 |          33.0618 |          15.0321 |
[32m[20221213 23:12:59 @agent_ppo2.py:185][0m |          -0.0040 |          34.1681 |          15.0308 |
[32m[20221213 23:12:59 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 23:12:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.83
[32m[20221213 23:12:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.67
[32m[20221213 23:12:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.75
[32m[20221213 23:12:59 @agent_ppo2.py:143][0m Total time:       0.44 min
[32m[20221213 23:12:59 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 23:12:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2019 --------------------------#
[32m[20221213 23:12:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:12:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:12:59 @agent_ppo2.py:185][0m |           0.0008 |          41.3143 |          14.9806 |
[32m[20221213 23:12:59 @agent_ppo2.py:185][0m |          -0.0103 |          40.1003 |          14.9479 |
[32m[20221213 23:12:59 @agent_ppo2.py:185][0m |          -0.0085 |          39.5925 |          14.9379 |
[32m[20221213 23:12:59 @agent_ppo2.py:185][0m |          -0.0127 |          38.9130 |          14.9662 |
[32m[20221213 23:12:59 @agent_ppo2.py:185][0m |          -0.0143 |          38.4742 |          14.9831 |
[32m[20221213 23:12:59 @agent_ppo2.py:185][0m |          -0.0159 |          38.1571 |          14.9811 |
[32m[20221213 23:13:00 @agent_ppo2.py:185][0m |          -0.0149 |          38.1913 |          14.9675 |
[32m[20221213 23:13:00 @agent_ppo2.py:185][0m |          -0.0173 |          37.8158 |          14.9821 |
[32m[20221213 23:13:00 @agent_ppo2.py:185][0m |          -0.0173 |          37.4803 |          14.9740 |
[32m[20221213 23:13:00 @agent_ppo2.py:185][0m |          -0.0133 |          37.5481 |          14.9894 |
[32m[20221213 23:13:00 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:13:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.06
[32m[20221213 23:13:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 371.51
[32m[20221213 23:13:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.78
[32m[20221213 23:13:00 @agent_ppo2.py:143][0m Total time:       0.47 min
[32m[20221213 23:13:00 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 23:13:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2020 --------------------------#
[32m[20221213 23:13:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 23:13:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:00 @agent_ppo2.py:185][0m |          -0.0009 |          49.6984 |          15.1817 |
[32m[20221213 23:13:00 @agent_ppo2.py:185][0m |           0.0115 |          52.8911 |          15.1747 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |           0.0046 |          51.6084 |          15.1814 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |          -0.0066 |          47.0279 |          15.1789 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |          -0.0100 |          46.5497 |          15.1931 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |          -0.0053 |          46.3194 |          15.1971 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |          -0.0111 |          46.1897 |          15.1813 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |          -0.0098 |          46.0051 |          15.1565 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |           0.0018 |          51.8167 |          15.1908 |
[32m[20221213 23:13:01 @agent_ppo2.py:185][0m |          -0.0089 |          45.7609 |          15.1774 |
[32m[20221213 23:13:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.87
[32m[20221213 23:13:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.37
[32m[20221213 23:13:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.52
[32m[20221213 23:13:01 @agent_ppo2.py:143][0m Total time:       0.49 min
[32m[20221213 23:13:01 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 23:13:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2021 --------------------------#
[32m[20221213 23:13:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0007 |          53.6360 |          15.0920 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0045 |          52.1505 |          15.0895 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0091 |          51.6005 |          15.0765 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0063 |          51.1916 |          15.0975 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0097 |          50.8882 |          15.0873 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0117 |          50.5993 |          15.0679 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0122 |          50.5560 |          15.0790 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0131 |          50.4065 |          15.0745 |
[32m[20221213 23:13:02 @agent_ppo2.py:185][0m |          -0.0104 |          50.1725 |          15.0770 |
[32m[20221213 23:13:03 @agent_ppo2.py:185][0m |          -0.0142 |          50.3013 |          15.0778 |
[32m[20221213 23:13:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.37
[32m[20221213 23:13:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.60
[32m[20221213 23:13:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.06
[32m[20221213 23:13:03 @agent_ppo2.py:143][0m Total time:       0.51 min
[32m[20221213 23:13:03 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 23:13:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2022 --------------------------#
[32m[20221213 23:13:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:03 @agent_ppo2.py:185][0m |           0.0003 |          51.4867 |          15.1316 |
[32m[20221213 23:13:03 @agent_ppo2.py:185][0m |          -0.0040 |          48.9823 |          15.1443 |
[32m[20221213 23:13:03 @agent_ppo2.py:185][0m |          -0.0070 |          48.1217 |          15.1406 |
[32m[20221213 23:13:03 @agent_ppo2.py:185][0m |          -0.0095 |          47.4417 |          15.1198 |
[32m[20221213 23:13:03 @agent_ppo2.py:185][0m |          -0.0078 |          46.9335 |          15.0936 |
[32m[20221213 23:13:03 @agent_ppo2.py:185][0m |          -0.0083 |          46.7742 |          15.0873 |
[32m[20221213 23:13:04 @agent_ppo2.py:185][0m |          -0.0116 |          46.3050 |          15.0696 |
[32m[20221213 23:13:04 @agent_ppo2.py:185][0m |          -0.0088 |          46.1719 |          15.0749 |
[32m[20221213 23:13:04 @agent_ppo2.py:185][0m |          -0.0101 |          45.9546 |          15.0680 |
[32m[20221213 23:13:04 @agent_ppo2.py:185][0m |          -0.0121 |          45.9270 |          15.0802 |
[32m[20221213 23:13:04 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 23:13:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.53
[32m[20221213 23:13:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.05
[32m[20221213 23:13:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.95
[32m[20221213 23:13:04 @agent_ppo2.py:143][0m Total time:       0.53 min
[32m[20221213 23:13:04 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 23:13:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2023 --------------------------#
[32m[20221213 23:13:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:04 @agent_ppo2.py:185][0m |          -0.0002 |          31.6379 |          15.0325 |
[32m[20221213 23:13:04 @agent_ppo2.py:185][0m |          -0.0006 |          23.4585 |          15.0438 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0019 |          21.4866 |          14.9943 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0095 |          20.3845 |          15.0166 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0115 |          19.8106 |          14.9817 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0084 |          19.4484 |          15.0133 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0126 |          19.1637 |          14.9933 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0141 |          19.1473 |          15.0027 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0063 |          18.7263 |          14.9886 |
[32m[20221213 23:13:05 @agent_ppo2.py:185][0m |          -0.0134 |          18.4606 |          15.0026 |
[32m[20221213 23:13:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.87
[32m[20221213 23:13:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.74
[32m[20221213 23:13:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.02
[32m[20221213 23:13:05 @agent_ppo2.py:143][0m Total time:       0.56 min
[32m[20221213 23:13:05 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 23:13:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2024 --------------------------#
[32m[20221213 23:13:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0019 |          58.7654 |          15.0996 |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0117 |          57.3696 |          15.1114 |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0107 |          56.5843 |          15.1280 |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0101 |          56.0308 |          15.1240 |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0077 |          56.1043 |          15.1318 |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0137 |          55.1820 |          15.1265 |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0145 |          54.9876 |          15.1282 |
[32m[20221213 23:13:06 @agent_ppo2.py:185][0m |          -0.0172 |          54.6331 |          15.1471 |
[32m[20221213 23:13:07 @agent_ppo2.py:185][0m |          -0.0155 |          54.4265 |          15.1339 |
[32m[20221213 23:13:07 @agent_ppo2.py:185][0m |          -0.0107 |          54.5026 |          15.1290 |
[32m[20221213 23:13:07 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 23:13:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.22
[32m[20221213 23:13:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.02
[32m[20221213 23:13:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.15
[32m[20221213 23:13:07 @agent_ppo2.py:143][0m Total time:       0.58 min
[32m[20221213 23:13:07 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 23:13:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2025 --------------------------#
[32m[20221213 23:13:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:07 @agent_ppo2.py:185][0m |           0.0006 |          35.2077 |          15.0803 |
[32m[20221213 23:13:07 @agent_ppo2.py:185][0m |          -0.0032 |          32.9083 |          15.0536 |
[32m[20221213 23:13:07 @agent_ppo2.py:185][0m |          -0.0084 |          32.2637 |          15.0597 |
[32m[20221213 23:13:07 @agent_ppo2.py:185][0m |          -0.0109 |          31.9676 |          15.0782 |
[32m[20221213 23:13:08 @agent_ppo2.py:185][0m |          -0.0075 |          31.6893 |          15.0654 |
[32m[20221213 23:13:08 @agent_ppo2.py:185][0m |          -0.0089 |          31.5156 |          15.0790 |
[32m[20221213 23:13:08 @agent_ppo2.py:185][0m |           0.0002 |          33.7917 |          15.0973 |
[32m[20221213 23:13:08 @agent_ppo2.py:185][0m |          -0.0100 |          31.2369 |          15.0789 |
[32m[20221213 23:13:08 @agent_ppo2.py:185][0m |          -0.0106 |          31.0875 |          15.1086 |
[32m[20221213 23:13:08 @agent_ppo2.py:185][0m |          -0.0048 |          31.6383 |          15.0993 |
[32m[20221213 23:13:08 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 23:13:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.69
[32m[20221213 23:13:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.05
[32m[20221213 23:13:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.45
[32m[20221213 23:13:08 @agent_ppo2.py:143][0m Total time:       0.60 min
[32m[20221213 23:13:08 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 23:13:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2026 --------------------------#
[32m[20221213 23:13:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |           0.0007 |          34.7225 |          14.9762 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0077 |          30.5096 |          14.9615 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0062 |          29.5076 |          14.9537 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0084 |          28.9542 |          14.9353 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0099 |          28.6279 |          14.9269 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |           0.0023 |          32.9116 |          14.9632 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0128 |          28.0720 |          14.9371 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0116 |          27.7458 |          14.9419 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0100 |          27.6147 |          14.9387 |
[32m[20221213 23:13:09 @agent_ppo2.py:185][0m |          -0.0115 |          27.4188 |          14.9444 |
[32m[20221213 23:13:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:13:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.11
[32m[20221213 23:13:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.28
[32m[20221213 23:13:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.41
[32m[20221213 23:13:10 @agent_ppo2.py:143][0m Total time:       0.63 min
[32m[20221213 23:13:10 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 23:13:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2027 --------------------------#
[32m[20221213 23:13:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:10 @agent_ppo2.py:185][0m |           0.0024 |          63.9348 |          15.1041 |
[32m[20221213 23:13:10 @agent_ppo2.py:185][0m |           0.0030 |          63.4401 |          15.0779 |
[32m[20221213 23:13:10 @agent_ppo2.py:185][0m |          -0.0069 |          62.3100 |          15.0878 |
[32m[20221213 23:13:10 @agent_ppo2.py:185][0m |           0.0028 |          64.7673 |          15.0999 |
[32m[20221213 23:13:10 @agent_ppo2.py:185][0m |          -0.0059 |          61.6926 |          15.0879 |
[32m[20221213 23:13:10 @agent_ppo2.py:185][0m |           0.0082 |          66.4027 |          15.0841 |
[32m[20221213 23:13:10 @agent_ppo2.py:185][0m |          -0.0103 |          61.6608 |          15.0870 |
[32m[20221213 23:13:11 @agent_ppo2.py:185][0m |          -0.0103 |          61.2372 |          15.0643 |
[32m[20221213 23:13:11 @agent_ppo2.py:185][0m |          -0.0115 |          61.1135 |          15.0630 |
[32m[20221213 23:13:11 @agent_ppo2.py:185][0m |          -0.0102 |          61.0062 |          15.0674 |
[32m[20221213 23:13:11 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 23:13:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.33
[32m[20221213 23:13:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.74
[32m[20221213 23:13:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.60
[32m[20221213 23:13:11 @agent_ppo2.py:143][0m Total time:       0.65 min
[32m[20221213 23:13:11 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 23:13:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2028 --------------------------#
[32m[20221213 23:13:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:13:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:11 @agent_ppo2.py:185][0m |          -0.0007 |          38.5485 |          15.0439 |
[32m[20221213 23:13:11 @agent_ppo2.py:185][0m |           0.0036 |          37.4955 |          15.0490 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |          -0.0023 |          36.2155 |          15.0417 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |          -0.0018 |          35.8414 |          15.0426 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |          -0.0077 |          35.4190 |          15.0398 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |          -0.0064 |          35.2502 |          15.0328 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |           0.0002 |          35.7015 |          15.0521 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |          -0.0051 |          35.0678 |          15.0089 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |          -0.0119 |          34.7199 |          15.0146 |
[32m[20221213 23:13:12 @agent_ppo2.py:185][0m |          -0.0097 |          34.6924 |          15.0056 |
[32m[20221213 23:13:12 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 23:13:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.01
[32m[20221213 23:13:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.93
[32m[20221213 23:13:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.94
[32m[20221213 23:13:12 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221213 23:13:12 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 23:13:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2029 --------------------------#
[32m[20221213 23:13:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:13 @agent_ppo2.py:185][0m |           0.0032 |          41.9626 |          14.8913 |
[32m[20221213 23:13:13 @agent_ppo2.py:185][0m |           0.0101 |          44.4698 |          14.8924 |
[32m[20221213 23:13:13 @agent_ppo2.py:185][0m |          -0.0038 |          39.0700 |          14.8579 |
[32m[20221213 23:13:13 @agent_ppo2.py:185][0m |          -0.0094 |          38.6265 |          14.8747 |
[32m[20221213 23:13:13 @agent_ppo2.py:185][0m |          -0.0026 |          39.1805 |          14.8429 |
[32m[20221213 23:13:13 @agent_ppo2.py:185][0m |          -0.0099 |          38.3293 |          14.8589 |
[32m[20221213 23:13:13 @agent_ppo2.py:185][0m |          -0.0119 |          38.0931 |          14.8297 |
[32m[20221213 23:13:14 @agent_ppo2.py:185][0m |          -0.0110 |          37.9685 |          14.8389 |
[32m[20221213 23:13:14 @agent_ppo2.py:185][0m |          -0.0175 |          37.7591 |          14.8298 |
[32m[20221213 23:13:14 @agent_ppo2.py:185][0m |          -0.0138 |          37.8985 |          14.8146 |
[32m[20221213 23:13:14 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 23:13:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.89
[32m[20221213 23:13:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.44
[32m[20221213 23:13:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.40
[32m[20221213 23:13:14 @agent_ppo2.py:143][0m Total time:       0.70 min
[32m[20221213 23:13:14 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 23:13:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2030 --------------------------#
[32m[20221213 23:13:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:13:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:14 @agent_ppo2.py:185][0m |           0.0016 |          54.5226 |          14.9834 |
[32m[20221213 23:13:14 @agent_ppo2.py:185][0m |          -0.0041 |          53.7355 |          15.0034 |
[32m[20221213 23:13:14 @agent_ppo2.py:185][0m |          -0.0108 |          53.2174 |          14.9830 |
[32m[20221213 23:13:15 @agent_ppo2.py:185][0m |          -0.0117 |          53.0410 |          15.0172 |
[32m[20221213 23:13:15 @agent_ppo2.py:185][0m |          -0.0090 |          52.7239 |          15.0336 |
[32m[20221213 23:13:15 @agent_ppo2.py:185][0m |          -0.0108 |          52.8587 |          15.0273 |
[32m[20221213 23:13:15 @agent_ppo2.py:185][0m |          -0.0107 |          52.5577 |          15.0220 |
[32m[20221213 23:13:15 @agent_ppo2.py:185][0m |          -0.0118 |          52.4901 |          15.0408 |
[32m[20221213 23:13:15 @agent_ppo2.py:185][0m |          -0.0113 |          52.4083 |          15.0510 |
[32m[20221213 23:13:15 @agent_ppo2.py:185][0m |          -0.0123 |          52.4009 |          15.0711 |
[32m[20221213 23:13:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.27
[32m[20221213 23:13:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.82
[32m[20221213 23:13:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.05
[32m[20221213 23:13:15 @agent_ppo2.py:143][0m Total time:       0.72 min
[32m[20221213 23:13:15 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 23:13:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2031 --------------------------#
[32m[20221213 23:13:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |          -0.0008 |          38.9965 |          14.8736 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |           0.0020 |          39.2631 |          14.8876 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |          -0.0082 |          36.0936 |          14.8700 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |          -0.0041 |          36.1200 |          14.8899 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |           0.0043 |          39.1604 |          14.8882 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |          -0.0092 |          34.7461 |          14.9058 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |          -0.0110 |          34.4115 |          14.8900 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |          -0.0113 |          34.2656 |          14.8835 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |          -0.0104 |          34.0170 |          14.9050 |
[32m[20221213 23:13:16 @agent_ppo2.py:185][0m |           0.0016 |          36.3646 |          14.9201 |
[32m[20221213 23:13:16 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:13:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.25
[32m[20221213 23:13:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.39
[32m[20221213 23:13:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.23
[32m[20221213 23:13:17 @agent_ppo2.py:143][0m Total time:       0.74 min
[32m[20221213 23:13:17 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 23:13:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2032 --------------------------#
[32m[20221213 23:13:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:17 @agent_ppo2.py:185][0m |          -0.0006 |          37.5903 |          15.0859 |
[32m[20221213 23:13:17 @agent_ppo2.py:185][0m |          -0.0009 |          35.4832 |          15.1049 |
[32m[20221213 23:13:17 @agent_ppo2.py:185][0m |          -0.0051 |          34.7047 |          15.1067 |
[32m[20221213 23:13:17 @agent_ppo2.py:185][0m |          -0.0048 |          34.3831 |          15.1153 |
[32m[20221213 23:13:17 @agent_ppo2.py:185][0m |          -0.0084 |          34.1443 |          15.1355 |
[32m[20221213 23:13:17 @agent_ppo2.py:185][0m |          -0.0138 |          33.8748 |          15.1334 |
[32m[20221213 23:13:17 @agent_ppo2.py:185][0m |          -0.0120 |          33.8318 |          15.1251 |
[32m[20221213 23:13:18 @agent_ppo2.py:185][0m |          -0.0120 |          33.6505 |          15.1477 |
[32m[20221213 23:13:18 @agent_ppo2.py:185][0m |          -0.0117 |          33.4357 |          15.1585 |
[32m[20221213 23:13:18 @agent_ppo2.py:185][0m |          -0.0112 |          33.4560 |          15.1641 |
[32m[20221213 23:13:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:13:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.38
[32m[20221213 23:13:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.41
[32m[20221213 23:13:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.29
[32m[20221213 23:13:18 @agent_ppo2.py:143][0m Total time:       0.76 min
[32m[20221213 23:13:18 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 23:13:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2033 --------------------------#
[32m[20221213 23:13:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:18 @agent_ppo2.py:185][0m |           0.0004 |          43.3165 |          14.9637 |
[32m[20221213 23:13:18 @agent_ppo2.py:185][0m |          -0.0031 |          41.5035 |          14.9378 |
[32m[20221213 23:13:18 @agent_ppo2.py:185][0m |          -0.0060 |          40.9767 |          14.9401 |
[32m[20221213 23:13:18 @agent_ppo2.py:185][0m |          -0.0059 |          40.8307 |          14.9296 |
[32m[20221213 23:13:19 @agent_ppo2.py:185][0m |          -0.0093 |          40.1683 |          14.9208 |
[32m[20221213 23:13:19 @agent_ppo2.py:185][0m |          -0.0125 |          39.8514 |          14.9285 |
[32m[20221213 23:13:19 @agent_ppo2.py:185][0m |          -0.0032 |          40.4170 |          14.9306 |
[32m[20221213 23:13:19 @agent_ppo2.py:185][0m |          -0.0022 |          41.0462 |          14.9353 |
[32m[20221213 23:13:19 @agent_ppo2.py:185][0m |          -0.0134 |          39.2986 |          14.9452 |
[32m[20221213 23:13:19 @agent_ppo2.py:185][0m |          -0.0066 |          40.8344 |          14.9355 |
[32m[20221213 23:13:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:13:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.50
[32m[20221213 23:13:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.35
[32m[20221213 23:13:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.18
[32m[20221213 23:13:19 @agent_ppo2.py:143][0m Total time:       0.79 min
[32m[20221213 23:13:19 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 23:13:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2034 --------------------------#
[32m[20221213 23:13:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |           0.0040 |          50.4106 |          15.0410 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0045 |          48.3989 |          15.0189 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0040 |          47.9432 |          15.0141 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0050 |          47.3736 |          15.0028 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0028 |          48.1633 |          14.9858 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0088 |          46.9706 |          14.9725 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0132 |          46.7636 |          14.9633 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0092 |          46.5261 |          14.9595 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0121 |          46.3742 |          14.9781 |
[32m[20221213 23:13:20 @agent_ppo2.py:185][0m |          -0.0141 |          46.2637 |          14.9447 |
[32m[20221213 23:13:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.34
[32m[20221213 23:13:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.51
[32m[20221213 23:13:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.58
[32m[20221213 23:13:20 @agent_ppo2.py:143][0m Total time:       0.81 min
[32m[20221213 23:13:20 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 23:13:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2035 --------------------------#
[32m[20221213 23:13:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0006 |          42.1314 |          15.0691 |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0028 |          40.1891 |          15.0684 |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0072 |          39.5908 |          15.0588 |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0059 |          39.3691 |          15.0508 |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0095 |          39.0846 |          15.0412 |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0088 |          38.9452 |          15.0307 |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0102 |          38.7642 |          15.0212 |
[32m[20221213 23:13:21 @agent_ppo2.py:185][0m |          -0.0030 |          39.5709 |          15.0249 |
[32m[20221213 23:13:22 @agent_ppo2.py:185][0m |          -0.0102 |          38.6056 |          15.0082 |
[32m[20221213 23:13:22 @agent_ppo2.py:185][0m |          -0.0097 |          38.3607 |          15.0210 |
[32m[20221213 23:13:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.24
[32m[20221213 23:13:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.18
[32m[20221213 23:13:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.65
[32m[20221213 23:13:22 @agent_ppo2.py:143][0m Total time:       0.83 min
[32m[20221213 23:13:22 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 23:13:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2036 --------------------------#
[32m[20221213 23:13:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:22 @agent_ppo2.py:185][0m |           0.0105 |          48.3989 |          15.1899 |
[32m[20221213 23:13:22 @agent_ppo2.py:185][0m |          -0.0052 |          45.3485 |          15.1844 |
[32m[20221213 23:13:22 @agent_ppo2.py:185][0m |          -0.0073 |          44.7716 |          15.1607 |
[32m[20221213 23:13:22 @agent_ppo2.py:185][0m |          -0.0088 |          44.3750 |          15.1753 |
[32m[20221213 23:13:22 @agent_ppo2.py:185][0m |          -0.0072 |          44.2767 |          15.1625 |
[32m[20221213 23:13:23 @agent_ppo2.py:185][0m |          -0.0068 |          44.5610 |          15.1561 |
[32m[20221213 23:13:23 @agent_ppo2.py:185][0m |          -0.0108 |          43.9304 |          15.1525 |
[32m[20221213 23:13:23 @agent_ppo2.py:185][0m |          -0.0112 |          43.6878 |          15.1642 |
[32m[20221213 23:13:23 @agent_ppo2.py:185][0m |          -0.0107 |          43.6327 |          15.1558 |
[32m[20221213 23:13:23 @agent_ppo2.py:185][0m |          -0.0052 |          44.1475 |          15.1535 |
[32m[20221213 23:13:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:13:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.14
[32m[20221213 23:13:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.90
[32m[20221213 23:13:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.52
[32m[20221213 23:13:23 @agent_ppo2.py:143][0m Total time:       0.85 min
[32m[20221213 23:13:23 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 23:13:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2037 --------------------------#
[32m[20221213 23:13:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:23 @agent_ppo2.py:185][0m |           0.0003 |          54.7531 |          15.0668 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0058 |          52.7786 |          15.0370 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0064 |          52.1849 |          15.0470 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0086 |          52.2023 |          15.0526 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0057 |          52.1563 |          15.0243 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0081 |          51.8335 |          15.0584 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0118 |          51.6579 |          15.0560 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0006 |          53.8296 |          15.0538 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0050 |          52.2017 |          15.0638 |
[32m[20221213 23:13:24 @agent_ppo2.py:185][0m |          -0.0107 |          51.1772 |          15.0709 |
[32m[20221213 23:13:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:13:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.27
[32m[20221213 23:13:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.86
[32m[20221213 23:13:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.74
[32m[20221213 23:13:24 @agent_ppo2.py:143][0m Total time:       0.87 min
[32m[20221213 23:13:24 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 23:13:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2038 --------------------------#
[32m[20221213 23:13:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0011 |          39.7681 |          15.0804 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0030 |          35.7992 |          15.0881 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0035 |          34.8641 |          15.0713 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0061 |          34.2977 |          15.0728 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0084 |          33.6244 |          15.0945 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0090 |          33.1105 |          15.0923 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0079 |          32.6742 |          15.0832 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0070 |          32.8342 |          15.0938 |
[32m[20221213 23:13:25 @agent_ppo2.py:185][0m |          -0.0099 |          32.2243 |          15.1017 |
[32m[20221213 23:13:26 @agent_ppo2.py:185][0m |          -0.0099 |          32.0136 |          15.1167 |
[32m[20221213 23:13:26 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.62
[32m[20221213 23:13:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.65
[32m[20221213 23:13:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.24
[32m[20221213 23:13:26 @agent_ppo2.py:143][0m Total time:       0.89 min
[32m[20221213 23:13:26 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 23:13:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2039 --------------------------#
[32m[20221213 23:13:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:26 @agent_ppo2.py:185][0m |           0.0018 |          44.5377 |          14.9289 |
[32m[20221213 23:13:26 @agent_ppo2.py:185][0m |           0.0008 |          43.6390 |          14.9028 |
[32m[20221213 23:13:26 @agent_ppo2.py:185][0m |          -0.0055 |          42.8898 |          14.9105 |
[32m[20221213 23:13:26 @agent_ppo2.py:185][0m |          -0.0035 |          42.7520 |          14.9021 |
[32m[20221213 23:13:26 @agent_ppo2.py:185][0m |          -0.0068 |          42.5469 |          14.8839 |
[32m[20221213 23:13:26 @agent_ppo2.py:185][0m |          -0.0076 |          42.3225 |          14.9085 |
[32m[20221213 23:13:27 @agent_ppo2.py:185][0m |          -0.0072 |          42.2415 |          14.8992 |
[32m[20221213 23:13:27 @agent_ppo2.py:185][0m |          -0.0106 |          42.1333 |          14.9047 |
[32m[20221213 23:13:27 @agent_ppo2.py:185][0m |          -0.0064 |          42.1030 |          14.8930 |
[32m[20221213 23:13:27 @agent_ppo2.py:185][0m |          -0.0073 |          41.9874 |          14.9008 |
[32m[20221213 23:13:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.06
[32m[20221213 23:13:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.51
[32m[20221213 23:13:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.89
[32m[20221213 23:13:27 @agent_ppo2.py:143][0m Total time:       0.92 min
[32m[20221213 23:13:27 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 23:13:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2040 --------------------------#
[32m[20221213 23:13:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:13:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:27 @agent_ppo2.py:185][0m |           0.0007 |          38.7309 |          15.2777 |
[32m[20221213 23:13:27 @agent_ppo2.py:185][0m |          -0.0069 |          37.2675 |          15.2720 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0068 |          36.8137 |          15.2542 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0065 |          36.5923 |          15.2414 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0081 |          36.3251 |          15.2504 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0009 |          37.5582 |          15.2439 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0087 |          36.0177 |          15.2384 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0110 |          35.9124 |          15.2346 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0052 |          36.1997 |          15.2480 |
[32m[20221213 23:13:28 @agent_ppo2.py:185][0m |          -0.0124 |          35.8215 |          15.2498 |
[32m[20221213 23:13:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.56
[32m[20221213 23:13:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.04
[32m[20221213 23:13:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.90
[32m[20221213 23:13:28 @agent_ppo2.py:143][0m Total time:       0.94 min
[32m[20221213 23:13:28 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 23:13:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2041 --------------------------#
[32m[20221213 23:13:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |           0.0019 |          43.4595 |          15.0238 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0028 |          42.1048 |          15.0018 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |           0.0012 |          41.8324 |          15.0246 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0079 |          40.6691 |          15.0160 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0089 |          40.3841 |          15.0065 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0121 |          40.0906 |          15.0070 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0108 |          39.7616 |          15.0454 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0086 |          39.4475 |          15.0312 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0085 |          39.4511 |          15.0132 |
[32m[20221213 23:13:29 @agent_ppo2.py:185][0m |          -0.0107 |          39.0225 |          15.0216 |
[32m[20221213 23:13:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:13:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.26
[32m[20221213 23:13:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.08
[32m[20221213 23:13:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:13:30 @agent_ppo2.py:143][0m Total time:       0.96 min
[32m[20221213 23:13:30 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 23:13:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2042 --------------------------#
[32m[20221213 23:13:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:30 @agent_ppo2.py:185][0m |          -0.0019 |          55.1606 |          14.9508 |
[32m[20221213 23:13:30 @agent_ppo2.py:185][0m |          -0.0014 |          53.7461 |          14.9352 |
[32m[20221213 23:13:30 @agent_ppo2.py:185][0m |           0.0038 |          54.3747 |          14.9481 |
[32m[20221213 23:13:30 @agent_ppo2.py:185][0m |           0.0081 |          58.0776 |          14.9088 |
[32m[20221213 23:13:30 @agent_ppo2.py:185][0m |          -0.0035 |          52.9575 |          14.9287 |
[32m[20221213 23:13:30 @agent_ppo2.py:185][0m |          -0.0068 |          52.7412 |          14.9188 |
[32m[20221213 23:13:31 @agent_ppo2.py:185][0m |          -0.0077 |          52.7563 |          14.9085 |
[32m[20221213 23:13:31 @agent_ppo2.py:185][0m |          -0.0077 |          52.7138 |          14.9226 |
[32m[20221213 23:13:31 @agent_ppo2.py:185][0m |          -0.0090 |          52.6295 |          14.9124 |
[32m[20221213 23:13:31 @agent_ppo2.py:185][0m |          -0.0084 |          52.4934 |          14.9259 |
[32m[20221213 23:13:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:13:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.39
[32m[20221213 23:13:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.12
[32m[20221213 23:13:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.79
[32m[20221213 23:13:31 @agent_ppo2.py:143][0m Total time:       0.98 min
[32m[20221213 23:13:31 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 23:13:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2043 --------------------------#
[32m[20221213 23:13:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:31 @agent_ppo2.py:185][0m |          -0.0044 |          42.2703 |          14.9461 |
[32m[20221213 23:13:31 @agent_ppo2.py:185][0m |          -0.0048 |          39.8251 |          14.9416 |
[32m[20221213 23:13:31 @agent_ppo2.py:185][0m |          -0.0135 |          39.0623 |          14.9470 |
[32m[20221213 23:13:32 @agent_ppo2.py:185][0m |          -0.0079 |          38.4319 |          14.9584 |
[32m[20221213 23:13:32 @agent_ppo2.py:185][0m |          -0.0124 |          37.9676 |          14.9419 |
[32m[20221213 23:13:32 @agent_ppo2.py:185][0m |          -0.0096 |          37.5340 |          14.9393 |
[32m[20221213 23:13:32 @agent_ppo2.py:185][0m |          -0.0145 |          37.3772 |          14.9464 |
[32m[20221213 23:13:32 @agent_ppo2.py:185][0m |          -0.0127 |          37.1077 |          14.9491 |
[32m[20221213 23:13:32 @agent_ppo2.py:185][0m |          -0.0147 |          36.8308 |          14.9389 |
[32m[20221213 23:13:32 @agent_ppo2.py:185][0m |          -0.0152 |          36.6317 |          14.9392 |
[32m[20221213 23:13:32 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.77
[32m[20221213 23:13:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.87
[32m[20221213 23:13:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.40
[32m[20221213 23:13:32 @agent_ppo2.py:143][0m Total time:       1.00 min
[32m[20221213 23:13:32 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 23:13:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2044 --------------------------#
[32m[20221213 23:13:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |           0.0040 |          50.3846 |          15.1420 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0041 |          46.5560 |          15.1594 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0060 |          45.6231 |          15.1373 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0057 |          44.5071 |          15.1351 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0056 |          43.8395 |          15.1235 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0106 |          43.3830 |          15.1240 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0036 |          43.4563 |          15.1185 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0073 |          42.6505 |          15.1242 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |          -0.0052 |          43.0711 |          15.1273 |
[32m[20221213 23:13:33 @agent_ppo2.py:185][0m |           0.0133 |          46.0321 |          15.1098 |
[32m[20221213 23:13:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:13:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.33
[32m[20221213 23:13:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.23
[32m[20221213 23:13:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.55
[32m[20221213 23:13:34 @agent_ppo2.py:143][0m Total time:       1.03 min
[32m[20221213 23:13:34 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 23:13:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2045 --------------------------#
[32m[20221213 23:13:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:34 @agent_ppo2.py:185][0m |          -0.0005 |          45.2028 |          14.9371 |
[32m[20221213 23:13:34 @agent_ppo2.py:185][0m |          -0.0019 |          42.6430 |          14.9333 |
[32m[20221213 23:13:34 @agent_ppo2.py:185][0m |          -0.0082 |          41.4796 |          14.9342 |
[32m[20221213 23:13:34 @agent_ppo2.py:185][0m |          -0.0125 |          40.8931 |          14.9415 |
[32m[20221213 23:13:34 @agent_ppo2.py:185][0m |          -0.0124 |          40.2615 |          14.9597 |
[32m[20221213 23:13:34 @agent_ppo2.py:185][0m |          -0.0082 |          40.0231 |          14.9621 |
[32m[20221213 23:13:34 @agent_ppo2.py:185][0m |          -0.0130 |          39.4493 |          14.9812 |
[32m[20221213 23:13:35 @agent_ppo2.py:185][0m |          -0.0124 |          39.2468 |          14.9698 |
[32m[20221213 23:13:35 @agent_ppo2.py:185][0m |          -0.0094 |          38.9221 |          14.9801 |
[32m[20221213 23:13:35 @agent_ppo2.py:185][0m |          -0.0123 |          38.6271 |          14.9974 |
[32m[20221213 23:13:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.68
[32m[20221213 23:13:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.36
[32m[20221213 23:13:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.56
[32m[20221213 23:13:35 @agent_ppo2.py:143][0m Total time:       1.05 min
[32m[20221213 23:13:35 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 23:13:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2046 --------------------------#
[32m[20221213 23:13:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:35 @agent_ppo2.py:185][0m |           0.0035 |          46.9158 |          14.9389 |
[32m[20221213 23:13:35 @agent_ppo2.py:185][0m |          -0.0045 |          43.9794 |          14.9317 |
[32m[20221213 23:13:35 @agent_ppo2.py:185][0m |          -0.0047 |          43.4347 |          14.9438 |
[32m[20221213 23:13:35 @agent_ppo2.py:185][0m |          -0.0070 |          42.5150 |          14.9088 |
[32m[20221213 23:13:36 @agent_ppo2.py:185][0m |          -0.0082 |          42.1879 |          14.8979 |
[32m[20221213 23:13:36 @agent_ppo2.py:185][0m |          -0.0046 |          43.0341 |          14.8970 |
[32m[20221213 23:13:36 @agent_ppo2.py:185][0m |          -0.0076 |          41.6486 |          14.8959 |
[32m[20221213 23:13:36 @agent_ppo2.py:185][0m |          -0.0122 |          41.4050 |          14.8782 |
[32m[20221213 23:13:36 @agent_ppo2.py:185][0m |          -0.0143 |          41.2695 |          14.8769 |
[32m[20221213 23:13:36 @agent_ppo2.py:185][0m |          -0.0059 |          43.5279 |          14.8683 |
[32m[20221213 23:13:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:13:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.81
[32m[20221213 23:13:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.79
[32m[20221213 23:13:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.90
[32m[20221213 23:13:36 @agent_ppo2.py:143][0m Total time:       1.07 min
[32m[20221213 23:13:36 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 23:13:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2047 --------------------------#
[32m[20221213 23:13:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |           0.0010 |          46.9576 |          15.0511 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0053 |          46.1797 |          15.0140 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0031 |          45.9240 |          15.0309 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0080 |          45.8747 |          15.0455 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0051 |          45.6921 |          15.0419 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0072 |          45.7622 |          15.0386 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0060 |          45.7856 |          15.0289 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0073 |          45.5185 |          15.0281 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0083 |          45.4805 |          15.0451 |
[32m[20221213 23:13:37 @agent_ppo2.py:185][0m |          -0.0061 |          45.5279 |          15.0116 |
[32m[20221213 23:13:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:13:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.59
[32m[20221213 23:13:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.02
[32m[20221213 23:13:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.84
[32m[20221213 23:13:38 @agent_ppo2.py:143][0m Total time:       1.09 min
[32m[20221213 23:13:38 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 23:13:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2048 --------------------------#
[32m[20221213 23:13:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0038 |          45.8348 |          15.0829 |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0060 |          43.9857 |          15.0779 |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0064 |          43.2652 |          15.0960 |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0067 |          42.7188 |          15.1128 |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0096 |          42.2244 |          15.1181 |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0086 |          42.1441 |          15.1346 |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0121 |          41.8665 |          15.1337 |
[32m[20221213 23:13:38 @agent_ppo2.py:185][0m |          -0.0129 |          41.6907 |          15.1490 |
[32m[20221213 23:13:39 @agent_ppo2.py:185][0m |          -0.0116 |          41.6396 |          15.1699 |
[32m[20221213 23:13:39 @agent_ppo2.py:185][0m |          -0.0110 |          41.4905 |          15.1813 |
[32m[20221213 23:13:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:13:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.85
[32m[20221213 23:13:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.65
[32m[20221213 23:13:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.45
[32m[20221213 23:13:39 @agent_ppo2.py:143][0m Total time:       1.11 min
[32m[20221213 23:13:39 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 23:13:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2049 --------------------------#
[32m[20221213 23:13:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:39 @agent_ppo2.py:185][0m |          -0.0030 |          47.9122 |          15.0978 |
[32m[20221213 23:13:39 @agent_ppo2.py:185][0m |          -0.0059 |          45.8201 |          15.0678 |
[32m[20221213 23:13:39 @agent_ppo2.py:185][0m |          -0.0074 |          44.8779 |          15.0852 |
[32m[20221213 23:13:39 @agent_ppo2.py:185][0m |          -0.0110 |          44.5943 |          15.0638 |
[32m[20221213 23:13:40 @agent_ppo2.py:185][0m |          -0.0090 |          44.1450 |          15.0584 |
[32m[20221213 23:13:40 @agent_ppo2.py:185][0m |           0.0011 |          45.5417 |          15.0714 |
[32m[20221213 23:13:40 @agent_ppo2.py:185][0m |          -0.0103 |          43.8112 |          15.0386 |
[32m[20221213 23:13:40 @agent_ppo2.py:185][0m |          -0.0136 |          43.4696 |          15.0337 |
[32m[20221213 23:13:40 @agent_ppo2.py:185][0m |          -0.0119 |          43.3131 |          15.0477 |
[32m[20221213 23:13:40 @agent_ppo2.py:185][0m |          -0.0139 |          43.1100 |          15.0484 |
[32m[20221213 23:13:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.37
[32m[20221213 23:13:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.87
[32m[20221213 23:13:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.62
[32m[20221213 23:13:40 @agent_ppo2.py:143][0m Total time:       1.14 min
[32m[20221213 23:13:40 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 23:13:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2050 --------------------------#
[32m[20221213 23:13:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:13:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:40 @agent_ppo2.py:185][0m |           0.0020 |          55.5442 |          14.9078 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0052 |          52.6727 |          14.8753 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0079 |          52.1130 |          14.9216 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0097 |          51.7267 |          14.9237 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0094 |          51.2905 |          14.9366 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0082 |          51.1082 |          14.9455 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0113 |          51.0017 |          14.9582 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0095 |          50.8809 |          14.9630 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0102 |          50.6136 |          14.9771 |
[32m[20221213 23:13:41 @agent_ppo2.py:185][0m |          -0.0120 |          50.6260 |          14.9848 |
[32m[20221213 23:13:41 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.90
[32m[20221213 23:13:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.51
[32m[20221213 23:13:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.69
[32m[20221213 23:13:41 @agent_ppo2.py:143][0m Total time:       1.16 min
[32m[20221213 23:13:41 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 23:13:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2051 --------------------------#
[32m[20221213 23:13:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |           0.0019 |          45.9725 |          15.0310 |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |          -0.0030 |          43.3494 |          15.0283 |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |          -0.0030 |          42.1705 |          15.0475 |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |          -0.0053 |          41.6681 |          15.0340 |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |           0.0035 |          43.2846 |          15.0214 |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |          -0.0120 |          41.3328 |          15.0386 |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |          -0.0102 |          40.9402 |          15.0530 |
[32m[20221213 23:13:42 @agent_ppo2.py:185][0m |          -0.0050 |          41.1314 |          15.0560 |
[32m[20221213 23:13:43 @agent_ppo2.py:185][0m |          -0.0158 |          40.7021 |          15.0586 |
[32m[20221213 23:13:43 @agent_ppo2.py:185][0m |          -0.0143 |          40.4522 |          15.0701 |
[32m[20221213 23:13:43 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.63
[32m[20221213 23:13:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.19
[32m[20221213 23:13:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.71
[32m[20221213 23:13:43 @agent_ppo2.py:143][0m Total time:       1.18 min
[32m[20221213 23:13:43 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 23:13:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2052 --------------------------#
[32m[20221213 23:13:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:43 @agent_ppo2.py:185][0m |          -0.0023 |          44.6129 |          15.2805 |
[32m[20221213 23:13:43 @agent_ppo2.py:185][0m |          -0.0110 |          42.5435 |          15.2867 |
[32m[20221213 23:13:43 @agent_ppo2.py:185][0m |          -0.0107 |          41.7495 |          15.2990 |
[32m[20221213 23:13:43 @agent_ppo2.py:185][0m |          -0.0115 |          41.3470 |          15.3060 |
[32m[20221213 23:13:43 @agent_ppo2.py:185][0m |          -0.0083 |          40.7114 |          15.2930 |
[32m[20221213 23:13:44 @agent_ppo2.py:185][0m |          -0.0047 |          44.5122 |          15.3038 |
[32m[20221213 23:13:44 @agent_ppo2.py:185][0m |          -0.0133 |          40.1899 |          15.3040 |
[32m[20221213 23:13:44 @agent_ppo2.py:185][0m |           0.0079 |          48.0090 |          15.3106 |
[32m[20221213 23:13:44 @agent_ppo2.py:185][0m |          -0.0096 |          41.1587 |          15.3230 |
[32m[20221213 23:13:44 @agent_ppo2.py:185][0m |          -0.0149 |          39.4375 |          15.3248 |
[32m[20221213 23:13:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.41
[32m[20221213 23:13:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.34
[32m[20221213 23:13:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.43
[32m[20221213 23:13:44 @agent_ppo2.py:143][0m Total time:       1.20 min
[32m[20221213 23:13:44 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 23:13:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2053 --------------------------#
[32m[20221213 23:13:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:44 @agent_ppo2.py:185][0m |           0.0046 |          42.7802 |          15.3258 |
[32m[20221213 23:13:44 @agent_ppo2.py:185][0m |          -0.0056 |          40.1608 |          15.3147 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |           0.0003 |          40.5200 |          15.3206 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |          -0.0057 |          38.9783 |          15.3110 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |          -0.0081 |          38.6417 |          15.3156 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |          -0.0077 |          38.4991 |          15.2961 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |          -0.0139 |          38.1219 |          15.2959 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |          -0.0128 |          37.9353 |          15.2900 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |          -0.0126 |          37.7657 |          15.2670 |
[32m[20221213 23:13:45 @agent_ppo2.py:185][0m |          -0.0137 |          37.6413 |          15.2651 |
[32m[20221213 23:13:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:13:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.42
[32m[20221213 23:13:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.18
[32m[20221213 23:13:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.65
[32m[20221213 23:13:45 @agent_ppo2.py:143][0m Total time:       1.22 min
[32m[20221213 23:13:45 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 23:13:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2054 --------------------------#
[32m[20221213 23:13:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |           0.0027 |          50.3778 |          15.1206 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0057 |          48.3020 |          15.1827 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0042 |          47.3123 |          15.1698 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0054 |          47.0821 |          15.1900 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0057 |          46.5365 |          15.2084 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0054 |          46.2290 |          15.1919 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0081 |          46.1827 |          15.2226 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0110 |          45.9169 |          15.2354 |
[32m[20221213 23:13:46 @agent_ppo2.py:185][0m |          -0.0053 |          46.6091 |          15.2313 |
[32m[20221213 23:13:47 @agent_ppo2.py:185][0m |          -0.0085 |          45.6767 |          15.2312 |
[32m[20221213 23:13:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.31
[32m[20221213 23:13:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.51
[32m[20221213 23:13:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.42
[32m[20221213 23:13:47 @agent_ppo2.py:143][0m Total time:       1.24 min
[32m[20221213 23:13:47 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 23:13:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2055 --------------------------#
[32m[20221213 23:13:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:47 @agent_ppo2.py:185][0m |           0.0144 |          51.4349 |          15.2551 |
[32m[20221213 23:13:47 @agent_ppo2.py:185][0m |          -0.0035 |          45.2434 |          15.2482 |
[32m[20221213 23:13:47 @agent_ppo2.py:185][0m |          -0.0041 |          44.7896 |          15.2540 |
[32m[20221213 23:13:47 @agent_ppo2.py:185][0m |          -0.0031 |          44.6756 |          15.2829 |
[32m[20221213 23:13:47 @agent_ppo2.py:185][0m |          -0.0067 |          44.5036 |          15.2760 |
[32m[20221213 23:13:47 @agent_ppo2.py:185][0m |          -0.0027 |          44.4037 |          15.2983 |
[32m[20221213 23:13:48 @agent_ppo2.py:185][0m |          -0.0049 |          44.5042 |          15.3098 |
[32m[20221213 23:13:48 @agent_ppo2.py:185][0m |          -0.0065 |          44.2892 |          15.3109 |
[32m[20221213 23:13:48 @agent_ppo2.py:185][0m |          -0.0045 |          44.4382 |          15.3271 |
[32m[20221213 23:13:48 @agent_ppo2.py:185][0m |          -0.0050 |          44.4052 |          15.3333 |
[32m[20221213 23:13:48 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:13:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.42
[32m[20221213 23:13:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.91
[32m[20221213 23:13:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.39
[32m[20221213 23:13:48 @agent_ppo2.py:143][0m Total time:       1.27 min
[32m[20221213 23:13:48 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 23:13:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2056 --------------------------#
[32m[20221213 23:13:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:48 @agent_ppo2.py:185][0m |           0.0007 |          49.7619 |          15.1304 |
[32m[20221213 23:13:48 @agent_ppo2.py:185][0m |          -0.0003 |          46.8765 |          15.1094 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0043 |          45.3025 |          15.1228 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0031 |          44.5997 |          15.1373 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0047 |          44.1506 |          15.1235 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0039 |          43.8282 |          15.1281 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0088 |          43.3520 |          15.1287 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0055 |          43.3695 |          15.1094 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0064 |          43.0887 |          15.1271 |
[32m[20221213 23:13:49 @agent_ppo2.py:185][0m |          -0.0078 |          42.6658 |          15.1468 |
[32m[20221213 23:13:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:13:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.52
[32m[20221213 23:13:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.50
[32m[20221213 23:13:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.23
[32m[20221213 23:13:49 @agent_ppo2.py:143][0m Total time:       1.29 min
[32m[20221213 23:13:49 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 23:13:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2057 --------------------------#
[32m[20221213 23:13:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |           0.0002 |          42.9167 |          15.2590 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0041 |          39.5717 |          15.2426 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0069 |          37.8749 |          15.2326 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0089 |          36.7804 |          15.2451 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0090 |          36.0877 |          15.2370 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0077 |          35.7531 |          15.2563 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0104 |          35.1340 |          15.2307 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0151 |          34.6163 |          15.2444 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0125 |          34.3276 |          15.2418 |
[32m[20221213 23:13:50 @agent_ppo2.py:185][0m |          -0.0153 |          33.7670 |          15.2580 |
[32m[20221213 23:13:50 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.45
[32m[20221213 23:13:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.78
[32m[20221213 23:13:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.25
[32m[20221213 23:13:51 @agent_ppo2.py:143][0m Total time:       1.31 min
[32m[20221213 23:13:51 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 23:13:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2058 --------------------------#
[32m[20221213 23:13:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:51 @agent_ppo2.py:185][0m |          -0.0047 |          49.9405 |          15.1972 |
[32m[20221213 23:13:51 @agent_ppo2.py:185][0m |          -0.0048 |          43.0597 |          15.1980 |
[32m[20221213 23:13:51 @agent_ppo2.py:185][0m |          -0.0048 |          41.3598 |          15.1821 |
[32m[20221213 23:13:51 @agent_ppo2.py:185][0m |          -0.0101 |          40.2515 |          15.1998 |
[32m[20221213 23:13:51 @agent_ppo2.py:185][0m |          -0.0087 |          39.5965 |          15.2120 |
[32m[20221213 23:13:51 @agent_ppo2.py:185][0m |          -0.0098 |          39.1114 |          15.2178 |
[32m[20221213 23:13:52 @agent_ppo2.py:185][0m |          -0.0064 |          38.6412 |          15.2077 |
[32m[20221213 23:13:52 @agent_ppo2.py:185][0m |          -0.0092 |          38.5223 |          15.2361 |
[32m[20221213 23:13:52 @agent_ppo2.py:185][0m |          -0.0124 |          38.1266 |          15.2317 |
[32m[20221213 23:13:52 @agent_ppo2.py:185][0m |          -0.0113 |          37.9019 |          15.2219 |
[32m[20221213 23:13:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.81
[32m[20221213 23:13:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.42
[32m[20221213 23:13:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:13:52 @agent_ppo2.py:143][0m Total time:       1.33 min
[32m[20221213 23:13:52 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 23:13:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2059 --------------------------#
[32m[20221213 23:13:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:52 @agent_ppo2.py:185][0m |           0.0005 |          52.0179 |          15.4157 |
[32m[20221213 23:13:52 @agent_ppo2.py:185][0m |          -0.0049 |          49.7292 |          15.3727 |
[32m[20221213 23:13:52 @agent_ppo2.py:185][0m |          -0.0073 |          48.7306 |          15.3780 |
[32m[20221213 23:13:53 @agent_ppo2.py:185][0m |          -0.0078 |          48.1345 |          15.3840 |
[32m[20221213 23:13:53 @agent_ppo2.py:185][0m |          -0.0085 |          47.1479 |          15.3806 |
[32m[20221213 23:13:53 @agent_ppo2.py:185][0m |          -0.0084 |          46.5680 |          15.3862 |
[32m[20221213 23:13:53 @agent_ppo2.py:185][0m |          -0.0112 |          46.1971 |          15.3803 |
[32m[20221213 23:13:53 @agent_ppo2.py:185][0m |          -0.0118 |          45.8734 |          15.3802 |
[32m[20221213 23:13:53 @agent_ppo2.py:185][0m |          -0.0097 |          45.7064 |          15.3767 |
[32m[20221213 23:13:53 @agent_ppo2.py:185][0m |          -0.0114 |          45.3670 |          15.3677 |
[32m[20221213 23:13:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.10
[32m[20221213 23:13:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.73
[32m[20221213 23:13:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.23
[32m[20221213 23:13:53 @agent_ppo2.py:143][0m Total time:       1.35 min
[32m[20221213 23:13:53 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 23:13:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2060 --------------------------#
[32m[20221213 23:13:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:13:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |           0.0010 |          45.3547 |          15.2182 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0065 |          38.5506 |          15.2321 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0041 |          37.1606 |          15.2380 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0084 |          36.4094 |          15.2600 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0101 |          35.8452 |          15.2658 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0086 |          35.5321 |          15.2690 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0172 |          35.1315 |          15.2747 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0103 |          34.8818 |          15.2869 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0172 |          34.6429 |          15.2977 |
[32m[20221213 23:13:54 @agent_ppo2.py:185][0m |          -0.0106 |          34.4640 |          15.2844 |
[32m[20221213 23:13:54 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:13:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.28
[32m[20221213 23:13:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.54
[32m[20221213 23:13:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.39
[32m[20221213 23:13:55 @agent_ppo2.py:143][0m Total time:       1.38 min
[32m[20221213 23:13:55 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 23:13:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2061 --------------------------#
[32m[20221213 23:13:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:55 @agent_ppo2.py:185][0m |           0.0004 |          42.1712 |          15.3157 |
[32m[20221213 23:13:55 @agent_ppo2.py:185][0m |          -0.0059 |          39.9587 |          15.3056 |
[32m[20221213 23:13:55 @agent_ppo2.py:185][0m |          -0.0111 |          39.4184 |          15.3035 |
[32m[20221213 23:13:55 @agent_ppo2.py:185][0m |          -0.0098 |          39.0876 |          15.3060 |
[32m[20221213 23:13:55 @agent_ppo2.py:185][0m |           0.0073 |          42.6135 |          15.2893 |
[32m[20221213 23:13:55 @agent_ppo2.py:185][0m |          -0.0033 |          40.7866 |          15.2966 |
[32m[20221213 23:13:55 @agent_ppo2.py:185][0m |          -0.0092 |          38.3643 |          15.3074 |
[32m[20221213 23:13:56 @agent_ppo2.py:185][0m |          -0.0182 |          38.1460 |          15.2981 |
[32m[20221213 23:13:56 @agent_ppo2.py:185][0m |          -0.0130 |          38.9273 |          15.3212 |
[32m[20221213 23:13:56 @agent_ppo2.py:185][0m |          -0.0178 |          37.8484 |          15.3411 |
[32m[20221213 23:13:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:13:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.33
[32m[20221213 23:13:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.40
[32m[20221213 23:13:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.36
[32m[20221213 23:13:56 @agent_ppo2.py:143][0m Total time:       1.40 min
[32m[20221213 23:13:56 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 23:13:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2062 --------------------------#
[32m[20221213 23:13:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:56 @agent_ppo2.py:185][0m |          -0.0021 |          38.0711 |          15.2111 |
[32m[20221213 23:13:56 @agent_ppo2.py:185][0m |           0.0030 |          33.7768 |          15.2019 |
[32m[20221213 23:13:56 @agent_ppo2.py:185][0m |          -0.0083 |          31.4410 |          15.2157 |
[32m[20221213 23:13:56 @agent_ppo2.py:185][0m |          -0.0104 |          30.2323 |          15.2106 |
[32m[20221213 23:13:57 @agent_ppo2.py:185][0m |          -0.0056 |          31.2149 |          15.2145 |
[32m[20221213 23:13:57 @agent_ppo2.py:185][0m |          -0.0169 |          29.7936 |          15.2366 |
[32m[20221213 23:13:57 @agent_ppo2.py:185][0m |          -0.0171 |          29.1300 |          15.2264 |
[32m[20221213 23:13:57 @agent_ppo2.py:185][0m |          -0.0100 |          28.9032 |          15.2207 |
[32m[20221213 23:13:57 @agent_ppo2.py:185][0m |          -0.0142 |          28.6234 |          15.2295 |
[32m[20221213 23:13:57 @agent_ppo2.py:185][0m |          -0.0177 |          28.4689 |          15.2372 |
[32m[20221213 23:13:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:13:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.20
[32m[20221213 23:13:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.68
[32m[20221213 23:13:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.11
[32m[20221213 23:13:57 @agent_ppo2.py:143][0m Total time:       1.42 min
[32m[20221213 23:13:57 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 23:13:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2063 --------------------------#
[32m[20221213 23:13:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:13:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:57 @agent_ppo2.py:185][0m |          -0.0020 |          42.8532 |          15.4657 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0047 |          40.8725 |          15.4747 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0081 |          39.7932 |          15.4710 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0073 |          39.3666 |          15.4819 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0123 |          38.9557 |          15.5003 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0072 |          38.6771 |          15.5199 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0109 |          38.3354 |          15.4935 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0099 |          38.2498 |          15.5004 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0109 |          38.0687 |          15.5123 |
[32m[20221213 23:13:58 @agent_ppo2.py:185][0m |          -0.0138 |          37.7877 |          15.5090 |
[32m[20221213 23:13:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:13:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.34
[32m[20221213 23:13:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.56
[32m[20221213 23:13:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.19
[32m[20221213 23:13:58 @agent_ppo2.py:143][0m Total time:       1.44 min
[32m[20221213 23:13:58 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 23:13:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2064 --------------------------#
[32m[20221213 23:13:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:13:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |          -0.0017 |          53.9951 |          15.5954 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |           0.0008 |          53.3175 |          15.5825 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |           0.0006 |          53.8016 |          15.5882 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |          -0.0077 |          52.6498 |          15.5911 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |          -0.0072 |          52.4369 |          15.6021 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |          -0.0089 |          52.2841 |          15.5805 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |          -0.0040 |          52.2788 |          15.5752 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |          -0.0095 |          52.0710 |          15.5965 |
[32m[20221213 23:13:59 @agent_ppo2.py:185][0m |          -0.0087 |          51.9057 |          15.5974 |
[32m[20221213 23:14:00 @agent_ppo2.py:185][0m |          -0.0131 |          51.8904 |          15.6118 |
[32m[20221213 23:14:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.63
[32m[20221213 23:14:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.72
[32m[20221213 23:14:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.11
[32m[20221213 23:14:00 @agent_ppo2.py:143][0m Total time:       1.46 min
[32m[20221213 23:14:00 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 23:14:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2065 --------------------------#
[32m[20221213 23:14:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:00 @agent_ppo2.py:185][0m |          -0.0019 |          31.9304 |          15.5716 |
[32m[20221213 23:14:00 @agent_ppo2.py:185][0m |          -0.0028 |          29.6179 |          15.5611 |
[32m[20221213 23:14:00 @agent_ppo2.py:185][0m |          -0.0107 |          27.6936 |          15.5501 |
[32m[20221213 23:14:00 @agent_ppo2.py:185][0m |          -0.0087 |          27.0141 |          15.5282 |
[32m[20221213 23:14:00 @agent_ppo2.py:185][0m |          -0.0081 |          26.4797 |          15.5188 |
[32m[20221213 23:14:00 @agent_ppo2.py:185][0m |          -0.0118 |          26.2837 |          15.5351 |
[32m[20221213 23:14:01 @agent_ppo2.py:185][0m |          -0.0119 |          25.9581 |          15.5174 |
[32m[20221213 23:14:01 @agent_ppo2.py:185][0m |          -0.0070 |          26.0864 |          15.5239 |
[32m[20221213 23:14:01 @agent_ppo2.py:185][0m |          -0.0151 |          25.6185 |          15.5270 |
[32m[20221213 23:14:01 @agent_ppo2.py:185][0m |          -0.0151 |          25.4271 |          15.5141 |
[32m[20221213 23:14:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.37
[32m[20221213 23:14:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.25
[32m[20221213 23:14:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.60
[32m[20221213 23:14:01 @agent_ppo2.py:143][0m Total time:       1.48 min
[32m[20221213 23:14:01 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 23:14:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2066 --------------------------#
[32m[20221213 23:14:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:01 @agent_ppo2.py:185][0m |           0.0027 |          51.9532 |          15.5252 |
[32m[20221213 23:14:01 @agent_ppo2.py:185][0m |          -0.0084 |          49.1908 |          15.5298 |
[32m[20221213 23:14:01 @agent_ppo2.py:185][0m |           0.0007 |          50.0260 |          15.5386 |
[32m[20221213 23:14:02 @agent_ppo2.py:185][0m |          -0.0090 |          48.1244 |          15.5418 |
[32m[20221213 23:14:02 @agent_ppo2.py:185][0m |          -0.0093 |          47.4333 |          15.5279 |
[32m[20221213 23:14:02 @agent_ppo2.py:185][0m |           0.0038 |          52.6607 |          15.5282 |
[32m[20221213 23:14:02 @agent_ppo2.py:185][0m |          -0.0103 |          47.1249 |          15.5359 |
[32m[20221213 23:14:02 @agent_ppo2.py:185][0m |          -0.0076 |          46.9202 |          15.5315 |
[32m[20221213 23:14:02 @agent_ppo2.py:185][0m |          -0.0105 |          46.6147 |          15.5307 |
[32m[20221213 23:14:02 @agent_ppo2.py:185][0m |          -0.0074 |          46.6045 |          15.5455 |
[32m[20221213 23:14:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.19
[32m[20221213 23:14:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 354.07
[32m[20221213 23:14:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.39
[32m[20221213 23:14:02 @agent_ppo2.py:143][0m Total time:       1.50 min
[32m[20221213 23:14:02 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 23:14:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2067 --------------------------#
[32m[20221213 23:14:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |           0.0000 |          44.0942 |          15.3780 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0054 |          41.7879 |          15.3600 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0061 |          40.9001 |          15.3481 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0079 |          40.6651 |          15.3316 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0061 |          40.1253 |          15.3484 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0043 |          41.3014 |          15.3299 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0028 |          41.4720 |          15.3180 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0129 |          39.1280 |          15.3286 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0064 |          39.4078 |          15.3452 |
[32m[20221213 23:14:03 @agent_ppo2.py:185][0m |          -0.0149 |          38.7728 |          15.3357 |
[32m[20221213 23:14:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:14:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.40
[32m[20221213 23:14:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.95
[32m[20221213 23:14:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.59
[32m[20221213 23:14:03 @agent_ppo2.py:143][0m Total time:       1.52 min
[32m[20221213 23:14:03 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 23:14:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2068 --------------------------#
[32m[20221213 23:14:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0023 |          44.1305 |          15.6153 |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0071 |          41.0111 |          15.6194 |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0061 |          39.5921 |          15.6100 |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0098 |          38.3004 |          15.5983 |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0033 |          37.7208 |          15.5609 |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0099 |          36.6536 |          15.5575 |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0131 |          36.2155 |          15.5640 |
[32m[20221213 23:14:04 @agent_ppo2.py:185][0m |          -0.0148 |          35.8188 |          15.5441 |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |          -0.0059 |          37.6662 |          15.5479 |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |          -0.0113 |          35.3744 |          15.5293 |
[32m[20221213 23:14:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.01
[32m[20221213 23:14:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.25
[32m[20221213 23:14:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.02
[32m[20221213 23:14:05 @agent_ppo2.py:143][0m Total time:       1.55 min
[32m[20221213 23:14:05 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 23:14:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2069 --------------------------#
[32m[20221213 23:14:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |           0.0080 |          52.9028 |          15.4133 |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |          -0.0030 |          47.2130 |          15.4320 |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |          -0.0035 |          48.6558 |          15.4154 |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |          -0.0086 |          46.7924 |          15.4313 |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |           0.0028 |          50.3589 |          15.4467 |
[32m[20221213 23:14:05 @agent_ppo2.py:185][0m |          -0.0007 |          49.3277 |          15.4078 |
[32m[20221213 23:14:06 @agent_ppo2.py:185][0m |          -0.0086 |          45.9315 |          15.4218 |
[32m[20221213 23:14:06 @agent_ppo2.py:185][0m |          -0.0090 |          46.0706 |          15.4370 |
[32m[20221213 23:14:06 @agent_ppo2.py:185][0m |          -0.0097 |          45.7834 |          15.4317 |
[32m[20221213 23:14:06 @agent_ppo2.py:185][0m |          -0.0112 |          45.5901 |          15.4543 |
[32m[20221213 23:14:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.19
[32m[20221213 23:14:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.26
[32m[20221213 23:14:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.56
[32m[20221213 23:14:06 @agent_ppo2.py:143][0m Total time:       1.57 min
[32m[20221213 23:14:06 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 23:14:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2070 --------------------------#
[32m[20221213 23:14:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:14:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:06 @agent_ppo2.py:185][0m |           0.0003 |          48.1588 |          15.6151 |
[32m[20221213 23:14:06 @agent_ppo2.py:185][0m |          -0.0048 |          43.9642 |          15.5983 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0050 |          42.6610 |          15.5939 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0105 |          42.0973 |          15.5775 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0176 |          41.5491 |          15.5899 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0109 |          41.4178 |          15.5904 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0127 |          40.7383 |          15.5934 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0134 |          40.3813 |          15.6198 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0091 |          40.5342 |          15.6267 |
[32m[20221213 23:14:07 @agent_ppo2.py:185][0m |          -0.0127 |          39.8875 |          15.6071 |
[32m[20221213 23:14:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.09
[32m[20221213 23:14:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.94
[32m[20221213 23:14:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.73
[32m[20221213 23:14:07 @agent_ppo2.py:143][0m Total time:       1.59 min
[32m[20221213 23:14:07 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 23:14:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2071 --------------------------#
[32m[20221213 23:14:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |           0.0018 |          46.1324 |          15.3815 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0055 |          44.4409 |          15.3617 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |           0.0007 |          43.9799 |          15.3543 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0100 |          43.0732 |          15.3765 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0110 |          42.6501 |          15.3651 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0055 |          42.4402 |          15.3512 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0065 |          42.5348 |          15.3345 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0120 |          41.6785 |          15.3438 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0109 |          41.3485 |          15.3369 |
[32m[20221213 23:14:08 @agent_ppo2.py:185][0m |          -0.0069 |          40.8206 |          15.3394 |
[32m[20221213 23:14:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.90
[32m[20221213 23:14:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.92
[32m[20221213 23:14:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.41
[32m[20221213 23:14:09 @agent_ppo2.py:143][0m Total time:       1.61 min
[32m[20221213 23:14:09 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 23:14:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2072 --------------------------#
[32m[20221213 23:14:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |           0.0052 |          47.8820 |          15.5425 |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |          -0.0063 |          44.3786 |          15.5202 |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |           0.0042 |          48.1422 |          15.5297 |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |          -0.0069 |          42.4998 |          15.5341 |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |          -0.0086 |          42.0395 |          15.5376 |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |          -0.0089 |          41.6500 |          15.5337 |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |          -0.0123 |          41.2269 |          15.5313 |
[32m[20221213 23:14:09 @agent_ppo2.py:185][0m |          -0.0104 |          41.0103 |          15.5335 |
[32m[20221213 23:14:10 @agent_ppo2.py:185][0m |          -0.0163 |          40.6941 |          15.5577 |
[32m[20221213 23:14:10 @agent_ppo2.py:185][0m |          -0.0085 |          40.5191 |          15.5444 |
[32m[20221213 23:14:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:14:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.87
[32m[20221213 23:14:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.41
[32m[20221213 23:14:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.76
[32m[20221213 23:14:10 @agent_ppo2.py:143][0m Total time:       1.63 min
[32m[20221213 23:14:10 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 23:14:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2073 --------------------------#
[32m[20221213 23:14:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:10 @agent_ppo2.py:185][0m |          -0.0055 |          51.4454 |          15.3808 |
[32m[20221213 23:14:10 @agent_ppo2.py:185][0m |          -0.0054 |          46.7563 |          15.3813 |
[32m[20221213 23:14:10 @agent_ppo2.py:185][0m |           0.0009 |          49.3032 |          15.3828 |
[32m[20221213 23:14:10 @agent_ppo2.py:185][0m |          -0.0063 |          44.8285 |          15.3980 |
[32m[20221213 23:14:10 @agent_ppo2.py:185][0m |          -0.0105 |          44.5447 |          15.3778 |
[32m[20221213 23:14:11 @agent_ppo2.py:185][0m |          -0.0136 |          44.3839 |          15.3924 |
[32m[20221213 23:14:11 @agent_ppo2.py:185][0m |          -0.0128 |          43.9710 |          15.3999 |
[32m[20221213 23:14:11 @agent_ppo2.py:185][0m |          -0.0056 |          44.4346 |          15.3949 |
[32m[20221213 23:14:11 @agent_ppo2.py:185][0m |          -0.0152 |          43.6664 |          15.3986 |
[32m[20221213 23:14:11 @agent_ppo2.py:185][0m |          -0.0155 |          43.6049 |          15.3958 |
[32m[20221213 23:14:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.68
[32m[20221213 23:14:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.54
[32m[20221213 23:14:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.01
[32m[20221213 23:14:11 @agent_ppo2.py:143][0m Total time:       1.65 min
[32m[20221213 23:14:11 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 23:14:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2074 --------------------------#
[32m[20221213 23:14:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:11 @agent_ppo2.py:185][0m |           0.0016 |          42.1410 |          15.3684 |
[32m[20221213 23:14:11 @agent_ppo2.py:185][0m |          -0.0004 |          39.3211 |          15.3848 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |          -0.0057 |          38.1197 |          15.3774 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |           0.0087 |          42.5639 |          15.3682 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |          -0.0084 |          37.3515 |          15.3778 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |          -0.0097 |          36.7857 |          15.3646 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |          -0.0114 |          36.5139 |          15.3809 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |          -0.0104 |          36.2343 |          15.3782 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |          -0.0088 |          36.1694 |          15.3741 |
[32m[20221213 23:14:12 @agent_ppo2.py:185][0m |          -0.0109 |          36.0652 |          15.3530 |
[32m[20221213 23:14:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.71
[32m[20221213 23:14:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.57
[32m[20221213 23:14:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.66
[32m[20221213 23:14:12 @agent_ppo2.py:143][0m Total time:       1.67 min
[32m[20221213 23:14:12 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 23:14:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2075 --------------------------#
[32m[20221213 23:14:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |           0.0043 |          37.8886 |          15.3619 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0037 |          33.1852 |          15.3296 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0003 |          33.2012 |          15.3314 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0063 |          31.4004 |          15.3180 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0011 |          31.4953 |          15.3230 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0063 |          30.6687 |          15.3156 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0113 |          30.6151 |          15.3089 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0146 |          30.2978 |          15.3060 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0097 |          30.0576 |          15.2814 |
[32m[20221213 23:14:13 @agent_ppo2.py:185][0m |          -0.0135 |          30.0136 |          15.2822 |
[32m[20221213 23:14:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.20
[32m[20221213 23:14:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.74
[32m[20221213 23:14:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.15
[32m[20221213 23:14:14 @agent_ppo2.py:143][0m Total time:       1.69 min
[32m[20221213 23:14:14 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 23:14:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2076 --------------------------#
[32m[20221213 23:14:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |           0.0020 |          52.8564 |          15.5403 |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |          -0.0044 |          51.5257 |          15.5534 |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |          -0.0093 |          51.1931 |          15.5367 |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |          -0.0054 |          51.0410 |          15.5417 |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |          -0.0014 |          51.7094 |          15.5309 |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |           0.0057 |          54.7805 |          15.5277 |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |          -0.0055 |          50.8925 |          15.5337 |
[32m[20221213 23:14:14 @agent_ppo2.py:185][0m |          -0.0044 |          52.4569 |          15.5174 |
[32m[20221213 23:14:15 @agent_ppo2.py:185][0m |          -0.0084 |          50.6249 |          15.5231 |
[32m[20221213 23:14:15 @agent_ppo2.py:185][0m |          -0.0108 |          50.4368 |          15.5160 |
[32m[20221213 23:14:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.86
[32m[20221213 23:14:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.88
[32m[20221213 23:14:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.82
[32m[20221213 23:14:15 @agent_ppo2.py:143][0m Total time:       1.71 min
[32m[20221213 23:14:15 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 23:14:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2077 --------------------------#
[32m[20221213 23:14:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:15 @agent_ppo2.py:185][0m |          -0.0022 |          52.3577 |          15.4830 |
[32m[20221213 23:14:15 @agent_ppo2.py:185][0m |           0.0014 |          47.0234 |          15.4476 |
[32m[20221213 23:14:15 @agent_ppo2.py:185][0m |          -0.0040 |          45.9495 |          15.4316 |
[32m[20221213 23:14:15 @agent_ppo2.py:185][0m |           0.0022 |          45.8815 |          15.4114 |
[32m[20221213 23:14:16 @agent_ppo2.py:185][0m |          -0.0086 |          45.0098 |          15.4012 |
[32m[20221213 23:14:16 @agent_ppo2.py:185][0m |          -0.0107 |          44.0991 |          15.3735 |
[32m[20221213 23:14:16 @agent_ppo2.py:185][0m |          -0.0065 |          44.0450 |          15.3755 |
[32m[20221213 23:14:16 @agent_ppo2.py:185][0m |          -0.0099 |          43.9710 |          15.3647 |
[32m[20221213 23:14:16 @agent_ppo2.py:185][0m |          -0.0058 |          43.4551 |          15.3565 |
[32m[20221213 23:14:16 @agent_ppo2.py:185][0m |          -0.0106 |          43.2387 |          15.3403 |
[32m[20221213 23:14:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:14:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.17
[32m[20221213 23:14:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.75
[32m[20221213 23:14:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.47
[32m[20221213 23:14:16 @agent_ppo2.py:143][0m Total time:       1.74 min
[32m[20221213 23:14:16 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 23:14:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2078 --------------------------#
[32m[20221213 23:14:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:16 @agent_ppo2.py:185][0m |          -0.0029 |          41.2811 |          15.5554 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0061 |          36.7578 |          15.5240 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0090 |          35.3997 |          15.5479 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0117 |          34.6112 |          15.5503 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0125 |          34.1235 |          15.5272 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0138 |          33.6380 |          15.5372 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0116 |          33.1016 |          15.5435 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0106 |          33.4567 |          15.5406 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0169 |          32.4504 |          15.5277 |
[32m[20221213 23:14:17 @agent_ppo2.py:185][0m |          -0.0142 |          32.3100 |          15.5493 |
[32m[20221213 23:14:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.56
[32m[20221213 23:14:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.43
[32m[20221213 23:14:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.74
[32m[20221213 23:14:17 @agent_ppo2.py:143][0m Total time:       1.76 min
[32m[20221213 23:14:17 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 23:14:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2079 --------------------------#
[32m[20221213 23:14:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |           0.0011 |          34.1252 |          15.3887 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0046 |          31.6620 |          15.3553 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0072 |          30.7343 |          15.3637 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0073 |          29.9300 |          15.3529 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0069 |          29.6246 |          15.3637 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0100 |          29.1306 |          15.3621 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0113 |          28.8472 |          15.3654 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0107 |          28.7011 |          15.3515 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0146 |          28.4628 |          15.3728 |
[32m[20221213 23:14:18 @agent_ppo2.py:185][0m |          -0.0108 |          28.2341 |          15.3686 |
[32m[20221213 23:14:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.57
[32m[20221213 23:14:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.86
[32m[20221213 23:14:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.68
[32m[20221213 23:14:19 @agent_ppo2.py:143][0m Total time:       1.78 min
[32m[20221213 23:14:19 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 23:14:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2080 --------------------------#
[32m[20221213 23:14:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:14:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:19 @agent_ppo2.py:185][0m |          -0.0010 |          36.8050 |          15.2266 |
[32m[20221213 23:14:19 @agent_ppo2.py:185][0m |           0.0018 |          32.4893 |          15.2337 |
[32m[20221213 23:14:19 @agent_ppo2.py:185][0m |          -0.0026 |          31.3235 |          15.2565 |
[32m[20221213 23:14:19 @agent_ppo2.py:185][0m |          -0.0021 |          30.6572 |          15.2665 |
[32m[20221213 23:14:19 @agent_ppo2.py:185][0m |          -0.0042 |          30.0914 |          15.2578 |
[32m[20221213 23:14:19 @agent_ppo2.py:185][0m |          -0.0059 |          29.7321 |          15.2721 |
[32m[20221213 23:14:19 @agent_ppo2.py:185][0m |          -0.0079 |          29.3660 |          15.2658 |
[32m[20221213 23:14:20 @agent_ppo2.py:185][0m |          -0.0087 |          29.2323 |          15.2635 |
[32m[20221213 23:14:20 @agent_ppo2.py:185][0m |          -0.0049 |          29.0275 |          15.2684 |
[32m[20221213 23:14:20 @agent_ppo2.py:185][0m |          -0.0071 |          28.7449 |          15.2757 |
[32m[20221213 23:14:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:14:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.73
[32m[20221213 23:14:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.53
[32m[20221213 23:14:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.70
[32m[20221213 23:14:20 @agent_ppo2.py:143][0m Total time:       1.80 min
[32m[20221213 23:14:20 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 23:14:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2081 --------------------------#
[32m[20221213 23:14:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:20 @agent_ppo2.py:185][0m |          -0.0003 |          33.7194 |          15.1862 |
[32m[20221213 23:14:20 @agent_ppo2.py:185][0m |          -0.0046 |          28.5864 |          15.1956 |
[32m[20221213 23:14:20 @agent_ppo2.py:185][0m |          -0.0062 |          27.6513 |          15.1918 |
[32m[20221213 23:14:20 @agent_ppo2.py:185][0m |          -0.0118 |          27.1116 |          15.1690 |
[32m[20221213 23:14:21 @agent_ppo2.py:185][0m |          -0.0116 |          26.7803 |          15.1779 |
[32m[20221213 23:14:21 @agent_ppo2.py:185][0m |          -0.0071 |          26.6625 |          15.1588 |
[32m[20221213 23:14:21 @agent_ppo2.py:185][0m |          -0.0039 |          26.2443 |          15.1351 |
[32m[20221213 23:14:21 @agent_ppo2.py:185][0m |          -0.0058 |          25.8859 |          15.1273 |
[32m[20221213 23:14:21 @agent_ppo2.py:185][0m |          -0.0156 |          25.6948 |          15.1482 |
[32m[20221213 23:14:21 @agent_ppo2.py:185][0m |          -0.0108 |          25.5413 |          15.1328 |
[32m[20221213 23:14:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.97
[32m[20221213 23:14:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.41
[32m[20221213 23:14:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.33
[32m[20221213 23:14:21 @agent_ppo2.py:143][0m Total time:       1.82 min
[32m[20221213 23:14:21 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 23:14:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2082 --------------------------#
[32m[20221213 23:14:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:21 @agent_ppo2.py:185][0m |          -0.0003 |          33.0770 |          15.4053 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0036 |          31.1178 |          15.3923 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0030 |          30.3289 |          15.3978 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0065 |          29.8296 |          15.3937 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0073 |          29.5154 |          15.3944 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0098 |          29.2096 |          15.3940 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0114 |          28.9609 |          15.3750 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0134 |          28.8919 |          15.3922 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0098 |          28.7469 |          15.4057 |
[32m[20221213 23:14:22 @agent_ppo2.py:185][0m |          -0.0141 |          28.4873 |          15.3938 |
[32m[20221213 23:14:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:14:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.17
[32m[20221213 23:14:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.51
[32m[20221213 23:14:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.93
[32m[20221213 23:14:22 @agent_ppo2.py:143][0m Total time:       1.84 min
[32m[20221213 23:14:22 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 23:14:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2083 --------------------------#
[32m[20221213 23:14:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |           0.0076 |          50.3787 |          15.2240 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0031 |          44.0339 |          15.2193 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0052 |          43.1905 |          15.2245 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0071 |          42.6285 |          15.2100 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0074 |          42.4326 |          15.2228 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0094 |          42.2412 |          15.2237 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0065 |          41.9257 |          15.2191 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0091 |          41.7783 |          15.2093 |
[32m[20221213 23:14:23 @agent_ppo2.py:185][0m |          -0.0081 |          41.6168 |          15.2153 |
[32m[20221213 23:14:24 @agent_ppo2.py:185][0m |          -0.0088 |          42.1631 |          15.2107 |
[32m[20221213 23:14:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:14:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.11
[32m[20221213 23:14:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.60
[32m[20221213 23:14:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.88
[32m[20221213 23:14:24 @agent_ppo2.py:143][0m Total time:       1.86 min
[32m[20221213 23:14:24 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 23:14:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2084 --------------------------#
[32m[20221213 23:14:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:24 @agent_ppo2.py:185][0m |           0.0013 |          37.6485 |          15.1831 |
[32m[20221213 23:14:24 @agent_ppo2.py:185][0m |          -0.0062 |          34.3894 |          15.1851 |
[32m[20221213 23:14:24 @agent_ppo2.py:185][0m |          -0.0078 |          33.3628 |          15.1780 |
[32m[20221213 23:14:24 @agent_ppo2.py:185][0m |          -0.0117 |          32.8979 |          15.1903 |
[32m[20221213 23:14:24 @agent_ppo2.py:185][0m |          -0.0105 |          32.3783 |          15.1931 |
[32m[20221213 23:14:25 @agent_ppo2.py:185][0m |          -0.0129 |          31.8796 |          15.1917 |
[32m[20221213 23:14:25 @agent_ppo2.py:185][0m |          -0.0139 |          31.7115 |          15.2171 |
[32m[20221213 23:14:25 @agent_ppo2.py:185][0m |          -0.0170 |          31.5070 |          15.1974 |
[32m[20221213 23:14:25 @agent_ppo2.py:185][0m |          -0.0163 |          31.1252 |          15.2324 |
[32m[20221213 23:14:25 @agent_ppo2.py:185][0m |          -0.0160 |          31.0304 |          15.1956 |
[32m[20221213 23:14:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:14:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.90
[32m[20221213 23:14:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.09
[32m[20221213 23:14:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.91
[32m[20221213 23:14:25 @agent_ppo2.py:143][0m Total time:       1.88 min
[32m[20221213 23:14:25 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 23:14:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2085 --------------------------#
[32m[20221213 23:14:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:25 @agent_ppo2.py:185][0m |           0.0111 |          53.6760 |          15.3793 |
[32m[20221213 23:14:25 @agent_ppo2.py:185][0m |           0.0031 |          49.7510 |          15.4091 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |          -0.0042 |          48.3696 |          15.3800 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |          -0.0064 |          47.9888 |          15.3957 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |          -0.0048 |          47.6862 |          15.4122 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |          -0.0009 |          48.0100 |          15.3839 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |          -0.0084 |          47.4839 |          15.4060 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |           0.0073 |          52.4999 |          15.4121 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |          -0.0066 |          47.6984 |          15.4153 |
[32m[20221213 23:14:26 @agent_ppo2.py:185][0m |          -0.0032 |          48.4510 |          15.4005 |
[32m[20221213 23:14:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:14:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.70
[32m[20221213 23:14:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.29
[32m[20221213 23:14:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.11
[32m[20221213 23:14:26 @agent_ppo2.py:143][0m Total time:       1.90 min
[32m[20221213 23:14:26 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 23:14:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2086 --------------------------#
[32m[20221213 23:14:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |           0.0008 |          51.0155 |          15.3149 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0036 |          49.9594 |          15.3338 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0047 |          49.5551 |          15.3316 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |           0.0082 |          54.8503 |          15.3267 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0066 |          49.3949 |          15.3515 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0069 |          49.1566 |          15.3445 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0083 |          49.0589 |          15.3489 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0074 |          49.0216 |          15.3465 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0091 |          48.9427 |          15.3476 |
[32m[20221213 23:14:27 @agent_ppo2.py:185][0m |          -0.0081 |          48.9763 |          15.3693 |
[32m[20221213 23:14:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:14:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.67
[32m[20221213 23:14:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.49
[32m[20221213 23:14:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.81
[32m[20221213 23:14:28 @agent_ppo2.py:143][0m Total time:       1.93 min
[32m[20221213 23:14:28 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 23:14:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2087 --------------------------#
[32m[20221213 23:14:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:28 @agent_ppo2.py:185][0m |           0.0036 |          51.1373 |          15.5400 |
[32m[20221213 23:14:28 @agent_ppo2.py:185][0m |          -0.0080 |          48.1724 |          15.5275 |
[32m[20221213 23:14:28 @agent_ppo2.py:185][0m |          -0.0071 |          47.7906 |          15.5240 |
[32m[20221213 23:14:28 @agent_ppo2.py:185][0m |          -0.0065 |          47.4452 |          15.5264 |
[32m[20221213 23:14:28 @agent_ppo2.py:185][0m |          -0.0101 |          47.1335 |          15.5292 |
[32m[20221213 23:14:28 @agent_ppo2.py:185][0m |          -0.0013 |          51.9860 |          15.5356 |
[32m[20221213 23:14:28 @agent_ppo2.py:185][0m |          -0.0131 |          46.2955 |          15.5259 |
[32m[20221213 23:14:29 @agent_ppo2.py:185][0m |          -0.0107 |          46.1414 |          15.5222 |
[32m[20221213 23:14:29 @agent_ppo2.py:185][0m |          -0.0146 |          45.9682 |          15.5137 |
[32m[20221213 23:14:29 @agent_ppo2.py:185][0m |          -0.0141 |          45.8047 |          15.5312 |
[32m[20221213 23:14:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.40
[32m[20221213 23:14:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.09
[32m[20221213 23:14:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.73
[32m[20221213 23:14:29 @agent_ppo2.py:143][0m Total time:       1.95 min
[32m[20221213 23:14:29 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 23:14:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2088 --------------------------#
[32m[20221213 23:14:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:29 @agent_ppo2.py:185][0m |          -0.0027 |          56.0164 |          15.5412 |
[32m[20221213 23:14:29 @agent_ppo2.py:185][0m |          -0.0060 |          53.2266 |          15.5281 |
[32m[20221213 23:14:29 @agent_ppo2.py:185][0m |          -0.0106 |          52.6083 |          15.5448 |
[32m[20221213 23:14:29 @agent_ppo2.py:185][0m |          -0.0096 |          52.2123 |          15.5236 |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |          -0.0059 |          54.1925 |          15.5226 |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |          -0.0126 |          51.6001 |          15.5371 |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |          -0.0033 |          53.7081 |          15.5375 |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |          -0.0126 |          51.1786 |          15.5558 |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |           0.0023 |          58.4940 |          15.5468 |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |          -0.0142 |          51.0168 |          15.5437 |
[32m[20221213 23:14:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.50
[32m[20221213 23:14:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.75
[32m[20221213 23:14:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.37
[32m[20221213 23:14:30 @agent_ppo2.py:143][0m Total time:       1.97 min
[32m[20221213 23:14:30 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 23:14:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2089 --------------------------#
[32m[20221213 23:14:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |           0.0039 |          50.0017 |          15.4352 |
[32m[20221213 23:14:30 @agent_ppo2.py:185][0m |          -0.0014 |          46.7127 |          15.4253 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0063 |          45.0964 |          15.4293 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0059 |          44.4904 |          15.4237 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0064 |          43.4072 |          15.4307 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0106 |          42.9704 |          15.4059 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0026 |          43.8385 |          15.4092 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0133 |          42.1768 |          15.4079 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0030 |          43.1202 |          15.4027 |
[32m[20221213 23:14:31 @agent_ppo2.py:185][0m |          -0.0125 |          41.5323 |          15.3918 |
[32m[20221213 23:14:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.56
[32m[20221213 23:14:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.51
[32m[20221213 23:14:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.83
[32m[20221213 23:14:31 @agent_ppo2.py:143][0m Total time:       1.99 min
[32m[20221213 23:14:31 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 23:14:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2090 --------------------------#
[32m[20221213 23:14:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:14:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0012 |          39.6329 |          15.4798 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0077 |          36.1621 |          15.4354 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0082 |          35.2156 |          15.4149 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0041 |          34.5070 |          15.4101 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0091 |          34.1104 |          15.3981 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0105 |          33.8849 |          15.4072 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |           0.0028 |          37.7743 |          15.4348 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0097 |          33.6326 |          15.4065 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0099 |          33.2888 |          15.4180 |
[32m[20221213 23:14:32 @agent_ppo2.py:185][0m |          -0.0106 |          33.0392 |          15.3973 |
[32m[20221213 23:14:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.87
[32m[20221213 23:14:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.19
[32m[20221213 23:14:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.25
[32m[20221213 23:14:33 @agent_ppo2.py:143][0m Total time:       2.01 min
[32m[20221213 23:14:33 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 23:14:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2091 --------------------------#
[32m[20221213 23:14:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:33 @agent_ppo2.py:185][0m |           0.0063 |          52.3757 |          15.2930 |
[32m[20221213 23:14:33 @agent_ppo2.py:185][0m |           0.0147 |          53.5158 |          15.2867 |
[32m[20221213 23:14:33 @agent_ppo2.py:185][0m |          -0.0057 |          49.8310 |          15.3188 |
[32m[20221213 23:14:33 @agent_ppo2.py:185][0m |          -0.0070 |          49.5759 |          15.3366 |
[32m[20221213 23:14:33 @agent_ppo2.py:185][0m |          -0.0082 |          49.5125 |          15.3485 |
[32m[20221213 23:14:33 @agent_ppo2.py:185][0m |          -0.0039 |          49.8244 |          15.3397 |
[32m[20221213 23:14:33 @agent_ppo2.py:185][0m |          -0.0104 |          49.3891 |          15.3665 |
[32m[20221213 23:14:34 @agent_ppo2.py:185][0m |          -0.0062 |          50.0077 |          15.3648 |
[32m[20221213 23:14:34 @agent_ppo2.py:185][0m |          -0.0049 |          51.4746 |          15.3560 |
[32m[20221213 23:14:34 @agent_ppo2.py:185][0m |          -0.0083 |          49.3092 |          15.3591 |
[32m[20221213 23:14:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:14:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.76
[32m[20221213 23:14:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.22
[32m[20221213 23:14:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.80
[32m[20221213 23:14:34 @agent_ppo2.py:143][0m Total time:       2.03 min
[32m[20221213 23:14:34 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 23:14:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2092 --------------------------#
[32m[20221213 23:14:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:34 @agent_ppo2.py:185][0m |           0.0011 |          50.1041 |          15.4445 |
[32m[20221213 23:14:34 @agent_ppo2.py:185][0m |          -0.0070 |          48.2285 |          15.4297 |
[32m[20221213 23:14:34 @agent_ppo2.py:185][0m |          -0.0052 |          47.4770 |          15.4439 |
[32m[20221213 23:14:34 @agent_ppo2.py:185][0m |          -0.0076 |          47.0641 |          15.4506 |
[32m[20221213 23:14:35 @agent_ppo2.py:185][0m |          -0.0067 |          46.6684 |          15.4484 |
[32m[20221213 23:14:35 @agent_ppo2.py:185][0m |          -0.0045 |          46.4767 |          15.4583 |
[32m[20221213 23:14:35 @agent_ppo2.py:185][0m |           0.0028 |          52.6088 |          15.4450 |
[32m[20221213 23:14:35 @agent_ppo2.py:185][0m |          -0.0086 |          46.5221 |          15.4429 |
[32m[20221213 23:14:35 @agent_ppo2.py:185][0m |          -0.0110 |          46.2047 |          15.4424 |
[32m[20221213 23:14:35 @agent_ppo2.py:185][0m |          -0.0105 |          46.1197 |          15.4540 |
[32m[20221213 23:14:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.29
[32m[20221213 23:14:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.53
[32m[20221213 23:14:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.77
[32m[20221213 23:14:35 @agent_ppo2.py:143][0m Total time:       2.05 min
[32m[20221213 23:14:35 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 23:14:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2093 --------------------------#
[32m[20221213 23:14:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:35 @agent_ppo2.py:185][0m |          -0.0021 |          43.4366 |          15.6320 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |          -0.0029 |          41.2624 |          15.6329 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |          -0.0064 |          40.6370 |          15.6413 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |          -0.0021 |          40.0327 |          15.6447 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |          -0.0046 |          39.8668 |          15.6739 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |           0.0004 |          40.3021 |          15.6763 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |          -0.0026 |          39.6058 |          15.7007 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |           0.0054 |          40.9425 |          15.7158 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |          -0.0066 |          39.3215 |          15.7192 |
[32m[20221213 23:14:36 @agent_ppo2.py:185][0m |          -0.0065 |          39.3145 |          15.7313 |
[32m[20221213 23:14:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.66
[32m[20221213 23:14:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.08
[32m[20221213 23:14:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.76
[32m[20221213 23:14:36 @agent_ppo2.py:143][0m Total time:       2.07 min
[32m[20221213 23:14:36 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 23:14:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2094 --------------------------#
[32m[20221213 23:14:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |           0.0043 |          33.5742 |          15.5901 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0043 |          28.7192 |          15.5693 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0060 |          27.8505 |          15.5610 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0064 |          27.4300 |          15.5609 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0115 |          27.0553 |          15.5622 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0074 |          26.7132 |          15.5579 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0098 |          26.5712 |          15.5658 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0083 |          26.3567 |          15.5568 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0126 |          26.2686 |          15.5657 |
[32m[20221213 23:14:37 @agent_ppo2.py:185][0m |          -0.0093 |          26.0509 |          15.5599 |
[32m[20221213 23:14:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.68
[32m[20221213 23:14:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.18
[32m[20221213 23:14:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.54
[32m[20221213 23:14:38 @agent_ppo2.py:143][0m Total time:       2.09 min
[32m[20221213 23:14:38 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 23:14:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2095 --------------------------#
[32m[20221213 23:14:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:38 @agent_ppo2.py:185][0m |           0.0020 |          37.0056 |          15.5298 |
[32m[20221213 23:14:38 @agent_ppo2.py:185][0m |           0.0029 |          34.5605 |          15.4958 |
[32m[20221213 23:14:38 @agent_ppo2.py:185][0m |          -0.0041 |          32.9398 |          15.4879 |
[32m[20221213 23:14:38 @agent_ppo2.py:185][0m |          -0.0092 |          32.5262 |          15.4808 |
[32m[20221213 23:14:38 @agent_ppo2.py:185][0m |          -0.0098 |          32.0072 |          15.4914 |
[32m[20221213 23:14:38 @agent_ppo2.py:185][0m |          -0.0110 |          31.8924 |          15.4790 |
[32m[20221213 23:14:38 @agent_ppo2.py:185][0m |          -0.0155 |          31.9370 |          15.4920 |
[32m[20221213 23:14:39 @agent_ppo2.py:185][0m |          -0.0080 |          31.3874 |          15.4912 |
[32m[20221213 23:14:39 @agent_ppo2.py:185][0m |          -0.0098 |          31.3588 |          15.5077 |
[32m[20221213 23:14:39 @agent_ppo2.py:185][0m |          -0.0118 |          31.2119 |          15.4921 |
[32m[20221213 23:14:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.58
[32m[20221213 23:14:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.03
[32m[20221213 23:14:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.71
[32m[20221213 23:14:39 @agent_ppo2.py:143][0m Total time:       2.11 min
[32m[20221213 23:14:39 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 23:14:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2096 --------------------------#
[32m[20221213 23:14:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:39 @agent_ppo2.py:185][0m |          -0.0003 |          35.9380 |          15.5802 |
[32m[20221213 23:14:39 @agent_ppo2.py:185][0m |          -0.0000 |          36.2441 |          15.5972 |
[32m[20221213 23:14:39 @agent_ppo2.py:185][0m |           0.0239 |          39.5924 |          15.5713 |
[32m[20221213 23:14:39 @agent_ppo2.py:185][0m |          -0.0079 |          32.9618 |          15.6161 |
[32m[20221213 23:14:40 @agent_ppo2.py:185][0m |          -0.0082 |          32.2081 |          15.5996 |
[32m[20221213 23:14:40 @agent_ppo2.py:185][0m |          -0.0075 |          32.0205 |          15.6113 |
[32m[20221213 23:14:40 @agent_ppo2.py:185][0m |          -0.0046 |          32.1014 |          15.6174 |
[32m[20221213 23:14:40 @agent_ppo2.py:185][0m |          -0.0076 |          31.6780 |          15.6156 |
[32m[20221213 23:14:40 @agent_ppo2.py:185][0m |          -0.0146 |          31.4307 |          15.6170 |
[32m[20221213 23:14:40 @agent_ppo2.py:185][0m |          -0.0152 |          31.2842 |          15.6047 |
[32m[20221213 23:14:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:14:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.68
[32m[20221213 23:14:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.44
[32m[20221213 23:14:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.22
[32m[20221213 23:14:40 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 23:14:40 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 23:14:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2097 --------------------------#
[32m[20221213 23:14:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:40 @agent_ppo2.py:185][0m |           0.0047 |          56.0193 |          15.3049 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |           0.0032 |          54.4720 |          15.3178 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |          -0.0051 |          53.7773 |          15.2754 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |          -0.0070 |          54.5349 |          15.2956 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |          -0.0086 |          54.0023 |          15.2991 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |          -0.0069 |          53.3393 |          15.2688 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |           0.0022 |          60.1093 |          15.2648 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |          -0.0133 |          53.3545 |          15.2742 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |          -0.0100 |          53.0994 |          15.2613 |
[32m[20221213 23:14:41 @agent_ppo2.py:185][0m |          -0.0113 |          52.9311 |          15.2623 |
[32m[20221213 23:14:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:14:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.48
[32m[20221213 23:14:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.06
[32m[20221213 23:14:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.08
[32m[20221213 23:14:41 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221213 23:14:41 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 23:14:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2098 --------------------------#
[32m[20221213 23:14:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |           0.0003 |          52.5571 |          15.4819 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0048 |          51.7662 |          15.4868 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0075 |          51.5368 |          15.4884 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0005 |          52.5129 |          15.4888 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0057 |          51.1219 |          15.4741 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0075 |          51.0138 |          15.4919 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0086 |          50.9847 |          15.4897 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0045 |          51.2324 |          15.4761 |
[32m[20221213 23:14:42 @agent_ppo2.py:185][0m |          -0.0086 |          50.8879 |          15.4924 |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0131 |          50.8877 |          15.4878 |
[32m[20221213 23:14:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.65
[32m[20221213 23:14:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.63
[32m[20221213 23:14:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.76
[32m[20221213 23:14:43 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221213 23:14:43 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 23:14:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2099 --------------------------#
[32m[20221213 23:14:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0028 |          40.1068 |          15.7445 |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0011 |          38.5283 |          15.7651 |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0056 |          38.2094 |          15.7749 |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0088 |          38.0481 |          15.7459 |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0078 |          37.7799 |          15.7787 |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0055 |          37.7985 |          15.7802 |
[32m[20221213 23:14:43 @agent_ppo2.py:185][0m |          -0.0064 |          37.5208 |          15.7729 |
[32m[20221213 23:14:44 @agent_ppo2.py:185][0m |          -0.0092 |          37.2893 |          15.7712 |
[32m[20221213 23:14:44 @agent_ppo2.py:185][0m |          -0.0097 |          37.4023 |          15.7775 |
[32m[20221213 23:14:44 @agent_ppo2.py:185][0m |           0.0067 |          39.8442 |          15.7933 |
[32m[20221213 23:14:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.82
[32m[20221213 23:14:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.61
[32m[20221213 23:14:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.59
[32m[20221213 23:14:44 @agent_ppo2.py:143][0m Total time:       2.20 min
[32m[20221213 23:14:44 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 23:14:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2100 --------------------------#
[32m[20221213 23:14:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:14:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:44 @agent_ppo2.py:185][0m |          -0.0038 |          46.5772 |          15.4219 |
[32m[20221213 23:14:44 @agent_ppo2.py:185][0m |          -0.0108 |          43.8545 |          15.4268 |
[32m[20221213 23:14:44 @agent_ppo2.py:185][0m |          -0.0081 |          43.9966 |          15.4389 |
[32m[20221213 23:14:45 @agent_ppo2.py:185][0m |          -0.0087 |          42.5808 |          15.4348 |
[32m[20221213 23:14:45 @agent_ppo2.py:185][0m |           0.0020 |          44.6847 |          15.4633 |
[32m[20221213 23:14:45 @agent_ppo2.py:185][0m |          -0.0102 |          41.3897 |          15.4297 |
[32m[20221213 23:14:45 @agent_ppo2.py:185][0m |          -0.0138 |          40.8757 |          15.4661 |
[32m[20221213 23:14:45 @agent_ppo2.py:185][0m |          -0.0140 |          40.6799 |          15.4494 |
[32m[20221213 23:14:45 @agent_ppo2.py:185][0m |          -0.0133 |          40.2350 |          15.4722 |
[32m[20221213 23:14:45 @agent_ppo2.py:185][0m |          -0.0176 |          39.9439 |          15.4674 |
[32m[20221213 23:14:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.94
[32m[20221213 23:14:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.63
[32m[20221213 23:14:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.23
[32m[20221213 23:14:45 @agent_ppo2.py:143][0m Total time:       2.22 min
[32m[20221213 23:14:45 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 23:14:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2101 --------------------------#
[32m[20221213 23:14:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |          -0.0021 |          34.2697 |          15.6299 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |          -0.0055 |          31.8663 |          15.6302 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |          -0.0056 |          30.4970 |          15.6208 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |           0.0036 |          32.5482 |          15.6289 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |          -0.0070 |          29.2467 |          15.6251 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |          -0.0068 |          28.7447 |          15.6557 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |          -0.0081 |          28.3118 |          15.6537 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |           0.0041 |          31.2401 |          15.6660 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |           0.0012 |          33.2235 |          15.6620 |
[32m[20221213 23:14:46 @agent_ppo2.py:185][0m |          -0.0102 |          27.8441 |          15.6614 |
[32m[20221213 23:14:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.95
[32m[20221213 23:14:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.83
[32m[20221213 23:14:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.70
[32m[20221213 23:14:46 @agent_ppo2.py:143][0m Total time:       2.24 min
[32m[20221213 23:14:46 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 23:14:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2102 --------------------------#
[32m[20221213 23:14:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0026 |          43.5401 |          15.8067 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0035 |          39.9599 |          15.8116 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0031 |          38.7907 |          15.8217 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0086 |          37.9191 |          15.8271 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0078 |          37.2725 |          15.8230 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0104 |          36.7740 |          15.8436 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0148 |          36.2519 |          15.8429 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0123 |          35.9705 |          15.8454 |
[32m[20221213 23:14:47 @agent_ppo2.py:185][0m |          -0.0129 |          35.5918 |          15.8521 |
[32m[20221213 23:14:48 @agent_ppo2.py:185][0m |          -0.0158 |          35.2942 |          15.8615 |
[32m[20221213 23:14:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.88
[32m[20221213 23:14:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.66
[32m[20221213 23:14:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.66
[32m[20221213 23:14:48 @agent_ppo2.py:143][0m Total time:       2.26 min
[32m[20221213 23:14:48 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 23:14:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2103 --------------------------#
[32m[20221213 23:14:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:48 @agent_ppo2.py:185][0m |          -0.0008 |          47.3421 |          15.6179 |
[32m[20221213 23:14:48 @agent_ppo2.py:185][0m |          -0.0071 |          46.2437 |          15.5980 |
[32m[20221213 23:14:48 @agent_ppo2.py:185][0m |          -0.0075 |          46.0967 |          15.5914 |
[32m[20221213 23:14:48 @agent_ppo2.py:185][0m |          -0.0099 |          45.9361 |          15.5955 |
[32m[20221213 23:14:48 @agent_ppo2.py:185][0m |          -0.0065 |          45.9444 |          15.6194 |
[32m[20221213 23:14:48 @agent_ppo2.py:185][0m |          -0.0116 |          45.4972 |          15.6158 |
[32m[20221213 23:14:49 @agent_ppo2.py:185][0m |          -0.0037 |          46.5593 |          15.6141 |
[32m[20221213 23:14:49 @agent_ppo2.py:185][0m |          -0.0072 |          46.2475 |          15.6345 |
[32m[20221213 23:14:49 @agent_ppo2.py:185][0m |          -0.0128 |          45.1569 |          15.6237 |
[32m[20221213 23:14:49 @agent_ppo2.py:185][0m |          -0.0074 |          46.1522 |          15.6363 |
[32m[20221213 23:14:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.70
[32m[20221213 23:14:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.82
[32m[20221213 23:14:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.86
[32m[20221213 23:14:49 @agent_ppo2.py:143][0m Total time:       2.28 min
[32m[20221213 23:14:49 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 23:14:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2104 --------------------------#
[32m[20221213 23:14:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:49 @agent_ppo2.py:185][0m |           0.0013 |          43.5969 |          15.5819 |
[32m[20221213 23:14:49 @agent_ppo2.py:185][0m |          -0.0017 |          42.0904 |          15.5711 |
[32m[20221213 23:14:49 @agent_ppo2.py:185][0m |          -0.0067 |          41.0926 |          15.5953 |
[32m[20221213 23:14:50 @agent_ppo2.py:185][0m |          -0.0093 |          40.7010 |          15.5743 |
[32m[20221213 23:14:50 @agent_ppo2.py:185][0m |          -0.0099 |          40.4262 |          15.5709 |
[32m[20221213 23:14:50 @agent_ppo2.py:185][0m |          -0.0088 |          40.2257 |          15.5704 |
[32m[20221213 23:14:50 @agent_ppo2.py:185][0m |           0.0023 |          41.9873 |          15.5869 |
[32m[20221213 23:14:50 @agent_ppo2.py:185][0m |          -0.0122 |          39.6877 |          15.5905 |
[32m[20221213 23:14:50 @agent_ppo2.py:185][0m |          -0.0127 |          39.5319 |          15.5814 |
[32m[20221213 23:14:50 @agent_ppo2.py:185][0m |          -0.0147 |          39.3030 |          15.5812 |
[32m[20221213 23:14:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.83
[32m[20221213 23:14:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.18
[32m[20221213 23:14:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.61
[32m[20221213 23:14:50 @agent_ppo2.py:143][0m Total time:       2.30 min
[32m[20221213 23:14:50 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 23:14:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2105 --------------------------#
[32m[20221213 23:14:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0035 |          33.4682 |          15.6802 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0022 |          31.4662 |          15.6782 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0103 |          30.5006 |          15.6560 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0121 |          30.4514 |          15.6413 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0144 |          29.9797 |          15.6504 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0130 |          29.5114 |          15.6232 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0100 |          29.4018 |          15.6163 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0166 |          29.1416 |          15.6170 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0099 |          28.9354 |          15.5990 |
[32m[20221213 23:14:51 @agent_ppo2.py:185][0m |          -0.0086 |          28.8535 |          15.6083 |
[32m[20221213 23:14:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:14:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.80
[32m[20221213 23:14:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.36
[32m[20221213 23:14:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.44
[32m[20221213 23:14:51 @agent_ppo2.py:143][0m Total time:       2.32 min
[32m[20221213 23:14:51 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 23:14:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2106 --------------------------#
[32m[20221213 23:14:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:14:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |          -0.0001 |          14.0964 |          15.7574 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |          -0.0005 |          12.7877 |          15.7401 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |          -0.0028 |          12.7177 |          15.7455 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |          -0.0017 |          12.6992 |          15.7211 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |           0.0058 |          13.4346 |          15.7310 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |           0.0093 |          13.4186 |          15.7108 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |          -0.0001 |          12.6959 |          15.7082 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |           0.0090 |          13.8640 |          15.7214 |
[32m[20221213 23:14:52 @agent_ppo2.py:185][0m |          -0.0005 |          12.6895 |          15.7071 |
[32m[20221213 23:14:53 @agent_ppo2.py:185][0m |          -0.0024 |          12.6754 |          15.7074 |
[32m[20221213 23:14:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:14:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:14:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:14:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.95
[32m[20221213 23:14:53 @agent_ppo2.py:143][0m Total time:       2.35 min
[32m[20221213 23:14:53 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 23:14:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2107 --------------------------#
[32m[20221213 23:14:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:53 @agent_ppo2.py:185][0m |           0.0070 |          49.8200 |          15.6549 |
[32m[20221213 23:14:53 @agent_ppo2.py:185][0m |           0.0075 |          48.7560 |          15.6927 |
[32m[20221213 23:14:53 @agent_ppo2.py:185][0m |          -0.0053 |          45.9339 |          15.6770 |
[32m[20221213 23:14:53 @agent_ppo2.py:185][0m |          -0.0073 |          45.5047 |          15.6765 |
[32m[20221213 23:14:53 @agent_ppo2.py:185][0m |          -0.0067 |          45.2663 |          15.6617 |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |          -0.0070 |          44.8764 |          15.6682 |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |          -0.0076 |          44.8214 |          15.6709 |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |          -0.0052 |          44.6667 |          15.6542 |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |          -0.0018 |          44.9808 |          15.6532 |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |          -0.0104 |          44.3121 |          15.6670 |
[32m[20221213 23:14:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.03
[32m[20221213 23:14:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.61
[32m[20221213 23:14:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.89
[32m[20221213 23:14:54 @agent_ppo2.py:143][0m Total time:       2.37 min
[32m[20221213 23:14:54 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 23:14:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2108 --------------------------#
[32m[20221213 23:14:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |           0.0027 |          38.8612 |          15.4581 |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |          -0.0022 |          36.8860 |          15.4578 |
[32m[20221213 23:14:54 @agent_ppo2.py:185][0m |          -0.0110 |          36.1344 |          15.4450 |
[32m[20221213 23:14:55 @agent_ppo2.py:185][0m |          -0.0077 |          35.6995 |          15.4568 |
[32m[20221213 23:14:55 @agent_ppo2.py:185][0m |          -0.0107 |          35.1723 |          15.4531 |
[32m[20221213 23:14:55 @agent_ppo2.py:185][0m |          -0.0104 |          34.9822 |          15.4366 |
[32m[20221213 23:14:55 @agent_ppo2.py:185][0m |          -0.0101 |          34.7794 |          15.4415 |
[32m[20221213 23:14:55 @agent_ppo2.py:185][0m |          -0.0146 |          34.6696 |          15.4414 |
[32m[20221213 23:14:55 @agent_ppo2.py:185][0m |          -0.0156 |          34.3679 |          15.4418 |
[32m[20221213 23:14:55 @agent_ppo2.py:185][0m |          -0.0085 |          34.2664 |          15.4480 |
[32m[20221213 23:14:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.78
[32m[20221213 23:14:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.66
[32m[20221213 23:14:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.26
[32m[20221213 23:14:55 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221213 23:14:55 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221213 23:14:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2109 --------------------------#
[32m[20221213 23:14:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:14:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |           0.0000 |          46.3114 |          15.4710 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |           0.0005 |          44.2999 |          15.4318 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |           0.0022 |          44.3307 |          15.4120 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |          -0.0064 |          43.2367 |          15.4115 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |          -0.0020 |          43.9559 |          15.4213 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |          -0.0092 |          42.8697 |          15.3994 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |          -0.0045 |          43.2297 |          15.4049 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |          -0.0117 |          42.3423 |          15.3535 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |          -0.0106 |          42.2118 |          15.3888 |
[32m[20221213 23:14:56 @agent_ppo2.py:185][0m |          -0.0111 |          42.0242 |          15.3703 |
[32m[20221213 23:14:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:14:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.91
[32m[20221213 23:14:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.67
[32m[20221213 23:14:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.24
[32m[20221213 23:14:56 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 23:14:56 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 23:14:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2110 --------------------------#
[32m[20221213 23:14:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:14:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |          -0.0015 |          52.7208 |          15.6074 |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |          -0.0045 |          50.3897 |          15.5956 |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |          -0.0063 |          48.8037 |          15.5729 |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |          -0.0014 |          48.4288 |          15.5816 |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |          -0.0061 |          46.7467 |          15.5969 |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |          -0.0105 |          46.0079 |          15.5719 |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |          -0.0063 |          45.8292 |          15.5728 |
[32m[20221213 23:14:57 @agent_ppo2.py:185][0m |           0.0009 |          49.6552 |          15.5744 |
[32m[20221213 23:14:58 @agent_ppo2.py:185][0m |          -0.0112 |          44.2209 |          15.5754 |
[32m[20221213 23:14:58 @agent_ppo2.py:185][0m |          -0.0069 |          43.8201 |          15.5705 |
[32m[20221213 23:14:58 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:14:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.35
[32m[20221213 23:14:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.12
[32m[20221213 23:14:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.51
[32m[20221213 23:14:58 @agent_ppo2.py:143][0m Total time:       2.43 min
[32m[20221213 23:14:58 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221213 23:14:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2111 --------------------------#
[32m[20221213 23:14:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:58 @agent_ppo2.py:185][0m |           0.0025 |          46.4217 |          15.4813 |
[32m[20221213 23:14:58 @agent_ppo2.py:185][0m |          -0.0034 |          44.7913 |          15.4772 |
[32m[20221213 23:14:58 @agent_ppo2.py:185][0m |          -0.0065 |          44.0074 |          15.4468 |
[32m[20221213 23:14:58 @agent_ppo2.py:185][0m |          -0.0044 |          43.5200 |          15.4568 |
[32m[20221213 23:14:58 @agent_ppo2.py:185][0m |           0.0065 |          48.4588 |          15.4393 |
[32m[20221213 23:14:59 @agent_ppo2.py:185][0m |          -0.0077 |          42.9293 |          15.4463 |
[32m[20221213 23:14:59 @agent_ppo2.py:185][0m |          -0.0115 |          42.3979 |          15.4464 |
[32m[20221213 23:14:59 @agent_ppo2.py:185][0m |          -0.0115 |          42.0855 |          15.4505 |
[32m[20221213 23:14:59 @agent_ppo2.py:185][0m |          -0.0102 |          41.8858 |          15.4244 |
[32m[20221213 23:14:59 @agent_ppo2.py:185][0m |          -0.0100 |          41.6748 |          15.4278 |
[32m[20221213 23:14:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:14:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.08
[32m[20221213 23:14:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.97
[32m[20221213 23:14:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.83
[32m[20221213 23:14:59 @agent_ppo2.py:143][0m Total time:       2.45 min
[32m[20221213 23:14:59 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 23:14:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2112 --------------------------#
[32m[20221213 23:14:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:14:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:14:59 @agent_ppo2.py:185][0m |           0.0046 |          51.9751 |          15.6666 |
[32m[20221213 23:14:59 @agent_ppo2.py:185][0m |          -0.0057 |          50.0134 |          15.6704 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |          -0.0054 |          49.6920 |          15.6737 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |          -0.0045 |          49.5165 |          15.6589 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |           0.0012 |          50.8721 |          15.6692 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |          -0.0068 |          49.3037 |          15.6549 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |          -0.0042 |          49.2670 |          15.6446 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |          -0.0044 |          49.2818 |          15.6587 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |          -0.0067 |          49.0968 |          15.6597 |
[32m[20221213 23:15:00 @agent_ppo2.py:185][0m |          -0.0025 |          49.2967 |          15.6688 |
[32m[20221213 23:15:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.64
[32m[20221213 23:15:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.67
[32m[20221213 23:15:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.86
[32m[20221213 23:15:00 @agent_ppo2.py:143][0m Total time:       2.47 min
[32m[20221213 23:15:00 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221213 23:15:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2113 --------------------------#
[32m[20221213 23:15:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |           0.0011 |          55.0549 |          15.4912 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0056 |          52.7923 |          15.4996 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0118 |          52.0771 |          15.4885 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0127 |          51.7025 |          15.5093 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0113 |          51.3269 |          15.4904 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0106 |          51.3583 |          15.5142 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0083 |          51.2159 |          15.4946 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0123 |          51.1041 |          15.4816 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0115 |          50.9497 |          15.4882 |
[32m[20221213 23:15:01 @agent_ppo2.py:185][0m |          -0.0143 |          50.6395 |          15.4839 |
[32m[20221213 23:15:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.89
[32m[20221213 23:15:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.82
[32m[20221213 23:15:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.45
[32m[20221213 23:15:02 @agent_ppo2.py:143][0m Total time:       2.49 min
[32m[20221213 23:15:02 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 23:15:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2114 --------------------------#
[32m[20221213 23:15:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0035 |          57.5058 |          15.3175 |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0050 |          54.4938 |          15.3123 |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0068 |          53.2325 |          15.3265 |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0046 |          52.7421 |          15.3136 |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0113 |          52.5447 |          15.3165 |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0071 |          52.7043 |          15.3194 |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0028 |          56.6330 |          15.3337 |
[32m[20221213 23:15:02 @agent_ppo2.py:185][0m |          -0.0107 |          51.7707 |          15.3210 |
[32m[20221213 23:15:03 @agent_ppo2.py:185][0m |          -0.0131 |          51.6748 |          15.3408 |
[32m[20221213 23:15:03 @agent_ppo2.py:185][0m |          -0.0141 |          51.5309 |          15.3592 |
[32m[20221213 23:15:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.98
[32m[20221213 23:15:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.07
[32m[20221213 23:15:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.31
[32m[20221213 23:15:03 @agent_ppo2.py:143][0m Total time:       2.51 min
[32m[20221213 23:15:03 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221213 23:15:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2115 --------------------------#
[32m[20221213 23:15:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:03 @agent_ppo2.py:185][0m |           0.0015 |          56.9940 |          15.5401 |
[32m[20221213 23:15:03 @agent_ppo2.py:185][0m |          -0.0054 |          55.7237 |          15.4999 |
[32m[20221213 23:15:03 @agent_ppo2.py:185][0m |          -0.0051 |          55.4288 |          15.5102 |
[32m[20221213 23:15:03 @agent_ppo2.py:185][0m |          -0.0047 |          54.8495 |          15.5266 |
[32m[20221213 23:15:03 @agent_ppo2.py:185][0m |          -0.0091 |          54.4123 |          15.5060 |
[32m[20221213 23:15:04 @agent_ppo2.py:185][0m |          -0.0108 |          54.3453 |          15.5004 |
[32m[20221213 23:15:04 @agent_ppo2.py:185][0m |          -0.0011 |          56.9538 |          15.4874 |
[32m[20221213 23:15:04 @agent_ppo2.py:185][0m |          -0.0109 |          54.0263 |          15.4902 |
[32m[20221213 23:15:04 @agent_ppo2.py:185][0m |          -0.0091 |          53.6363 |          15.4795 |
[32m[20221213 23:15:04 @agent_ppo2.py:185][0m |          -0.0030 |          55.2184 |          15.4756 |
[32m[20221213 23:15:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:15:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.06
[32m[20221213 23:15:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.07
[32m[20221213 23:15:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.17
[32m[20221213 23:15:04 @agent_ppo2.py:143][0m Total time:       2.53 min
[32m[20221213 23:15:04 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 23:15:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2116 --------------------------#
[32m[20221213 23:15:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:04 @agent_ppo2.py:185][0m |          -0.0018 |          49.4296 |          15.6961 |
[32m[20221213 23:15:04 @agent_ppo2.py:185][0m |           0.0026 |          50.4248 |          15.7019 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |          -0.0078 |          46.1233 |          15.6986 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |           0.0000 |          49.0163 |          15.7144 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |          -0.0085 |          43.9659 |          15.6927 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |          -0.0056 |          43.0479 |          15.7080 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |           0.0019 |          45.4341 |          15.7031 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |          -0.0083 |          41.8325 |          15.6982 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |          -0.0116 |          41.1569 |          15.7196 |
[32m[20221213 23:15:05 @agent_ppo2.py:185][0m |          -0.0107 |          40.7389 |          15.7181 |
[32m[20221213 23:15:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.02
[32m[20221213 23:15:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.00
[32m[20221213 23:15:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.61
[32m[20221213 23:15:05 @agent_ppo2.py:143][0m Total time:       2.56 min
[32m[20221213 23:15:05 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221213 23:15:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2117 --------------------------#
[32m[20221213 23:15:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |           0.0037 |          40.6307 |          15.6259 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0035 |          33.0027 |          15.6003 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0044 |          32.2724 |          15.5738 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0021 |          31.5384 |          15.5651 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0024 |          31.5836 |          15.5565 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0067 |          31.0481 |          15.5286 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0005 |          31.1787 |          15.5073 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0102 |          30.6806 |          15.5159 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0047 |          30.5952 |          15.4843 |
[32m[20221213 23:15:06 @agent_ppo2.py:185][0m |          -0.0144 |          30.3914 |          15.4721 |
[32m[20221213 23:15:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.56
[32m[20221213 23:15:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.87
[32m[20221213 23:15:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.46
[32m[20221213 23:15:07 @agent_ppo2.py:143][0m Total time:       2.58 min
[32m[20221213 23:15:07 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 23:15:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2118 --------------------------#
[32m[20221213 23:15:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:07 @agent_ppo2.py:185][0m |           0.0060 |          45.2779 |          15.4600 |
[32m[20221213 23:15:07 @agent_ppo2.py:185][0m |           0.0056 |          44.5428 |          15.4456 |
[32m[20221213 23:15:07 @agent_ppo2.py:185][0m |           0.0021 |          44.8691 |          15.4677 |
[32m[20221213 23:15:07 @agent_ppo2.py:185][0m |          -0.0061 |          42.1197 |          15.4496 |
[32m[20221213 23:15:07 @agent_ppo2.py:185][0m |          -0.0106 |          41.7833 |          15.4560 |
[32m[20221213 23:15:07 @agent_ppo2.py:185][0m |          -0.0045 |          42.1293 |          15.4335 |
[32m[20221213 23:15:07 @agent_ppo2.py:185][0m |          -0.0105 |          41.7717 |          15.4346 |
[32m[20221213 23:15:08 @agent_ppo2.py:185][0m |          -0.0120 |          41.5778 |          15.4237 |
[32m[20221213 23:15:08 @agent_ppo2.py:185][0m |          -0.0111 |          41.6327 |          15.4277 |
[32m[20221213 23:15:08 @agent_ppo2.py:185][0m |          -0.0112 |          41.4398 |          15.4304 |
[32m[20221213 23:15:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.75
[32m[20221213 23:15:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.21
[32m[20221213 23:15:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.46
[32m[20221213 23:15:08 @agent_ppo2.py:143][0m Total time:       2.60 min
[32m[20221213 23:15:08 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221213 23:15:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2119 --------------------------#
[32m[20221213 23:15:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:08 @agent_ppo2.py:185][0m |           0.0001 |          48.5834 |          15.5945 |
[32m[20221213 23:15:08 @agent_ppo2.py:185][0m |          -0.0036 |          46.8687 |          15.5867 |
[32m[20221213 23:15:08 @agent_ppo2.py:185][0m |          -0.0048 |          46.2651 |          15.5922 |
[32m[20221213 23:15:08 @agent_ppo2.py:185][0m |          -0.0069 |          46.0650 |          15.5776 |
[32m[20221213 23:15:09 @agent_ppo2.py:185][0m |          -0.0104 |          45.6549 |          15.5680 |
[32m[20221213 23:15:09 @agent_ppo2.py:185][0m |          -0.0089 |          45.5864 |          15.5732 |
[32m[20221213 23:15:09 @agent_ppo2.py:185][0m |          -0.0029 |          49.9419 |          15.5729 |
[32m[20221213 23:15:09 @agent_ppo2.py:185][0m |          -0.0127 |          45.6641 |          15.5557 |
[32m[20221213 23:15:09 @agent_ppo2.py:185][0m |          -0.0093 |          45.2166 |          15.5540 |
[32m[20221213 23:15:09 @agent_ppo2.py:185][0m |          -0.0027 |          47.9486 |          15.5593 |
[32m[20221213 23:15:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:15:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.96
[32m[20221213 23:15:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.71
[32m[20221213 23:15:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.57
[32m[20221213 23:15:09 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221213 23:15:09 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 23:15:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2120 --------------------------#
[32m[20221213 23:15:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 23:15:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:09 @agent_ppo2.py:185][0m |          -0.0019 |          31.7467 |          15.6358 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0084 |          28.0441 |          15.6360 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0070 |          27.1687 |          15.6245 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0133 |          26.6908 |          15.6347 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0154 |          26.2890 |          15.6198 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0109 |          25.9706 |          15.6150 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0078 |          25.7979 |          15.6159 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0170 |          25.6127 |          15.6096 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0145 |          25.4237 |          15.6208 |
[32m[20221213 23:15:10 @agent_ppo2.py:185][0m |          -0.0138 |          25.1612 |          15.6068 |
[32m[20221213 23:15:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:15:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.89
[32m[20221213 23:15:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.21
[32m[20221213 23:15:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.37
[32m[20221213 23:15:10 @agent_ppo2.py:143][0m Total time:       2.64 min
[32m[20221213 23:15:10 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221213 23:15:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2121 --------------------------#
[32m[20221213 23:15:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0010 |          39.2934 |          15.5594 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |           0.0154 |          41.0694 |          15.5454 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0029 |          35.9021 |          15.5538 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0055 |          35.3018 |          15.5769 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0073 |          35.0606 |          15.5603 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0005 |          36.1193 |          15.5473 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0072 |          34.6621 |          15.5452 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0068 |          34.5552 |          15.5272 |
[32m[20221213 23:15:11 @agent_ppo2.py:185][0m |          -0.0055 |          34.3845 |          15.5655 |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |          -0.0066 |          34.2345 |          15.5312 |
[32m[20221213 23:15:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.43
[32m[20221213 23:15:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.71
[32m[20221213 23:15:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.86
[32m[20221213 23:15:12 @agent_ppo2.py:143][0m Total time:       2.66 min
[32m[20221213 23:15:12 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 23:15:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2122 --------------------------#
[32m[20221213 23:15:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |           0.0006 |          47.9593 |          15.5424 |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |          -0.0009 |          45.1300 |          15.5260 |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |          -0.0061 |          44.3440 |          15.5248 |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |          -0.0064 |          43.7901 |          15.5071 |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |           0.0055 |          47.0236 |          15.4711 |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |          -0.0072 |          43.2463 |          15.4983 |
[32m[20221213 23:15:12 @agent_ppo2.py:185][0m |          -0.0061 |          42.9323 |          15.5089 |
[32m[20221213 23:15:13 @agent_ppo2.py:185][0m |          -0.0114 |          42.7977 |          15.4934 |
[32m[20221213 23:15:13 @agent_ppo2.py:185][0m |          -0.0055 |          42.6831 |          15.4961 |
[32m[20221213 23:15:13 @agent_ppo2.py:185][0m |          -0.0100 |          42.4392 |          15.4774 |
[32m[20221213 23:15:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.39
[32m[20221213 23:15:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.05
[32m[20221213 23:15:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.25
[32m[20221213 23:15:13 @agent_ppo2.py:143][0m Total time:       2.68 min
[32m[20221213 23:15:13 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221213 23:15:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2123 --------------------------#
[32m[20221213 23:15:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:13 @agent_ppo2.py:185][0m |          -0.0056 |          36.3336 |          15.5401 |
[32m[20221213 23:15:13 @agent_ppo2.py:185][0m |           0.0020 |          36.0047 |          15.5394 |
[32m[20221213 23:15:13 @agent_ppo2.py:185][0m |          -0.0055 |          34.3220 |          15.5335 |
[32m[20221213 23:15:13 @agent_ppo2.py:185][0m |          -0.0075 |          33.5462 |          15.5310 |
[32m[20221213 23:15:14 @agent_ppo2.py:185][0m |          -0.0045 |          34.6222 |          15.5468 |
[32m[20221213 23:15:14 @agent_ppo2.py:185][0m |          -0.0157 |          33.2274 |          15.5298 |
[32m[20221213 23:15:14 @agent_ppo2.py:185][0m |          -0.0137 |          32.8892 |          15.5398 |
[32m[20221213 23:15:14 @agent_ppo2.py:185][0m |          -0.0175 |          32.8159 |          15.5443 |
[32m[20221213 23:15:14 @agent_ppo2.py:185][0m |          -0.0116 |          33.0616 |          15.5456 |
[32m[20221213 23:15:14 @agent_ppo2.py:185][0m |          -0.0187 |          32.5405 |          15.5548 |
[32m[20221213 23:15:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.17
[32m[20221213 23:15:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.43
[32m[20221213 23:15:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.26
[32m[20221213 23:15:14 @agent_ppo2.py:143][0m Total time:       2.70 min
[32m[20221213 23:15:14 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 23:15:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2124 --------------------------#
[32m[20221213 23:15:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |           0.0097 |          43.2485 |          15.5143 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0071 |          39.6296 |          15.4771 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0099 |          39.2207 |          15.4908 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0082 |          39.0853 |          15.5012 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0093 |          38.8609 |          15.5236 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0086 |          38.7882 |          15.5186 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0119 |          38.4790 |          15.5003 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0094 |          39.0857 |          15.5444 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0134 |          38.1721 |          15.5417 |
[32m[20221213 23:15:15 @agent_ppo2.py:185][0m |          -0.0174 |          38.1724 |          15.5314 |
[32m[20221213 23:15:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:15:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.95
[32m[20221213 23:15:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.33
[32m[20221213 23:15:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.26
[32m[20221213 23:15:15 @agent_ppo2.py:143][0m Total time:       2.72 min
[32m[20221213 23:15:15 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221213 23:15:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2125 --------------------------#
[32m[20221213 23:15:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |           0.0004 |          51.2797 |          15.4170 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |          -0.0082 |          50.2597 |          15.4390 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |          -0.0040 |          51.2391 |          15.4429 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |          -0.0083 |          49.5724 |          15.4343 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |          -0.0098 |          49.4052 |          15.4607 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |          -0.0089 |          49.3231 |          15.4475 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |          -0.0104 |          49.3749 |          15.4404 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |          -0.0125 |          49.0674 |          15.4397 |
[32m[20221213 23:15:16 @agent_ppo2.py:185][0m |           0.0104 |          58.7433 |          15.4429 |
[32m[20221213 23:15:17 @agent_ppo2.py:185][0m |          -0.0110 |          49.1825 |          15.4378 |
[32m[20221213 23:15:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.05
[32m[20221213 23:15:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.81
[32m[20221213 23:15:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.29
[32m[20221213 23:15:17 @agent_ppo2.py:143][0m Total time:       2.74 min
[32m[20221213 23:15:17 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 23:15:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2126 --------------------------#
[32m[20221213 23:15:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:17 @agent_ppo2.py:185][0m |          -0.0028 |          32.2082 |          15.4514 |
[32m[20221213 23:15:17 @agent_ppo2.py:185][0m |          -0.0031 |          30.3721 |          15.4849 |
[32m[20221213 23:15:17 @agent_ppo2.py:185][0m |          -0.0079 |          29.8319 |          15.5121 |
[32m[20221213 23:15:17 @agent_ppo2.py:185][0m |          -0.0055 |          29.8269 |          15.5127 |
[32m[20221213 23:15:17 @agent_ppo2.py:185][0m |          -0.0099 |          29.4294 |          15.5032 |
[32m[20221213 23:15:17 @agent_ppo2.py:185][0m |          -0.0076 |          29.1434 |          15.5060 |
[32m[20221213 23:15:18 @agent_ppo2.py:185][0m |          -0.0079 |          29.3073 |          15.5105 |
[32m[20221213 23:15:18 @agent_ppo2.py:185][0m |          -0.0104 |          29.0599 |          15.5113 |
[32m[20221213 23:15:18 @agent_ppo2.py:185][0m |          -0.0118 |          29.5704 |          15.5061 |
[32m[20221213 23:15:18 @agent_ppo2.py:185][0m |          -0.0120 |          28.7451 |          15.5159 |
[32m[20221213 23:15:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.92
[32m[20221213 23:15:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.13
[32m[20221213 23:15:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.96
[32m[20221213 23:15:18 @agent_ppo2.py:143][0m Total time:       2.77 min
[32m[20221213 23:15:18 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221213 23:15:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2127 --------------------------#
[32m[20221213 23:15:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:18 @agent_ppo2.py:185][0m |           0.0026 |          41.1264 |          15.5673 |
[32m[20221213 23:15:18 @agent_ppo2.py:185][0m |          -0.0031 |          40.3501 |          15.5681 |
[32m[20221213 23:15:18 @agent_ppo2.py:185][0m |          -0.0047 |          40.0857 |          15.5501 |
[32m[20221213 23:15:19 @agent_ppo2.py:185][0m |          -0.0047 |          39.8941 |          15.5491 |
[32m[20221213 23:15:19 @agent_ppo2.py:185][0m |          -0.0063 |          39.6975 |          15.5284 |
[32m[20221213 23:15:19 @agent_ppo2.py:185][0m |          -0.0093 |          39.5236 |          15.5404 |
[32m[20221213 23:15:19 @agent_ppo2.py:185][0m |          -0.0080 |          39.3506 |          15.5327 |
[32m[20221213 23:15:19 @agent_ppo2.py:185][0m |          -0.0102 |          39.2481 |          15.5222 |
[32m[20221213 23:15:19 @agent_ppo2.py:185][0m |          -0.0072 |          39.0681 |          15.5252 |
[32m[20221213 23:15:19 @agent_ppo2.py:185][0m |          -0.0087 |          39.0860 |          15.4978 |
[32m[20221213 23:15:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.75
[32m[20221213 23:15:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.08
[32m[20221213 23:15:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.15
[32m[20221213 23:15:19 @agent_ppo2.py:143][0m Total time:       2.79 min
[32m[20221213 23:15:19 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 23:15:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2128 --------------------------#
[32m[20221213 23:15:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |           0.0045 |          41.6394 |          15.5118 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |           0.0000 |          39.3353 |          15.4847 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0064 |          38.1190 |          15.4631 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0052 |          37.8183 |          15.4626 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0080 |          37.2990 |          15.4605 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0071 |          37.0805 |          15.4378 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0072 |          36.6426 |          15.4391 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0041 |          37.6988 |          15.4119 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0120 |          36.1880 |          15.4388 |
[32m[20221213 23:15:20 @agent_ppo2.py:185][0m |          -0.0115 |          35.9345 |          15.4002 |
[32m[20221213 23:15:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.51
[32m[20221213 23:15:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.84
[32m[20221213 23:15:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.80
[32m[20221213 23:15:20 @agent_ppo2.py:143][0m Total time:       2.81 min
[32m[20221213 23:15:20 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221213 23:15:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2129 --------------------------#
[32m[20221213 23:15:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |           0.0051 |          44.0252 |          15.5769 |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |          -0.0019 |          42.5202 |          15.5758 |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |          -0.0040 |          42.1965 |          15.5872 |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |          -0.0080 |          41.8495 |          15.5769 |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |          -0.0068 |          41.4497 |          15.5723 |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |          -0.0091 |          41.4265 |          15.5912 |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |          -0.0037 |          43.0744 |          15.5998 |
[32m[20221213 23:15:21 @agent_ppo2.py:185][0m |          -0.0064 |          41.5408 |          15.6027 |
[32m[20221213 23:15:22 @agent_ppo2.py:185][0m |          -0.0103 |          40.8044 |          15.6111 |
[32m[20221213 23:15:22 @agent_ppo2.py:185][0m |          -0.0106 |          40.7185 |          15.6026 |
[32m[20221213 23:15:22 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:15:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.06
[32m[20221213 23:15:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.93
[32m[20221213 23:15:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.03
[32m[20221213 23:15:22 @agent_ppo2.py:143][0m Total time:       2.83 min
[32m[20221213 23:15:22 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 23:15:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2130 --------------------------#
[32m[20221213 23:15:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:15:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:22 @agent_ppo2.py:185][0m |          -0.0002 |          45.0380 |          15.4863 |
[32m[20221213 23:15:22 @agent_ppo2.py:185][0m |          -0.0037 |          43.0064 |          15.4760 |
[32m[20221213 23:15:22 @agent_ppo2.py:185][0m |          -0.0049 |          42.5402 |          15.4719 |
[32m[20221213 23:15:22 @agent_ppo2.py:185][0m |          -0.0049 |          42.3708 |          15.4888 |
[32m[20221213 23:15:22 @agent_ppo2.py:185][0m |           0.0088 |          48.1845 |          15.4672 |
[32m[20221213 23:15:23 @agent_ppo2.py:185][0m |           0.0031 |          43.8858 |          15.4870 |
[32m[20221213 23:15:23 @agent_ppo2.py:185][0m |          -0.0067 |          41.8727 |          15.4964 |
[32m[20221213 23:15:23 @agent_ppo2.py:185][0m |          -0.0029 |          42.5321 |          15.4928 |
[32m[20221213 23:15:23 @agent_ppo2.py:185][0m |           0.0086 |          44.3537 |          15.4977 |
[32m[20221213 23:15:23 @agent_ppo2.py:185][0m |          -0.0045 |          41.8592 |          15.5034 |
[32m[20221213 23:15:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.05
[32m[20221213 23:15:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.69
[32m[20221213 23:15:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.28
[32m[20221213 23:15:23 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221213 23:15:23 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221213 23:15:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2131 --------------------------#
[32m[20221213 23:15:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:23 @agent_ppo2.py:185][0m |           0.0104 |          41.9166 |          15.5804 |
[32m[20221213 23:15:23 @agent_ppo2.py:185][0m |          -0.0086 |          34.2880 |          15.5757 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0121 |          33.5010 |          15.5530 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0114 |          32.7967 |          15.5390 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0123 |          32.2092 |          15.5403 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0129 |          32.0350 |          15.5422 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0159 |          31.8280 |          15.5326 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0154 |          31.3727 |          15.5378 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0180 |          31.1726 |          15.5133 |
[32m[20221213 23:15:24 @agent_ppo2.py:185][0m |          -0.0144 |          31.0430 |          15.5251 |
[32m[20221213 23:15:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.62
[32m[20221213 23:15:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.39
[32m[20221213 23:15:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.68
[32m[20221213 23:15:24 @agent_ppo2.py:143][0m Total time:       2.87 min
[32m[20221213 23:15:24 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 23:15:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2132 --------------------------#
[32m[20221213 23:15:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:15:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0039 |          10.9525 |          15.7961 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0007 |          10.0221 |          15.8104 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0014 |           9.9946 |          15.7957 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |           0.0002 |           9.9399 |          15.7842 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0072 |           9.9417 |          15.7943 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0017 |           9.9163 |          15.8353 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0048 |           9.8942 |          15.8039 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |           0.0021 |          10.0540 |          15.8007 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0044 |           9.8807 |          15.8094 |
[32m[20221213 23:15:25 @agent_ppo2.py:185][0m |          -0.0031 |           9.8672 |          15.7780 |
[32m[20221213 23:15:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:15:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:15:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:15:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.53
[32m[20221213 23:15:26 @agent_ppo2.py:143][0m Total time:       2.89 min
[32m[20221213 23:15:26 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221213 23:15:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2133 --------------------------#
[32m[20221213 23:15:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |           0.0016 |          40.8528 |          15.7536 |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |          -0.0052 |          38.7142 |          15.7493 |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |          -0.0017 |          37.6427 |          15.7429 |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |          -0.0070 |          37.1228 |          15.7534 |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |          -0.0068 |          36.7094 |          15.7426 |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |          -0.0086 |          36.2238 |          15.7511 |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |          -0.0045 |          38.1327 |          15.7549 |
[32m[20221213 23:15:26 @agent_ppo2.py:185][0m |          -0.0087 |          36.0080 |          15.7343 |
[32m[20221213 23:15:27 @agent_ppo2.py:185][0m |          -0.0064 |          35.8394 |          15.7464 |
[32m[20221213 23:15:27 @agent_ppo2.py:185][0m |          -0.0106 |          35.2453 |          15.7423 |
[32m[20221213 23:15:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.24
[32m[20221213 23:15:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.31
[32m[20221213 23:15:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.47
[32m[20221213 23:15:27 @agent_ppo2.py:143][0m Total time:       2.91 min
[32m[20221213 23:15:27 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 23:15:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2134 --------------------------#
[32m[20221213 23:15:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:27 @agent_ppo2.py:185][0m |          -0.0018 |          46.2564 |          15.5201 |
[32m[20221213 23:15:27 @agent_ppo2.py:185][0m |          -0.0067 |          42.6917 |          15.4777 |
[32m[20221213 23:15:27 @agent_ppo2.py:185][0m |          -0.0126 |          41.2459 |          15.4739 |
[32m[20221213 23:15:27 @agent_ppo2.py:185][0m |          -0.0056 |          40.2025 |          15.4516 |
[32m[20221213 23:15:27 @agent_ppo2.py:185][0m |          -0.0096 |          39.5959 |          15.4262 |
[32m[20221213 23:15:28 @agent_ppo2.py:185][0m |          -0.0146 |          39.3598 |          15.4391 |
[32m[20221213 23:15:28 @agent_ppo2.py:185][0m |          -0.0119 |          39.0230 |          15.4184 |
[32m[20221213 23:15:28 @agent_ppo2.py:185][0m |          -0.0178 |          38.8229 |          15.4164 |
[32m[20221213 23:15:28 @agent_ppo2.py:185][0m |          -0.0157 |          38.6499 |          15.4314 |
[32m[20221213 23:15:28 @agent_ppo2.py:185][0m |          -0.0078 |          40.0113 |          15.4097 |
[32m[20221213 23:15:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:15:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.18
[32m[20221213 23:15:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.88
[32m[20221213 23:15:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.06
[32m[20221213 23:15:28 @agent_ppo2.py:143][0m Total time:       2.93 min
[32m[20221213 23:15:28 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221213 23:15:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2135 --------------------------#
[32m[20221213 23:15:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:28 @agent_ppo2.py:185][0m |           0.0003 |          40.5670 |          15.5395 |
[32m[20221213 23:15:28 @agent_ppo2.py:185][0m |          -0.0101 |          36.2276 |          15.5556 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0082 |          35.3993 |          15.5333 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0080 |          35.2512 |          15.5507 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0088 |          34.7730 |          15.5210 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0005 |          39.8362 |          15.5321 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0127 |          34.3414 |          15.5220 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0053 |          35.9164 |          15.5391 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0110 |          34.7422 |          15.5232 |
[32m[20221213 23:15:29 @agent_ppo2.py:185][0m |          -0.0128 |          34.0164 |          15.5261 |
[32m[20221213 23:15:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.98
[32m[20221213 23:15:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.43
[32m[20221213 23:15:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.77
[32m[20221213 23:15:29 @agent_ppo2.py:143][0m Total time:       2.96 min
[32m[20221213 23:15:29 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 23:15:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2136 --------------------------#
[32m[20221213 23:15:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0038 |          39.4867 |          15.5883 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |           0.0036 |          37.4202 |          15.5805 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0010 |          36.4747 |          15.5419 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0059 |          36.3562 |          15.5474 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0064 |          36.3265 |          15.5391 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0058 |          36.0591 |          15.5264 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0048 |          35.9882 |          15.5258 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |           0.0042 |          38.0659 |          15.5200 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0082 |          36.1400 |          15.5074 |
[32m[20221213 23:15:30 @agent_ppo2.py:185][0m |          -0.0076 |          35.8592 |          15.4996 |
[32m[20221213 23:15:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.14
[32m[20221213 23:15:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.66
[32m[20221213 23:15:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.15
[32m[20221213 23:15:31 @agent_ppo2.py:143][0m Total time:       2.98 min
[32m[20221213 23:15:31 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221213 23:15:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2137 --------------------------#
[32m[20221213 23:15:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:31 @agent_ppo2.py:185][0m |           0.0040 |          51.2824 |          15.6405 |
[32m[20221213 23:15:31 @agent_ppo2.py:185][0m |          -0.0055 |          48.4524 |          15.6091 |
[32m[20221213 23:15:31 @agent_ppo2.py:185][0m |          -0.0106 |          47.1735 |          15.6088 |
[32m[20221213 23:15:31 @agent_ppo2.py:185][0m |          -0.0089 |          46.2555 |          15.6290 |
[32m[20221213 23:15:31 @agent_ppo2.py:185][0m |          -0.0115 |          45.7025 |          15.6204 |
[32m[20221213 23:15:31 @agent_ppo2.py:185][0m |          -0.0125 |          44.9692 |          15.6211 |
[32m[20221213 23:15:31 @agent_ppo2.py:185][0m |          -0.0141 |          44.4596 |          15.6243 |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |          -0.0120 |          43.9452 |          15.6402 |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |          -0.0111 |          43.9138 |          15.6403 |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |          -0.0137 |          43.4121 |          15.6568 |
[32m[20221213 23:15:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.57
[32m[20221213 23:15:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.12
[32m[20221213 23:15:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.95
[32m[20221213 23:15:32 @agent_ppo2.py:143][0m Total time:       3.00 min
[32m[20221213 23:15:32 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 23:15:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2138 --------------------------#
[32m[20221213 23:15:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |          -0.0007 |          44.9211 |          15.7083 |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |          -0.0034 |          41.6862 |          15.6918 |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |           0.0043 |          41.7007 |          15.6881 |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |          -0.0082 |          39.2782 |          15.6740 |
[32m[20221213 23:15:32 @agent_ppo2.py:185][0m |          -0.0062 |          38.5108 |          15.6579 |
[32m[20221213 23:15:33 @agent_ppo2.py:185][0m |          -0.0101 |          37.9093 |          15.6432 |
[32m[20221213 23:15:33 @agent_ppo2.py:185][0m |          -0.0102 |          37.3055 |          15.6228 |
[32m[20221213 23:15:33 @agent_ppo2.py:185][0m |          -0.0010 |          38.3253 |          15.6228 |
[32m[20221213 23:15:33 @agent_ppo2.py:185][0m |          -0.0034 |          37.7961 |          15.6009 |
[32m[20221213 23:15:33 @agent_ppo2.py:185][0m |          -0.0064 |          36.0757 |          15.5873 |
[32m[20221213 23:15:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.81
[32m[20221213 23:15:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.96
[32m[20221213 23:15:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.09
[32m[20221213 23:15:33 @agent_ppo2.py:143][0m Total time:       3.02 min
[32m[20221213 23:15:33 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221213 23:15:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2139 --------------------------#
[32m[20221213 23:15:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:33 @agent_ppo2.py:185][0m |          -0.0034 |          37.3718 |          15.5783 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0048 |          26.3123 |          15.5600 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0092 |          25.4186 |          15.5377 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0119 |          24.9024 |          15.5279 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0124 |          25.6936 |          15.5304 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0118 |          24.3208 |          15.5110 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0080 |          24.6056 |          15.5102 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0066 |          25.3205 |          15.4774 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0102 |          23.7986 |          15.4693 |
[32m[20221213 23:15:34 @agent_ppo2.py:185][0m |          -0.0104 |          23.6073 |          15.4641 |
[32m[20221213 23:15:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:15:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.16
[32m[20221213 23:15:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.10
[32m[20221213 23:15:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.80
[32m[20221213 23:15:34 @agent_ppo2.py:143][0m Total time:       3.04 min
[32m[20221213 23:15:34 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 23:15:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2140 --------------------------#
[32m[20221213 23:15:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:15:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0004 |          28.8683 |          15.6293 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0055 |          24.1130 |          15.6392 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0035 |          23.7568 |          15.6127 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0101 |          23.4012 |          15.6331 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0068 |          23.1072 |          15.6500 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0101 |          22.7259 |          15.6445 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0110 |          22.6660 |          15.6220 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0122 |          22.4756 |          15.6207 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0049 |          22.3990 |          15.6280 |
[32m[20221213 23:15:35 @agent_ppo2.py:185][0m |          -0.0144 |          22.3233 |          15.6243 |
[32m[20221213 23:15:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.29
[32m[20221213 23:15:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.65
[32m[20221213 23:15:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.99
[32m[20221213 23:15:36 @agent_ppo2.py:143][0m Total time:       3.06 min
[32m[20221213 23:15:36 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221213 23:15:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2141 --------------------------#
[32m[20221213 23:15:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:36 @agent_ppo2.py:185][0m |          -0.0013 |          32.5758 |          15.1432 |
[32m[20221213 23:15:36 @agent_ppo2.py:185][0m |          -0.0050 |          28.8839 |          15.1701 |
[32m[20221213 23:15:36 @agent_ppo2.py:185][0m |          -0.0095 |          28.1772 |          15.1713 |
[32m[20221213 23:15:36 @agent_ppo2.py:185][0m |          -0.0105 |          27.7364 |          15.1619 |
[32m[20221213 23:15:36 @agent_ppo2.py:185][0m |          -0.0011 |          30.1132 |          15.1767 |
[32m[20221213 23:15:36 @agent_ppo2.py:185][0m |          -0.0150 |          27.3063 |          15.1795 |
[32m[20221213 23:15:36 @agent_ppo2.py:185][0m |          -0.0127 |          27.0950 |          15.1980 |
[32m[20221213 23:15:37 @agent_ppo2.py:185][0m |          -0.0140 |          26.7539 |          15.1831 |
[32m[20221213 23:15:37 @agent_ppo2.py:185][0m |          -0.0139 |          26.7288 |          15.1916 |
[32m[20221213 23:15:37 @agent_ppo2.py:185][0m |          -0.0148 |          26.4985 |          15.2021 |
[32m[20221213 23:15:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.57
[32m[20221213 23:15:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.61
[32m[20221213 23:15:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.05
[32m[20221213 23:15:37 @agent_ppo2.py:143][0m Total time:       3.08 min
[32m[20221213 23:15:37 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 23:15:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2142 --------------------------#
[32m[20221213 23:15:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:37 @agent_ppo2.py:185][0m |           0.0020 |          30.0019 |          15.5148 |
[32m[20221213 23:15:37 @agent_ppo2.py:185][0m |          -0.0047 |          26.1944 |          15.5282 |
[32m[20221213 23:15:37 @agent_ppo2.py:185][0m |          -0.0064 |          25.0626 |          15.4830 |
[32m[20221213 23:15:37 @agent_ppo2.py:185][0m |          -0.0074 |          24.5821 |          15.4824 |
[32m[20221213 23:15:38 @agent_ppo2.py:185][0m |          -0.0076 |          24.7120 |          15.4775 |
[32m[20221213 23:15:38 @agent_ppo2.py:185][0m |          -0.0077 |          23.8986 |          15.4539 |
[32m[20221213 23:15:38 @agent_ppo2.py:185][0m |           0.0003 |          26.4304 |          15.4909 |
[32m[20221213 23:15:38 @agent_ppo2.py:185][0m |          -0.0147 |          23.3771 |          15.4680 |
[32m[20221213 23:15:38 @agent_ppo2.py:185][0m |          -0.0137 |          22.9830 |          15.4485 |
[32m[20221213 23:15:38 @agent_ppo2.py:185][0m |           0.0016 |          25.9794 |          15.4631 |
[32m[20221213 23:15:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.87
[32m[20221213 23:15:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.09
[32m[20221213 23:15:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.81
[32m[20221213 23:15:38 @agent_ppo2.py:143][0m Total time:       3.10 min
[32m[20221213 23:15:38 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221213 23:15:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2143 --------------------------#
[32m[20221213 23:15:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:38 @agent_ppo2.py:185][0m |           0.0021 |          31.0385 |          15.4136 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0039 |          28.1530 |          15.3948 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0066 |          27.2330 |          15.3796 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0076 |          27.0040 |          15.3756 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0054 |          26.3728 |          15.3686 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0052 |          26.0705 |          15.3762 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0103 |          26.0045 |          15.3549 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0093 |          25.6327 |          15.3674 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0129 |          25.5432 |          15.3466 |
[32m[20221213 23:15:39 @agent_ppo2.py:185][0m |          -0.0056 |          25.6459 |          15.3602 |
[32m[20221213 23:15:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:15:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.61
[32m[20221213 23:15:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.38
[32m[20221213 23:15:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.91
[32m[20221213 23:15:39 @agent_ppo2.py:143][0m Total time:       3.12 min
[32m[20221213 23:15:39 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 23:15:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2144 --------------------------#
[32m[20221213 23:15:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |           0.0120 |          45.8001 |          15.3460 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0015 |          40.9365 |          15.3464 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0042 |          40.2237 |          15.3497 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0107 |          39.4777 |          15.3499 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0019 |          39.6828 |          15.3588 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0060 |          38.7078 |          15.3667 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0012 |          40.9872 |          15.3891 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0091 |          38.6355 |          15.3959 |
[32m[20221213 23:15:40 @agent_ppo2.py:185][0m |          -0.0133 |          38.0465 |          15.3975 |
[32m[20221213 23:15:41 @agent_ppo2.py:185][0m |          -0.0108 |          37.9713 |          15.3973 |
[32m[20221213 23:15:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.83
[32m[20221213 23:15:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.97
[32m[20221213 23:15:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.75
[32m[20221213 23:15:41 @agent_ppo2.py:143][0m Total time:       3.14 min
[32m[20221213 23:15:41 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221213 23:15:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2145 --------------------------#
[32m[20221213 23:15:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:41 @agent_ppo2.py:185][0m |           0.0015 |          41.1768 |          15.6745 |
[32m[20221213 23:15:41 @agent_ppo2.py:185][0m |          -0.0048 |          38.1161 |          15.6262 |
[32m[20221213 23:15:41 @agent_ppo2.py:185][0m |          -0.0033 |          36.9746 |          15.6195 |
[32m[20221213 23:15:41 @agent_ppo2.py:185][0m |          -0.0080 |          36.2081 |          15.6200 |
[32m[20221213 23:15:41 @agent_ppo2.py:185][0m |          -0.0097 |          35.8081 |          15.5943 |
[32m[20221213 23:15:41 @agent_ppo2.py:185][0m |          -0.0178 |          35.0440 |          15.6011 |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0094 |          34.6841 |          15.5897 |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0133 |          34.1765 |          15.5878 |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0114 |          33.8234 |          15.5932 |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0136 |          33.3207 |          15.5894 |
[32m[20221213 23:15:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.13
[32m[20221213 23:15:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.57
[32m[20221213 23:15:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.08
[32m[20221213 23:15:42 @agent_ppo2.py:143][0m Total time:       3.16 min
[32m[20221213 23:15:42 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 23:15:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2146 --------------------------#
[32m[20221213 23:15:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0031 |          55.3602 |          15.4987 |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0053 |          54.0076 |          15.4512 |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0079 |          53.4619 |          15.4546 |
[32m[20221213 23:15:42 @agent_ppo2.py:185][0m |          -0.0061 |          53.1865 |          15.4688 |
[32m[20221213 23:15:43 @agent_ppo2.py:185][0m |          -0.0088 |          52.8983 |          15.4508 |
[32m[20221213 23:15:43 @agent_ppo2.py:185][0m |          -0.0093 |          52.7952 |          15.4593 |
[32m[20221213 23:15:43 @agent_ppo2.py:185][0m |          -0.0091 |          52.6245 |          15.4585 |
[32m[20221213 23:15:43 @agent_ppo2.py:185][0m |          -0.0110 |          52.3779 |          15.4745 |
[32m[20221213 23:15:43 @agent_ppo2.py:185][0m |          -0.0138 |          52.3531 |          15.4713 |
[32m[20221213 23:15:43 @agent_ppo2.py:185][0m |          -0.0114 |          52.3847 |          15.4792 |
[32m[20221213 23:15:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.53
[32m[20221213 23:15:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.98
[32m[20221213 23:15:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.65
[32m[20221213 23:15:43 @agent_ppo2.py:143][0m Total time:       3.19 min
[32m[20221213 23:15:43 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221213 23:15:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2147 --------------------------#
[32m[20221213 23:15:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0005 |          46.0687 |          15.4248 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0042 |          40.2348 |          15.4062 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0058 |          38.8294 |          15.3899 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0069 |          38.1048 |          15.4010 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0158 |          37.3434 |          15.4071 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0151 |          37.1677 |          15.3899 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0141 |          36.6834 |          15.3916 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0121 |          36.5309 |          15.4070 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0027 |          38.3178 |          15.4115 |
[32m[20221213 23:15:44 @agent_ppo2.py:185][0m |          -0.0121 |          36.4398 |          15.4158 |
[32m[20221213 23:15:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.14
[32m[20221213 23:15:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.50
[32m[20221213 23:15:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.04
[32m[20221213 23:15:44 @agent_ppo2.py:143][0m Total time:       3.21 min
[32m[20221213 23:15:44 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 23:15:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2148 --------------------------#
[32m[20221213 23:15:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0025 |          36.1879 |          15.6117 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |           0.0000 |          33.1596 |          15.6155 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0054 |          32.0057 |          15.5917 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0101 |          31.1340 |          15.6072 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0094 |          30.7314 |          15.5933 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0161 |          30.3030 |          15.5878 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0140 |          29.8745 |          15.5760 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0131 |          29.5040 |          15.5791 |
[32m[20221213 23:15:45 @agent_ppo2.py:185][0m |          -0.0177 |          29.1610 |          15.5852 |
[32m[20221213 23:15:46 @agent_ppo2.py:185][0m |          -0.0161 |          28.9439 |          15.5910 |
[32m[20221213 23:15:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:15:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.47
[32m[20221213 23:15:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.33
[32m[20221213 23:15:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.21
[32m[20221213 23:15:46 @agent_ppo2.py:143][0m Total time:       3.23 min
[32m[20221213 23:15:46 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221213 23:15:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2149 --------------------------#
[32m[20221213 23:15:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:46 @agent_ppo2.py:185][0m |          -0.0009 |          38.8192 |          15.4790 |
[32m[20221213 23:15:46 @agent_ppo2.py:185][0m |          -0.0059 |          34.8029 |          15.4629 |
[32m[20221213 23:15:46 @agent_ppo2.py:185][0m |          -0.0167 |          33.4834 |          15.4385 |
[32m[20221213 23:15:46 @agent_ppo2.py:185][0m |          -0.0099 |          32.3612 |          15.4093 |
[32m[20221213 23:15:46 @agent_ppo2.py:185][0m |          -0.0143 |          31.5215 |          15.4146 |
[32m[20221213 23:15:46 @agent_ppo2.py:185][0m |          -0.0041 |          31.8640 |          15.3896 |
[32m[20221213 23:15:47 @agent_ppo2.py:185][0m |          -0.0137 |          30.4228 |          15.3902 |
[32m[20221213 23:15:47 @agent_ppo2.py:185][0m |          -0.0188 |          30.0034 |          15.3817 |
[32m[20221213 23:15:47 @agent_ppo2.py:185][0m |          -0.0186 |          29.7393 |          15.3636 |
[32m[20221213 23:15:47 @agent_ppo2.py:185][0m |          -0.0167 |          29.3003 |          15.3699 |
[32m[20221213 23:15:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.83
[32m[20221213 23:15:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.99
[32m[20221213 23:15:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.65
[32m[20221213 23:15:47 @agent_ppo2.py:143][0m Total time:       3.25 min
[32m[20221213 23:15:47 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 23:15:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2150 --------------------------#
[32m[20221213 23:15:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:15:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:47 @agent_ppo2.py:185][0m |          -0.0013 |          37.8710 |          15.3902 |
[32m[20221213 23:15:47 @agent_ppo2.py:185][0m |          -0.0068 |          35.5212 |          15.3808 |
[32m[20221213 23:15:47 @agent_ppo2.py:185][0m |          -0.0072 |          34.2671 |          15.3769 |
[32m[20221213 23:15:48 @agent_ppo2.py:185][0m |          -0.0127 |          33.7154 |          15.3765 |
[32m[20221213 23:15:48 @agent_ppo2.py:185][0m |          -0.0119 |          33.1396 |          15.3774 |
[32m[20221213 23:15:48 @agent_ppo2.py:185][0m |          -0.0131 |          32.7938 |          15.3803 |
[32m[20221213 23:15:48 @agent_ppo2.py:185][0m |          -0.0121 |          32.2809 |          15.3872 |
[32m[20221213 23:15:48 @agent_ppo2.py:185][0m |          -0.0140 |          31.9369 |          15.3956 |
[32m[20221213 23:15:48 @agent_ppo2.py:185][0m |          -0.0154 |          31.7213 |          15.3793 |
[32m[20221213 23:15:48 @agent_ppo2.py:185][0m |          -0.0190 |          31.3773 |          15.3821 |
[32m[20221213 23:15:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.62
[32m[20221213 23:15:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.16
[32m[20221213 23:15:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.53
[32m[20221213 23:15:48 @agent_ppo2.py:143][0m Total time:       3.27 min
[32m[20221213 23:15:48 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221213 23:15:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2151 --------------------------#
[32m[20221213 23:15:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |           0.0015 |          42.4074 |          15.3431 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |           0.0147 |          45.5161 |          15.2917 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0072 |          38.2140 |          15.2683 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0063 |          37.9009 |          15.2619 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0109 |          37.0644 |          15.2501 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0089 |          36.9090 |          15.2436 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0109 |          36.5224 |          15.2259 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0086 |          36.2487 |          15.1953 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0123 |          36.1100 |          15.1893 |
[32m[20221213 23:15:49 @agent_ppo2.py:185][0m |          -0.0182 |          35.7861 |          15.1767 |
[32m[20221213 23:15:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:15:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.12
[32m[20221213 23:15:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.19
[32m[20221213 23:15:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.10
[32m[20221213 23:15:49 @agent_ppo2.py:143][0m Total time:       3.29 min
[32m[20221213 23:15:49 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 23:15:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2152 --------------------------#
[32m[20221213 23:15:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |          -0.0019 |          42.9792 |          15.4589 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |          -0.0072 |          41.5798 |          15.4580 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |          -0.0064 |          41.0920 |          15.4396 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |          -0.0053 |          40.8504 |          15.4416 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |          -0.0056 |          40.7854 |          15.4304 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |          -0.0079 |          40.4741 |          15.4251 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |           0.0033 |          44.1109 |          15.4234 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |           0.0077 |          44.3580 |          15.4498 |
[32m[20221213 23:15:50 @agent_ppo2.py:185][0m |          -0.0100 |          40.1439 |          15.4204 |
[32m[20221213 23:15:51 @agent_ppo2.py:185][0m |          -0.0107 |          40.2543 |          15.4249 |
[32m[20221213 23:15:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:15:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.11
[32m[20221213 23:15:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.90
[32m[20221213 23:15:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.58
[32m[20221213 23:15:51 @agent_ppo2.py:143][0m Total time:       3.31 min
[32m[20221213 23:15:51 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221213 23:15:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2153 --------------------------#
[32m[20221213 23:15:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:51 @agent_ppo2.py:185][0m |          -0.0012 |          36.4013 |          15.5010 |
[32m[20221213 23:15:51 @agent_ppo2.py:185][0m |          -0.0095 |          31.2254 |          15.5353 |
[32m[20221213 23:15:51 @agent_ppo2.py:185][0m |          -0.0051 |          29.6181 |          15.5126 |
[32m[20221213 23:15:51 @agent_ppo2.py:185][0m |          -0.0119 |          28.3607 |          15.5150 |
[32m[20221213 23:15:51 @agent_ppo2.py:185][0m |          -0.0092 |          27.9415 |          15.5226 |
[32m[20221213 23:15:52 @agent_ppo2.py:185][0m |          -0.0142 |          27.4792 |          15.5344 |
[32m[20221213 23:15:52 @agent_ppo2.py:185][0m |          -0.0173 |          27.0043 |          15.5431 |
[32m[20221213 23:15:52 @agent_ppo2.py:185][0m |          -0.0143 |          26.5521 |          15.5402 |
[32m[20221213 23:15:52 @agent_ppo2.py:185][0m |          -0.0187 |          26.2388 |          15.5558 |
[32m[20221213 23:15:52 @agent_ppo2.py:185][0m |          -0.0125 |          26.1096 |          15.5426 |
[32m[20221213 23:15:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:15:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.88
[32m[20221213 23:15:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.65
[32m[20221213 23:15:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.85
[32m[20221213 23:15:52 @agent_ppo2.py:143][0m Total time:       3.33 min
[32m[20221213 23:15:52 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 23:15:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2154 --------------------------#
[32m[20221213 23:15:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:52 @agent_ppo2.py:185][0m |           0.0089 |          43.5326 |          15.4337 |
[32m[20221213 23:15:52 @agent_ppo2.py:185][0m |          -0.0079 |          39.1111 |          15.4234 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0066 |          38.0589 |          15.4705 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0084 |          37.4159 |          15.4670 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0081 |          37.0524 |          15.4802 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0138 |          36.4878 |          15.4992 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0115 |          35.9431 |          15.5152 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0174 |          35.7076 |          15.5105 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0146 |          35.2822 |          15.5367 |
[32m[20221213 23:15:53 @agent_ppo2.py:185][0m |          -0.0144 |          35.2124 |          15.5255 |
[32m[20221213 23:15:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:15:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.83
[32m[20221213 23:15:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.19
[32m[20221213 23:15:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.74
[32m[20221213 23:15:53 @agent_ppo2.py:143][0m Total time:       3.36 min
[32m[20221213 23:15:53 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221213 23:15:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2155 --------------------------#
[32m[20221213 23:15:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |           0.0009 |          50.1120 |          15.2367 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0045 |          48.9753 |          15.2509 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0029 |          48.6550 |          15.2563 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |           0.0045 |          53.7289 |          15.2424 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0074 |          48.3813 |          15.2473 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0086 |          48.1967 |          15.2458 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0049 |          48.0360 |          15.2701 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0073 |          47.9909 |          15.2570 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0056 |          47.9101 |          15.2931 |
[32m[20221213 23:15:54 @agent_ppo2.py:185][0m |          -0.0029 |          48.1660 |          15.2871 |
[32m[20221213 23:15:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:15:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.67
[32m[20221213 23:15:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.61
[32m[20221213 23:15:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.18
[32m[20221213 23:15:55 @agent_ppo2.py:143][0m Total time:       3.38 min
[32m[20221213 23:15:55 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 23:15:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2156 --------------------------#
[32m[20221213 23:15:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:15:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:55 @agent_ppo2.py:185][0m |           0.0051 |          46.4872 |          15.5359 |
[32m[20221213 23:15:55 @agent_ppo2.py:185][0m |          -0.0057 |          43.7685 |          15.5248 |
[32m[20221213 23:15:55 @agent_ppo2.py:185][0m |          -0.0025 |          43.6677 |          15.5575 |
[32m[20221213 23:15:55 @agent_ppo2.py:185][0m |          -0.0081 |          42.3252 |          15.5522 |
[32m[20221213 23:15:55 @agent_ppo2.py:185][0m |          -0.0066 |          42.0612 |          15.5699 |
[32m[20221213 23:15:55 @agent_ppo2.py:185][0m |          -0.0106 |          41.7456 |          15.5621 |
[32m[20221213 23:15:56 @agent_ppo2.py:185][0m |          -0.0097 |          41.2798 |          15.5780 |
[32m[20221213 23:15:56 @agent_ppo2.py:185][0m |          -0.0110 |          41.2090 |          15.5811 |
[32m[20221213 23:15:56 @agent_ppo2.py:185][0m |          -0.0123 |          41.2704 |          15.5790 |
[32m[20221213 23:15:56 @agent_ppo2.py:185][0m |          -0.0104 |          41.0933 |          15.5830 |
[32m[20221213 23:15:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:15:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.10
[32m[20221213 23:15:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.42
[32m[20221213 23:15:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.85
[32m[20221213 23:15:56 @agent_ppo2.py:143][0m Total time:       3.40 min
[32m[20221213 23:15:56 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221213 23:15:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2157 --------------------------#
[32m[20221213 23:15:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:56 @agent_ppo2.py:185][0m |           0.0092 |          51.5897 |          15.6278 |
[32m[20221213 23:15:56 @agent_ppo2.py:185][0m |          -0.0050 |          46.4783 |          15.6034 |
[32m[20221213 23:15:56 @agent_ppo2.py:185][0m |          -0.0068 |          45.8541 |          15.5999 |
[32m[20221213 23:15:57 @agent_ppo2.py:185][0m |          -0.0026 |          46.0656 |          15.6138 |
[32m[20221213 23:15:57 @agent_ppo2.py:185][0m |          -0.0086 |          45.3566 |          15.6230 |
[32m[20221213 23:15:57 @agent_ppo2.py:185][0m |          -0.0102 |          45.1217 |          15.6254 |
[32m[20221213 23:15:57 @agent_ppo2.py:185][0m |          -0.0110 |          45.0106 |          15.6275 |
[32m[20221213 23:15:57 @agent_ppo2.py:185][0m |          -0.0078 |          44.9055 |          15.6441 |
[32m[20221213 23:15:57 @agent_ppo2.py:185][0m |          -0.0148 |          44.7819 |          15.6180 |
[32m[20221213 23:15:57 @agent_ppo2.py:185][0m |          -0.0109 |          44.5750 |          15.6373 |
[32m[20221213 23:15:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:15:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.06
[32m[20221213 23:15:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.86
[32m[20221213 23:15:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.43
[32m[20221213 23:15:57 @agent_ppo2.py:143][0m Total time:       3.42 min
[32m[20221213 23:15:57 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 23:15:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2158 --------------------------#
[32m[20221213 23:15:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |           0.0001 |          44.2946 |          15.5809 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |          -0.0027 |          43.2534 |          15.5816 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |          -0.0072 |          42.9335 |          15.5967 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |          -0.0047 |          42.7755 |          15.5986 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |           0.0022 |          44.9376 |          15.5956 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |          -0.0030 |          43.1093 |          15.6049 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |          -0.0091 |          42.4483 |          15.5960 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |           0.0025 |          47.7143 |          15.6069 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |          -0.0087 |          42.3725 |          15.6038 |
[32m[20221213 23:15:58 @agent_ppo2.py:185][0m |          -0.0043 |          42.6864 |          15.6215 |
[32m[20221213 23:15:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:15:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.54
[32m[20221213 23:15:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.80
[32m[20221213 23:15:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.03
[32m[20221213 23:15:59 @agent_ppo2.py:143][0m Total time:       3.44 min
[32m[20221213 23:15:59 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221213 23:15:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2159 --------------------------#
[32m[20221213 23:15:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:15:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |          -0.0004 |          44.9569 |          15.5804 |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |          -0.0044 |          44.2768 |          15.5749 |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |           0.0158 |          49.3243 |          15.5352 |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |          -0.0026 |          43.9100 |          15.5495 |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |          -0.0073 |          43.4977 |          15.5642 |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |          -0.0088 |          43.3747 |          15.5354 |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |          -0.0086 |          43.3891 |          15.5478 |
[32m[20221213 23:15:59 @agent_ppo2.py:185][0m |          -0.0033 |          44.3980 |          15.5272 |
[32m[20221213 23:16:00 @agent_ppo2.py:185][0m |          -0.0112 |          43.2028 |          15.5076 |
[32m[20221213 23:16:00 @agent_ppo2.py:185][0m |          -0.0108 |          43.1342 |          15.5147 |
[32m[20221213 23:16:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.81
[32m[20221213 23:16:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.44
[32m[20221213 23:16:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.84
[32m[20221213 23:16:00 @agent_ppo2.py:143][0m Total time:       3.46 min
[32m[20221213 23:16:00 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 23:16:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2160 --------------------------#
[32m[20221213 23:16:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:16:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:00 @agent_ppo2.py:185][0m |          -0.0051 |          49.2831 |          15.6780 |
[32m[20221213 23:16:00 @agent_ppo2.py:185][0m |          -0.0052 |          48.1771 |          15.6668 |
[32m[20221213 23:16:00 @agent_ppo2.py:185][0m |          -0.0071 |          47.8103 |          15.6864 |
[32m[20221213 23:16:00 @agent_ppo2.py:185][0m |          -0.0087 |          47.7219 |          15.6696 |
[32m[20221213 23:16:00 @agent_ppo2.py:185][0m |          -0.0082 |          47.5401 |          15.6932 |
[32m[20221213 23:16:01 @agent_ppo2.py:185][0m |          -0.0082 |          47.3150 |          15.6894 |
[32m[20221213 23:16:01 @agent_ppo2.py:185][0m |          -0.0089 |          47.1665 |          15.6885 |
[32m[20221213 23:16:01 @agent_ppo2.py:185][0m |          -0.0094 |          47.0238 |          15.6851 |
[32m[20221213 23:16:01 @agent_ppo2.py:185][0m |          -0.0109 |          47.0940 |          15.6919 |
[32m[20221213 23:16:01 @agent_ppo2.py:185][0m |          -0.0074 |          47.5304 |          15.7054 |
[32m[20221213 23:16:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.44
[32m[20221213 23:16:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.08
[32m[20221213 23:16:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.64
[32m[20221213 23:16:01 @agent_ppo2.py:143][0m Total time:       3.48 min
[32m[20221213 23:16:01 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221213 23:16:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2161 --------------------------#
[32m[20221213 23:16:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:01 @agent_ppo2.py:185][0m |          -0.0028 |          49.4812 |          15.6675 |
[32m[20221213 23:16:01 @agent_ppo2.py:185][0m |           0.0002 |          49.1095 |          15.6548 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0069 |          48.4661 |          15.6523 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0083 |          48.3373 |          15.6733 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0091 |          48.1454 |          15.6622 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0082 |          47.9851 |          15.6716 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0079 |          48.2121 |          15.6727 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0108 |          47.7911 |          15.6762 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0067 |          48.1472 |          15.6927 |
[32m[20221213 23:16:02 @agent_ppo2.py:185][0m |          -0.0139 |          47.6650 |          15.6683 |
[32m[20221213 23:16:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.32
[32m[20221213 23:16:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.91
[32m[20221213 23:16:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.61
[32m[20221213 23:16:02 @agent_ppo2.py:143][0m Total time:       3.50 min
[32m[20221213 23:16:02 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 23:16:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2162 --------------------------#
[32m[20221213 23:16:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0024 |          46.2167 |          15.6423 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0038 |          44.4832 |          15.6478 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0045 |          43.9130 |          15.6466 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0073 |          43.7182 |          15.6368 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0058 |          43.4807 |          15.6422 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0063 |          43.3361 |          15.6260 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0021 |          44.0886 |          15.6506 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0122 |          43.0095 |          15.6333 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0105 |          42.8984 |          15.6448 |
[32m[20221213 23:16:03 @agent_ppo2.py:185][0m |          -0.0095 |          42.6967 |          15.6595 |
[32m[20221213 23:16:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:16:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.15
[32m[20221213 23:16:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.86
[32m[20221213 23:16:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.84
[32m[20221213 23:16:04 @agent_ppo2.py:143][0m Total time:       3.53 min
[32m[20221213 23:16:04 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221213 23:16:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2163 --------------------------#
[32m[20221213 23:16:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:04 @agent_ppo2.py:185][0m |           0.0125 |          33.1710 |          15.8034 |
[32m[20221213 23:16:04 @agent_ppo2.py:185][0m |          -0.0033 |          28.3354 |          15.7917 |
[32m[20221213 23:16:04 @agent_ppo2.py:185][0m |          -0.0018 |          26.9955 |          15.7927 |
[32m[20221213 23:16:04 @agent_ppo2.py:185][0m |          -0.0105 |          26.2200 |          15.8001 |
[32m[20221213 23:16:04 @agent_ppo2.py:185][0m |          -0.0017 |          25.5977 |          15.7934 |
[32m[20221213 23:16:04 @agent_ppo2.py:185][0m |          -0.0084 |          25.2261 |          15.7938 |
[32m[20221213 23:16:04 @agent_ppo2.py:185][0m |          -0.0107 |          24.7992 |          15.8022 |
[32m[20221213 23:16:05 @agent_ppo2.py:185][0m |          -0.0094 |          24.6003 |          15.7821 |
[32m[20221213 23:16:05 @agent_ppo2.py:185][0m |           0.0149 |          30.9914 |          15.7832 |
[32m[20221213 23:16:05 @agent_ppo2.py:185][0m |          -0.0074 |          24.5447 |          15.7732 |
[32m[20221213 23:16:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.27
[32m[20221213 23:16:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.40
[32m[20221213 23:16:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.82
[32m[20221213 23:16:05 @agent_ppo2.py:143][0m Total time:       3.55 min
[32m[20221213 23:16:05 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 23:16:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2164 --------------------------#
[32m[20221213 23:16:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:05 @agent_ppo2.py:185][0m |           0.0070 |          46.7267 |          15.7221 |
[32m[20221213 23:16:05 @agent_ppo2.py:185][0m |          -0.0031 |          43.9208 |          15.6944 |
[32m[20221213 23:16:05 @agent_ppo2.py:185][0m |          -0.0092 |          43.4855 |          15.7049 |
[32m[20221213 23:16:05 @agent_ppo2.py:185][0m |          -0.0104 |          42.9185 |          15.6887 |
[32m[20221213 23:16:06 @agent_ppo2.py:185][0m |          -0.0006 |          46.8481 |          15.6831 |
[32m[20221213 23:16:06 @agent_ppo2.py:185][0m |          -0.0100 |          42.6291 |          15.6641 |
[32m[20221213 23:16:06 @agent_ppo2.py:185][0m |          -0.0084 |          42.7587 |          15.6769 |
[32m[20221213 23:16:06 @agent_ppo2.py:185][0m |          -0.0125 |          42.5700 |          15.6729 |
[32m[20221213 23:16:06 @agent_ppo2.py:185][0m |          -0.0107 |          42.3737 |          15.6686 |
[32m[20221213 23:16:06 @agent_ppo2.py:185][0m |          -0.0158 |          41.7071 |          15.6538 |
[32m[20221213 23:16:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.61
[32m[20221213 23:16:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.44
[32m[20221213 23:16:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.88
[32m[20221213 23:16:06 @agent_ppo2.py:143][0m Total time:       3.57 min
[32m[20221213 23:16:06 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221213 23:16:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2165 --------------------------#
[32m[20221213 23:16:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:06 @agent_ppo2.py:185][0m |           0.0034 |          52.3788 |          15.6416 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |           0.0001 |          48.6658 |          15.6181 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0062 |          47.5146 |          15.6152 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0079 |          46.5982 |          15.6160 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0091 |          45.9680 |          15.6126 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0093 |          45.2989 |          15.5943 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0092 |          44.8916 |          15.5981 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0081 |          44.5500 |          15.5869 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0121 |          44.0898 |          15.5712 |
[32m[20221213 23:16:07 @agent_ppo2.py:185][0m |          -0.0123 |          44.0935 |          15.5704 |
[32m[20221213 23:16:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.73
[32m[20221213 23:16:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.21
[32m[20221213 23:16:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 203.77
[32m[20221213 23:16:07 @agent_ppo2.py:143][0m Total time:       3.59 min
[32m[20221213 23:16:07 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 23:16:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2166 --------------------------#
[32m[20221213 23:16:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |           0.0191 |          66.1740 |          15.4902 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0034 |          53.0526 |          15.4747 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0091 |          50.7780 |          15.4920 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0055 |          49.8051 |          15.4781 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0090 |          49.2727 |          15.4848 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0101 |          48.7580 |          15.4698 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0028 |          48.9905 |          15.4667 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0097 |          48.1517 |          15.4534 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0126 |          47.7470 |          15.4628 |
[32m[20221213 23:16:08 @agent_ppo2.py:185][0m |          -0.0128 |          47.4926 |          15.4448 |
[32m[20221213 23:16:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.38
[32m[20221213 23:16:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.33
[32m[20221213 23:16:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.74
[32m[20221213 23:16:09 @agent_ppo2.py:143][0m Total time:       3.61 min
[32m[20221213 23:16:09 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221213 23:16:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2167 --------------------------#
[32m[20221213 23:16:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:09 @agent_ppo2.py:185][0m |          -0.0026 |          48.9277 |          15.6448 |
[32m[20221213 23:16:09 @agent_ppo2.py:185][0m |          -0.0025 |          47.3962 |          15.6542 |
[32m[20221213 23:16:09 @agent_ppo2.py:185][0m |          -0.0014 |          46.9715 |          15.6433 |
[32m[20221213 23:16:09 @agent_ppo2.py:185][0m |          -0.0041 |          46.7478 |          15.6397 |
[32m[20221213 23:16:09 @agent_ppo2.py:185][0m |          -0.0059 |          46.6164 |          15.6515 |
[32m[20221213 23:16:09 @agent_ppo2.py:185][0m |          -0.0054 |          46.3742 |          15.6452 |
[32m[20221213 23:16:09 @agent_ppo2.py:185][0m |          -0.0036 |          46.2301 |          15.6421 |
[32m[20221213 23:16:10 @agent_ppo2.py:185][0m |          -0.0086 |          46.2461 |          15.6369 |
[32m[20221213 23:16:10 @agent_ppo2.py:185][0m |          -0.0063 |          46.2283 |          15.6374 |
[32m[20221213 23:16:10 @agent_ppo2.py:185][0m |           0.0037 |          48.3892 |          15.6568 |
[32m[20221213 23:16:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:16:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.41
[32m[20221213 23:16:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.29
[32m[20221213 23:16:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.21
[32m[20221213 23:16:10 @agent_ppo2.py:143][0m Total time:       3.63 min
[32m[20221213 23:16:10 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 23:16:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2168 --------------------------#
[32m[20221213 23:16:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:10 @agent_ppo2.py:185][0m |          -0.0002 |          46.1632 |          15.6292 |
[32m[20221213 23:16:10 @agent_ppo2.py:185][0m |          -0.0024 |          45.3975 |          15.6223 |
[32m[20221213 23:16:10 @agent_ppo2.py:185][0m |          -0.0003 |          45.9303 |          15.6288 |
[32m[20221213 23:16:10 @agent_ppo2.py:185][0m |          -0.0043 |          44.9568 |          15.6381 |
[32m[20221213 23:16:11 @agent_ppo2.py:185][0m |          -0.0078 |          44.6905 |          15.6080 |
[32m[20221213 23:16:11 @agent_ppo2.py:185][0m |          -0.0069 |          44.7951 |          15.6300 |
[32m[20221213 23:16:11 @agent_ppo2.py:185][0m |          -0.0097 |          44.5934 |          15.6294 |
[32m[20221213 23:16:11 @agent_ppo2.py:185][0m |          -0.0079 |          44.4893 |          15.6147 |
[32m[20221213 23:16:11 @agent_ppo2.py:185][0m |          -0.0074 |          44.4580 |          15.6129 |
[32m[20221213 23:16:11 @agent_ppo2.py:185][0m |          -0.0043 |          44.3938 |          15.6118 |
[32m[20221213 23:16:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.60
[32m[20221213 23:16:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.65
[32m[20221213 23:16:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.92
[32m[20221213 23:16:11 @agent_ppo2.py:143][0m Total time:       3.65 min
[32m[20221213 23:16:11 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221213 23:16:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2169 --------------------------#
[32m[20221213 23:16:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:11 @agent_ppo2.py:185][0m |           0.0022 |          48.1402 |          15.8708 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0028 |          46.5694 |          15.8840 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0055 |          46.0183 |          15.8625 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0060 |          45.6504 |          15.8471 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0053 |          45.5250 |          15.8658 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0011 |          46.5349 |          15.8469 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0069 |          45.2903 |          15.8362 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0061 |          45.4458 |          15.8477 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0048 |          45.1624 |          15.8518 |
[32m[20221213 23:16:12 @agent_ppo2.py:185][0m |          -0.0104 |          45.0050 |          15.8604 |
[32m[20221213 23:16:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.20
[32m[20221213 23:16:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.68
[32m[20221213 23:16:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.69
[32m[20221213 23:16:12 @agent_ppo2.py:143][0m Total time:       3.67 min
[32m[20221213 23:16:12 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 23:16:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2170 --------------------------#
[32m[20221213 23:16:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:16:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0004 |          54.4569 |          15.5659 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0026 |          52.9194 |          15.5810 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0004 |          52.8627 |          15.5746 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0069 |          52.1406 |          15.5658 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0068 |          52.3768 |          15.5681 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0097 |          51.7645 |          15.5388 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0078 |          52.0672 |          15.5435 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0096 |          51.5134 |          15.5301 |
[32m[20221213 23:16:13 @agent_ppo2.py:185][0m |          -0.0105 |          51.3501 |          15.5463 |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |          -0.0129 |          51.1842 |          15.5536 |
[32m[20221213 23:16:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.75
[32m[20221213 23:16:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.51
[32m[20221213 23:16:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.84
[32m[20221213 23:16:14 @agent_ppo2.py:143][0m Total time:       3.69 min
[32m[20221213 23:16:14 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221213 23:16:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2171 --------------------------#
[32m[20221213 23:16:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |           0.0020 |          57.1217 |          15.3775 |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |          -0.0037 |          52.9685 |          15.3553 |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |          -0.0037 |          51.5896 |          15.3505 |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |          -0.0007 |          51.7111 |          15.3311 |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |          -0.0084 |          50.7491 |          15.2964 |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |          -0.0060 |          50.3263 |          15.3017 |
[32m[20221213 23:16:14 @agent_ppo2.py:185][0m |          -0.0103 |          49.9447 |          15.2894 |
[32m[20221213 23:16:15 @agent_ppo2.py:185][0m |          -0.0118 |          49.6582 |          15.2975 |
[32m[20221213 23:16:15 @agent_ppo2.py:185][0m |          -0.0118 |          49.5656 |          15.2816 |
[32m[20221213 23:16:15 @agent_ppo2.py:185][0m |          -0.0145 |          49.5783 |          15.2666 |
[32m[20221213 23:16:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.52
[32m[20221213 23:16:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.85
[32m[20221213 23:16:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.39
[32m[20221213 23:16:15 @agent_ppo2.py:143][0m Total time:       3.71 min
[32m[20221213 23:16:15 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 23:16:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2172 --------------------------#
[32m[20221213 23:16:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:15 @agent_ppo2.py:185][0m |          -0.0026 |          44.0275 |          15.5006 |
[32m[20221213 23:16:15 @agent_ppo2.py:185][0m |          -0.0040 |          42.9771 |          15.5077 |
[32m[20221213 23:16:15 @agent_ppo2.py:185][0m |           0.0013 |          43.2572 |          15.4945 |
[32m[20221213 23:16:16 @agent_ppo2.py:185][0m |          -0.0024 |          42.2681 |          15.5208 |
[32m[20221213 23:16:16 @agent_ppo2.py:185][0m |          -0.0058 |          42.0859 |          15.4908 |
[32m[20221213 23:16:16 @agent_ppo2.py:185][0m |          -0.0048 |          42.0049 |          15.4977 |
[32m[20221213 23:16:16 @agent_ppo2.py:185][0m |          -0.0076 |          41.8638 |          15.4946 |
[32m[20221213 23:16:16 @agent_ppo2.py:185][0m |          -0.0079 |          41.7958 |          15.5210 |
[32m[20221213 23:16:16 @agent_ppo2.py:185][0m |          -0.0091 |          41.7289 |          15.5207 |
[32m[20221213 23:16:16 @agent_ppo2.py:185][0m |          -0.0098 |          41.6695 |          15.5286 |
[32m[20221213 23:16:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:16:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.23
[32m[20221213 23:16:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.72
[32m[20221213 23:16:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.18
[32m[20221213 23:16:16 @agent_ppo2.py:143][0m Total time:       3.74 min
[32m[20221213 23:16:16 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221213 23:16:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2173 --------------------------#
[32m[20221213 23:16:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |           0.0040 |          34.9090 |          15.6502 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0054 |          32.0104 |          15.6422 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0102 |          31.3482 |          15.6334 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0091 |          30.7935 |          15.6366 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0073 |          32.0720 |          15.6393 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0139 |          30.1187 |          15.6404 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0176 |          29.9341 |          15.6466 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0139 |          29.7088 |          15.6433 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0163 |          29.6669 |          15.6365 |
[32m[20221213 23:16:17 @agent_ppo2.py:185][0m |          -0.0116 |          29.5492 |          15.6269 |
[32m[20221213 23:16:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.51
[32m[20221213 23:16:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.99
[32m[20221213 23:16:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.52
[32m[20221213 23:16:17 @agent_ppo2.py:143][0m Total time:       3.76 min
[32m[20221213 23:16:17 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 23:16:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2174 --------------------------#
[32m[20221213 23:16:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |           0.0028 |          47.7778 |          15.6208 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |           0.0063 |          50.3016 |          15.5957 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |          -0.0056 |          46.7166 |          15.5716 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |          -0.0071 |          46.2555 |          15.5730 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |          -0.0088 |          45.9976 |          15.5739 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |          -0.0081 |          45.9151 |          15.5704 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |          -0.0100 |          45.8652 |          15.5606 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |          -0.0113 |          45.6704 |          15.5624 |
[32m[20221213 23:16:18 @agent_ppo2.py:185][0m |          -0.0096 |          45.6207 |          15.5671 |
[32m[20221213 23:16:19 @agent_ppo2.py:185][0m |          -0.0126 |          45.5902 |          15.5714 |
[32m[20221213 23:16:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.05
[32m[20221213 23:16:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.61
[32m[20221213 23:16:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:16:19 @agent_ppo2.py:143][0m Total time:       3.78 min
[32m[20221213 23:16:19 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221213 23:16:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2175 --------------------------#
[32m[20221213 23:16:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:19 @agent_ppo2.py:185][0m |           0.0010 |          52.4361 |          15.5591 |
[32m[20221213 23:16:19 @agent_ppo2.py:185][0m |           0.0102 |          57.2524 |          15.5632 |
[32m[20221213 23:16:19 @agent_ppo2.py:185][0m |          -0.0010 |          51.6401 |          15.5343 |
[32m[20221213 23:16:19 @agent_ppo2.py:185][0m |          -0.0078 |          51.1600 |          15.5698 |
[32m[20221213 23:16:19 @agent_ppo2.py:185][0m |          -0.0060 |          51.0696 |          15.5913 |
[32m[20221213 23:16:19 @agent_ppo2.py:185][0m |          -0.0020 |          50.9805 |          15.5887 |
[32m[20221213 23:16:20 @agent_ppo2.py:185][0m |          -0.0067 |          50.6180 |          15.5719 |
[32m[20221213 23:16:20 @agent_ppo2.py:185][0m |          -0.0077 |          50.6870 |          15.5724 |
[32m[20221213 23:16:20 @agent_ppo2.py:185][0m |          -0.0083 |          50.4834 |          15.5691 |
[32m[20221213 23:16:20 @agent_ppo2.py:185][0m |          -0.0071 |          50.3723 |          15.5968 |
[32m[20221213 23:16:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.56
[32m[20221213 23:16:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.43
[32m[20221213 23:16:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.56
[32m[20221213 23:16:20 @agent_ppo2.py:143][0m Total time:       3.80 min
[32m[20221213 23:16:20 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 23:16:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2176 --------------------------#
[32m[20221213 23:16:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:20 @agent_ppo2.py:185][0m |           0.0023 |          42.8094 |          15.7061 |
[32m[20221213 23:16:20 @agent_ppo2.py:185][0m |          -0.0019 |          39.3397 |          15.6979 |
[32m[20221213 23:16:20 @agent_ppo2.py:185][0m |          -0.0048 |          37.9716 |          15.6963 |
[32m[20221213 23:16:21 @agent_ppo2.py:185][0m |           0.0048 |          41.2143 |          15.6883 |
[32m[20221213 23:16:21 @agent_ppo2.py:185][0m |          -0.0079 |          36.8627 |          15.6598 |
[32m[20221213 23:16:21 @agent_ppo2.py:185][0m |          -0.0091 |          36.4455 |          15.6759 |
[32m[20221213 23:16:21 @agent_ppo2.py:185][0m |          -0.0115 |          35.8328 |          15.6888 |
[32m[20221213 23:16:21 @agent_ppo2.py:185][0m |           0.0003 |          36.9374 |          15.6807 |
[32m[20221213 23:16:21 @agent_ppo2.py:185][0m |          -0.0094 |          35.2296 |          15.6925 |
[32m[20221213 23:16:21 @agent_ppo2.py:185][0m |          -0.0055 |          34.8551 |          15.6794 |
[32m[20221213 23:16:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:16:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.11
[32m[20221213 23:16:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.62
[32m[20221213 23:16:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.18
[32m[20221213 23:16:21 @agent_ppo2.py:143][0m Total time:       3.82 min
[32m[20221213 23:16:21 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221213 23:16:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2177 --------------------------#
[32m[20221213 23:16:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0043 |          51.1541 |          15.6421 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0044 |          48.3257 |          15.6152 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0062 |          47.0697 |          15.6135 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0021 |          48.8559 |          15.6429 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0032 |          47.6490 |          15.6062 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0049 |          45.5801 |          15.6128 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0130 |          45.2806 |          15.6178 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0112 |          45.0714 |          15.6085 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0149 |          44.8299 |          15.6168 |
[32m[20221213 23:16:22 @agent_ppo2.py:185][0m |          -0.0144 |          44.6972 |          15.6156 |
[32m[20221213 23:16:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:16:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.94
[32m[20221213 23:16:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.94
[32m[20221213 23:16:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.47
[32m[20221213 23:16:22 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221213 23:16:22 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 23:16:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2178 --------------------------#
[32m[20221213 23:16:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |           0.0035 |          57.5944 |          15.4774 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0027 |          56.6511 |          15.4646 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0020 |          56.7262 |          15.4369 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0061 |          56.1039 |          15.4678 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0062 |          55.7324 |          15.4479 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0075 |          55.7610 |          15.4382 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0067 |          55.6196 |          15.4184 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0091 |          55.5250 |          15.4295 |
[32m[20221213 23:16:23 @agent_ppo2.py:185][0m |          -0.0111 |          55.5224 |          15.4275 |
[32m[20221213 23:16:24 @agent_ppo2.py:185][0m |          -0.0112 |          55.3351 |          15.4318 |
[32m[20221213 23:16:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.70
[32m[20221213 23:16:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.22
[32m[20221213 23:16:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.54
[32m[20221213 23:16:24 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221213 23:16:24 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221213 23:16:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2179 --------------------------#
[32m[20221213 23:16:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:24 @agent_ppo2.py:185][0m |           0.0009 |          57.1913 |          15.5296 |
[32m[20221213 23:16:24 @agent_ppo2.py:185][0m |           0.0000 |          53.2898 |          15.5175 |
[32m[20221213 23:16:24 @agent_ppo2.py:185][0m |          -0.0045 |          50.7836 |          15.5155 |
[32m[20221213 23:16:24 @agent_ppo2.py:185][0m |          -0.0066 |          49.9995 |          15.5220 |
[32m[20221213 23:16:24 @agent_ppo2.py:185][0m |          -0.0007 |          50.5813 |          15.5193 |
[32m[20221213 23:16:24 @agent_ppo2.py:185][0m |          -0.0111 |          49.4096 |          15.5111 |
[32m[20221213 23:16:25 @agent_ppo2.py:185][0m |          -0.0136 |          49.1858 |          15.5147 |
[32m[20221213 23:16:25 @agent_ppo2.py:185][0m |          -0.0119 |          49.0243 |          15.5320 |
[32m[20221213 23:16:25 @agent_ppo2.py:185][0m |          -0.0044 |          50.2498 |          15.5259 |
[32m[20221213 23:16:25 @agent_ppo2.py:185][0m |          -0.0095 |          48.5554 |          15.5392 |
[32m[20221213 23:16:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.39
[32m[20221213 23:16:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.52
[32m[20221213 23:16:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.22
[32m[20221213 23:16:25 @agent_ppo2.py:143][0m Total time:       3.88 min
[32m[20221213 23:16:25 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 23:16:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2180 --------------------------#
[32m[20221213 23:16:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:16:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:25 @agent_ppo2.py:185][0m |           0.0045 |          52.7662 |          15.7469 |
[32m[20221213 23:16:25 @agent_ppo2.py:185][0m |          -0.0024 |          51.5891 |          15.7550 |
[32m[20221213 23:16:25 @agent_ppo2.py:185][0m |          -0.0023 |          51.6490 |          15.7392 |
[32m[20221213 23:16:26 @agent_ppo2.py:185][0m |          -0.0056 |          51.1236 |          15.7362 |
[32m[20221213 23:16:26 @agent_ppo2.py:185][0m |          -0.0083 |          51.0122 |          15.7509 |
[32m[20221213 23:16:26 @agent_ppo2.py:185][0m |          -0.0064 |          50.8716 |          15.7430 |
[32m[20221213 23:16:26 @agent_ppo2.py:185][0m |          -0.0087 |          50.7862 |          15.7444 |
[32m[20221213 23:16:26 @agent_ppo2.py:185][0m |          -0.0060 |          50.8045 |          15.7594 |
[32m[20221213 23:16:26 @agent_ppo2.py:185][0m |          -0.0086 |          50.7794 |          15.7354 |
[32m[20221213 23:16:26 @agent_ppo2.py:185][0m |          -0.0080 |          50.6649 |          15.7465 |
[32m[20221213 23:16:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.02
[32m[20221213 23:16:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.21
[32m[20221213 23:16:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.00
[32m[20221213 23:16:26 @agent_ppo2.py:143][0m Total time:       3.90 min
[32m[20221213 23:16:26 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221213 23:16:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2181 --------------------------#
[32m[20221213 23:16:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0012 |          55.2811 |          15.6897 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0066 |          48.1052 |          15.6809 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0088 |          46.8410 |          15.6629 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0094 |          45.9674 |          15.6657 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0104 |          45.4539 |          15.6453 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0114 |          44.8092 |          15.6399 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0126 |          44.8077 |          15.6338 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0134 |          44.3624 |          15.6399 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0131 |          44.4193 |          15.6430 |
[32m[20221213 23:16:27 @agent_ppo2.py:185][0m |          -0.0124 |          44.0554 |          15.6342 |
[32m[20221213 23:16:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:16:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.87
[32m[20221213 23:16:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.86
[32m[20221213 23:16:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.69
[32m[20221213 23:16:28 @agent_ppo2.py:143][0m Total time:       3.92 min
[32m[20221213 23:16:28 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 23:16:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2182 --------------------------#
[32m[20221213 23:16:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0022 |          52.0626 |          15.6940 |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0056 |          51.2489 |          15.6358 |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0006 |          52.5864 |          15.6399 |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0053 |          50.7634 |          15.6029 |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0078 |          50.7055 |          15.6013 |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0080 |          50.5617 |          15.6030 |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0069 |          50.7229 |          15.5900 |
[32m[20221213 23:16:28 @agent_ppo2.py:185][0m |          -0.0091 |          50.3295 |          15.5857 |
[32m[20221213 23:16:29 @agent_ppo2.py:185][0m |          -0.0067 |          50.3130 |          15.5638 |
[32m[20221213 23:16:29 @agent_ppo2.py:185][0m |          -0.0079 |          50.3967 |          15.5629 |
[32m[20221213 23:16:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.13
[32m[20221213 23:16:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.83
[32m[20221213 23:16:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.11
[32m[20221213 23:16:29 @agent_ppo2.py:143][0m Total time:       3.95 min
[32m[20221213 23:16:29 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221213 23:16:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2183 --------------------------#
[32m[20221213 23:16:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:29 @agent_ppo2.py:185][0m |          -0.0003 |          39.1726 |          15.5524 |
[32m[20221213 23:16:29 @agent_ppo2.py:185][0m |          -0.0068 |          33.2592 |          15.5428 |
[32m[20221213 23:16:29 @agent_ppo2.py:185][0m |          -0.0066 |          32.4389 |          15.5589 |
[32m[20221213 23:16:29 @agent_ppo2.py:185][0m |          -0.0179 |          31.5845 |          15.5409 |
[32m[20221213 23:16:29 @agent_ppo2.py:185][0m |          -0.0102 |          31.2014 |          15.5455 |
[32m[20221213 23:16:30 @agent_ppo2.py:185][0m |          -0.0076 |          31.5412 |          15.5622 |
[32m[20221213 23:16:30 @agent_ppo2.py:185][0m |          -0.0001 |          35.3051 |          15.5604 |
[32m[20221213 23:16:30 @agent_ppo2.py:185][0m |          -0.0064 |          31.2303 |          15.5558 |
[32m[20221213 23:16:30 @agent_ppo2.py:185][0m |          -0.0011 |          30.7381 |          15.5661 |
[32m[20221213 23:16:30 @agent_ppo2.py:185][0m |          -0.0130 |          30.1213 |          15.5607 |
[32m[20221213 23:16:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.85
[32m[20221213 23:16:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.97
[32m[20221213 23:16:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.51
[32m[20221213 23:16:30 @agent_ppo2.py:143][0m Total time:       3.97 min
[32m[20221213 23:16:30 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 23:16:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2184 --------------------------#
[32m[20221213 23:16:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:30 @agent_ppo2.py:185][0m |           0.0030 |          49.1030 |          15.5190 |
[32m[20221213 23:16:30 @agent_ppo2.py:185][0m |          -0.0051 |          46.7275 |          15.4972 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0053 |          45.8206 |          15.5002 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0061 |          45.5452 |          15.4882 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0073 |          44.9665 |          15.5043 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0100 |          44.6735 |          15.4995 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0121 |          44.5092 |          15.4981 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0088 |          44.2325 |          15.4819 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0105 |          44.0189 |          15.5061 |
[32m[20221213 23:16:31 @agent_ppo2.py:185][0m |          -0.0052 |          44.4503 |          15.4986 |
[32m[20221213 23:16:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.56
[32m[20221213 23:16:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.10
[32m[20221213 23:16:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.92
[32m[20221213 23:16:31 @agent_ppo2.py:143][0m Total time:       3.99 min
[32m[20221213 23:16:31 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221213 23:16:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2185 --------------------------#
[32m[20221213 23:16:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |           0.0041 |          32.9034 |          15.6369 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |           0.0031 |          29.6048 |          15.6143 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0029 |          26.2998 |          15.5988 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0059 |          25.6573 |          15.6047 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0080 |          25.2614 |          15.6151 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0055 |          25.1164 |          15.6093 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0051 |          25.9410 |          15.6102 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0181 |          24.6498 |          15.6064 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0082 |          24.4701 |          15.6306 |
[32m[20221213 23:16:32 @agent_ppo2.py:185][0m |          -0.0044 |          24.3696 |          15.6092 |
[32m[20221213 23:16:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.30
[32m[20221213 23:16:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.22
[32m[20221213 23:16:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.41
[32m[20221213 23:16:33 @agent_ppo2.py:143][0m Total time:       4.01 min
[32m[20221213 23:16:33 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 23:16:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2186 --------------------------#
[32m[20221213 23:16:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |          -0.0023 |          50.7942 |          15.5997 |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |          -0.0042 |          49.8625 |          15.5998 |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |          -0.0039 |          49.4067 |          15.6067 |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |          -0.0045 |          49.2905 |          15.6053 |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |           0.0036 |          51.1113 |          15.6037 |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |          -0.0059 |          49.1229 |          15.5863 |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |          -0.0082 |          48.9079 |          15.5964 |
[32m[20221213 23:16:33 @agent_ppo2.py:185][0m |          -0.0085 |          48.7015 |          15.6024 |
[32m[20221213 23:16:34 @agent_ppo2.py:185][0m |          -0.0070 |          48.6746 |          15.6026 |
[32m[20221213 23:16:34 @agent_ppo2.py:185][0m |          -0.0090 |          48.5179 |          15.6020 |
[32m[20221213 23:16:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:16:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.45
[32m[20221213 23:16:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.01
[32m[20221213 23:16:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.93
[32m[20221213 23:16:34 @agent_ppo2.py:143][0m Total time:       4.03 min
[32m[20221213 23:16:34 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221213 23:16:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2187 --------------------------#
[32m[20221213 23:16:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:34 @agent_ppo2.py:185][0m |          -0.0029 |          54.7095 |          15.5862 |
[32m[20221213 23:16:34 @agent_ppo2.py:185][0m |          -0.0044 |          54.0159 |          15.5819 |
[32m[20221213 23:16:34 @agent_ppo2.py:185][0m |          -0.0059 |          54.0383 |          15.6011 |
[32m[20221213 23:16:34 @agent_ppo2.py:185][0m |          -0.0070 |          53.6663 |          15.6166 |
[32m[20221213 23:16:34 @agent_ppo2.py:185][0m |          -0.0014 |          54.8567 |          15.6242 |
[32m[20221213 23:16:35 @agent_ppo2.py:185][0m |          -0.0013 |          55.5534 |          15.6408 |
[32m[20221213 23:16:35 @agent_ppo2.py:185][0m |           0.0052 |          57.9690 |          15.6255 |
[32m[20221213 23:16:35 @agent_ppo2.py:185][0m |          -0.0124 |          53.1995 |          15.6551 |
[32m[20221213 23:16:35 @agent_ppo2.py:185][0m |          -0.0125 |          52.9204 |          15.6662 |
[32m[20221213 23:16:35 @agent_ppo2.py:185][0m |          -0.0097 |          52.9606 |          15.6918 |
[32m[20221213 23:16:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.68
[32m[20221213 23:16:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.64
[32m[20221213 23:16:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.25
[32m[20221213 23:16:35 @agent_ppo2.py:143][0m Total time:       4.05 min
[32m[20221213 23:16:35 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 23:16:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2188 --------------------------#
[32m[20221213 23:16:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:35 @agent_ppo2.py:185][0m |           0.0021 |          51.4623 |          15.4840 |
[32m[20221213 23:16:35 @agent_ppo2.py:185][0m |           0.0034 |          49.2540 |          15.4660 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |          -0.0026 |          47.1537 |          15.4849 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |          -0.0086 |          46.9242 |          15.4840 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |          -0.0025 |          46.4349 |          15.4981 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |           0.0044 |          49.1482 |          15.4999 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |           0.0047 |          48.2193 |          15.5214 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |          -0.0033 |          45.9069 |          15.5137 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |          -0.0061 |          45.7652 |          15.5044 |
[32m[20221213 23:16:36 @agent_ppo2.py:185][0m |          -0.0044 |          45.6338 |          15.4961 |
[32m[20221213 23:16:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.07
[32m[20221213 23:16:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.79
[32m[20221213 23:16:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.62
[32m[20221213 23:16:36 @agent_ppo2.py:143][0m Total time:       4.07 min
[32m[20221213 23:16:36 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221213 23:16:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2189 --------------------------#
[32m[20221213 23:16:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0027 |          53.7472 |          15.7043 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0085 |          51.3348 |          15.7345 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0113 |          50.6085 |          15.7321 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0091 |          51.4145 |          15.7310 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0142 |          49.7300 |          15.7361 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0126 |          49.3786 |          15.7490 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0141 |          49.2076 |          15.7648 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0135 |          49.1559 |          15.7563 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0139 |          49.0261 |          15.7644 |
[32m[20221213 23:16:37 @agent_ppo2.py:185][0m |          -0.0136 |          48.7873 |          15.7798 |
[32m[20221213 23:16:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.27
[32m[20221213 23:16:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.90
[32m[20221213 23:16:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.92
[32m[20221213 23:16:38 @agent_ppo2.py:143][0m Total time:       4.09 min
[32m[20221213 23:16:38 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 23:16:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2190 --------------------------#
[32m[20221213 23:16:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:16:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:38 @agent_ppo2.py:185][0m |          -0.0014 |          61.2566 |          15.9687 |
[32m[20221213 23:16:38 @agent_ppo2.py:185][0m |          -0.0054 |          60.0173 |          15.9785 |
[32m[20221213 23:16:38 @agent_ppo2.py:185][0m |          -0.0093 |          59.5918 |          15.9785 |
[32m[20221213 23:16:38 @agent_ppo2.py:185][0m |          -0.0090 |          59.2193 |          15.9848 |
[32m[20221213 23:16:38 @agent_ppo2.py:185][0m |          -0.0038 |          60.9106 |          16.0083 |
[32m[20221213 23:16:38 @agent_ppo2.py:185][0m |          -0.0124 |          58.9417 |          15.9988 |
[32m[20221213 23:16:38 @agent_ppo2.py:185][0m |          -0.0131 |          58.8671 |          16.0217 |
[32m[20221213 23:16:39 @agent_ppo2.py:185][0m |          -0.0123 |          58.6115 |          16.0321 |
[32m[20221213 23:16:39 @agent_ppo2.py:185][0m |          -0.0063 |          60.8274 |          16.0377 |
[32m[20221213 23:16:39 @agent_ppo2.py:185][0m |          -0.0116 |          58.5544 |          16.0525 |
[32m[20221213 23:16:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.00
[32m[20221213 23:16:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.01
[32m[20221213 23:16:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.35
[32m[20221213 23:16:39 @agent_ppo2.py:143][0m Total time:       4.11 min
[32m[20221213 23:16:39 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221213 23:16:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2191 --------------------------#
[32m[20221213 23:16:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:39 @agent_ppo2.py:185][0m |           0.0003 |          45.5170 |          15.7468 |
[32m[20221213 23:16:39 @agent_ppo2.py:185][0m |          -0.0033 |          43.9929 |          15.7307 |
[32m[20221213 23:16:39 @agent_ppo2.py:185][0m |          -0.0043 |          43.5077 |          15.7319 |
[32m[20221213 23:16:39 @agent_ppo2.py:185][0m |          -0.0054 |          43.3169 |          15.7392 |
[32m[20221213 23:16:40 @agent_ppo2.py:185][0m |          -0.0093 |          43.1521 |          15.7432 |
[32m[20221213 23:16:40 @agent_ppo2.py:185][0m |          -0.0059 |          42.9862 |          15.7440 |
[32m[20221213 23:16:40 @agent_ppo2.py:185][0m |          -0.0098 |          42.7802 |          15.7330 |
[32m[20221213 23:16:40 @agent_ppo2.py:185][0m |          -0.0082 |          42.7529 |          15.7129 |
[32m[20221213 23:16:40 @agent_ppo2.py:185][0m |          -0.0097 |          42.5849 |          15.7024 |
[32m[20221213 23:16:40 @agent_ppo2.py:185][0m |           0.0102 |          47.3288 |          15.7173 |
[32m[20221213 23:16:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:16:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.41
[32m[20221213 23:16:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.04
[32m[20221213 23:16:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.08
[32m[20221213 23:16:40 @agent_ppo2.py:143][0m Total time:       4.13 min
[32m[20221213 23:16:40 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 23:16:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2192 --------------------------#
[32m[20221213 23:16:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:40 @agent_ppo2.py:185][0m |          -0.0027 |          54.9405 |          15.6837 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0107 |          53.3250 |          15.6544 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0105 |          52.9282 |          15.6617 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0133 |          52.5837 |          15.6812 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0119 |          52.4201 |          15.6604 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0047 |          54.2397 |          15.6549 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0092 |          52.3063 |          15.6575 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0156 |          51.9997 |          15.6539 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0036 |          55.5740 |          15.6401 |
[32m[20221213 23:16:41 @agent_ppo2.py:185][0m |          -0.0134 |          52.0255 |          15.6659 |
[32m[20221213 23:16:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.00
[32m[20221213 23:16:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.45
[32m[20221213 23:16:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.86
[32m[20221213 23:16:41 @agent_ppo2.py:143][0m Total time:       4.16 min
[32m[20221213 23:16:41 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221213 23:16:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2193 --------------------------#
[32m[20221213 23:16:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |           0.0023 |          45.6243 |          15.5945 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0050 |          42.4577 |          15.5895 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0069 |          41.7213 |          15.5854 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0113 |          41.2602 |          15.5905 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0078 |          40.9358 |          15.5698 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0076 |          40.5940 |          15.5806 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0103 |          40.9935 |          15.5874 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0107 |          40.2566 |          15.5795 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0157 |          40.1037 |          15.5667 |
[32m[20221213 23:16:42 @agent_ppo2.py:185][0m |          -0.0160 |          40.0177 |          15.5877 |
[32m[20221213 23:16:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.14
[32m[20221213 23:16:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.36
[32m[20221213 23:16:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.46
[32m[20221213 23:16:43 @agent_ppo2.py:143][0m Total time:       4.18 min
[32m[20221213 23:16:43 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 23:16:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2194 --------------------------#
[32m[20221213 23:16:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:43 @agent_ppo2.py:185][0m |          -0.0012 |          47.2693 |          15.9177 |
[32m[20221213 23:16:43 @agent_ppo2.py:185][0m |           0.0098 |          50.7372 |          15.8658 |
[32m[20221213 23:16:43 @agent_ppo2.py:185][0m |          -0.0019 |          46.9407 |          15.8569 |
[32m[20221213 23:16:43 @agent_ppo2.py:185][0m |          -0.0070 |          45.1913 |          15.9011 |
[32m[20221213 23:16:43 @agent_ppo2.py:185][0m |          -0.0077 |          45.0935 |          15.8833 |
[32m[20221213 23:16:43 @agent_ppo2.py:185][0m |          -0.0112 |          44.9591 |          15.8998 |
[32m[20221213 23:16:43 @agent_ppo2.py:185][0m |          -0.0101 |          44.8204 |          15.8992 |
[32m[20221213 23:16:44 @agent_ppo2.py:185][0m |          -0.0106 |          44.7843 |          15.8830 |
[32m[20221213 23:16:44 @agent_ppo2.py:185][0m |          -0.0101 |          44.6542 |          15.8710 |
[32m[20221213 23:16:44 @agent_ppo2.py:185][0m |          -0.0130 |          44.6524 |          15.8911 |
[32m[20221213 23:16:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.68
[32m[20221213 23:16:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.07
[32m[20221213 23:16:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:16:44 @agent_ppo2.py:143][0m Total time:       4.20 min
[32m[20221213 23:16:44 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221213 23:16:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2195 --------------------------#
[32m[20221213 23:16:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:44 @agent_ppo2.py:185][0m |           0.0052 |          46.6519 |          15.7705 |
[32m[20221213 23:16:44 @agent_ppo2.py:185][0m |          -0.0098 |          42.3021 |          15.7627 |
[32m[20221213 23:16:44 @agent_ppo2.py:185][0m |          -0.0084 |          41.2820 |          15.7552 |
[32m[20221213 23:16:44 @agent_ppo2.py:185][0m |          -0.0098 |          40.9659 |          15.7489 |
[32m[20221213 23:16:45 @agent_ppo2.py:185][0m |          -0.0113 |          40.5749 |          15.7575 |
[32m[20221213 23:16:45 @agent_ppo2.py:185][0m |          -0.0158 |          40.2317 |          15.7478 |
[32m[20221213 23:16:45 @agent_ppo2.py:185][0m |          -0.0126 |          40.0332 |          15.7438 |
[32m[20221213 23:16:45 @agent_ppo2.py:185][0m |          -0.0153 |          39.8350 |          15.7516 |
[32m[20221213 23:16:45 @agent_ppo2.py:185][0m |          -0.0088 |          41.3449 |          15.7563 |
[32m[20221213 23:16:45 @agent_ppo2.py:185][0m |          -0.0123 |          40.0200 |          15.7528 |
[32m[20221213 23:16:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.30
[32m[20221213 23:16:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.29
[32m[20221213 23:16:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.66
[32m[20221213 23:16:45 @agent_ppo2.py:143][0m Total time:       4.22 min
[32m[20221213 23:16:45 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 23:16:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2196 --------------------------#
[32m[20221213 23:16:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:45 @agent_ppo2.py:185][0m |          -0.0004 |          52.4324 |          15.6374 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |           0.0024 |          51.6736 |          15.6427 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |          -0.0093 |          49.2556 |          15.6428 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |          -0.0104 |          49.0524 |          15.6339 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |          -0.0032 |          49.7779 |          15.6288 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |           0.0019 |          52.6951 |          15.6305 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |          -0.0062 |          48.2064 |          15.6379 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |          -0.0094 |          47.6929 |          15.6326 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |          -0.0112 |          47.7081 |          15.6353 |
[32m[20221213 23:16:46 @agent_ppo2.py:185][0m |          -0.0148 |          47.4958 |          15.6342 |
[32m[20221213 23:16:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:16:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.23
[32m[20221213 23:16:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.45
[32m[20221213 23:16:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.57
[32m[20221213 23:16:46 @agent_ppo2.py:143][0m Total time:       4.24 min
[32m[20221213 23:16:46 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221213 23:16:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2197 --------------------------#
[32m[20221213 23:16:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |           0.0035 |          55.1173 |          15.8208 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |           0.0063 |          54.5270 |          15.8030 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |          -0.0038 |          50.0755 |          15.7709 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |          -0.0044 |          49.8349 |          15.7547 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |          -0.0081 |          49.1664 |          15.7757 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |          -0.0091 |          48.8504 |          15.7393 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |          -0.0101 |          49.0370 |          15.7313 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |          -0.0103 |          48.2045 |          15.7495 |
[32m[20221213 23:16:47 @agent_ppo2.py:185][0m |          -0.0134 |          48.3878 |          15.7190 |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |          -0.0087 |          48.1418 |          15.7308 |
[32m[20221213 23:16:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.77
[32m[20221213 23:16:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.11
[32m[20221213 23:16:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.55
[32m[20221213 23:16:48 @agent_ppo2.py:143][0m Total time:       4.26 min
[32m[20221213 23:16:48 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 23:16:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2198 --------------------------#
[32m[20221213 23:16:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |          -0.0013 |          48.8004 |          15.8214 |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |          -0.0043 |          47.9291 |          15.8127 |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |           0.0019 |          49.7470 |          15.8120 |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |          -0.0064 |          47.7638 |          15.7899 |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |          -0.0052 |          47.6761 |          15.7932 |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |          -0.0072 |          47.5613 |          15.7979 |
[32m[20221213 23:16:48 @agent_ppo2.py:185][0m |          -0.0065 |          47.4063 |          15.7801 |
[32m[20221213 23:16:49 @agent_ppo2.py:185][0m |          -0.0068 |          47.5028 |          15.7799 |
[32m[20221213 23:16:49 @agent_ppo2.py:185][0m |          -0.0021 |          48.1350 |          15.7987 |
[32m[20221213 23:16:49 @agent_ppo2.py:185][0m |          -0.0081 |          47.3027 |          15.7814 |
[32m[20221213 23:16:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.28
[32m[20221213 23:16:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.61
[32m[20221213 23:16:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.84
[32m[20221213 23:16:49 @agent_ppo2.py:143][0m Total time:       4.28 min
[32m[20221213 23:16:49 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221213 23:16:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2199 --------------------------#
[32m[20221213 23:16:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:49 @agent_ppo2.py:185][0m |           0.0055 |          39.7505 |          15.8264 |
[32m[20221213 23:16:49 @agent_ppo2.py:185][0m |           0.0001 |          34.6479 |          15.8339 |
[32m[20221213 23:16:49 @agent_ppo2.py:185][0m |          -0.0072 |          33.9905 |          15.8374 |
[32m[20221213 23:16:50 @agent_ppo2.py:185][0m |           0.0013 |          33.5577 |          15.8319 |
[32m[20221213 23:16:50 @agent_ppo2.py:185][0m |           0.0004 |          33.6550 |          15.8312 |
[32m[20221213 23:16:50 @agent_ppo2.py:185][0m |          -0.0064 |          33.0482 |          15.8291 |
[32m[20221213 23:16:50 @agent_ppo2.py:185][0m |          -0.0078 |          32.8125 |          15.8128 |
[32m[20221213 23:16:50 @agent_ppo2.py:185][0m |          -0.0065 |          33.2861 |          15.8142 |
[32m[20221213 23:16:50 @agent_ppo2.py:185][0m |          -0.0088 |          32.6057 |          15.8183 |
[32m[20221213 23:16:50 @agent_ppo2.py:185][0m |          -0.0074 |          32.6040 |          15.8070 |
[32m[20221213 23:16:50 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:16:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.64
[32m[20221213 23:16:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.51
[32m[20221213 23:16:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.60
[32m[20221213 23:16:50 @agent_ppo2.py:143][0m Total time:       4.30 min
[32m[20221213 23:16:50 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 23:16:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2200 --------------------------#
[32m[20221213 23:16:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:16:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |           0.0104 |          62.8954 |          15.7124 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0042 |          54.4997 |          15.6757 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0041 |          54.1820 |          15.7057 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0102 |          53.4439 |          15.7179 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0099 |          53.4509 |          15.7171 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0089 |          53.1788 |          15.7212 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0101 |          53.1444 |          15.7424 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0105 |          53.0332 |          15.7263 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0111 |          53.0087 |          15.7317 |
[32m[20221213 23:16:51 @agent_ppo2.py:185][0m |          -0.0102 |          52.7746 |          15.7360 |
[32m[20221213 23:16:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:16:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.95
[32m[20221213 23:16:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.49
[32m[20221213 23:16:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.50
[32m[20221213 23:16:51 @agent_ppo2.py:143][0m Total time:       4.32 min
[32m[20221213 23:16:51 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221213 23:16:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2201 --------------------------#
[32m[20221213 23:16:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |           0.0014 |          52.5363 |          15.6353 |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |          -0.0059 |          49.4150 |          15.6109 |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |          -0.0035 |          48.5174 |          15.5921 |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |          -0.0102 |          47.9728 |          15.5837 |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |          -0.0045 |          47.6626 |          15.5711 |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |          -0.0083 |          47.2352 |          15.5751 |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |          -0.0102 |          46.9498 |          15.5607 |
[32m[20221213 23:16:52 @agent_ppo2.py:185][0m |          -0.0102 |          47.0091 |          15.5685 |
[32m[20221213 23:16:53 @agent_ppo2.py:185][0m |          -0.0116 |          46.5994 |          15.5476 |
[32m[20221213 23:16:53 @agent_ppo2.py:185][0m |          -0.0114 |          46.5863 |          15.5415 |
[32m[20221213 23:16:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:16:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.82
[32m[20221213 23:16:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.92
[32m[20221213 23:16:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.37
[32m[20221213 23:16:53 @agent_ppo2.py:143][0m Total time:       4.35 min
[32m[20221213 23:16:53 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 23:16:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2202 --------------------------#
[32m[20221213 23:16:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:53 @agent_ppo2.py:185][0m |           0.0015 |          36.7351 |          15.5802 |
[32m[20221213 23:16:53 @agent_ppo2.py:185][0m |          -0.0054 |          34.4428 |          15.5926 |
[32m[20221213 23:16:53 @agent_ppo2.py:185][0m |           0.0042 |          36.8133 |          15.5984 |
[32m[20221213 23:16:53 @agent_ppo2.py:185][0m |          -0.0033 |          33.4946 |          15.5770 |
[32m[20221213 23:16:53 @agent_ppo2.py:185][0m |          -0.0111 |          33.3029 |          15.5764 |
[32m[20221213 23:16:54 @agent_ppo2.py:185][0m |          -0.0074 |          32.9424 |          15.5679 |
[32m[20221213 23:16:54 @agent_ppo2.py:185][0m |          -0.0124 |          32.8717 |          15.5852 |
[32m[20221213 23:16:54 @agent_ppo2.py:185][0m |          -0.0085 |          33.1094 |          15.5839 |
[32m[20221213 23:16:54 @agent_ppo2.py:185][0m |          -0.0102 |          32.5485 |          15.5807 |
[32m[20221213 23:16:54 @agent_ppo2.py:185][0m |          -0.0152 |          32.5083 |          15.5650 |
[32m[20221213 23:16:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:16:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.06
[32m[20221213 23:16:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.18
[32m[20221213 23:16:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.95
[32m[20221213 23:16:54 @agent_ppo2.py:143][0m Total time:       4.37 min
[32m[20221213 23:16:54 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221213 23:16:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2203 --------------------------#
[32m[20221213 23:16:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:54 @agent_ppo2.py:185][0m |          -0.0014 |          41.5462 |          15.7350 |
[32m[20221213 23:16:54 @agent_ppo2.py:185][0m |          -0.0049 |          38.9997 |          15.7083 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0114 |          38.2298 |          15.7117 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0087 |          37.7924 |          15.7173 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0105 |          37.4304 |          15.7139 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0172 |          37.0805 |          15.7150 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0154 |          37.0118 |          15.7141 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0147 |          36.6396 |          15.6970 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0112 |          36.6175 |          15.7112 |
[32m[20221213 23:16:55 @agent_ppo2.py:185][0m |          -0.0160 |          36.3015 |          15.6959 |
[32m[20221213 23:16:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.89
[32m[20221213 23:16:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.45
[32m[20221213 23:16:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.64
[32m[20221213 23:16:55 @agent_ppo2.py:143][0m Total time:       4.39 min
[32m[20221213 23:16:55 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 23:16:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2204 --------------------------#
[32m[20221213 23:16:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |           0.0013 |          44.1373 |          15.6423 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0032 |          41.3563 |          15.6351 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0064 |          40.3651 |          15.6078 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |           0.0006 |          41.6440 |          15.6133 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0091 |          39.2291 |          15.6164 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0123 |          38.7916 |          15.6038 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0124 |          38.5106 |          15.6162 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0130 |          38.2375 |          15.5940 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0037 |          40.8086 |          15.6030 |
[32m[20221213 23:16:56 @agent_ppo2.py:185][0m |          -0.0128 |          37.8397 |          15.6037 |
[32m[20221213 23:16:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.84
[32m[20221213 23:16:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.91
[32m[20221213 23:16:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.94
[32m[20221213 23:16:57 @agent_ppo2.py:143][0m Total time:       4.41 min
[32m[20221213 23:16:57 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221213 23:16:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2205 --------------------------#
[32m[20221213 23:16:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:16:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |           0.0019 |          47.3138 |          15.6890 |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |          -0.0044 |          45.0123 |          15.6659 |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |          -0.0064 |          44.1613 |          15.6695 |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |          -0.0059 |          43.7049 |          15.6495 |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |          -0.0077 |          43.4623 |          15.6604 |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |          -0.0050 |          43.1008 |          15.6505 |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |          -0.0087 |          42.9711 |          15.6400 |
[32m[20221213 23:16:57 @agent_ppo2.py:185][0m |          -0.0095 |          42.6749 |          15.6317 |
[32m[20221213 23:16:58 @agent_ppo2.py:185][0m |          -0.0110 |          42.5888 |          15.6349 |
[32m[20221213 23:16:58 @agent_ppo2.py:185][0m |          -0.0108 |          42.5742 |          15.6298 |
[32m[20221213 23:16:58 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:16:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.83
[32m[20221213 23:16:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.95
[32m[20221213 23:16:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.37
[32m[20221213 23:16:58 @agent_ppo2.py:143][0m Total time:       4.43 min
[32m[20221213 23:16:58 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 23:16:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2206 --------------------------#
[32m[20221213 23:16:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:58 @agent_ppo2.py:185][0m |           0.0017 |          44.3082 |          15.8092 |
[32m[20221213 23:16:58 @agent_ppo2.py:185][0m |          -0.0068 |          40.2803 |          15.8008 |
[32m[20221213 23:16:58 @agent_ppo2.py:185][0m |          -0.0094 |          39.4450 |          15.7728 |
[32m[20221213 23:16:58 @agent_ppo2.py:185][0m |          -0.0075 |          39.7956 |          15.7643 |
[32m[20221213 23:16:58 @agent_ppo2.py:185][0m |          -0.0141 |          38.6596 |          15.7687 |
[32m[20221213 23:16:59 @agent_ppo2.py:185][0m |          -0.0149 |          38.2729 |          15.7598 |
[32m[20221213 23:16:59 @agent_ppo2.py:185][0m |          -0.0156 |          38.2178 |          15.7671 |
[32m[20221213 23:16:59 @agent_ppo2.py:185][0m |          -0.0171 |          37.9691 |          15.7613 |
[32m[20221213 23:16:59 @agent_ppo2.py:185][0m |          -0.0113 |          38.4968 |          15.7479 |
[32m[20221213 23:16:59 @agent_ppo2.py:185][0m |          -0.0072 |          39.3431 |          15.7496 |
[32m[20221213 23:16:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:16:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.18
[32m[20221213 23:16:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.34
[32m[20221213 23:16:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.51
[32m[20221213 23:16:59 @agent_ppo2.py:143][0m Total time:       4.45 min
[32m[20221213 23:16:59 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221213 23:16:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2207 --------------------------#
[32m[20221213 23:16:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:16:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:16:59 @agent_ppo2.py:185][0m |          -0.0018 |          50.4446 |          15.6920 |
[32m[20221213 23:16:59 @agent_ppo2.py:185][0m |          -0.0055 |          47.5143 |          15.6615 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0079 |          46.6227 |          15.6603 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0114 |          46.2888 |          15.6571 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0067 |          46.3474 |          15.6593 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0068 |          45.6821 |          15.6495 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0121 |          45.6007 |          15.6543 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0148 |          45.2535 |          15.6497 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0108 |          45.2562 |          15.6697 |
[32m[20221213 23:17:00 @agent_ppo2.py:185][0m |          -0.0052 |          50.9917 |          15.6619 |
[32m[20221213 23:17:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.33
[32m[20221213 23:17:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.86
[32m[20221213 23:17:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.71
[32m[20221213 23:17:00 @agent_ppo2.py:143][0m Total time:       4.47 min
[32m[20221213 23:17:00 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 23:17:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2208 --------------------------#
[32m[20221213 23:17:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0000 |          43.3016 |          15.7881 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |           0.0029 |          40.6599 |          15.7804 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0047 |          38.9719 |          15.7461 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0093 |          38.2734 |          15.7664 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0103 |          37.5277 |          15.7596 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0082 |          37.1758 |          15.7562 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0135 |          36.7811 |          15.7762 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0148 |          36.5181 |          15.7616 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0107 |          36.3020 |          15.7567 |
[32m[20221213 23:17:01 @agent_ppo2.py:185][0m |          -0.0130 |          36.2486 |          15.7607 |
[32m[20221213 23:17:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.32
[32m[20221213 23:17:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.91
[32m[20221213 23:17:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.77
[32m[20221213 23:17:02 @agent_ppo2.py:143][0m Total time:       4.49 min
[32m[20221213 23:17:02 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221213 23:17:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2209 --------------------------#
[32m[20221213 23:17:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:02 @agent_ppo2.py:185][0m |           0.0004 |          27.0400 |          15.7725 |
[32m[20221213 23:17:02 @agent_ppo2.py:185][0m |          -0.0076 |          25.1735 |          15.7657 |
[32m[20221213 23:17:02 @agent_ppo2.py:185][0m |          -0.0074 |          24.1103 |          15.7739 |
[32m[20221213 23:17:02 @agent_ppo2.py:185][0m |          -0.0123 |          23.6124 |          15.7575 |
[32m[20221213 23:17:02 @agent_ppo2.py:185][0m |           0.0015 |          27.8732 |          15.7654 |
[32m[20221213 23:17:02 @agent_ppo2.py:185][0m |          -0.0062 |          23.1415 |          15.7427 |
[32m[20221213 23:17:02 @agent_ppo2.py:185][0m |          -0.0114 |          22.8151 |          15.7639 |
[32m[20221213 23:17:03 @agent_ppo2.py:185][0m |          -0.0146 |          22.6417 |          15.7496 |
[32m[20221213 23:17:03 @agent_ppo2.py:185][0m |          -0.0145 |          22.4634 |          15.7451 |
[32m[20221213 23:17:03 @agent_ppo2.py:185][0m |          -0.0160 |          22.3675 |          15.7387 |
[32m[20221213 23:17:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.87
[32m[20221213 23:17:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.25
[32m[20221213 23:17:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.35
[32m[20221213 23:17:03 @agent_ppo2.py:143][0m Total time:       4.51 min
[32m[20221213 23:17:03 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 23:17:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2210 --------------------------#
[32m[20221213 23:17:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:17:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:03 @agent_ppo2.py:185][0m |           0.0005 |          39.0781 |          15.5959 |
[32m[20221213 23:17:03 @agent_ppo2.py:185][0m |          -0.0032 |          36.7621 |          15.5734 |
[32m[20221213 23:17:03 @agent_ppo2.py:185][0m |          -0.0036 |          35.7172 |          15.5678 |
[32m[20221213 23:17:03 @agent_ppo2.py:185][0m |          -0.0060 |          34.9338 |          15.5714 |
[32m[20221213 23:17:04 @agent_ppo2.py:185][0m |          -0.0077 |          34.2077 |          15.5839 |
[32m[20221213 23:17:04 @agent_ppo2.py:185][0m |          -0.0117 |          33.7610 |          15.5726 |
[32m[20221213 23:17:04 @agent_ppo2.py:185][0m |          -0.0116 |          33.0490 |          15.5632 |
[32m[20221213 23:17:04 @agent_ppo2.py:185][0m |           0.0056 |          36.4467 |          15.5876 |
[32m[20221213 23:17:04 @agent_ppo2.py:185][0m |          -0.0093 |          32.2573 |          15.5857 |
[32m[20221213 23:17:04 @agent_ppo2.py:185][0m |          -0.0077 |          31.9402 |          15.5873 |
[32m[20221213 23:17:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:17:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.33
[32m[20221213 23:17:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.64
[32m[20221213 23:17:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.21
[32m[20221213 23:17:04 @agent_ppo2.py:143][0m Total time:       4.54 min
[32m[20221213 23:17:04 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221213 23:17:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2211 --------------------------#
[32m[20221213 23:17:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:04 @agent_ppo2.py:185][0m |           0.0045 |          52.0879 |          15.6649 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |           0.0073 |          55.9126 |          15.6497 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0002 |          49.9874 |          15.6265 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0099 |          47.8524 |          15.6303 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0101 |          47.4810 |          15.6239 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0094 |          46.9090 |          15.6183 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0126 |          46.8702 |          15.6231 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0098 |          46.7002 |          15.6156 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0126 |          46.6515 |          15.6206 |
[32m[20221213 23:17:05 @agent_ppo2.py:185][0m |          -0.0127 |          46.4265 |          15.6257 |
[32m[20221213 23:17:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.83
[32m[20221213 23:17:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.70
[32m[20221213 23:17:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.87
[32m[20221213 23:17:05 @agent_ppo2.py:143][0m Total time:       4.56 min
[32m[20221213 23:17:05 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 23:17:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2212 --------------------------#
[32m[20221213 23:17:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |           0.0149 |          35.1259 |          15.4081 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0004 |          30.3374 |          15.4224 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0075 |          29.3730 |          15.4222 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0069 |          28.6452 |          15.3938 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0084 |          28.0912 |          15.4120 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0134 |          27.7124 |          15.4098 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0085 |          27.4425 |          15.3796 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0082 |          27.3528 |          15.3991 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0126 |          26.8457 |          15.3837 |
[32m[20221213 23:17:06 @agent_ppo2.py:185][0m |          -0.0165 |          26.6053 |          15.3960 |
[32m[20221213 23:17:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.68
[32m[20221213 23:17:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.36
[32m[20221213 23:17:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.99
[32m[20221213 23:17:07 @agent_ppo2.py:143][0m Total time:       4.58 min
[32m[20221213 23:17:07 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221213 23:17:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2213 --------------------------#
[32m[20221213 23:17:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:07 @agent_ppo2.py:185][0m |          -0.0012 |          51.4459 |          15.6040 |
[32m[20221213 23:17:07 @agent_ppo2.py:185][0m |          -0.0033 |          48.4005 |          15.5793 |
[32m[20221213 23:17:07 @agent_ppo2.py:185][0m |          -0.0063 |          47.2012 |          15.5612 |
[32m[20221213 23:17:07 @agent_ppo2.py:185][0m |          -0.0049 |          46.5479 |          15.5786 |
[32m[20221213 23:17:07 @agent_ppo2.py:185][0m |          -0.0094 |          45.9248 |          15.5753 |
[32m[20221213 23:17:07 @agent_ppo2.py:185][0m |          -0.0109 |          45.5544 |          15.5622 |
[32m[20221213 23:17:07 @agent_ppo2.py:185][0m |          -0.0091 |          45.1237 |          15.5593 |
[32m[20221213 23:17:08 @agent_ppo2.py:185][0m |          -0.0094 |          45.0420 |          15.5606 |
[32m[20221213 23:17:08 @agent_ppo2.py:185][0m |          -0.0106 |          44.5579 |          15.5406 |
[32m[20221213 23:17:08 @agent_ppo2.py:185][0m |          -0.0137 |          44.3302 |          15.5173 |
[32m[20221213 23:17:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.00
[32m[20221213 23:17:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.20
[32m[20221213 23:17:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.68
[32m[20221213 23:17:08 @agent_ppo2.py:143][0m Total time:       4.60 min
[32m[20221213 23:17:08 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 23:17:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2214 --------------------------#
[32m[20221213 23:17:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:08 @agent_ppo2.py:185][0m |          -0.0074 |          46.0373 |          15.6909 |
[32m[20221213 23:17:08 @agent_ppo2.py:185][0m |          -0.0069 |          44.7660 |          15.6777 |
[32m[20221213 23:17:08 @agent_ppo2.py:185][0m |          -0.0095 |          44.3123 |          15.6890 |
[32m[20221213 23:17:08 @agent_ppo2.py:185][0m |          -0.0143 |          44.0881 |          15.6848 |
[32m[20221213 23:17:09 @agent_ppo2.py:185][0m |          -0.0119 |          43.8857 |          15.6947 |
[32m[20221213 23:17:09 @agent_ppo2.py:185][0m |          -0.0151 |          43.7685 |          15.6961 |
[32m[20221213 23:17:09 @agent_ppo2.py:185][0m |          -0.0113 |          43.6199 |          15.7037 |
[32m[20221213 23:17:09 @agent_ppo2.py:185][0m |          -0.0098 |          43.7864 |          15.7230 |
[32m[20221213 23:17:09 @agent_ppo2.py:185][0m |          -0.0148 |          43.3522 |          15.7017 |
[32m[20221213 23:17:09 @agent_ppo2.py:185][0m |          -0.0171 |          43.2358 |          15.7216 |
[32m[20221213 23:17:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.13
[32m[20221213 23:17:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.64
[32m[20221213 23:17:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.24
[32m[20221213 23:17:09 @agent_ppo2.py:143][0m Total time:       4.62 min
[32m[20221213 23:17:09 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221213 23:17:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2215 --------------------------#
[32m[20221213 23:17:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:09 @agent_ppo2.py:185][0m |           0.0063 |          51.0676 |          15.6215 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0038 |          47.1132 |          15.6350 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0094 |          45.9114 |          15.6124 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0019 |          46.2679 |          15.6305 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0049 |          45.6020 |          15.6201 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |           0.0081 |          52.0859 |          15.6055 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0102 |          44.5407 |          15.6147 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0105 |          44.0886 |          15.5914 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0130 |          44.0438 |          15.6155 |
[32m[20221213 23:17:10 @agent_ppo2.py:185][0m |          -0.0100 |          43.8910 |          15.5798 |
[32m[20221213 23:17:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.99
[32m[20221213 23:17:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.11
[32m[20221213 23:17:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.15
[32m[20221213 23:17:10 @agent_ppo2.py:143][0m Total time:       4.64 min
[32m[20221213 23:17:10 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 23:17:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2216 --------------------------#
[32m[20221213 23:17:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0021 |          35.8419 |          15.6824 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0044 |          31.6844 |          15.6653 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0084 |          30.4765 |          15.6584 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0116 |          29.7888 |          15.6412 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0149 |          29.1463 |          15.6280 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0139 |          28.6093 |          15.6227 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0119 |          28.3827 |          15.6152 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0101 |          28.0108 |          15.6133 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0121 |          28.3031 |          15.5798 |
[32m[20221213 23:17:11 @agent_ppo2.py:185][0m |          -0.0182 |          27.5950 |          15.6035 |
[32m[20221213 23:17:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.40
[32m[20221213 23:17:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 362.03
[32m[20221213 23:17:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.07
[32m[20221213 23:17:12 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221213 23:17:12 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221213 23:17:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2217 --------------------------#
[32m[20221213 23:17:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:12 @agent_ppo2.py:185][0m |           0.0013 |          44.5931 |          15.8535 |
[32m[20221213 23:17:12 @agent_ppo2.py:185][0m |          -0.0058 |          40.5119 |          15.8393 |
[32m[20221213 23:17:12 @agent_ppo2.py:185][0m |          -0.0007 |          40.2393 |          15.8312 |
[32m[20221213 23:17:12 @agent_ppo2.py:185][0m |          -0.0047 |          39.2087 |          15.8204 |
[32m[20221213 23:17:12 @agent_ppo2.py:185][0m |          -0.0128 |          38.1343 |          15.8059 |
[32m[20221213 23:17:12 @agent_ppo2.py:185][0m |          -0.0068 |          38.2972 |          15.7993 |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0123 |          37.2985 |          15.7998 |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0186 |          37.0949 |          15.7724 |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0135 |          36.7803 |          15.7792 |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0142 |          36.5422 |          15.7645 |
[32m[20221213 23:17:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.27
[32m[20221213 23:17:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.68
[32m[20221213 23:17:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.19
[32m[20221213 23:17:13 @agent_ppo2.py:143][0m Total time:       4.68 min
[32m[20221213 23:17:13 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 23:17:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2218 --------------------------#
[32m[20221213 23:17:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0022 |          48.4018 |          15.5951 |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0038 |          47.0424 |          15.5710 |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0067 |          46.8703 |          15.5877 |
[32m[20221213 23:17:13 @agent_ppo2.py:185][0m |          -0.0023 |          47.2813 |          15.5933 |
[32m[20221213 23:17:14 @agent_ppo2.py:185][0m |          -0.0075 |          46.5654 |          15.5985 |
[32m[20221213 23:17:14 @agent_ppo2.py:185][0m |          -0.0001 |          51.0293 |          15.5924 |
[32m[20221213 23:17:14 @agent_ppo2.py:185][0m |          -0.0097 |          46.2896 |          15.5948 |
[32m[20221213 23:17:14 @agent_ppo2.py:185][0m |          -0.0094 |          46.1856 |          15.6071 |
[32m[20221213 23:17:14 @agent_ppo2.py:185][0m |          -0.0106 |          45.9772 |          15.6147 |
[32m[20221213 23:17:14 @agent_ppo2.py:185][0m |          -0.0086 |          45.9917 |          15.6299 |
[32m[20221213 23:17:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.89
[32m[20221213 23:17:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.95
[32m[20221213 23:17:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.91
[32m[20221213 23:17:14 @agent_ppo2.py:143][0m Total time:       4.70 min
[32m[20221213 23:17:14 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221213 23:17:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2219 --------------------------#
[32m[20221213 23:17:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:14 @agent_ppo2.py:185][0m |          -0.0040 |          47.1368 |          15.5785 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0085 |          45.3903 |          15.5512 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0057 |          44.8653 |          15.5512 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0025 |          47.2178 |          15.5240 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0105 |          44.4618 |          15.5169 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0086 |          44.4065 |          15.5368 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0092 |          44.4823 |          15.5179 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0106 |          44.3515 |          15.5062 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0121 |          44.0163 |          15.5243 |
[32m[20221213 23:17:15 @agent_ppo2.py:185][0m |          -0.0079 |          44.2075 |          15.5152 |
[32m[20221213 23:17:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:17:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.94
[32m[20221213 23:17:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.93
[32m[20221213 23:17:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.89
[32m[20221213 23:17:15 @agent_ppo2.py:143][0m Total time:       4.72 min
[32m[20221213 23:17:15 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 23:17:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2220 --------------------------#
[32m[20221213 23:17:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:17:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |           0.0052 |          44.9523 |          15.6078 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |          -0.0068 |          42.6250 |          15.6091 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |          -0.0035 |          42.7796 |          15.6176 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |          -0.0068 |          41.5928 |          15.6130 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |          -0.0011 |          44.5107 |          15.6134 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |          -0.0088 |          41.3122 |          15.5989 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |           0.0149 |          51.8450 |          15.6107 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |          -0.0109 |          41.1473 |          15.6162 |
[32m[20221213 23:17:16 @agent_ppo2.py:185][0m |          -0.0012 |          43.5055 |          15.6301 |
[32m[20221213 23:17:17 @agent_ppo2.py:185][0m |          -0.0109 |          40.4878 |          15.6241 |
[32m[20221213 23:17:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.54
[32m[20221213 23:17:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.03
[32m[20221213 23:17:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.52
[32m[20221213 23:17:17 @agent_ppo2.py:143][0m Total time:       4.74 min
[32m[20221213 23:17:17 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221213 23:17:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2221 --------------------------#
[32m[20221213 23:17:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:17 @agent_ppo2.py:185][0m |          -0.0011 |          46.4053 |          15.6109 |
[32m[20221213 23:17:17 @agent_ppo2.py:185][0m |          -0.0049 |          45.0906 |          15.6109 |
[32m[20221213 23:17:17 @agent_ppo2.py:185][0m |          -0.0073 |          44.6041 |          15.6235 |
[32m[20221213 23:17:17 @agent_ppo2.py:185][0m |          -0.0092 |          44.2331 |          15.6067 |
[32m[20221213 23:17:17 @agent_ppo2.py:185][0m |          -0.0076 |          43.9205 |          15.6214 |
[32m[20221213 23:17:17 @agent_ppo2.py:185][0m |          -0.0105 |          43.7401 |          15.6119 |
[32m[20221213 23:17:18 @agent_ppo2.py:185][0m |          -0.0082 |          44.0777 |          15.6284 |
[32m[20221213 23:17:18 @agent_ppo2.py:185][0m |          -0.0118 |          43.2615 |          15.6130 |
[32m[20221213 23:17:18 @agent_ppo2.py:185][0m |          -0.0121 |          43.0869 |          15.6260 |
[32m[20221213 23:17:18 @agent_ppo2.py:185][0m |          -0.0092 |          43.0529 |          15.6290 |
[32m[20221213 23:17:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.19
[32m[20221213 23:17:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.44
[32m[20221213 23:17:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.46
[32m[20221213 23:17:18 @agent_ppo2.py:143][0m Total time:       4.77 min
[32m[20221213 23:17:18 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 23:17:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2222 --------------------------#
[32m[20221213 23:17:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:18 @agent_ppo2.py:185][0m |           0.0112 |          42.7037 |          15.6580 |
[32m[20221213 23:17:18 @agent_ppo2.py:185][0m |          -0.0101 |          38.5131 |          15.6521 |
[32m[20221213 23:17:18 @agent_ppo2.py:185][0m |           0.0007 |          38.4535 |          15.6391 |
[32m[20221213 23:17:19 @agent_ppo2.py:185][0m |          -0.0049 |          36.4834 |          15.6592 |
[32m[20221213 23:17:19 @agent_ppo2.py:185][0m |          -0.0099 |          35.7482 |          15.6378 |
[32m[20221213 23:17:19 @agent_ppo2.py:185][0m |          -0.0080 |          35.4840 |          15.6548 |
[32m[20221213 23:17:19 @agent_ppo2.py:185][0m |          -0.0104 |          35.3632 |          15.6257 |
[32m[20221213 23:17:19 @agent_ppo2.py:185][0m |          -0.0113 |          34.8989 |          15.6280 |
[32m[20221213 23:17:19 @agent_ppo2.py:185][0m |          -0.0138 |          34.4622 |          15.6298 |
[32m[20221213 23:17:19 @agent_ppo2.py:185][0m |          -0.0105 |          34.3238 |          15.6126 |
[32m[20221213 23:17:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.07
[32m[20221213 23:17:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.58
[32m[20221213 23:17:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.37
[32m[20221213 23:17:19 @agent_ppo2.py:143][0m Total time:       4.79 min
[32m[20221213 23:17:19 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221213 23:17:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2223 --------------------------#
[32m[20221213 23:17:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |           0.0007 |          36.8151 |          15.6807 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0080 |          30.3552 |          15.6581 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0030 |          29.2720 |          15.6459 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0035 |          28.7036 |          15.6477 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0073 |          28.0827 |          15.6147 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0037 |          27.7157 |          15.6203 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |           0.0018 |          29.0897 |          15.6111 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0140 |          27.1980 |          15.5902 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0053 |          27.3202 |          15.5900 |
[32m[20221213 23:17:20 @agent_ppo2.py:185][0m |          -0.0081 |          26.7915 |          15.5832 |
[32m[20221213 23:17:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.65
[32m[20221213 23:17:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.48
[32m[20221213 23:17:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.09
[32m[20221213 23:17:20 @agent_ppo2.py:143][0m Total time:       4.81 min
[32m[20221213 23:17:20 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 23:17:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2224 --------------------------#
[32m[20221213 23:17:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0004 |          48.1010 |          15.6648 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0095 |          46.8630 |          15.6711 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0094 |          46.5297 |          15.6667 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0084 |          46.1634 |          15.6574 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0108 |          45.9827 |          15.6424 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |           0.0022 |          49.4112 |          15.6737 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0061 |          46.3718 |          15.6633 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0120 |          45.4640 |          15.6784 |
[32m[20221213 23:17:21 @agent_ppo2.py:185][0m |          -0.0092 |          45.5402 |          15.6604 |
[32m[20221213 23:17:22 @agent_ppo2.py:185][0m |          -0.0117 |          44.9557 |          15.6580 |
[32m[20221213 23:17:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:17:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.99
[32m[20221213 23:17:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.98
[32m[20221213 23:17:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.91
[32m[20221213 23:17:22 @agent_ppo2.py:143][0m Total time:       4.83 min
[32m[20221213 23:17:22 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221213 23:17:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2225 --------------------------#
[32m[20221213 23:17:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:22 @agent_ppo2.py:185][0m |          -0.0029 |          49.6274 |          15.5482 |
[32m[20221213 23:17:22 @agent_ppo2.py:185][0m |          -0.0026 |          48.3743 |          15.5227 |
[32m[20221213 23:17:22 @agent_ppo2.py:185][0m |          -0.0051 |          48.1797 |          15.5186 |
[32m[20221213 23:17:22 @agent_ppo2.py:185][0m |          -0.0056 |          47.9264 |          15.5073 |
[32m[20221213 23:17:22 @agent_ppo2.py:185][0m |           0.0009 |          48.9412 |          15.5169 |
[32m[20221213 23:17:22 @agent_ppo2.py:185][0m |          -0.0061 |          47.7470 |          15.4885 |
[32m[20221213 23:17:23 @agent_ppo2.py:185][0m |          -0.0094 |          47.4708 |          15.4923 |
[32m[20221213 23:17:23 @agent_ppo2.py:185][0m |          -0.0080 |          47.4576 |          15.5040 |
[32m[20221213 23:17:23 @agent_ppo2.py:185][0m |          -0.0094 |          47.4367 |          15.4924 |
[32m[20221213 23:17:23 @agent_ppo2.py:185][0m |          -0.0087 |          47.4206 |          15.4908 |
[32m[20221213 23:17:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.54
[32m[20221213 23:17:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.36
[32m[20221213 23:17:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.59
[32m[20221213 23:17:23 @agent_ppo2.py:143][0m Total time:       4.85 min
[32m[20221213 23:17:23 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 23:17:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2226 --------------------------#
[32m[20221213 23:17:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:23 @agent_ppo2.py:185][0m |          -0.0012 |          55.0226 |          15.7123 |
[32m[20221213 23:17:23 @agent_ppo2.py:185][0m |          -0.0071 |          53.5957 |          15.7015 |
[32m[20221213 23:17:23 @agent_ppo2.py:185][0m |          -0.0092 |          53.3558 |          15.6980 |
[32m[20221213 23:17:24 @agent_ppo2.py:185][0m |          -0.0117 |          52.9383 |          15.6793 |
[32m[20221213 23:17:24 @agent_ppo2.py:185][0m |          -0.0126 |          52.7657 |          15.6829 |
[32m[20221213 23:17:24 @agent_ppo2.py:185][0m |          -0.0002 |          55.7083 |          15.6747 |
[32m[20221213 23:17:24 @agent_ppo2.py:185][0m |          -0.0135 |          52.2220 |          15.6864 |
[32m[20221213 23:17:24 @agent_ppo2.py:185][0m |          -0.0114 |          52.2073 |          15.6472 |
[32m[20221213 23:17:24 @agent_ppo2.py:185][0m |          -0.0040 |          55.3381 |          15.6743 |
[32m[20221213 23:17:24 @agent_ppo2.py:185][0m |          -0.0087 |          52.2461 |          15.6676 |
[32m[20221213 23:17:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.66
[32m[20221213 23:17:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.01
[32m[20221213 23:17:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.70
[32m[20221213 23:17:24 @agent_ppo2.py:143][0m Total time:       4.87 min
[32m[20221213 23:17:24 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221213 23:17:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2227 --------------------------#
[32m[20221213 23:17:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0001 |          48.6964 |          15.4979 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0048 |          44.5840 |          15.4812 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0019 |          47.3103 |          15.4930 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0102 |          43.6237 |          15.4994 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0135 |          43.1583 |          15.4972 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0129 |          42.9512 |          15.5054 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0147 |          42.7834 |          15.5040 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0127 |          42.5667 |          15.5278 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0165 |          42.6170 |          15.5159 |
[32m[20221213 23:17:25 @agent_ppo2.py:185][0m |          -0.0167 |          42.2695 |          15.5202 |
[32m[20221213 23:17:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.18
[32m[20221213 23:17:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.29
[32m[20221213 23:17:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.46
[32m[20221213 23:17:26 @agent_ppo2.py:143][0m Total time:       4.89 min
[32m[20221213 23:17:26 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 23:17:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2228 --------------------------#
[32m[20221213 23:17:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0010 |          43.0183 |          15.4829 |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0029 |          40.5214 |          15.4805 |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0085 |          38.8368 |          15.5007 |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0111 |          37.6152 |          15.4942 |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0091 |          36.8443 |          15.5132 |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0092 |          36.3078 |          15.5094 |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0098 |          35.8778 |          15.5173 |
[32m[20221213 23:17:26 @agent_ppo2.py:185][0m |          -0.0111 |          35.5730 |          15.5349 |
[32m[20221213 23:17:27 @agent_ppo2.py:185][0m |          -0.0120 |          35.2476 |          15.5238 |
[32m[20221213 23:17:27 @agent_ppo2.py:185][0m |          -0.0120 |          34.9665 |          15.5385 |
[32m[20221213 23:17:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.61
[32m[20221213 23:17:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.54
[32m[20221213 23:17:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.64
[32m[20221213 23:17:27 @agent_ppo2.py:143][0m Total time:       4.91 min
[32m[20221213 23:17:27 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221213 23:17:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2229 --------------------------#
[32m[20221213 23:17:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:27 @agent_ppo2.py:185][0m |           0.0023 |          43.3651 |          15.6913 |
[32m[20221213 23:17:27 @agent_ppo2.py:185][0m |          -0.0035 |          31.6165 |          15.7003 |
[32m[20221213 23:17:27 @agent_ppo2.py:185][0m |          -0.0004 |          29.8765 |          15.6776 |
[32m[20221213 23:17:27 @agent_ppo2.py:185][0m |          -0.0042 |          28.6567 |          15.6962 |
[32m[20221213 23:17:27 @agent_ppo2.py:185][0m |          -0.0161 |          28.2714 |          15.6924 |
[32m[20221213 23:17:28 @agent_ppo2.py:185][0m |          -0.0056 |          27.7937 |          15.6849 |
[32m[20221213 23:17:28 @agent_ppo2.py:185][0m |          -0.0095 |          28.6802 |          15.6842 |
[32m[20221213 23:17:28 @agent_ppo2.py:185][0m |          -0.0067 |          27.4437 |          15.6929 |
[32m[20221213 23:17:28 @agent_ppo2.py:185][0m |          -0.0134 |          26.6337 |          15.6944 |
[32m[20221213 23:17:28 @agent_ppo2.py:185][0m |          -0.0096 |          26.8006 |          15.6974 |
[32m[20221213 23:17:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:17:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.31
[32m[20221213 23:17:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.31
[32m[20221213 23:17:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.18
[32m[20221213 23:17:28 @agent_ppo2.py:143][0m Total time:       4.93 min
[32m[20221213 23:17:28 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 23:17:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2230 --------------------------#
[32m[20221213 23:17:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:17:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:28 @agent_ppo2.py:185][0m |           0.0010 |          49.8203 |          15.7576 |
[32m[20221213 23:17:28 @agent_ppo2.py:185][0m |          -0.0038 |          48.5113 |          15.7538 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |          -0.0048 |          48.4725 |          15.7440 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |           0.0095 |          52.2709 |          15.7499 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |          -0.0045 |          48.2364 |          15.7692 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |          -0.0062 |          47.8298 |          15.7783 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |          -0.0098 |          47.8108 |          15.7673 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |          -0.0073 |          48.0016 |          15.7838 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |          -0.0095 |          47.7392 |          15.7758 |
[32m[20221213 23:17:29 @agent_ppo2.py:185][0m |          -0.0093 |          47.7661 |          15.7882 |
[32m[20221213 23:17:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.43
[32m[20221213 23:17:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.45
[32m[20221213 23:17:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.69
[32m[20221213 23:17:29 @agent_ppo2.py:143][0m Total time:       4.95 min
[32m[20221213 23:17:29 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221213 23:17:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2231 --------------------------#
[32m[20221213 23:17:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |           0.0000 |          46.1057 |          15.7518 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0023 |          44.4059 |          15.7575 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0070 |          43.9724 |          15.7554 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0061 |          43.8356 |          15.7550 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0043 |          43.4884 |          15.7743 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0078 |          43.2758 |          15.7758 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0106 |          43.1228 |          15.7750 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0089 |          43.1026 |          15.7848 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0038 |          45.2231 |          15.7859 |
[32m[20221213 23:17:30 @agent_ppo2.py:185][0m |          -0.0067 |          42.9659 |          15.7820 |
[32m[20221213 23:17:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.05
[32m[20221213 23:17:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.54
[32m[20221213 23:17:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.33
[32m[20221213 23:17:31 @agent_ppo2.py:143][0m Total time:       4.98 min
[32m[20221213 23:17:31 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 23:17:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2232 --------------------------#
[32m[20221213 23:17:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0022 |          46.0809 |          15.6603 |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0059 |          44.1751 |          15.6248 |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0127 |          43.6191 |          15.6412 |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0072 |          43.3577 |          15.6362 |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0131 |          43.0398 |          15.6318 |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0135 |          42.8881 |          15.6409 |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0133 |          42.6412 |          15.6315 |
[32m[20221213 23:17:31 @agent_ppo2.py:185][0m |          -0.0134 |          42.4990 |          15.6416 |
[32m[20221213 23:17:32 @agent_ppo2.py:185][0m |          -0.0131 |          42.4624 |          15.6388 |
[32m[20221213 23:17:32 @agent_ppo2.py:185][0m |          -0.0182 |          42.3598 |          15.6352 |
[32m[20221213 23:17:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.38
[32m[20221213 23:17:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.50
[32m[20221213 23:17:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.44
[32m[20221213 23:17:32 @agent_ppo2.py:143][0m Total time:       5.00 min
[32m[20221213 23:17:32 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221213 23:17:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2233 --------------------------#
[32m[20221213 23:17:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:32 @agent_ppo2.py:185][0m |           0.0008 |          39.0525 |          15.7861 |
[32m[20221213 23:17:32 @agent_ppo2.py:185][0m |          -0.0048 |          36.5879 |          15.7763 |
[32m[20221213 23:17:32 @agent_ppo2.py:185][0m |           0.0020 |          35.7451 |          15.7793 |
[32m[20221213 23:17:32 @agent_ppo2.py:185][0m |          -0.0036 |          35.5299 |          15.7729 |
[32m[20221213 23:17:32 @agent_ppo2.py:185][0m |          -0.0038 |          35.3326 |          15.7750 |
[32m[20221213 23:17:33 @agent_ppo2.py:185][0m |          -0.0128 |          34.8785 |          15.7750 |
[32m[20221213 23:17:33 @agent_ppo2.py:185][0m |          -0.0109 |          34.8489 |          15.7785 |
[32m[20221213 23:17:33 @agent_ppo2.py:185][0m |          -0.0064 |          34.9393 |          15.7696 |
[32m[20221213 23:17:33 @agent_ppo2.py:185][0m |          -0.0077 |          34.5482 |          15.7732 |
[32m[20221213 23:17:33 @agent_ppo2.py:185][0m |          -0.0118 |          34.5514 |          15.8034 |
[32m[20221213 23:17:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.37
[32m[20221213 23:17:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.96
[32m[20221213 23:17:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.33
[32m[20221213 23:17:33 @agent_ppo2.py:143][0m Total time:       5.02 min
[32m[20221213 23:17:33 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 23:17:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2234 --------------------------#
[32m[20221213 23:17:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:33 @agent_ppo2.py:185][0m |           0.0012 |          34.2647 |          15.5371 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0092 |          31.3813 |          15.5449 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0073 |          30.8820 |          15.5416 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0079 |          30.3316 |          15.5514 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0119 |          30.2242 |          15.5457 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0055 |          30.0335 |          15.5557 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0054 |          31.6294 |          15.5485 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0094 |          29.6268 |          15.5531 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0124 |          29.5156 |          15.5538 |
[32m[20221213 23:17:34 @agent_ppo2.py:185][0m |          -0.0124 |          29.3694 |          15.5393 |
[32m[20221213 23:17:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:17:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.36
[32m[20221213 23:17:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.76
[32m[20221213 23:17:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.17
[32m[20221213 23:17:34 @agent_ppo2.py:143][0m Total time:       5.04 min
[32m[20221213 23:17:34 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221213 23:17:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2235 --------------------------#
[32m[20221213 23:17:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0036 |          49.7866 |          15.7176 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0110 |          48.6310 |          15.7152 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0049 |          50.5709 |          15.7187 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0115 |          47.8509 |          15.7118 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0147 |          47.6032 |          15.7237 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0142 |          47.2996 |          15.7311 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0148 |          47.2917 |          15.7397 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0138 |          47.1203 |          15.7430 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0125 |          46.9552 |          15.7380 |
[32m[20221213 23:17:35 @agent_ppo2.py:185][0m |          -0.0161 |          46.9452 |          15.7339 |
[32m[20221213 23:17:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.38
[32m[20221213 23:17:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.45
[32m[20221213 23:17:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.65
[32m[20221213 23:17:36 @agent_ppo2.py:143][0m Total time:       5.06 min
[32m[20221213 23:17:36 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 23:17:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2236 --------------------------#
[32m[20221213 23:17:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:36 @agent_ppo2.py:185][0m |           0.0015 |          45.2001 |          15.7234 |
[32m[20221213 23:17:36 @agent_ppo2.py:185][0m |           0.0022 |          44.5253 |          15.7117 |
[32m[20221213 23:17:36 @agent_ppo2.py:185][0m |           0.0059 |          46.9805 |          15.7095 |
[32m[20221213 23:17:36 @agent_ppo2.py:185][0m |          -0.0035 |          42.3147 |          15.7153 |
[32m[20221213 23:17:36 @agent_ppo2.py:185][0m |          -0.0092 |          41.9754 |          15.7090 |
[32m[20221213 23:17:36 @agent_ppo2.py:185][0m |          -0.0100 |          41.7330 |          15.7054 |
[32m[20221213 23:17:36 @agent_ppo2.py:185][0m |          -0.0111 |          41.5749 |          15.7102 |
[32m[20221213 23:17:37 @agent_ppo2.py:185][0m |          -0.0080 |          41.6688 |          15.7039 |
[32m[20221213 23:17:37 @agent_ppo2.py:185][0m |          -0.0070 |          41.3234 |          15.6979 |
[32m[20221213 23:17:37 @agent_ppo2.py:185][0m |          -0.0113 |          41.3117 |          15.7290 |
[32m[20221213 23:17:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.53
[32m[20221213 23:17:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.46
[32m[20221213 23:17:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.31
[32m[20221213 23:17:37 @agent_ppo2.py:143][0m Total time:       5.08 min
[32m[20221213 23:17:37 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221213 23:17:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2237 --------------------------#
[32m[20221213 23:17:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:37 @agent_ppo2.py:185][0m |           0.0018 |          26.9357 |          15.6903 |
[32m[20221213 23:17:37 @agent_ppo2.py:185][0m |           0.0020 |          23.4950 |          15.6650 |
[32m[20221213 23:17:37 @agent_ppo2.py:185][0m |           0.0043 |          24.4636 |          15.6590 |
[32m[20221213 23:17:37 @agent_ppo2.py:185][0m |          -0.0021 |          21.3678 |          15.6537 |
[32m[20221213 23:17:38 @agent_ppo2.py:185][0m |          -0.0082 |          20.8082 |          15.6614 |
[32m[20221213 23:17:38 @agent_ppo2.py:185][0m |          -0.0218 |          20.4768 |          15.6546 |
[32m[20221213 23:17:38 @agent_ppo2.py:185][0m |          -0.0137 |          20.2820 |          15.6559 |
[32m[20221213 23:17:38 @agent_ppo2.py:185][0m |          -0.0189 |          19.8234 |          15.6417 |
[32m[20221213 23:17:38 @agent_ppo2.py:185][0m |          -0.0141 |          19.6852 |          15.6491 |
[32m[20221213 23:17:38 @agent_ppo2.py:185][0m |          -0.0160 |          19.4490 |          15.6475 |
[32m[20221213 23:17:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.28
[32m[20221213 23:17:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.11
[32m[20221213 23:17:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.90
[32m[20221213 23:17:38 @agent_ppo2.py:143][0m Total time:       5.10 min
[32m[20221213 23:17:38 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 23:17:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2238 --------------------------#
[32m[20221213 23:17:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:38 @agent_ppo2.py:185][0m |          -0.0016 |          27.1613 |          15.7794 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0061 |          22.4355 |          15.7461 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0104 |          19.9624 |          15.7374 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0092 |          18.8481 |          15.7374 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |           0.0079 |          20.0565 |          15.7302 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0164 |          17.3955 |          15.7130 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0092 |          16.9117 |          15.7165 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0150 |          16.1711 |          15.7089 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0153 |          16.0010 |          15.7062 |
[32m[20221213 23:17:39 @agent_ppo2.py:185][0m |          -0.0138 |          15.5376 |          15.6985 |
[32m[20221213 23:17:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:17:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.82
[32m[20221213 23:17:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.73
[32m[20221213 23:17:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.17
[32m[20221213 23:17:39 @agent_ppo2.py:143][0m Total time:       5.12 min
[32m[20221213 23:17:39 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221213 23:17:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2239 --------------------------#
[32m[20221213 23:17:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0013 |          48.9311 |          15.7732 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0052 |          46.7104 |          15.7973 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0048 |          45.8483 |          15.7882 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0085 |          45.5155 |          15.7873 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0047 |          45.7487 |          15.7820 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0091 |          44.9614 |          15.7874 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0071 |          44.7925 |          15.7777 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0090 |          44.7131 |          15.7913 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0130 |          44.4741 |          15.7865 |
[32m[20221213 23:17:40 @agent_ppo2.py:185][0m |          -0.0113 |          44.5377 |          15.7871 |
[32m[20221213 23:17:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.07
[32m[20221213 23:17:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.65
[32m[20221213 23:17:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.97
[32m[20221213 23:17:41 @agent_ppo2.py:143][0m Total time:       5.14 min
[32m[20221213 23:17:41 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 23:17:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2240 --------------------------#
[32m[20221213 23:17:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:17:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:41 @agent_ppo2.py:185][0m |          -0.0015 |          57.2925 |          15.7862 |
[32m[20221213 23:17:41 @agent_ppo2.py:185][0m |           0.0049 |          57.1249 |          15.7597 |
[32m[20221213 23:17:41 @agent_ppo2.py:185][0m |          -0.0067 |          55.2737 |          15.7734 |
[32m[20221213 23:17:41 @agent_ppo2.py:185][0m |          -0.0054 |          54.7690 |          15.7719 |
[32m[20221213 23:17:41 @agent_ppo2.py:185][0m |          -0.0049 |          54.5464 |          15.7959 |
[32m[20221213 23:17:41 @agent_ppo2.py:185][0m |          -0.0028 |          55.1063 |          15.7747 |
[32m[20221213 23:17:41 @agent_ppo2.py:185][0m |          -0.0096 |          54.6177 |          15.7694 |
[32m[20221213 23:17:42 @agent_ppo2.py:185][0m |          -0.0085 |          54.2333 |          15.7809 |
[32m[20221213 23:17:42 @agent_ppo2.py:185][0m |          -0.0100 |          53.9721 |          15.7814 |
[32m[20221213 23:17:42 @agent_ppo2.py:185][0m |          -0.0092 |          53.9474 |          15.7777 |
[32m[20221213 23:17:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.09
[32m[20221213 23:17:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.68
[32m[20221213 23:17:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.86
[32m[20221213 23:17:42 @agent_ppo2.py:143][0m Total time:       5.16 min
[32m[20221213 23:17:42 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221213 23:17:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2241 --------------------------#
[32m[20221213 23:17:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:42 @agent_ppo2.py:185][0m |           0.0028 |          45.7975 |          15.7054 |
[32m[20221213 23:17:42 @agent_ppo2.py:185][0m |          -0.0029 |          43.6820 |          15.6828 |
[32m[20221213 23:17:42 @agent_ppo2.py:185][0m |          -0.0055 |          43.0772 |          15.6925 |
[32m[20221213 23:17:42 @agent_ppo2.py:185][0m |          -0.0008 |          43.3772 |          15.6894 |
[32m[20221213 23:17:43 @agent_ppo2.py:185][0m |           0.0100 |          48.4428 |          15.6808 |
[32m[20221213 23:17:43 @agent_ppo2.py:185][0m |           0.0052 |          48.6242 |          15.6682 |
[32m[20221213 23:17:43 @agent_ppo2.py:185][0m |          -0.0090 |          42.1879 |          15.6570 |
[32m[20221213 23:17:43 @agent_ppo2.py:185][0m |          -0.0122 |          41.8866 |          15.6673 |
[32m[20221213 23:17:43 @agent_ppo2.py:185][0m |          -0.0059 |          42.6278 |          15.6689 |
[32m[20221213 23:17:43 @agent_ppo2.py:185][0m |          -0.0120 |          41.5954 |          15.6547 |
[32m[20221213 23:17:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.33
[32m[20221213 23:17:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.45
[32m[20221213 23:17:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.75
[32m[20221213 23:17:43 @agent_ppo2.py:143][0m Total time:       5.19 min
[32m[20221213 23:17:43 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 23:17:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2242 --------------------------#
[32m[20221213 23:17:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:43 @agent_ppo2.py:185][0m |           0.0031 |          32.1582 |          15.8041 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0064 |          27.1567 |          15.7938 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0078 |          25.1886 |          15.7803 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0133 |          24.3281 |          15.7862 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0069 |          23.7296 |          15.7741 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0143 |          23.1959 |          15.7846 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0153 |          22.7457 |          15.7796 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0168 |          22.3730 |          15.7636 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0180 |          22.2529 |          15.7649 |
[32m[20221213 23:17:44 @agent_ppo2.py:185][0m |          -0.0152 |          21.9507 |          15.7566 |
[32m[20221213 23:17:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.62
[32m[20221213 23:17:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.07
[32m[20221213 23:17:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.75
[32m[20221213 23:17:44 @agent_ppo2.py:143][0m Total time:       5.21 min
[32m[20221213 23:17:44 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221213 23:17:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2243 --------------------------#
[32m[20221213 23:17:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0034 |          53.2234 |          15.7473 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0076 |          52.1839 |          15.7303 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0066 |          51.5712 |          15.7566 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0088 |          51.4193 |          15.7446 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0075 |          51.1590 |          15.7416 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0109 |          51.0666 |          15.7268 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0025 |          52.8435 |          15.7232 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0112 |          50.6064 |          15.7086 |
[32m[20221213 23:17:45 @agent_ppo2.py:185][0m |          -0.0074 |          50.4742 |          15.7293 |
[32m[20221213 23:17:46 @agent_ppo2.py:185][0m |          -0.0123 |          50.5273 |          15.7296 |
[32m[20221213 23:17:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:17:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.13
[32m[20221213 23:17:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.96
[32m[20221213 23:17:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.76
[32m[20221213 23:17:46 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221213 23:17:46 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 23:17:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2244 --------------------------#
[32m[20221213 23:17:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:46 @agent_ppo2.py:185][0m |           0.0047 |          40.3877 |          15.6199 |
[32m[20221213 23:17:46 @agent_ppo2.py:185][0m |          -0.0034 |          36.0156 |          15.5914 |
[32m[20221213 23:17:46 @agent_ppo2.py:185][0m |          -0.0110 |          34.7054 |          15.5957 |
[32m[20221213 23:17:46 @agent_ppo2.py:185][0m |           0.0050 |          38.7209 |          15.5919 |
[32m[20221213 23:17:46 @agent_ppo2.py:185][0m |          -0.0144 |          33.8575 |          15.5780 |
[32m[20221213 23:17:46 @agent_ppo2.py:185][0m |          -0.0134 |          33.0345 |          15.5781 |
[32m[20221213 23:17:47 @agent_ppo2.py:185][0m |          -0.0217 |          32.6665 |          15.5727 |
[32m[20221213 23:17:47 @agent_ppo2.py:185][0m |          -0.0171 |          32.4147 |          15.5775 |
[32m[20221213 23:17:47 @agent_ppo2.py:185][0m |          -0.0136 |          31.9846 |          15.5832 |
[32m[20221213 23:17:47 @agent_ppo2.py:185][0m |          -0.0124 |          31.8107 |          15.5904 |
[32m[20221213 23:17:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:17:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.91
[32m[20221213 23:17:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.40
[32m[20221213 23:17:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.20
[32m[20221213 23:17:47 @agent_ppo2.py:143][0m Total time:       5.25 min
[32m[20221213 23:17:47 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221213 23:17:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2245 --------------------------#
[32m[20221213 23:17:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:47 @agent_ppo2.py:185][0m |           0.0027 |          53.5668 |          15.6603 |
[32m[20221213 23:17:47 @agent_ppo2.py:185][0m |           0.0011 |          54.6253 |          15.6645 |
[32m[20221213 23:17:47 @agent_ppo2.py:185][0m |          -0.0004 |          53.2587 |          15.6445 |
[32m[20221213 23:17:48 @agent_ppo2.py:185][0m |          -0.0028 |          52.2052 |          15.6457 |
[32m[20221213 23:17:48 @agent_ppo2.py:185][0m |          -0.0108 |          51.4433 |          15.6517 |
[32m[20221213 23:17:48 @agent_ppo2.py:185][0m |          -0.0106 |          51.1476 |          15.6402 |
[32m[20221213 23:17:48 @agent_ppo2.py:185][0m |          -0.0075 |          51.0001 |          15.6348 |
[32m[20221213 23:17:48 @agent_ppo2.py:185][0m |          -0.0104 |          50.7822 |          15.6516 |
[32m[20221213 23:17:48 @agent_ppo2.py:185][0m |          -0.0110 |          50.5264 |          15.6387 |
[32m[20221213 23:17:48 @agent_ppo2.py:185][0m |          -0.0103 |          50.5002 |          15.6404 |
[32m[20221213 23:17:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.20
[32m[20221213 23:17:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.59
[32m[20221213 23:17:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.08
[32m[20221213 23:17:48 @agent_ppo2.py:143][0m Total time:       5.27 min
[32m[20221213 23:17:48 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 23:17:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2246 --------------------------#
[32m[20221213 23:17:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |           0.0048 |          33.1218 |          15.7993 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0023 |          29.9976 |          15.7858 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0005 |          28.6622 |          15.7729 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0085 |          27.8743 |          15.7527 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |           0.0001 |          28.4807 |          15.7560 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0090 |          26.9612 |          15.7465 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0109 |          26.6166 |          15.7554 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0109 |          26.3140 |          15.7528 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0086 |          25.9549 |          15.7514 |
[32m[20221213 23:17:49 @agent_ppo2.py:185][0m |          -0.0095 |          25.7307 |          15.7388 |
[32m[20221213 23:17:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.26
[32m[20221213 23:17:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.23
[32m[20221213 23:17:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.29
[32m[20221213 23:17:49 @agent_ppo2.py:143][0m Total time:       5.29 min
[32m[20221213 23:17:49 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221213 23:17:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2247 --------------------------#
[32m[20221213 23:17:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |           0.0047 |          52.2493 |          15.6501 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0045 |          48.7068 |          15.6424 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0105 |          47.9614 |          15.6557 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0078 |          47.4650 |          15.6523 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0070 |          47.2173 |          15.6498 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0088 |          46.9821 |          15.6414 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0144 |          47.0900 |          15.6544 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0118 |          46.5854 |          15.6473 |
[32m[20221213 23:17:50 @agent_ppo2.py:185][0m |          -0.0042 |          47.5546 |          15.6610 |
[32m[20221213 23:17:51 @agent_ppo2.py:185][0m |          -0.0120 |          46.3374 |          15.6440 |
[32m[20221213 23:17:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.23
[32m[20221213 23:17:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.14
[32m[20221213 23:17:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.35
[32m[20221213 23:17:51 @agent_ppo2.py:143][0m Total time:       5.31 min
[32m[20221213 23:17:51 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 23:17:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2248 --------------------------#
[32m[20221213 23:17:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:51 @agent_ppo2.py:185][0m |           0.0049 |          49.5515 |          15.6871 |
[32m[20221213 23:17:51 @agent_ppo2.py:185][0m |          -0.0076 |          46.4597 |          15.6814 |
[32m[20221213 23:17:51 @agent_ppo2.py:185][0m |          -0.0070 |          45.5755 |          15.6795 |
[32m[20221213 23:17:51 @agent_ppo2.py:185][0m |          -0.0014 |          46.0506 |          15.6716 |
[32m[20221213 23:17:51 @agent_ppo2.py:185][0m |          -0.0105 |          44.4479 |          15.6990 |
[32m[20221213 23:17:51 @agent_ppo2.py:185][0m |          -0.0021 |          44.6722 |          15.7017 |
[32m[20221213 23:17:52 @agent_ppo2.py:185][0m |          -0.0096 |          43.6360 |          15.7011 |
[32m[20221213 23:17:52 @agent_ppo2.py:185][0m |          -0.0098 |          43.4309 |          15.6726 |
[32m[20221213 23:17:52 @agent_ppo2.py:185][0m |          -0.0054 |          44.6297 |          15.6917 |
[32m[20221213 23:17:52 @agent_ppo2.py:185][0m |          -0.0144 |          43.1964 |          15.6996 |
[32m[20221213 23:17:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:17:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.88
[32m[20221213 23:17:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.60
[32m[20221213 23:17:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.30
[32m[20221213 23:17:52 @agent_ppo2.py:143][0m Total time:       5.33 min
[32m[20221213 23:17:52 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221213 23:17:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2249 --------------------------#
[32m[20221213 23:17:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:52 @agent_ppo2.py:185][0m |          -0.0003 |          54.2244 |          15.7002 |
[32m[20221213 23:17:52 @agent_ppo2.py:185][0m |           0.0139 |          60.1020 |          15.6932 |
[32m[20221213 23:17:52 @agent_ppo2.py:185][0m |          -0.0087 |          51.9853 |          15.6781 |
[32m[20221213 23:17:53 @agent_ppo2.py:185][0m |          -0.0027 |          51.8679 |          15.6974 |
[32m[20221213 23:17:53 @agent_ppo2.py:185][0m |          -0.0092 |          50.9522 |          15.6761 |
[32m[20221213 23:17:53 @agent_ppo2.py:185][0m |          -0.0090 |          50.9157 |          15.7033 |
[32m[20221213 23:17:53 @agent_ppo2.py:185][0m |          -0.0091 |          50.7129 |          15.6905 |
[32m[20221213 23:17:53 @agent_ppo2.py:185][0m |          -0.0111 |          50.2622 |          15.6981 |
[32m[20221213 23:17:53 @agent_ppo2.py:185][0m |          -0.0011 |          53.1657 |          15.6877 |
[32m[20221213 23:17:53 @agent_ppo2.py:185][0m |          -0.0040 |          51.1642 |          15.7032 |
[32m[20221213 23:17:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:17:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.96
[32m[20221213 23:17:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.77
[32m[20221213 23:17:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.39
[32m[20221213 23:17:53 @agent_ppo2.py:143][0m Total time:       5.35 min
[32m[20221213 23:17:53 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 23:17:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2250 --------------------------#
[32m[20221213 23:17:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:17:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0025 |          55.1575 |          15.7640 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0069 |          53.5610 |          15.7265 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0055 |          53.3496 |          15.7325 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0086 |          52.8491 |          15.7415 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0092 |          52.5867 |          15.7436 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0121 |          52.4662 |          15.7473 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0116 |          52.2753 |          15.7472 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0124 |          52.2842 |          15.7453 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0063 |          52.7565 |          15.7484 |
[32m[20221213 23:17:54 @agent_ppo2.py:185][0m |          -0.0115 |          51.8619 |          15.7322 |
[32m[20221213 23:17:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:17:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.50
[32m[20221213 23:17:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.42
[32m[20221213 23:17:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.90
[32m[20221213 23:17:55 @agent_ppo2.py:143][0m Total time:       5.38 min
[32m[20221213 23:17:55 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221213 23:17:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2251 --------------------------#
[32m[20221213 23:17:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |          -0.0027 |          50.6391 |          15.6608 |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |          -0.0050 |          46.7496 |          15.6669 |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |          -0.0104 |          45.9383 |          15.6657 |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |          -0.0109 |          45.6173 |          15.6639 |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |           0.0033 |          48.0264 |          15.6682 |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |           0.0010 |          49.6984 |          15.6598 |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |          -0.0135 |          45.0746 |          15.6700 |
[32m[20221213 23:17:55 @agent_ppo2.py:185][0m |          -0.0148 |          44.9768 |          15.6623 |
[32m[20221213 23:17:56 @agent_ppo2.py:185][0m |          -0.0108 |          44.9217 |          15.6671 |
[32m[20221213 23:17:56 @agent_ppo2.py:185][0m |          -0.0135 |          44.7156 |          15.6667 |
[32m[20221213 23:17:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:17:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.11
[32m[20221213 23:17:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.04
[32m[20221213 23:17:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.19
[32m[20221213 23:17:56 @agent_ppo2.py:143][0m Total time:       5.40 min
[32m[20221213 23:17:56 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 23:17:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2252 --------------------------#
[32m[20221213 23:17:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:56 @agent_ppo2.py:185][0m |          -0.0008 |          48.8602 |          15.8873 |
[32m[20221213 23:17:56 @agent_ppo2.py:185][0m |          -0.0028 |          44.5058 |          15.8601 |
[32m[20221213 23:17:56 @agent_ppo2.py:185][0m |          -0.0078 |          42.8587 |          15.8538 |
[32m[20221213 23:17:56 @agent_ppo2.py:185][0m |          -0.0124 |          41.9650 |          15.8528 |
[32m[20221213 23:17:57 @agent_ppo2.py:185][0m |          -0.0118 |          41.3904 |          15.8625 |
[32m[20221213 23:17:57 @agent_ppo2.py:185][0m |          -0.0163 |          41.0100 |          15.8538 |
[32m[20221213 23:17:57 @agent_ppo2.py:185][0m |          -0.0142 |          40.5892 |          15.8608 |
[32m[20221213 23:17:57 @agent_ppo2.py:185][0m |          -0.0154 |          40.2363 |          15.8535 |
[32m[20221213 23:17:57 @agent_ppo2.py:185][0m |          -0.0137 |          40.0146 |          15.8552 |
[32m[20221213 23:17:57 @agent_ppo2.py:185][0m |          -0.0143 |          39.8375 |          15.8524 |
[32m[20221213 23:17:57 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 23:17:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.00
[32m[20221213 23:17:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.89
[32m[20221213 23:17:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.35
[32m[20221213 23:17:57 @agent_ppo2.py:143][0m Total time:       5.42 min
[32m[20221213 23:17:57 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221213 23:17:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2253 --------------------------#
[32m[20221213 23:17:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:17:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:57 @agent_ppo2.py:185][0m |          -0.0028 |          49.6201 |          15.7514 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0051 |          46.7615 |          15.7322 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |           0.0066 |          51.6158 |          15.7135 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0115 |          45.3938 |          15.7204 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0085 |          44.6879 |          15.7018 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0093 |          44.3843 |          15.7071 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0101 |          44.0859 |          15.7058 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0084 |          43.8565 |          15.7151 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0096 |          43.6838 |          15.7201 |
[32m[20221213 23:17:58 @agent_ppo2.py:185][0m |          -0.0123 |          43.4771 |          15.7028 |
[32m[20221213 23:17:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:17:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.01
[32m[20221213 23:17:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.70
[32m[20221213 23:17:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.74
[32m[20221213 23:17:58 @agent_ppo2.py:143][0m Total time:       5.44 min
[32m[20221213 23:17:58 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 23:17:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2254 --------------------------#
[32m[20221213 23:17:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:17:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0016 |          52.8164 |          15.6423 |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0049 |          51.3404 |          15.6431 |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0006 |          52.5024 |          15.6544 |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0087 |          50.7982 |          15.6377 |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0086 |          50.6838 |          15.6652 |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0052 |          51.1271 |          15.6611 |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0095 |          50.4529 |          15.6495 |
[32m[20221213 23:17:59 @agent_ppo2.py:185][0m |          -0.0121 |          50.3150 |          15.6527 |
[32m[20221213 23:18:00 @agent_ppo2.py:185][0m |          -0.0061 |          50.8262 |          15.6514 |
[32m[20221213 23:18:00 @agent_ppo2.py:185][0m |          -0.0109 |          50.3408 |          15.6569 |
[32m[20221213 23:18:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:18:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.70
[32m[20221213 23:18:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.80
[32m[20221213 23:18:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.61
[32m[20221213 23:18:00 @agent_ppo2.py:143][0m Total time:       5.46 min
[32m[20221213 23:18:00 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221213 23:18:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2255 --------------------------#
[32m[20221213 23:18:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:00 @agent_ppo2.py:185][0m |           0.0015 |          54.4092 |          15.7202 |
[32m[20221213 23:18:00 @agent_ppo2.py:185][0m |          -0.0004 |          50.9485 |          15.6856 |
[32m[20221213 23:18:00 @agent_ppo2.py:185][0m |          -0.0070 |          49.5444 |          15.6801 |
[32m[20221213 23:18:00 @agent_ppo2.py:185][0m |           0.0034 |          52.3328 |          15.6602 |
[32m[20221213 23:18:00 @agent_ppo2.py:185][0m |          -0.0083 |          48.9262 |          15.6494 |
[32m[20221213 23:18:01 @agent_ppo2.py:185][0m |          -0.0088 |          48.8164 |          15.6782 |
[32m[20221213 23:18:01 @agent_ppo2.py:185][0m |          -0.0106 |          48.3830 |          15.6781 |
[32m[20221213 23:18:01 @agent_ppo2.py:185][0m |          -0.0153 |          48.2148 |          15.6898 |
[32m[20221213 23:18:01 @agent_ppo2.py:185][0m |          -0.0112 |          48.5140 |          15.6768 |
[32m[20221213 23:18:01 @agent_ppo2.py:185][0m |          -0.0115 |          48.2246 |          15.6633 |
[32m[20221213 23:18:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.56
[32m[20221213 23:18:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.92
[32m[20221213 23:18:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.41
[32m[20221213 23:18:01 @agent_ppo2.py:143][0m Total time:       5.48 min
[32m[20221213 23:18:01 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 23:18:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2256 --------------------------#
[32m[20221213 23:18:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:01 @agent_ppo2.py:185][0m |          -0.0040 |          48.2706 |          15.8714 |
[32m[20221213 23:18:01 @agent_ppo2.py:185][0m |          -0.0080 |          45.7103 |          15.8641 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |          -0.0115 |          44.6993 |          15.8588 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |          -0.0100 |          44.0386 |          15.8526 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |          -0.0105 |          43.5055 |          15.8604 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |          -0.0006 |          45.3193 |          15.8533 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |          -0.0083 |          43.4255 |          15.8610 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |           0.0060 |          47.6011 |          15.8666 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |          -0.0129 |          42.8040 |          15.8407 |
[32m[20221213 23:18:02 @agent_ppo2.py:185][0m |          -0.0050 |          44.9685 |          15.8751 |
[32m[20221213 23:18:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.32
[32m[20221213 23:18:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.21
[32m[20221213 23:18:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.00
[32m[20221213 23:18:02 @agent_ppo2.py:143][0m Total time:       5.50 min
[32m[20221213 23:18:02 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221213 23:18:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2257 --------------------------#
[32m[20221213 23:18:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0020 |          53.9508 |          15.5688 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0020 |          52.1697 |          15.5400 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0082 |          51.3318 |          15.5370 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |           0.0016 |          56.0366 |          15.5160 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0071 |          50.8148 |          15.4972 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0092 |          50.3972 |          15.4910 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0097 |          50.3298 |          15.5100 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0097 |          50.1565 |          15.4976 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0096 |          50.0341 |          15.4938 |
[32m[20221213 23:18:03 @agent_ppo2.py:185][0m |          -0.0121 |          50.0827 |          15.4859 |
[32m[20221213 23:18:03 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:18:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.32
[32m[20221213 23:18:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.15
[32m[20221213 23:18:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.72
[32m[20221213 23:18:04 @agent_ppo2.py:143][0m Total time:       5.53 min
[32m[20221213 23:18:04 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 23:18:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2258 --------------------------#
[32m[20221213 23:18:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:04 @agent_ppo2.py:185][0m |           0.0008 |          57.1090 |          15.9587 |
[32m[20221213 23:18:04 @agent_ppo2.py:185][0m |          -0.0071 |          54.3899 |          15.9624 |
[32m[20221213 23:18:04 @agent_ppo2.py:185][0m |           0.0003 |          59.6401 |          15.9505 |
[32m[20221213 23:18:04 @agent_ppo2.py:185][0m |          -0.0081 |          53.5048 |          15.9531 |
[32m[20221213 23:18:04 @agent_ppo2.py:185][0m |          -0.0095 |          53.1389 |          15.9436 |
[32m[20221213 23:18:04 @agent_ppo2.py:185][0m |          -0.0124 |          52.9792 |          15.9443 |
[32m[20221213 23:18:04 @agent_ppo2.py:185][0m |          -0.0049 |          53.1741 |          15.9566 |
[32m[20221213 23:18:05 @agent_ppo2.py:185][0m |          -0.0124 |          52.3795 |          15.9673 |
[32m[20221213 23:18:05 @agent_ppo2.py:185][0m |          -0.0124 |          52.4682 |          15.9681 |
[32m[20221213 23:18:05 @agent_ppo2.py:185][0m |          -0.0026 |          56.0822 |          15.9529 |
[32m[20221213 23:18:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.84
[32m[20221213 23:18:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.79
[32m[20221213 23:18:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.34
[32m[20221213 23:18:05 @agent_ppo2.py:143][0m Total time:       5.55 min
[32m[20221213 23:18:05 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221213 23:18:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2259 --------------------------#
[32m[20221213 23:18:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:05 @agent_ppo2.py:185][0m |           0.0061 |          39.2643 |          15.5878 |
[32m[20221213 23:18:05 @agent_ppo2.py:185][0m |           0.0043 |          32.9378 |          15.5688 |
[32m[20221213 23:18:05 @agent_ppo2.py:185][0m |          -0.0099 |          31.2353 |          15.5732 |
[32m[20221213 23:18:05 @agent_ppo2.py:185][0m |          -0.0082 |          30.6381 |          15.5510 |
[32m[20221213 23:18:06 @agent_ppo2.py:185][0m |          -0.0072 |          30.1276 |          15.5416 |
[32m[20221213 23:18:06 @agent_ppo2.py:185][0m |          -0.0114 |          29.6597 |          15.5508 |
[32m[20221213 23:18:06 @agent_ppo2.py:185][0m |          -0.0057 |          29.2614 |          15.5298 |
[32m[20221213 23:18:06 @agent_ppo2.py:185][0m |          -0.0089 |          29.1582 |          15.5313 |
[32m[20221213 23:18:06 @agent_ppo2.py:185][0m |          -0.0080 |          28.7953 |          15.5149 |
[32m[20221213 23:18:06 @agent_ppo2.py:185][0m |          -0.0117 |          28.3962 |          15.5040 |
[32m[20221213 23:18:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.63
[32m[20221213 23:18:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.26
[32m[20221213 23:18:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.50
[32m[20221213 23:18:06 @agent_ppo2.py:143][0m Total time:       5.57 min
[32m[20221213 23:18:06 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 23:18:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2260 --------------------------#
[32m[20221213 23:18:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:18:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:06 @agent_ppo2.py:185][0m |          -0.0057 |          48.5457 |          15.6338 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0062 |          42.6993 |          15.6297 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0059 |          41.4589 |          15.6475 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0124 |          40.7626 |          15.6279 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0120 |          40.4949 |          15.6031 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0076 |          40.7150 |          15.6144 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0153 |          39.3259 |          15.5963 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0128 |          39.1237 |          15.6005 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0111 |          39.6585 |          15.6022 |
[32m[20221213 23:18:07 @agent_ppo2.py:185][0m |          -0.0185 |          38.8952 |          15.5824 |
[32m[20221213 23:18:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.77
[32m[20221213 23:18:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.48
[32m[20221213 23:18:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.08
[32m[20221213 23:18:07 @agent_ppo2.py:143][0m Total time:       5.59 min
[32m[20221213 23:18:07 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221213 23:18:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2261 --------------------------#
[32m[20221213 23:18:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0035 |          52.1613 |          15.7254 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0060 |          51.0892 |          15.6932 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0073 |          50.9014 |          15.7031 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0088 |          50.6404 |          15.7168 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0107 |          50.4358 |          15.6876 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0119 |          50.3171 |          15.7010 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0060 |          51.3245 |          15.6995 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0122 |          50.1265 |          15.7122 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0083 |          50.3000 |          15.6923 |
[32m[20221213 23:18:08 @agent_ppo2.py:185][0m |          -0.0043 |          50.6663 |          15.7145 |
[32m[20221213 23:18:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.55
[32m[20221213 23:18:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.45
[32m[20221213 23:18:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.20
[32m[20221213 23:18:09 @agent_ppo2.py:143][0m Total time:       5.61 min
[32m[20221213 23:18:09 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 23:18:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2262 --------------------------#
[32m[20221213 23:18:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:09 @agent_ppo2.py:185][0m |           0.0028 |          33.5998 |          15.6914 |
[32m[20221213 23:18:09 @agent_ppo2.py:185][0m |          -0.0068 |          29.1577 |          15.6780 |
[32m[20221213 23:18:09 @agent_ppo2.py:185][0m |          -0.0141 |          27.7821 |          15.6694 |
[32m[20221213 23:18:09 @agent_ppo2.py:185][0m |          -0.0084 |          26.7277 |          15.6578 |
[32m[20221213 23:18:09 @agent_ppo2.py:185][0m |          -0.0152 |          25.9110 |          15.6744 |
[32m[20221213 23:18:09 @agent_ppo2.py:185][0m |          -0.0113 |          25.3917 |          15.6614 |
[32m[20221213 23:18:09 @agent_ppo2.py:185][0m |          -0.0145 |          24.9744 |          15.6691 |
[32m[20221213 23:18:10 @agent_ppo2.py:185][0m |          -0.0180 |          24.6532 |          15.6566 |
[32m[20221213 23:18:10 @agent_ppo2.py:185][0m |          -0.0139 |          24.3416 |          15.6510 |
[32m[20221213 23:18:10 @agent_ppo2.py:185][0m |          -0.0171 |          23.9493 |          15.6629 |
[32m[20221213 23:18:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:18:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.22
[32m[20221213 23:18:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.78
[32m[20221213 23:18:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.30
[32m[20221213 23:18:10 @agent_ppo2.py:143][0m Total time:       5.63 min
[32m[20221213 23:18:10 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221213 23:18:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2263 --------------------------#
[32m[20221213 23:18:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:10 @agent_ppo2.py:185][0m |          -0.0013 |          56.0888 |          15.7328 |
[32m[20221213 23:18:10 @agent_ppo2.py:185][0m |          -0.0002 |          55.3299 |          15.7054 |
[32m[20221213 23:18:10 @agent_ppo2.py:185][0m |          -0.0003 |          56.1177 |          15.7083 |
[32m[20221213 23:18:10 @agent_ppo2.py:185][0m |          -0.0035 |          54.3330 |          15.6910 |
[32m[20221213 23:18:11 @agent_ppo2.py:185][0m |          -0.0070 |          54.1488 |          15.7048 |
[32m[20221213 23:18:11 @agent_ppo2.py:185][0m |          -0.0072 |          54.1186 |          15.6910 |
[32m[20221213 23:18:11 @agent_ppo2.py:185][0m |          -0.0071 |          53.8318 |          15.6928 |
[32m[20221213 23:18:11 @agent_ppo2.py:185][0m |          -0.0078 |          53.8192 |          15.6843 |
[32m[20221213 23:18:11 @agent_ppo2.py:185][0m |          -0.0089 |          53.6429 |          15.6835 |
[32m[20221213 23:18:11 @agent_ppo2.py:185][0m |          -0.0076 |          53.6258 |          15.6828 |
[32m[20221213 23:18:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.78
[32m[20221213 23:18:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.72
[32m[20221213 23:18:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.58
[32m[20221213 23:18:11 @agent_ppo2.py:143][0m Total time:       5.65 min
[32m[20221213 23:18:11 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 23:18:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2264 --------------------------#
[32m[20221213 23:18:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:11 @agent_ppo2.py:185][0m |           0.0093 |          50.1919 |          15.7737 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |           0.0075 |          49.9326 |          15.7296 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |           0.0007 |          49.0879 |          15.7824 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |          -0.0062 |          48.0529 |          15.7670 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |          -0.0026 |          48.4259 |          15.7500 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |           0.0035 |          52.2962 |          15.7621 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |          -0.0034 |          47.8506 |          15.7554 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |          -0.0053 |          47.6063 |          15.7467 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |          -0.0084 |          47.4796 |          15.7685 |
[32m[20221213 23:18:12 @agent_ppo2.py:185][0m |           0.0049 |          52.2450 |          15.7788 |
[32m[20221213 23:18:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.85
[32m[20221213 23:18:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.53
[32m[20221213 23:18:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.18
[32m[20221213 23:18:12 @agent_ppo2.py:143][0m Total time:       5.67 min
[32m[20221213 23:18:12 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221213 23:18:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2265 --------------------------#
[32m[20221213 23:18:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0063 |          37.0260 |          15.5930 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0045 |          33.4810 |          15.6081 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0101 |          32.5257 |          15.5734 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0081 |          32.0149 |          15.5955 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0138 |          31.5486 |          15.5865 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0099 |          31.2510 |          15.5907 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0155 |          30.9668 |          15.5947 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0144 |          30.7204 |          15.5984 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0170 |          30.6426 |          15.5887 |
[32m[20221213 23:18:13 @agent_ppo2.py:185][0m |          -0.0142 |          30.4776 |          15.6092 |
[32m[20221213 23:18:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.37
[32m[20221213 23:18:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.73
[32m[20221213 23:18:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.94
[32m[20221213 23:18:14 @agent_ppo2.py:143][0m Total time:       5.69 min
[32m[20221213 23:18:14 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 23:18:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2266 --------------------------#
[32m[20221213 23:18:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:14 @agent_ppo2.py:185][0m |          -0.0031 |          49.4156 |          15.4958 |
[32m[20221213 23:18:14 @agent_ppo2.py:185][0m |          -0.0002 |          44.0926 |          15.4976 |
[32m[20221213 23:18:14 @agent_ppo2.py:185][0m |          -0.0057 |          42.8498 |          15.4822 |
[32m[20221213 23:18:14 @agent_ppo2.py:185][0m |          -0.0109 |          42.0215 |          15.4853 |
[32m[20221213 23:18:14 @agent_ppo2.py:185][0m |          -0.0082 |          41.2582 |          15.4856 |
[32m[20221213 23:18:14 @agent_ppo2.py:185][0m |           0.0049 |          48.7334 |          15.4689 |
[32m[20221213 23:18:14 @agent_ppo2.py:185][0m |          -0.0092 |          40.8249 |          15.4721 |
[32m[20221213 23:18:15 @agent_ppo2.py:185][0m |          -0.0090 |          40.1665 |          15.4687 |
[32m[20221213 23:18:15 @agent_ppo2.py:185][0m |          -0.0140 |          39.9561 |          15.4571 |
[32m[20221213 23:18:15 @agent_ppo2.py:185][0m |          -0.0092 |          39.8006 |          15.4587 |
[32m[20221213 23:18:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.63
[32m[20221213 23:18:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.34
[32m[20221213 23:18:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.43
[32m[20221213 23:18:15 @agent_ppo2.py:143][0m Total time:       5.71 min
[32m[20221213 23:18:15 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221213 23:18:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2267 --------------------------#
[32m[20221213 23:18:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:15 @agent_ppo2.py:185][0m |           0.0029 |          51.6408 |          15.7030 |
[32m[20221213 23:18:15 @agent_ppo2.py:185][0m |          -0.0054 |          50.3277 |          15.6830 |
[32m[20221213 23:18:15 @agent_ppo2.py:185][0m |          -0.0031 |          50.0680 |          15.6873 |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0075 |          49.6304 |          15.6759 |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0055 |          49.3757 |          15.6668 |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0070 |          49.5079 |          15.6667 |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0066 |          49.1028 |          15.6725 |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0106 |          49.0769 |          15.6562 |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0111 |          48.9948 |          15.6495 |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0101 |          48.9414 |          15.6478 |
[32m[20221213 23:18:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:18:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.75
[32m[20221213 23:18:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.32
[32m[20221213 23:18:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.08
[32m[20221213 23:18:16 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 23:18:16 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 23:18:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2268 --------------------------#
[32m[20221213 23:18:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:16 @agent_ppo2.py:185][0m |          -0.0026 |          51.7034 |          15.5739 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0049 |          49.2449 |          15.5557 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |           0.0081 |          54.2984 |          15.5374 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0094 |          48.0144 |          15.5164 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0072 |          47.9193 |          15.5130 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0102 |          46.8853 |          15.5187 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0114 |          46.6815 |          15.5247 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0089 |          46.6318 |          15.5074 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0129 |          46.3645 |          15.5136 |
[32m[20221213 23:18:17 @agent_ppo2.py:185][0m |          -0.0072 |          47.1773 |          15.4865 |
[32m[20221213 23:18:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.91
[32m[20221213 23:18:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.45
[32m[20221213 23:18:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.39
[32m[20221213 23:18:17 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221213 23:18:17 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221213 23:18:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2269 --------------------------#
[32m[20221213 23:18:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0013 |          27.0730 |          15.6935 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0070 |          24.1348 |          15.6797 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0093 |          22.9289 |          15.6846 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0062 |          22.1476 |          15.6983 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0166 |          21.9166 |          15.6915 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0085 |          21.4072 |          15.7168 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0102 |          20.7616 |          15.7168 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0136 |          20.9277 |          15.7145 |
[32m[20221213 23:18:18 @agent_ppo2.py:185][0m |          -0.0119 |          20.4129 |          15.7310 |
[32m[20221213 23:18:19 @agent_ppo2.py:185][0m |          -0.0158 |          20.3367 |          15.7412 |
[32m[20221213 23:18:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.13
[32m[20221213 23:18:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.51
[32m[20221213 23:18:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.43
[32m[20221213 23:18:19 @agent_ppo2.py:143][0m Total time:       5.78 min
[32m[20221213 23:18:19 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 23:18:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2270 --------------------------#
[32m[20221213 23:18:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:18:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:19 @agent_ppo2.py:185][0m |           0.0105 |          60.0800 |          15.5259 |
[32m[20221213 23:18:19 @agent_ppo2.py:185][0m |          -0.0045 |          52.1686 |          15.4987 |
[32m[20221213 23:18:19 @agent_ppo2.py:185][0m |          -0.0077 |          51.0959 |          15.4819 |
[32m[20221213 23:18:19 @agent_ppo2.py:185][0m |          -0.0083 |          50.4297 |          15.4894 |
[32m[20221213 23:18:19 @agent_ppo2.py:185][0m |          -0.0124 |          49.9510 |          15.4900 |
[32m[20221213 23:18:19 @agent_ppo2.py:185][0m |          -0.0100 |          49.6204 |          15.4613 |
[32m[20221213 23:18:20 @agent_ppo2.py:185][0m |          -0.0109 |          49.2059 |          15.4704 |
[32m[20221213 23:18:20 @agent_ppo2.py:185][0m |          -0.0091 |          49.2697 |          15.4747 |
[32m[20221213 23:18:20 @agent_ppo2.py:185][0m |          -0.0142 |          48.4383 |          15.4708 |
[32m[20221213 23:18:20 @agent_ppo2.py:185][0m |          -0.0128 |          48.2377 |          15.4657 |
[32m[20221213 23:18:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.65
[32m[20221213 23:18:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.50
[32m[20221213 23:18:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.94
[32m[20221213 23:18:20 @agent_ppo2.py:143][0m Total time:       5.80 min
[32m[20221213 23:18:20 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221213 23:18:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2271 --------------------------#
[32m[20221213 23:18:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:20 @agent_ppo2.py:185][0m |           0.0013 |          56.1245 |          15.7444 |
[32m[20221213 23:18:20 @agent_ppo2.py:185][0m |          -0.0080 |          52.0840 |          15.7206 |
[32m[20221213 23:18:20 @agent_ppo2.py:185][0m |          -0.0112 |          50.7663 |          15.7293 |
[32m[20221213 23:18:21 @agent_ppo2.py:185][0m |          -0.0171 |          49.8401 |          15.7144 |
[32m[20221213 23:18:21 @agent_ppo2.py:185][0m |          -0.0010 |          53.4340 |          15.7063 |
[32m[20221213 23:18:21 @agent_ppo2.py:185][0m |          -0.0152 |          48.7038 |          15.6947 |
[32m[20221213 23:18:21 @agent_ppo2.py:185][0m |           0.0025 |          53.0689 |          15.7078 |
[32m[20221213 23:18:21 @agent_ppo2.py:185][0m |          -0.0157 |          48.0871 |          15.7003 |
[32m[20221213 23:18:21 @agent_ppo2.py:185][0m |          -0.0177 |          47.0822 |          15.6940 |
[32m[20221213 23:18:21 @agent_ppo2.py:185][0m |          -0.0156 |          47.0284 |          15.7132 |
[32m[20221213 23:18:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.87
[32m[20221213 23:18:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.26
[32m[20221213 23:18:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.85
[32m[20221213 23:18:21 @agent_ppo2.py:143][0m Total time:       5.82 min
[32m[20221213 23:18:21 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 23:18:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2272 --------------------------#
[32m[20221213 23:18:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |           0.0005 |          64.6149 |          15.5626 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0051 |          61.4861 |          15.5505 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0075 |          60.4964 |          15.5498 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0084 |          59.8435 |          15.5476 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0086 |          59.3723 |          15.5446 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0086 |          59.0169 |          15.5353 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |           0.0053 |          61.7645 |          15.5312 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0012 |          60.4415 |          15.5258 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0015 |          60.1089 |          15.5259 |
[32m[20221213 23:18:22 @agent_ppo2.py:185][0m |          -0.0093 |          58.4565 |          15.5188 |
[32m[20221213 23:18:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.00
[32m[20221213 23:18:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.65
[32m[20221213 23:18:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 178.20
[32m[20221213 23:18:22 @agent_ppo2.py:143][0m Total time:       5.84 min
[32m[20221213 23:18:22 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221213 23:18:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2273 --------------------------#
[32m[20221213 23:18:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0041 |          33.7767 |          15.6075 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0050 |          29.4149 |          15.5865 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0042 |          27.7778 |          15.5960 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0113 |          27.0849 |          15.6086 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0139 |          26.4561 |          15.6042 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0112 |          25.8355 |          15.6079 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0163 |          25.5022 |          15.6175 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0130 |          25.0510 |          15.6189 |
[32m[20221213 23:18:23 @agent_ppo2.py:185][0m |          -0.0154 |          24.7728 |          15.6191 |
[32m[20221213 23:18:24 @agent_ppo2.py:185][0m |          -0.0151 |          24.4936 |          15.6299 |
[32m[20221213 23:18:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.18
[32m[20221213 23:18:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.68
[32m[20221213 23:18:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.11
[32m[20221213 23:18:24 @agent_ppo2.py:143][0m Total time:       5.86 min
[32m[20221213 23:18:24 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 23:18:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2274 --------------------------#
[32m[20221213 23:18:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:24 @agent_ppo2.py:185][0m |           0.0009 |          37.7561 |          15.5693 |
[32m[20221213 23:18:24 @agent_ppo2.py:185][0m |          -0.0107 |          34.5964 |          15.5542 |
[32m[20221213 23:18:24 @agent_ppo2.py:185][0m |          -0.0016 |          33.2878 |          15.5547 |
[32m[20221213 23:18:24 @agent_ppo2.py:185][0m |          -0.0039 |          32.5587 |          15.5497 |
[32m[20221213 23:18:24 @agent_ppo2.py:185][0m |          -0.0108 |          31.6803 |          15.5456 |
[32m[20221213 23:18:24 @agent_ppo2.py:185][0m |          -0.0099 |          31.1018 |          15.5386 |
[32m[20221213 23:18:25 @agent_ppo2.py:185][0m |          -0.0100 |          30.8303 |          15.5289 |
[32m[20221213 23:18:25 @agent_ppo2.py:185][0m |          -0.0170 |          30.5557 |          15.5393 |
[32m[20221213 23:18:25 @agent_ppo2.py:185][0m |          -0.0172 |          30.1824 |          15.5317 |
[32m[20221213 23:18:25 @agent_ppo2.py:185][0m |          -0.0098 |          29.6704 |          15.5243 |
[32m[20221213 23:18:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.28
[32m[20221213 23:18:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.78
[32m[20221213 23:18:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.73
[32m[20221213 23:18:25 @agent_ppo2.py:143][0m Total time:       5.88 min
[32m[20221213 23:18:25 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221213 23:18:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2275 --------------------------#
[32m[20221213 23:18:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:25 @agent_ppo2.py:185][0m |           0.0055 |          47.3531 |          15.5402 |
[32m[20221213 23:18:25 @agent_ppo2.py:185][0m |           0.0003 |          44.6017 |          15.5755 |
[32m[20221213 23:18:25 @agent_ppo2.py:185][0m |          -0.0018 |          43.2198 |          15.5460 |
[32m[20221213 23:18:26 @agent_ppo2.py:185][0m |          -0.0056 |          42.4134 |          15.5678 |
[32m[20221213 23:18:26 @agent_ppo2.py:185][0m |          -0.0071 |          42.1513 |          15.5634 |
[32m[20221213 23:18:26 @agent_ppo2.py:185][0m |          -0.0049 |          41.7407 |          15.5750 |
[32m[20221213 23:18:26 @agent_ppo2.py:185][0m |          -0.0083 |          41.4297 |          15.5771 |
[32m[20221213 23:18:26 @agent_ppo2.py:185][0m |          -0.0074 |          41.2694 |          15.5833 |
[32m[20221213 23:18:26 @agent_ppo2.py:185][0m |          -0.0091 |          41.0340 |          15.6000 |
[32m[20221213 23:18:26 @agent_ppo2.py:185][0m |          -0.0076 |          40.7447 |          15.5753 |
[32m[20221213 23:18:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.11
[32m[20221213 23:18:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.84
[32m[20221213 23:18:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.16
[32m[20221213 23:18:26 @agent_ppo2.py:143][0m Total time:       5.90 min
[32m[20221213 23:18:26 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 23:18:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2276 --------------------------#
[32m[20221213 23:18:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |           0.0105 |          50.6973 |          15.7360 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0006 |          43.4948 |          15.7484 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0020 |          41.4090 |          15.7241 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0087 |          39.6337 |          15.7184 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0093 |          37.9540 |          15.7077 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0145 |          36.9894 |          15.6973 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0077 |          37.0078 |          15.6907 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0198 |          35.7626 |          15.6702 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0140 |          35.0561 |          15.6763 |
[32m[20221213 23:18:27 @agent_ppo2.py:185][0m |          -0.0203 |          34.7584 |          15.6578 |
[32m[20221213 23:18:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:18:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.54
[32m[20221213 23:18:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.57
[32m[20221213 23:18:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.90
[32m[20221213 23:18:27 @agent_ppo2.py:143][0m Total time:       5.92 min
[32m[20221213 23:18:27 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221213 23:18:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2277 --------------------------#
[32m[20221213 23:18:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |           0.0009 |          54.3100 |          15.5440 |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |          -0.0042 |          53.3211 |          15.5448 |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |          -0.0058 |          52.9687 |          15.5384 |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |          -0.0056 |          52.7618 |          15.5254 |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |           0.0045 |          56.8303 |          15.5276 |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |          -0.0056 |          52.5123 |          15.5294 |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |          -0.0048 |          52.2239 |          15.5040 |
[32m[20221213 23:18:28 @agent_ppo2.py:185][0m |          -0.0071 |          52.3284 |          15.5083 |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0095 |          52.0839 |          15.4930 |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0059 |          52.1245 |          15.5078 |
[32m[20221213 23:18:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.92
[32m[20221213 23:18:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.86
[32m[20221213 23:18:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.23
[32m[20221213 23:18:29 @agent_ppo2.py:143][0m Total time:       5.95 min
[32m[20221213 23:18:29 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 23:18:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2278 --------------------------#
[32m[20221213 23:18:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0001 |          28.3593 |          15.6588 |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0054 |          25.8490 |          15.6499 |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0125 |          24.7141 |          15.6264 |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0083 |          24.0143 |          15.6292 |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0064 |          23.2878 |          15.6360 |
[32m[20221213 23:18:29 @agent_ppo2.py:185][0m |          -0.0131 |          23.0686 |          15.6277 |
[32m[20221213 23:18:30 @agent_ppo2.py:185][0m |          -0.0124 |          22.5384 |          15.5997 |
[32m[20221213 23:18:30 @agent_ppo2.py:185][0m |          -0.0090 |          22.0243 |          15.6253 |
[32m[20221213 23:18:30 @agent_ppo2.py:185][0m |          -0.0122 |          21.7658 |          15.6205 |
[32m[20221213 23:18:30 @agent_ppo2.py:185][0m |          -0.0183 |          21.7345 |          15.6179 |
[32m[20221213 23:18:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.72
[32m[20221213 23:18:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.15
[32m[20221213 23:18:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.71
[32m[20221213 23:18:30 @agent_ppo2.py:143][0m Total time:       5.97 min
[32m[20221213 23:18:30 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221213 23:18:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2279 --------------------------#
[32m[20221213 23:18:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:30 @agent_ppo2.py:185][0m |          -0.0031 |          55.8620 |          15.6235 |
[32m[20221213 23:18:30 @agent_ppo2.py:185][0m |          -0.0085 |          54.4552 |          15.6023 |
[32m[20221213 23:18:30 @agent_ppo2.py:185][0m |          -0.0076 |          54.4157 |          15.6283 |
[32m[20221213 23:18:31 @agent_ppo2.py:185][0m |          -0.0106 |          54.2398 |          15.6293 |
[32m[20221213 23:18:31 @agent_ppo2.py:185][0m |          -0.0099 |          54.0692 |          15.6399 |
[32m[20221213 23:18:31 @agent_ppo2.py:185][0m |          -0.0100 |          53.9076 |          15.6502 |
[32m[20221213 23:18:31 @agent_ppo2.py:185][0m |          -0.0081 |          54.0419 |          15.6521 |
[32m[20221213 23:18:31 @agent_ppo2.py:185][0m |          -0.0094 |          53.8855 |          15.6594 |
[32m[20221213 23:18:31 @agent_ppo2.py:185][0m |          -0.0148 |          53.6809 |          15.6811 |
[32m[20221213 23:18:31 @agent_ppo2.py:185][0m |          -0.0134 |          53.4817 |          15.6754 |
[32m[20221213 23:18:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.51
[32m[20221213 23:18:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.20
[32m[20221213 23:18:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.07
[32m[20221213 23:18:31 @agent_ppo2.py:143][0m Total time:       5.99 min
[32m[20221213 23:18:31 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 23:18:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2280 --------------------------#
[32m[20221213 23:18:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:18:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0010 |          21.2184 |          15.5970 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |           0.0004 |          19.0607 |          15.5843 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0036 |          18.7728 |          15.5929 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0002 |          18.6204 |          15.5823 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0038 |          18.4810 |          15.5686 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0008 |          18.4753 |          15.5831 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0032 |          18.4028 |          15.5517 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0063 |          18.4205 |          15.5640 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0035 |          18.3837 |          15.5766 |
[32m[20221213 23:18:32 @agent_ppo2.py:185][0m |          -0.0036 |          18.3085 |          15.5533 |
[32m[20221213 23:18:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:18:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:18:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:18:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.58
[32m[20221213 23:18:32 @agent_ppo2.py:143][0m Total time:       6.01 min
[32m[20221213 23:18:32 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221213 23:18:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2281 --------------------------#
[32m[20221213 23:18:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0007 |          51.4362 |          15.7708 |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0027 |          47.5784 |          15.7595 |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0035 |          46.2661 |          15.7687 |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0053 |          45.3067 |          15.7277 |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0059 |          45.1153 |          15.7537 |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0140 |          43.8832 |          15.7537 |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0100 |          44.2137 |          15.7447 |
[32m[20221213 23:18:33 @agent_ppo2.py:185][0m |          -0.0167 |          42.8758 |          15.7510 |
[32m[20221213 23:18:34 @agent_ppo2.py:185][0m |          -0.0163 |          42.4640 |          15.7489 |
[32m[20221213 23:18:34 @agent_ppo2.py:185][0m |          -0.0127 |          42.2262 |          15.7473 |
[32m[20221213 23:18:34 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.50
[32m[20221213 23:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.02
[32m[20221213 23:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.91
[32m[20221213 23:18:34 @agent_ppo2.py:143][0m Total time:       6.03 min
[32m[20221213 23:18:34 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 23:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2282 --------------------------#
[32m[20221213 23:18:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:34 @agent_ppo2.py:185][0m |           0.0009 |          54.1082 |          15.6576 |
[32m[20221213 23:18:34 @agent_ppo2.py:185][0m |          -0.0024 |          53.0173 |          15.6636 |
[32m[20221213 23:18:34 @agent_ppo2.py:185][0m |          -0.0072 |          52.7186 |          15.6599 |
[32m[20221213 23:18:34 @agent_ppo2.py:185][0m |          -0.0073 |          52.6042 |          15.6560 |
[32m[20221213 23:18:34 @agent_ppo2.py:185][0m |          -0.0101 |          52.2786 |          15.6690 |
[32m[20221213 23:18:35 @agent_ppo2.py:185][0m |          -0.0101 |          52.3165 |          15.6636 |
[32m[20221213 23:18:35 @agent_ppo2.py:185][0m |           0.0005 |          56.4664 |          15.6968 |
[32m[20221213 23:18:35 @agent_ppo2.py:185][0m |          -0.0087 |          52.0936 |          15.6606 |
[32m[20221213 23:18:35 @agent_ppo2.py:185][0m |          -0.0102 |          51.9572 |          15.6873 |
[32m[20221213 23:18:35 @agent_ppo2.py:185][0m |          -0.0114 |          51.8148 |          15.7004 |
[32m[20221213 23:18:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:18:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.02
[32m[20221213 23:18:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.25
[32m[20221213 23:18:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.77
[32m[20221213 23:18:35 @agent_ppo2.py:143][0m Total time:       6.05 min
[32m[20221213 23:18:35 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221213 23:18:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2283 --------------------------#
[32m[20221213 23:18:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:35 @agent_ppo2.py:185][0m |          -0.0026 |          41.7567 |          15.7088 |
[32m[20221213 23:18:35 @agent_ppo2.py:185][0m |          -0.0016 |          38.8066 |          15.6870 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |          -0.0073 |          37.7628 |          15.7064 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |          -0.0103 |          37.0816 |          15.6937 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |          -0.0104 |          36.5023 |          15.7024 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |          -0.0101 |          35.9623 |          15.7184 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |           0.0049 |          44.7425 |          15.7160 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |          -0.0119 |          35.3786 |          15.7305 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |          -0.0166 |          35.0029 |          15.7462 |
[32m[20221213 23:18:36 @agent_ppo2.py:185][0m |          -0.0186 |          34.7789 |          15.7411 |
[32m[20221213 23:18:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.34
[32m[20221213 23:18:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.22
[32m[20221213 23:18:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.28
[32m[20221213 23:18:36 @agent_ppo2.py:143][0m Total time:       6.07 min
[32m[20221213 23:18:36 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 23:18:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2284 --------------------------#
[32m[20221213 23:18:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0018 |          53.8520 |          15.5644 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0047 |          52.8993 |          15.5463 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0057 |          52.3754 |          15.5440 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0078 |          52.1427 |          15.5131 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0115 |          52.1353 |          15.5063 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0071 |          52.4141 |          15.4885 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0120 |          51.7039 |          15.4865 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0107 |          51.7376 |          15.4717 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |          -0.0105 |          51.4839 |          15.4484 |
[32m[20221213 23:18:37 @agent_ppo2.py:185][0m |           0.0121 |          60.8436 |          15.4491 |
[32m[20221213 23:18:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:18:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.81
[32m[20221213 23:18:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.38
[32m[20221213 23:18:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.81
[32m[20221213 23:18:38 @agent_ppo2.py:143][0m Total time:       6.09 min
[32m[20221213 23:18:38 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221213 23:18:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2285 --------------------------#
[32m[20221213 23:18:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |           0.0006 |          44.7447 |          15.7556 |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |           0.0079 |          44.5966 |          15.7444 |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |          -0.0096 |          37.8719 |          15.7518 |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |          -0.0127 |          36.9497 |          15.7565 |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |          -0.0083 |          36.0786 |          15.7369 |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |          -0.0121 |          35.5571 |          15.7507 |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |          -0.0042 |          35.1482 |          15.7568 |
[32m[20221213 23:18:38 @agent_ppo2.py:185][0m |          -0.0143 |          34.9283 |          15.7543 |
[32m[20221213 23:18:39 @agent_ppo2.py:185][0m |          -0.0095 |          34.4041 |          15.7434 |
[32m[20221213 23:18:39 @agent_ppo2.py:185][0m |          -0.0172 |          33.9243 |          15.7381 |
[32m[20221213 23:18:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.10
[32m[20221213 23:18:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.89
[32m[20221213 23:18:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.10
[32m[20221213 23:18:39 @agent_ppo2.py:143][0m Total time:       6.11 min
[32m[20221213 23:18:39 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 23:18:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2286 --------------------------#
[32m[20221213 23:18:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:39 @agent_ppo2.py:185][0m |          -0.0007 |          66.8916 |          15.5660 |
[32m[20221213 23:18:39 @agent_ppo2.py:185][0m |          -0.0002 |          63.2619 |          15.5687 |
[32m[20221213 23:18:39 @agent_ppo2.py:185][0m |          -0.0002 |          63.5718 |          15.5661 |
[32m[20221213 23:18:39 @agent_ppo2.py:185][0m |           0.0046 |          66.9498 |          15.5551 |
[32m[20221213 23:18:39 @agent_ppo2.py:185][0m |          -0.0081 |          59.1101 |          15.5516 |
[32m[20221213 23:18:40 @agent_ppo2.py:185][0m |          -0.0052 |          59.9360 |          15.5499 |
[32m[20221213 23:18:40 @agent_ppo2.py:185][0m |          -0.0094 |          58.2357 |          15.5534 |
[32m[20221213 23:18:40 @agent_ppo2.py:185][0m |          -0.0101 |          57.6042 |          15.5436 |
[32m[20221213 23:18:40 @agent_ppo2.py:185][0m |          -0.0138 |          57.2694 |          15.5449 |
[32m[20221213 23:18:40 @agent_ppo2.py:185][0m |          -0.0156 |          56.8914 |          15.5458 |
[32m[20221213 23:18:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:18:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.83
[32m[20221213 23:18:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.60
[32m[20221213 23:18:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.39
[32m[20221213 23:18:40 @agent_ppo2.py:143][0m Total time:       6.13 min
[32m[20221213 23:18:40 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221213 23:18:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2287 --------------------------#
[32m[20221213 23:18:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:40 @agent_ppo2.py:185][0m |           0.0001 |          50.1535 |          15.5205 |
[32m[20221213 23:18:40 @agent_ppo2.py:185][0m |          -0.0050 |          46.3006 |          15.5055 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0072 |          44.1425 |          15.5000 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0040 |          43.4286 |          15.4890 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0125 |          41.6891 |          15.4934 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0121 |          40.9928 |          15.4956 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0159 |          40.5094 |          15.4911 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0129 |          40.2149 |          15.4816 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0101 |          39.9736 |          15.4935 |
[32m[20221213 23:18:41 @agent_ppo2.py:185][0m |          -0.0157 |          39.1746 |          15.5118 |
[32m[20221213 23:18:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.70
[32m[20221213 23:18:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.66
[32m[20221213 23:18:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.86
[32m[20221213 23:18:41 @agent_ppo2.py:143][0m Total time:       6.15 min
[32m[20221213 23:18:41 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 23:18:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2288 --------------------------#
[32m[20221213 23:18:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0026 |          53.9436 |          15.5952 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0028 |          51.6498 |          15.5865 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |           0.0205 |          61.0165 |          15.5812 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0014 |          51.3130 |          15.5710 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0083 |          50.7032 |          15.5876 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0076 |          50.6192 |          15.5970 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0088 |          50.6227 |          15.5986 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0110 |          50.5812 |          15.5885 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0009 |          53.0400 |          15.5810 |
[32m[20221213 23:18:42 @agent_ppo2.py:185][0m |          -0.0068 |          50.3341 |          15.5591 |
[32m[20221213 23:18:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:18:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.38
[32m[20221213 23:18:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.43
[32m[20221213 23:18:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.70
[32m[20221213 23:18:43 @agent_ppo2.py:143][0m Total time:       6.18 min
[32m[20221213 23:18:43 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221213 23:18:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2289 --------------------------#
[32m[20221213 23:18:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |           0.0014 |          54.0675 |          15.8016 |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |          -0.0019 |          51.8882 |          15.7964 |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |          -0.0091 |          51.2193 |          15.7823 |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |          -0.0114 |          50.9561 |          15.7820 |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |          -0.0049 |          51.3369 |          15.7663 |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |          -0.0132 |          50.4096 |          15.7689 |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |          -0.0120 |          50.5866 |          15.7675 |
[32m[20221213 23:18:43 @agent_ppo2.py:185][0m |          -0.0134 |          50.2352 |          15.7666 |
[32m[20221213 23:18:44 @agent_ppo2.py:185][0m |          -0.0133 |          50.0088 |          15.7704 |
[32m[20221213 23:18:44 @agent_ppo2.py:185][0m |          -0.0150 |          49.8456 |          15.7688 |
[32m[20221213 23:18:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.44
[32m[20221213 23:18:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.97
[32m[20221213 23:18:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.65
[32m[20221213 23:18:44 @agent_ppo2.py:143][0m Total time:       6.20 min
[32m[20221213 23:18:44 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 23:18:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2290 --------------------------#
[32m[20221213 23:18:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:18:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:44 @agent_ppo2.py:185][0m |           0.0080 |          37.8194 |          15.7105 |
[32m[20221213 23:18:44 @agent_ppo2.py:185][0m |          -0.0065 |          31.7499 |          15.6862 |
[32m[20221213 23:18:44 @agent_ppo2.py:185][0m |          -0.0056 |          29.1686 |          15.6884 |
[32m[20221213 23:18:44 @agent_ppo2.py:185][0m |          -0.0084 |          27.8473 |          15.7065 |
[32m[20221213 23:18:44 @agent_ppo2.py:185][0m |          -0.0071 |          27.0275 |          15.7000 |
[32m[20221213 23:18:45 @agent_ppo2.py:185][0m |          -0.0159 |          26.4664 |          15.6894 |
[32m[20221213 23:18:45 @agent_ppo2.py:185][0m |          -0.0141 |          25.8801 |          15.6918 |
[32m[20221213 23:18:45 @agent_ppo2.py:185][0m |          -0.0164 |          25.9614 |          15.6976 |
[32m[20221213 23:18:45 @agent_ppo2.py:185][0m |          -0.0186 |          25.1501 |          15.6751 |
[32m[20221213 23:18:45 @agent_ppo2.py:185][0m |          -0.0170 |          24.8716 |          15.6758 |
[32m[20221213 23:18:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.33
[32m[20221213 23:18:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.59
[32m[20221213 23:18:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.10
[32m[20221213 23:18:45 @agent_ppo2.py:143][0m Total time:       6.22 min
[32m[20221213 23:18:45 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221213 23:18:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2291 --------------------------#
[32m[20221213 23:18:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:45 @agent_ppo2.py:185][0m |          -0.0031 |          39.5451 |          15.7360 |
[32m[20221213 23:18:45 @agent_ppo2.py:185][0m |          -0.0016 |          34.6274 |          15.7358 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0064 |          33.2997 |          15.7582 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0098 |          32.6098 |          15.7389 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0083 |          31.7263 |          15.7512 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0128 |          31.3877 |          15.7283 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0144 |          31.1778 |          15.7281 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0143 |          30.6417 |          15.7308 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0152 |          30.2324 |          15.7358 |
[32m[20221213 23:18:46 @agent_ppo2.py:185][0m |          -0.0166 |          29.9411 |          15.7245 |
[32m[20221213 23:18:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.00
[32m[20221213 23:18:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.51
[32m[20221213 23:18:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.30
[32m[20221213 23:18:46 @agent_ppo2.py:143][0m Total time:       6.24 min
[32m[20221213 23:18:46 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 23:18:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2292 --------------------------#
[32m[20221213 23:18:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0021 |          62.7016 |          15.7908 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0066 |          60.1162 |          15.7844 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0083 |          58.9892 |          15.7720 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0028 |          61.2129 |          15.7727 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0093 |          57.2404 |          15.7760 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0103 |          56.7802 |          15.7826 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0091 |          56.3365 |          15.7944 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0084 |          56.9232 |          15.7778 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0114 |          55.6750 |          15.7698 |
[32m[20221213 23:18:47 @agent_ppo2.py:185][0m |          -0.0133 |          55.2953 |          15.7712 |
[32m[20221213 23:18:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.12
[32m[20221213 23:18:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.51
[32m[20221213 23:18:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.56
[32m[20221213 23:18:48 @agent_ppo2.py:143][0m Total time:       6.26 min
[32m[20221213 23:18:48 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221213 23:18:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2293 --------------------------#
[32m[20221213 23:18:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |           0.0082 |          51.9375 |          15.6855 |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |          -0.0033 |          44.0792 |          15.6664 |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |          -0.0040 |          42.1965 |          15.6761 |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |          -0.0016 |          41.2822 |          15.6638 |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |          -0.0046 |          40.2849 |          15.6491 |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |          -0.0086 |          39.7103 |          15.6518 |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |          -0.0122 |          39.0988 |          15.6413 |
[32m[20221213 23:18:48 @agent_ppo2.py:185][0m |          -0.0064 |          39.3748 |          15.6341 |
[32m[20221213 23:18:49 @agent_ppo2.py:185][0m |          -0.0103 |          38.4583 |          15.6299 |
[32m[20221213 23:18:49 @agent_ppo2.py:185][0m |          -0.0060 |          38.1902 |          15.6277 |
[32m[20221213 23:18:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.63
[32m[20221213 23:18:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.58
[32m[20221213 23:18:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.55
[32m[20221213 23:18:49 @agent_ppo2.py:143][0m Total time:       6.28 min
[32m[20221213 23:18:49 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 23:18:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2294 --------------------------#
[32m[20221213 23:18:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:49 @agent_ppo2.py:185][0m |           0.0008 |          63.2685 |          15.5458 |
[32m[20221213 23:18:49 @agent_ppo2.py:185][0m |           0.0019 |          62.4849 |          15.5342 |
[32m[20221213 23:18:49 @agent_ppo2.py:185][0m |          -0.0060 |          59.7039 |          15.4921 |
[32m[20221213 23:18:49 @agent_ppo2.py:185][0m |          -0.0077 |          59.2846 |          15.5137 |
[32m[20221213 23:18:49 @agent_ppo2.py:185][0m |          -0.0067 |          59.0823 |          15.5148 |
[32m[20221213 23:18:50 @agent_ppo2.py:185][0m |          -0.0115 |          58.9013 |          15.5231 |
[32m[20221213 23:18:50 @agent_ppo2.py:185][0m |          -0.0108 |          58.6503 |          15.5218 |
[32m[20221213 23:18:50 @agent_ppo2.py:185][0m |          -0.0128 |          58.5000 |          15.5205 |
[32m[20221213 23:18:50 @agent_ppo2.py:185][0m |          -0.0063 |          60.1687 |          15.5249 |
[32m[20221213 23:18:50 @agent_ppo2.py:185][0m |          -0.0069 |          58.6140 |          15.5117 |
[32m[20221213 23:18:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:18:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.90
[32m[20221213 23:18:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.83
[32m[20221213 23:18:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.96
[32m[20221213 23:18:50 @agent_ppo2.py:143][0m Total time:       6.30 min
[32m[20221213 23:18:50 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221213 23:18:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2295 --------------------------#
[32m[20221213 23:18:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:50 @agent_ppo2.py:185][0m |          -0.0000 |          40.6815 |          15.6516 |
[32m[20221213 23:18:50 @agent_ppo2.py:185][0m |          -0.0037 |          38.5626 |          15.6299 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0078 |          37.8163 |          15.6294 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0070 |          37.5903 |          15.5921 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0064 |          37.7639 |          15.5970 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0100 |          37.2868 |          15.5687 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0028 |          38.2224 |          15.5610 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0086 |          37.0393 |          15.5552 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0009 |          40.0588 |          15.5618 |
[32m[20221213 23:18:51 @agent_ppo2.py:185][0m |          -0.0116 |          36.9486 |          15.5336 |
[32m[20221213 23:18:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:18:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.86
[32m[20221213 23:18:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.09
[32m[20221213 23:18:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.73
[32m[20221213 23:18:51 @agent_ppo2.py:143][0m Total time:       6.32 min
[32m[20221213 23:18:51 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 23:18:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2296 --------------------------#
[32m[20221213 23:18:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0000 |          30.1000 |          15.5717 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0024 |          26.8219 |          15.5552 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0004 |          26.5083 |          15.5586 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0112 |          24.4356 |          15.5484 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0101 |          23.7327 |          15.5571 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0102 |          23.1116 |          15.5552 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0078 |          23.6775 |          15.5721 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |           0.0008 |          26.1328 |          15.5440 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0132 |          22.2723 |          15.5780 |
[32m[20221213 23:18:52 @agent_ppo2.py:185][0m |          -0.0143 |          21.7828 |          15.5907 |
[32m[20221213 23:18:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.95
[32m[20221213 23:18:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.80
[32m[20221213 23:18:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.55
[32m[20221213 23:18:53 @agent_ppo2.py:143][0m Total time:       6.34 min
[32m[20221213 23:18:53 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221213 23:18:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2297 --------------------------#
[32m[20221213 23:18:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |          -0.0024 |          62.2342 |          15.4710 |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |          -0.0057 |          59.5560 |          15.4534 |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |          -0.0064 |          58.1372 |          15.4488 |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |          -0.0085 |          57.2355 |          15.4728 |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |           0.0007 |          59.5806 |          15.4668 |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |          -0.0065 |          56.9829 |          15.4519 |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |          -0.0061 |          57.2820 |          15.4629 |
[32m[20221213 23:18:53 @agent_ppo2.py:185][0m |          -0.0135 |          56.0161 |          15.4583 |
[32m[20221213 23:18:54 @agent_ppo2.py:185][0m |          -0.0130 |          56.0014 |          15.4465 |
[32m[20221213 23:18:54 @agent_ppo2.py:185][0m |          -0.0108 |          55.6088 |          15.4517 |
[32m[20221213 23:18:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.78
[32m[20221213 23:18:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.91
[32m[20221213 23:18:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.58
[32m[20221213 23:18:54 @agent_ppo2.py:143][0m Total time:       6.36 min
[32m[20221213 23:18:54 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 23:18:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2298 --------------------------#
[32m[20221213 23:18:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:54 @agent_ppo2.py:185][0m |          -0.0029 |          47.5270 |          15.6958 |
[32m[20221213 23:18:54 @agent_ppo2.py:185][0m |          -0.0087 |          44.0582 |          15.6818 |
[32m[20221213 23:18:54 @agent_ppo2.py:185][0m |          -0.0090 |          42.8148 |          15.6897 |
[32m[20221213 23:18:54 @agent_ppo2.py:185][0m |          -0.0149 |          41.9160 |          15.6640 |
[32m[20221213 23:18:54 @agent_ppo2.py:185][0m |          -0.0127 |          41.1614 |          15.6794 |
[32m[20221213 23:18:55 @agent_ppo2.py:185][0m |          -0.0129 |          40.5807 |          15.7040 |
[32m[20221213 23:18:55 @agent_ppo2.py:185][0m |          -0.0124 |          39.9391 |          15.6850 |
[32m[20221213 23:18:55 @agent_ppo2.py:185][0m |          -0.0145 |          39.5355 |          15.6852 |
[32m[20221213 23:18:55 @agent_ppo2.py:185][0m |          -0.0164 |          39.1297 |          15.7000 |
[32m[20221213 23:18:55 @agent_ppo2.py:185][0m |          -0.0136 |          38.8740 |          15.6991 |
[32m[20221213 23:18:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.63
[32m[20221213 23:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.24
[32m[20221213 23:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.04
[32m[20221213 23:18:55 @agent_ppo2.py:143][0m Total time:       6.38 min
[32m[20221213 23:18:55 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221213 23:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2299 --------------------------#
[32m[20221213 23:18:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:18:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:55 @agent_ppo2.py:185][0m |           0.0045 |          28.8930 |          15.6498 |
[32m[20221213 23:18:55 @agent_ppo2.py:185][0m |          -0.0078 |          25.1978 |          15.6358 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0085 |          23.9114 |          15.6518 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0073 |          23.3246 |          15.6280 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0091 |          22.8746 |          15.6209 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0052 |          22.5157 |          15.6205 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0055 |          22.2742 |          15.6336 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0135 |          22.1943 |          15.6362 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0210 |          21.9618 |          15.6222 |
[32m[20221213 23:18:56 @agent_ppo2.py:185][0m |          -0.0174 |          21.7447 |          15.6113 |
[32m[20221213 23:18:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.54
[32m[20221213 23:18:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.75
[32m[20221213 23:18:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.10
[32m[20221213 23:18:56 @agent_ppo2.py:143][0m Total time:       6.40 min
[32m[20221213 23:18:56 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 23:18:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2300 --------------------------#
[32m[20221213 23:18:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:18:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0017 |          39.3866 |          15.7495 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0037 |          36.5027 |          15.7249 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0111 |          35.6857 |          15.7313 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0101 |          35.1821 |          15.7158 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0080 |          34.7366 |          15.6963 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0111 |          34.5630 |          15.7035 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0175 |          34.3146 |          15.6930 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0117 |          34.1519 |          15.6751 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0048 |          34.5780 |          15.6899 |
[32m[20221213 23:18:57 @agent_ppo2.py:185][0m |          -0.0091 |          34.0454 |          15.6578 |
[32m[20221213 23:18:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:18:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.22
[32m[20221213 23:18:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.36
[32m[20221213 23:18:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.50
[32m[20221213 23:18:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 515.50
[32m[20221213 23:18:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 515.50
[32m[20221213 23:18:58 @agent_ppo2.py:143][0m Total time:       6.43 min
[32m[20221213 23:18:58 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221213 23:18:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2301 --------------------------#
[32m[20221213 23:18:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:58 @agent_ppo2.py:185][0m |          -0.0076 |          32.1495 |          15.6716 |
[32m[20221213 23:18:58 @agent_ppo2.py:185][0m |          -0.0109 |          28.8896 |          15.6578 |
[32m[20221213 23:18:58 @agent_ppo2.py:185][0m |          -0.0106 |          27.8008 |          15.6563 |
[32m[20221213 23:18:58 @agent_ppo2.py:185][0m |          -0.0150 |          27.3785 |          15.6719 |
[32m[20221213 23:18:58 @agent_ppo2.py:185][0m |          -0.0093 |          27.1662 |          15.6625 |
[32m[20221213 23:18:58 @agent_ppo2.py:185][0m |          -0.0152 |          26.9085 |          15.6872 |
[32m[20221213 23:18:58 @agent_ppo2.py:185][0m |          -0.0128 |          26.7958 |          15.6794 |
[32m[20221213 23:18:59 @agent_ppo2.py:185][0m |          -0.0095 |          26.5223 |          15.6812 |
[32m[20221213 23:18:59 @agent_ppo2.py:185][0m |          -0.0137 |          26.3468 |          15.6813 |
[32m[20221213 23:18:59 @agent_ppo2.py:185][0m |          -0.0184 |          26.3793 |          15.6825 |
[32m[20221213 23:18:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:18:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.14
[32m[20221213 23:18:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.95
[32m[20221213 23:18:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.46
[32m[20221213 23:18:59 @agent_ppo2.py:143][0m Total time:       6.45 min
[32m[20221213 23:18:59 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 23:18:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2302 --------------------------#
[32m[20221213 23:18:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:18:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:18:59 @agent_ppo2.py:185][0m |           0.0024 |          61.4800 |          15.5920 |
[32m[20221213 23:18:59 @agent_ppo2.py:185][0m |          -0.0052 |          58.8184 |          15.6186 |
[32m[20221213 23:18:59 @agent_ppo2.py:185][0m |          -0.0073 |          58.0701 |          15.5824 |
[32m[20221213 23:18:59 @agent_ppo2.py:185][0m |          -0.0079 |          57.6502 |          15.5745 |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |           0.0018 |          65.6776 |          15.5812 |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |          -0.0027 |          58.8231 |          15.5431 |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |          -0.0006 |          64.7276 |          15.5577 |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |          -0.0104 |          56.9088 |          15.5556 |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |          -0.0127 |          56.5016 |          15.5572 |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |          -0.0093 |          56.4035 |          15.5370 |
[32m[20221213 23:19:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.60
[32m[20221213 23:19:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.50
[32m[20221213 23:19:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.78
[32m[20221213 23:19:00 @agent_ppo2.py:143][0m Total time:       6.47 min
[32m[20221213 23:19:00 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221213 23:19:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2303 --------------------------#
[32m[20221213 23:19:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |           0.0015 |          29.6305 |          15.4828 |
[32m[20221213 23:19:00 @agent_ppo2.py:185][0m |           0.0034 |          26.0831 |          15.4754 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0011 |          24.0752 |          15.4565 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0063 |          22.9986 |          15.4688 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0091 |          22.6884 |          15.4533 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0068 |          22.2847 |          15.4578 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0102 |          21.8850 |          15.4556 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0132 |          21.7990 |          15.4546 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0115 |          21.5243 |          15.4700 |
[32m[20221213 23:19:01 @agent_ppo2.py:185][0m |          -0.0071 |          21.2967 |          15.4534 |
[32m[20221213 23:19:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.79
[32m[20221213 23:19:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.35
[32m[20221213 23:19:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.58
[32m[20221213 23:19:01 @agent_ppo2.py:143][0m Total time:       6.49 min
[32m[20221213 23:19:01 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221213 23:19:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2304 --------------------------#
[32m[20221213 23:19:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0032 |          48.6093 |          15.5481 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0071 |          46.1325 |          15.5147 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0080 |          45.3217 |          15.5172 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |           0.0043 |          51.3164 |          15.5081 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0083 |          44.2980 |          15.5106 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0137 |          43.9604 |          15.5169 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0094 |          44.7150 |          15.5156 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0103 |          43.5848 |          15.5298 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0141 |          43.2590 |          15.5048 |
[32m[20221213 23:19:02 @agent_ppo2.py:185][0m |          -0.0053 |          46.3197 |          15.5046 |
[32m[20221213 23:19:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.91
[32m[20221213 23:19:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.01
[32m[20221213 23:19:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 210.32
[32m[20221213 23:19:03 @agent_ppo2.py:143][0m Total time:       6.51 min
[32m[20221213 23:19:03 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221213 23:19:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2305 --------------------------#
[32m[20221213 23:19:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:03 @agent_ppo2.py:185][0m |           0.0078 |          48.0484 |          15.4848 |
[32m[20221213 23:19:03 @agent_ppo2.py:185][0m |          -0.0016 |          40.4974 |          15.4698 |
[32m[20221213 23:19:03 @agent_ppo2.py:185][0m |          -0.0085 |          39.4289 |          15.4528 |
[32m[20221213 23:19:03 @agent_ppo2.py:185][0m |          -0.0094 |          38.5492 |          15.4866 |
[32m[20221213 23:19:03 @agent_ppo2.py:185][0m |          -0.0106 |          38.1715 |          15.4621 |
[32m[20221213 23:19:03 @agent_ppo2.py:185][0m |          -0.0055 |          39.4990 |          15.4720 |
[32m[20221213 23:19:03 @agent_ppo2.py:185][0m |          -0.0144 |          37.1842 |          15.4577 |
[32m[20221213 23:19:04 @agent_ppo2.py:185][0m |          -0.0207 |          37.0196 |          15.4744 |
[32m[20221213 23:19:04 @agent_ppo2.py:185][0m |          -0.0112 |          37.0677 |          15.4714 |
[32m[20221213 23:19:04 @agent_ppo2.py:185][0m |          -0.0108 |          38.0873 |          15.4626 |
[32m[20221213 23:19:04 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:19:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.10
[32m[20221213 23:19:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.79
[32m[20221213 23:19:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.63
[32m[20221213 23:19:04 @agent_ppo2.py:143][0m Total time:       6.53 min
[32m[20221213 23:19:04 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221213 23:19:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2306 --------------------------#
[32m[20221213 23:19:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:04 @agent_ppo2.py:185][0m |          -0.0073 |          39.8659 |          15.6965 |
[32m[20221213 23:19:04 @agent_ppo2.py:185][0m |          -0.0061 |          37.1858 |          15.6938 |
[32m[20221213 23:19:04 @agent_ppo2.py:185][0m |          -0.0007 |          36.5820 |          15.6946 |
[32m[20221213 23:19:04 @agent_ppo2.py:185][0m |          -0.0081 |          35.4807 |          15.7030 |
[32m[20221213 23:19:05 @agent_ppo2.py:185][0m |          -0.0118 |          35.1937 |          15.6932 |
[32m[20221213 23:19:05 @agent_ppo2.py:185][0m |          -0.0110 |          34.9885 |          15.7102 |
[32m[20221213 23:19:05 @agent_ppo2.py:185][0m |          -0.0090 |          34.5043 |          15.6926 |
[32m[20221213 23:19:05 @agent_ppo2.py:185][0m |          -0.0130 |          34.1204 |          15.6978 |
[32m[20221213 23:19:05 @agent_ppo2.py:185][0m |          -0.0084 |          34.1587 |          15.6890 |
[32m[20221213 23:19:05 @agent_ppo2.py:185][0m |          -0.0146 |          33.5957 |          15.7002 |
[32m[20221213 23:19:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.22
[32m[20221213 23:19:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.97
[32m[20221213 23:19:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.89
[32m[20221213 23:19:05 @agent_ppo2.py:143][0m Total time:       6.55 min
[32m[20221213 23:19:05 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221213 23:19:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2307 --------------------------#
[32m[20221213 23:19:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:05 @agent_ppo2.py:185][0m |          -0.0044 |          40.3303 |          15.7170 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0077 |          35.8477 |          15.6925 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0123 |          34.6570 |          15.7128 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0052 |          33.6650 |          15.6990 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |           0.0095 |          41.8774 |          15.6895 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0089 |          33.1222 |          15.7054 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0104 |          32.4444 |          15.7046 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0114 |          32.0295 |          15.6856 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0108 |          31.7026 |          15.7044 |
[32m[20221213 23:19:06 @agent_ppo2.py:185][0m |          -0.0131 |          31.4958 |          15.6919 |
[32m[20221213 23:19:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.48
[32m[20221213 23:19:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.17
[32m[20221213 23:19:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.33
[32m[20221213 23:19:06 @agent_ppo2.py:143][0m Total time:       6.57 min
[32m[20221213 23:19:06 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221213 23:19:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2308 --------------------------#
[32m[20221213 23:19:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |           0.0008 |          47.7592 |          15.5930 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0035 |          46.9310 |          15.5857 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0057 |          46.7055 |          15.6052 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |           0.0012 |          49.2524 |          15.6086 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0061 |          46.4738 |          15.6079 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0078 |          46.6141 |          15.6080 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0089 |          46.2267 |          15.6235 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0102 |          46.2435 |          15.6212 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0078 |          46.1564 |          15.6308 |
[32m[20221213 23:19:07 @agent_ppo2.py:185][0m |          -0.0077 |          46.1426 |          15.6252 |
[32m[20221213 23:19:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.53
[32m[20221213 23:19:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.50
[32m[20221213 23:19:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.78
[32m[20221213 23:19:08 @agent_ppo2.py:143][0m Total time:       6.59 min
[32m[20221213 23:19:08 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221213 23:19:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2309 --------------------------#
[32m[20221213 23:19:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:08 @agent_ppo2.py:185][0m |          -0.0031 |          53.1246 |          15.6440 |
[32m[20221213 23:19:08 @agent_ppo2.py:185][0m |          -0.0096 |          50.1798 |          15.6402 |
[32m[20221213 23:19:08 @agent_ppo2.py:185][0m |          -0.0085 |          48.4294 |          15.6388 |
[32m[20221213 23:19:08 @agent_ppo2.py:185][0m |          -0.0108 |          47.6284 |          15.6256 |
[32m[20221213 23:19:08 @agent_ppo2.py:185][0m |          -0.0083 |          46.7977 |          15.6410 |
[32m[20221213 23:19:08 @agent_ppo2.py:185][0m |          -0.0120 |          46.4905 |          15.6107 |
[32m[20221213 23:19:08 @agent_ppo2.py:185][0m |          -0.0067 |          46.4986 |          15.6391 |
[32m[20221213 23:19:09 @agent_ppo2.py:185][0m |          -0.0149 |          46.0751 |          15.6321 |
[32m[20221213 23:19:09 @agent_ppo2.py:185][0m |          -0.0136 |          45.7618 |          15.6256 |
[32m[20221213 23:19:09 @agent_ppo2.py:185][0m |          -0.0135 |          45.7102 |          15.6338 |
[32m[20221213 23:19:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.32
[32m[20221213 23:19:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.44
[32m[20221213 23:19:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.90
[32m[20221213 23:19:09 @agent_ppo2.py:143][0m Total time:       6.61 min
[32m[20221213 23:19:09 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221213 23:19:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2310 --------------------------#
[32m[20221213 23:19:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:19:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:09 @agent_ppo2.py:185][0m |          -0.0001 |          63.9027 |          15.6350 |
[32m[20221213 23:19:09 @agent_ppo2.py:185][0m |          -0.0066 |          58.9758 |          15.5938 |
[32m[20221213 23:19:09 @agent_ppo2.py:185][0m |          -0.0080 |          56.9599 |          15.5731 |
[32m[20221213 23:19:09 @agent_ppo2.py:185][0m |          -0.0074 |          55.8576 |          15.5911 |
[32m[20221213 23:19:10 @agent_ppo2.py:185][0m |          -0.0118 |          55.2585 |          15.5645 |
[32m[20221213 23:19:10 @agent_ppo2.py:185][0m |          -0.0098 |          54.6515 |          15.5833 |
[32m[20221213 23:19:10 @agent_ppo2.py:185][0m |          -0.0119 |          54.0407 |          15.5673 |
[32m[20221213 23:19:10 @agent_ppo2.py:185][0m |          -0.0109 |          53.7894 |          15.5564 |
[32m[20221213 23:19:10 @agent_ppo2.py:185][0m |          -0.0138 |          53.5001 |          15.5492 |
[32m[20221213 23:19:10 @agent_ppo2.py:185][0m |          -0.0133 |          53.1907 |          15.5426 |
[32m[20221213 23:19:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:19:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.39
[32m[20221213 23:19:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.73
[32m[20221213 23:19:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.34
[32m[20221213 23:19:10 @agent_ppo2.py:143][0m Total time:       6.64 min
[32m[20221213 23:19:10 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221213 23:19:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2311 --------------------------#
[32m[20221213 23:19:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:10 @agent_ppo2.py:185][0m |          -0.0065 |          57.4079 |          15.8434 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0046 |          53.4804 |          15.8319 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0026 |          53.2407 |          15.8172 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0092 |          51.8774 |          15.8189 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0108 |          51.3277 |          15.8426 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0093 |          50.9409 |          15.8127 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0120 |          50.5363 |          15.8270 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0129 |          50.4033 |          15.8210 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0085 |          51.6140 |          15.8277 |
[32m[20221213 23:19:11 @agent_ppo2.py:185][0m |          -0.0111 |          49.8325 |          15.8209 |
[32m[20221213 23:19:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.33
[32m[20221213 23:19:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.18
[32m[20221213 23:19:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.61
[32m[20221213 23:19:11 @agent_ppo2.py:143][0m Total time:       6.66 min
[32m[20221213 23:19:11 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221213 23:19:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2312 --------------------------#
[32m[20221213 23:19:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0011 |          55.2892 |          15.6094 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0076 |          51.0815 |          15.5723 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0062 |          49.5419 |          15.5736 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0085 |          48.7370 |          15.5664 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0106 |          48.1243 |          15.5763 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0111 |          47.8670 |          15.5718 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0096 |          47.4915 |          15.5822 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0108 |          47.2429 |          15.5798 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0095 |          46.9845 |          15.5672 |
[32m[20221213 23:19:12 @agent_ppo2.py:185][0m |          -0.0076 |          47.0144 |          15.5698 |
[32m[20221213 23:19:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.64
[32m[20221213 23:19:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.72
[32m[20221213 23:19:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.18
[32m[20221213 23:19:13 @agent_ppo2.py:143][0m Total time:       6.68 min
[32m[20221213 23:19:13 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221213 23:19:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2313 --------------------------#
[32m[20221213 23:19:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:13 @agent_ppo2.py:185][0m |           0.0014 |          33.8888 |          15.6681 |
[32m[20221213 23:19:13 @agent_ppo2.py:185][0m |          -0.0052 |          30.1265 |          15.6546 |
[32m[20221213 23:19:13 @agent_ppo2.py:185][0m |          -0.0102 |          28.9199 |          15.6464 |
[32m[20221213 23:19:13 @agent_ppo2.py:185][0m |          -0.0161 |          28.6958 |          15.6455 |
[32m[20221213 23:19:13 @agent_ppo2.py:185][0m |          -0.0107 |          27.6604 |          15.6263 |
[32m[20221213 23:19:13 @agent_ppo2.py:185][0m |          -0.0107 |          27.1952 |          15.6117 |
[32m[20221213 23:19:13 @agent_ppo2.py:185][0m |          -0.0075 |          27.6016 |          15.6231 |
[32m[20221213 23:19:14 @agent_ppo2.py:185][0m |          -0.0083 |          26.6187 |          15.5855 |
[32m[20221213 23:19:14 @agent_ppo2.py:185][0m |          -0.0169 |          26.3674 |          15.5949 |
[32m[20221213 23:19:14 @agent_ppo2.py:185][0m |          -0.0209 |          26.1084 |          15.5843 |
[32m[20221213 23:19:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.86
[32m[20221213 23:19:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.71
[32m[20221213 23:19:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.20
[32m[20221213 23:19:14 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221213 23:19:14 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221213 23:19:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2314 --------------------------#
[32m[20221213 23:19:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:14 @agent_ppo2.py:185][0m |           0.0055 |          25.7638 |          15.5935 |
[32m[20221213 23:19:14 @agent_ppo2.py:185][0m |          -0.0037 |          21.6018 |          15.5818 |
[32m[20221213 23:19:14 @agent_ppo2.py:185][0m |          -0.0075 |          20.7216 |          15.5738 |
[32m[20221213 23:19:14 @agent_ppo2.py:185][0m |          -0.0074 |          20.2314 |          15.5558 |
[32m[20221213 23:19:15 @agent_ppo2.py:185][0m |          -0.0089 |          19.8450 |          15.5670 |
[32m[20221213 23:19:15 @agent_ppo2.py:185][0m |          -0.0168 |          19.4791 |          15.5717 |
[32m[20221213 23:19:15 @agent_ppo2.py:185][0m |          -0.0109 |          19.1159 |          15.5602 |
[32m[20221213 23:19:15 @agent_ppo2.py:185][0m |          -0.0159 |          19.2235 |          15.5610 |
[32m[20221213 23:19:15 @agent_ppo2.py:185][0m |          -0.0118 |          18.9141 |          15.5327 |
[32m[20221213 23:19:15 @agent_ppo2.py:185][0m |          -0.0191 |          18.7081 |          15.5488 |
[32m[20221213 23:19:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:19:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.96
[32m[20221213 23:19:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.07
[32m[20221213 23:19:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.13
[32m[20221213 23:19:15 @agent_ppo2.py:143][0m Total time:       6.72 min
[32m[20221213 23:19:15 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221213 23:19:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2315 --------------------------#
[32m[20221213 23:19:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:15 @agent_ppo2.py:185][0m |          -0.0028 |          34.1165 |          15.3835 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0033 |          31.2762 |          15.3958 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0072 |          30.3183 |          15.4064 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0071 |          29.9727 |          15.3899 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0127 |          29.5163 |          15.3988 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0130 |          29.1781 |          15.4053 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0100 |          29.0659 |          15.4089 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0112 |          28.6359 |          15.4216 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0129 |          28.4651 |          15.4268 |
[32m[20221213 23:19:16 @agent_ppo2.py:185][0m |          -0.0137 |          28.2597 |          15.4224 |
[32m[20221213 23:19:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.08
[32m[20221213 23:19:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.88
[32m[20221213 23:19:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.21
[32m[20221213 23:19:16 @agent_ppo2.py:143][0m Total time:       6.74 min
[32m[20221213 23:19:16 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221213 23:19:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2316 --------------------------#
[32m[20221213 23:19:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |           0.0015 |          52.9108 |          15.6074 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0074 |          49.2231 |          15.5931 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0053 |          48.5722 |          15.6046 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |           0.0093 |          52.4034 |          15.5876 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0124 |          47.2910 |          15.5729 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0066 |          45.8865 |          15.5616 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0143 |          45.4573 |          15.5626 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0120 |          45.0602 |          15.5527 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0142 |          44.7645 |          15.5464 |
[32m[20221213 23:19:17 @agent_ppo2.py:185][0m |          -0.0129 |          44.4865 |          15.5422 |
[32m[20221213 23:19:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.54
[32m[20221213 23:19:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.35
[32m[20221213 23:19:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.41
[32m[20221213 23:19:18 @agent_ppo2.py:143][0m Total time:       6.76 min
[32m[20221213 23:19:18 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221213 23:19:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2317 --------------------------#
[32m[20221213 23:19:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:18 @agent_ppo2.py:185][0m |          -0.0016 |          49.7918 |          15.4700 |
[32m[20221213 23:19:18 @agent_ppo2.py:185][0m |          -0.0052 |          48.2689 |          15.4411 |
[32m[20221213 23:19:18 @agent_ppo2.py:185][0m |          -0.0038 |          47.7377 |          15.4479 |
[32m[20221213 23:19:18 @agent_ppo2.py:185][0m |          -0.0078 |          47.6732 |          15.4347 |
[32m[20221213 23:19:18 @agent_ppo2.py:185][0m |           0.0001 |          51.9584 |          15.4277 |
[32m[20221213 23:19:18 @agent_ppo2.py:185][0m |          -0.0065 |          47.5665 |          15.4024 |
[32m[20221213 23:19:18 @agent_ppo2.py:185][0m |          -0.0062 |          47.6468 |          15.4220 |
[32m[20221213 23:19:19 @agent_ppo2.py:185][0m |          -0.0085 |          47.2514 |          15.4263 |
[32m[20221213 23:19:19 @agent_ppo2.py:185][0m |          -0.0106 |          47.0429 |          15.4389 |
[32m[20221213 23:19:19 @agent_ppo2.py:185][0m |          -0.0006 |          48.5428 |          15.4453 |
[32m[20221213 23:19:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.79
[32m[20221213 23:19:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.44
[32m[20221213 23:19:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.11
[32m[20221213 23:19:19 @agent_ppo2.py:143][0m Total time:       6.78 min
[32m[20221213 23:19:19 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221213 23:19:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2318 --------------------------#
[32m[20221213 23:19:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:19 @agent_ppo2.py:185][0m |          -0.0000 |          58.2625 |          15.5904 |
[32m[20221213 23:19:19 @agent_ppo2.py:185][0m |          -0.0045 |          56.5865 |          15.5903 |
[32m[20221213 23:19:19 @agent_ppo2.py:185][0m |          -0.0009 |          57.0580 |          15.5940 |
[32m[20221213 23:19:19 @agent_ppo2.py:185][0m |          -0.0073 |          56.0181 |          15.5949 |
[32m[20221213 23:19:20 @agent_ppo2.py:185][0m |           0.0015 |          59.3542 |          15.6093 |
[32m[20221213 23:19:20 @agent_ppo2.py:185][0m |          -0.0045 |          55.6768 |          15.6029 |
[32m[20221213 23:19:20 @agent_ppo2.py:185][0m |          -0.0072 |          55.5147 |          15.6020 |
[32m[20221213 23:19:20 @agent_ppo2.py:185][0m |          -0.0126 |          55.3494 |          15.5946 |
[32m[20221213 23:19:20 @agent_ppo2.py:185][0m |          -0.0104 |          55.2581 |          15.5969 |
[32m[20221213 23:19:20 @agent_ppo2.py:185][0m |          -0.0092 |          55.0938 |          15.6087 |
[32m[20221213 23:19:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.77
[32m[20221213 23:19:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.93
[32m[20221213 23:19:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.58
[32m[20221213 23:19:20 @agent_ppo2.py:143][0m Total time:       6.80 min
[32m[20221213 23:19:20 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221213 23:19:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2319 --------------------------#
[32m[20221213 23:19:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:20 @agent_ppo2.py:185][0m |           0.0013 |          57.3157 |          15.6429 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0050 |          55.3478 |          15.6094 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0085 |          54.7647 |          15.6103 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0088 |          54.1901 |          15.6015 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0092 |          54.0251 |          15.5959 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0124 |          53.8747 |          15.6009 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0064 |          53.9409 |          15.5847 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0097 |          53.6856 |          15.5792 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0081 |          53.8431 |          15.5789 |
[32m[20221213 23:19:21 @agent_ppo2.py:185][0m |          -0.0053 |          55.2454 |          15.5724 |
[32m[20221213 23:19:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:19:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.67
[32m[20221213 23:19:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.35
[32m[20221213 23:19:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.86
[32m[20221213 23:19:21 @agent_ppo2.py:143][0m Total time:       6.82 min
[32m[20221213 23:19:21 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221213 23:19:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2320 --------------------------#
[32m[20221213 23:19:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:19:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |           0.0016 |          50.1317 |          15.6190 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0060 |          48.8259 |          15.5911 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0021 |          48.8477 |          15.6096 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0081 |          47.8371 |          15.5803 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0100 |          47.6744 |          15.6160 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0116 |          47.4281 |          15.6044 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0098 |          47.4532 |          15.6136 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0093 |          47.3078 |          15.5855 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0044 |          48.5126 |          15.6083 |
[32m[20221213 23:19:22 @agent_ppo2.py:185][0m |          -0.0063 |          47.7531 |          15.5794 |
[32m[20221213 23:19:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.10
[32m[20221213 23:19:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.69
[32m[20221213 23:19:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.82
[32m[20221213 23:19:23 @agent_ppo2.py:143][0m Total time:       6.84 min
[32m[20221213 23:19:23 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221213 23:19:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2321 --------------------------#
[32m[20221213 23:19:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:23 @agent_ppo2.py:185][0m |           0.0022 |          47.6210 |          15.6715 |
[32m[20221213 23:19:23 @agent_ppo2.py:185][0m |          -0.0051 |          43.8394 |          15.6551 |
[32m[20221213 23:19:23 @agent_ppo2.py:185][0m |          -0.0010 |          44.9718 |          15.6433 |
[32m[20221213 23:19:23 @agent_ppo2.py:185][0m |          -0.0071 |          41.9282 |          15.6517 |
[32m[20221213 23:19:23 @agent_ppo2.py:185][0m |          -0.0065 |          41.4639 |          15.6254 |
[32m[20221213 23:19:23 @agent_ppo2.py:185][0m |          -0.0105 |          40.9678 |          15.6152 |
[32m[20221213 23:19:23 @agent_ppo2.py:185][0m |          -0.0115 |          40.6857 |          15.6241 |
[32m[20221213 23:19:24 @agent_ppo2.py:185][0m |          -0.0127 |          40.2311 |          15.6199 |
[32m[20221213 23:19:24 @agent_ppo2.py:185][0m |          -0.0118 |          39.8434 |          15.6181 |
[32m[20221213 23:19:24 @agent_ppo2.py:185][0m |          -0.0175 |          39.6354 |          15.6223 |
[32m[20221213 23:19:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.16
[32m[20221213 23:19:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.40
[32m[20221213 23:19:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.47
[32m[20221213 23:19:24 @agent_ppo2.py:143][0m Total time:       6.86 min
[32m[20221213 23:19:24 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221213 23:19:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2322 --------------------------#
[32m[20221213 23:19:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:24 @agent_ppo2.py:185][0m |          -0.0001 |          47.6786 |          15.7095 |
[32m[20221213 23:19:24 @agent_ppo2.py:185][0m |           0.0116 |          50.0412 |          15.6933 |
[32m[20221213 23:19:24 @agent_ppo2.py:185][0m |          -0.0052 |          41.2299 |          15.6611 |
[32m[20221213 23:19:24 @agent_ppo2.py:185][0m |          -0.0069 |          40.1405 |          15.6797 |
[32m[20221213 23:19:25 @agent_ppo2.py:185][0m |           0.0082 |          42.4515 |          15.6913 |
[32m[20221213 23:19:25 @agent_ppo2.py:185][0m |          -0.0120 |          38.4884 |          15.6806 |
[32m[20221213 23:19:25 @agent_ppo2.py:185][0m |           0.0039 |          42.4778 |          15.6684 |
[32m[20221213 23:19:25 @agent_ppo2.py:185][0m |          -0.0132 |          37.3370 |          15.6687 |
[32m[20221213 23:19:25 @agent_ppo2.py:185][0m |          -0.0065 |          40.0586 |          15.6606 |
[32m[20221213 23:19:25 @agent_ppo2.py:185][0m |          -0.0156 |          36.4899 |          15.6578 |
[32m[20221213 23:19:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:19:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.02
[32m[20221213 23:19:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.31
[32m[20221213 23:19:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.40
[32m[20221213 23:19:25 @agent_ppo2.py:143][0m Total time:       6.89 min
[32m[20221213 23:19:25 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221213 23:19:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2323 --------------------------#
[32m[20221213 23:19:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:25 @agent_ppo2.py:185][0m |           0.0019 |          53.3839 |          15.5149 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0068 |          50.4962 |          15.4621 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0060 |          49.4199 |          15.4795 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0107 |          48.5506 |          15.4714 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |           0.0045 |          55.9761 |          15.4556 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0135 |          47.8579 |          15.4421 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0019 |          51.3988 |          15.4488 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0128 |          47.1008 |          15.4298 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0115 |          46.7573 |          15.4426 |
[32m[20221213 23:19:26 @agent_ppo2.py:185][0m |          -0.0063 |          49.4939 |          15.4363 |
[32m[20221213 23:19:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.27
[32m[20221213 23:19:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.23
[32m[20221213 23:19:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.59
[32m[20221213 23:19:26 @agent_ppo2.py:143][0m Total time:       6.91 min
[32m[20221213 23:19:26 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221213 23:19:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2324 --------------------------#
[32m[20221213 23:19:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0012 |          70.9627 |          15.6028 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0053 |          67.4692 |          15.5941 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0079 |          66.5199 |          15.5963 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0053 |          66.2040 |          15.6062 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0116 |          65.5429 |          15.5875 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0112 |          65.4387 |          15.5742 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0103 |          65.0748 |          15.5801 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0103 |          64.5761 |          15.5766 |
[32m[20221213 23:19:27 @agent_ppo2.py:185][0m |          -0.0130 |          64.5529 |          15.5573 |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |          -0.0093 |          64.2304 |          15.5708 |
[32m[20221213 23:19:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:19:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.51
[32m[20221213 23:19:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.18
[32m[20221213 23:19:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.58
[32m[20221213 23:19:28 @agent_ppo2.py:143][0m Total time:       6.93 min
[32m[20221213 23:19:28 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221213 23:19:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2325 --------------------------#
[32m[20221213 23:19:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |           0.0007 |          51.5456 |          15.3844 |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |          -0.0042 |          50.6406 |          15.3538 |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |           0.0008 |          51.4293 |          15.3285 |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |          -0.0045 |          50.2814 |          15.3371 |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |          -0.0089 |          50.1687 |          15.3003 |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |          -0.0057 |          50.3471 |          15.3307 |
[32m[20221213 23:19:28 @agent_ppo2.py:185][0m |          -0.0088 |          49.9061 |          15.3151 |
[32m[20221213 23:19:29 @agent_ppo2.py:185][0m |          -0.0102 |          49.9879 |          15.3371 |
[32m[20221213 23:19:29 @agent_ppo2.py:185][0m |          -0.0084 |          49.7470 |          15.3377 |
[32m[20221213 23:19:29 @agent_ppo2.py:185][0m |          -0.0118 |          49.8539 |          15.3487 |
[32m[20221213 23:19:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.87
[32m[20221213 23:19:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.10
[32m[20221213 23:19:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.40
[32m[20221213 23:19:29 @agent_ppo2.py:143][0m Total time:       6.95 min
[32m[20221213 23:19:29 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221213 23:19:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2326 --------------------------#
[32m[20221213 23:19:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:29 @agent_ppo2.py:185][0m |          -0.0018 |          44.9974 |          15.5193 |
[32m[20221213 23:19:29 @agent_ppo2.py:185][0m |          -0.0039 |          42.4163 |          15.5529 |
[32m[20221213 23:19:29 @agent_ppo2.py:185][0m |          -0.0048 |          41.4114 |          15.5523 |
[32m[20221213 23:19:29 @agent_ppo2.py:185][0m |          -0.0060 |          40.8341 |          15.5539 |
[32m[20221213 23:19:30 @agent_ppo2.py:185][0m |          -0.0099 |          40.4768 |          15.5643 |
[32m[20221213 23:19:30 @agent_ppo2.py:185][0m |          -0.0088 |          40.2052 |          15.5870 |
[32m[20221213 23:19:30 @agent_ppo2.py:185][0m |          -0.0102 |          39.9959 |          15.5917 |
[32m[20221213 23:19:30 @agent_ppo2.py:185][0m |          -0.0111 |          39.6911 |          15.5896 |
[32m[20221213 23:19:30 @agent_ppo2.py:185][0m |          -0.0136 |          39.6183 |          15.6084 |
[32m[20221213 23:19:30 @agent_ppo2.py:185][0m |          -0.0105 |          39.3803 |          15.6141 |
[32m[20221213 23:19:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.38
[32m[20221213 23:19:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.10
[32m[20221213 23:19:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.57
[32m[20221213 23:19:30 @agent_ppo2.py:143][0m Total time:       6.97 min
[32m[20221213 23:19:30 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221213 23:19:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2327 --------------------------#
[32m[20221213 23:19:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:30 @agent_ppo2.py:185][0m |           0.0017 |          32.0586 |          15.4735 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0055 |          28.0383 |          15.4737 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0016 |          27.1216 |          15.4665 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0071 |          26.4894 |          15.4960 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0095 |          26.1992 |          15.4741 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0060 |          26.2403 |          15.4969 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0106 |          25.7657 |          15.4877 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0100 |          25.4951 |          15.4909 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0188 |          25.2559 |          15.4885 |
[32m[20221213 23:19:31 @agent_ppo2.py:185][0m |          -0.0028 |          25.2999 |          15.4748 |
[32m[20221213 23:19:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.69
[32m[20221213 23:19:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 346.78
[32m[20221213 23:19:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.41
[32m[20221213 23:19:31 @agent_ppo2.py:143][0m Total time:       6.99 min
[32m[20221213 23:19:31 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221213 23:19:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2328 --------------------------#
[32m[20221213 23:19:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |           0.0008 |          62.8248 |          15.6944 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0024 |          60.7604 |          15.7034 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0075 |          59.6120 |          15.7002 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0092 |          59.1823 |          15.7007 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0001 |          61.5648 |          15.6747 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0083 |          58.1905 |          15.6580 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0126 |          57.9962 |          15.6872 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0139 |          57.7599 |          15.6808 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0122 |          57.2743 |          15.6843 |
[32m[20221213 23:19:32 @agent_ppo2.py:185][0m |          -0.0169 |          57.1904 |          15.6754 |
[32m[20221213 23:19:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.82
[32m[20221213 23:19:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.62
[32m[20221213 23:19:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.30
[32m[20221213 23:19:33 @agent_ppo2.py:143][0m Total time:       7.01 min
[32m[20221213 23:19:33 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221213 23:19:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2329 --------------------------#
[32m[20221213 23:19:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:33 @agent_ppo2.py:185][0m |           0.0015 |          50.1997 |          15.4760 |
[32m[20221213 23:19:33 @agent_ppo2.py:185][0m |          -0.0069 |          46.0693 |          15.4961 |
[32m[20221213 23:19:33 @agent_ppo2.py:185][0m |          -0.0068 |          44.5013 |          15.4961 |
[32m[20221213 23:19:33 @agent_ppo2.py:185][0m |          -0.0027 |          44.0290 |          15.4926 |
[32m[20221213 23:19:33 @agent_ppo2.py:185][0m |          -0.0103 |          42.8996 |          15.4931 |
[32m[20221213 23:19:33 @agent_ppo2.py:185][0m |          -0.0018 |          42.7574 |          15.4970 |
[32m[20221213 23:19:34 @agent_ppo2.py:185][0m |          -0.0118 |          41.7438 |          15.4777 |
[32m[20221213 23:19:34 @agent_ppo2.py:185][0m |          -0.0136 |          41.6943 |          15.4858 |
[32m[20221213 23:19:34 @agent_ppo2.py:185][0m |          -0.0052 |          42.2949 |          15.4802 |
[32m[20221213 23:19:34 @agent_ppo2.py:185][0m |          -0.0079 |          41.2022 |          15.4740 |
[32m[20221213 23:19:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:19:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.24
[32m[20221213 23:19:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.01
[32m[20221213 23:19:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.16
[32m[20221213 23:19:34 @agent_ppo2.py:143][0m Total time:       7.03 min
[32m[20221213 23:19:34 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221213 23:19:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2330 --------------------------#
[32m[20221213 23:19:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:19:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:34 @agent_ppo2.py:185][0m |          -0.0004 |          44.2757 |          15.7003 |
[32m[20221213 23:19:34 @agent_ppo2.py:185][0m |          -0.0068 |          41.7424 |          15.6686 |
[32m[20221213 23:19:34 @agent_ppo2.py:185][0m |          -0.0040 |          41.2919 |          15.6892 |
[32m[20221213 23:19:35 @agent_ppo2.py:185][0m |          -0.0103 |          40.7797 |          15.6862 |
[32m[20221213 23:19:35 @agent_ppo2.py:185][0m |          -0.0093 |          40.2946 |          15.6863 |
[32m[20221213 23:19:35 @agent_ppo2.py:185][0m |          -0.0119 |          39.8275 |          15.6831 |
[32m[20221213 23:19:35 @agent_ppo2.py:185][0m |          -0.0098 |          39.7223 |          15.6899 |
[32m[20221213 23:19:35 @agent_ppo2.py:185][0m |          -0.0124 |          39.3780 |          15.6801 |
[32m[20221213 23:19:35 @agent_ppo2.py:185][0m |          -0.0133 |          39.2100 |          15.7016 |
[32m[20221213 23:19:35 @agent_ppo2.py:185][0m |           0.0040 |          47.5669 |          15.6949 |
[32m[20221213 23:19:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.20
[32m[20221213 23:19:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.00
[32m[20221213 23:19:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.44
[32m[20221213 23:19:35 @agent_ppo2.py:143][0m Total time:       7.05 min
[32m[20221213 23:19:35 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221213 23:19:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2331 --------------------------#
[32m[20221213 23:19:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |           0.0053 |          54.1908 |          15.4816 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0061 |          49.8647 |          15.4901 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0097 |          47.8207 |          15.4916 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0106 |          46.0000 |          15.4714 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0086 |          45.2998 |          15.4881 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0089 |          44.6543 |          15.4709 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0081 |          44.3204 |          15.4638 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0123 |          43.7577 |          15.4498 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0115 |          43.2298 |          15.4604 |
[32m[20221213 23:19:36 @agent_ppo2.py:185][0m |          -0.0139 |          42.8040 |          15.4420 |
[32m[20221213 23:19:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.29
[32m[20221213 23:19:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.17
[32m[20221213 23:19:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 227.94
[32m[20221213 23:19:36 @agent_ppo2.py:143][0m Total time:       7.07 min
[32m[20221213 23:19:36 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221213 23:19:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2332 --------------------------#
[32m[20221213 23:19:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |           0.0034 |          46.7548 |          15.5605 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0067 |          37.5613 |          15.5384 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0026 |          36.4147 |          15.5189 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0097 |          35.9299 |          15.5138 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0063 |          35.5675 |          15.5026 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0092 |          35.2599 |          15.4969 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0081 |          35.0350 |          15.4872 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0073 |          34.4848 |          15.4905 |
[32m[20221213 23:19:37 @agent_ppo2.py:185][0m |          -0.0154 |          34.2731 |          15.4877 |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0134 |          34.1234 |          15.4883 |
[32m[20221213 23:19:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:19:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.14
[32m[20221213 23:19:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.12
[32m[20221213 23:19:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.29
[32m[20221213 23:19:38 @agent_ppo2.py:143][0m Total time:       7.09 min
[32m[20221213 23:19:38 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221213 23:19:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2333 --------------------------#
[32m[20221213 23:19:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0006 |          52.1541 |          15.6130 |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0046 |          51.0406 |          15.6027 |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0046 |          50.8142 |          15.5891 |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0037 |          50.4499 |          15.5992 |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0034 |          50.6072 |          15.5874 |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0091 |          50.1091 |          15.5773 |
[32m[20221213 23:19:38 @agent_ppo2.py:185][0m |          -0.0066 |          50.0570 |          15.5688 |
[32m[20221213 23:19:39 @agent_ppo2.py:185][0m |          -0.0066 |          49.9457 |          15.5706 |
[32m[20221213 23:19:39 @agent_ppo2.py:185][0m |          -0.0042 |          50.1924 |          15.5564 |
[32m[20221213 23:19:39 @agent_ppo2.py:185][0m |          -0.0040 |          50.6006 |          15.5532 |
[32m[20221213 23:19:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.06
[32m[20221213 23:19:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.19
[32m[20221213 23:19:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.49
[32m[20221213 23:19:39 @agent_ppo2.py:143][0m Total time:       7.12 min
[32m[20221213 23:19:39 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221213 23:19:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2334 --------------------------#
[32m[20221213 23:19:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:39 @agent_ppo2.py:185][0m |           0.0034 |          48.7277 |          15.6431 |
[32m[20221213 23:19:39 @agent_ppo2.py:185][0m |          -0.0022 |          44.4801 |          15.6292 |
[32m[20221213 23:19:39 @agent_ppo2.py:185][0m |          -0.0067 |          42.4941 |          15.6082 |
[32m[20221213 23:19:40 @agent_ppo2.py:185][0m |           0.0015 |          41.3767 |          15.6120 |
[32m[20221213 23:19:40 @agent_ppo2.py:185][0m |          -0.0026 |          42.3505 |          15.6187 |
[32m[20221213 23:19:40 @agent_ppo2.py:185][0m |          -0.0061 |          40.0686 |          15.6013 |
[32m[20221213 23:19:40 @agent_ppo2.py:185][0m |          -0.0057 |          39.6879 |          15.6121 |
[32m[20221213 23:19:40 @agent_ppo2.py:185][0m |           0.0029 |          43.8813 |          15.6128 |
[32m[20221213 23:19:40 @agent_ppo2.py:185][0m |          -0.0065 |          40.3052 |          15.6192 |
[32m[20221213 23:19:40 @agent_ppo2.py:185][0m |          -0.0098 |          38.7059 |          15.6052 |
[32m[20221213 23:19:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:19:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.91
[32m[20221213 23:19:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.27
[32m[20221213 23:19:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.23
[32m[20221213 23:19:40 @agent_ppo2.py:143][0m Total time:       7.14 min
[32m[20221213 23:19:40 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221213 23:19:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2335 --------------------------#
[32m[20221213 23:19:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0020 |          24.3378 |          15.3974 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0062 |          14.6732 |          15.3916 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0021 |          13.5699 |          15.4015 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0086 |          13.1012 |          15.3691 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0110 |          12.6288 |          15.3760 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0101 |          12.3548 |          15.3748 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0145 |          12.2776 |          15.3814 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0137 |          12.0233 |          15.3842 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0148 |          11.7963 |          15.3858 |
[32m[20221213 23:19:41 @agent_ppo2.py:185][0m |          -0.0092 |          11.6529 |          15.3685 |
[32m[20221213 23:19:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.87
[32m[20221213 23:19:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.96
[32m[20221213 23:19:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.99
[32m[20221213 23:19:41 @agent_ppo2.py:143][0m Total time:       7.16 min
[32m[20221213 23:19:41 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221213 23:19:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2336 --------------------------#
[32m[20221213 23:19:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0015 |          61.2037 |          15.5312 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0064 |          59.7522 |          15.4803 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |           0.0036 |          63.2676 |          15.4975 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0137 |          59.4899 |          15.4751 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0072 |          58.6615 |          15.4827 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0050 |          60.8109 |          15.4810 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0048 |          59.2680 |          15.4779 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0102 |          58.0537 |          15.4573 |
[32m[20221213 23:19:42 @agent_ppo2.py:185][0m |          -0.0004 |          61.9455 |          15.4633 |
[32m[20221213 23:19:43 @agent_ppo2.py:185][0m |          -0.0124 |          57.8351 |          15.4402 |
[32m[20221213 23:19:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.54
[32m[20221213 23:19:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.12
[32m[20221213 23:19:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.95
[32m[20221213 23:19:43 @agent_ppo2.py:143][0m Total time:       7.18 min
[32m[20221213 23:19:43 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221213 23:19:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2337 --------------------------#
[32m[20221213 23:19:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:43 @agent_ppo2.py:185][0m |          -0.0007 |          24.7002 |          15.5994 |
[32m[20221213 23:19:43 @agent_ppo2.py:185][0m |          -0.0062 |          18.2126 |          15.5995 |
[32m[20221213 23:19:43 @agent_ppo2.py:185][0m |          -0.0001 |          17.0900 |          15.5919 |
[32m[20221213 23:19:43 @agent_ppo2.py:185][0m |          -0.0091 |          16.6258 |          15.6117 |
[32m[20221213 23:19:43 @agent_ppo2.py:185][0m |          -0.0069 |          16.5205 |          15.6055 |
[32m[20221213 23:19:43 @agent_ppo2.py:185][0m |          -0.0102 |          16.1707 |          15.6323 |
[32m[20221213 23:19:44 @agent_ppo2.py:185][0m |          -0.0092 |          15.7928 |          15.6306 |
[32m[20221213 23:19:44 @agent_ppo2.py:185][0m |          -0.0056 |          15.5643 |          15.6304 |
[32m[20221213 23:19:44 @agent_ppo2.py:185][0m |          -0.0080 |          15.4431 |          15.6387 |
[32m[20221213 23:19:44 @agent_ppo2.py:185][0m |          -0.0101 |          15.2997 |          15.6560 |
[32m[20221213 23:19:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.93
[32m[20221213 23:19:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.75
[32m[20221213 23:19:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.15
[32m[20221213 23:19:44 @agent_ppo2.py:143][0m Total time:       7.20 min
[32m[20221213 23:19:44 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221213 23:19:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2338 --------------------------#
[32m[20221213 23:19:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:44 @agent_ppo2.py:185][0m |           0.0006 |          50.5708 |          15.6246 |
[32m[20221213 23:19:44 @agent_ppo2.py:185][0m |           0.0013 |          49.7961 |          15.6156 |
[32m[20221213 23:19:44 @agent_ppo2.py:185][0m |          -0.0038 |          49.2120 |          15.5931 |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |          -0.0040 |          48.9982 |          15.6031 |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |          -0.0051 |          48.9056 |          15.5988 |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |          -0.0076 |          48.8527 |          15.6021 |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |           0.0008 |          49.3879 |          15.6189 |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |          -0.0072 |          48.6017 |          15.6101 |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |          -0.0061 |          48.5028 |          15.6031 |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |          -0.0057 |          48.4429 |          15.6209 |
[32m[20221213 23:19:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:19:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.25
[32m[20221213 23:19:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.39
[32m[20221213 23:19:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.75
[32m[20221213 23:19:45 @agent_ppo2.py:143][0m Total time:       7.22 min
[32m[20221213 23:19:45 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221213 23:19:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2339 --------------------------#
[32m[20221213 23:19:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:45 @agent_ppo2.py:185][0m |          -0.0004 |          56.2303 |          15.5193 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |          -0.0053 |          54.9829 |          15.4936 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |          -0.0048 |          54.4613 |          15.4905 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |           0.0058 |          59.2145 |          15.4957 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |          -0.0043 |          54.4398 |          15.5059 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |           0.0032 |          57.4036 |          15.4881 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |          -0.0079 |          53.8207 |          15.4938 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |          -0.0034 |          54.9379 |          15.4773 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |          -0.0064 |          53.6323 |          15.4695 |
[32m[20221213 23:19:46 @agent_ppo2.py:185][0m |          -0.0091 |          53.4149 |          15.4793 |
[32m[20221213 23:19:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.12
[32m[20221213 23:19:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.87
[32m[20221213 23:19:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.21
[32m[20221213 23:19:46 @agent_ppo2.py:143][0m Total time:       7.24 min
[32m[20221213 23:19:46 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221213 23:19:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2340 --------------------------#
[32m[20221213 23:19:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:19:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0045 |          44.1630 |          15.5983 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0010 |          37.3786 |          15.5665 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0057 |          37.1222 |          15.5599 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |           0.0041 |          38.4939 |          15.5669 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0112 |          34.8878 |          15.5665 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0128 |          33.9606 |          15.5592 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0117 |          33.6764 |          15.5425 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0150 |          33.2309 |          15.5466 |
[32m[20221213 23:19:47 @agent_ppo2.py:185][0m |          -0.0068 |          33.9532 |          15.5268 |
[32m[20221213 23:19:48 @agent_ppo2.py:185][0m |          -0.0196 |          32.8193 |          15.5313 |
[32m[20221213 23:19:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.10
[32m[20221213 23:19:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.13
[32m[20221213 23:19:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.43
[32m[20221213 23:19:48 @agent_ppo2.py:143][0m Total time:       7.26 min
[32m[20221213 23:19:48 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221213 23:19:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2341 --------------------------#
[32m[20221213 23:19:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:48 @agent_ppo2.py:185][0m |           0.0016 |          57.4936 |          15.3655 |
[32m[20221213 23:19:48 @agent_ppo2.py:185][0m |          -0.0069 |          55.7925 |          15.3484 |
[32m[20221213 23:19:48 @agent_ppo2.py:185][0m |          -0.0054 |          55.0079 |          15.3226 |
[32m[20221213 23:19:48 @agent_ppo2.py:185][0m |          -0.0095 |          54.7393 |          15.3096 |
[32m[20221213 23:19:48 @agent_ppo2.py:185][0m |          -0.0120 |          54.5349 |          15.3319 |
[32m[20221213 23:19:48 @agent_ppo2.py:185][0m |          -0.0080 |          54.7197 |          15.3245 |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |          -0.0090 |          54.1242 |          15.3394 |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |          -0.0013 |          57.4443 |          15.3231 |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |          -0.0065 |          54.2442 |          15.3036 |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |          -0.0116 |          53.7573 |          15.3120 |
[32m[20221213 23:19:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.57
[32m[20221213 23:19:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.30
[32m[20221213 23:19:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.21
[32m[20221213 23:19:49 @agent_ppo2.py:143][0m Total time:       7.28 min
[32m[20221213 23:19:49 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221213 23:19:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2342 --------------------------#
[32m[20221213 23:19:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:19:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |           0.0037 |          54.7674 |          15.5439 |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |          -0.0040 |          53.4566 |          15.5023 |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |          -0.0060 |          52.8735 |          15.5184 |
[32m[20221213 23:19:49 @agent_ppo2.py:185][0m |          -0.0097 |          52.7476 |          15.5098 |
[32m[20221213 23:19:50 @agent_ppo2.py:185][0m |          -0.0107 |          52.5949 |          15.4866 |
[32m[20221213 23:19:50 @agent_ppo2.py:185][0m |          -0.0111 |          52.4560 |          15.5007 |
[32m[20221213 23:19:50 @agent_ppo2.py:185][0m |          -0.0013 |          54.2070 |          15.4719 |
[32m[20221213 23:19:50 @agent_ppo2.py:185][0m |          -0.0058 |          52.7124 |          15.4886 |
[32m[20221213 23:19:50 @agent_ppo2.py:185][0m |          -0.0112 |          52.2042 |          15.4920 |
[32m[20221213 23:19:50 @agent_ppo2.py:185][0m |          -0.0067 |          52.6355 |          15.4918 |
[32m[20221213 23:19:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:19:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.58
[32m[20221213 23:19:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.67
[32m[20221213 23:19:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.78
[32m[20221213 23:19:50 @agent_ppo2.py:143][0m Total time:       7.30 min
[32m[20221213 23:19:50 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221213 23:19:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2343 --------------------------#
[32m[20221213 23:19:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:50 @agent_ppo2.py:185][0m |           0.0023 |          42.9608 |          15.4700 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |          -0.0041 |          39.5399 |          15.4667 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |           0.0015 |          39.7112 |          15.4587 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |          -0.0064 |          38.4215 |          15.4657 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |          -0.0081 |          38.1730 |          15.4783 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |          -0.0118 |          37.7759 |          15.4717 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |          -0.0058 |          37.6177 |          15.4726 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |           0.0136 |          44.4762 |          15.4909 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |          -0.0089 |          37.5704 |          15.4915 |
[32m[20221213 23:19:51 @agent_ppo2.py:185][0m |          -0.0141 |          37.0706 |          15.4918 |
[32m[20221213 23:19:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:19:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.96
[32m[20221213 23:19:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.76
[32m[20221213 23:19:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.18
[32m[20221213 23:19:51 @agent_ppo2.py:143][0m Total time:       7.32 min
[32m[20221213 23:19:51 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221213 23:19:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2344 --------------------------#
[32m[20221213 23:19:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |           0.0017 |          22.4485 |          15.4547 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |           0.0001 |          18.9615 |          15.4246 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |          -0.0076 |          17.8436 |          15.4332 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |          -0.0058 |          17.2495 |          15.4242 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |          -0.0045 |          16.9133 |          15.4087 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |          -0.0094 |          16.4773 |          15.4161 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |          -0.0107 |          16.8258 |          15.4224 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |          -0.0150 |          16.0726 |          15.4126 |
[32m[20221213 23:19:52 @agent_ppo2.py:185][0m |          -0.0116 |          15.9519 |          15.3992 |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |          -0.0146 |          15.7940 |          15.3834 |
[32m[20221213 23:19:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.37
[32m[20221213 23:19:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.75
[32m[20221213 23:19:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.40
[32m[20221213 23:19:53 @agent_ppo2.py:143][0m Total time:       7.34 min
[32m[20221213 23:19:53 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221213 23:19:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2345 --------------------------#
[32m[20221213 23:19:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |          -0.0056 |          53.2192 |          15.5161 |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |          -0.0028 |          52.3130 |          15.5522 |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |          -0.0087 |          51.4268 |          15.5605 |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |          -0.0073 |          51.2700 |          15.5687 |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |          -0.0056 |          51.1650 |          15.5728 |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |          -0.0096 |          51.0353 |          15.5562 |
[32m[20221213 23:19:53 @agent_ppo2.py:185][0m |           0.0070 |          53.6766 |          15.5676 |
[32m[20221213 23:19:54 @agent_ppo2.py:185][0m |          -0.0078 |          50.7421 |          15.5587 |
[32m[20221213 23:19:54 @agent_ppo2.py:185][0m |          -0.0077 |          50.5268 |          15.5674 |
[32m[20221213 23:19:54 @agent_ppo2.py:185][0m |          -0.0104 |          50.6373 |          15.5898 |
[32m[20221213 23:19:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:19:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.82
[32m[20221213 23:19:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.75
[32m[20221213 23:19:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.25
[32m[20221213 23:19:54 @agent_ppo2.py:143][0m Total time:       7.36 min
[32m[20221213 23:19:54 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221213 23:19:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2346 --------------------------#
[32m[20221213 23:19:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:54 @agent_ppo2.py:185][0m |           0.0018 |          47.5547 |          15.4022 |
[32m[20221213 23:19:54 @agent_ppo2.py:185][0m |          -0.0058 |          43.5347 |          15.3722 |
[32m[20221213 23:19:54 @agent_ppo2.py:185][0m |          -0.0069 |          42.4380 |          15.3884 |
[32m[20221213 23:19:54 @agent_ppo2.py:185][0m |          -0.0003 |          43.5854 |          15.3566 |
[32m[20221213 23:19:55 @agent_ppo2.py:185][0m |          -0.0099 |          41.6727 |          15.3630 |
[32m[20221213 23:19:55 @agent_ppo2.py:185][0m |          -0.0133 |          40.7433 |          15.3485 |
[32m[20221213 23:19:55 @agent_ppo2.py:185][0m |          -0.0073 |          40.7727 |          15.3711 |
[32m[20221213 23:19:55 @agent_ppo2.py:185][0m |          -0.0093 |          39.9663 |          15.3391 |
[32m[20221213 23:19:55 @agent_ppo2.py:185][0m |          -0.0125 |          39.7539 |          15.3424 |
[32m[20221213 23:19:55 @agent_ppo2.py:185][0m |          -0.0154 |          39.1362 |          15.3542 |
[32m[20221213 23:19:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:19:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.08
[32m[20221213 23:19:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.35
[32m[20221213 23:19:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.17
[32m[20221213 23:19:55 @agent_ppo2.py:143][0m Total time:       7.39 min
[32m[20221213 23:19:55 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221213 23:19:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2347 --------------------------#
[32m[20221213 23:19:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:55 @agent_ppo2.py:185][0m |           0.0007 |          31.9744 |          15.5714 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0047 |          28.6362 |          15.5565 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0102 |          27.3923 |          15.5405 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0100 |          26.5909 |          15.5521 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0100 |          26.1330 |          15.5403 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0140 |          25.8138 |          15.5334 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0156 |          25.4359 |          15.5349 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0137 |          25.0678 |          15.5180 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0118 |          25.3736 |          15.5156 |
[32m[20221213 23:19:56 @agent_ppo2.py:185][0m |          -0.0149 |          24.7298 |          15.5119 |
[32m[20221213 23:19:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:19:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.90
[32m[20221213 23:19:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.84
[32m[20221213 23:19:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.33
[32m[20221213 23:19:56 @agent_ppo2.py:143][0m Total time:       7.41 min
[32m[20221213 23:19:56 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221213 23:19:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2348 --------------------------#
[32m[20221213 23:19:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |           0.0044 |          57.0312 |          15.3318 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0040 |          55.1588 |          15.3600 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0045 |          54.9249 |          15.3246 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0081 |          54.3881 |          15.3330 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0077 |          54.1756 |          15.3226 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0070 |          53.8885 |          15.3210 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0100 |          53.8071 |          15.3384 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0105 |          53.5951 |          15.3135 |
[32m[20221213 23:19:57 @agent_ppo2.py:185][0m |          -0.0109 |          53.6883 |          15.3383 |
[32m[20221213 23:19:58 @agent_ppo2.py:185][0m |          -0.0092 |          53.8555 |          15.3270 |
[32m[20221213 23:19:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:19:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.31
[32m[20221213 23:19:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.22
[32m[20221213 23:19:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.77
[32m[20221213 23:19:58 @agent_ppo2.py:143][0m Total time:       7.43 min
[32m[20221213 23:19:58 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221213 23:19:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2349 --------------------------#
[32m[20221213 23:19:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:19:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:58 @agent_ppo2.py:185][0m |           0.0111 |          57.0602 |          15.3272 |
[32m[20221213 23:19:58 @agent_ppo2.py:185][0m |          -0.0066 |          50.2959 |          15.3065 |
[32m[20221213 23:19:58 @agent_ppo2.py:185][0m |          -0.0073 |          50.0745 |          15.3036 |
[32m[20221213 23:19:58 @agent_ppo2.py:185][0m |          -0.0077 |          49.9652 |          15.3046 |
[32m[20221213 23:19:58 @agent_ppo2.py:185][0m |          -0.0066 |          50.0673 |          15.3049 |
[32m[20221213 23:19:58 @agent_ppo2.py:185][0m |          -0.0088 |          49.7780 |          15.2890 |
[32m[20221213 23:19:59 @agent_ppo2.py:185][0m |          -0.0097 |          49.6689 |          15.2677 |
[32m[20221213 23:19:59 @agent_ppo2.py:185][0m |          -0.0090 |          49.5454 |          15.2775 |
[32m[20221213 23:19:59 @agent_ppo2.py:185][0m |          -0.0121 |          49.5495 |          15.2625 |
[32m[20221213 23:19:59 @agent_ppo2.py:185][0m |          -0.0035 |          51.8353 |          15.2642 |
[32m[20221213 23:19:59 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:19:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.14
[32m[20221213 23:19:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.26
[32m[20221213 23:19:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.68
[32m[20221213 23:19:59 @agent_ppo2.py:143][0m Total time:       7.45 min
[32m[20221213 23:19:59 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221213 23:19:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2350 --------------------------#
[32m[20221213 23:19:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:19:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:19:59 @agent_ppo2.py:185][0m |          -0.0022 |          47.0552 |          15.3897 |
[32m[20221213 23:19:59 @agent_ppo2.py:185][0m |          -0.0014 |          46.7263 |          15.3832 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0039 |          46.5162 |          15.3898 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0066 |          45.9130 |          15.3818 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0082 |          45.8158 |          15.3939 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0097 |          45.6808 |          15.3863 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0102 |          45.6188 |          15.3925 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0110 |          45.5440 |          15.3832 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0023 |          47.4535 |          15.3761 |
[32m[20221213 23:20:00 @agent_ppo2.py:185][0m |          -0.0081 |          45.4436 |          15.3645 |
[32m[20221213 23:20:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:20:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.07
[32m[20221213 23:20:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.47
[32m[20221213 23:20:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.25
[32m[20221213 23:20:00 @agent_ppo2.py:143][0m Total time:       7.47 min
[32m[20221213 23:20:00 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221213 23:20:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2351 --------------------------#
[32m[20221213 23:20:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |           0.0000 |          59.7003 |          15.4057 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |           0.0037 |          57.8150 |          15.4151 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0057 |          56.3754 |          15.3886 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0072 |          55.4289 |          15.4028 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0076 |          54.8958 |          15.3828 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0100 |          54.6531 |          15.3929 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0114 |          54.5708 |          15.3949 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0094 |          54.1739 |          15.4040 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0101 |          54.4721 |          15.3853 |
[32m[20221213 23:20:01 @agent_ppo2.py:185][0m |          -0.0104 |          54.0130 |          15.3766 |
[32m[20221213 23:20:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:20:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.71
[32m[20221213 23:20:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.81
[32m[20221213 23:20:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.50
[32m[20221213 23:20:02 @agent_ppo2.py:143][0m Total time:       7.49 min
[32m[20221213 23:20:02 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221213 23:20:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2352 --------------------------#
[32m[20221213 23:20:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:02 @agent_ppo2.py:185][0m |           0.0015 |          59.3287 |          15.4440 |
[32m[20221213 23:20:02 @agent_ppo2.py:185][0m |          -0.0059 |          55.3239 |          15.4721 |
[32m[20221213 23:20:02 @agent_ppo2.py:185][0m |          -0.0074 |          53.5087 |          15.4818 |
[32m[20221213 23:20:02 @agent_ppo2.py:185][0m |          -0.0080 |          52.5267 |          15.4702 |
[32m[20221213 23:20:02 @agent_ppo2.py:185][0m |          -0.0072 |          51.4095 |          15.4542 |
[32m[20221213 23:20:02 @agent_ppo2.py:185][0m |          -0.0114 |          50.6549 |          15.4615 |
[32m[20221213 23:20:02 @agent_ppo2.py:185][0m |          -0.0122 |          50.0058 |          15.4687 |
[32m[20221213 23:20:03 @agent_ppo2.py:185][0m |          -0.0123 |          49.6726 |          15.4538 |
[32m[20221213 23:20:03 @agent_ppo2.py:185][0m |          -0.0044 |          52.1452 |          15.4534 |
[32m[20221213 23:20:03 @agent_ppo2.py:185][0m |          -0.0126 |          49.1797 |          15.4630 |
[32m[20221213 23:20:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:20:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.35
[32m[20221213 23:20:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.00
[32m[20221213 23:20:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.25
[32m[20221213 23:20:03 @agent_ppo2.py:143][0m Total time:       7.51 min
[32m[20221213 23:20:03 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221213 23:20:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2353 --------------------------#
[32m[20221213 23:20:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:20:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:03 @agent_ppo2.py:185][0m |          -0.0048 |          24.5950 |          15.4360 |
[32m[20221213 23:20:03 @agent_ppo2.py:185][0m |          -0.0057 |          21.0871 |          15.4213 |
[32m[20221213 23:20:03 @agent_ppo2.py:185][0m |          -0.0055 |          20.1777 |          15.4331 |
[32m[20221213 23:20:03 @agent_ppo2.py:185][0m |          -0.0059 |          19.9550 |          15.4273 |
[32m[20221213 23:20:04 @agent_ppo2.py:185][0m |          -0.0120 |          19.7014 |          15.4243 |
[32m[20221213 23:20:04 @agent_ppo2.py:185][0m |          -0.0107 |          19.0881 |          15.4357 |
[32m[20221213 23:20:04 @agent_ppo2.py:185][0m |          -0.0130 |          18.9485 |          15.4319 |
[32m[20221213 23:20:04 @agent_ppo2.py:185][0m |          -0.0160 |          18.8585 |          15.4396 |
[32m[20221213 23:20:04 @agent_ppo2.py:185][0m |          -0.0132 |          18.5172 |          15.4305 |
[32m[20221213 23:20:04 @agent_ppo2.py:185][0m |          -0.0088 |          18.4499 |          15.4310 |
[32m[20221213 23:20:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:20:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.38
[32m[20221213 23:20:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.55
[32m[20221213 23:20:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.79
[32m[20221213 23:20:04 @agent_ppo2.py:143][0m Total time:       7.54 min
[32m[20221213 23:20:04 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221213 23:20:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2354 --------------------------#
[32m[20221213 23:20:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:20:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0038 |          55.8158 |          15.4960 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0023 |          55.9229 |          15.4799 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0057 |          54.2390 |          15.4705 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0060 |          54.0125 |          15.4747 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0091 |          53.9008 |          15.4697 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |           0.0018 |          61.2980 |          15.4641 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0086 |          53.8802 |          15.4275 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0082 |          53.2822 |          15.4483 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0094 |          53.1756 |          15.4299 |
[32m[20221213 23:20:05 @agent_ppo2.py:185][0m |          -0.0125 |          53.0282 |          15.4263 |
[32m[20221213 23:20:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:20:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.63
[32m[20221213 23:20:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.02
[32m[20221213 23:20:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.70
[32m[20221213 23:20:05 @agent_ppo2.py:143][0m Total time:       7.56 min
[32m[20221213 23:20:05 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221213 23:20:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2355 --------------------------#
[32m[20221213 23:20:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0014 |          43.0184 |          15.2391 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0026 |          39.4420 |          15.2200 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0060 |          38.3446 |          15.1985 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0057 |          37.7472 |          15.1891 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0132 |          37.2472 |          15.1727 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0078 |          37.0920 |          15.1761 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0088 |          36.7501 |          15.1552 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0099 |          36.4869 |          15.1464 |
[32m[20221213 23:20:06 @agent_ppo2.py:185][0m |          -0.0134 |          36.1724 |          15.1314 |
[32m[20221213 23:20:07 @agent_ppo2.py:185][0m |          -0.0135 |          36.0892 |          15.1181 |
[32m[20221213 23:20:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.91
[32m[20221213 23:20:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.49
[32m[20221213 23:20:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.06
[32m[20221213 23:20:07 @agent_ppo2.py:143][0m Total time:       7.58 min
[32m[20221213 23:20:07 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221213 23:20:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2356 --------------------------#
[32m[20221213 23:20:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:07 @agent_ppo2.py:185][0m |          -0.0024 |          48.2688 |          15.2460 |
[32m[20221213 23:20:07 @agent_ppo2.py:185][0m |          -0.0078 |          44.7011 |          15.2211 |
[32m[20221213 23:20:07 @agent_ppo2.py:185][0m |          -0.0088 |          43.5383 |          15.2326 |
[32m[20221213 23:20:07 @agent_ppo2.py:185][0m |          -0.0099 |          42.9895 |          15.2271 |
[32m[20221213 23:20:07 @agent_ppo2.py:185][0m |          -0.0117 |          42.5182 |          15.2309 |
[32m[20221213 23:20:07 @agent_ppo2.py:185][0m |          -0.0100 |          42.2920 |          15.2206 |
[32m[20221213 23:20:08 @agent_ppo2.py:185][0m |          -0.0119 |          41.9980 |          15.2239 |
[32m[20221213 23:20:08 @agent_ppo2.py:185][0m |          -0.0119 |          41.7479 |          15.2373 |
[32m[20221213 23:20:08 @agent_ppo2.py:185][0m |          -0.0149 |          41.4884 |          15.2266 |
[32m[20221213 23:20:08 @agent_ppo2.py:185][0m |          -0.0130 |          41.3389 |          15.2219 |
[32m[20221213 23:20:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:20:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.81
[32m[20221213 23:20:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.52
[32m[20221213 23:20:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.63
[32m[20221213 23:20:08 @agent_ppo2.py:143][0m Total time:       7.60 min
[32m[20221213 23:20:08 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221213 23:20:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2357 --------------------------#
[32m[20221213 23:20:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:08 @agent_ppo2.py:185][0m |          -0.0029 |          53.5421 |          15.2624 |
[32m[20221213 23:20:08 @agent_ppo2.py:185][0m |          -0.0064 |          50.4979 |          15.2239 |
[32m[20221213 23:20:08 @agent_ppo2.py:185][0m |          -0.0038 |          50.2459 |          15.2007 |
[32m[20221213 23:20:09 @agent_ppo2.py:185][0m |          -0.0102 |          49.5871 |          15.2028 |
[32m[20221213 23:20:09 @agent_ppo2.py:185][0m |          -0.0066 |          49.3840 |          15.2064 |
[32m[20221213 23:20:09 @agent_ppo2.py:185][0m |          -0.0095 |          49.1729 |          15.1862 |
[32m[20221213 23:20:09 @agent_ppo2.py:185][0m |          -0.0106 |          49.0135 |          15.1625 |
[32m[20221213 23:20:09 @agent_ppo2.py:185][0m |          -0.0057 |          50.0668 |          15.1793 |
[32m[20221213 23:20:09 @agent_ppo2.py:185][0m |          -0.0109 |          48.7861 |          15.1580 |
[32m[20221213 23:20:09 @agent_ppo2.py:185][0m |          -0.0029 |          50.5662 |          15.1701 |
[32m[20221213 23:20:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:20:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.70
[32m[20221213 23:20:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.99
[32m[20221213 23:20:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.64
[32m[20221213 23:20:09 @agent_ppo2.py:143][0m Total time:       7.62 min
[32m[20221213 23:20:09 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221213 23:20:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2358 --------------------------#
[32m[20221213 23:20:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0001 |          44.0476 |          15.5077 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0064 |          38.8380 |          15.4890 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0106 |          37.6347 |          15.4919 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0059 |          37.2406 |          15.4750 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0107 |          36.5619 |          15.4944 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0119 |          36.1734 |          15.4945 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0045 |          38.5877 |          15.4854 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0100 |          35.7132 |          15.5081 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0162 |          35.5321 |          15.5024 |
[32m[20221213 23:20:10 @agent_ppo2.py:185][0m |          -0.0140 |          35.2048 |          15.5015 |
[32m[20221213 23:20:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.92
[32m[20221213 23:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.27
[32m[20221213 23:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.86
[32m[20221213 23:20:10 @agent_ppo2.py:143][0m Total time:       7.64 min
[32m[20221213 23:20:10 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221213 23:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2359 --------------------------#
[32m[20221213 23:20:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:20:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |           0.0016 |          47.9760 |          15.1129 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0048 |          44.1828 |          15.0697 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0070 |          43.0406 |          15.0495 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0113 |          42.1541 |          15.0438 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0125 |          41.5979 |          15.0468 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0002 |          45.7187 |          15.0060 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0124 |          41.3066 |          15.0149 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0169 |          40.7972 |          15.0136 |
[32m[20221213 23:20:11 @agent_ppo2.py:185][0m |          -0.0153 |          40.5412 |          15.0031 |
[32m[20221213 23:20:12 @agent_ppo2.py:185][0m |          -0.0151 |          40.4081 |          14.9637 |
[32m[20221213 23:20:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.17
[32m[20221213 23:20:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.63
[32m[20221213 23:20:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.51
[32m[20221213 23:20:12 @agent_ppo2.py:143][0m Total time:       7.66 min
[32m[20221213 23:20:12 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221213 23:20:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2360 --------------------------#
[32m[20221213 23:20:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:20:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:12 @agent_ppo2.py:185][0m |          -0.0032 |          35.8776 |          15.1911 |
[32m[20221213 23:20:12 @agent_ppo2.py:185][0m |           0.0001 |          32.2442 |          15.1904 |
[32m[20221213 23:20:12 @agent_ppo2.py:185][0m |          -0.0081 |          30.6690 |          15.1931 |
[32m[20221213 23:20:12 @agent_ppo2.py:185][0m |          -0.0025 |          30.1092 |          15.2009 |
[32m[20221213 23:20:12 @agent_ppo2.py:185][0m |          -0.0010 |          30.5095 |          15.1983 |
[32m[20221213 23:20:12 @agent_ppo2.py:185][0m |          -0.0105 |          29.7666 |          15.2076 |
[32m[20221213 23:20:13 @agent_ppo2.py:185][0m |          -0.0104 |          28.9872 |          15.2134 |
[32m[20221213 23:20:13 @agent_ppo2.py:185][0m |          -0.0087 |          28.6924 |          15.2106 |
[32m[20221213 23:20:13 @agent_ppo2.py:185][0m |          -0.0106 |          28.4945 |          15.2027 |
[32m[20221213 23:20:13 @agent_ppo2.py:185][0m |          -0.0119 |          28.2112 |          15.2109 |
[32m[20221213 23:20:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:20:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.06
[32m[20221213 23:20:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.13
[32m[20221213 23:20:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.44
[32m[20221213 23:20:13 @agent_ppo2.py:143][0m Total time:       7.68 min
[32m[20221213 23:20:13 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221213 23:20:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2361 --------------------------#
[32m[20221213 23:20:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:13 @agent_ppo2.py:185][0m |           0.0010 |          28.1663 |          15.1872 |
[32m[20221213 23:20:13 @agent_ppo2.py:185][0m |          -0.0119 |          23.2332 |          15.1641 |
[32m[20221213 23:20:13 @agent_ppo2.py:185][0m |          -0.0122 |          21.4104 |          15.1676 |
[32m[20221213 23:20:14 @agent_ppo2.py:185][0m |          -0.0089 |          20.1156 |          15.1638 |
[32m[20221213 23:20:14 @agent_ppo2.py:185][0m |          -0.0072 |          19.5657 |          15.1610 |
[32m[20221213 23:20:14 @agent_ppo2.py:185][0m |          -0.0182 |          18.6765 |          15.1437 |
[32m[20221213 23:20:14 @agent_ppo2.py:185][0m |          -0.0168 |          18.3331 |          15.1607 |
[32m[20221213 23:20:14 @agent_ppo2.py:185][0m |          -0.0146 |          17.9250 |          15.1486 |
[32m[20221213 23:20:14 @agent_ppo2.py:185][0m |          -0.0150 |          17.3975 |          15.1463 |
[32m[20221213 23:20:14 @agent_ppo2.py:185][0m |          -0.0206 |          17.0826 |          15.1309 |
[32m[20221213 23:20:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.10
[32m[20221213 23:20:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.28
[32m[20221213 23:20:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.29
[32m[20221213 23:20:14 @agent_ppo2.py:143][0m Total time:       7.70 min
[32m[20221213 23:20:14 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221213 23:20:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2362 --------------------------#
[32m[20221213 23:20:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0012 |          23.9699 |          15.3423 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0062 |          21.0576 |          15.3404 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0059 |          19.8208 |          15.3324 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0111 |          19.1226 |          15.3157 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0028 |          18.4269 |          15.3138 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0094 |          18.1090 |          15.3069 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0095 |          17.7000 |          15.3100 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0019 |          17.4342 |          15.2867 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0142 |          17.1600 |          15.3065 |
[32m[20221213 23:20:15 @agent_ppo2.py:185][0m |          -0.0113 |          16.9252 |          15.2894 |
[32m[20221213 23:20:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:20:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.74
[32m[20221213 23:20:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.33
[32m[20221213 23:20:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 346.62
[32m[20221213 23:20:15 @agent_ppo2.py:143][0m Total time:       7.72 min
[32m[20221213 23:20:15 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221213 23:20:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2363 --------------------------#
[32m[20221213 23:20:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |           0.0031 |          34.2918 |          15.3214 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |          -0.0038 |          30.6418 |          15.3115 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |          -0.0014 |          31.2494 |          15.3038 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |          -0.0034 |          28.9078 |          15.2788 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |          -0.0098 |          28.4691 |          15.2902 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |          -0.0100 |          28.0092 |          15.2842 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |           0.0017 |          29.1529 |          15.2909 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |          -0.0075 |          27.5371 |          15.2740 |
[32m[20221213 23:20:16 @agent_ppo2.py:185][0m |          -0.0182 |          27.2316 |          15.2785 |
[32m[20221213 23:20:17 @agent_ppo2.py:185][0m |          -0.0073 |          27.2032 |          15.2868 |
[32m[20221213 23:20:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.52
[32m[20221213 23:20:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.54
[32m[20221213 23:20:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.23
[32m[20221213 23:20:17 @agent_ppo2.py:143][0m Total time:       7.75 min
[32m[20221213 23:20:17 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221213 23:20:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2364 --------------------------#
[32m[20221213 23:20:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:17 @agent_ppo2.py:185][0m |          -0.0012 |          45.2783 |          15.2007 |
[32m[20221213 23:20:17 @agent_ppo2.py:185][0m |          -0.0042 |          42.5734 |          15.1942 |
[32m[20221213 23:20:17 @agent_ppo2.py:185][0m |          -0.0080 |          41.4629 |          15.1889 |
[32m[20221213 23:20:17 @agent_ppo2.py:185][0m |          -0.0098 |          40.9064 |          15.1966 |
[32m[20221213 23:20:17 @agent_ppo2.py:185][0m |          -0.0096 |          40.3976 |          15.1879 |
[32m[20221213 23:20:17 @agent_ppo2.py:185][0m |          -0.0077 |          39.9453 |          15.1771 |
[32m[20221213 23:20:18 @agent_ppo2.py:185][0m |          -0.0167 |          39.5232 |          15.1786 |
[32m[20221213 23:20:18 @agent_ppo2.py:185][0m |          -0.0089 |          39.2664 |          15.1808 |
[32m[20221213 23:20:18 @agent_ppo2.py:185][0m |          -0.0138 |          39.1302 |          15.1808 |
[32m[20221213 23:20:18 @agent_ppo2.py:185][0m |          -0.0167 |          39.0435 |          15.1924 |
[32m[20221213 23:20:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.63
[32m[20221213 23:20:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.34
[32m[20221213 23:20:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.80
[32m[20221213 23:20:18 @agent_ppo2.py:143][0m Total time:       7.77 min
[32m[20221213 23:20:18 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221213 23:20:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2365 --------------------------#
[32m[20221213 23:20:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:18 @agent_ppo2.py:185][0m |           0.0002 |          46.5767 |          15.2886 |
[32m[20221213 23:20:18 @agent_ppo2.py:185][0m |          -0.0053 |          44.7889 |          15.2965 |
[32m[20221213 23:20:18 @agent_ppo2.py:185][0m |          -0.0052 |          44.4479 |          15.2790 |
[32m[20221213 23:20:19 @agent_ppo2.py:185][0m |          -0.0024 |          44.6325 |          15.2820 |
[32m[20221213 23:20:19 @agent_ppo2.py:185][0m |          -0.0078 |          44.1163 |          15.2632 |
[32m[20221213 23:20:19 @agent_ppo2.py:185][0m |          -0.0087 |          43.8630 |          15.2712 |
[32m[20221213 23:20:19 @agent_ppo2.py:185][0m |          -0.0184 |          43.8870 |          15.2805 |
[32m[20221213 23:20:19 @agent_ppo2.py:185][0m |          -0.0087 |          43.6897 |          15.2552 |
[32m[20221213 23:20:19 @agent_ppo2.py:185][0m |          -0.0051 |          44.2398 |          15.2691 |
[32m[20221213 23:20:19 @agent_ppo2.py:185][0m |          -0.0102 |          43.6418 |          15.2790 |
[32m[20221213 23:20:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:20:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.65
[32m[20221213 23:20:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.61
[32m[20221213 23:20:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.02
[32m[20221213 23:20:19 @agent_ppo2.py:143][0m Total time:       7.79 min
[32m[20221213 23:20:19 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221213 23:20:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2366 --------------------------#
[32m[20221213 23:20:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0000 |          53.9362 |          15.3415 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0060 |          50.0790 |          15.3152 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0050 |          48.6501 |          15.3145 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0081 |          47.9269 |          15.3002 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0123 |          46.6903 |          15.3117 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0138 |          46.1157 |          15.3148 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0070 |          47.6875 |          15.3335 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0172 |          45.1612 |          15.3331 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0191 |          44.8805 |          15.3305 |
[32m[20221213 23:20:20 @agent_ppo2.py:185][0m |          -0.0101 |          45.7038 |          15.3387 |
[32m[20221213 23:20:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.55
[32m[20221213 23:20:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.01
[32m[20221213 23:20:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.65
[32m[20221213 23:20:20 @agent_ppo2.py:143][0m Total time:       7.81 min
[32m[20221213 23:20:20 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221213 23:20:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2367 --------------------------#
[32m[20221213 23:20:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |           0.0012 |          46.6592 |          15.1722 |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |           0.0048 |          46.7343 |          15.2047 |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |          -0.0049 |          43.9748 |          15.1849 |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |          -0.0095 |          43.4191 |          15.1851 |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |          -0.0072 |          43.0544 |          15.1760 |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |          -0.0107 |          42.8509 |          15.1772 |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |          -0.0123 |          42.7374 |          15.1638 |
[32m[20221213 23:20:21 @agent_ppo2.py:185][0m |          -0.0122 |          42.4264 |          15.1754 |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |          -0.0116 |          42.2184 |          15.1537 |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |          -0.0127 |          42.0365 |          15.1425 |
[32m[20221213 23:20:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.69
[32m[20221213 23:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.36
[32m[20221213 23:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.14
[32m[20221213 23:20:22 @agent_ppo2.py:143][0m Total time:       7.83 min
[32m[20221213 23:20:22 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221213 23:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2368 --------------------------#
[32m[20221213 23:20:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |           0.0015 |          29.8669 |          15.1090 |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |          -0.0054 |          24.8393 |          15.0994 |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |          -0.0074 |          23.5757 |          15.0887 |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |          -0.0143 |          22.8525 |          15.0956 |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |          -0.0087 |          22.1226 |          15.0858 |
[32m[20221213 23:20:22 @agent_ppo2.py:185][0m |          -0.0162 |          22.0681 |          15.0696 |
[32m[20221213 23:20:23 @agent_ppo2.py:185][0m |          -0.0057 |          21.5589 |          15.0553 |
[32m[20221213 23:20:23 @agent_ppo2.py:185][0m |          -0.0135 |          21.2168 |          15.0529 |
[32m[20221213 23:20:23 @agent_ppo2.py:185][0m |          -0.0161 |          20.9673 |          15.0511 |
[32m[20221213 23:20:23 @agent_ppo2.py:185][0m |          -0.0136 |          20.7665 |          15.0567 |
[32m[20221213 23:20:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.46
[32m[20221213 23:20:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.84
[32m[20221213 23:20:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.56
[32m[20221213 23:20:23 @agent_ppo2.py:143][0m Total time:       7.85 min
[32m[20221213 23:20:23 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221213 23:20:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2369 --------------------------#
[32m[20221213 23:20:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:23 @agent_ppo2.py:185][0m |           0.0006 |          67.3292 |          15.1464 |
[32m[20221213 23:20:23 @agent_ppo2.py:185][0m |          -0.0055 |          65.0460 |          15.1313 |
[32m[20221213 23:20:23 @agent_ppo2.py:185][0m |          -0.0087 |          64.2473 |          15.1369 |
[32m[20221213 23:20:24 @agent_ppo2.py:185][0m |          -0.0067 |          64.0990 |          15.1188 |
[32m[20221213 23:20:24 @agent_ppo2.py:185][0m |          -0.0079 |          63.5637 |          15.0873 |
[32m[20221213 23:20:24 @agent_ppo2.py:185][0m |          -0.0097 |          63.4278 |          15.1140 |
[32m[20221213 23:20:24 @agent_ppo2.py:185][0m |          -0.0112 |          62.9765 |          15.1002 |
[32m[20221213 23:20:24 @agent_ppo2.py:185][0m |          -0.0135 |          62.9432 |          15.0864 |
[32m[20221213 23:20:24 @agent_ppo2.py:185][0m |          -0.0113 |          62.8102 |          15.0785 |
[32m[20221213 23:20:24 @agent_ppo2.py:185][0m |          -0.0136 |          62.6198 |          15.0835 |
[32m[20221213 23:20:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.65
[32m[20221213 23:20:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.08
[32m[20221213 23:20:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.08
[32m[20221213 23:20:24 @agent_ppo2.py:143][0m Total time:       7.87 min
[32m[20221213 23:20:24 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221213 23:20:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2370 --------------------------#
[32m[20221213 23:20:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:20:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |           0.0003 |          43.1324 |          14.9641 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |           0.0002 |          39.6279 |          14.9800 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0069 |          38.3520 |          14.9699 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0076 |          37.3994 |          14.9615 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0119 |          36.7323 |          14.9701 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0089 |          36.4741 |          14.9669 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0149 |          35.7677 |          14.9654 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0136 |          35.8116 |          14.9701 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0124 |          35.2319 |          14.9786 |
[32m[20221213 23:20:25 @agent_ppo2.py:185][0m |          -0.0141 |          34.9844 |          14.9688 |
[32m[20221213 23:20:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.38
[32m[20221213 23:20:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.65
[32m[20221213 23:20:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.92
[32m[20221213 23:20:25 @agent_ppo2.py:143][0m Total time:       7.89 min
[32m[20221213 23:20:25 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221213 23:20:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2371 --------------------------#
[32m[20221213 23:20:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |           0.0017 |          63.4213 |          15.0321 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |          -0.0056 |          59.3561 |          15.0237 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |           0.0039 |          62.9436 |          15.0083 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |          -0.0106 |          56.8539 |          14.9923 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |          -0.0022 |          61.0100 |          15.0058 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |          -0.0097 |          56.5626 |          14.9954 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |          -0.0118 |          55.4985 |          14.9962 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |          -0.0111 |          55.1969 |          15.0071 |
[32m[20221213 23:20:26 @agent_ppo2.py:185][0m |          -0.0135 |          54.9541 |          15.0124 |
[32m[20221213 23:20:27 @agent_ppo2.py:185][0m |          -0.0151 |          54.4907 |          14.9984 |
[32m[20221213 23:20:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.72
[32m[20221213 23:20:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.86
[32m[20221213 23:20:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.51
[32m[20221213 23:20:27 @agent_ppo2.py:143][0m Total time:       7.91 min
[32m[20221213 23:20:27 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221213 23:20:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2372 --------------------------#
[32m[20221213 23:20:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:27 @agent_ppo2.py:185][0m |           0.0014 |          57.6102 |          15.2273 |
[32m[20221213 23:20:27 @agent_ppo2.py:185][0m |           0.0018 |          57.5929 |          15.2148 |
[32m[20221213 23:20:27 @agent_ppo2.py:185][0m |           0.0029 |          59.0741 |          15.2092 |
[32m[20221213 23:20:27 @agent_ppo2.py:185][0m |          -0.0065 |          52.2115 |          15.1862 |
[32m[20221213 23:20:27 @agent_ppo2.py:185][0m |          -0.0045 |          54.8179 |          15.2116 |
[32m[20221213 23:20:27 @agent_ppo2.py:185][0m |          -0.0114 |          50.9054 |          15.2170 |
[32m[20221213 23:20:28 @agent_ppo2.py:185][0m |          -0.0132 |          50.3458 |          15.2188 |
[32m[20221213 23:20:28 @agent_ppo2.py:185][0m |          -0.0134 |          49.9388 |          15.2180 |
[32m[20221213 23:20:28 @agent_ppo2.py:185][0m |          -0.0134 |          49.5821 |          15.2300 |
[32m[20221213 23:20:28 @agent_ppo2.py:185][0m |          -0.0115 |          49.2952 |          15.2221 |
[32m[20221213 23:20:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:20:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.68
[32m[20221213 23:20:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.60
[32m[20221213 23:20:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.44
[32m[20221213 23:20:28 @agent_ppo2.py:143][0m Total time:       7.93 min
[32m[20221213 23:20:28 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221213 23:20:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2373 --------------------------#
[32m[20221213 23:20:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:28 @agent_ppo2.py:185][0m |           0.0140 |          59.6269 |          15.1645 |
[32m[20221213 23:20:28 @agent_ppo2.py:185][0m |           0.0013 |          49.2180 |          15.1395 |
[32m[20221213 23:20:28 @agent_ppo2.py:185][0m |          -0.0107 |          47.6722 |          15.1196 |
[32m[20221213 23:20:29 @agent_ppo2.py:185][0m |          -0.0101 |          46.7207 |          15.1298 |
[32m[20221213 23:20:29 @agent_ppo2.py:185][0m |          -0.0091 |          46.4614 |          15.1215 |
[32m[20221213 23:20:29 @agent_ppo2.py:185][0m |           0.0005 |          48.5892 |          15.1338 |
[32m[20221213 23:20:29 @agent_ppo2.py:185][0m |           0.0023 |          55.0387 |          15.1180 |
[32m[20221213 23:20:29 @agent_ppo2.py:185][0m |          -0.0130 |          45.3982 |          15.1332 |
[32m[20221213 23:20:29 @agent_ppo2.py:185][0m |          -0.0129 |          45.1658 |          15.1203 |
[32m[20221213 23:20:29 @agent_ppo2.py:185][0m |          -0.0032 |          47.6393 |          15.1194 |
[32m[20221213 23:20:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.65
[32m[20221213 23:20:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.93
[32m[20221213 23:20:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.88
[32m[20221213 23:20:29 @agent_ppo2.py:143][0m Total time:       7.95 min
[32m[20221213 23:20:29 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221213 23:20:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2374 --------------------------#
[32m[20221213 23:20:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |           0.0044 |          38.5264 |          14.8655 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0051 |          32.1695 |          14.8179 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0049 |          29.8078 |          14.8356 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0086 |          28.2268 |          14.8398 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0096 |          27.3491 |          14.8590 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0092 |          26.6266 |          14.8265 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0102 |          25.9988 |          14.8476 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0024 |          26.7456 |          14.8254 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0151 |          25.2742 |          14.8115 |
[32m[20221213 23:20:30 @agent_ppo2.py:185][0m |          -0.0143 |          25.4593 |          14.8410 |
[32m[20221213 23:20:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:20:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.76
[32m[20221213 23:20:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.55
[32m[20221213 23:20:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.68
[32m[20221213 23:20:30 @agent_ppo2.py:143][0m Total time:       7.97 min
[32m[20221213 23:20:30 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221213 23:20:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2375 --------------------------#
[32m[20221213 23:20:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |           0.0004 |          46.0020 |          15.2109 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0035 |          43.3684 |          15.2303 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0048 |          42.7985 |          15.1979 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0064 |          42.5465 |          15.2116 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0054 |          42.3031 |          15.2207 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0069 |          41.9479 |          15.2382 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0096 |          41.7087 |          15.2207 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0120 |          41.6354 |          15.2365 |
[32m[20221213 23:20:31 @agent_ppo2.py:185][0m |          -0.0098 |          41.2984 |          15.2361 |
[32m[20221213 23:20:32 @agent_ppo2.py:185][0m |          -0.0114 |          41.1962 |          15.2509 |
[32m[20221213 23:20:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.61
[32m[20221213 23:20:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.20
[32m[20221213 23:20:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.61
[32m[20221213 23:20:32 @agent_ppo2.py:143][0m Total time:       8.00 min
[32m[20221213 23:20:32 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221213 23:20:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2376 --------------------------#
[32m[20221213 23:20:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:32 @agent_ppo2.py:185][0m |          -0.0057 |          59.2118 |          14.9981 |
[32m[20221213 23:20:32 @agent_ppo2.py:185][0m |          -0.0058 |          50.3295 |          14.9905 |
[32m[20221213 23:20:32 @agent_ppo2.py:185][0m |          -0.0074 |          48.3903 |          14.9772 |
[32m[20221213 23:20:32 @agent_ppo2.py:185][0m |          -0.0111 |          47.2826 |          14.9878 |
[32m[20221213 23:20:32 @agent_ppo2.py:185][0m |          -0.0091 |          46.4545 |          14.9902 |
[32m[20221213 23:20:32 @agent_ppo2.py:185][0m |          -0.0102 |          45.6429 |          14.9594 |
[32m[20221213 23:20:33 @agent_ppo2.py:185][0m |          -0.0143 |          45.0710 |          14.9529 |
[32m[20221213 23:20:33 @agent_ppo2.py:185][0m |          -0.0140 |          44.6544 |          14.9623 |
[32m[20221213 23:20:33 @agent_ppo2.py:185][0m |          -0.0109 |          44.3439 |          14.9501 |
[32m[20221213 23:20:33 @agent_ppo2.py:185][0m |          -0.0171 |          43.6995 |          14.9544 |
[32m[20221213 23:20:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:20:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.00
[32m[20221213 23:20:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.93
[32m[20221213 23:20:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.53
[32m[20221213 23:20:33 @agent_ppo2.py:143][0m Total time:       8.02 min
[32m[20221213 23:20:33 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221213 23:20:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2377 --------------------------#
[32m[20221213 23:20:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:33 @agent_ppo2.py:185][0m |           0.0010 |          52.9465 |          14.9723 |
[32m[20221213 23:20:33 @agent_ppo2.py:185][0m |          -0.0067 |          49.8707 |          14.9343 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |          -0.0033 |          52.3806 |          14.9439 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |          -0.0073 |          48.6437 |          14.9007 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |          -0.0130 |          48.3215 |          14.9406 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |           0.0001 |          55.9593 |          14.9145 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |          -0.0065 |          48.7824 |          14.9370 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |          -0.0161 |          47.5108 |          14.9273 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |          -0.0125 |          48.0558 |          14.9389 |
[32m[20221213 23:20:34 @agent_ppo2.py:185][0m |          -0.0095 |          47.7156 |          14.9372 |
[32m[20221213 23:20:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:20:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.40
[32m[20221213 23:20:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.21
[32m[20221213 23:20:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.11
[32m[20221213 23:20:34 @agent_ppo2.py:143][0m Total time:       8.04 min
[32m[20221213 23:20:34 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221213 23:20:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2378 --------------------------#
[32m[20221213 23:20:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |           0.0005 |          42.9471 |          14.8670 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0041 |          41.1067 |          14.8731 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0063 |          40.7260 |          14.8784 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0035 |          40.5268 |          14.8764 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0109 |          40.4201 |          14.8778 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0072 |          40.2486 |          14.8548 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0055 |          40.3354 |          14.8676 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0050 |          40.2341 |          14.8684 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0078 |          40.1302 |          14.8670 |
[32m[20221213 23:20:35 @agent_ppo2.py:185][0m |          -0.0060 |          40.1540 |          14.8581 |
[32m[20221213 23:20:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.35
[32m[20221213 23:20:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.43
[32m[20221213 23:20:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.06
[32m[20221213 23:20:36 @agent_ppo2.py:143][0m Total time:       8.06 min
[32m[20221213 23:20:36 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221213 23:20:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2379 --------------------------#
[32m[20221213 23:20:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |           0.0014 |          47.5019 |          15.1196 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |          -0.0003 |          42.4621 |          15.0965 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |          -0.0054 |          41.0125 |          15.1185 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |          -0.0039 |          40.2241 |          15.1280 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |           0.0000 |          39.9370 |          15.1380 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |           0.0072 |          45.1342 |          15.1319 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |          -0.0066 |          39.6750 |          15.1278 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |          -0.0018 |          39.9118 |          15.1508 |
[32m[20221213 23:20:36 @agent_ppo2.py:185][0m |          -0.0004 |          41.2826 |          15.1557 |
[32m[20221213 23:20:37 @agent_ppo2.py:185][0m |          -0.0065 |          38.9490 |          15.1644 |
[32m[20221213 23:20:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:20:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.55
[32m[20221213 23:20:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.32
[32m[20221213 23:20:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.84
[32m[20221213 23:20:37 @agent_ppo2.py:143][0m Total time:       8.08 min
[32m[20221213 23:20:37 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221213 23:20:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2380 --------------------------#
[32m[20221213 23:20:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:20:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:37 @agent_ppo2.py:185][0m |           0.0049 |          37.4260 |          15.0698 |
[32m[20221213 23:20:37 @agent_ppo2.py:185][0m |          -0.0033 |          30.9661 |          15.0487 |
[32m[20221213 23:20:37 @agent_ppo2.py:185][0m |          -0.0128 |          29.4221 |          15.0650 |
[32m[20221213 23:20:37 @agent_ppo2.py:185][0m |          -0.0035 |          28.3933 |          15.0709 |
[32m[20221213 23:20:37 @agent_ppo2.py:185][0m |          -0.0085 |          27.5705 |          15.0854 |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0095 |          27.1327 |          15.1057 |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0150 |          26.7624 |          15.1033 |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0072 |          26.9305 |          15.1046 |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0123 |          26.1105 |          15.1149 |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0181 |          25.9187 |          15.1284 |
[32m[20221213 23:20:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.61
[32m[20221213 23:20:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.92
[32m[20221213 23:20:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.14
[32m[20221213 23:20:38 @agent_ppo2.py:143][0m Total time:       8.10 min
[32m[20221213 23:20:38 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221213 23:20:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2381 --------------------------#
[32m[20221213 23:20:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0008 |          51.9213 |          15.3667 |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0034 |          51.3776 |          15.3785 |
[32m[20221213 23:20:38 @agent_ppo2.py:185][0m |          -0.0051 |          50.7887 |          15.3675 |
[32m[20221213 23:20:39 @agent_ppo2.py:185][0m |          -0.0083 |          50.5558 |          15.3385 |
[32m[20221213 23:20:39 @agent_ppo2.py:185][0m |          -0.0004 |          51.4180 |          15.3658 |
[32m[20221213 23:20:39 @agent_ppo2.py:185][0m |          -0.0069 |          50.3228 |          15.3464 |
[32m[20221213 23:20:39 @agent_ppo2.py:185][0m |          -0.0084 |          50.1563 |          15.3475 |
[32m[20221213 23:20:39 @agent_ppo2.py:185][0m |          -0.0092 |          50.0601 |          15.3468 |
[32m[20221213 23:20:39 @agent_ppo2.py:185][0m |          -0.0115 |          50.0454 |          15.3346 |
[32m[20221213 23:20:39 @agent_ppo2.py:185][0m |          -0.0099 |          49.8983 |          15.3303 |
[32m[20221213 23:20:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:20:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.46
[32m[20221213 23:20:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.38
[32m[20221213 23:20:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.67
[32m[20221213 23:20:39 @agent_ppo2.py:143][0m Total time:       8.12 min
[32m[20221213 23:20:39 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221213 23:20:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2382 --------------------------#
[32m[20221213 23:20:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0028 |          58.2087 |          15.3306 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0067 |          54.7121 |          15.3055 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0086 |          53.1242 |          15.3241 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0099 |          52.1119 |          15.3334 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0146 |          51.5766 |          15.3438 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0055 |          54.1918 |          15.3267 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0122 |          50.7276 |          15.3059 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0155 |          50.0827 |          15.3174 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0156 |          50.0977 |          15.3287 |
[32m[20221213 23:20:40 @agent_ppo2.py:185][0m |          -0.0162 |          49.6676 |          15.3256 |
[32m[20221213 23:20:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.19
[32m[20221213 23:20:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.19
[32m[20221213 23:20:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.01
[32m[20221213 23:20:40 @agent_ppo2.py:143][0m Total time:       8.14 min
[32m[20221213 23:20:40 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221213 23:20:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2383 --------------------------#
[32m[20221213 23:20:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |           0.0089 |          58.3075 |          15.1105 |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |          -0.0070 |          51.9488 |          15.1339 |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |          -0.0067 |          50.9399 |          15.1571 |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |          -0.0080 |          50.1787 |          15.1400 |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |          -0.0056 |          50.6117 |          15.1283 |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |          -0.0072 |          50.2439 |          15.1577 |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |          -0.0091 |          49.1718 |          15.1184 |
[32m[20221213 23:20:41 @agent_ppo2.py:185][0m |          -0.0037 |          50.1024 |          15.1277 |
[32m[20221213 23:20:42 @agent_ppo2.py:185][0m |          -0.0154 |          48.6245 |          15.1347 |
[32m[20221213 23:20:42 @agent_ppo2.py:185][0m |          -0.0144 |          48.7001 |          15.1201 |
[32m[20221213 23:20:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.72
[32m[20221213 23:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.21
[32m[20221213 23:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.94
[32m[20221213 23:20:42 @agent_ppo2.py:143][0m Total time:       8.16 min
[32m[20221213 23:20:42 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221213 23:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2384 --------------------------#
[32m[20221213 23:20:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:20:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:42 @agent_ppo2.py:185][0m |           0.0052 |          52.1084 |          15.1432 |
[32m[20221213 23:20:42 @agent_ppo2.py:185][0m |          -0.0073 |          46.6672 |          15.1245 |
[32m[20221213 23:20:42 @agent_ppo2.py:185][0m |          -0.0108 |          45.4539 |          15.1427 |
[32m[20221213 23:20:42 @agent_ppo2.py:185][0m |          -0.0017 |          46.0352 |          15.1234 |
[32m[20221213 23:20:42 @agent_ppo2.py:185][0m |          -0.0053 |          44.6228 |          15.1225 |
[32m[20221213 23:20:43 @agent_ppo2.py:185][0m |          -0.0097 |          43.9719 |          15.1313 |
[32m[20221213 23:20:43 @agent_ppo2.py:185][0m |          -0.0070 |          43.7957 |          15.1491 |
[32m[20221213 23:20:43 @agent_ppo2.py:185][0m |          -0.0134 |          43.6283 |          15.1371 |
[32m[20221213 23:20:43 @agent_ppo2.py:185][0m |          -0.0072 |          44.0883 |          15.1443 |
[32m[20221213 23:20:43 @agent_ppo2.py:185][0m |          -0.0130 |          43.0879 |          15.1287 |
[32m[20221213 23:20:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.84
[32m[20221213 23:20:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.00
[32m[20221213 23:20:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.37
[32m[20221213 23:20:43 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 526.37
[32m[20221213 23:20:43 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 526.37
[32m[20221213 23:20:43 @agent_ppo2.py:143][0m Total time:       8.18 min
[32m[20221213 23:20:43 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221213 23:20:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2385 --------------------------#
[32m[20221213 23:20:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:43 @agent_ppo2.py:185][0m |           0.0086 |          59.1301 |          15.2466 |
[32m[20221213 23:20:43 @agent_ppo2.py:185][0m |          -0.0018 |          51.2132 |          15.2103 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |          -0.0050 |          49.7951 |          15.1910 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |          -0.0006 |          49.7382 |          15.1905 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |          -0.0078 |          48.3850 |          15.1601 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |          -0.0067 |          47.5421 |          15.1457 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |           0.0023 |          49.4614 |          15.1269 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |          -0.0100 |          46.5400 |          15.1138 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |          -0.0143 |          46.0081 |          15.1120 |
[32m[20221213 23:20:44 @agent_ppo2.py:185][0m |          -0.0148 |          45.8831 |          15.1316 |
[32m[20221213 23:20:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.27
[32m[20221213 23:20:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.42
[32m[20221213 23:20:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.48
[32m[20221213 23:20:44 @agent_ppo2.py:143][0m Total time:       8.20 min
[32m[20221213 23:20:44 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221213 23:20:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2386 --------------------------#
[32m[20221213 23:20:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |           0.0018 |          38.1606 |          15.2892 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0024 |          34.3359 |          15.2976 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0070 |          32.8837 |          15.2989 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0123 |          32.0048 |          15.2829 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0064 |          31.2026 |          15.2545 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0117 |          30.6978 |          15.2491 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0137 |          30.1432 |          15.2472 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0157 |          29.8627 |          15.2483 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0134 |          29.5017 |          15.2302 |
[32m[20221213 23:20:45 @agent_ppo2.py:185][0m |          -0.0051 |          29.5463 |          15.2265 |
[32m[20221213 23:20:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:20:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.19
[32m[20221213 23:20:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.91
[32m[20221213 23:20:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.86
[32m[20221213 23:20:46 @agent_ppo2.py:143][0m Total time:       8.23 min
[32m[20221213 23:20:46 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221213 23:20:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2387 --------------------------#
[32m[20221213 23:20:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |           0.0030 |          57.6789 |          15.2436 |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |          -0.0032 |          53.7792 |          15.2348 |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |          -0.0058 |          52.7038 |          15.2414 |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |          -0.0092 |          52.0532 |          15.2460 |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |          -0.0126 |          51.6399 |          15.2671 |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |          -0.0122 |          51.2261 |          15.2772 |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |          -0.0130 |          50.8565 |          15.2791 |
[32m[20221213 23:20:46 @agent_ppo2.py:185][0m |          -0.0125 |          50.5849 |          15.2719 |
[32m[20221213 23:20:47 @agent_ppo2.py:185][0m |          -0.0137 |          50.3776 |          15.2891 |
[32m[20221213 23:20:47 @agent_ppo2.py:185][0m |          -0.0154 |          50.1404 |          15.2967 |
[32m[20221213 23:20:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.59
[32m[20221213 23:20:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.62
[32m[20221213 23:20:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.68
[32m[20221213 23:20:47 @agent_ppo2.py:143][0m Total time:       8.25 min
[32m[20221213 23:20:47 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221213 23:20:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2388 --------------------------#
[32m[20221213 23:20:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:47 @agent_ppo2.py:185][0m |           0.0006 |          53.6902 |          15.0375 |
[32m[20221213 23:20:47 @agent_ppo2.py:185][0m |          -0.0060 |          51.4868 |          15.0176 |
[32m[20221213 23:20:47 @agent_ppo2.py:185][0m |          -0.0068 |          50.4594 |          15.0101 |
[32m[20221213 23:20:47 @agent_ppo2.py:185][0m |          -0.0105 |          49.9309 |          14.9869 |
[32m[20221213 23:20:47 @agent_ppo2.py:185][0m |          -0.0101 |          49.4534 |          14.9992 |
[32m[20221213 23:20:48 @agent_ppo2.py:185][0m |          -0.0080 |          49.9105 |          14.9799 |
[32m[20221213 23:20:48 @agent_ppo2.py:185][0m |          -0.0116 |          48.9207 |          14.9393 |
[32m[20221213 23:20:48 @agent_ppo2.py:185][0m |          -0.0105 |          48.6108 |          14.9612 |
[32m[20221213 23:20:48 @agent_ppo2.py:185][0m |          -0.0158 |          48.4911 |          14.9523 |
[32m[20221213 23:20:48 @agent_ppo2.py:185][0m |          -0.0140 |          48.1320 |          14.9214 |
[32m[20221213 23:20:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.98
[32m[20221213 23:20:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.95
[32m[20221213 23:20:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.56
[32m[20221213 23:20:48 @agent_ppo2.py:143][0m Total time:       8.27 min
[32m[20221213 23:20:48 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221213 23:20:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2389 --------------------------#
[32m[20221213 23:20:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:48 @agent_ppo2.py:185][0m |          -0.0013 |          47.1983 |          15.1536 |
[32m[20221213 23:20:48 @agent_ppo2.py:185][0m |          -0.0072 |          42.7890 |          15.1325 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0103 |          41.9423 |          15.1343 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0028 |          41.7601 |          15.1441 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0051 |          40.8658 |          15.1553 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0083 |          40.1383 |          15.1553 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0153 |          39.7386 |          15.1562 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0121 |          39.4438 |          15.1464 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0112 |          39.3167 |          15.1493 |
[32m[20221213 23:20:49 @agent_ppo2.py:185][0m |          -0.0118 |          39.0105 |          15.1384 |
[32m[20221213 23:20:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.30
[32m[20221213 23:20:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.14
[32m[20221213 23:20:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.56
[32m[20221213 23:20:49 @agent_ppo2.py:143][0m Total time:       8.29 min
[32m[20221213 23:20:49 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221213 23:20:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2390 --------------------------#
[32m[20221213 23:20:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:20:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |           0.0000 |          37.8974 |          15.0347 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0070 |          33.6351 |          15.0173 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0028 |          32.5186 |          15.0115 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0081 |          31.6544 |          15.0044 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0040 |          31.3795 |          14.9886 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0100 |          30.8746 |          14.9730 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0077 |          30.5394 |          14.9646 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0140 |          30.1356 |          14.9584 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0164 |          29.8674 |          14.9431 |
[32m[20221213 23:20:50 @agent_ppo2.py:185][0m |          -0.0129 |          29.6942 |          14.9428 |
[32m[20221213 23:20:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.68
[32m[20221213 23:20:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.30
[32m[20221213 23:20:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.88
[32m[20221213 23:20:51 @agent_ppo2.py:143][0m Total time:       8.31 min
[32m[20221213 23:20:51 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221213 23:20:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2391 --------------------------#
[32m[20221213 23:20:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |           0.0009 |          55.2961 |          15.0375 |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |          -0.0079 |          49.4953 |          15.0163 |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |          -0.0126 |          48.0745 |          15.0200 |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |          -0.0017 |          47.5269 |          15.0148 |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |          -0.0113 |          46.5287 |          15.0199 |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |          -0.0141 |          45.6547 |          15.0077 |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |          -0.0003 |          46.7919 |          15.0116 |
[32m[20221213 23:20:51 @agent_ppo2.py:185][0m |          -0.0074 |          44.9657 |          15.0026 |
[32m[20221213 23:20:52 @agent_ppo2.py:185][0m |          -0.0111 |          44.5484 |          15.0149 |
[32m[20221213 23:20:52 @agent_ppo2.py:185][0m |          -0.0124 |          44.5455 |          15.0098 |
[32m[20221213 23:20:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:20:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.53
[32m[20221213 23:20:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.78
[32m[20221213 23:20:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.97
[32m[20221213 23:20:52 @agent_ppo2.py:143][0m Total time:       8.33 min
[32m[20221213 23:20:52 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221213 23:20:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2392 --------------------------#
[32m[20221213 23:20:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:52 @agent_ppo2.py:185][0m |           0.0014 |          49.0630 |          15.0070 |
[32m[20221213 23:20:52 @agent_ppo2.py:185][0m |           0.0018 |          44.4767 |          14.9947 |
[32m[20221213 23:20:52 @agent_ppo2.py:185][0m |          -0.0076 |          43.1739 |          14.9511 |
[32m[20221213 23:20:52 @agent_ppo2.py:185][0m |          -0.0087 |          41.6386 |          14.9471 |
[32m[20221213 23:20:52 @agent_ppo2.py:185][0m |          -0.0105 |          41.1252 |          14.9596 |
[32m[20221213 23:20:53 @agent_ppo2.py:185][0m |          -0.0129 |          40.2510 |          14.9495 |
[32m[20221213 23:20:53 @agent_ppo2.py:185][0m |          -0.0139 |          39.9054 |          14.9566 |
[32m[20221213 23:20:53 @agent_ppo2.py:185][0m |          -0.0123 |          39.4680 |          14.9337 |
[32m[20221213 23:20:53 @agent_ppo2.py:185][0m |          -0.0131 |          39.1947 |          14.9572 |
[32m[20221213 23:20:53 @agent_ppo2.py:185][0m |          -0.0123 |          39.1311 |          14.9525 |
[32m[20221213 23:20:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.49
[32m[20221213 23:20:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.91
[32m[20221213 23:20:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.59
[32m[20221213 23:20:53 @agent_ppo2.py:143][0m Total time:       8.35 min
[32m[20221213 23:20:53 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221213 23:20:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2393 --------------------------#
[32m[20221213 23:20:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:53 @agent_ppo2.py:185][0m |           0.0031 |          49.6092 |          14.9786 |
[32m[20221213 23:20:53 @agent_ppo2.py:185][0m |          -0.0052 |          44.0017 |          14.9914 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0106 |          41.3736 |          14.9729 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0091 |          39.8051 |          14.9682 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0087 |          38.8050 |          14.9605 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0145 |          38.4709 |          14.9472 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0076 |          37.8441 |          14.9389 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0141 |          37.0141 |          14.9265 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0148 |          36.8002 |          14.9161 |
[32m[20221213 23:20:54 @agent_ppo2.py:185][0m |          -0.0157 |          36.5500 |          14.9167 |
[32m[20221213 23:20:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.94
[32m[20221213 23:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.41
[32m[20221213 23:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.25
[32m[20221213 23:20:54 @agent_ppo2.py:143][0m Total time:       8.37 min
[32m[20221213 23:20:54 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221213 23:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2394 --------------------------#
[32m[20221213 23:20:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0003 |          50.7278 |          14.9433 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0059 |          45.1795 |          14.9525 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0077 |          43.2447 |          14.9494 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |           0.0013 |          46.9860 |          14.9671 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0117 |          41.2178 |          14.9805 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0110 |          40.5021 |          14.9751 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0145 |          39.8496 |          14.9832 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0139 |          39.5566 |          14.9809 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0064 |          40.8042 |          14.9788 |
[32m[20221213 23:20:55 @agent_ppo2.py:185][0m |          -0.0051 |          41.2159 |          14.9796 |
[32m[20221213 23:20:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.75
[32m[20221213 23:20:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.83
[32m[20221213 23:20:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:20:56 @agent_ppo2.py:143][0m Total time:       8.39 min
[32m[20221213 23:20:56 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221213 23:20:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2395 --------------------------#
[32m[20221213 23:20:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0008 |          60.6243 |          14.8610 |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0049 |          54.0238 |          14.8816 |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0062 |          52.5019 |          14.8900 |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0086 |          51.9252 |          14.8838 |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0103 |          51.2031 |          14.8757 |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0154 |          51.3063 |          14.8754 |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0036 |          50.7147 |          14.8833 |
[32m[20221213 23:20:56 @agent_ppo2.py:185][0m |          -0.0095 |          51.5353 |          14.9068 |
[32m[20221213 23:20:57 @agent_ppo2.py:185][0m |          -0.0141 |          50.2486 |          14.9050 |
[32m[20221213 23:20:57 @agent_ppo2.py:185][0m |          -0.0149 |          49.8424 |          14.9064 |
[32m[20221213 23:20:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:20:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.99
[32m[20221213 23:20:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.26
[32m[20221213 23:20:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.37
[32m[20221213 23:20:57 @agent_ppo2.py:143][0m Total time:       8.41 min
[32m[20221213 23:20:57 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221213 23:20:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2396 --------------------------#
[32m[20221213 23:20:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:20:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:57 @agent_ppo2.py:185][0m |           0.0058 |          40.5005 |          14.9753 |
[32m[20221213 23:20:57 @agent_ppo2.py:185][0m |           0.0053 |          36.3236 |          14.9148 |
[32m[20221213 23:20:57 @agent_ppo2.py:185][0m |          -0.0054 |          34.0389 |          14.9216 |
[32m[20221213 23:20:57 @agent_ppo2.py:185][0m |          -0.0064 |          33.0769 |          14.9134 |
[32m[20221213 23:20:57 @agent_ppo2.py:185][0m |          -0.0106 |          32.3850 |          14.9115 |
[32m[20221213 23:20:58 @agent_ppo2.py:185][0m |          -0.0086 |          31.7283 |          14.9289 |
[32m[20221213 23:20:58 @agent_ppo2.py:185][0m |          -0.0059 |          31.3615 |          14.9204 |
[32m[20221213 23:20:58 @agent_ppo2.py:185][0m |          -0.0106 |          30.9610 |          14.9122 |
[32m[20221213 23:20:58 @agent_ppo2.py:185][0m |          -0.0038 |          31.0962 |          14.9098 |
[32m[20221213 23:20:58 @agent_ppo2.py:185][0m |          -0.0113 |          30.1898 |          14.8885 |
[32m[20221213 23:20:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:20:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.05
[32m[20221213 23:20:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.23
[32m[20221213 23:20:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.26
[32m[20221213 23:20:58 @agent_ppo2.py:143][0m Total time:       8.43 min
[32m[20221213 23:20:58 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221213 23:20:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2397 --------------------------#
[32m[20221213 23:20:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:20:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:20:58 @agent_ppo2.py:185][0m |           0.0011 |          53.8523 |          15.0724 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0063 |          48.9586 |          15.0514 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0100 |          47.2659 |          15.0714 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0059 |          46.1449 |          15.0557 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0081 |          44.8523 |          15.0672 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0147 |          44.5162 |          15.0751 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0149 |          43.3414 |          15.0778 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0144 |          43.1244 |          15.0821 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0163 |          42.1863 |          15.0847 |
[32m[20221213 23:20:59 @agent_ppo2.py:185][0m |          -0.0097 |          42.3515 |          15.0964 |
[32m[20221213 23:20:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:20:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.27
[32m[20221213 23:20:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.44
[32m[20221213 23:20:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.66
[32m[20221213 23:20:59 @agent_ppo2.py:143][0m Total time:       8.46 min
[32m[20221213 23:20:59 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221213 23:20:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2398 --------------------------#
[32m[20221213 23:21:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |           0.0031 |          42.3769 |          14.9869 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0002 |          32.6553 |          14.9765 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0093 |          28.9423 |          14.9787 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0088 |          26.9640 |          14.9818 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0128 |          25.6569 |          14.9441 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0108 |          25.1026 |          14.9612 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0043 |          24.5856 |          14.9566 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0130 |          24.1992 |          14.9505 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0160 |          24.0273 |          14.9425 |
[32m[20221213 23:21:00 @agent_ppo2.py:185][0m |          -0.0188 |          23.6107 |          14.9388 |
[32m[20221213 23:21:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 308.78
[32m[20221213 23:21:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.18
[32m[20221213 23:21:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.85
[32m[20221213 23:21:01 @agent_ppo2.py:143][0m Total time:       8.48 min
[32m[20221213 23:21:01 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221213 23:21:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2399 --------------------------#
[32m[20221213 23:21:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |           0.0035 |          44.1843 |          14.9784 |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |          -0.0056 |          38.3349 |          14.9499 |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |          -0.0092 |          36.4811 |          14.9596 |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |          -0.0090 |          35.1936 |          14.9644 |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |          -0.0085 |          34.3804 |          14.9465 |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |          -0.0194 |          33.8119 |          14.9323 |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |          -0.0124 |          33.2242 |          14.9588 |
[32m[20221213 23:21:01 @agent_ppo2.py:185][0m |          -0.0136 |          32.7172 |          14.9381 |
[32m[20221213 23:21:02 @agent_ppo2.py:185][0m |          -0.0022 |          36.0618 |          14.9384 |
[32m[20221213 23:21:02 @agent_ppo2.py:185][0m |          -0.0142 |          32.4379 |          14.9555 |
[32m[20221213 23:21:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.03
[32m[20221213 23:21:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.50
[32m[20221213 23:21:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.37
[32m[20221213 23:21:02 @agent_ppo2.py:143][0m Total time:       8.50 min
[32m[20221213 23:21:02 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221213 23:21:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2400 --------------------------#
[32m[20221213 23:21:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:21:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:02 @agent_ppo2.py:185][0m |          -0.0001 |          48.4420 |          14.9163 |
[32m[20221213 23:21:02 @agent_ppo2.py:185][0m |          -0.0034 |          45.0696 |          14.9251 |
[32m[20221213 23:21:02 @agent_ppo2.py:185][0m |           0.0012 |          45.6530 |          14.9283 |
[32m[20221213 23:21:02 @agent_ppo2.py:185][0m |          -0.0127 |          43.3027 |          14.9329 |
[32m[20221213 23:21:03 @agent_ppo2.py:185][0m |          -0.0104 |          42.5989 |          14.9234 |
[32m[20221213 23:21:03 @agent_ppo2.py:185][0m |          -0.0128 |          42.2129 |          14.9332 |
[32m[20221213 23:21:03 @agent_ppo2.py:185][0m |          -0.0097 |          41.8183 |          14.9344 |
[32m[20221213 23:21:03 @agent_ppo2.py:185][0m |          -0.0156 |          41.4689 |          14.9494 |
[32m[20221213 23:21:03 @agent_ppo2.py:185][0m |          -0.0128 |          41.3360 |          14.9485 |
[32m[20221213 23:21:03 @agent_ppo2.py:185][0m |          -0.0113 |          41.0560 |          14.9531 |
[32m[20221213 23:21:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:21:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.36
[32m[20221213 23:21:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.50
[32m[20221213 23:21:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.56
[32m[20221213 23:21:03 @agent_ppo2.py:143][0m Total time:       8.52 min
[32m[20221213 23:21:03 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221213 23:21:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2401 --------------------------#
[32m[20221213 23:21:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:21:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:03 @agent_ppo2.py:185][0m |           0.0012 |          53.7372 |          14.9160 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0064 |          50.0604 |          14.8793 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0098 |          49.1845 |          14.8702 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0082 |          48.7392 |          14.8571 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0122 |          48.0719 |          14.8571 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0110 |          47.7304 |          14.8188 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0121 |          47.4264 |          14.8089 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0152 |          47.2617 |          14.8129 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0186 |          47.0332 |          14.8075 |
[32m[20221213 23:21:04 @agent_ppo2.py:185][0m |          -0.0164 |          46.7304 |          14.7926 |
[32m[20221213 23:21:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:21:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.12
[32m[20221213 23:21:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.09
[32m[20221213 23:21:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.77
[32m[20221213 23:21:04 @agent_ppo2.py:143][0m Total time:       8.54 min
[32m[20221213 23:21:04 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221213 23:21:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2402 --------------------------#
[32m[20221213 23:21:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0007 |          50.6330 |          14.9100 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0041 |          47.1669 |          14.9152 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0072 |          45.9304 |          14.9073 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0087 |          45.0627 |          14.9152 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0111 |          44.2691 |          14.9191 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0103 |          44.1146 |          14.9246 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0110 |          43.8893 |          14.8956 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0116 |          43.7047 |          14.9228 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0088 |          44.1491 |          14.9193 |
[32m[20221213 23:21:05 @agent_ppo2.py:185][0m |          -0.0132 |          43.4811 |          14.9208 |
[32m[20221213 23:21:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.13
[32m[20221213 23:21:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.77
[32m[20221213 23:21:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.65
[32m[20221213 23:21:06 @agent_ppo2.py:143][0m Total time:       8.56 min
[32m[20221213 23:21:06 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221213 23:21:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2403 --------------------------#
[32m[20221213 23:21:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:06 @agent_ppo2.py:185][0m |           0.0012 |          55.8310 |          14.8817 |
[32m[20221213 23:21:06 @agent_ppo2.py:185][0m |          -0.0031 |          53.3555 |          14.8667 |
[32m[20221213 23:21:06 @agent_ppo2.py:185][0m |          -0.0065 |          51.2595 |          14.8609 |
[32m[20221213 23:21:06 @agent_ppo2.py:185][0m |          -0.0130 |          50.3934 |          14.8343 |
[32m[20221213 23:21:06 @agent_ppo2.py:185][0m |          -0.0101 |          49.6003 |          14.8528 |
[32m[20221213 23:21:06 @agent_ppo2.py:185][0m |          -0.0119 |          49.1760 |          14.8513 |
[32m[20221213 23:21:06 @agent_ppo2.py:185][0m |          -0.0066 |          49.1301 |          14.8490 |
[32m[20221213 23:21:07 @agent_ppo2.py:185][0m |          -0.0169 |          48.6046 |          14.8369 |
[32m[20221213 23:21:07 @agent_ppo2.py:185][0m |          -0.0146 |          48.2196 |          14.8328 |
[32m[20221213 23:21:07 @agent_ppo2.py:185][0m |          -0.0128 |          49.0521 |          14.8335 |
[32m[20221213 23:21:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.09
[32m[20221213 23:21:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.47
[32m[20221213 23:21:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.77
[32m[20221213 23:21:07 @agent_ppo2.py:143][0m Total time:       8.58 min
[32m[20221213 23:21:07 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221213 23:21:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2404 --------------------------#
[32m[20221213 23:21:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:07 @agent_ppo2.py:185][0m |          -0.0002 |          55.4054 |          15.2006 |
[32m[20221213 23:21:07 @agent_ppo2.py:185][0m |          -0.0020 |          49.5057 |          15.2070 |
[32m[20221213 23:21:07 @agent_ppo2.py:185][0m |          -0.0064 |          47.7458 |          15.1945 |
[32m[20221213 23:21:07 @agent_ppo2.py:185][0m |          -0.0085 |          46.6088 |          15.2129 |
[32m[20221213 23:21:08 @agent_ppo2.py:185][0m |          -0.0048 |          46.1525 |          15.2335 |
[32m[20221213 23:21:08 @agent_ppo2.py:185][0m |          -0.0008 |          46.6968 |          15.2115 |
[32m[20221213 23:21:08 @agent_ppo2.py:185][0m |          -0.0121 |          44.8680 |          15.2283 |
[32m[20221213 23:21:08 @agent_ppo2.py:185][0m |          -0.0044 |          44.4153 |          15.2337 |
[32m[20221213 23:21:08 @agent_ppo2.py:185][0m |          -0.0098 |          44.0832 |          15.2178 |
[32m[20221213 23:21:08 @agent_ppo2.py:185][0m |          -0.0033 |          44.3607 |          15.2318 |
[32m[20221213 23:21:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:21:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.72
[32m[20221213 23:21:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.10
[32m[20221213 23:21:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.47
[32m[20221213 23:21:08 @agent_ppo2.py:143][0m Total time:       8.60 min
[32m[20221213 23:21:08 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221213 23:21:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2405 --------------------------#
[32m[20221213 23:21:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:08 @agent_ppo2.py:185][0m |          -0.0001 |          57.1835 |          15.0302 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0036 |          55.2087 |          15.0133 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0066 |          54.1428 |          15.0080 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0026 |          55.2917 |          15.0195 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |           0.0020 |          56.2687 |          15.0273 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0069 |          53.2416 |          15.0166 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0046 |          53.2585 |          15.0209 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0061 |          52.7632 |          15.0116 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0123 |          52.0692 |          15.0186 |
[32m[20221213 23:21:09 @agent_ppo2.py:185][0m |          -0.0073 |          52.6505 |          15.0268 |
[32m[20221213 23:21:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:21:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.58
[32m[20221213 23:21:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.19
[32m[20221213 23:21:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.74
[32m[20221213 23:21:09 @agent_ppo2.py:143][0m Total time:       8.62 min
[32m[20221213 23:21:09 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221213 23:21:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2406 --------------------------#
[32m[20221213 23:21:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |          -0.0016 |          56.4635 |          15.1281 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |          -0.0004 |          51.1863 |          15.1383 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |          -0.0034 |          49.6470 |          15.1304 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |           0.0034 |          52.3036 |          15.1463 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |          -0.0048 |          48.3664 |          15.1445 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |           0.0065 |          56.4720 |          15.1394 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |          -0.0091 |          47.9135 |          15.1499 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |          -0.0109 |          47.4366 |          15.1478 |
[32m[20221213 23:21:10 @agent_ppo2.py:185][0m |          -0.0064 |          47.2438 |          15.1605 |
[32m[20221213 23:21:11 @agent_ppo2.py:185][0m |          -0.0093 |          47.0105 |          15.1713 |
[32m[20221213 23:21:11 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:21:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.11
[32m[20221213 23:21:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.27
[32m[20221213 23:21:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.53
[32m[20221213 23:21:11 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221213 23:21:11 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221213 23:21:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2407 --------------------------#
[32m[20221213 23:21:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:11 @agent_ppo2.py:185][0m |           0.0045 |          50.4416 |          15.1044 |
[32m[20221213 23:21:11 @agent_ppo2.py:185][0m |          -0.0080 |          45.5777 |          15.1217 |
[32m[20221213 23:21:11 @agent_ppo2.py:185][0m |          -0.0069 |          44.0550 |          15.1254 |
[32m[20221213 23:21:11 @agent_ppo2.py:185][0m |          -0.0099 |          43.9219 |          15.1437 |
[32m[20221213 23:21:11 @agent_ppo2.py:185][0m |          -0.0042 |          46.0665 |          15.1442 |
[32m[20221213 23:21:11 @agent_ppo2.py:185][0m |          -0.0129 |          42.7776 |          15.1324 |
[32m[20221213 23:21:12 @agent_ppo2.py:185][0m |          -0.0108 |          42.7748 |          15.1748 |
[32m[20221213 23:21:12 @agent_ppo2.py:185][0m |          -0.0063 |          42.3921 |          15.1599 |
[32m[20221213 23:21:12 @agent_ppo2.py:185][0m |          -0.0132 |          42.1559 |          15.1782 |
[32m[20221213 23:21:12 @agent_ppo2.py:185][0m |          -0.0118 |          42.3716 |          15.1905 |
[32m[20221213 23:21:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:21:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.37
[32m[20221213 23:21:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.05
[32m[20221213 23:21:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.13
[32m[20221213 23:21:12 @agent_ppo2.py:143][0m Total time:       8.67 min
[32m[20221213 23:21:12 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221213 23:21:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2408 --------------------------#
[32m[20221213 23:21:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:12 @agent_ppo2.py:185][0m |           0.0036 |          40.5866 |          15.0894 |
[32m[20221213 23:21:12 @agent_ppo2.py:185][0m |          -0.0002 |          35.6706 |          15.1023 |
[32m[20221213 23:21:12 @agent_ppo2.py:185][0m |          -0.0057 |          33.5195 |          15.0934 |
[32m[20221213 23:21:13 @agent_ppo2.py:185][0m |          -0.0077 |          32.5060 |          15.0781 |
[32m[20221213 23:21:13 @agent_ppo2.py:185][0m |          -0.0096 |          31.5241 |          15.0707 |
[32m[20221213 23:21:13 @agent_ppo2.py:185][0m |          -0.0044 |          31.2990 |          15.0615 |
[32m[20221213 23:21:13 @agent_ppo2.py:185][0m |          -0.0124 |          30.6628 |          15.0533 |
[32m[20221213 23:21:13 @agent_ppo2.py:185][0m |          -0.0080 |          30.1423 |          15.0594 |
[32m[20221213 23:21:13 @agent_ppo2.py:185][0m |          -0.0144 |          30.5340 |          15.0263 |
[32m[20221213 23:21:13 @agent_ppo2.py:185][0m |          -0.0172 |          29.5746 |          15.0482 |
[32m[20221213 23:21:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.56
[32m[20221213 23:21:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.25
[32m[20221213 23:21:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.83
[32m[20221213 23:21:13 @agent_ppo2.py:143][0m Total time:       8.69 min
[32m[20221213 23:21:13 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221213 23:21:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2409 --------------------------#
[32m[20221213 23:21:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |           0.0012 |          51.6894 |          15.4444 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |          -0.0033 |          49.0007 |          15.4376 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |           0.0032 |          48.6103 |          15.4571 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |          -0.0063 |          47.4699 |          15.4654 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |          -0.0054 |          47.0115 |          15.4657 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |           0.0047 |          48.1629 |          15.4767 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |          -0.0070 |          46.8328 |          15.4751 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |          -0.0084 |          45.9052 |          15.5013 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |          -0.0092 |          45.8195 |          15.4986 |
[32m[20221213 23:21:14 @agent_ppo2.py:185][0m |          -0.0041 |          46.0830 |          15.4893 |
[32m[20221213 23:21:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.23
[32m[20221213 23:21:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.71
[32m[20221213 23:21:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.63
[32m[20221213 23:21:14 @agent_ppo2.py:143][0m Total time:       8.71 min
[32m[20221213 23:21:14 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221213 23:21:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2410 --------------------------#
[32m[20221213 23:21:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:21:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0008 |          41.4402 |          15.0677 |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0044 |          34.2657 |          15.0641 |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0068 |          32.3547 |          15.0187 |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0136 |          31.3866 |          15.0492 |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0107 |          30.6430 |          15.0421 |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0178 |          30.0542 |          15.0233 |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0117 |          29.6776 |          15.0040 |
[32m[20221213 23:21:15 @agent_ppo2.py:185][0m |          -0.0158 |          29.3387 |          15.0151 |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |          -0.0161 |          28.9312 |          15.0067 |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |          -0.0174 |          28.5798 |          15.0070 |
[32m[20221213 23:21:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:21:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.63
[32m[20221213 23:21:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.65
[32m[20221213 23:21:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.56
[32m[20221213 23:21:16 @agent_ppo2.py:143][0m Total time:       8.73 min
[32m[20221213 23:21:16 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221213 23:21:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2411 --------------------------#
[32m[20221213 23:21:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |           0.0009 |          48.5206 |          14.8989 |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |          -0.0042 |          43.8322 |          14.9143 |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |          -0.0096 |          41.8837 |          14.9077 |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |          -0.0057 |          40.7461 |          14.8954 |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |          -0.0114 |          39.9791 |          14.8765 |
[32m[20221213 23:21:16 @agent_ppo2.py:185][0m |          -0.0130 |          39.5206 |          14.8753 |
[32m[20221213 23:21:17 @agent_ppo2.py:185][0m |          -0.0142 |          39.3685 |          14.8745 |
[32m[20221213 23:21:17 @agent_ppo2.py:185][0m |          -0.0149 |          38.6224 |          14.8733 |
[32m[20221213 23:21:17 @agent_ppo2.py:185][0m |          -0.0103 |          38.3660 |          14.8706 |
[32m[20221213 23:21:17 @agent_ppo2.py:185][0m |          -0.0109 |          38.1322 |          14.8552 |
[32m[20221213 23:21:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.24
[32m[20221213 23:21:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.43
[32m[20221213 23:21:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.39
[32m[20221213 23:21:17 @agent_ppo2.py:143][0m Total time:       8.75 min
[32m[20221213 23:21:17 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221213 23:21:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2412 --------------------------#
[32m[20221213 23:21:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:17 @agent_ppo2.py:185][0m |          -0.0018 |          52.4155 |          15.0471 |
[32m[20221213 23:21:17 @agent_ppo2.py:185][0m |          -0.0029 |          47.6878 |          15.0697 |
[32m[20221213 23:21:17 @agent_ppo2.py:185][0m |          -0.0056 |          46.6754 |          15.0684 |
[32m[20221213 23:21:18 @agent_ppo2.py:185][0m |          -0.0079 |          46.3955 |          15.0730 |
[32m[20221213 23:21:18 @agent_ppo2.py:185][0m |          -0.0151 |          45.9770 |          15.0776 |
[32m[20221213 23:21:18 @agent_ppo2.py:185][0m |          -0.0089 |          46.2470 |          15.0891 |
[32m[20221213 23:21:18 @agent_ppo2.py:185][0m |          -0.0123 |          45.5773 |          15.0959 |
[32m[20221213 23:21:18 @agent_ppo2.py:185][0m |          -0.0146 |          45.2316 |          15.1082 |
[32m[20221213 23:21:18 @agent_ppo2.py:185][0m |          -0.0163 |          45.0298 |          15.1052 |
[32m[20221213 23:21:18 @agent_ppo2.py:185][0m |          -0.0134 |          44.9282 |          15.1145 |
[32m[20221213 23:21:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.07
[32m[20221213 23:21:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.31
[32m[20221213 23:21:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.46
[32m[20221213 23:21:18 @agent_ppo2.py:143][0m Total time:       8.77 min
[32m[20221213 23:21:18 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221213 23:21:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2413 --------------------------#
[32m[20221213 23:21:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |           0.0136 |          52.7748 |          15.2503 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0056 |          42.4352 |          15.2289 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0047 |          41.6694 |          15.2230 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0068 |          41.3249 |          15.2094 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |           0.0085 |          48.5283 |          15.2199 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0036 |          40.8950 |          15.1974 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0117 |          40.6698 |          15.2119 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0117 |          40.5800 |          15.1898 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0113 |          40.4523 |          15.1881 |
[32m[20221213 23:21:19 @agent_ppo2.py:185][0m |          -0.0108 |          40.2422 |          15.1928 |
[32m[20221213 23:21:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.15
[32m[20221213 23:21:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.99
[32m[20221213 23:21:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.77
[32m[20221213 23:21:19 @agent_ppo2.py:143][0m Total time:       8.79 min
[32m[20221213 23:21:19 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221213 23:21:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2414 --------------------------#
[32m[20221213 23:21:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |           0.0036 |          58.9565 |          15.0213 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0079 |          56.9889 |          15.0312 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0041 |          57.0306 |          15.0515 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0058 |          55.9423 |          15.0447 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0079 |          55.9334 |          15.0480 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0051 |          56.3876 |          15.0488 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0091 |          55.6204 |          15.0223 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0090 |          55.5097 |          15.0417 |
[32m[20221213 23:21:20 @agent_ppo2.py:185][0m |          -0.0093 |          55.4461 |          15.0373 |
[32m[20221213 23:21:21 @agent_ppo2.py:185][0m |          -0.0096 |          55.8090 |          15.0306 |
[32m[20221213 23:21:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.02
[32m[20221213 23:21:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.03
[32m[20221213 23:21:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.97
[32m[20221213 23:21:21 @agent_ppo2.py:143][0m Total time:       8.81 min
[32m[20221213 23:21:21 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221213 23:21:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2415 --------------------------#
[32m[20221213 23:21:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:21 @agent_ppo2.py:185][0m |          -0.0015 |          39.2804 |          14.9776 |
[32m[20221213 23:21:21 @agent_ppo2.py:185][0m |          -0.0060 |          36.3441 |          14.9269 |
[32m[20221213 23:21:21 @agent_ppo2.py:185][0m |          -0.0124 |          35.3134 |          14.9474 |
[32m[20221213 23:21:21 @agent_ppo2.py:185][0m |          -0.0012 |          35.8353 |          14.9169 |
[32m[20221213 23:21:21 @agent_ppo2.py:185][0m |          -0.0093 |          33.8137 |          14.9148 |
[32m[20221213 23:21:21 @agent_ppo2.py:185][0m |          -0.0111 |          33.6694 |          14.8834 |
[32m[20221213 23:21:22 @agent_ppo2.py:185][0m |          -0.0128 |          32.9025 |          14.9085 |
[32m[20221213 23:21:22 @agent_ppo2.py:185][0m |          -0.0135 |          32.5105 |          14.9089 |
[32m[20221213 23:21:22 @agent_ppo2.py:185][0m |          -0.0120 |          32.3853 |          14.8772 |
[32m[20221213 23:21:22 @agent_ppo2.py:185][0m |          -0.0126 |          31.9938 |          14.8825 |
[32m[20221213 23:21:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:21:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.77
[32m[20221213 23:21:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.46
[32m[20221213 23:21:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.68
[32m[20221213 23:21:22 @agent_ppo2.py:143][0m Total time:       8.83 min
[32m[20221213 23:21:22 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221213 23:21:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2416 --------------------------#
[32m[20221213 23:21:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:22 @agent_ppo2.py:185][0m |           0.0001 |          46.0758 |          14.8785 |
[32m[20221213 23:21:22 @agent_ppo2.py:185][0m |          -0.0094 |          44.4367 |          14.8859 |
[32m[20221213 23:21:22 @agent_ppo2.py:185][0m |          -0.0121 |          43.7178 |          14.8688 |
[32m[20221213 23:21:23 @agent_ppo2.py:185][0m |           0.0020 |          48.3339 |          14.8598 |
[32m[20221213 23:21:23 @agent_ppo2.py:185][0m |          -0.0092 |          42.7024 |          14.8535 |
[32m[20221213 23:21:23 @agent_ppo2.py:185][0m |          -0.0134 |          42.3213 |          14.8622 |
[32m[20221213 23:21:23 @agent_ppo2.py:185][0m |          -0.0139 |          42.0874 |          14.8636 |
[32m[20221213 23:21:23 @agent_ppo2.py:185][0m |          -0.0157 |          42.0702 |          14.8764 |
[32m[20221213 23:21:23 @agent_ppo2.py:185][0m |          -0.0149 |          41.7444 |          14.8681 |
[32m[20221213 23:21:23 @agent_ppo2.py:185][0m |          -0.0144 |          41.6244 |          14.8713 |
[32m[20221213 23:21:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.47
[32m[20221213 23:21:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.95
[32m[20221213 23:21:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.31
[32m[20221213 23:21:23 @agent_ppo2.py:143][0m Total time:       8.85 min
[32m[20221213 23:21:23 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221213 23:21:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2417 --------------------------#
[32m[20221213 23:21:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:21:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |           0.0003 |          54.0801 |          15.0343 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0017 |          51.6774 |          15.0178 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0091 |          50.4318 |          15.0132 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0069 |          50.1215 |          15.0065 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0132 |          49.5765 |          15.0155 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0065 |          50.5274 |          15.0033 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0108 |          48.9828 |          15.0198 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0160 |          48.6640 |          15.0142 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0127 |          48.6930 |          15.0225 |
[32m[20221213 23:21:24 @agent_ppo2.py:185][0m |          -0.0101 |          48.9466 |          15.0045 |
[32m[20221213 23:21:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.64
[32m[20221213 23:21:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.20
[32m[20221213 23:21:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.27
[32m[20221213 23:21:24 @agent_ppo2.py:143][0m Total time:       8.87 min
[32m[20221213 23:21:24 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221213 23:21:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2418 --------------------------#
[32m[20221213 23:21:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |           0.0017 |          46.1239 |          14.9884 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0046 |          42.5574 |          14.9732 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0053 |          41.0415 |          14.9764 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0104 |          39.7290 |          14.9773 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0078 |          39.0116 |          14.9684 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0082 |          38.4422 |          14.9431 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0075 |          37.7871 |          14.9413 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0134 |          37.4128 |          14.9604 |
[32m[20221213 23:21:25 @agent_ppo2.py:185][0m |          -0.0065 |          38.4711 |          14.9367 |
[32m[20221213 23:21:26 @agent_ppo2.py:185][0m |           0.0008 |          42.2884 |          14.9580 |
[32m[20221213 23:21:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.72
[32m[20221213 23:21:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.27
[32m[20221213 23:21:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.04
[32m[20221213 23:21:26 @agent_ppo2.py:143][0m Total time:       8.90 min
[32m[20221213 23:21:26 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221213 23:21:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2419 --------------------------#
[32m[20221213 23:21:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:26 @agent_ppo2.py:185][0m |           0.0024 |          45.5825 |          15.0034 |
[32m[20221213 23:21:26 @agent_ppo2.py:185][0m |          -0.0083 |          42.7441 |          14.9893 |
[32m[20221213 23:21:26 @agent_ppo2.py:185][0m |           0.0039 |          48.1617 |          14.9741 |
[32m[20221213 23:21:26 @agent_ppo2.py:185][0m |          -0.0071 |          41.6990 |          14.9708 |
[32m[20221213 23:21:26 @agent_ppo2.py:185][0m |          -0.0073 |          42.2661 |          14.9846 |
[32m[20221213 23:21:26 @agent_ppo2.py:185][0m |          -0.0019 |          43.0769 |          14.9687 |
[32m[20221213 23:21:27 @agent_ppo2.py:185][0m |          -0.0124 |          40.8451 |          14.9647 |
[32m[20221213 23:21:27 @agent_ppo2.py:185][0m |          -0.0097 |          40.5800 |          14.9358 |
[32m[20221213 23:21:27 @agent_ppo2.py:185][0m |          -0.0129 |          40.4005 |          14.9433 |
[32m[20221213 23:21:27 @agent_ppo2.py:185][0m |          -0.0122 |          40.0911 |          14.9691 |
[32m[20221213 23:21:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:21:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.86
[32m[20221213 23:21:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.21
[32m[20221213 23:21:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.72
[32m[20221213 23:21:27 @agent_ppo2.py:143][0m Total time:       8.92 min
[32m[20221213 23:21:27 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221213 23:21:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2420 --------------------------#
[32m[20221213 23:21:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:21:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:27 @agent_ppo2.py:185][0m |           0.0045 |          42.3282 |          15.0098 |
[32m[20221213 23:21:27 @agent_ppo2.py:185][0m |          -0.0018 |          36.2351 |          15.0024 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0008 |          35.4992 |          15.0245 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0117 |          35.1182 |          14.9941 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0079 |          34.7401 |          15.0035 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0076 |          34.4869 |          15.0441 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0092 |          34.3916 |          15.0349 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0152 |          34.0756 |          15.0305 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0089 |          34.3711 |          15.0529 |
[32m[20221213 23:21:28 @agent_ppo2.py:185][0m |          -0.0163 |          33.8210 |          15.0527 |
[32m[20221213 23:21:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:21:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.32
[32m[20221213 23:21:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.34
[32m[20221213 23:21:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.26
[32m[20221213 23:21:28 @agent_ppo2.py:143][0m Total time:       8.94 min
[32m[20221213 23:21:28 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221213 23:21:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2421 --------------------------#
[32m[20221213 23:21:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0010 |          47.8928 |          15.1421 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0030 |          44.6960 |          15.0732 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0031 |          43.7337 |          15.0701 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0072 |          42.8858 |          15.0632 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0045 |          42.5338 |          15.0342 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0078 |          42.2346 |          15.0461 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |           0.0013 |          47.7250 |          15.0103 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0110 |          41.7790 |          15.0029 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0100 |          41.8006 |          15.0037 |
[32m[20221213 23:21:29 @agent_ppo2.py:185][0m |          -0.0065 |          41.9369 |          15.0204 |
[32m[20221213 23:21:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.49
[32m[20221213 23:21:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.30
[32m[20221213 23:21:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.38
[32m[20221213 23:21:30 @agent_ppo2.py:143][0m Total time:       8.96 min
[32m[20221213 23:21:30 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221213 23:21:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2422 --------------------------#
[32m[20221213 23:21:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |           0.0030 |          53.7582 |          14.8000 |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |           0.0056 |          52.8429 |          14.7996 |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |          -0.0080 |          50.3413 |          14.8224 |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |           0.0015 |          50.3452 |          14.8092 |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |          -0.0066 |          48.9158 |          14.8126 |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |          -0.0130 |          48.4575 |          14.8254 |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |           0.0102 |          57.0782 |          14.8092 |
[32m[20221213 23:21:30 @agent_ppo2.py:185][0m |           0.0029 |          53.7637 |          14.8308 |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0126 |          47.8135 |          14.8128 |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0088 |          48.0123 |          14.8449 |
[32m[20221213 23:21:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.66
[32m[20221213 23:21:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.57
[32m[20221213 23:21:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.91
[32m[20221213 23:21:31 @agent_ppo2.py:143][0m Total time:       8.98 min
[32m[20221213 23:21:31 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221213 23:21:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2423 --------------------------#
[32m[20221213 23:21:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0025 |          54.9330 |          14.9763 |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0086 |          52.6698 |          14.9558 |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0078 |          51.8850 |          14.9349 |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0071 |          51.5380 |          14.9361 |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0115 |          51.0692 |          14.9340 |
[32m[20221213 23:21:31 @agent_ppo2.py:185][0m |          -0.0097 |          50.7196 |          14.9194 |
[32m[20221213 23:21:32 @agent_ppo2.py:185][0m |          -0.0120 |          50.3688 |          14.9270 |
[32m[20221213 23:21:32 @agent_ppo2.py:185][0m |          -0.0133 |          50.1337 |          14.8979 |
[32m[20221213 23:21:32 @agent_ppo2.py:185][0m |          -0.0073 |          50.1016 |          14.8998 |
[32m[20221213 23:21:32 @agent_ppo2.py:185][0m |          -0.0077 |          50.6900 |          14.9115 |
[32m[20221213 23:21:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.84
[32m[20221213 23:21:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.83
[32m[20221213 23:21:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.28
[32m[20221213 23:21:32 @agent_ppo2.py:143][0m Total time:       9.00 min
[32m[20221213 23:21:32 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221213 23:21:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2424 --------------------------#
[32m[20221213 23:21:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:32 @agent_ppo2.py:185][0m |          -0.0018 |          40.0889 |          14.8233 |
[32m[20221213 23:21:32 @agent_ppo2.py:185][0m |          -0.0042 |          36.2428 |          14.8011 |
[32m[20221213 23:21:32 @agent_ppo2.py:185][0m |           0.0007 |          34.7130 |          14.8471 |
[32m[20221213 23:21:33 @agent_ppo2.py:185][0m |          -0.0099 |          33.6609 |          14.8183 |
[32m[20221213 23:21:33 @agent_ppo2.py:185][0m |          -0.0069 |          32.6939 |          14.8313 |
[32m[20221213 23:21:33 @agent_ppo2.py:185][0m |          -0.0012 |          32.7822 |          14.8425 |
[32m[20221213 23:21:33 @agent_ppo2.py:185][0m |          -0.0089 |          31.9254 |          14.8118 |
[32m[20221213 23:21:33 @agent_ppo2.py:185][0m |          -0.0064 |          32.2996 |          14.8388 |
[32m[20221213 23:21:33 @agent_ppo2.py:185][0m |          -0.0064 |          31.8207 |          14.8543 |
[32m[20221213 23:21:33 @agent_ppo2.py:185][0m |           0.0007 |          37.5657 |          14.8373 |
[32m[20221213 23:21:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.97
[32m[20221213 23:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.00
[32m[20221213 23:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.25
[32m[20221213 23:21:33 @agent_ppo2.py:143][0m Total time:       9.02 min
[32m[20221213 23:21:33 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221213 23:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2425 --------------------------#
[32m[20221213 23:21:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:21:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |           0.0025 |          32.3193 |          14.9406 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0011 |          22.6295 |          14.9131 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0022 |          21.6145 |          14.9242 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0100 |          20.9592 |          14.9061 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0009 |          20.4457 |          14.8823 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0130 |          20.1955 |          14.8804 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0066 |          19.9361 |          14.8571 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0080 |          19.7800 |          14.8602 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0155 |          19.5899 |          14.8567 |
[32m[20221213 23:21:34 @agent_ppo2.py:185][0m |          -0.0075 |          19.4410 |          14.8441 |
[32m[20221213 23:21:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.92
[32m[20221213 23:21:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.88
[32m[20221213 23:21:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.77
[32m[20221213 23:21:35 @agent_ppo2.py:143][0m Total time:       9.04 min
[32m[20221213 23:21:35 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221213 23:21:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2426 --------------------------#
[32m[20221213 23:21:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:21:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |           0.0020 |          30.8589 |          15.0224 |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |          -0.0031 |          27.5331 |          15.0278 |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |           0.0083 |          30.1375 |          15.0187 |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |          -0.0119 |          25.8056 |          15.0263 |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |          -0.0108 |          25.3575 |          15.0231 |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |          -0.0107 |          24.9322 |          14.9989 |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |          -0.0128 |          25.0408 |          14.9916 |
[32m[20221213 23:21:35 @agent_ppo2.py:185][0m |          -0.0133 |          24.3418 |          15.0005 |
[32m[20221213 23:21:36 @agent_ppo2.py:185][0m |          -0.0150 |          24.1176 |          15.0044 |
[32m[20221213 23:21:36 @agent_ppo2.py:185][0m |          -0.0124 |          23.9851 |          14.9741 |
[32m[20221213 23:21:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.13
[32m[20221213 23:21:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.54
[32m[20221213 23:21:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.18
[32m[20221213 23:21:36 @agent_ppo2.py:143][0m Total time:       9.06 min
[32m[20221213 23:21:36 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221213 23:21:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2427 --------------------------#
[32m[20221213 23:21:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:36 @agent_ppo2.py:185][0m |          -0.0017 |          31.9606 |          14.9347 |
[32m[20221213 23:21:36 @agent_ppo2.py:185][0m |           0.0010 |          28.6982 |          14.9376 |
[32m[20221213 23:21:36 @agent_ppo2.py:185][0m |          -0.0066 |          27.9346 |          14.9424 |
[32m[20221213 23:21:36 @agent_ppo2.py:185][0m |          -0.0120 |          27.2417 |          14.9583 |
[32m[20221213 23:21:36 @agent_ppo2.py:185][0m |          -0.0130 |          27.0004 |          14.9652 |
[32m[20221213 23:21:37 @agent_ppo2.py:185][0m |          -0.0056 |          27.4073 |          14.9676 |
[32m[20221213 23:21:37 @agent_ppo2.py:185][0m |          -0.0101 |          26.7049 |          14.9817 |
[32m[20221213 23:21:37 @agent_ppo2.py:185][0m |          -0.0168 |          26.5397 |          14.9905 |
[32m[20221213 23:21:37 @agent_ppo2.py:185][0m |          -0.0143 |          26.3405 |          14.9971 |
[32m[20221213 23:21:37 @agent_ppo2.py:185][0m |          -0.0211 |          26.1858 |          15.0082 |
[32m[20221213 23:21:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.87
[32m[20221213 23:21:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.56
[32m[20221213 23:21:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.32
[32m[20221213 23:21:37 @agent_ppo2.py:143][0m Total time:       9.08 min
[32m[20221213 23:21:37 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221213 23:21:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2428 --------------------------#
[32m[20221213 23:21:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:37 @agent_ppo2.py:185][0m |           0.0010 |          25.2763 |          15.0381 |
[32m[20221213 23:21:37 @agent_ppo2.py:185][0m |          -0.0070 |          22.0247 |          14.9979 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0095 |          21.2657 |          15.0154 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0081 |          20.8048 |          14.9837 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0134 |          20.5520 |          14.9780 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0124 |          20.4817 |          14.9759 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0117 |          20.0335 |          14.9504 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0099 |          19.8317 |          14.9605 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0092 |          19.6685 |          14.9432 |
[32m[20221213 23:21:38 @agent_ppo2.py:185][0m |          -0.0136 |          19.4401 |          14.9311 |
[32m[20221213 23:21:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.55
[32m[20221213 23:21:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.35
[32m[20221213 23:21:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.90
[32m[20221213 23:21:38 @agent_ppo2.py:143][0m Total time:       9.10 min
[32m[20221213 23:21:38 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221213 23:21:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2429 --------------------------#
[32m[20221213 23:21:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0026 |          54.1094 |          14.6912 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0067 |          51.4094 |          14.7272 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0074 |          50.0775 |          14.6922 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0091 |          49.5753 |          14.7090 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0119 |          48.9776 |          14.7019 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0102 |          48.6672 |          14.6913 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0106 |          48.1577 |          14.6814 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0125 |          47.8449 |          14.6648 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0107 |          48.1165 |          14.6708 |
[32m[20221213 23:21:39 @agent_ppo2.py:185][0m |          -0.0139 |          47.0461 |          14.6675 |
[32m[20221213 23:21:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:21:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.31
[32m[20221213 23:21:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.15
[32m[20221213 23:21:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.00
[32m[20221213 23:21:40 @agent_ppo2.py:143][0m Total time:       9.12 min
[32m[20221213 23:21:40 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221213 23:21:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2430 --------------------------#
[32m[20221213 23:21:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:21:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |          -0.0018 |          31.7762 |          14.9045 |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |           0.0060 |          28.2124 |          14.9234 |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |          -0.0033 |          27.1775 |          14.9027 |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |          -0.0035 |          26.5598 |          14.9139 |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |          -0.0036 |          25.5802 |          14.9437 |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |          -0.0132 |          25.0245 |          14.9550 |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |          -0.0130 |          24.8524 |          14.9688 |
[32m[20221213 23:21:40 @agent_ppo2.py:185][0m |          -0.0136 |          24.6423 |          14.9510 |
[32m[20221213 23:21:41 @agent_ppo2.py:185][0m |          -0.0156 |          24.3950 |          14.9804 |
[32m[20221213 23:21:41 @agent_ppo2.py:185][0m |          -0.0171 |          24.4716 |          14.9838 |
[32m[20221213 23:21:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.46
[32m[20221213 23:21:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.03
[32m[20221213 23:21:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.03
[32m[20221213 23:21:41 @agent_ppo2.py:143][0m Total time:       9.15 min
[32m[20221213 23:21:41 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221213 23:21:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2431 --------------------------#
[32m[20221213 23:21:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:21:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:41 @agent_ppo2.py:185][0m |          -0.0026 |          44.7789 |          14.8768 |
[32m[20221213 23:21:41 @agent_ppo2.py:185][0m |          -0.0070 |          40.8708 |          14.8700 |
[32m[20221213 23:21:41 @agent_ppo2.py:185][0m |          -0.0120 |          40.0704 |          14.8962 |
[32m[20221213 23:21:41 @agent_ppo2.py:185][0m |          -0.0132 |          39.3027 |          14.8890 |
[32m[20221213 23:21:41 @agent_ppo2.py:185][0m |          -0.0029 |          41.1540 |          14.8847 |
[32m[20221213 23:21:42 @agent_ppo2.py:185][0m |          -0.0141 |          38.8187 |          14.8751 |
[32m[20221213 23:21:42 @agent_ppo2.py:185][0m |          -0.0135 |          38.4856 |          14.8854 |
[32m[20221213 23:21:42 @agent_ppo2.py:185][0m |          -0.0206 |          38.3542 |          14.9062 |
[32m[20221213 23:21:42 @agent_ppo2.py:185][0m |          -0.0125 |          38.0016 |          14.8928 |
[32m[20221213 23:21:42 @agent_ppo2.py:185][0m |          -0.0156 |          38.1769 |          14.9175 |
[32m[20221213 23:21:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.77
[32m[20221213 23:21:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.81
[32m[20221213 23:21:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.57
[32m[20221213 23:21:42 @agent_ppo2.py:143][0m Total time:       9.17 min
[32m[20221213 23:21:42 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221213 23:21:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2432 --------------------------#
[32m[20221213 23:21:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:21:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:42 @agent_ppo2.py:185][0m |           0.0020 |          44.5782 |          14.9628 |
[32m[20221213 23:21:42 @agent_ppo2.py:185][0m |          -0.0029 |          39.6114 |          14.9433 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0087 |          38.1160 |          14.9582 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0063 |          37.6483 |          14.9581 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0147 |          36.6181 |          14.9608 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0143 |          36.0450 |          14.9500 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0102 |          36.3419 |          14.9713 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0134 |          35.2301 |          14.9478 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0054 |          35.4222 |          14.9513 |
[32m[20221213 23:21:43 @agent_ppo2.py:185][0m |          -0.0158 |          34.5772 |          14.9485 |
[32m[20221213 23:21:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.38
[32m[20221213 23:21:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.58
[32m[20221213 23:21:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.31
[32m[20221213 23:21:43 @agent_ppo2.py:143][0m Total time:       9.19 min
[32m[20221213 23:21:43 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221213 23:21:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2433 --------------------------#
[32m[20221213 23:21:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |           0.0037 |          31.4637 |          15.1711 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0023 |          26.4239 |          15.1801 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0103 |          25.0434 |          15.1562 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0142 |          24.1010 |          15.1647 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0143 |          23.5880 |          15.1532 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0176 |          23.1949 |          15.1640 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0073 |          22.8914 |          15.1625 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0111 |          22.4763 |          15.1548 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0225 |          22.1157 |          15.1565 |
[32m[20221213 23:21:44 @agent_ppo2.py:185][0m |          -0.0194 |          21.8683 |          15.1641 |
[32m[20221213 23:21:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.54
[32m[20221213 23:21:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.79
[32m[20221213 23:21:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.52
[32m[20221213 23:21:45 @agent_ppo2.py:143][0m Total time:       9.21 min
[32m[20221213 23:21:45 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221213 23:21:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2434 --------------------------#
[32m[20221213 23:21:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |           0.0055 |          53.7963 |          14.8925 |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |          -0.0042 |          49.8890 |          14.9064 |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |          -0.0046 |          48.4455 |          14.9173 |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |          -0.0067 |          47.4136 |          14.9187 |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |          -0.0099 |          47.1976 |          14.9212 |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |          -0.0104 |          46.4847 |          14.9242 |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |          -0.0095 |          45.5652 |          14.9146 |
[32m[20221213 23:21:45 @agent_ppo2.py:185][0m |          -0.0102 |          45.1142 |          14.9378 |
[32m[20221213 23:21:46 @agent_ppo2.py:185][0m |          -0.0055 |          48.9485 |          14.9364 |
[32m[20221213 23:21:46 @agent_ppo2.py:185][0m |          -0.0157 |          44.7230 |          14.9461 |
[32m[20221213 23:21:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:21:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.75
[32m[20221213 23:21:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.47
[32m[20221213 23:21:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.94
[32m[20221213 23:21:46 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221213 23:21:46 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221213 23:21:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2435 --------------------------#
[32m[20221213 23:21:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:46 @agent_ppo2.py:185][0m |           0.0026 |          45.1436 |          14.9220 |
[32m[20221213 23:21:46 @agent_ppo2.py:185][0m |          -0.0049 |          42.3623 |          14.9097 |
[32m[20221213 23:21:46 @agent_ppo2.py:185][0m |          -0.0066 |          41.3221 |          14.9230 |
[32m[20221213 23:21:46 @agent_ppo2.py:185][0m |          -0.0092 |          40.8123 |          14.9150 |
[32m[20221213 23:21:46 @agent_ppo2.py:185][0m |          -0.0109 |          40.6365 |          14.9150 |
[32m[20221213 23:21:47 @agent_ppo2.py:185][0m |          -0.0074 |          40.5752 |          14.9056 |
[32m[20221213 23:21:47 @agent_ppo2.py:185][0m |          -0.0072 |          39.8642 |          14.9121 |
[32m[20221213 23:21:47 @agent_ppo2.py:185][0m |          -0.0123 |          39.6580 |          14.9005 |
[32m[20221213 23:21:47 @agent_ppo2.py:185][0m |          -0.0135 |          39.3838 |          14.9087 |
[32m[20221213 23:21:47 @agent_ppo2.py:185][0m |          -0.0077 |          39.2463 |          14.9077 |
[32m[20221213 23:21:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.13
[32m[20221213 23:21:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.92
[32m[20221213 23:21:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.81
[32m[20221213 23:21:47 @agent_ppo2.py:143][0m Total time:       9.25 min
[32m[20221213 23:21:47 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221213 23:21:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2436 --------------------------#
[32m[20221213 23:21:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:47 @agent_ppo2.py:185][0m |           0.0008 |          40.2232 |          14.9324 |
[32m[20221213 23:21:47 @agent_ppo2.py:185][0m |          -0.0058 |          37.6713 |          14.9228 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0070 |          36.5529 |          14.8966 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0131 |          36.0550 |          14.8813 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0136 |          35.5130 |          14.8714 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0061 |          36.2615 |          14.8882 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0120 |          35.0524 |          14.8705 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0173 |          34.6265 |          14.8548 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0166 |          34.5136 |          14.8440 |
[32m[20221213 23:21:48 @agent_ppo2.py:185][0m |          -0.0154 |          34.3423 |          14.8417 |
[32m[20221213 23:21:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.86
[32m[20221213 23:21:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.60
[32m[20221213 23:21:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.93
[32m[20221213 23:21:48 @agent_ppo2.py:143][0m Total time:       9.27 min
[32m[20221213 23:21:48 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221213 23:21:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2437 --------------------------#
[32m[20221213 23:21:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |           0.0004 |          55.8960 |          14.9753 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0029 |          53.7051 |          14.9845 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0040 |          53.1317 |          15.0073 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0049 |          52.5886 |          14.9962 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0057 |          52.1848 |          15.0371 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0079 |          51.9700 |          15.0171 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0083 |          51.5919 |          15.0365 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0069 |          51.5059 |          15.0306 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0039 |          52.2307 |          15.0362 |
[32m[20221213 23:21:49 @agent_ppo2.py:185][0m |          -0.0059 |          51.3184 |          15.0631 |
[32m[20221213 23:21:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.40
[32m[20221213 23:21:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.19
[32m[20221213 23:21:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.56
[32m[20221213 23:21:50 @agent_ppo2.py:143][0m Total time:       9.29 min
[32m[20221213 23:21:50 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221213 23:21:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2438 --------------------------#
[32m[20221213 23:21:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |           0.0008 |          41.8477 |          15.0234 |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |          -0.0062 |          38.0892 |          15.0033 |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |          -0.0076 |          36.8600 |          15.0018 |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |          -0.0099 |          36.2783 |          14.9867 |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |          -0.0114 |          35.7266 |          14.9987 |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |          -0.0171 |          35.5595 |          15.0003 |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |          -0.0123 |          35.2384 |          14.9853 |
[32m[20221213 23:21:50 @agent_ppo2.py:185][0m |          -0.0126 |          34.9544 |          14.9899 |
[32m[20221213 23:21:51 @agent_ppo2.py:185][0m |          -0.0067 |          35.3506 |          14.9740 |
[32m[20221213 23:21:51 @agent_ppo2.py:185][0m |          -0.0168 |          34.4524 |          14.9774 |
[32m[20221213 23:21:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.86
[32m[20221213 23:21:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.99
[32m[20221213 23:21:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.27
[32m[20221213 23:21:51 @agent_ppo2.py:143][0m Total time:       9.31 min
[32m[20221213 23:21:51 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221213 23:21:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2439 --------------------------#
[32m[20221213 23:21:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:51 @agent_ppo2.py:185][0m |           0.0003 |          51.5906 |          14.8534 |
[32m[20221213 23:21:51 @agent_ppo2.py:185][0m |          -0.0045 |          49.4675 |          14.8561 |
[32m[20221213 23:21:51 @agent_ppo2.py:185][0m |          -0.0060 |          48.0542 |          14.8403 |
[32m[20221213 23:21:51 @agent_ppo2.py:185][0m |          -0.0110 |          47.6460 |          14.8735 |
[32m[20221213 23:21:51 @agent_ppo2.py:185][0m |          -0.0040 |          47.4190 |          14.8511 |
[32m[20221213 23:21:52 @agent_ppo2.py:185][0m |          -0.0079 |          46.5170 |          14.8263 |
[32m[20221213 23:21:52 @agent_ppo2.py:185][0m |          -0.0090 |          46.5688 |          14.8221 |
[32m[20221213 23:21:52 @agent_ppo2.py:185][0m |          -0.0116 |          45.9166 |          14.8358 |
[32m[20221213 23:21:52 @agent_ppo2.py:185][0m |          -0.0104 |          45.7016 |          14.8373 |
[32m[20221213 23:21:52 @agent_ppo2.py:185][0m |          -0.0111 |          45.4484 |          14.8270 |
[32m[20221213 23:21:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.10
[32m[20221213 23:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.46
[32m[20221213 23:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.05
[32m[20221213 23:21:52 @agent_ppo2.py:143][0m Total time:       9.33 min
[32m[20221213 23:21:52 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221213 23:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2440 --------------------------#
[32m[20221213 23:21:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:21:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:52 @agent_ppo2.py:185][0m |           0.0019 |          47.8853 |          15.0641 |
[32m[20221213 23:21:52 @agent_ppo2.py:185][0m |          -0.0079 |          41.9750 |          15.0712 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0087 |          40.2612 |          15.0691 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0061 |          39.2779 |          15.0568 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0125 |          38.3267 |          15.0619 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0118 |          37.7670 |          15.0552 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0228 |          37.3152 |          15.0531 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0111 |          36.9587 |          15.0603 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0088 |          40.6802 |          15.0668 |
[32m[20221213 23:21:53 @agent_ppo2.py:185][0m |          -0.0193 |          36.3779 |          15.0484 |
[32m[20221213 23:21:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.58
[32m[20221213 23:21:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.88
[32m[20221213 23:21:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.07
[32m[20221213 23:21:53 @agent_ppo2.py:143][0m Total time:       9.36 min
[32m[20221213 23:21:53 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221213 23:21:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2441 --------------------------#
[32m[20221213 23:21:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |           0.0145 |          59.4711 |          14.9503 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |          -0.0032 |          49.9116 |          14.9123 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |           0.0031 |          50.2985 |          14.9310 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |           0.0047 |          53.1062 |          14.9000 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |          -0.0068 |          48.0499 |          14.8725 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |          -0.0143 |          47.3223 |          14.8774 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |          -0.0105 |          47.1984 |          14.8444 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |          -0.0126 |          46.9759 |          14.8543 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |          -0.0101 |          46.8779 |          14.8449 |
[32m[20221213 23:21:54 @agent_ppo2.py:185][0m |          -0.0072 |          46.8770 |          14.8286 |
[32m[20221213 23:21:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:21:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.93
[32m[20221213 23:21:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.01
[32m[20221213 23:21:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.90
[32m[20221213 23:21:55 @agent_ppo2.py:143][0m Total time:       9.38 min
[32m[20221213 23:21:55 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221213 23:21:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2442 --------------------------#
[32m[20221213 23:21:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0016 |          37.8752 |          15.1894 |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0081 |          33.1077 |          15.1754 |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0039 |          31.9744 |          15.1745 |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0099 |          31.7644 |          15.1786 |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0071 |          31.0578 |          15.1730 |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0171 |          30.6189 |          15.1480 |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0147 |          30.3642 |          15.1456 |
[32m[20221213 23:21:55 @agent_ppo2.py:185][0m |          -0.0170 |          30.1299 |          15.1490 |
[32m[20221213 23:21:56 @agent_ppo2.py:185][0m |          -0.0192 |          29.9506 |          15.1367 |
[32m[20221213 23:21:56 @agent_ppo2.py:185][0m |          -0.0146 |          29.7353 |          15.1323 |
[32m[20221213 23:21:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:21:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.22
[32m[20221213 23:21:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.49
[32m[20221213 23:21:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.78
[32m[20221213 23:21:56 @agent_ppo2.py:143][0m Total time:       9.40 min
[32m[20221213 23:21:56 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221213 23:21:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2443 --------------------------#
[32m[20221213 23:21:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:56 @agent_ppo2.py:185][0m |           0.0007 |          50.3110 |          14.7049 |
[32m[20221213 23:21:56 @agent_ppo2.py:185][0m |          -0.0048 |          46.3103 |          14.6824 |
[32m[20221213 23:21:56 @agent_ppo2.py:185][0m |          -0.0062 |          45.0014 |          14.6369 |
[32m[20221213 23:21:56 @agent_ppo2.py:185][0m |          -0.0082 |          44.2122 |          14.6479 |
[32m[20221213 23:21:56 @agent_ppo2.py:185][0m |          -0.0005 |          46.3882 |          14.6461 |
[32m[20221213 23:21:57 @agent_ppo2.py:185][0m |          -0.0119 |          43.6012 |          14.5901 |
[32m[20221213 23:21:57 @agent_ppo2.py:185][0m |          -0.0095 |          42.8930 |          14.6036 |
[32m[20221213 23:21:57 @agent_ppo2.py:185][0m |          -0.0050 |          46.2515 |          14.6235 |
[32m[20221213 23:21:57 @agent_ppo2.py:185][0m |          -0.0110 |          42.5522 |          14.6275 |
[32m[20221213 23:21:57 @agent_ppo2.py:185][0m |          -0.0110 |          42.2798 |          14.6319 |
[32m[20221213 23:21:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:21:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.49
[32m[20221213 23:21:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.69
[32m[20221213 23:21:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.07
[32m[20221213 23:21:57 @agent_ppo2.py:143][0m Total time:       9.42 min
[32m[20221213 23:21:57 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221213 23:21:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2444 --------------------------#
[32m[20221213 23:21:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:57 @agent_ppo2.py:185][0m |           0.0033 |          41.4094 |          14.6291 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0016 |          37.7732 |          14.6115 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0065 |          36.2786 |          14.5968 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0128 |          35.6404 |          14.5822 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0125 |          35.0487 |          14.5645 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0115 |          34.6300 |          14.5602 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0141 |          34.1678 |          14.5529 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0058 |          34.9637 |          14.5475 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0152 |          33.5260 |          14.5392 |
[32m[20221213 23:21:58 @agent_ppo2.py:185][0m |          -0.0164 |          33.3839 |          14.5277 |
[32m[20221213 23:21:58 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:21:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.59
[32m[20221213 23:21:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.19
[32m[20221213 23:21:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.79
[32m[20221213 23:21:58 @agent_ppo2.py:143][0m Total time:       9.44 min
[32m[20221213 23:21:58 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221213 23:21:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2445 --------------------------#
[32m[20221213 23:21:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:21:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |           0.0050 |          59.6041 |          14.9116 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0074 |          56.3203 |          14.9206 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0116 |          55.3902 |          14.9038 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0060 |          54.7077 |          14.8890 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0128 |          54.4834 |          14.8773 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0116 |          54.1232 |          14.8747 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |           0.0063 |          58.8465 |          14.8667 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0100 |          53.4927 |          14.8697 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0136 |          53.3707 |          14.8611 |
[32m[20221213 23:21:59 @agent_ppo2.py:185][0m |          -0.0139 |          52.9127 |          14.8648 |
[32m[20221213 23:21:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:22:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.39
[32m[20221213 23:22:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.81
[32m[20221213 23:22:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.39
[32m[20221213 23:22:00 @agent_ppo2.py:143][0m Total time:       9.46 min
[32m[20221213 23:22:00 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221213 23:22:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2446 --------------------------#
[32m[20221213 23:22:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:00 @agent_ppo2.py:185][0m |           0.0043 |          54.3021 |          14.9297 |
[32m[20221213 23:22:00 @agent_ppo2.py:185][0m |          -0.0045 |          48.6934 |          14.9180 |
[32m[20221213 23:22:00 @agent_ppo2.py:185][0m |          -0.0119 |          47.0557 |          14.9198 |
[32m[20221213 23:22:00 @agent_ppo2.py:185][0m |          -0.0098 |          45.9992 |          14.9196 |
[32m[20221213 23:22:00 @agent_ppo2.py:185][0m |          -0.0019 |          49.0049 |          14.9179 |
[32m[20221213 23:22:00 @agent_ppo2.py:185][0m |          -0.0114 |          44.6133 |          14.9172 |
[32m[20221213 23:22:01 @agent_ppo2.py:185][0m |          -0.0119 |          43.9652 |          14.9342 |
[32m[20221213 23:22:01 @agent_ppo2.py:185][0m |          -0.0148 |          43.5553 |          14.9430 |
[32m[20221213 23:22:01 @agent_ppo2.py:185][0m |          -0.0124 |          43.3933 |          14.9553 |
[32m[20221213 23:22:01 @agent_ppo2.py:185][0m |          -0.0139 |          42.8275 |          14.9570 |
[32m[20221213 23:22:01 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:22:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.96
[32m[20221213 23:22:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.96
[32m[20221213 23:22:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.89
[32m[20221213 23:22:01 @agent_ppo2.py:143][0m Total time:       9.48 min
[32m[20221213 23:22:01 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221213 23:22:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2447 --------------------------#
[32m[20221213 23:22:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:01 @agent_ppo2.py:185][0m |           0.0165 |          61.6487 |          14.7480 |
[32m[20221213 23:22:01 @agent_ppo2.py:185][0m |          -0.0068 |          52.4388 |          14.7601 |
[32m[20221213 23:22:01 @agent_ppo2.py:185][0m |          -0.0045 |          52.0286 |          14.7597 |
[32m[20221213 23:22:02 @agent_ppo2.py:185][0m |          -0.0094 |          51.2422 |          14.7649 |
[32m[20221213 23:22:02 @agent_ppo2.py:185][0m |          -0.0036 |          51.9570 |          14.7836 |
[32m[20221213 23:22:02 @agent_ppo2.py:185][0m |          -0.0118 |          50.7097 |          14.7813 |
[32m[20221213 23:22:02 @agent_ppo2.py:185][0m |          -0.0109 |          50.2649 |          14.7805 |
[32m[20221213 23:22:02 @agent_ppo2.py:185][0m |          -0.0133 |          49.8488 |          14.7780 |
[32m[20221213 23:22:02 @agent_ppo2.py:185][0m |          -0.0133 |          49.6647 |          14.8021 |
[32m[20221213 23:22:02 @agent_ppo2.py:185][0m |          -0.0167 |          49.6710 |          14.7947 |
[32m[20221213 23:22:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:22:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.70
[32m[20221213 23:22:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.08
[32m[20221213 23:22:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.91
[32m[20221213 23:22:02 @agent_ppo2.py:143][0m Total time:       9.50 min
[32m[20221213 23:22:02 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221213 23:22:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2448 --------------------------#
[32m[20221213 23:22:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0002 |          46.6411 |          14.6459 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0085 |          43.7604 |          14.6322 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0066 |          42.9097 |          14.6441 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0115 |          42.3979 |          14.6103 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0047 |          43.1840 |          14.6227 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0134 |          41.8993 |          14.6365 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0015 |          46.7387 |          14.6313 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0120 |          41.3982 |          14.6161 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0094 |          42.4750 |          14.6197 |
[32m[20221213 23:22:03 @agent_ppo2.py:185][0m |          -0.0159 |          40.9669 |          14.6247 |
[32m[20221213 23:22:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:22:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.37
[32m[20221213 23:22:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.73
[32m[20221213 23:22:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.37
[32m[20221213 23:22:04 @agent_ppo2.py:143][0m Total time:       9.53 min
[32m[20221213 23:22:04 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221213 23:22:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2449 --------------------------#
[32m[20221213 23:22:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:04 @agent_ppo2.py:185][0m |           0.0010 |          36.7625 |          14.6776 |
[32m[20221213 23:22:04 @agent_ppo2.py:185][0m |          -0.0040 |          33.7451 |          14.6720 |
[32m[20221213 23:22:04 @agent_ppo2.py:185][0m |          -0.0007 |          32.9324 |          14.6671 |
[32m[20221213 23:22:04 @agent_ppo2.py:185][0m |          -0.0060 |          32.0127 |          14.6694 |
[32m[20221213 23:22:04 @agent_ppo2.py:185][0m |           0.0005 |          34.3801 |          14.6637 |
[32m[20221213 23:22:04 @agent_ppo2.py:185][0m |          -0.0107 |          31.2150 |          14.6858 |
[32m[20221213 23:22:04 @agent_ppo2.py:185][0m |          -0.0086 |          30.7313 |          14.6789 |
[32m[20221213 23:22:05 @agent_ppo2.py:185][0m |          -0.0042 |          30.6237 |          14.6863 |
[32m[20221213 23:22:05 @agent_ppo2.py:185][0m |          -0.0090 |          31.2863 |          14.6981 |
[32m[20221213 23:22:05 @agent_ppo2.py:185][0m |          -0.0105 |          30.3241 |          14.6929 |
[32m[20221213 23:22:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.28
[32m[20221213 23:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.89
[32m[20221213 23:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.51
[32m[20221213 23:22:05 @agent_ppo2.py:143][0m Total time:       9.55 min
[32m[20221213 23:22:05 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221213 23:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2450 --------------------------#
[32m[20221213 23:22:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:22:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:05 @agent_ppo2.py:185][0m |           0.0028 |          39.4046 |          14.6427 |
[32m[20221213 23:22:05 @agent_ppo2.py:185][0m |          -0.0030 |          35.6722 |          14.6442 |
[32m[20221213 23:22:05 @agent_ppo2.py:185][0m |          -0.0067 |          34.4891 |          14.6408 |
[32m[20221213 23:22:05 @agent_ppo2.py:185][0m |          -0.0085 |          33.7947 |          14.6612 |
[32m[20221213 23:22:06 @agent_ppo2.py:185][0m |          -0.0090 |          33.2855 |          14.6428 |
[32m[20221213 23:22:06 @agent_ppo2.py:185][0m |          -0.0075 |          33.2543 |          14.6474 |
[32m[20221213 23:22:06 @agent_ppo2.py:185][0m |          -0.0109 |          32.9653 |          14.6423 |
[32m[20221213 23:22:06 @agent_ppo2.py:185][0m |          -0.0159 |          32.4609 |          14.6631 |
[32m[20221213 23:22:06 @agent_ppo2.py:185][0m |          -0.0158 |          32.3797 |          14.6510 |
[32m[20221213 23:22:06 @agent_ppo2.py:185][0m |          -0.0194 |          32.2659 |          14.6604 |
[32m[20221213 23:22:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.43
[32m[20221213 23:22:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.46
[32m[20221213 23:22:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.63
[32m[20221213 23:22:06 @agent_ppo2.py:143][0m Total time:       9.57 min
[32m[20221213 23:22:06 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221213 23:22:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2451 --------------------------#
[32m[20221213 23:22:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:06 @agent_ppo2.py:185][0m |           0.0083 |          54.4115 |          14.8428 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0040 |          49.2480 |          14.8308 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0080 |          48.0709 |          14.8188 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0107 |          47.3923 |          14.7952 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0099 |          46.7293 |          14.7773 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0069 |          46.9034 |          14.7799 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0090 |          46.0538 |          14.7542 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0108 |          45.8236 |          14.7697 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0137 |          45.4286 |          14.7651 |
[32m[20221213 23:22:07 @agent_ppo2.py:185][0m |          -0.0150 |          45.1798 |          14.7488 |
[32m[20221213 23:22:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.46
[32m[20221213 23:22:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.66
[32m[20221213 23:22:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.73
[32m[20221213 23:22:07 @agent_ppo2.py:143][0m Total time:       9.59 min
[32m[20221213 23:22:07 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221213 23:22:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2452 --------------------------#
[32m[20221213 23:22:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0023 |          41.6254 |          14.9442 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0105 |          37.2464 |          14.9814 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0084 |          35.5692 |          14.9833 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0115 |          34.4235 |          14.9648 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0136 |          33.6595 |          14.9720 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0140 |          33.1318 |          14.9751 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0113 |          32.6611 |          14.9708 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0126 |          32.2948 |          14.9868 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0146 |          32.0216 |          14.9745 |
[32m[20221213 23:22:08 @agent_ppo2.py:185][0m |          -0.0152 |          31.7579 |          14.9796 |
[32m[20221213 23:22:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:22:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.74
[32m[20221213 23:22:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.92
[32m[20221213 23:22:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.29
[32m[20221213 23:22:09 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221213 23:22:09 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221213 23:22:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2453 --------------------------#
[32m[20221213 23:22:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:22:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:09 @agent_ppo2.py:185][0m |          -0.0007 |          52.9420 |          14.8655 |
[32m[20221213 23:22:09 @agent_ppo2.py:185][0m |          -0.0012 |          45.7765 |          14.8749 |
[32m[20221213 23:22:09 @agent_ppo2.py:185][0m |          -0.0087 |          45.1608 |          14.8666 |
[32m[20221213 23:22:09 @agent_ppo2.py:185][0m |          -0.0099 |          44.3986 |          14.8298 |
[32m[20221213 23:22:09 @agent_ppo2.py:185][0m |          -0.0113 |          43.9245 |          14.8288 |
[32m[20221213 23:22:09 @agent_ppo2.py:185][0m |          -0.0109 |          43.7806 |          14.8433 |
[32m[20221213 23:22:09 @agent_ppo2.py:185][0m |          -0.0146 |          43.3981 |          14.8286 |
[32m[20221213 23:22:10 @agent_ppo2.py:185][0m |          -0.0075 |          43.4665 |          14.8408 |
[32m[20221213 23:22:10 @agent_ppo2.py:185][0m |          -0.0136 |          42.9179 |          14.8352 |
[32m[20221213 23:22:10 @agent_ppo2.py:185][0m |          -0.0140 |          42.8166 |          14.8162 |
[32m[20221213 23:22:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:22:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.41
[32m[20221213 23:22:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.70
[32m[20221213 23:22:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.14
[32m[20221213 23:22:10 @agent_ppo2.py:143][0m Total time:       9.63 min
[32m[20221213 23:22:10 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221213 23:22:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2454 --------------------------#
[32m[20221213 23:22:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:10 @agent_ppo2.py:185][0m |           0.0109 |          52.8371 |          14.6636 |
[32m[20221213 23:22:10 @agent_ppo2.py:185][0m |          -0.0019 |          43.4741 |          14.6780 |
[32m[20221213 23:22:10 @agent_ppo2.py:185][0m |          -0.0068 |          41.1553 |          14.6808 |
[32m[20221213 23:22:10 @agent_ppo2.py:185][0m |          -0.0072 |          39.4206 |          14.7144 |
[32m[20221213 23:22:11 @agent_ppo2.py:185][0m |          -0.0097 |          38.0642 |          14.6898 |
[32m[20221213 23:22:11 @agent_ppo2.py:185][0m |          -0.0061 |          37.0732 |          14.7077 |
[32m[20221213 23:22:11 @agent_ppo2.py:185][0m |          -0.0114 |          36.1295 |          14.6963 |
[32m[20221213 23:22:11 @agent_ppo2.py:185][0m |          -0.0112 |          35.4734 |          14.7235 |
[32m[20221213 23:22:11 @agent_ppo2.py:185][0m |          -0.0117 |          35.0122 |          14.7305 |
[32m[20221213 23:22:11 @agent_ppo2.py:185][0m |          -0.0133 |          34.5410 |          14.7278 |
[32m[20221213 23:22:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.04
[32m[20221213 23:22:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.84
[32m[20221213 23:22:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.37
[32m[20221213 23:22:11 @agent_ppo2.py:143][0m Total time:       9.65 min
[32m[20221213 23:22:11 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221213 23:22:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2455 --------------------------#
[32m[20221213 23:22:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:11 @agent_ppo2.py:185][0m |           0.0053 |          47.8494 |          14.9011 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0095 |          43.2484 |          14.9101 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0051 |          41.1107 |          14.9074 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0123 |          39.6966 |          14.9024 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0094 |          38.7002 |          14.9101 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0104 |          38.1457 |          14.9022 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0117 |          37.3385 |          14.8999 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0124 |          37.0844 |          14.8875 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0143 |          36.7754 |          14.9052 |
[32m[20221213 23:22:12 @agent_ppo2.py:185][0m |          -0.0143 |          36.2921 |          14.8966 |
[32m[20221213 23:22:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.45
[32m[20221213 23:22:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.99
[32m[20221213 23:22:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.88
[32m[20221213 23:22:12 @agent_ppo2.py:143][0m Total time:       9.67 min
[32m[20221213 23:22:12 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221213 23:22:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2456 --------------------------#
[32m[20221213 23:22:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0002 |          58.2122 |          14.9641 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0036 |          54.7405 |          14.9679 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0109 |          53.6288 |          14.9573 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0093 |          52.4288 |          14.9488 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0102 |          52.2028 |          14.9415 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0130 |          51.9129 |          14.9097 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0170 |          51.5895 |          14.9317 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0127 |          51.3408 |          14.9147 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0129 |          50.9467 |          14.9231 |
[32m[20221213 23:22:13 @agent_ppo2.py:185][0m |          -0.0167 |          50.7212 |          14.9059 |
[32m[20221213 23:22:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.30
[32m[20221213 23:22:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.27
[32m[20221213 23:22:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.36
[32m[20221213 23:22:14 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221213 23:22:14 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221213 23:22:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2457 --------------------------#
[32m[20221213 23:22:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:14 @agent_ppo2.py:185][0m |           0.0020 |          44.3809 |          14.8117 |
[32m[20221213 23:22:14 @agent_ppo2.py:185][0m |          -0.0072 |          41.6017 |          14.8059 |
[32m[20221213 23:22:14 @agent_ppo2.py:185][0m |          -0.0044 |          40.6345 |          14.7815 |
[32m[20221213 23:22:14 @agent_ppo2.py:185][0m |          -0.0090 |          39.5735 |          14.7921 |
[32m[20221213 23:22:14 @agent_ppo2.py:185][0m |          -0.0097 |          39.0903 |          14.7686 |
[32m[20221213 23:22:14 @agent_ppo2.py:185][0m |          -0.0124 |          38.8459 |          14.7610 |
[32m[20221213 23:22:14 @agent_ppo2.py:185][0m |          -0.0149 |          38.6706 |          14.7663 |
[32m[20221213 23:22:15 @agent_ppo2.py:185][0m |          -0.0140 |          38.2582 |          14.7750 |
[32m[20221213 23:22:15 @agent_ppo2.py:185][0m |          -0.0176 |          38.2126 |          14.7634 |
[32m[20221213 23:22:15 @agent_ppo2.py:185][0m |          -0.0153 |          37.8292 |          14.7763 |
[32m[20221213 23:22:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.94
[32m[20221213 23:22:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.28
[32m[20221213 23:22:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.78
[32m[20221213 23:22:15 @agent_ppo2.py:143][0m Total time:       9.71 min
[32m[20221213 23:22:15 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221213 23:22:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2458 --------------------------#
[32m[20221213 23:22:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:15 @agent_ppo2.py:185][0m |          -0.0057 |          50.6061 |          14.9825 |
[32m[20221213 23:22:15 @agent_ppo2.py:185][0m |          -0.0009 |          46.5096 |          15.0216 |
[32m[20221213 23:22:15 @agent_ppo2.py:185][0m |          -0.0077 |          43.9152 |          15.0024 |
[32m[20221213 23:22:15 @agent_ppo2.py:185][0m |          -0.0121 |          42.4350 |          15.0495 |
[32m[20221213 23:22:16 @agent_ppo2.py:185][0m |          -0.0075 |          41.8445 |          15.0346 |
[32m[20221213 23:22:16 @agent_ppo2.py:185][0m |          -0.0094 |          40.9954 |          15.0389 |
[32m[20221213 23:22:16 @agent_ppo2.py:185][0m |          -0.0102 |          40.8885 |          15.0481 |
[32m[20221213 23:22:16 @agent_ppo2.py:185][0m |          -0.0017 |          40.2876 |          15.0739 |
[32m[20221213 23:22:16 @agent_ppo2.py:185][0m |          -0.0134 |          39.6524 |          15.0594 |
[32m[20221213 23:22:16 @agent_ppo2.py:185][0m |          -0.0147 |          39.5263 |          15.0747 |
[32m[20221213 23:22:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:22:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.80
[32m[20221213 23:22:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.87
[32m[20221213 23:22:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.60
[32m[20221213 23:22:16 @agent_ppo2.py:143][0m Total time:       9.74 min
[32m[20221213 23:22:16 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221213 23:22:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2459 --------------------------#
[32m[20221213 23:22:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:16 @agent_ppo2.py:185][0m |           0.0034 |          50.1354 |          14.8268 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0038 |          45.0914 |          14.8333 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |           0.0004 |          45.9120 |          14.8263 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0065 |          42.4381 |          14.8158 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0038 |          42.3366 |          14.8389 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0088 |          40.6626 |          14.8115 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0094 |          40.2167 |          14.8172 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0102 |          39.6046 |          14.7979 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0137 |          39.2502 |          14.7850 |
[32m[20221213 23:22:17 @agent_ppo2.py:185][0m |          -0.0128 |          38.9570 |          14.7918 |
[32m[20221213 23:22:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.06
[32m[20221213 23:22:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.30
[32m[20221213 23:22:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.11
[32m[20221213 23:22:17 @agent_ppo2.py:143][0m Total time:       9.76 min
[32m[20221213 23:22:17 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221213 23:22:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2460 --------------------------#
[32m[20221213 23:22:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0060 |          57.1159 |          14.9266 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0053 |          33.7995 |          14.9052 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |           0.0021 |          37.4717 |          14.9074 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0005 |          32.0686 |          14.8603 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0048 |          31.1092 |          14.8739 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0106 |          30.8558 |          14.8424 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0046 |          30.5407 |          14.8487 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0093 |          30.4650 |          14.8564 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0164 |          29.9286 |          14.8217 |
[32m[20221213 23:22:18 @agent_ppo2.py:185][0m |          -0.0087 |          32.0251 |          14.8239 |
[32m[20221213 23:22:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:22:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.68
[32m[20221213 23:22:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.72
[32m[20221213 23:22:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.59
[32m[20221213 23:22:19 @agent_ppo2.py:143][0m Total time:       9.78 min
[32m[20221213 23:22:19 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221213 23:22:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2461 --------------------------#
[32m[20221213 23:22:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:19 @agent_ppo2.py:185][0m |          -0.0035 |          33.7078 |          14.8323 |
[32m[20221213 23:22:19 @agent_ppo2.py:185][0m |          -0.0126 |          23.5144 |          14.8029 |
[32m[20221213 23:22:19 @agent_ppo2.py:185][0m |          -0.0146 |          21.9374 |          14.7827 |
[32m[20221213 23:22:19 @agent_ppo2.py:185][0m |          -0.0016 |          21.1137 |          14.7700 |
[32m[20221213 23:22:19 @agent_ppo2.py:185][0m |          -0.0109 |          20.3176 |          14.7434 |
[32m[20221213 23:22:19 @agent_ppo2.py:185][0m |          -0.0127 |          19.8248 |          14.7083 |
[32m[20221213 23:22:19 @agent_ppo2.py:185][0m |          -0.0189 |          19.2779 |          14.7080 |
[32m[20221213 23:22:20 @agent_ppo2.py:185][0m |          -0.0167 |          19.0549 |          14.6875 |
[32m[20221213 23:22:20 @agent_ppo2.py:185][0m |          -0.0131 |          18.6411 |          14.6529 |
[32m[20221213 23:22:20 @agent_ppo2.py:185][0m |          -0.0156 |          18.4759 |          14.6515 |
[32m[20221213 23:22:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.21
[32m[20221213 23:22:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.62
[32m[20221213 23:22:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.50
[32m[20221213 23:22:20 @agent_ppo2.py:143][0m Total time:       9.80 min
[32m[20221213 23:22:20 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221213 23:22:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2462 --------------------------#
[32m[20221213 23:22:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:20 @agent_ppo2.py:185][0m |          -0.0002 |          43.4197 |          14.7493 |
[32m[20221213 23:22:20 @agent_ppo2.py:185][0m |          -0.0062 |          38.4763 |          14.7221 |
[32m[20221213 23:22:20 @agent_ppo2.py:185][0m |          -0.0092 |          37.2525 |          14.7251 |
[32m[20221213 23:22:20 @agent_ppo2.py:185][0m |          -0.0081 |          36.4443 |          14.6971 |
[32m[20221213 23:22:21 @agent_ppo2.py:185][0m |          -0.0140 |          35.9539 |          14.7124 |
[32m[20221213 23:22:21 @agent_ppo2.py:185][0m |          -0.0113 |          35.5833 |          14.7085 |
[32m[20221213 23:22:21 @agent_ppo2.py:185][0m |          -0.0123 |          35.4400 |          14.6811 |
[32m[20221213 23:22:21 @agent_ppo2.py:185][0m |          -0.0120 |          35.0055 |          14.6615 |
[32m[20221213 23:22:21 @agent_ppo2.py:185][0m |          -0.0136 |          34.8782 |          14.6708 |
[32m[20221213 23:22:21 @agent_ppo2.py:185][0m |          -0.0062 |          35.3791 |          14.6740 |
[32m[20221213 23:22:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:22:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.20
[32m[20221213 23:22:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.30
[32m[20221213 23:22:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.30
[32m[20221213 23:22:21 @agent_ppo2.py:143][0m Total time:       9.82 min
[32m[20221213 23:22:21 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221213 23:22:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2463 --------------------------#
[32m[20221213 23:22:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:21 @agent_ppo2.py:185][0m |           0.0072 |          40.0810 |          14.5255 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0070 |          36.9482 |          14.4863 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0041 |          36.1053 |          14.4872 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0108 |          35.6313 |          14.4973 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0085 |          35.3072 |          14.4860 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0048 |          35.4529 |          14.4977 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0136 |          34.3377 |          14.4933 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0106 |          34.1780 |          14.5081 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0139 |          33.9509 |          14.4979 |
[32m[20221213 23:22:22 @agent_ppo2.py:185][0m |          -0.0102 |          33.5519 |          14.5108 |
[32m[20221213 23:22:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:22:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.56
[32m[20221213 23:22:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.51
[32m[20221213 23:22:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.75
[32m[20221213 23:22:22 @agent_ppo2.py:143][0m Total time:       9.84 min
[32m[20221213 23:22:22 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221213 23:22:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2464 --------------------------#
[32m[20221213 23:22:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0019 |          30.1165 |          14.6450 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0055 |          23.0961 |          14.6317 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0091 |          21.5065 |          14.6565 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0043 |          20.7774 |          14.6800 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0109 |          20.1646 |          14.6949 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0107 |          19.7478 |          14.6956 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0118 |          19.5230 |          14.6925 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0095 |          19.7395 |          14.7019 |
[32m[20221213 23:22:23 @agent_ppo2.py:185][0m |          -0.0144 |          19.0074 |          14.7217 |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |          -0.0184 |          18.8546 |          14.7303 |
[32m[20221213 23:22:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.50
[32m[20221213 23:22:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.74
[32m[20221213 23:22:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.85
[32m[20221213 23:22:24 @agent_ppo2.py:143][0m Total time:       9.86 min
[32m[20221213 23:22:24 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221213 23:22:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2465 --------------------------#
[32m[20221213 23:22:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:22:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |           0.0087 |          29.2866 |          14.8140 |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |          -0.0035 |          24.9082 |          14.8115 |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |          -0.0065 |          23.8502 |          14.8097 |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |          -0.0060 |          23.2419 |          14.7755 |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |          -0.0104 |          23.0003 |          14.7997 |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |          -0.0080 |          22.6629 |          14.7692 |
[32m[20221213 23:22:24 @agent_ppo2.py:185][0m |          -0.0107 |          22.2958 |          14.7754 |
[32m[20221213 23:22:25 @agent_ppo2.py:185][0m |          -0.0022 |          23.3889 |          14.7780 |
[32m[20221213 23:22:25 @agent_ppo2.py:185][0m |          -0.0102 |          22.0334 |          14.7871 |
[32m[20221213 23:22:25 @agent_ppo2.py:185][0m |          -0.0114 |          21.8627 |          14.7467 |
[32m[20221213 23:22:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.78
[32m[20221213 23:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.60
[32m[20221213 23:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.15
[32m[20221213 23:22:25 @agent_ppo2.py:143][0m Total time:       9.88 min
[32m[20221213 23:22:25 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221213 23:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2466 --------------------------#
[32m[20221213 23:22:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:22:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:25 @agent_ppo2.py:185][0m |          -0.0008 |          47.1634 |          14.6594 |
[32m[20221213 23:22:25 @agent_ppo2.py:185][0m |           0.0006 |          44.8238 |          14.6361 |
[32m[20221213 23:22:25 @agent_ppo2.py:185][0m |          -0.0088 |          42.8374 |          14.6222 |
[32m[20221213 23:22:25 @agent_ppo2.py:185][0m |          -0.0070 |          42.2704 |          14.6075 |
[32m[20221213 23:22:26 @agent_ppo2.py:185][0m |          -0.0080 |          42.1424 |          14.6069 |
[32m[20221213 23:22:26 @agent_ppo2.py:185][0m |          -0.0093 |          42.0105 |          14.6039 |
[32m[20221213 23:22:26 @agent_ppo2.py:185][0m |          -0.0147 |          41.6724 |          14.5667 |
[32m[20221213 23:22:26 @agent_ppo2.py:185][0m |          -0.0072 |          41.4125 |          14.5911 |
[32m[20221213 23:22:26 @agent_ppo2.py:185][0m |          -0.0132 |          41.0007 |          14.5613 |
[32m[20221213 23:22:26 @agent_ppo2.py:185][0m |          -0.0117 |          40.8820 |          14.5880 |
[32m[20221213 23:22:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.00
[32m[20221213 23:22:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.42
[32m[20221213 23:22:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.39
[32m[20221213 23:22:26 @agent_ppo2.py:143][0m Total time:       9.90 min
[32m[20221213 23:22:26 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221213 23:22:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2467 --------------------------#
[32m[20221213 23:22:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:26 @agent_ppo2.py:185][0m |           0.0049 |          56.3077 |          14.6922 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |           0.0027 |          54.8691 |          14.6752 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |          -0.0054 |          52.0855 |          14.6752 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |          -0.0077 |          51.5376 |          14.6878 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |          -0.0089 |          51.0231 |          14.7026 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |           0.0049 |          57.7550 |          14.6778 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |          -0.0055 |          50.1601 |          14.6698 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |          -0.0148 |          50.1062 |          14.6705 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |          -0.0121 |          49.6715 |          14.6741 |
[32m[20221213 23:22:27 @agent_ppo2.py:185][0m |          -0.0082 |          50.0208 |          14.6533 |
[32m[20221213 23:22:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:22:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.55
[32m[20221213 23:22:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.14
[32m[20221213 23:22:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.27
[32m[20221213 23:22:27 @agent_ppo2.py:143][0m Total time:       9.92 min
[32m[20221213 23:22:27 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221213 23:22:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2468 --------------------------#
[32m[20221213 23:22:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |           0.0030 |          47.7336 |          14.5495 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0063 |          43.5370 |          14.5817 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0079 |          42.4830 |          14.5718 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0106 |          42.0219 |          14.5679 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0061 |          41.7476 |          14.5839 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0100 |          41.4113 |          14.6281 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0110 |          41.1965 |          14.6440 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0095 |          40.9702 |          14.6544 |
[32m[20221213 23:22:28 @agent_ppo2.py:185][0m |          -0.0151 |          40.8660 |          14.6393 |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |          -0.0048 |          41.3725 |          14.6803 |
[32m[20221213 23:22:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.06
[32m[20221213 23:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.60
[32m[20221213 23:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.27
[32m[20221213 23:22:29 @agent_ppo2.py:143][0m Total time:       9.94 min
[32m[20221213 23:22:29 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221213 23:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2469 --------------------------#
[32m[20221213 23:22:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |           0.0049 |          30.4371 |          14.5670 |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |          -0.0083 |          25.1008 |          14.5377 |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |          -0.0086 |          24.2665 |          14.5225 |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |          -0.0053 |          23.9440 |          14.5183 |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |          -0.0111 |          23.6490 |          14.5294 |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |          -0.0096 |          23.4843 |          14.5228 |
[32m[20221213 23:22:29 @agent_ppo2.py:185][0m |          -0.0046 |          26.0007 |          14.5336 |
[32m[20221213 23:22:30 @agent_ppo2.py:185][0m |          -0.0105 |          23.1868 |          14.5429 |
[32m[20221213 23:22:30 @agent_ppo2.py:185][0m |          -0.0101 |          22.9899 |          14.5252 |
[32m[20221213 23:22:30 @agent_ppo2.py:185][0m |          -0.0175 |          22.9755 |          14.5555 |
[32m[20221213 23:22:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:22:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.75
[32m[20221213 23:22:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.38
[32m[20221213 23:22:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.73
[32m[20221213 23:22:30 @agent_ppo2.py:143][0m Total time:       9.96 min
[32m[20221213 23:22:30 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221213 23:22:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2470 --------------------------#
[32m[20221213 23:22:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:22:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:30 @agent_ppo2.py:185][0m |          -0.0004 |          46.1165 |          14.7774 |
[32m[20221213 23:22:30 @agent_ppo2.py:185][0m |          -0.0019 |          43.9957 |          14.7558 |
[32m[20221213 23:22:30 @agent_ppo2.py:185][0m |          -0.0060 |          43.1762 |          14.7132 |
[32m[20221213 23:22:30 @agent_ppo2.py:185][0m |          -0.0047 |          42.8492 |          14.7138 |
[32m[20221213 23:22:31 @agent_ppo2.py:185][0m |          -0.0083 |          42.6086 |          14.7058 |
[32m[20221213 23:22:31 @agent_ppo2.py:185][0m |          -0.0060 |          42.5139 |          14.6941 |
[32m[20221213 23:22:31 @agent_ppo2.py:185][0m |          -0.0063 |          42.2155 |          14.6955 |
[32m[20221213 23:22:31 @agent_ppo2.py:185][0m |           0.0043 |          44.3572 |          14.6854 |
[32m[20221213 23:22:31 @agent_ppo2.py:185][0m |          -0.0065 |          41.8937 |          14.6517 |
[32m[20221213 23:22:31 @agent_ppo2.py:185][0m |          -0.0028 |          42.0123 |          14.6442 |
[32m[20221213 23:22:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.33
[32m[20221213 23:22:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.98
[32m[20221213 23:22:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.60
[32m[20221213 23:22:31 @agent_ppo2.py:143][0m Total time:       9.99 min
[32m[20221213 23:22:31 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221213 23:22:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2471 --------------------------#
[32m[20221213 23:22:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:31 @agent_ppo2.py:185][0m |           0.0006 |          55.0166 |          14.5046 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0062 |          53.1304 |          14.5344 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0005 |          55.9946 |          14.5380 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0089 |          52.6003 |          14.5425 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0107 |          52.4393 |          14.5323 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0093 |          52.2070 |          14.5359 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0108 |          52.2976 |          14.5352 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0105 |          52.0472 |          14.5633 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0134 |          52.0661 |          14.5537 |
[32m[20221213 23:22:32 @agent_ppo2.py:185][0m |          -0.0124 |          51.9161 |          14.5478 |
[32m[20221213 23:22:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.98
[32m[20221213 23:22:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.10
[32m[20221213 23:22:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.97
[32m[20221213 23:22:32 @agent_ppo2.py:143][0m Total time:      10.01 min
[32m[20221213 23:22:32 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221213 23:22:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2472 --------------------------#
[32m[20221213 23:22:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0011 |          46.6495 |          14.5606 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0039 |          41.6058 |          14.5848 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |           0.0004 |          41.2279 |          14.5949 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0095 |          39.0677 |          14.6191 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0109 |          38.3765 |          14.6239 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0121 |          37.9244 |          14.6479 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0120 |          37.6678 |          14.6584 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0105 |          37.3990 |          14.6575 |
[32m[20221213 23:22:33 @agent_ppo2.py:185][0m |          -0.0170 |          37.0338 |          14.6636 |
[32m[20221213 23:22:34 @agent_ppo2.py:185][0m |          -0.0168 |          36.9389 |          14.6789 |
[32m[20221213 23:22:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:22:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.45
[32m[20221213 23:22:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.25
[32m[20221213 23:22:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.75
[32m[20221213 23:22:34 @agent_ppo2.py:143][0m Total time:      10.03 min
[32m[20221213 23:22:34 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221213 23:22:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2473 --------------------------#
[32m[20221213 23:22:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:34 @agent_ppo2.py:185][0m |          -0.0042 |          33.2239 |          14.6328 |
[32m[20221213 23:22:34 @agent_ppo2.py:185][0m |          -0.0064 |          28.6237 |          14.6092 |
[32m[20221213 23:22:34 @agent_ppo2.py:185][0m |          -0.0093 |          27.2407 |          14.6053 |
[32m[20221213 23:22:34 @agent_ppo2.py:185][0m |          -0.0055 |          26.2869 |          14.6194 |
[32m[20221213 23:22:34 @agent_ppo2.py:185][0m |          -0.0145 |          25.7640 |          14.6050 |
[32m[20221213 23:22:34 @agent_ppo2.py:185][0m |          -0.0126 |          25.3458 |          14.5998 |
[32m[20221213 23:22:35 @agent_ppo2.py:185][0m |          -0.0113 |          24.9519 |          14.5803 |
[32m[20221213 23:22:35 @agent_ppo2.py:185][0m |          -0.0086 |          24.7353 |          14.5885 |
[32m[20221213 23:22:35 @agent_ppo2.py:185][0m |          -0.0048 |          26.1466 |          14.5972 |
[32m[20221213 23:22:35 @agent_ppo2.py:185][0m |          -0.0122 |          24.2564 |          14.6039 |
[32m[20221213 23:22:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.09
[32m[20221213 23:22:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.71
[32m[20221213 23:22:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.00
[32m[20221213 23:22:35 @agent_ppo2.py:143][0m Total time:      10.05 min
[32m[20221213 23:22:35 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221213 23:22:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2474 --------------------------#
[32m[20221213 23:22:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:35 @agent_ppo2.py:185][0m |           0.0006 |          40.6566 |          14.5886 |
[32m[20221213 23:22:35 @agent_ppo2.py:185][0m |           0.0039 |          38.9940 |          14.5906 |
[32m[20221213 23:22:35 @agent_ppo2.py:185][0m |          -0.0085 |          37.3987 |          14.5686 |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0090 |          36.4907 |          14.6015 |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0089 |          35.9489 |          14.6020 |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0090 |          35.4245 |          14.6096 |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0081 |          34.8313 |          14.5895 |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0083 |          34.7046 |          14.5987 |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0114 |          34.1584 |          14.6077 |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0145 |          33.8849 |          14.6279 |
[32m[20221213 23:22:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.85
[32m[20221213 23:22:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.14
[32m[20221213 23:22:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.11
[32m[20221213 23:22:36 @agent_ppo2.py:143][0m Total time:      10.07 min
[32m[20221213 23:22:36 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221213 23:22:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2475 --------------------------#
[32m[20221213 23:22:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:36 @agent_ppo2.py:185][0m |          -0.0041 |          51.6233 |          14.6413 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0065 |          46.6363 |          14.6369 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0091 |          43.1669 |          14.6074 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0082 |          42.0773 |          14.6036 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0095 |          41.5538 |          14.5838 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |           0.0033 |          47.3091 |          14.5913 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0006 |          42.5050 |          14.5908 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0125 |          40.8863 |          14.5824 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0104 |          40.0487 |          14.5903 |
[32m[20221213 23:22:37 @agent_ppo2.py:185][0m |          -0.0159 |          39.7653 |          14.5581 |
[32m[20221213 23:22:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.81
[32m[20221213 23:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.83
[32m[20221213 23:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.34
[32m[20221213 23:22:37 @agent_ppo2.py:143][0m Total time:      10.09 min
[32m[20221213 23:22:37 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221213 23:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2476 --------------------------#
[32m[20221213 23:22:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |           0.0002 |          35.6660 |          14.5491 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0003 |          29.4903 |          14.5443 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0008 |          28.8809 |          14.5380 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0082 |          27.5380 |          14.5190 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0083 |          27.0946 |          14.5526 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0112 |          26.8389 |          14.5642 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0052 |          27.8791 |          14.5392 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0133 |          26.2343 |          14.5395 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0073 |          26.0014 |          14.5572 |
[32m[20221213 23:22:38 @agent_ppo2.py:185][0m |          -0.0108 |          25.8048 |          14.5549 |
[32m[20221213 23:22:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.22
[32m[20221213 23:22:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.36
[32m[20221213 23:22:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.66
[32m[20221213 23:22:39 @agent_ppo2.py:143][0m Total time:      10.11 min
[32m[20221213 23:22:39 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221213 23:22:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2477 --------------------------#
[32m[20221213 23:22:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:39 @agent_ppo2.py:185][0m |           0.0094 |          45.8140 |          14.5216 |
[32m[20221213 23:22:39 @agent_ppo2.py:185][0m |          -0.0034 |          38.2197 |          14.5202 |
[32m[20221213 23:22:39 @agent_ppo2.py:185][0m |          -0.0124 |          36.6612 |          14.5041 |
[32m[20221213 23:22:39 @agent_ppo2.py:185][0m |          -0.0121 |          36.0461 |          14.4855 |
[32m[20221213 23:22:39 @agent_ppo2.py:185][0m |          -0.0028 |          35.8504 |          14.4680 |
[32m[20221213 23:22:39 @agent_ppo2.py:185][0m |          -0.0086 |          34.7917 |          14.4841 |
[32m[20221213 23:22:40 @agent_ppo2.py:185][0m |          -0.0102 |          34.6803 |          14.4767 |
[32m[20221213 23:22:40 @agent_ppo2.py:185][0m |          -0.0038 |          36.4763 |          14.4338 |
[32m[20221213 23:22:40 @agent_ppo2.py:185][0m |          -0.0141 |          33.7009 |          14.4292 |
[32m[20221213 23:22:40 @agent_ppo2.py:185][0m |          -0.0146 |          33.5654 |          14.4431 |
[32m[20221213 23:22:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:22:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.89
[32m[20221213 23:22:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.01
[32m[20221213 23:22:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.32
[32m[20221213 23:22:40 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221213 23:22:40 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221213 23:22:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2478 --------------------------#
[32m[20221213 23:22:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:40 @agent_ppo2.py:185][0m |          -0.0003 |          46.3945 |          14.4985 |
[32m[20221213 23:22:40 @agent_ppo2.py:185][0m |          -0.0004 |          42.9508 |          14.5056 |
[32m[20221213 23:22:40 @agent_ppo2.py:185][0m |          -0.0072 |          41.6836 |          14.5008 |
[32m[20221213 23:22:41 @agent_ppo2.py:185][0m |          -0.0035 |          41.1213 |          14.5347 |
[32m[20221213 23:22:41 @agent_ppo2.py:185][0m |          -0.0098 |          40.6629 |          14.5009 |
[32m[20221213 23:22:41 @agent_ppo2.py:185][0m |          -0.0015 |          41.7012 |          14.5339 |
[32m[20221213 23:22:41 @agent_ppo2.py:185][0m |          -0.0115 |          39.9458 |          14.5203 |
[32m[20221213 23:22:41 @agent_ppo2.py:185][0m |          -0.0136 |          39.7061 |          14.5233 |
[32m[20221213 23:22:41 @agent_ppo2.py:185][0m |          -0.0120 |          39.3688 |          14.5091 |
[32m[20221213 23:22:41 @agent_ppo2.py:185][0m |          -0.0096 |          39.1392 |          14.5070 |
[32m[20221213 23:22:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.30
[32m[20221213 23:22:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.71
[32m[20221213 23:22:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.78
[32m[20221213 23:22:41 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221213 23:22:41 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221213 23:22:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2479 --------------------------#
[32m[20221213 23:22:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0029 |          49.7000 |          14.2366 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0102 |          45.3963 |          14.2200 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0103 |          44.0784 |          14.2638 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0102 |          43.2331 |          14.2506 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0103 |          42.8082 |          14.2713 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0136 |          42.5695 |          14.2510 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0161 |          41.9876 |          14.2649 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0116 |          41.2013 |          14.2656 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0078 |          42.2323 |          14.2883 |
[32m[20221213 23:22:42 @agent_ppo2.py:185][0m |          -0.0142 |          40.6948 |          14.2802 |
[32m[20221213 23:22:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.04
[32m[20221213 23:22:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.78
[32m[20221213 23:22:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.72
[32m[20221213 23:22:42 @agent_ppo2.py:143][0m Total time:      10.17 min
[32m[20221213 23:22:42 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221213 23:22:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2480 --------------------------#
[32m[20221213 23:22:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:22:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0034 |          36.5462 |          14.6844 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0103 |          31.3306 |          14.6946 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0093 |          30.3320 |          14.6888 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0109 |          29.6656 |          14.6980 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0032 |          32.7657 |          14.7028 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0146 |          28.8223 |          14.7169 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0174 |          28.2995 |          14.7022 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0104 |          28.4013 |          14.7124 |
[32m[20221213 23:22:43 @agent_ppo2.py:185][0m |          -0.0150 |          27.9240 |          14.7136 |
[32m[20221213 23:22:44 @agent_ppo2.py:185][0m |          -0.0193 |          27.5691 |          14.7119 |
[32m[20221213 23:22:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.29
[32m[20221213 23:22:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.29
[32m[20221213 23:22:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.48
[32m[20221213 23:22:44 @agent_ppo2.py:143][0m Total time:      10.19 min
[32m[20221213 23:22:44 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221213 23:22:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2481 --------------------------#
[32m[20221213 23:22:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:44 @agent_ppo2.py:185][0m |           0.0001 |          42.0725 |          14.4616 |
[32m[20221213 23:22:44 @agent_ppo2.py:185][0m |          -0.0060 |          38.9918 |          14.4720 |
[32m[20221213 23:22:44 @agent_ppo2.py:185][0m |          -0.0048 |          37.9412 |          14.4750 |
[32m[20221213 23:22:44 @agent_ppo2.py:185][0m |          -0.0048 |          37.2903 |          14.4900 |
[32m[20221213 23:22:44 @agent_ppo2.py:185][0m |          -0.0104 |          36.4944 |          14.4995 |
[32m[20221213 23:22:44 @agent_ppo2.py:185][0m |          -0.0101 |          36.1434 |          14.5128 |
[32m[20221213 23:22:45 @agent_ppo2.py:185][0m |          -0.0111 |          35.7808 |          14.5039 |
[32m[20221213 23:22:45 @agent_ppo2.py:185][0m |          -0.0110 |          35.5348 |          14.5140 |
[32m[20221213 23:22:45 @agent_ppo2.py:185][0m |          -0.0113 |          35.1799 |          14.5089 |
[32m[20221213 23:22:45 @agent_ppo2.py:185][0m |          -0.0129 |          34.9902 |          14.5390 |
[32m[20221213 23:22:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.08
[32m[20221213 23:22:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.47
[32m[20221213 23:22:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.33
[32m[20221213 23:22:45 @agent_ppo2.py:143][0m Total time:      10.22 min
[32m[20221213 23:22:45 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221213 23:22:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2482 --------------------------#
[32m[20221213 23:22:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:45 @agent_ppo2.py:185][0m |          -0.0006 |          38.8506 |          14.5321 |
[32m[20221213 23:22:45 @agent_ppo2.py:185][0m |           0.0044 |          37.0320 |          14.4854 |
[32m[20221213 23:22:45 @agent_ppo2.py:185][0m |          -0.0080 |          33.9581 |          14.5144 |
[32m[20221213 23:22:46 @agent_ppo2.py:185][0m |          -0.0066 |          33.3811 |          14.5156 |
[32m[20221213 23:22:46 @agent_ppo2.py:185][0m |          -0.0002 |          40.6128 |          14.4868 |
[32m[20221213 23:22:46 @agent_ppo2.py:185][0m |          -0.0131 |          32.9401 |          14.4965 |
[32m[20221213 23:22:46 @agent_ppo2.py:185][0m |          -0.0149 |          32.0970 |          14.4930 |
[32m[20221213 23:22:46 @agent_ppo2.py:185][0m |          -0.0166 |          31.7835 |          14.4922 |
[32m[20221213 23:22:46 @agent_ppo2.py:185][0m |          -0.0178 |          31.3952 |          14.4961 |
[32m[20221213 23:22:46 @agent_ppo2.py:185][0m |          -0.0119 |          31.1554 |          14.4805 |
[32m[20221213 23:22:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:22:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.18
[32m[20221213 23:22:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.75
[32m[20221213 23:22:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.73
[32m[20221213 23:22:46 @agent_ppo2.py:143][0m Total time:      10.24 min
[32m[20221213 23:22:46 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221213 23:22:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2483 --------------------------#
[32m[20221213 23:22:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0078 |          40.1665 |          14.3478 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0048 |          37.5533 |          14.2999 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0094 |          36.6998 |          14.3362 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0138 |          36.2010 |          14.3107 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0110 |          36.3868 |          14.2995 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0166 |          35.6498 |          14.2973 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0183 |          35.3085 |          14.2796 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0150 |          35.0469 |          14.2780 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0174 |          34.7808 |          14.2720 |
[32m[20221213 23:22:47 @agent_ppo2.py:185][0m |          -0.0214 |          34.6346 |          14.2851 |
[32m[20221213 23:22:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.03
[32m[20221213 23:22:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.25
[32m[20221213 23:22:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.93
[32m[20221213 23:22:47 @agent_ppo2.py:143][0m Total time:      10.26 min
[32m[20221213 23:22:47 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221213 23:22:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2484 --------------------------#
[32m[20221213 23:22:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |           0.0230 |          55.3979 |          14.4297 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0095 |          40.6513 |          14.4243 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0084 |          38.0688 |          14.4342 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0139 |          36.6919 |          14.4520 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0140 |          35.9459 |          14.4592 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0127 |          35.5245 |          14.4649 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0037 |          36.5849 |          14.4862 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0123 |          34.6116 |          14.4930 |
[32m[20221213 23:22:48 @agent_ppo2.py:185][0m |          -0.0117 |          34.0745 |          14.4954 |
[32m[20221213 23:22:49 @agent_ppo2.py:185][0m |          -0.0141 |          33.9717 |          14.5129 |
[32m[20221213 23:22:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.16
[32m[20221213 23:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.08
[32m[20221213 23:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.97
[32m[20221213 23:22:49 @agent_ppo2.py:143][0m Total time:      10.28 min
[32m[20221213 23:22:49 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221213 23:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2485 --------------------------#
[32m[20221213 23:22:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:22:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:49 @agent_ppo2.py:185][0m |          -0.0003 |          51.4232 |          14.8898 |
[32m[20221213 23:22:49 @agent_ppo2.py:185][0m |          -0.0039 |          47.4262 |          14.8843 |
[32m[20221213 23:22:49 @agent_ppo2.py:185][0m |          -0.0099 |          45.3654 |          14.8637 |
[32m[20221213 23:22:49 @agent_ppo2.py:185][0m |          -0.0097 |          44.1704 |          14.8648 |
[32m[20221213 23:22:49 @agent_ppo2.py:185][0m |          -0.0099 |          43.3588 |          14.8801 |
[32m[20221213 23:22:49 @agent_ppo2.py:185][0m |          -0.0061 |          44.4547 |          14.9004 |
[32m[20221213 23:22:50 @agent_ppo2.py:185][0m |          -0.0070 |          42.3321 |          14.9012 |
[32m[20221213 23:22:50 @agent_ppo2.py:185][0m |          -0.0147 |          41.9160 |          14.9108 |
[32m[20221213 23:22:50 @agent_ppo2.py:185][0m |          -0.0131 |          41.2905 |          14.9129 |
[32m[20221213 23:22:50 @agent_ppo2.py:185][0m |          -0.0155 |          41.0459 |          14.9243 |
[32m[20221213 23:22:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.07
[32m[20221213 23:22:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.12
[32m[20221213 23:22:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.00
[32m[20221213 23:22:50 @agent_ppo2.py:143][0m Total time:      10.30 min
[32m[20221213 23:22:50 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221213 23:22:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2486 --------------------------#
[32m[20221213 23:22:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:50 @agent_ppo2.py:185][0m |           0.0007 |          50.2197 |          14.5524 |
[32m[20221213 23:22:50 @agent_ppo2.py:185][0m |          -0.0053 |          45.8399 |          14.5269 |
[32m[20221213 23:22:50 @agent_ppo2.py:185][0m |          -0.0056 |          44.4601 |          14.4896 |
[32m[20221213 23:22:51 @agent_ppo2.py:185][0m |          -0.0081 |          43.6089 |          14.4770 |
[32m[20221213 23:22:51 @agent_ppo2.py:185][0m |          -0.0095 |          42.8801 |          14.4603 |
[32m[20221213 23:22:51 @agent_ppo2.py:185][0m |          -0.0058 |          42.3052 |          14.4683 |
[32m[20221213 23:22:51 @agent_ppo2.py:185][0m |          -0.0018 |          44.5040 |          14.4353 |
[32m[20221213 23:22:51 @agent_ppo2.py:185][0m |          -0.0138 |          41.3980 |          14.4030 |
[32m[20221213 23:22:51 @agent_ppo2.py:185][0m |          -0.0138 |          40.9432 |          14.4203 |
[32m[20221213 23:22:51 @agent_ppo2.py:185][0m |          -0.0061 |          41.8840 |          14.3852 |
[32m[20221213 23:22:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:22:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.21
[32m[20221213 23:22:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.56
[32m[20221213 23:22:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.33
[32m[20221213 23:22:51 @agent_ppo2.py:143][0m Total time:      10.32 min
[32m[20221213 23:22:51 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221213 23:22:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2487 --------------------------#
[32m[20221213 23:22:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0027 |          37.9127 |          14.5801 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0074 |          32.2828 |          14.5933 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0023 |          31.5523 |          14.5774 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0121 |          30.2849 |          14.5608 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0090 |          29.8193 |          14.5623 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0081 |          29.3581 |          14.5755 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0095 |          29.2721 |          14.5903 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0123 |          28.8188 |          14.6005 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0130 |          28.8536 |          14.5871 |
[32m[20221213 23:22:52 @agent_ppo2.py:185][0m |          -0.0086 |          28.1914 |          14.5849 |
[32m[20221213 23:22:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:22:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.36
[32m[20221213 23:22:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.06
[32m[20221213 23:22:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.38
[32m[20221213 23:22:52 @agent_ppo2.py:143][0m Total time:      10.34 min
[32m[20221213 23:22:52 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221213 23:22:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2488 --------------------------#
[32m[20221213 23:22:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |           0.0024 |          40.6316 |          14.7687 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |           0.0056 |          38.5789 |          14.7105 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |          -0.0074 |          36.1461 |          14.7170 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |          -0.0076 |          35.3696 |          14.7055 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |          -0.0085 |          34.8559 |          14.7129 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |          -0.0086 |          34.5626 |          14.7097 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |          -0.0028 |          36.3181 |          14.6958 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |          -0.0110 |          34.1044 |          14.6738 |
[32m[20221213 23:22:53 @agent_ppo2.py:185][0m |          -0.0056 |          34.0411 |          14.7252 |
[32m[20221213 23:22:54 @agent_ppo2.py:185][0m |          -0.0098 |          34.4143 |          14.6949 |
[32m[20221213 23:22:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.74
[32m[20221213 23:22:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.68
[32m[20221213 23:22:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.54
[32m[20221213 23:22:54 @agent_ppo2.py:143][0m Total time:      10.36 min
[32m[20221213 23:22:54 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221213 23:22:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2489 --------------------------#
[32m[20221213 23:22:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:54 @agent_ppo2.py:185][0m |           0.0006 |          41.1712 |          14.5100 |
[32m[20221213 23:22:54 @agent_ppo2.py:185][0m |           0.0092 |          43.2671 |          14.5254 |
[32m[20221213 23:22:54 @agent_ppo2.py:185][0m |          -0.0045 |          38.2880 |          14.5138 |
[32m[20221213 23:22:54 @agent_ppo2.py:185][0m |          -0.0055 |          37.4267 |          14.5450 |
[32m[20221213 23:22:54 @agent_ppo2.py:185][0m |          -0.0111 |          37.1528 |          14.5458 |
[32m[20221213 23:22:54 @agent_ppo2.py:185][0m |          -0.0067 |          36.9644 |          14.5405 |
[32m[20221213 23:22:55 @agent_ppo2.py:185][0m |          -0.0075 |          36.7456 |          14.5303 |
[32m[20221213 23:22:55 @agent_ppo2.py:185][0m |          -0.0092 |          36.5829 |          14.5600 |
[32m[20221213 23:22:55 @agent_ppo2.py:185][0m |          -0.0118 |          36.4034 |          14.5617 |
[32m[20221213 23:22:55 @agent_ppo2.py:185][0m |          -0.0094 |          36.5157 |          14.5855 |
[32m[20221213 23:22:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:22:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.63
[32m[20221213 23:22:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.56
[32m[20221213 23:22:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.15
[32m[20221213 23:22:55 @agent_ppo2.py:143][0m Total time:      10.38 min
[32m[20221213 23:22:55 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221213 23:22:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2490 --------------------------#
[32m[20221213 23:22:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:55 @agent_ppo2.py:185][0m |          -0.0029 |          37.7877 |          14.7121 |
[32m[20221213 23:22:55 @agent_ppo2.py:185][0m |          -0.0084 |          35.4003 |          14.7232 |
[32m[20221213 23:22:55 @agent_ppo2.py:185][0m |          -0.0103 |          34.6684 |          14.7221 |
[32m[20221213 23:22:56 @agent_ppo2.py:185][0m |          -0.0087 |          34.5871 |          14.7418 |
[32m[20221213 23:22:56 @agent_ppo2.py:185][0m |          -0.0062 |          34.0005 |          14.7341 |
[32m[20221213 23:22:56 @agent_ppo2.py:185][0m |          -0.0129 |          33.7116 |          14.7493 |
[32m[20221213 23:22:56 @agent_ppo2.py:185][0m |          -0.0134 |          33.6252 |          14.7650 |
[32m[20221213 23:22:56 @agent_ppo2.py:185][0m |          -0.0126 |          33.5867 |          14.7577 |
[32m[20221213 23:22:56 @agent_ppo2.py:185][0m |          -0.0132 |          33.5112 |          14.7767 |
[32m[20221213 23:22:56 @agent_ppo2.py:185][0m |          -0.0138 |          33.3612 |          14.7697 |
[32m[20221213 23:22:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:22:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.62
[32m[20221213 23:22:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.56
[32m[20221213 23:22:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.26
[32m[20221213 23:22:56 @agent_ppo2.py:143][0m Total time:      10.40 min
[32m[20221213 23:22:56 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221213 23:22:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2491 --------------------------#
[32m[20221213 23:22:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0022 |          55.8994 |          14.6688 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0002 |          55.3582 |          14.6899 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |           0.0055 |          56.4987 |          14.7279 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0073 |          54.1621 |          14.7233 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |           0.0072 |          60.4317 |          14.7231 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0076 |          53.8041 |          14.7475 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0071 |          53.8968 |          14.7843 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0094 |          53.5948 |          14.8023 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0102 |          53.6069 |          14.8067 |
[32m[20221213 23:22:57 @agent_ppo2.py:185][0m |          -0.0055 |          53.9182 |          14.8295 |
[32m[20221213 23:22:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:22:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.05
[32m[20221213 23:22:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.34
[32m[20221213 23:22:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.89
[32m[20221213 23:22:57 @agent_ppo2.py:143][0m Total time:      10.42 min
[32m[20221213 23:22:57 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221213 23:22:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2492 --------------------------#
[32m[20221213 23:22:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:22:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0079 |          23.6736 |          15.0027 |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0059 |          18.5448 |          14.9659 |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0096 |          17.3693 |          14.9629 |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0122 |          16.7288 |          14.9612 |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0153 |          16.2067 |          14.9311 |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0141 |          15.7643 |          14.9282 |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0224 |          15.4253 |          14.9052 |
[32m[20221213 23:22:58 @agent_ppo2.py:185][0m |          -0.0175 |          15.3091 |          14.8875 |
[32m[20221213 23:22:59 @agent_ppo2.py:185][0m |          -0.0224 |          14.9750 |          14.8729 |
[32m[20221213 23:22:59 @agent_ppo2.py:185][0m |          -0.0267 |          14.6360 |          14.8657 |
[32m[20221213 23:22:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:22:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.80
[32m[20221213 23:22:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.46
[32m[20221213 23:22:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.12
[32m[20221213 23:22:59 @agent_ppo2.py:143][0m Total time:      10.45 min
[32m[20221213 23:22:59 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221213 23:22:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2493 --------------------------#
[32m[20221213 23:22:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:22:59 @agent_ppo2.py:185][0m |          -0.0025 |          40.2825 |          14.7015 |
[32m[20221213 23:22:59 @agent_ppo2.py:185][0m |           0.0063 |          36.2410 |          14.7055 |
[32m[20221213 23:22:59 @agent_ppo2.py:185][0m |          -0.0042 |          33.8835 |          14.6828 |
[32m[20221213 23:22:59 @agent_ppo2.py:185][0m |          -0.0077 |          33.0040 |          14.6889 |
[32m[20221213 23:22:59 @agent_ppo2.py:185][0m |          -0.0011 |          32.9905 |          14.6861 |
[32m[20221213 23:23:00 @agent_ppo2.py:185][0m |          -0.0126 |          31.8106 |          14.6928 |
[32m[20221213 23:23:00 @agent_ppo2.py:185][0m |          -0.0097 |          31.3879 |          14.6853 |
[32m[20221213 23:23:00 @agent_ppo2.py:185][0m |          -0.0080 |          30.9878 |          14.6554 |
[32m[20221213 23:23:00 @agent_ppo2.py:185][0m |          -0.0131 |          30.7928 |          14.6650 |
[32m[20221213 23:23:00 @agent_ppo2.py:185][0m |          -0.0119 |          30.4376 |          14.6630 |
[32m[20221213 23:23:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.15
[32m[20221213 23:23:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.57
[32m[20221213 23:23:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.92
[32m[20221213 23:23:00 @agent_ppo2.py:143][0m Total time:      10.47 min
[32m[20221213 23:23:00 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221213 23:23:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2494 --------------------------#
[32m[20221213 23:23:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:23:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:00 @agent_ppo2.py:185][0m |          -0.0030 |          41.9344 |          14.6207 |
[32m[20221213 23:23:00 @agent_ppo2.py:185][0m |          -0.0040 |          38.1470 |          14.6026 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |          -0.0031 |          36.6978 |          14.5919 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |          -0.0069 |          35.9787 |          14.5762 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |          -0.0048 |          35.6631 |          14.5711 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |          -0.0053 |          35.5317 |          14.5726 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |           0.0037 |          36.8348 |          14.5568 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |          -0.0058 |          35.2338 |          14.5729 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |          -0.0009 |          35.7944 |          14.5554 |
[32m[20221213 23:23:01 @agent_ppo2.py:185][0m |          -0.0051 |          34.8491 |          14.5044 |
[32m[20221213 23:23:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.31
[32m[20221213 23:23:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.16
[32m[20221213 23:23:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.67
[32m[20221213 23:23:01 @agent_ppo2.py:143][0m Total time:      10.49 min
[32m[20221213 23:23:01 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221213 23:23:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2495 --------------------------#
[32m[20221213 23:23:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |           0.0008 |          45.6506 |          14.6382 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0036 |          42.3160 |          14.6321 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0061 |          41.5923 |          14.6019 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0100 |          41.0088 |          14.6102 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0114 |          40.4311 |          14.6025 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0111 |          40.1275 |          14.5794 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0048 |          40.1543 |          14.5852 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0087 |          39.9558 |          14.5876 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0156 |          39.5938 |          14.5749 |
[32m[20221213 23:23:02 @agent_ppo2.py:185][0m |          -0.0119 |          39.2133 |          14.5636 |
[32m[20221213 23:23:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.15
[32m[20221213 23:23:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.52
[32m[20221213 23:23:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.26
[32m[20221213 23:23:02 @agent_ppo2.py:143][0m Total time:      10.51 min
[32m[20221213 23:23:02 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221213 23:23:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2496 --------------------------#
[32m[20221213 23:23:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0023 |          49.2613 |          14.6289 |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0063 |          47.3556 |          14.6294 |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0100 |          46.8197 |          14.6071 |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0068 |          46.3882 |          14.6311 |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0092 |          45.8528 |          14.6150 |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0131 |          45.5264 |          14.6142 |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0127 |          45.0576 |          14.5872 |
[32m[20221213 23:23:03 @agent_ppo2.py:185][0m |          -0.0122 |          44.8855 |          14.5985 |
[32m[20221213 23:23:04 @agent_ppo2.py:185][0m |          -0.0147 |          44.8483 |          14.5944 |
[32m[20221213 23:23:04 @agent_ppo2.py:185][0m |          -0.0090 |          44.9280 |          14.5811 |
[32m[20221213 23:23:04 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 23:23:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.98
[32m[20221213 23:23:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.63
[32m[20221213 23:23:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.35
[32m[20221213 23:23:04 @agent_ppo2.py:143][0m Total time:      10.53 min
[32m[20221213 23:23:04 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221213 23:23:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2497 --------------------------#
[32m[20221213 23:23:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:04 @agent_ppo2.py:185][0m |           0.0008 |          53.8459 |          14.5816 |
[32m[20221213 23:23:04 @agent_ppo2.py:185][0m |          -0.0047 |          51.6026 |          14.5878 |
[32m[20221213 23:23:04 @agent_ppo2.py:185][0m |           0.0026 |          58.7820 |          14.6064 |
[32m[20221213 23:23:04 @agent_ppo2.py:185][0m |          -0.0056 |          51.0301 |          14.5951 |
[32m[20221213 23:23:05 @agent_ppo2.py:185][0m |          -0.0069 |          51.0719 |          14.6238 |
[32m[20221213 23:23:05 @agent_ppo2.py:185][0m |           0.0074 |          54.7230 |          14.6207 |
[32m[20221213 23:23:05 @agent_ppo2.py:185][0m |          -0.0080 |          50.6805 |          14.6290 |
[32m[20221213 23:23:05 @agent_ppo2.py:185][0m |          -0.0092 |          50.3627 |          14.6286 |
[32m[20221213 23:23:05 @agent_ppo2.py:185][0m |          -0.0104 |          50.2817 |          14.6482 |
[32m[20221213 23:23:05 @agent_ppo2.py:185][0m |          -0.0004 |          53.7852 |          14.6512 |
[32m[20221213 23:23:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.86
[32m[20221213 23:23:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.86
[32m[20221213 23:23:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.76
[32m[20221213 23:23:05 @agent_ppo2.py:143][0m Total time:      10.55 min
[32m[20221213 23:23:05 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221213 23:23:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2498 --------------------------#
[32m[20221213 23:23:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:05 @agent_ppo2.py:185][0m |          -0.0014 |          48.5836 |          14.5889 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0019 |          46.0225 |          14.5607 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0085 |          43.5800 |          14.5774 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0102 |          42.6129 |          14.5701 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0128 |          41.8861 |          14.5721 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0006 |          45.4231 |          14.6121 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0128 |          41.5875 |          14.5833 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0073 |          41.2000 |          14.6066 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0057 |          42.6658 |          14.6149 |
[32m[20221213 23:23:06 @agent_ppo2.py:185][0m |          -0.0149 |          40.8180 |          14.6482 |
[32m[20221213 23:23:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.19
[32m[20221213 23:23:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.10
[32m[20221213 23:23:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.19
[32m[20221213 23:23:06 @agent_ppo2.py:143][0m Total time:      10.57 min
[32m[20221213 23:23:06 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221213 23:23:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2499 --------------------------#
[32m[20221213 23:23:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |           0.0032 |          47.1919 |          14.6489 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0085 |          43.5656 |          14.6784 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0053 |          42.6676 |          14.6468 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0078 |          42.3678 |          14.6767 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0043 |          41.8509 |          14.6898 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0093 |          41.3821 |          14.6724 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0047 |          42.7142 |          14.6985 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0098 |          40.9335 |          14.6965 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |          -0.0044 |          42.7354 |          14.7003 |
[32m[20221213 23:23:07 @agent_ppo2.py:185][0m |           0.0010 |          45.2522 |          14.7172 |
[32m[20221213 23:23:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.91
[32m[20221213 23:23:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.54
[32m[20221213 23:23:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.27
[32m[20221213 23:23:08 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 526.37
[32m[20221213 23:23:08 @agent_ppo2.py:143][0m Total time:      10.59 min
[32m[20221213 23:23:08 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221213 23:23:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2500 --------------------------#
[32m[20221213 23:23:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:23:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:08 @agent_ppo2.py:185][0m |          -0.0007 |          34.6815 |          14.7078 |
[32m[20221213 23:23:08 @agent_ppo2.py:185][0m |           0.0023 |          29.5910 |          14.7116 |
[32m[20221213 23:23:08 @agent_ppo2.py:185][0m |          -0.0091 |          27.8665 |          14.7036 |
[32m[20221213 23:23:08 @agent_ppo2.py:185][0m |          -0.0115 |          27.3734 |          14.7079 |
[32m[20221213 23:23:08 @agent_ppo2.py:185][0m |          -0.0125 |          26.4523 |          14.6892 |
[32m[20221213 23:23:08 @agent_ppo2.py:185][0m |          -0.0053 |          27.0679 |          14.7288 |
[32m[20221213 23:23:08 @agent_ppo2.py:185][0m |          -0.0080 |          25.7384 |          14.6946 |
[32m[20221213 23:23:09 @agent_ppo2.py:185][0m |          -0.0168 |          25.3452 |          14.7128 |
[32m[20221213 23:23:09 @agent_ppo2.py:185][0m |          -0.0185 |          25.1110 |          14.7048 |
[32m[20221213 23:23:09 @agent_ppo2.py:185][0m |          -0.0107 |          24.9239 |          14.7145 |
[32m[20221213 23:23:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.03
[32m[20221213 23:23:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.59
[32m[20221213 23:23:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.22
[32m[20221213 23:23:09 @agent_ppo2.py:143][0m Total time:      10.61 min
[32m[20221213 23:23:09 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221213 23:23:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2501 --------------------------#
[32m[20221213 23:23:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:23:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:09 @agent_ppo2.py:185][0m |          -0.0004 |          49.3274 |          14.6113 |
[32m[20221213 23:23:09 @agent_ppo2.py:185][0m |          -0.0034 |          47.6823 |          14.5579 |
[32m[20221213 23:23:09 @agent_ppo2.py:185][0m |          -0.0056 |          47.3693 |          14.5594 |
[32m[20221213 23:23:09 @agent_ppo2.py:185][0m |          -0.0107 |          46.9838 |          14.5276 |
[32m[20221213 23:23:10 @agent_ppo2.py:185][0m |          -0.0094 |          47.0196 |          14.5188 |
[32m[20221213 23:23:10 @agent_ppo2.py:185][0m |          -0.0100 |          46.7425 |          14.5022 |
[32m[20221213 23:23:10 @agent_ppo2.py:185][0m |          -0.0076 |          46.6910 |          14.4894 |
[32m[20221213 23:23:10 @agent_ppo2.py:185][0m |          -0.0094 |          46.3624 |          14.4647 |
[32m[20221213 23:23:10 @agent_ppo2.py:185][0m |          -0.0105 |          46.1750 |          14.4832 |
[32m[20221213 23:23:10 @agent_ppo2.py:185][0m |          -0.0108 |          46.1205 |          14.4619 |
[32m[20221213 23:23:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:23:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.71
[32m[20221213 23:23:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.00
[32m[20221213 23:23:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.39
[32m[20221213 23:23:10 @agent_ppo2.py:143][0m Total time:      10.63 min
[32m[20221213 23:23:10 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221213 23:23:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2502 --------------------------#
[32m[20221213 23:23:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:10 @agent_ppo2.py:185][0m |          -0.0003 |          60.1852 |          14.2805 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |           0.0040 |          61.4044 |          14.2588 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0064 |          56.9885 |          14.2454 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0106 |          56.3144 |          14.2484 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0141 |          55.8365 |          14.2336 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0136 |          55.4912 |          14.2435 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0171 |          55.3635 |          14.2054 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0151 |          54.9369 |          14.2330 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0178 |          54.8204 |          14.2403 |
[32m[20221213 23:23:11 @agent_ppo2.py:185][0m |          -0.0130 |          54.3729 |          14.2127 |
[32m[20221213 23:23:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.39
[32m[20221213 23:23:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.49
[32m[20221213 23:23:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:23:11 @agent_ppo2.py:143][0m Total time:      10.66 min
[32m[20221213 23:23:11 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221213 23:23:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2503 --------------------------#
[32m[20221213 23:23:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:23:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |           0.0091 |          50.7640 |          14.7085 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0059 |          45.8821 |          14.7209 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0059 |          45.5529 |          14.7333 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0073 |          45.0141 |          14.7611 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0109 |          44.8460 |          14.8055 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0083 |          44.9913 |          14.8225 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0101 |          44.5553 |          14.8443 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0084 |          44.5866 |          14.8548 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0092 |          44.4042 |          14.8822 |
[32m[20221213 23:23:12 @agent_ppo2.py:185][0m |          -0.0128 |          44.4190 |          14.8871 |
[32m[20221213 23:23:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.85
[32m[20221213 23:23:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.18
[32m[20221213 23:23:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.94
[32m[20221213 23:23:13 @agent_ppo2.py:143][0m Total time:      10.68 min
[32m[20221213 23:23:13 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221213 23:23:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2504 --------------------------#
[32m[20221213 23:23:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:13 @agent_ppo2.py:185][0m |          -0.0003 |          58.1495 |          14.7269 |
[32m[20221213 23:23:13 @agent_ppo2.py:185][0m |          -0.0078 |          56.3812 |          14.7494 |
[32m[20221213 23:23:13 @agent_ppo2.py:185][0m |          -0.0082 |          55.6283 |          14.7369 |
[32m[20221213 23:23:13 @agent_ppo2.py:185][0m |          -0.0082 |          55.2028 |          14.7227 |
[32m[20221213 23:23:13 @agent_ppo2.py:185][0m |          -0.0087 |          54.7716 |          14.7340 |
[32m[20221213 23:23:13 @agent_ppo2.py:185][0m |          -0.0090 |          54.6071 |          14.7300 |
[32m[20221213 23:23:13 @agent_ppo2.py:185][0m |          -0.0064 |          54.6412 |          14.6990 |
[32m[20221213 23:23:14 @agent_ppo2.py:185][0m |          -0.0076 |          54.4832 |          14.7080 |
[32m[20221213 23:23:14 @agent_ppo2.py:185][0m |          -0.0131 |          53.8942 |          14.6852 |
[32m[20221213 23:23:14 @agent_ppo2.py:185][0m |          -0.0119 |          53.8186 |          14.6829 |
[32m[20221213 23:23:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.70
[32m[20221213 23:23:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.22
[32m[20221213 23:23:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.19
[32m[20221213 23:23:14 @agent_ppo2.py:143][0m Total time:      10.70 min
[32m[20221213 23:23:14 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221213 23:23:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2505 --------------------------#
[32m[20221213 23:23:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:14 @agent_ppo2.py:185][0m |           0.0017 |          53.1534 |          14.4951 |
[32m[20221213 23:23:14 @agent_ppo2.py:185][0m |          -0.0065 |          50.8624 |          14.4694 |
[32m[20221213 23:23:14 @agent_ppo2.py:185][0m |          -0.0027 |          50.3688 |          14.4917 |
[32m[20221213 23:23:14 @agent_ppo2.py:185][0m |          -0.0083 |          49.5999 |          14.4537 |
[32m[20221213 23:23:15 @agent_ppo2.py:185][0m |          -0.0014 |          51.4573 |          14.4556 |
[32m[20221213 23:23:15 @agent_ppo2.py:185][0m |          -0.0104 |          48.9277 |          14.4458 |
[32m[20221213 23:23:15 @agent_ppo2.py:185][0m |          -0.0118 |          48.4869 |          14.4354 |
[32m[20221213 23:23:15 @agent_ppo2.py:185][0m |          -0.0151 |          48.3858 |          14.4327 |
[32m[20221213 23:23:15 @agent_ppo2.py:185][0m |          -0.0104 |          48.2378 |          14.4432 |
[32m[20221213 23:23:15 @agent_ppo2.py:185][0m |          -0.0145 |          48.0160 |          14.4285 |
[32m[20221213 23:23:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:23:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.01
[32m[20221213 23:23:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.36
[32m[20221213 23:23:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.04
[32m[20221213 23:23:15 @agent_ppo2.py:143][0m Total time:      10.72 min
[32m[20221213 23:23:15 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221213 23:23:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2506 --------------------------#
[32m[20221213 23:23:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:15 @agent_ppo2.py:185][0m |          -0.0011 |          55.5482 |          14.2912 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0037 |          52.2764 |          14.3003 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0059 |          50.2988 |          14.3199 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0065 |          48.7160 |          14.3204 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0099 |          47.3203 |          14.3040 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0039 |          46.5776 |          14.2989 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0074 |          46.3437 |          14.3275 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0097 |          45.5668 |          14.3077 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0124 |          45.2974 |          14.2957 |
[32m[20221213 23:23:16 @agent_ppo2.py:185][0m |          -0.0024 |          48.3033 |          14.3046 |
[32m[20221213 23:23:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:23:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.25
[32m[20221213 23:23:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.35
[32m[20221213 23:23:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.85
[32m[20221213 23:23:16 @agent_ppo2.py:143][0m Total time:      10.74 min
[32m[20221213 23:23:16 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221213 23:23:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2507 --------------------------#
[32m[20221213 23:23:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:23:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |           0.0017 |          48.7481 |          14.6204 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |           0.0016 |          43.0313 |          14.5896 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |          -0.0010 |          40.7668 |          14.5870 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |           0.0021 |          42.5234 |          14.5989 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |          -0.0074 |          39.0177 |          14.5682 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |          -0.0063 |          38.3715 |          14.6072 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |          -0.0050 |          37.7949 |          14.5827 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |          -0.0076 |          37.6077 |          14.6065 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |          -0.0062 |          37.2096 |          14.5799 |
[32m[20221213 23:23:17 @agent_ppo2.py:185][0m |          -0.0099 |          37.0347 |          14.5930 |
[32m[20221213 23:23:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.96
[32m[20221213 23:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.40
[32m[20221213 23:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.69
[32m[20221213 23:23:18 @agent_ppo2.py:143][0m Total time:      10.76 min
[32m[20221213 23:23:18 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221213 23:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2508 --------------------------#
[32m[20221213 23:23:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:18 @agent_ppo2.py:185][0m |          -0.0035 |          51.9572 |          14.6595 |
[32m[20221213 23:23:18 @agent_ppo2.py:185][0m |           0.0030 |          46.5666 |          14.6387 |
[32m[20221213 23:23:18 @agent_ppo2.py:185][0m |          -0.0122 |          42.7168 |          14.6449 |
[32m[20221213 23:23:18 @agent_ppo2.py:185][0m |          -0.0075 |          41.2444 |          14.6422 |
[32m[20221213 23:23:18 @agent_ppo2.py:185][0m |          -0.0093 |          40.4689 |          14.6199 |
[32m[20221213 23:23:18 @agent_ppo2.py:185][0m |          -0.0160 |          39.8900 |          14.6140 |
[32m[20221213 23:23:18 @agent_ppo2.py:185][0m |          -0.0090 |          40.1768 |          14.5950 |
[32m[20221213 23:23:19 @agent_ppo2.py:185][0m |          -0.0143 |          39.0592 |          14.5943 |
[32m[20221213 23:23:19 @agent_ppo2.py:185][0m |          -0.0151 |          38.6923 |          14.5751 |
[32m[20221213 23:23:19 @agent_ppo2.py:185][0m |          -0.0111 |          38.5688 |          14.5802 |
[32m[20221213 23:23:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.96
[32m[20221213 23:23:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.62
[32m[20221213 23:23:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.73
[32m[20221213 23:23:19 @agent_ppo2.py:143][0m Total time:      10.78 min
[32m[20221213 23:23:19 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221213 23:23:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2509 --------------------------#
[32m[20221213 23:23:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:19 @agent_ppo2.py:185][0m |          -0.0016 |          47.7200 |          14.7011 |
[32m[20221213 23:23:19 @agent_ppo2.py:185][0m |          -0.0072 |          43.7902 |          14.6525 |
[32m[20221213 23:23:19 @agent_ppo2.py:185][0m |          -0.0080 |          42.6697 |          14.6488 |
[32m[20221213 23:23:19 @agent_ppo2.py:185][0m |          -0.0064 |          42.0672 |          14.6232 |
[32m[20221213 23:23:20 @agent_ppo2.py:185][0m |          -0.0109 |          41.6235 |          14.6173 |
[32m[20221213 23:23:20 @agent_ppo2.py:185][0m |          -0.0089 |          41.2722 |          14.5918 |
[32m[20221213 23:23:20 @agent_ppo2.py:185][0m |          -0.0133 |          40.9069 |          14.6116 |
[32m[20221213 23:23:20 @agent_ppo2.py:185][0m |          -0.0142 |          40.6215 |          14.6178 |
[32m[20221213 23:23:20 @agent_ppo2.py:185][0m |          -0.0105 |          40.4065 |          14.6271 |
[32m[20221213 23:23:20 @agent_ppo2.py:185][0m |          -0.0164 |          40.1476 |          14.5940 |
[32m[20221213 23:23:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.89
[32m[20221213 23:23:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.43
[32m[20221213 23:23:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.06
[32m[20221213 23:23:20 @agent_ppo2.py:143][0m Total time:      10.80 min
[32m[20221213 23:23:20 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221213 23:23:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2510 --------------------------#
[32m[20221213 23:23:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:23:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:20 @agent_ppo2.py:185][0m |          -0.0028 |          53.2145 |          14.4564 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0040 |          52.1963 |          14.3985 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0039 |          51.9279 |          14.4038 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0030 |          51.8572 |          14.4044 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |           0.0053 |          56.8708 |          14.3944 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0014 |          53.8817 |          14.3979 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0073 |          51.1187 |          14.3631 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0111 |          51.1021 |          14.3753 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0075 |          50.9944 |          14.3861 |
[32m[20221213 23:23:21 @agent_ppo2.py:185][0m |          -0.0108 |          50.9007 |          14.3827 |
[32m[20221213 23:23:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:23:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.21
[32m[20221213 23:23:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.27
[32m[20221213 23:23:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.11
[32m[20221213 23:23:21 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221213 23:23:21 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221213 23:23:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2511 --------------------------#
[32m[20221213 23:23:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |           0.0000 |          48.0588 |          14.4936 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0058 |          47.2911 |          14.4552 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0064 |          46.6378 |          14.4802 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |           0.0066 |          49.2763 |          14.4648 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0066 |          46.1086 |          14.4897 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0094 |          45.8331 |          14.4969 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0097 |          45.7927 |          14.4819 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0032 |          46.3713 |          14.4765 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0106 |          45.6569 |          14.5014 |
[32m[20221213 23:23:22 @agent_ppo2.py:185][0m |          -0.0071 |          45.4861 |          14.4947 |
[32m[20221213 23:23:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.86
[32m[20221213 23:23:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.38
[32m[20221213 23:23:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.36
[32m[20221213 23:23:23 @agent_ppo2.py:143][0m Total time:      10.84 min
[32m[20221213 23:23:23 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221213 23:23:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2512 --------------------------#
[32m[20221213 23:23:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:23 @agent_ppo2.py:185][0m |           0.0016 |          45.2373 |          14.4838 |
[32m[20221213 23:23:23 @agent_ppo2.py:185][0m |          -0.0067 |          41.6162 |          14.3945 |
[32m[20221213 23:23:23 @agent_ppo2.py:185][0m |          -0.0059 |          40.4589 |          14.3963 |
[32m[20221213 23:23:23 @agent_ppo2.py:185][0m |          -0.0070 |          39.6833 |          14.3983 |
[32m[20221213 23:23:23 @agent_ppo2.py:185][0m |          -0.0094 |          39.0877 |          14.3738 |
[32m[20221213 23:23:23 @agent_ppo2.py:185][0m |          -0.0103 |          38.5582 |          14.3613 |
[32m[20221213 23:23:23 @agent_ppo2.py:185][0m |          -0.0092 |          38.1036 |          14.3814 |
[32m[20221213 23:23:24 @agent_ppo2.py:185][0m |          -0.0088 |          37.8040 |          14.3555 |
[32m[20221213 23:23:24 @agent_ppo2.py:185][0m |          -0.0124 |          37.5752 |          14.3435 |
[32m[20221213 23:23:24 @agent_ppo2.py:185][0m |          -0.0118 |          37.2419 |          14.3545 |
[32m[20221213 23:23:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.81
[32m[20221213 23:23:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.93
[32m[20221213 23:23:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.91
[32m[20221213 23:23:24 @agent_ppo2.py:143][0m Total time:      10.86 min
[32m[20221213 23:23:24 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221213 23:23:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2513 --------------------------#
[32m[20221213 23:23:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:24 @agent_ppo2.py:185][0m |           0.0002 |          31.5857 |          14.1723 |
[32m[20221213 23:23:24 @agent_ppo2.py:185][0m |          -0.0024 |          24.6527 |          14.1919 |
[32m[20221213 23:23:24 @agent_ppo2.py:185][0m |          -0.0056 |          23.0615 |          14.1477 |
[32m[20221213 23:23:24 @agent_ppo2.py:185][0m |          -0.0133 |          22.1157 |          14.1559 |
[32m[20221213 23:23:25 @agent_ppo2.py:185][0m |          -0.0117 |          21.5033 |          14.1630 |
[32m[20221213 23:23:25 @agent_ppo2.py:185][0m |          -0.0127 |          21.3555 |          14.1456 |
[32m[20221213 23:23:25 @agent_ppo2.py:185][0m |          -0.0179 |          20.7371 |          14.1484 |
[32m[20221213 23:23:25 @agent_ppo2.py:185][0m |          -0.0066 |          20.4736 |          14.1592 |
[32m[20221213 23:23:25 @agent_ppo2.py:185][0m |          -0.0083 |          21.5160 |          14.1598 |
[32m[20221213 23:23:25 @agent_ppo2.py:185][0m |          -0.0169 |          19.8939 |          14.1600 |
[32m[20221213 23:23:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.57
[32m[20221213 23:23:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.38
[32m[20221213 23:23:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.63
[32m[20221213 23:23:25 @agent_ppo2.py:143][0m Total time:      10.89 min
[32m[20221213 23:23:25 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221213 23:23:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2514 --------------------------#
[32m[20221213 23:23:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:25 @agent_ppo2.py:185][0m |          -0.0009 |          43.4595 |          14.3023 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0083 |          39.6834 |          14.2910 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0073 |          38.0974 |          14.3015 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0109 |          36.8727 |          14.2887 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0119 |          36.4992 |          14.3041 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0111 |          35.4351 |          14.3141 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0144 |          35.0754 |          14.3330 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0121 |          34.8161 |          14.3210 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0136 |          34.1858 |          14.3151 |
[32m[20221213 23:23:26 @agent_ppo2.py:185][0m |          -0.0163 |          33.7601 |          14.3251 |
[32m[20221213 23:23:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.42
[32m[20221213 23:23:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.15
[32m[20221213 23:23:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.30
[32m[20221213 23:23:26 @agent_ppo2.py:143][0m Total time:      10.91 min
[32m[20221213 23:23:26 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221213 23:23:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2515 --------------------------#
[32m[20221213 23:23:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |           0.0032 |          51.7520 |          14.1648 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0092 |          47.3859 |          14.1390 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0043 |          46.7866 |          14.1739 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0082 |          45.4303 |          14.1602 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0099 |          44.9768 |          14.1390 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0116 |          44.5036 |          14.1378 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0111 |          44.2965 |          14.1360 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0129 |          43.8038 |          14.1111 |
[32m[20221213 23:23:27 @agent_ppo2.py:185][0m |          -0.0157 |          43.5879 |          14.1313 |
[32m[20221213 23:23:28 @agent_ppo2.py:185][0m |          -0.0215 |          43.7969 |          14.1259 |
[32m[20221213 23:23:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:23:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.82
[32m[20221213 23:23:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.70
[32m[20221213 23:23:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.40
[32m[20221213 23:23:28 @agent_ppo2.py:143][0m Total time:      10.93 min
[32m[20221213 23:23:28 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221213 23:23:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2516 --------------------------#
[32m[20221213 23:23:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:28 @agent_ppo2.py:185][0m |          -0.0003 |          34.7161 |          14.6093 |
[32m[20221213 23:23:28 @agent_ppo2.py:185][0m |          -0.0060 |          31.9440 |          14.6056 |
[32m[20221213 23:23:28 @agent_ppo2.py:185][0m |          -0.0034 |          31.1444 |          14.6039 |
[32m[20221213 23:23:28 @agent_ppo2.py:185][0m |          -0.0063 |          30.6312 |          14.6452 |
[32m[20221213 23:23:28 @agent_ppo2.py:185][0m |          -0.0042 |          30.1928 |          14.6621 |
[32m[20221213 23:23:28 @agent_ppo2.py:185][0m |          -0.0031 |          30.0089 |          14.6902 |
[32m[20221213 23:23:29 @agent_ppo2.py:185][0m |          -0.0099 |          29.8164 |          14.6628 |
[32m[20221213 23:23:29 @agent_ppo2.py:185][0m |          -0.0091 |          29.8511 |          14.6901 |
[32m[20221213 23:23:29 @agent_ppo2.py:185][0m |          -0.0102 |          29.6286 |          14.7197 |
[32m[20221213 23:23:29 @agent_ppo2.py:185][0m |          -0.0111 |          29.4528 |          14.7390 |
[32m[20221213 23:23:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:23:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.91
[32m[20221213 23:23:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.17
[32m[20221213 23:23:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.05
[32m[20221213 23:23:29 @agent_ppo2.py:143][0m Total time:      10.95 min
[32m[20221213 23:23:29 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221213 23:23:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2517 --------------------------#
[32m[20221213 23:23:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:29 @agent_ppo2.py:185][0m |          -0.0018 |          57.5563 |          14.5989 |
[32m[20221213 23:23:29 @agent_ppo2.py:185][0m |          -0.0057 |          55.0385 |          14.5868 |
[32m[20221213 23:23:29 @agent_ppo2.py:185][0m |          -0.0103 |          53.9137 |          14.5878 |
[32m[20221213 23:23:30 @agent_ppo2.py:185][0m |          -0.0127 |          53.5779 |          14.5807 |
[32m[20221213 23:23:30 @agent_ppo2.py:185][0m |          -0.0133 |          53.2158 |          14.5666 |
[32m[20221213 23:23:30 @agent_ppo2.py:185][0m |          -0.0110 |          52.9998 |          14.5721 |
[32m[20221213 23:23:30 @agent_ppo2.py:185][0m |          -0.0140 |          52.6732 |          14.5627 |
[32m[20221213 23:23:30 @agent_ppo2.py:185][0m |          -0.0156 |          52.7315 |          14.5650 |
[32m[20221213 23:23:30 @agent_ppo2.py:185][0m |          -0.0148 |          52.4609 |          14.5537 |
[32m[20221213 23:23:30 @agent_ppo2.py:185][0m |          -0.0084 |          53.2721 |          14.5725 |
[32m[20221213 23:23:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.36
[32m[20221213 23:23:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.96
[32m[20221213 23:23:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.45
[32m[20221213 23:23:30 @agent_ppo2.py:143][0m Total time:      10.97 min
[32m[20221213 23:23:30 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221213 23:23:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2518 --------------------------#
[32m[20221213 23:23:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |           0.0002 |          40.8224 |          14.6262 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0021 |          35.2110 |          14.6600 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0040 |          34.1659 |          14.6381 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0068 |          33.5351 |          14.5944 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0064 |          33.1325 |          14.6108 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |           0.0093 |          39.3067 |          14.6145 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0105 |          32.2741 |          14.6460 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0101 |          31.9744 |          14.6365 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0074 |          31.8389 |          14.6289 |
[32m[20221213 23:23:31 @agent_ppo2.py:185][0m |          -0.0085 |          31.6410 |          14.6285 |
[32m[20221213 23:23:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.80
[32m[20221213 23:23:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.64
[32m[20221213 23:23:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.00
[32m[20221213 23:23:31 @agent_ppo2.py:143][0m Total time:      10.99 min
[32m[20221213 23:23:31 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221213 23:23:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2519 --------------------------#
[32m[20221213 23:23:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0013 |          48.5464 |          14.5089 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0054 |          46.9908 |          14.5041 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0110 |          46.3548 |          14.5074 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0120 |          45.6802 |          14.5098 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0111 |          45.2055 |          14.5353 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0131 |          44.9032 |          14.5527 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0104 |          44.9084 |          14.5608 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0127 |          44.4114 |          14.5511 |
[32m[20221213 23:23:32 @agent_ppo2.py:185][0m |          -0.0161 |          44.2493 |          14.5593 |
[32m[20221213 23:23:33 @agent_ppo2.py:185][0m |          -0.0156 |          43.9463 |          14.5586 |
[32m[20221213 23:23:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.72
[32m[20221213 23:23:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.86
[32m[20221213 23:23:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.62
[32m[20221213 23:23:33 @agent_ppo2.py:143][0m Total time:      11.01 min
[32m[20221213 23:23:33 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221213 23:23:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2520 --------------------------#
[32m[20221213 23:23:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:23:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:33 @agent_ppo2.py:185][0m |          -0.0006 |          50.4546 |          14.4440 |
[32m[20221213 23:23:33 @agent_ppo2.py:185][0m |          -0.0024 |          47.5643 |          14.4332 |
[32m[20221213 23:23:33 @agent_ppo2.py:185][0m |           0.0048 |          50.8197 |          14.4388 |
[32m[20221213 23:23:33 @agent_ppo2.py:185][0m |          -0.0053 |          46.3711 |          14.4400 |
[32m[20221213 23:23:33 @agent_ppo2.py:185][0m |          -0.0107 |          45.8645 |          14.4073 |
[32m[20221213 23:23:33 @agent_ppo2.py:185][0m |          -0.0070 |          45.3579 |          14.4128 |
[32m[20221213 23:23:34 @agent_ppo2.py:185][0m |          -0.0094 |          44.8685 |          14.4003 |
[32m[20221213 23:23:34 @agent_ppo2.py:185][0m |          -0.0059 |          46.2020 |          14.3877 |
[32m[20221213 23:23:34 @agent_ppo2.py:185][0m |          -0.0049 |          45.3654 |          14.3857 |
[32m[20221213 23:23:34 @agent_ppo2.py:185][0m |          -0.0144 |          44.5332 |          14.3884 |
[32m[20221213 23:23:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:23:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.45
[32m[20221213 23:23:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.36
[32m[20221213 23:23:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.88
[32m[20221213 23:23:34 @agent_ppo2.py:143][0m Total time:      11.03 min
[32m[20221213 23:23:34 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221213 23:23:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2521 --------------------------#
[32m[20221213 23:23:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:34 @agent_ppo2.py:185][0m |           0.0018 |          55.0410 |          14.3496 |
[32m[20221213 23:23:34 @agent_ppo2.py:185][0m |          -0.0048 |          50.9225 |          14.3677 |
[32m[20221213 23:23:34 @agent_ppo2.py:185][0m |           0.0019 |          54.0875 |          14.3448 |
[32m[20221213 23:23:35 @agent_ppo2.py:185][0m |          -0.0086 |          49.3476 |          14.3697 |
[32m[20221213 23:23:35 @agent_ppo2.py:185][0m |          -0.0099 |          48.7923 |          14.3609 |
[32m[20221213 23:23:35 @agent_ppo2.py:185][0m |          -0.0074 |          48.6193 |          14.3356 |
[32m[20221213 23:23:35 @agent_ppo2.py:185][0m |          -0.0091 |          48.1651 |          14.3615 |
[32m[20221213 23:23:35 @agent_ppo2.py:185][0m |          -0.0118 |          48.0334 |          14.3557 |
[32m[20221213 23:23:35 @agent_ppo2.py:185][0m |          -0.0092 |          48.2788 |          14.3708 |
[32m[20221213 23:23:35 @agent_ppo2.py:185][0m |          -0.0117 |          47.6772 |          14.3754 |
[32m[20221213 23:23:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.40
[32m[20221213 23:23:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.81
[32m[20221213 23:23:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.81
[32m[20221213 23:23:35 @agent_ppo2.py:143][0m Total time:      11.05 min
[32m[20221213 23:23:35 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221213 23:23:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2522 --------------------------#
[32m[20221213 23:23:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0018 |          57.7065 |          14.7479 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0038 |          56.4751 |          14.7490 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0111 |          55.5644 |          14.7263 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0110 |          55.0574 |          14.7361 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |           0.0066 |          61.9091 |          14.7395 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0038 |          57.8243 |          14.7426 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0069 |          54.3633 |          14.7484 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0082 |          55.4171 |          14.7390 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0104 |          53.6877 |          14.7627 |
[32m[20221213 23:23:36 @agent_ppo2.py:185][0m |          -0.0131 |          53.2862 |          14.7587 |
[32m[20221213 23:23:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.43
[32m[20221213 23:23:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.97
[32m[20221213 23:23:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.63
[32m[20221213 23:23:36 @agent_ppo2.py:143][0m Total time:      11.07 min
[32m[20221213 23:23:36 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221213 23:23:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2523 --------------------------#
[32m[20221213 23:23:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0003 |          57.2909 |          14.3573 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0043 |          53.0484 |          14.3299 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0069 |          50.3031 |          14.2982 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0071 |          49.0585 |          14.2904 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0075 |          48.5362 |          14.3054 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0121 |          48.1539 |          14.2808 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0128 |          47.7201 |          14.2967 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0013 |          54.5330 |          14.2745 |
[32m[20221213 23:23:37 @agent_ppo2.py:185][0m |          -0.0016 |          49.3938 |          14.2818 |
[32m[20221213 23:23:38 @agent_ppo2.py:185][0m |          -0.0138 |          46.6889 |          14.2943 |
[32m[20221213 23:23:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.20
[32m[20221213 23:23:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.12
[32m[20221213 23:23:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.76
[32m[20221213 23:23:38 @agent_ppo2.py:143][0m Total time:      11.09 min
[32m[20221213 23:23:38 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221213 23:23:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2524 --------------------------#
[32m[20221213 23:23:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:38 @agent_ppo2.py:185][0m |           0.0004 |          61.5621 |          14.3027 |
[32m[20221213 23:23:38 @agent_ppo2.py:185][0m |          -0.0061 |          60.5627 |          14.3020 |
[32m[20221213 23:23:38 @agent_ppo2.py:185][0m |          -0.0097 |          60.3061 |          14.3117 |
[32m[20221213 23:23:38 @agent_ppo2.py:185][0m |          -0.0062 |          60.3660 |          14.3128 |
[32m[20221213 23:23:38 @agent_ppo2.py:185][0m |          -0.0110 |          59.9288 |          14.3226 |
[32m[20221213 23:23:38 @agent_ppo2.py:185][0m |          -0.0120 |          59.9209 |          14.3327 |
[32m[20221213 23:23:39 @agent_ppo2.py:185][0m |          -0.0135 |          59.8059 |          14.3637 |
[32m[20221213 23:23:39 @agent_ppo2.py:185][0m |          -0.0126 |          59.7034 |          14.3571 |
[32m[20221213 23:23:39 @agent_ppo2.py:185][0m |          -0.0099 |          59.7107 |          14.3686 |
[32m[20221213 23:23:39 @agent_ppo2.py:185][0m |          -0.0152 |          59.5671 |          14.3697 |
[32m[20221213 23:23:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.97
[32m[20221213 23:23:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.65
[32m[20221213 23:23:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.49
[32m[20221213 23:23:39 @agent_ppo2.py:143][0m Total time:      11.12 min
[32m[20221213 23:23:39 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221213 23:23:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2525 --------------------------#
[32m[20221213 23:23:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:39 @agent_ppo2.py:185][0m |           0.0011 |          33.7262 |          14.6102 |
[32m[20221213 23:23:39 @agent_ppo2.py:185][0m |          -0.0018 |          27.9377 |          14.5787 |
[32m[20221213 23:23:39 @agent_ppo2.py:185][0m |          -0.0082 |          26.7570 |          14.5716 |
[32m[20221213 23:23:40 @agent_ppo2.py:185][0m |          -0.0049 |          26.1887 |          14.5657 |
[32m[20221213 23:23:40 @agent_ppo2.py:185][0m |          -0.0128 |          25.7754 |          14.5557 |
[32m[20221213 23:23:40 @agent_ppo2.py:185][0m |          -0.0079 |          25.4457 |          14.5666 |
[32m[20221213 23:23:40 @agent_ppo2.py:185][0m |          -0.0060 |          25.2708 |          14.5530 |
[32m[20221213 23:23:40 @agent_ppo2.py:185][0m |          -0.0110 |          25.1195 |          14.5501 |
[32m[20221213 23:23:40 @agent_ppo2.py:185][0m |          -0.0089 |          24.9785 |          14.5513 |
[32m[20221213 23:23:40 @agent_ppo2.py:185][0m |          -0.0082 |          25.0277 |          14.5537 |
[32m[20221213 23:23:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:23:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.93
[32m[20221213 23:23:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.32
[32m[20221213 23:23:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.39
[32m[20221213 23:23:40 @agent_ppo2.py:143][0m Total time:      11.14 min
[32m[20221213 23:23:40 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221213 23:23:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2526 --------------------------#
[32m[20221213 23:23:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |           0.0009 |          50.1825 |          14.7844 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0026 |          47.2667 |          14.7573 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0025 |          46.1416 |          14.7715 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0051 |          45.5184 |          14.7287 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0059 |          45.0083 |          14.7270 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0091 |          44.6311 |          14.7183 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0092 |          44.3424 |          14.7280 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0081 |          44.0420 |          14.6870 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0016 |          44.6545 |          14.6971 |
[32m[20221213 23:23:41 @agent_ppo2.py:185][0m |          -0.0092 |          43.6044 |          14.6875 |
[32m[20221213 23:23:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.47
[32m[20221213 23:23:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.58
[32m[20221213 23:23:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.81
[32m[20221213 23:23:41 @agent_ppo2.py:143][0m Total time:      11.16 min
[32m[20221213 23:23:41 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221213 23:23:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2527 --------------------------#
[32m[20221213 23:23:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |           0.0077 |          60.7264 |          14.4472 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0044 |          57.0561 |          14.4393 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0009 |          58.3747 |          14.4326 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0036 |          56.5963 |          14.4835 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0072 |          56.0114 |          14.4603 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0099 |          55.7829 |          14.4574 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0115 |          55.6222 |          14.4835 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0091 |          55.4720 |          14.4601 |
[32m[20221213 23:23:42 @agent_ppo2.py:185][0m |          -0.0107 |          55.4240 |          14.4732 |
[32m[20221213 23:23:43 @agent_ppo2.py:185][0m |          -0.0118 |          55.2075 |          14.4990 |
[32m[20221213 23:23:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.63
[32m[20221213 23:23:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.64
[32m[20221213 23:23:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.38
[32m[20221213 23:23:43 @agent_ppo2.py:143][0m Total time:      11.18 min
[32m[20221213 23:23:43 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221213 23:23:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2528 --------------------------#
[32m[20221213 23:23:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:43 @agent_ppo2.py:185][0m |          -0.0006 |          53.9697 |          14.4104 |
[32m[20221213 23:23:43 @agent_ppo2.py:185][0m |          -0.0064 |          49.6855 |          14.4185 |
[32m[20221213 23:23:43 @agent_ppo2.py:185][0m |          -0.0012 |          49.6749 |          14.4295 |
[32m[20221213 23:23:43 @agent_ppo2.py:185][0m |          -0.0106 |          48.2574 |          14.4340 |
[32m[20221213 23:23:43 @agent_ppo2.py:185][0m |          -0.0113 |          47.9020 |          14.4404 |
[32m[20221213 23:23:43 @agent_ppo2.py:185][0m |          -0.0130 |          47.7183 |          14.4545 |
[32m[20221213 23:23:44 @agent_ppo2.py:185][0m |          -0.0142 |          47.5183 |          14.4610 |
[32m[20221213 23:23:44 @agent_ppo2.py:185][0m |          -0.0113 |          47.4386 |          14.4462 |
[32m[20221213 23:23:44 @agent_ppo2.py:185][0m |          -0.0141 |          47.2449 |          14.4769 |
[32m[20221213 23:23:44 @agent_ppo2.py:185][0m |          -0.0105 |          47.3241 |          14.4757 |
[32m[20221213 23:23:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.68
[32m[20221213 23:23:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.09
[32m[20221213 23:23:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.07
[32m[20221213 23:23:44 @agent_ppo2.py:143][0m Total time:      11.20 min
[32m[20221213 23:23:44 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221213 23:23:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2529 --------------------------#
[32m[20221213 23:23:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:44 @agent_ppo2.py:185][0m |           0.0008 |          34.2129 |          14.5334 |
[32m[20221213 23:23:44 @agent_ppo2.py:185][0m |          -0.0017 |          28.0057 |          14.4933 |
[32m[20221213 23:23:44 @agent_ppo2.py:185][0m |          -0.0066 |          27.5601 |          14.4696 |
[32m[20221213 23:23:45 @agent_ppo2.py:185][0m |          -0.0043 |          26.2196 |          14.4691 |
[32m[20221213 23:23:45 @agent_ppo2.py:185][0m |          -0.0094 |          25.7851 |          14.4605 |
[32m[20221213 23:23:45 @agent_ppo2.py:185][0m |          -0.0141 |          25.4155 |          14.4514 |
[32m[20221213 23:23:45 @agent_ppo2.py:185][0m |          -0.0139 |          25.3233 |          14.4346 |
[32m[20221213 23:23:45 @agent_ppo2.py:185][0m |          -0.0107 |          24.9736 |          14.4247 |
[32m[20221213 23:23:45 @agent_ppo2.py:185][0m |          -0.0127 |          24.8387 |          14.4153 |
[32m[20221213 23:23:45 @agent_ppo2.py:185][0m |          -0.0163 |          24.7901 |          14.4051 |
[32m[20221213 23:23:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:23:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.58
[32m[20221213 23:23:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.52
[32m[20221213 23:23:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.41
[32m[20221213 23:23:45 @agent_ppo2.py:143][0m Total time:      11.22 min
[32m[20221213 23:23:45 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221213 23:23:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2530 --------------------------#
[32m[20221213 23:23:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:23:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |           0.0017 |          58.1581 |          14.4250 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0062 |          56.7620 |          14.4146 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0088 |          56.5231 |          14.4206 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0065 |          56.3714 |          14.3970 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0068 |          56.1760 |          14.3934 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0050 |          56.1891 |          14.4116 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0083 |          55.9865 |          14.3819 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |           0.0028 |          62.3413 |          14.3704 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0093 |          55.9908 |          14.3716 |
[32m[20221213 23:23:46 @agent_ppo2.py:185][0m |          -0.0082 |          55.7838 |          14.3639 |
[32m[20221213 23:23:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:23:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.90
[32m[20221213 23:23:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.45
[32m[20221213 23:23:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.23
[32m[20221213 23:23:47 @agent_ppo2.py:143][0m Total time:      11.24 min
[32m[20221213 23:23:47 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221213 23:23:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2531 --------------------------#
[32m[20221213 23:23:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:23:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |           0.0006 |          62.9044 |          14.4137 |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |           0.0070 |          63.9026 |          14.3914 |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |          -0.0038 |          61.1808 |          14.3892 |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |          -0.0052 |          60.8777 |          14.3877 |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |          -0.0047 |          60.5351 |          14.3804 |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |          -0.0075 |          60.3269 |          14.4018 |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |          -0.0049 |          60.0236 |          14.3681 |
[32m[20221213 23:23:47 @agent_ppo2.py:185][0m |          -0.0025 |          60.7442 |          14.3863 |
[32m[20221213 23:23:48 @agent_ppo2.py:185][0m |          -0.0076 |          59.8515 |          14.3861 |
[32m[20221213 23:23:48 @agent_ppo2.py:185][0m |          -0.0024 |          60.8962 |          14.3790 |
[32m[20221213 23:23:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.84
[32m[20221213 23:23:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.92
[32m[20221213 23:23:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.14
[32m[20221213 23:23:48 @agent_ppo2.py:143][0m Total time:      11.26 min
[32m[20221213 23:23:48 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221213 23:23:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2532 --------------------------#
[32m[20221213 23:23:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:48 @agent_ppo2.py:185][0m |           0.0102 |          56.8869 |          14.2824 |
[32m[20221213 23:23:48 @agent_ppo2.py:185][0m |          -0.0043 |          50.5039 |          14.2774 |
[32m[20221213 23:23:48 @agent_ppo2.py:185][0m |          -0.0039 |          49.8609 |          14.2764 |
[32m[20221213 23:23:48 @agent_ppo2.py:185][0m |          -0.0096 |          49.5867 |          14.2977 |
[32m[20221213 23:23:48 @agent_ppo2.py:185][0m |          -0.0028 |          50.9430 |          14.2715 |
[32m[20221213 23:23:49 @agent_ppo2.py:185][0m |          -0.0114 |          49.2093 |          14.2664 |
[32m[20221213 23:23:49 @agent_ppo2.py:185][0m |          -0.0117 |          49.1049 |          14.2541 |
[32m[20221213 23:23:49 @agent_ppo2.py:185][0m |          -0.0059 |          49.2414 |          14.2461 |
[32m[20221213 23:23:49 @agent_ppo2.py:185][0m |          -0.0077 |          49.2518 |          14.2557 |
[32m[20221213 23:23:49 @agent_ppo2.py:185][0m |          -0.0105 |          48.8220 |          14.2458 |
[32m[20221213 23:23:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.28
[32m[20221213 23:23:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.93
[32m[20221213 23:23:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.65
[32m[20221213 23:23:49 @agent_ppo2.py:143][0m Total time:      11.28 min
[32m[20221213 23:23:49 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221213 23:23:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2533 --------------------------#
[32m[20221213 23:23:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:49 @agent_ppo2.py:185][0m |           0.0012 |          55.4088 |          14.2381 |
[32m[20221213 23:23:49 @agent_ppo2.py:185][0m |          -0.0037 |          54.6159 |          14.1778 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |          -0.0046 |          54.3959 |          14.2139 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |           0.0027 |          56.9043 |          14.1905 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |          -0.0068 |          54.1565 |          14.2370 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |          -0.0001 |          56.5500 |          14.2264 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |          -0.0040 |          54.0677 |          14.2348 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |          -0.0090 |          53.6085 |          14.2395 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |          -0.0092 |          53.5207 |          14.2496 |
[32m[20221213 23:23:50 @agent_ppo2.py:185][0m |          -0.0101 |          53.5813 |          14.2378 |
[32m[20221213 23:23:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:23:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.78
[32m[20221213 23:23:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.25
[32m[20221213 23:23:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.65
[32m[20221213 23:23:50 @agent_ppo2.py:143][0m Total time:      11.30 min
[32m[20221213 23:23:50 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221213 23:23:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2534 --------------------------#
[32m[20221213 23:23:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |           0.0030 |          36.7644 |          14.3790 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0013 |          33.6027 |          14.3603 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0089 |          32.7705 |          14.3821 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0093 |          33.0507 |          14.3675 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0146 |          31.9535 |          14.3637 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0103 |          31.4727 |          14.3862 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0109 |          31.3830 |          14.3959 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0146 |          31.1031 |          14.3894 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0136 |          31.0009 |          14.3766 |
[32m[20221213 23:23:51 @agent_ppo2.py:185][0m |          -0.0128 |          30.8440 |          14.3820 |
[32m[20221213 23:23:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:23:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.74
[32m[20221213 23:23:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.34
[32m[20221213 23:23:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.22
[32m[20221213 23:23:52 @agent_ppo2.py:143][0m Total time:      11.33 min
[32m[20221213 23:23:52 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221213 23:23:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2535 --------------------------#
[32m[20221213 23:23:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |           0.0024 |          42.8397 |          14.3328 |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |          -0.0046 |          40.8716 |          14.3065 |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |           0.0089 |          45.5547 |          14.2943 |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |          -0.0035 |          39.9825 |          14.2382 |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |          -0.0071 |          39.8207 |          14.2838 |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |          -0.0054 |          39.5351 |          14.2667 |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |          -0.0038 |          40.1593 |          14.2562 |
[32m[20221213 23:23:52 @agent_ppo2.py:185][0m |          -0.0083 |          39.2602 |          14.2264 |
[32m[20221213 23:23:53 @agent_ppo2.py:185][0m |          -0.0100 |          39.2069 |          14.2106 |
[32m[20221213 23:23:53 @agent_ppo2.py:185][0m |          -0.0035 |          39.4914 |          14.1730 |
[32m[20221213 23:23:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.14
[32m[20221213 23:23:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.85
[32m[20221213 23:23:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.47
[32m[20221213 23:23:53 @agent_ppo2.py:143][0m Total time:      11.35 min
[32m[20221213 23:23:53 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221213 23:23:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2536 --------------------------#
[32m[20221213 23:23:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:53 @agent_ppo2.py:185][0m |          -0.0008 |          31.9572 |          14.3116 |
[32m[20221213 23:23:53 @agent_ppo2.py:185][0m |           0.0001 |          28.7451 |          14.3030 |
[32m[20221213 23:23:53 @agent_ppo2.py:185][0m |          -0.0029 |          27.2461 |          14.3037 |
[32m[20221213 23:23:53 @agent_ppo2.py:185][0m |          -0.0058 |          26.7127 |          14.2745 |
[32m[20221213 23:23:53 @agent_ppo2.py:185][0m |          -0.0125 |          26.0561 |          14.2861 |
[32m[20221213 23:23:54 @agent_ppo2.py:185][0m |          -0.0064 |          26.1615 |          14.2787 |
[32m[20221213 23:23:54 @agent_ppo2.py:185][0m |          -0.0154 |          25.5249 |          14.2878 |
[32m[20221213 23:23:54 @agent_ppo2.py:185][0m |          -0.0185 |          25.1760 |          14.2701 |
[32m[20221213 23:23:54 @agent_ppo2.py:185][0m |          -0.0171 |          24.9473 |          14.2751 |
[32m[20221213 23:23:54 @agent_ppo2.py:185][0m |          -0.0199 |          24.8464 |          14.2633 |
[32m[20221213 23:23:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.75
[32m[20221213 23:23:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.52
[32m[20221213 23:23:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.26
[32m[20221213 23:23:54 @agent_ppo2.py:143][0m Total time:      11.37 min
[32m[20221213 23:23:54 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221213 23:23:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2537 --------------------------#
[32m[20221213 23:23:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:54 @agent_ppo2.py:185][0m |          -0.0013 |          54.7206 |          14.1935 |
[32m[20221213 23:23:54 @agent_ppo2.py:185][0m |          -0.0022 |          52.2121 |          14.2137 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0034 |          50.8355 |          14.1921 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0052 |          50.3444 |          14.1607 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0065 |          50.0458 |          14.1572 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0067 |          49.8652 |          14.1660 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0076 |          49.5313 |          14.1526 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0028 |          51.4187 |          14.1318 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0102 |          49.3450 |          14.1464 |
[32m[20221213 23:23:55 @agent_ppo2.py:185][0m |          -0.0071 |          49.4049 |          14.1435 |
[32m[20221213 23:23:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.76
[32m[20221213 23:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.06
[32m[20221213 23:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.41
[32m[20221213 23:23:55 @agent_ppo2.py:143][0m Total time:      11.39 min
[32m[20221213 23:23:55 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221213 23:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2538 --------------------------#
[32m[20221213 23:23:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |           0.0046 |          30.0762 |          14.1346 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0062 |          26.5488 |          14.1182 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0080 |          25.7138 |          14.1375 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0054 |          25.3603 |          14.1362 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0102 |          24.9218 |          14.1335 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0093 |          24.4440 |          14.1072 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0010 |          28.9559 |          14.1113 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0117 |          24.3759 |          14.1136 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0118 |          23.8086 |          14.1083 |
[32m[20221213 23:23:56 @agent_ppo2.py:185][0m |          -0.0190 |          23.5860 |          14.0903 |
[32m[20221213 23:23:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:23:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.33
[32m[20221213 23:23:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.71
[32m[20221213 23:23:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.54
[32m[20221213 23:23:57 @agent_ppo2.py:143][0m Total time:      11.41 min
[32m[20221213 23:23:57 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221213 23:23:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2539 --------------------------#
[32m[20221213 23:23:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0030 |          46.3396 |          14.2059 |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0006 |          44.5902 |          14.2480 |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0056 |          43.4186 |          14.2027 |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0080 |          42.8244 |          14.2285 |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0092 |          42.0400 |          14.1962 |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0097 |          40.9342 |          14.2159 |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0104 |          40.3909 |          14.2151 |
[32m[20221213 23:23:57 @agent_ppo2.py:185][0m |          -0.0145 |          40.0493 |          14.2306 |
[32m[20221213 23:23:58 @agent_ppo2.py:185][0m |          -0.0086 |          39.6794 |          14.2561 |
[32m[20221213 23:23:58 @agent_ppo2.py:185][0m |          -0.0100 |          39.4634 |          14.2441 |
[32m[20221213 23:23:58 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:23:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.18
[32m[20221213 23:23:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.66
[32m[20221213 23:23:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.02
[32m[20221213 23:23:58 @agent_ppo2.py:143][0m Total time:      11.43 min
[32m[20221213 23:23:58 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221213 23:23:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2540 --------------------------#
[32m[20221213 23:23:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:23:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:58 @agent_ppo2.py:185][0m |           0.0054 |          48.2919 |          14.2709 |
[32m[20221213 23:23:58 @agent_ppo2.py:185][0m |          -0.0020 |          45.7794 |          14.2712 |
[32m[20221213 23:23:58 @agent_ppo2.py:185][0m |          -0.0057 |          45.0207 |          14.2487 |
[32m[20221213 23:23:58 @agent_ppo2.py:185][0m |          -0.0075 |          44.6195 |          14.2527 |
[32m[20221213 23:23:59 @agent_ppo2.py:185][0m |          -0.0067 |          44.4814 |          14.2441 |
[32m[20221213 23:23:59 @agent_ppo2.py:185][0m |          -0.0075 |          44.1533 |          14.2601 |
[32m[20221213 23:23:59 @agent_ppo2.py:185][0m |          -0.0086 |          44.1922 |          14.2531 |
[32m[20221213 23:23:59 @agent_ppo2.py:185][0m |           0.0113 |          50.8605 |          14.2667 |
[32m[20221213 23:23:59 @agent_ppo2.py:185][0m |          -0.0089 |          44.2148 |          14.2598 |
[32m[20221213 23:23:59 @agent_ppo2.py:185][0m |          -0.0110 |          43.7087 |          14.2710 |
[32m[20221213 23:23:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.17
[32m[20221213 23:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.99
[32m[20221213 23:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.40
[32m[20221213 23:23:59 @agent_ppo2.py:143][0m Total time:      11.45 min
[32m[20221213 23:23:59 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221213 23:23:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2541 --------------------------#
[32m[20221213 23:23:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:23:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:23:59 @agent_ppo2.py:185][0m |           0.0009 |          56.6278 |          14.3707 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0030 |          55.6295 |          14.3640 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0027 |          55.3075 |          14.3811 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |           0.0015 |          56.7284 |          14.3809 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0066 |          55.1005 |          14.3755 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0076 |          54.8668 |          14.4012 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0094 |          54.8972 |          14.4002 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0093 |          54.8253 |          14.4104 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0078 |          55.0662 |          14.4061 |
[32m[20221213 23:24:00 @agent_ppo2.py:185][0m |          -0.0112 |          54.6789 |          14.4199 |
[32m[20221213 23:24:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:24:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.42
[32m[20221213 23:24:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.28
[32m[20221213 23:24:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.09
[32m[20221213 23:24:00 @agent_ppo2.py:143][0m Total time:      11.47 min
[32m[20221213 23:24:00 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221213 23:24:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2542 --------------------------#
[32m[20221213 23:24:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |           0.0025 |          58.9067 |          14.1792 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0043 |          57.7034 |          14.1627 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0082 |          57.5613 |          14.1590 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0073 |          57.4565 |          14.1872 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0131 |          57.3970 |          14.1727 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0024 |          59.7521 |          14.1739 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0083 |          57.0296 |          14.1816 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0144 |          57.0440 |          14.1837 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0122 |          56.9981 |          14.1738 |
[32m[20221213 23:24:01 @agent_ppo2.py:185][0m |          -0.0079 |          57.1852 |          14.1788 |
[32m[20221213 23:24:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:24:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.66
[32m[20221213 23:24:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.05
[32m[20221213 23:24:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.13
[32m[20221213 23:24:02 @agent_ppo2.py:143][0m Total time:      11.49 min
[32m[20221213 23:24:02 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221213 23:24:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2543 --------------------------#
[32m[20221213 23:24:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:02 @agent_ppo2.py:185][0m |          -0.0024 |          44.2444 |          14.1410 |
[32m[20221213 23:24:02 @agent_ppo2.py:185][0m |          -0.0031 |          38.7375 |          14.1414 |
[32m[20221213 23:24:02 @agent_ppo2.py:185][0m |          -0.0081 |          37.5206 |          14.1454 |
[32m[20221213 23:24:02 @agent_ppo2.py:185][0m |          -0.0073 |          36.9180 |          14.1407 |
[32m[20221213 23:24:02 @agent_ppo2.py:185][0m |          -0.0119 |          36.6932 |          14.1233 |
[32m[20221213 23:24:02 @agent_ppo2.py:185][0m |          -0.0068 |          37.2567 |          14.1445 |
[32m[20221213 23:24:02 @agent_ppo2.py:185][0m |          -0.0032 |          38.1307 |          14.1706 |
[32m[20221213 23:24:03 @agent_ppo2.py:185][0m |          -0.0122 |          35.9353 |          14.1548 |
[32m[20221213 23:24:03 @agent_ppo2.py:185][0m |          -0.0153 |          35.6883 |          14.1917 |
[32m[20221213 23:24:03 @agent_ppo2.py:185][0m |          -0.0162 |          35.4407 |          14.1880 |
[32m[20221213 23:24:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:24:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.06
[32m[20221213 23:24:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.42
[32m[20221213 23:24:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.93
[32m[20221213 23:24:03 @agent_ppo2.py:143][0m Total time:      11.52 min
[32m[20221213 23:24:03 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221213 23:24:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2544 --------------------------#
[32m[20221213 23:24:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:24:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:03 @agent_ppo2.py:185][0m |           0.0082 |          62.3405 |          14.1144 |
[32m[20221213 23:24:03 @agent_ppo2.py:185][0m |          -0.0023 |          58.4683 |          14.1595 |
[32m[20221213 23:24:03 @agent_ppo2.py:185][0m |          -0.0064 |          57.6282 |          14.1470 |
[32m[20221213 23:24:04 @agent_ppo2.py:185][0m |          -0.0050 |          57.1126 |          14.1502 |
[32m[20221213 23:24:04 @agent_ppo2.py:185][0m |           0.0012 |          58.7936 |          14.1834 |
[32m[20221213 23:24:04 @agent_ppo2.py:185][0m |          -0.0069 |          56.7623 |          14.1767 |
[32m[20221213 23:24:04 @agent_ppo2.py:185][0m |          -0.0082 |          56.7468 |          14.1952 |
[32m[20221213 23:24:04 @agent_ppo2.py:185][0m |           0.0006 |          63.0403 |          14.2302 |
[32m[20221213 23:24:04 @agent_ppo2.py:185][0m |          -0.0076 |          56.6963 |          14.1981 |
[32m[20221213 23:24:04 @agent_ppo2.py:185][0m |          -0.0003 |          58.5591 |          14.2224 |
[32m[20221213 23:24:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:24:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.60
[32m[20221213 23:24:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.98
[32m[20221213 23:24:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.80
[32m[20221213 23:24:04 @agent_ppo2.py:143][0m Total time:      11.54 min
[32m[20221213 23:24:04 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221213 23:24:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2545 --------------------------#
[32m[20221213 23:24:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |           0.0018 |          52.4964 |          14.4908 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0046 |          51.0636 |          14.5068 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0092 |          50.4043 |          14.4942 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0065 |          49.4274 |          14.5019 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0065 |          49.5119 |          14.5034 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0114 |          48.6755 |          14.5277 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0117 |          48.3688 |          14.5353 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0060 |          50.4850 |          14.5000 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0137 |          47.9219 |          14.5155 |
[32m[20221213 23:24:05 @agent_ppo2.py:185][0m |          -0.0147 |          47.7672 |          14.5083 |
[32m[20221213 23:24:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:24:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.93
[32m[20221213 23:24:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.57
[32m[20221213 23:24:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.42
[32m[20221213 23:24:06 @agent_ppo2.py:143][0m Total time:      11.56 min
[32m[20221213 23:24:06 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221213 23:24:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2546 --------------------------#
[32m[20221213 23:24:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:06 @agent_ppo2.py:185][0m |           0.0007 |          45.4510 |          14.3531 |
[32m[20221213 23:24:06 @agent_ppo2.py:185][0m |          -0.0119 |          42.3201 |          14.3243 |
[32m[20221213 23:24:06 @agent_ppo2.py:185][0m |          -0.0070 |          41.1909 |          14.3245 |
[32m[20221213 23:24:06 @agent_ppo2.py:185][0m |          -0.0053 |          40.6079 |          14.3307 |
[32m[20221213 23:24:06 @agent_ppo2.py:185][0m |          -0.0073 |          40.1325 |          14.3460 |
[32m[20221213 23:24:06 @agent_ppo2.py:185][0m |          -0.0083 |          40.0801 |          14.3417 |
[32m[20221213 23:24:06 @agent_ppo2.py:185][0m |          -0.0107 |          39.6126 |          14.3649 |
[32m[20221213 23:24:07 @agent_ppo2.py:185][0m |          -0.0155 |          39.1706 |          14.3682 |
[32m[20221213 23:24:07 @agent_ppo2.py:185][0m |          -0.0157 |          39.1785 |          14.3631 |
[32m[20221213 23:24:07 @agent_ppo2.py:185][0m |          -0.0135 |          38.8403 |          14.3815 |
[32m[20221213 23:24:07 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:24:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.55
[32m[20221213 23:24:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.48
[32m[20221213 23:24:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.50
[32m[20221213 23:24:07 @agent_ppo2.py:143][0m Total time:      11.58 min
[32m[20221213 23:24:07 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221213 23:24:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2547 --------------------------#
[32m[20221213 23:24:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:07 @agent_ppo2.py:185][0m |          -0.0029 |          30.7535 |          14.5204 |
[32m[20221213 23:24:07 @agent_ppo2.py:185][0m |          -0.0023 |          27.0104 |          14.5607 |
[32m[20221213 23:24:07 @agent_ppo2.py:185][0m |           0.0006 |          25.7960 |          14.5273 |
[32m[20221213 23:24:07 @agent_ppo2.py:185][0m |          -0.0064 |          23.5762 |          14.5382 |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |          -0.0079 |          22.8849 |          14.5312 |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |          -0.0085 |          22.5372 |          14.5413 |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |          -0.0153 |          21.9902 |          14.5119 |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |          -0.0165 |          21.5561 |          14.5321 |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |          -0.0177 |          21.1655 |          14.5261 |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |          -0.0165 |          20.9999 |          14.5324 |
[32m[20221213 23:24:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.44
[32m[20221213 23:24:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.09
[32m[20221213 23:24:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.21
[32m[20221213 23:24:08 @agent_ppo2.py:143][0m Total time:      11.60 min
[32m[20221213 23:24:08 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221213 23:24:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2548 --------------------------#
[32m[20221213 23:24:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |           0.0021 |          48.5718 |          14.3392 |
[32m[20221213 23:24:08 @agent_ppo2.py:185][0m |          -0.0049 |          45.2307 |          14.3351 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0091 |          43.8697 |          14.3101 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0086 |          43.4297 |          14.3088 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0097 |          43.2682 |          14.2826 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0092 |          43.0425 |          14.2926 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0094 |          43.0777 |          14.2725 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0109 |          42.8603 |          14.2443 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0081 |          42.8136 |          14.2763 |
[32m[20221213 23:24:09 @agent_ppo2.py:185][0m |          -0.0106 |          42.6854 |          14.2577 |
[32m[20221213 23:24:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:24:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.96
[32m[20221213 23:24:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.20
[32m[20221213 23:24:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.50
[32m[20221213 23:24:09 @agent_ppo2.py:143][0m Total time:      11.62 min
[32m[20221213 23:24:09 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221213 23:24:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2549 --------------------------#
[32m[20221213 23:24:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0041 |          62.8863 |          14.6239 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |           0.0004 |          62.2606 |          14.6059 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0075 |          59.7126 |          14.6524 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0092 |          59.2688 |          14.6689 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0124 |          59.0506 |          14.6485 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0028 |          62.0342 |          14.6730 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0084 |          61.9200 |          14.6387 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0130 |          58.5955 |          14.6654 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0141 |          58.5727 |          14.6636 |
[32m[20221213 23:24:10 @agent_ppo2.py:185][0m |          -0.0078 |          61.8729 |          14.6704 |
[32m[20221213 23:24:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.92
[32m[20221213 23:24:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.19
[32m[20221213 23:24:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.14
[32m[20221213 23:24:11 @agent_ppo2.py:143][0m Total time:      11.64 min
[32m[20221213 23:24:11 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221213 23:24:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2550 --------------------------#
[32m[20221213 23:24:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:24:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:11 @agent_ppo2.py:185][0m |           0.0167 |          40.6464 |          14.5598 |
[32m[20221213 23:24:11 @agent_ppo2.py:185][0m |          -0.0080 |          33.1435 |          14.5244 |
[32m[20221213 23:24:11 @agent_ppo2.py:185][0m |          -0.0055 |          31.6764 |          14.5141 |
[32m[20221213 23:24:11 @agent_ppo2.py:185][0m |          -0.0080 |          31.0531 |          14.4880 |
[32m[20221213 23:24:11 @agent_ppo2.py:185][0m |          -0.0139 |          30.7171 |          14.5141 |
[32m[20221213 23:24:11 @agent_ppo2.py:185][0m |          -0.0164 |          30.2062 |          14.4926 |
[32m[20221213 23:24:11 @agent_ppo2.py:185][0m |          -0.0159 |          29.6566 |          14.4906 |
[32m[20221213 23:24:12 @agent_ppo2.py:185][0m |          -0.0131 |          29.4856 |          14.5100 |
[32m[20221213 23:24:12 @agent_ppo2.py:185][0m |          -0.0154 |          29.1368 |          14.4915 |
[32m[20221213 23:24:12 @agent_ppo2.py:185][0m |          -0.0167 |          29.3249 |          14.4911 |
[32m[20221213 23:24:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.91
[32m[20221213 23:24:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.50
[32m[20221213 23:24:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.69
[32m[20221213 23:24:12 @agent_ppo2.py:143][0m Total time:      11.66 min
[32m[20221213 23:24:12 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221213 23:24:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2551 --------------------------#
[32m[20221213 23:24:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:12 @agent_ppo2.py:185][0m |          -0.0020 |          41.6235 |          14.4887 |
[32m[20221213 23:24:12 @agent_ppo2.py:185][0m |           0.0034 |          36.6921 |          14.4879 |
[32m[20221213 23:24:12 @agent_ppo2.py:185][0m |          -0.0041 |          34.9314 |          14.4779 |
[32m[20221213 23:24:12 @agent_ppo2.py:185][0m |          -0.0025 |          36.1961 |          14.4946 |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0074 |          33.5094 |          14.4955 |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0098 |          32.9877 |          14.5049 |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0135 |          32.5417 |          14.4894 |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0129 |          32.4243 |          14.5359 |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0117 |          32.1159 |          14.5095 |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0130 |          31.8780 |          14.4901 |
[32m[20221213 23:24:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.60
[32m[20221213 23:24:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.49
[32m[20221213 23:24:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.06
[32m[20221213 23:24:13 @agent_ppo2.py:143][0m Total time:      11.68 min
[32m[20221213 23:24:13 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221213 23:24:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2552 --------------------------#
[32m[20221213 23:24:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0001 |          46.4803 |          14.5437 |
[32m[20221213 23:24:13 @agent_ppo2.py:185][0m |          -0.0043 |          43.8869 |          14.5448 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |           0.0055 |          43.6816 |          14.5309 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |          -0.0003 |          42.8489 |          14.5265 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |          -0.0052 |          42.1602 |          14.5351 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |          -0.0039 |          41.8486 |          14.5342 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |          -0.0084 |          41.7922 |          14.5589 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |          -0.0068 |          41.3725 |          14.5412 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |          -0.0048 |          41.2208 |          14.5201 |
[32m[20221213 23:24:14 @agent_ppo2.py:185][0m |          -0.0021 |          42.4439 |          14.5271 |
[32m[20221213 23:24:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.83
[32m[20221213 23:24:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.75
[32m[20221213 23:24:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.70
[32m[20221213 23:24:14 @agent_ppo2.py:143][0m Total time:      11.71 min
[32m[20221213 23:24:14 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221213 23:24:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2553 --------------------------#
[32m[20221213 23:24:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |           0.0049 |          53.3346 |          14.3743 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0056 |          49.9481 |          14.3833 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0095 |          49.0831 |          14.3865 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0077 |          48.8542 |          14.3807 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0105 |          48.0107 |          14.3896 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0106 |          47.6957 |          14.4076 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0116 |          47.4277 |          14.3996 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0087 |          47.2799 |          14.4159 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0120 |          46.9528 |          14.4207 |
[32m[20221213 23:24:15 @agent_ppo2.py:185][0m |          -0.0122 |          46.9797 |          14.4077 |
[32m[20221213 23:24:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:24:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.26
[32m[20221213 23:24:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.78
[32m[20221213 23:24:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.93
[32m[20221213 23:24:16 @agent_ppo2.py:143][0m Total time:      11.73 min
[32m[20221213 23:24:16 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221213 23:24:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2554 --------------------------#
[32m[20221213 23:24:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:16 @agent_ppo2.py:185][0m |           0.0005 |          43.6641 |          14.6818 |
[32m[20221213 23:24:16 @agent_ppo2.py:185][0m |          -0.0059 |          38.0592 |          14.6626 |
[32m[20221213 23:24:16 @agent_ppo2.py:185][0m |          -0.0064 |          36.6959 |          14.6520 |
[32m[20221213 23:24:16 @agent_ppo2.py:185][0m |          -0.0137 |          35.9207 |          14.6416 |
[32m[20221213 23:24:16 @agent_ppo2.py:185][0m |          -0.0077 |          35.5465 |          14.6657 |
[32m[20221213 23:24:16 @agent_ppo2.py:185][0m |          -0.0097 |          34.7194 |          14.6383 |
[32m[20221213 23:24:16 @agent_ppo2.py:185][0m |          -0.0035 |          35.2475 |          14.6092 |
[32m[20221213 23:24:17 @agent_ppo2.py:185][0m |          -0.0108 |          33.9933 |          14.6391 |
[32m[20221213 23:24:17 @agent_ppo2.py:185][0m |          -0.0136 |          33.5936 |          14.6637 |
[32m[20221213 23:24:17 @agent_ppo2.py:185][0m |          -0.0110 |          33.3051 |          14.6428 |
[32m[20221213 23:24:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.38
[32m[20221213 23:24:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.81
[32m[20221213 23:24:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.88
[32m[20221213 23:24:17 @agent_ppo2.py:143][0m Total time:      11.75 min
[32m[20221213 23:24:17 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221213 23:24:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2555 --------------------------#
[32m[20221213 23:24:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:17 @agent_ppo2.py:185][0m |          -0.0050 |          62.0268 |          14.5520 |
[32m[20221213 23:24:17 @agent_ppo2.py:185][0m |          -0.0073 |          56.7200 |          14.5171 |
[32m[20221213 23:24:17 @agent_ppo2.py:185][0m |          -0.0085 |          55.5055 |          14.4944 |
[32m[20221213 23:24:17 @agent_ppo2.py:185][0m |          -0.0103 |          54.7769 |          14.5081 |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |          -0.0068 |          54.6574 |          14.4833 |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |          -0.0127 |          53.6731 |          14.4570 |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |          -0.0120 |          53.0171 |          14.4660 |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |          -0.0107 |          52.5762 |          14.4406 |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |          -0.0132 |          52.1286 |          14.4435 |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |          -0.0133 |          52.0655 |          14.4383 |
[32m[20221213 23:24:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.17
[32m[20221213 23:24:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.07
[32m[20221213 23:24:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.89
[32m[20221213 23:24:18 @agent_ppo2.py:143][0m Total time:      11.77 min
[32m[20221213 23:24:18 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221213 23:24:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2556 --------------------------#
[32m[20221213 23:24:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |           0.0066 |          45.4355 |          14.5840 |
[32m[20221213 23:24:18 @agent_ppo2.py:185][0m |          -0.0061 |          39.7528 |          14.5742 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0036 |          38.4353 |          14.5889 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0102 |          37.4363 |          14.5896 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0054 |          36.5217 |          14.5963 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0115 |          36.4168 |          14.5977 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0139 |          35.8362 |          14.5857 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0118 |          35.5274 |          14.6164 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0174 |          35.3312 |          14.6086 |
[32m[20221213 23:24:19 @agent_ppo2.py:185][0m |          -0.0163 |          35.0339 |          14.5978 |
[32m[20221213 23:24:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.65
[32m[20221213 23:24:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.67
[32m[20221213 23:24:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.54
[32m[20221213 23:24:19 @agent_ppo2.py:143][0m Total time:      11.79 min
[32m[20221213 23:24:19 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221213 23:24:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2557 --------------------------#
[32m[20221213 23:24:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0067 |          37.6609 |          14.4431 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0045 |          35.1147 |          14.4200 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0035 |          35.5629 |          14.4197 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0106 |          33.7263 |          14.4204 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0128 |          33.5761 |          14.4087 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0156 |          33.3198 |          14.3818 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0118 |          32.8802 |          14.3751 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0108 |          33.4069 |          14.3809 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0131 |          32.7896 |          14.3597 |
[32m[20221213 23:24:20 @agent_ppo2.py:185][0m |          -0.0160 |          32.4701 |          14.3484 |
[32m[20221213 23:24:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.52
[32m[20221213 23:24:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.35
[32m[20221213 23:24:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.37
[32m[20221213 23:24:21 @agent_ppo2.py:143][0m Total time:      11.81 min
[32m[20221213 23:24:21 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221213 23:24:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2558 --------------------------#
[32m[20221213 23:24:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:21 @agent_ppo2.py:185][0m |          -0.0029 |          37.4868 |          14.4753 |
[32m[20221213 23:24:21 @agent_ppo2.py:185][0m |          -0.0044 |          36.0866 |          14.4611 |
[32m[20221213 23:24:21 @agent_ppo2.py:185][0m |          -0.0096 |          34.0216 |          14.4580 |
[32m[20221213 23:24:21 @agent_ppo2.py:185][0m |           0.0047 |          41.2902 |          14.4451 |
[32m[20221213 23:24:21 @agent_ppo2.py:185][0m |          -0.0125 |          33.7671 |          14.4321 |
[32m[20221213 23:24:21 @agent_ppo2.py:185][0m |          -0.0124 |          32.7803 |          14.4352 |
[32m[20221213 23:24:21 @agent_ppo2.py:185][0m |          -0.0156 |          32.3026 |          14.4268 |
[32m[20221213 23:24:22 @agent_ppo2.py:185][0m |          -0.0161 |          32.1113 |          14.4012 |
[32m[20221213 23:24:22 @agent_ppo2.py:185][0m |          -0.0131 |          32.0434 |          14.3740 |
[32m[20221213 23:24:22 @agent_ppo2.py:185][0m |          -0.0156 |          31.8448 |          14.3814 |
[32m[20221213 23:24:22 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:24:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.32
[32m[20221213 23:24:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.12
[32m[20221213 23:24:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.83
[32m[20221213 23:24:22 @agent_ppo2.py:143][0m Total time:      11.83 min
[32m[20221213 23:24:22 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221213 23:24:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2559 --------------------------#
[32m[20221213 23:24:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:22 @agent_ppo2.py:185][0m |          -0.0002 |          56.6049 |          14.4370 |
[32m[20221213 23:24:22 @agent_ppo2.py:185][0m |          -0.0035 |          55.2119 |          14.4545 |
[32m[20221213 23:24:22 @agent_ppo2.py:185][0m |          -0.0037 |          54.4549 |          14.4245 |
[32m[20221213 23:24:22 @agent_ppo2.py:185][0m |          -0.0062 |          54.0326 |          14.4075 |
[32m[20221213 23:24:23 @agent_ppo2.py:185][0m |          -0.0072 |          53.7276 |          14.4121 |
[32m[20221213 23:24:23 @agent_ppo2.py:185][0m |           0.0004 |          55.6516 |          14.4049 |
[32m[20221213 23:24:23 @agent_ppo2.py:185][0m |          -0.0069 |          53.3888 |          14.3674 |
[32m[20221213 23:24:23 @agent_ppo2.py:185][0m |          -0.0087 |          53.3175 |          14.3790 |
[32m[20221213 23:24:23 @agent_ppo2.py:185][0m |          -0.0073 |          53.1813 |          14.3751 |
[32m[20221213 23:24:23 @agent_ppo2.py:185][0m |          -0.0092 |          53.0031 |          14.3748 |
[32m[20221213 23:24:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.11
[32m[20221213 23:24:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.34
[32m[20221213 23:24:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.15
[32m[20221213 23:24:23 @agent_ppo2.py:143][0m Total time:      11.85 min
[32m[20221213 23:24:23 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221213 23:24:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2560 --------------------------#
[32m[20221213 23:24:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:24:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:23 @agent_ppo2.py:185][0m |          -0.0026 |          31.8525 |          14.4740 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0037 |          31.4012 |          14.4975 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0080 |          28.8411 |          14.4796 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0052 |          28.1652 |          14.5014 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0167 |          27.5001 |          14.4966 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0181 |          27.1605 |          14.4852 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0003 |          33.0120 |          14.4666 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0143 |          26.6091 |          14.4909 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0207 |          26.3067 |          14.4814 |
[32m[20221213 23:24:24 @agent_ppo2.py:185][0m |          -0.0196 |          25.9845 |          14.4864 |
[32m[20221213 23:24:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.96
[32m[20221213 23:24:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.42
[32m[20221213 23:24:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.23
[32m[20221213 23:24:24 @agent_ppo2.py:143][0m Total time:      11.87 min
[32m[20221213 23:24:24 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221213 23:24:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2561 --------------------------#
[32m[20221213 23:24:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |           0.0009 |          52.6346 |          14.2528 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |           0.0005 |          52.5068 |          14.2737 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0083 |          50.8444 |          14.2616 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0071 |          50.5839 |          14.2657 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0108 |          50.5102 |          14.2423 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0083 |          50.4502 |          14.2678 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0080 |          50.2642 |          14.2667 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0109 |          50.3308 |          14.2518 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0125 |          50.2659 |          14.2753 |
[32m[20221213 23:24:25 @agent_ppo2.py:185][0m |          -0.0100 |          50.1831 |          14.2409 |
[32m[20221213 23:24:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.63
[32m[20221213 23:24:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.04
[32m[20221213 23:24:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.47
[32m[20221213 23:24:26 @agent_ppo2.py:143][0m Total time:      11.89 min
[32m[20221213 23:24:26 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221213 23:24:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2562 --------------------------#
[32m[20221213 23:24:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:26 @agent_ppo2.py:185][0m |          -0.0022 |          37.3488 |          14.2805 |
[32m[20221213 23:24:26 @agent_ppo2.py:185][0m |          -0.0091 |          35.3415 |          14.2685 |
[32m[20221213 23:24:26 @agent_ppo2.py:185][0m |          -0.0110 |          34.5932 |          14.2266 |
[32m[20221213 23:24:26 @agent_ppo2.py:185][0m |          -0.0109 |          34.2218 |          14.2283 |
[32m[20221213 23:24:26 @agent_ppo2.py:185][0m |          -0.0104 |          34.0079 |          14.2088 |
[32m[20221213 23:24:26 @agent_ppo2.py:185][0m |          -0.0155 |          33.7854 |          14.2182 |
[32m[20221213 23:24:26 @agent_ppo2.py:185][0m |          -0.0151 |          33.5092 |          14.2051 |
[32m[20221213 23:24:27 @agent_ppo2.py:185][0m |          -0.0147 |          33.3832 |          14.1953 |
[32m[20221213 23:24:27 @agent_ppo2.py:185][0m |          -0.0178 |          33.1757 |          14.1730 |
[32m[20221213 23:24:27 @agent_ppo2.py:185][0m |          -0.0181 |          33.1564 |          14.1808 |
[32m[20221213 23:24:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.33
[32m[20221213 23:24:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.17
[32m[20221213 23:24:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.89
[32m[20221213 23:24:27 @agent_ppo2.py:143][0m Total time:      11.91 min
[32m[20221213 23:24:27 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221213 23:24:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2563 --------------------------#
[32m[20221213 23:24:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:24:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:27 @agent_ppo2.py:185][0m |          -0.0020 |          55.6294 |          14.2849 |
[32m[20221213 23:24:27 @agent_ppo2.py:185][0m |          -0.0055 |          53.2015 |          14.2579 |
[32m[20221213 23:24:27 @agent_ppo2.py:185][0m |          -0.0065 |          51.9219 |          14.2756 |
[32m[20221213 23:24:27 @agent_ppo2.py:185][0m |          -0.0107 |          51.1078 |          14.2663 |
[32m[20221213 23:24:28 @agent_ppo2.py:185][0m |          -0.0080 |          50.7135 |          14.2640 |
[32m[20221213 23:24:28 @agent_ppo2.py:185][0m |          -0.0118 |          50.5044 |          14.2433 |
[32m[20221213 23:24:28 @agent_ppo2.py:185][0m |          -0.0082 |          51.8569 |          14.2566 |
[32m[20221213 23:24:28 @agent_ppo2.py:185][0m |          -0.0123 |          49.7253 |          14.2594 |
[32m[20221213 23:24:28 @agent_ppo2.py:185][0m |          -0.0125 |          49.4039 |          14.2462 |
[32m[20221213 23:24:28 @agent_ppo2.py:185][0m |          -0.0121 |          49.0220 |          14.2275 |
[32m[20221213 23:24:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:24:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.82
[32m[20221213 23:24:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.17
[32m[20221213 23:24:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.36
[32m[20221213 23:24:28 @agent_ppo2.py:143][0m Total time:      11.94 min
[32m[20221213 23:24:28 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221213 23:24:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2564 --------------------------#
[32m[20221213 23:24:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:28 @agent_ppo2.py:185][0m |           0.0048 |          35.4565 |          14.0869 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0050 |          31.0301 |          14.0965 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0046 |          30.2989 |          14.0654 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0093 |          29.4175 |          14.0501 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0135 |          28.8497 |          14.0239 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0092 |          28.4752 |          13.9999 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0125 |          28.2207 |          13.9718 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0081 |          29.3572 |          13.9700 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0161 |          27.9452 |          13.9539 |
[32m[20221213 23:24:29 @agent_ppo2.py:185][0m |          -0.0156 |          27.6368 |          13.9196 |
[32m[20221213 23:24:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:24:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.46
[32m[20221213 23:24:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.57
[32m[20221213 23:24:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.89
[32m[20221213 23:24:29 @agent_ppo2.py:143][0m Total time:      11.96 min
[32m[20221213 23:24:29 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221213 23:24:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2565 --------------------------#
[32m[20221213 23:24:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |           0.0015 |          46.2209 |          14.0580 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0084 |          42.5379 |          14.0562 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0125 |          41.1794 |          14.0622 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0114 |          40.2278 |          14.0497 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0015 |          44.4776 |          14.0124 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0005 |          43.1357 |          14.0202 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0075 |          38.9089 |          14.0209 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0028 |          39.9921 |          14.0080 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0147 |          38.8623 |          13.9951 |
[32m[20221213 23:24:30 @agent_ppo2.py:185][0m |          -0.0143 |          38.1643 |          13.9834 |
[32m[20221213 23:24:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.47
[32m[20221213 23:24:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.09
[32m[20221213 23:24:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.11
[32m[20221213 23:24:31 @agent_ppo2.py:143][0m Total time:      11.98 min
[32m[20221213 23:24:31 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221213 23:24:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2566 --------------------------#
[32m[20221213 23:24:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:31 @agent_ppo2.py:185][0m |           0.0107 |          51.7498 |          13.9421 |
[32m[20221213 23:24:31 @agent_ppo2.py:185][0m |           0.0173 |          54.2442 |          13.9695 |
[32m[20221213 23:24:31 @agent_ppo2.py:185][0m |          -0.0049 |          47.5019 |          13.9913 |
[32m[20221213 23:24:31 @agent_ppo2.py:185][0m |          -0.0054 |          47.2571 |          13.9969 |
[32m[20221213 23:24:31 @agent_ppo2.py:185][0m |          -0.0074 |          47.1843 |          13.9926 |
[32m[20221213 23:24:31 @agent_ppo2.py:185][0m |          -0.0102 |          47.0525 |          13.9903 |
[32m[20221213 23:24:31 @agent_ppo2.py:185][0m |          -0.0075 |          47.0263 |          13.9954 |
[32m[20221213 23:24:32 @agent_ppo2.py:185][0m |          -0.0059 |          47.1189 |          14.0009 |
[32m[20221213 23:24:32 @agent_ppo2.py:185][0m |          -0.0112 |          46.9760 |          14.0203 |
[32m[20221213 23:24:32 @agent_ppo2.py:185][0m |          -0.0062 |          47.0981 |          14.0125 |
[32m[20221213 23:24:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.60
[32m[20221213 23:24:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.50
[32m[20221213 23:24:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.30
[32m[20221213 23:24:32 @agent_ppo2.py:143][0m Total time:      12.00 min
[32m[20221213 23:24:32 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221213 23:24:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2567 --------------------------#
[32m[20221213 23:24:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:32 @agent_ppo2.py:185][0m |           0.0023 |          49.6111 |          13.7070 |
[32m[20221213 23:24:32 @agent_ppo2.py:185][0m |          -0.0029 |          47.2702 |          13.6656 |
[32m[20221213 23:24:32 @agent_ppo2.py:185][0m |          -0.0087 |          46.2319 |          13.6688 |
[32m[20221213 23:24:32 @agent_ppo2.py:185][0m |          -0.0092 |          45.5234 |          13.6760 |
[32m[20221213 23:24:33 @agent_ppo2.py:185][0m |          -0.0119 |          44.8817 |          13.6549 |
[32m[20221213 23:24:33 @agent_ppo2.py:185][0m |          -0.0078 |          44.5482 |          13.6589 |
[32m[20221213 23:24:33 @agent_ppo2.py:185][0m |          -0.0064 |          45.0378 |          13.6544 |
[32m[20221213 23:24:33 @agent_ppo2.py:185][0m |           0.0025 |          48.3140 |          13.6360 |
[32m[20221213 23:24:33 @agent_ppo2.py:185][0m |          -0.0057 |          44.8082 |          13.6345 |
[32m[20221213 23:24:33 @agent_ppo2.py:185][0m |          -0.0132 |          43.4010 |          13.6476 |
[32m[20221213 23:24:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:24:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.28
[32m[20221213 23:24:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.96
[32m[20221213 23:24:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.74
[32m[20221213 23:24:33 @agent_ppo2.py:143][0m Total time:      12.02 min
[32m[20221213 23:24:33 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221213 23:24:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2568 --------------------------#
[32m[20221213 23:24:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:33 @agent_ppo2.py:185][0m |          -0.0036 |          41.2605 |          13.9460 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0068 |          38.7622 |          13.9353 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0103 |          37.8645 |          13.9414 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0117 |          37.1287 |          13.9421 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0039 |          39.0332 |          13.9489 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0079 |          36.4035 |          13.9348 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0165 |          35.7989 |          13.9349 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0138 |          35.6602 |          13.9452 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0160 |          35.3583 |          13.9407 |
[32m[20221213 23:24:34 @agent_ppo2.py:185][0m |          -0.0192 |          35.3450 |          13.9415 |
[32m[20221213 23:24:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:24:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.76
[32m[20221213 23:24:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.94
[32m[20221213 23:24:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.83
[32m[20221213 23:24:34 @agent_ppo2.py:143][0m Total time:      12.04 min
[32m[20221213 23:24:34 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221213 23:24:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2569 --------------------------#
[32m[20221213 23:24:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:24:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |           0.0136 |          59.6582 |          13.8088 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |           0.0090 |          56.4365 |          13.8029 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |          -0.0010 |          51.1767 |          13.8057 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |          -0.0037 |          50.8288 |          13.8095 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |           0.0025 |          53.9915 |          13.8506 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |          -0.0079 |          50.1618 |          13.8660 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |          -0.0114 |          49.8767 |          13.8723 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |          -0.0107 |          49.7047 |          13.8556 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |          -0.0121 |          49.6980 |          13.8704 |
[32m[20221213 23:24:35 @agent_ppo2.py:185][0m |          -0.0116 |          49.4714 |          13.8918 |
[32m[20221213 23:24:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.63
[32m[20221213 23:24:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.45
[32m[20221213 23:24:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.63
[32m[20221213 23:24:36 @agent_ppo2.py:143][0m Total time:      12.06 min
[32m[20221213 23:24:36 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221213 23:24:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2570 --------------------------#
[32m[20221213 23:24:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:24:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:36 @agent_ppo2.py:185][0m |           0.0001 |          42.8100 |          14.1127 |
[32m[20221213 23:24:36 @agent_ppo2.py:185][0m |          -0.0056 |          39.5523 |          14.0893 |
[32m[20221213 23:24:36 @agent_ppo2.py:185][0m |          -0.0048 |          38.2944 |          14.0851 |
[32m[20221213 23:24:36 @agent_ppo2.py:185][0m |          -0.0117 |          36.8030 |          14.0625 |
[32m[20221213 23:24:36 @agent_ppo2.py:185][0m |          -0.0106 |          35.8439 |          14.0532 |
[32m[20221213 23:24:36 @agent_ppo2.py:185][0m |          -0.0148 |          35.2905 |          14.0438 |
[32m[20221213 23:24:36 @agent_ppo2.py:185][0m |          -0.0135 |          34.7520 |          14.0366 |
[32m[20221213 23:24:37 @agent_ppo2.py:185][0m |          -0.0187 |          34.2085 |          14.0051 |
[32m[20221213 23:24:37 @agent_ppo2.py:185][0m |          -0.0085 |          34.0383 |          13.9894 |
[32m[20221213 23:24:37 @agent_ppo2.py:185][0m |          -0.0201 |          33.5725 |          13.9889 |
[32m[20221213 23:24:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.58
[32m[20221213 23:24:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.98
[32m[20221213 23:24:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 208.67
[32m[20221213 23:24:37 @agent_ppo2.py:143][0m Total time:      12.08 min
[32m[20221213 23:24:37 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221213 23:24:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2571 --------------------------#
[32m[20221213 23:24:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:37 @agent_ppo2.py:185][0m |           0.0001 |          48.2602 |          13.7821 |
[32m[20221213 23:24:37 @agent_ppo2.py:185][0m |           0.0006 |          42.3098 |          13.7995 |
[32m[20221213 23:24:37 @agent_ppo2.py:185][0m |          -0.0079 |          40.5513 |          13.7716 |
[32m[20221213 23:24:37 @agent_ppo2.py:185][0m |          -0.0111 |          39.7722 |          13.8128 |
[32m[20221213 23:24:38 @agent_ppo2.py:185][0m |          -0.0077 |          39.2100 |          13.7598 |
[32m[20221213 23:24:38 @agent_ppo2.py:185][0m |          -0.0129 |          38.6854 |          13.7349 |
[32m[20221213 23:24:38 @agent_ppo2.py:185][0m |          -0.0109 |          38.4578 |          13.7508 |
[32m[20221213 23:24:38 @agent_ppo2.py:185][0m |          -0.0089 |          37.9720 |          13.7087 |
[32m[20221213 23:24:38 @agent_ppo2.py:185][0m |          -0.0134 |          37.7332 |          13.7353 |
[32m[20221213 23:24:38 @agent_ppo2.py:185][0m |          -0.0160 |          37.4511 |          13.7394 |
[32m[20221213 23:24:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.65
[32m[20221213 23:24:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.59
[32m[20221213 23:24:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.67
[32m[20221213 23:24:38 @agent_ppo2.py:143][0m Total time:      12.10 min
[32m[20221213 23:24:38 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221213 23:24:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2572 --------------------------#
[32m[20221213 23:24:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:38 @agent_ppo2.py:185][0m |           0.0010 |          38.5270 |          13.7663 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0012 |          35.4605 |          13.7330 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0047 |          34.0451 |          13.6998 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0033 |          33.3813 |          13.7227 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0142 |          32.6969 |          13.7109 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0066 |          32.2654 |          13.6596 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0125 |          31.8660 |          13.6734 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0158 |          31.9693 |          13.6498 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0083 |          32.4286 |          13.6411 |
[32m[20221213 23:24:39 @agent_ppo2.py:185][0m |          -0.0201 |          31.7630 |          13.6371 |
[32m[20221213 23:24:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:24:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.45
[32m[20221213 23:24:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.42
[32m[20221213 23:24:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:24:39 @agent_ppo2.py:143][0m Total time:      12.12 min
[32m[20221213 23:24:39 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221213 23:24:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2573 --------------------------#
[32m[20221213 23:24:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:24:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |           0.0039 |          52.6695 |          13.2881 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0011 |          50.4093 |          13.2430 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0062 |          49.6663 |          13.2288 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0035 |          49.3916 |          13.2490 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0005 |          49.5637 |          13.2252 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0058 |          49.0432 |          13.2096 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0072 |          48.8089 |          13.2021 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0091 |          48.6804 |          13.2104 |
[32m[20221213 23:24:40 @agent_ppo2.py:185][0m |          -0.0053 |          48.6140 |          13.1942 |
[32m[20221213 23:24:41 @agent_ppo2.py:185][0m |          -0.0033 |          49.4147 |          13.2066 |
[32m[20221213 23:24:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:24:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.02
[32m[20221213 23:24:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.50
[32m[20221213 23:24:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.66
[32m[20221213 23:24:41 @agent_ppo2.py:143][0m Total time:      12.14 min
[32m[20221213 23:24:41 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221213 23:24:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2574 --------------------------#
[32m[20221213 23:24:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:41 @agent_ppo2.py:185][0m |           0.0003 |          60.0607 |          13.6170 |
[32m[20221213 23:24:41 @agent_ppo2.py:185][0m |          -0.0089 |          58.8185 |          13.6004 |
[32m[20221213 23:24:41 @agent_ppo2.py:185][0m |          -0.0078 |          58.3580 |          13.5951 |
[32m[20221213 23:24:41 @agent_ppo2.py:185][0m |          -0.0114 |          58.2363 |          13.5920 |
[32m[20221213 23:24:41 @agent_ppo2.py:185][0m |          -0.0102 |          57.9507 |          13.6129 |
[32m[20221213 23:24:41 @agent_ppo2.py:185][0m |          -0.0081 |          57.6956 |          13.5971 |
[32m[20221213 23:24:42 @agent_ppo2.py:185][0m |          -0.0129 |          57.4208 |          13.5698 |
[32m[20221213 23:24:42 @agent_ppo2.py:185][0m |          -0.0153 |          57.4838 |          13.5844 |
[32m[20221213 23:24:42 @agent_ppo2.py:185][0m |          -0.0138 |          57.5704 |          13.5819 |
[32m[20221213 23:24:42 @agent_ppo2.py:185][0m |          -0.0139 |          57.2719 |          13.5682 |
[32m[20221213 23:24:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.60
[32m[20221213 23:24:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.88
[32m[20221213 23:24:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.99
[32m[20221213 23:24:42 @agent_ppo2.py:143][0m Total time:      12.17 min
[32m[20221213 23:24:42 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221213 23:24:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2575 --------------------------#
[32m[20221213 23:24:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:42 @agent_ppo2.py:185][0m |           0.0003 |          47.5289 |          13.6169 |
[32m[20221213 23:24:42 @agent_ppo2.py:185][0m |          -0.0092 |          46.0589 |          13.6290 |
[32m[20221213 23:24:42 @agent_ppo2.py:185][0m |           0.0015 |          50.1276 |          13.6057 |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |          -0.0073 |          45.3961 |          13.6010 |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |          -0.0095 |          45.5296 |          13.6133 |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |          -0.0121 |          44.9399 |          13.6105 |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |          -0.0124 |          44.8457 |          13.5980 |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |          -0.0097 |          44.7639 |          13.6002 |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |          -0.0128 |          44.7155 |          13.6004 |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |          -0.0119 |          44.6076 |          13.6122 |
[32m[20221213 23:24:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.78
[32m[20221213 23:24:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.49
[32m[20221213 23:24:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.33
[32m[20221213 23:24:43 @agent_ppo2.py:143][0m Total time:      12.19 min
[32m[20221213 23:24:43 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221213 23:24:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2576 --------------------------#
[32m[20221213 23:24:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:43 @agent_ppo2.py:185][0m |           0.0005 |          48.9582 |          13.4109 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |           0.0124 |          49.2468 |          13.4224 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0065 |          41.0634 |          13.4408 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0106 |          39.0609 |          13.4349 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0111 |          37.4010 |          13.4521 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0114 |          36.5135 |          13.4360 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0140 |          36.1040 |          13.4484 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0070 |          38.2083 |          13.4424 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0057 |          35.1921 |          13.4242 |
[32m[20221213 23:24:44 @agent_ppo2.py:185][0m |          -0.0150 |          34.8200 |          13.4410 |
[32m[20221213 23:24:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.24
[32m[20221213 23:24:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.98
[32m[20221213 23:24:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.57
[32m[20221213 23:24:44 @agent_ppo2.py:143][0m Total time:      12.21 min
[32m[20221213 23:24:44 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221213 23:24:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2577 --------------------------#
[32m[20221213 23:24:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |           0.0003 |          60.1823 |          13.7066 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |          -0.0034 |          58.4386 |          13.7246 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |          -0.0071 |          57.7893 |          13.6893 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |          -0.0068 |          57.5429 |          13.6846 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |          -0.0088 |          57.3122 |          13.6774 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |           0.0031 |          60.2221 |          13.6885 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |          -0.0025 |          58.7306 |          13.6798 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |          -0.0078 |          56.8824 |          13.6492 |
[32m[20221213 23:24:45 @agent_ppo2.py:185][0m |           0.0014 |          60.0306 |          13.6668 |
[32m[20221213 23:24:46 @agent_ppo2.py:185][0m |          -0.0095 |          56.6041 |          13.6726 |
[32m[20221213 23:24:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:24:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.72
[32m[20221213 23:24:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.08
[32m[20221213 23:24:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.00
[32m[20221213 23:24:46 @agent_ppo2.py:143][0m Total time:      12.23 min
[32m[20221213 23:24:46 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221213 23:24:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2578 --------------------------#
[32m[20221213 23:24:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:46 @agent_ppo2.py:185][0m |          -0.0005 |          56.2952 |          13.1304 |
[32m[20221213 23:24:46 @agent_ppo2.py:185][0m |           0.0022 |          37.0924 |          13.1294 |
[32m[20221213 23:24:46 @agent_ppo2.py:185][0m |          -0.0039 |          34.3548 |          13.1292 |
[32m[20221213 23:24:46 @agent_ppo2.py:185][0m |          -0.0122 |          33.0435 |          13.1211 |
[32m[20221213 23:24:46 @agent_ppo2.py:185][0m |          -0.0107 |          31.7163 |          13.1237 |
[32m[20221213 23:24:46 @agent_ppo2.py:185][0m |          -0.0053 |          30.9852 |          13.1623 |
[32m[20221213 23:24:47 @agent_ppo2.py:185][0m |          -0.0125 |          30.7667 |          13.1659 |
[32m[20221213 23:24:47 @agent_ppo2.py:185][0m |          -0.0109 |          29.8512 |          13.1703 |
[32m[20221213 23:24:47 @agent_ppo2.py:185][0m |          -0.0119 |          29.5093 |          13.1392 |
[32m[20221213 23:24:47 @agent_ppo2.py:185][0m |          -0.0122 |          29.3031 |          13.1534 |
[32m[20221213 23:24:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.82
[32m[20221213 23:24:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.95
[32m[20221213 23:24:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.63
[32m[20221213 23:24:47 @agent_ppo2.py:143][0m Total time:      12.25 min
[32m[20221213 23:24:47 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221213 23:24:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2579 --------------------------#
[32m[20221213 23:24:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:24:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:47 @agent_ppo2.py:185][0m |          -0.0037 |          46.0371 |          13.6635 |
[32m[20221213 23:24:47 @agent_ppo2.py:185][0m |           0.0007 |          43.9640 |          13.6724 |
[32m[20221213 23:24:47 @agent_ppo2.py:185][0m |           0.0003 |          41.9416 |          13.6504 |
[32m[20221213 23:24:48 @agent_ppo2.py:185][0m |          -0.0078 |          41.3980 |          13.6366 |
[32m[20221213 23:24:48 @agent_ppo2.py:185][0m |          -0.0085 |          40.9387 |          13.6340 |
[32m[20221213 23:24:48 @agent_ppo2.py:185][0m |          -0.0078 |          40.7386 |          13.6454 |
[32m[20221213 23:24:48 @agent_ppo2.py:185][0m |          -0.0079 |          40.4730 |          13.6487 |
[32m[20221213 23:24:48 @agent_ppo2.py:185][0m |          -0.0096 |          40.3330 |          13.6565 |
[32m[20221213 23:24:48 @agent_ppo2.py:185][0m |          -0.0072 |          40.1650 |          13.6590 |
[32m[20221213 23:24:48 @agent_ppo2.py:185][0m |          -0.0111 |          40.0556 |          13.6713 |
[32m[20221213 23:24:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.25
[32m[20221213 23:24:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.60
[32m[20221213 23:24:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.55
[32m[20221213 23:24:48 @agent_ppo2.py:143][0m Total time:      12.27 min
[32m[20221213 23:24:48 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221213 23:24:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2580 --------------------------#
[32m[20221213 23:24:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:24:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |           0.0059 |          53.5462 |          13.5402 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0008 |          52.2233 |          13.5570 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0046 |          51.0492 |          13.5783 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0070 |          50.8812 |          13.5546 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0070 |          50.6664 |          13.5414 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0007 |          51.1163 |          13.5610 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0115 |          50.4879 |          13.5790 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0056 |          50.9186 |          13.5536 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0088 |          50.3104 |          13.5744 |
[32m[20221213 23:24:49 @agent_ppo2.py:185][0m |          -0.0114 |          50.2900 |          13.6011 |
[32m[20221213 23:24:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:24:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.17
[32m[20221213 23:24:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.97
[32m[20221213 23:24:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.97
[32m[20221213 23:24:49 @agent_ppo2.py:143][0m Total time:      12.29 min
[32m[20221213 23:24:49 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221213 23:24:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2581 --------------------------#
[32m[20221213 23:24:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0030 |          44.8748 |          13.6000 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |           0.0005 |          33.5013 |          13.5751 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0070 |          32.1520 |          13.5505 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0074 |          31.6495 |          13.5524 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0027 |          31.5044 |          13.5438 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0043 |          31.1958 |          13.5506 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0077 |          30.8928 |          13.5593 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0101 |          32.7188 |          13.5110 |
[32m[20221213 23:24:50 @agent_ppo2.py:185][0m |          -0.0138 |          30.6757 |          13.5288 |
[32m[20221213 23:24:51 @agent_ppo2.py:185][0m |          -0.0091 |          30.6221 |          13.5374 |
[32m[20221213 23:24:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.10
[32m[20221213 23:24:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.34
[32m[20221213 23:24:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.24
[32m[20221213 23:24:51 @agent_ppo2.py:143][0m Total time:      12.31 min
[32m[20221213 23:24:51 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221213 23:24:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2582 --------------------------#
[32m[20221213 23:24:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:24:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:51 @agent_ppo2.py:185][0m |          -0.0042 |          31.4460 |          13.4950 |
[32m[20221213 23:24:51 @agent_ppo2.py:185][0m |          -0.0009 |          25.3699 |          13.5171 |
[32m[20221213 23:24:51 @agent_ppo2.py:185][0m |          -0.0033 |          24.3100 |          13.5217 |
[32m[20221213 23:24:51 @agent_ppo2.py:185][0m |          -0.0064 |          23.8288 |          13.5227 |
[32m[20221213 23:24:51 @agent_ppo2.py:185][0m |          -0.0053 |          23.6290 |          13.4904 |
[32m[20221213 23:24:51 @agent_ppo2.py:185][0m |          -0.0040 |          23.2595 |          13.5102 |
[32m[20221213 23:24:52 @agent_ppo2.py:185][0m |          -0.0045 |          25.2023 |          13.4997 |
[32m[20221213 23:24:52 @agent_ppo2.py:185][0m |          -0.0052 |          22.8209 |          13.5367 |
[32m[20221213 23:24:52 @agent_ppo2.py:185][0m |          -0.0097 |          22.7943 |          13.5172 |
[32m[20221213 23:24:52 @agent_ppo2.py:185][0m |          -0.0088 |          22.6652 |          13.4914 |
[32m[20221213 23:24:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:24:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.26
[32m[20221213 23:24:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.18
[32m[20221213 23:24:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.15
[32m[20221213 23:24:52 @agent_ppo2.py:143][0m Total time:      12.33 min
[32m[20221213 23:24:52 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221213 23:24:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2583 --------------------------#
[32m[20221213 23:24:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:52 @agent_ppo2.py:185][0m |           0.0075 |          33.6480 |          13.3975 |
[32m[20221213 23:24:52 @agent_ppo2.py:185][0m |          -0.0048 |          29.5243 |          13.3498 |
[32m[20221213 23:24:52 @agent_ppo2.py:185][0m |          -0.0082 |          28.6564 |          13.3941 |
[32m[20221213 23:24:53 @agent_ppo2.py:185][0m |          -0.0093 |          28.1706 |          13.3549 |
[32m[20221213 23:24:53 @agent_ppo2.py:185][0m |          -0.0096 |          27.9237 |          13.3457 |
[32m[20221213 23:24:53 @agent_ppo2.py:185][0m |          -0.0179 |          27.7028 |          13.3362 |
[32m[20221213 23:24:53 @agent_ppo2.py:185][0m |          -0.0146 |          27.3948 |          13.3244 |
[32m[20221213 23:24:53 @agent_ppo2.py:185][0m |          -0.0116 |          27.2733 |          13.2903 |
[32m[20221213 23:24:53 @agent_ppo2.py:185][0m |          -0.0157 |          27.0624 |          13.2782 |
[32m[20221213 23:24:53 @agent_ppo2.py:185][0m |          -0.0153 |          26.9321 |          13.2800 |
[32m[20221213 23:24:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.87
[32m[20221213 23:24:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.27
[32m[20221213 23:24:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.53
[32m[20221213 23:24:53 @agent_ppo2.py:143][0m Total time:      12.35 min
[32m[20221213 23:24:53 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221213 23:24:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2584 --------------------------#
[32m[20221213 23:24:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:24:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |           0.0073 |          27.4396 |          13.1913 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0096 |          23.3419 |          13.1873 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0027 |          22.3897 |          13.1785 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0056 |          21.6715 |          13.1861 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0043 |          21.3253 |          13.1902 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0082 |          21.2321 |          13.1566 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0089 |          20.7750 |          13.1476 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0164 |          20.6492 |          13.1613 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0127 |          20.4034 |          13.1577 |
[32m[20221213 23:24:54 @agent_ppo2.py:185][0m |          -0.0177 |          20.7657 |          13.1833 |
[32m[20221213 23:24:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.40
[32m[20221213 23:24:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.23
[32m[20221213 23:24:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.45
[32m[20221213 23:24:54 @agent_ppo2.py:143][0m Total time:      12.37 min
[32m[20221213 23:24:54 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221213 23:24:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2585 --------------------------#
[32m[20221213 23:24:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |           0.0064 |          33.9551 |          13.4190 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0067 |          31.4141 |          13.4440 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0045 |          30.7775 |          13.4149 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0040 |          29.9252 |          13.4117 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0125 |          29.6832 |          13.4241 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0095 |          29.2292 |          13.3867 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0093 |          28.8593 |          13.3930 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0139 |          28.7188 |          13.4003 |
[32m[20221213 23:24:55 @agent_ppo2.py:185][0m |          -0.0109 |          28.5165 |          13.4081 |
[32m[20221213 23:24:56 @agent_ppo2.py:185][0m |          -0.0175 |          28.5657 |          13.3778 |
[32m[20221213 23:24:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.47
[32m[20221213 23:24:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.30
[32m[20221213 23:24:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.32
[32m[20221213 23:24:56 @agent_ppo2.py:143][0m Total time:      12.39 min
[32m[20221213 23:24:56 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221213 23:24:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2586 --------------------------#
[32m[20221213 23:24:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:56 @agent_ppo2.py:185][0m |           0.0016 |          34.8399 |          13.2233 |
[32m[20221213 23:24:56 @agent_ppo2.py:185][0m |          -0.0086 |          32.2842 |          13.1950 |
[32m[20221213 23:24:56 @agent_ppo2.py:185][0m |          -0.0046 |          31.2633 |          13.2035 |
[32m[20221213 23:24:56 @agent_ppo2.py:185][0m |          -0.0077 |          30.7557 |          13.2039 |
[32m[20221213 23:24:56 @agent_ppo2.py:185][0m |          -0.0094 |          30.3092 |          13.2066 |
[32m[20221213 23:24:56 @agent_ppo2.py:185][0m |          -0.0109 |          30.2142 |          13.1756 |
[32m[20221213 23:24:57 @agent_ppo2.py:185][0m |          -0.0111 |          29.9589 |          13.2108 |
[32m[20221213 23:24:57 @agent_ppo2.py:185][0m |          -0.0160 |          29.4538 |          13.1732 |
[32m[20221213 23:24:57 @agent_ppo2.py:185][0m |          -0.0096 |          29.7181 |          13.1919 |
[32m[20221213 23:24:57 @agent_ppo2.py:185][0m |          -0.0163 |          29.2714 |          13.1724 |
[32m[20221213 23:24:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:24:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.95
[32m[20221213 23:24:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.61
[32m[20221213 23:24:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.32
[32m[20221213 23:24:57 @agent_ppo2.py:143][0m Total time:      12.42 min
[32m[20221213 23:24:57 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221213 23:24:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2587 --------------------------#
[32m[20221213 23:24:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:57 @agent_ppo2.py:185][0m |           0.0023 |          47.4049 |          13.3125 |
[32m[20221213 23:24:57 @agent_ppo2.py:185][0m |          -0.0016 |          46.6313 |          13.3372 |
[32m[20221213 23:24:57 @agent_ppo2.py:185][0m |          -0.0043 |          46.2238 |          13.3087 |
[32m[20221213 23:24:58 @agent_ppo2.py:185][0m |          -0.0050 |          46.5639 |          13.2841 |
[32m[20221213 23:24:58 @agent_ppo2.py:185][0m |          -0.0049 |          45.8099 |          13.2896 |
[32m[20221213 23:24:58 @agent_ppo2.py:185][0m |          -0.0093 |          45.7714 |          13.2949 |
[32m[20221213 23:24:58 @agent_ppo2.py:185][0m |          -0.0117 |          45.7734 |          13.2912 |
[32m[20221213 23:24:58 @agent_ppo2.py:185][0m |          -0.0117 |          45.6401 |          13.2712 |
[32m[20221213 23:24:58 @agent_ppo2.py:185][0m |          -0.0093 |          45.4542 |          13.2692 |
[32m[20221213 23:24:58 @agent_ppo2.py:185][0m |           0.0004 |          50.0285 |          13.2495 |
[32m[20221213 23:24:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:24:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.02
[32m[20221213 23:24:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.37
[32m[20221213 23:24:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.61
[32m[20221213 23:24:58 @agent_ppo2.py:143][0m Total time:      12.44 min
[32m[20221213 23:24:58 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221213 23:24:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2588 --------------------------#
[32m[20221213 23:24:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:24:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |           0.0064 |          46.5727 |          13.1479 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0052 |          41.5729 |          13.0566 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0041 |          40.0555 |          13.0808 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0103 |          39.3084 |          13.0610 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0099 |          38.8708 |          13.0517 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0136 |          38.4193 |          13.0723 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0138 |          38.2433 |          13.0586 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0109 |          38.2034 |          13.0582 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0148 |          37.6402 |          13.0515 |
[32m[20221213 23:24:59 @agent_ppo2.py:185][0m |          -0.0048 |          41.2293 |          13.0618 |
[32m[20221213 23:24:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:24:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.64
[32m[20221213 23:24:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.36
[32m[20221213 23:24:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.56
[32m[20221213 23:24:59 @agent_ppo2.py:143][0m Total time:      12.46 min
[32m[20221213 23:24:59 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221213 23:24:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2589 --------------------------#
[32m[20221213 23:25:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |           0.0000 |          46.7594 |          13.2307 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |           0.0007 |          43.7249 |          13.2954 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |          -0.0022 |          42.3386 |          13.2952 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |           0.0131 |          48.7890 |          13.3400 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |          -0.0066 |          41.4781 |          13.3415 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |          -0.0063 |          40.7893 |          13.3148 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |          -0.0050 |          40.5563 |          13.3213 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |          -0.0092 |          40.3307 |          13.3667 |
[32m[20221213 23:25:00 @agent_ppo2.py:185][0m |          -0.0101 |          39.9724 |          13.3621 |
[32m[20221213 23:25:01 @agent_ppo2.py:185][0m |          -0.0095 |          39.6762 |          13.3508 |
[32m[20221213 23:25:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:25:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.42
[32m[20221213 23:25:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.49
[32m[20221213 23:25:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.87
[32m[20221213 23:25:01 @agent_ppo2.py:143][0m Total time:      12.48 min
[32m[20221213 23:25:01 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221213 23:25:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2590 --------------------------#
[32m[20221213 23:25:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:25:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:01 @agent_ppo2.py:185][0m |           0.0033 |          29.3726 |          12.9942 |
[32m[20221213 23:25:01 @agent_ppo2.py:185][0m |          -0.0060 |          26.5078 |          13.0129 |
[32m[20221213 23:25:01 @agent_ppo2.py:185][0m |           0.0073 |          30.6647 |          12.9820 |
[32m[20221213 23:25:01 @agent_ppo2.py:185][0m |          -0.0055 |          25.8852 |          12.9915 |
[32m[20221213 23:25:01 @agent_ppo2.py:185][0m |          -0.0061 |          25.2760 |          12.9451 |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0053 |          24.9249 |          12.9687 |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0130 |          24.5912 |          12.9741 |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0137 |          24.4753 |          12.9814 |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0149 |          24.3165 |          13.0022 |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0165 |          24.2550 |          12.9674 |
[32m[20221213 23:25:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.49
[32m[20221213 23:25:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.76
[32m[20221213 23:25:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.64
[32m[20221213 23:25:02 @agent_ppo2.py:143][0m Total time:      12.50 min
[32m[20221213 23:25:02 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221213 23:25:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2591 --------------------------#
[32m[20221213 23:25:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0015 |          43.7657 |          13.2124 |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0040 |          40.7946 |          13.2067 |
[32m[20221213 23:25:02 @agent_ppo2.py:185][0m |          -0.0113 |          40.1066 |          13.2028 |
[32m[20221213 23:25:03 @agent_ppo2.py:185][0m |          -0.0121 |          39.4011 |          13.2146 |
[32m[20221213 23:25:03 @agent_ppo2.py:185][0m |          -0.0114 |          38.9676 |          13.1899 |
[32m[20221213 23:25:03 @agent_ppo2.py:185][0m |          -0.0067 |          41.0397 |          13.1983 |
[32m[20221213 23:25:03 @agent_ppo2.py:185][0m |          -0.0075 |          40.5679 |          13.1850 |
[32m[20221213 23:25:03 @agent_ppo2.py:185][0m |          -0.0106 |          38.1215 |          13.2012 |
[32m[20221213 23:25:03 @agent_ppo2.py:185][0m |          -0.0160 |          38.0761 |          13.1875 |
[32m[20221213 23:25:03 @agent_ppo2.py:185][0m |          -0.0064 |          40.2582 |          13.1948 |
[32m[20221213 23:25:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:25:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.38
[32m[20221213 23:25:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.02
[32m[20221213 23:25:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.52
[32m[20221213 23:25:03 @agent_ppo2.py:143][0m Total time:      12.52 min
[32m[20221213 23:25:03 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221213 23:25:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2592 --------------------------#
[32m[20221213 23:25:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |           0.0019 |          34.7850 |          13.1655 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0039 |          31.2584 |          13.1711 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |           0.0084 |          35.5244 |          13.1581 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0024 |          29.4826 |          13.1392 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0040 |          29.1941 |          13.1572 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0077 |          28.5469 |          13.1676 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0061 |          28.1422 |          13.1921 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0142 |          27.9854 |          13.1203 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0109 |          27.7247 |          13.1544 |
[32m[20221213 23:25:04 @agent_ppo2.py:185][0m |          -0.0147 |          27.6841 |          13.1568 |
[32m[20221213 23:25:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:25:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.47
[32m[20221213 23:25:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.59
[32m[20221213 23:25:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.69
[32m[20221213 23:25:05 @agent_ppo2.py:143][0m Total time:      12.54 min
[32m[20221213 23:25:05 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221213 23:25:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2593 --------------------------#
[32m[20221213 23:25:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |           0.0048 |          34.5898 |          12.9045 |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |           0.0042 |          32.6401 |          12.8896 |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |          -0.0022 |          30.1860 |          12.9228 |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |          -0.0061 |          29.6567 |          12.8843 |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |          -0.0064 |          29.1780 |          12.8915 |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |          -0.0075 |          28.9698 |          12.8690 |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |          -0.0100 |          28.7944 |          12.8651 |
[32m[20221213 23:25:05 @agent_ppo2.py:185][0m |          -0.0128 |          28.5481 |          12.8630 |
[32m[20221213 23:25:06 @agent_ppo2.py:185][0m |          -0.0049 |          28.5857 |          12.8607 |
[32m[20221213 23:25:06 @agent_ppo2.py:185][0m |          -0.0110 |          28.1783 |          12.8657 |
[32m[20221213 23:25:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.52
[32m[20221213 23:25:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.67
[32m[20221213 23:25:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.80
[32m[20221213 23:25:06 @agent_ppo2.py:143][0m Total time:      12.56 min
[32m[20221213 23:25:06 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221213 23:25:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2594 --------------------------#
[32m[20221213 23:25:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:25:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:06 @agent_ppo2.py:185][0m |           0.0011 |          10.1742 |          12.6667 |
[32m[20221213 23:25:06 @agent_ppo2.py:185][0m |           0.0044 |           9.7125 |          12.6353 |
[32m[20221213 23:25:06 @agent_ppo2.py:185][0m |          -0.0010 |           9.4892 |          12.6328 |
[32m[20221213 23:25:06 @agent_ppo2.py:185][0m |           0.0108 |           9.9220 |          12.6426 |
[32m[20221213 23:25:06 @agent_ppo2.py:185][0m |          -0.0036 |           9.4462 |          12.5939 |
[32m[20221213 23:25:07 @agent_ppo2.py:185][0m |          -0.0038 |           9.4365 |          12.6552 |
[32m[20221213 23:25:07 @agent_ppo2.py:185][0m |           0.0062 |           9.8912 |          12.6067 |
[32m[20221213 23:25:07 @agent_ppo2.py:185][0m |          -0.0033 |           9.4238 |          12.6045 |
[32m[20221213 23:25:07 @agent_ppo2.py:185][0m |          -0.0073 |           9.4091 |          12.6081 |
[32m[20221213 23:25:07 @agent_ppo2.py:185][0m |          -0.0026 |           9.4008 |          12.5686 |
[32m[20221213 23:25:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:25:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:25:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:25:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.36
[32m[20221213 23:25:07 @agent_ppo2.py:143][0m Total time:      12.58 min
[32m[20221213 23:25:07 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221213 23:25:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2595 --------------------------#
[32m[20221213 23:25:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:07 @agent_ppo2.py:185][0m |           0.0027 |          49.6127 |          12.8318 |
[32m[20221213 23:25:07 @agent_ppo2.py:185][0m |          -0.0057 |          44.7260 |          12.8131 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0081 |          42.9353 |          12.8132 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0063 |          42.7878 |          12.7647 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0107 |          41.4776 |          12.7805 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0018 |          41.4987 |          12.7545 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0099 |          40.4142 |          12.7449 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0149 |          40.0094 |          12.7064 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0181 |          39.6463 |          12.7326 |
[32m[20221213 23:25:08 @agent_ppo2.py:185][0m |          -0.0150 |          39.4830 |          12.6963 |
[32m[20221213 23:25:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.26
[32m[20221213 23:25:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.52
[32m[20221213 23:25:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.94
[32m[20221213 23:25:08 @agent_ppo2.py:143][0m Total time:      12.60 min
[32m[20221213 23:25:08 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221213 23:25:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2596 --------------------------#
[32m[20221213 23:25:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0002 |          59.2606 |          12.6164 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0065 |          55.4174 |          12.6143 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0046 |          54.3531 |          12.6050 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0088 |          53.5594 |          12.5981 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0093 |          53.2951 |          12.5751 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0120 |          52.9217 |          12.5699 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0092 |          52.4783 |          12.5421 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0052 |          53.3571 |          12.5124 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0107 |          52.0927 |          12.5142 |
[32m[20221213 23:25:09 @agent_ppo2.py:185][0m |          -0.0149 |          51.9551 |          12.5230 |
[32m[20221213 23:25:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:25:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.45
[32m[20221213 23:25:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.72
[32m[20221213 23:25:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.76
[32m[20221213 23:25:10 @agent_ppo2.py:143][0m Total time:      12.63 min
[32m[20221213 23:25:10 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221213 23:25:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2597 --------------------------#
[32m[20221213 23:25:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0036 |          45.5831 |          12.5530 |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0052 |          35.1227 |          12.5488 |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0052 |          32.9294 |          12.5349 |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0119 |          31.9897 |          12.5491 |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0098 |          31.2188 |          12.5250 |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0069 |          31.1595 |          12.5274 |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0153 |          30.2567 |          12.4985 |
[32m[20221213 23:25:10 @agent_ppo2.py:185][0m |          -0.0101 |          30.1302 |          12.5005 |
[32m[20221213 23:25:11 @agent_ppo2.py:185][0m |          -0.0120 |          29.9019 |          12.5107 |
[32m[20221213 23:25:11 @agent_ppo2.py:185][0m |          -0.0127 |          29.6668 |          12.4796 |
[32m[20221213 23:25:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.05
[32m[20221213 23:25:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.35
[32m[20221213 23:25:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.81
[32m[20221213 23:25:11 @agent_ppo2.py:143][0m Total time:      12.65 min
[32m[20221213 23:25:11 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221213 23:25:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2598 --------------------------#
[32m[20221213 23:25:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:11 @agent_ppo2.py:185][0m |           0.0024 |          50.2531 |          12.6117 |
[32m[20221213 23:25:11 @agent_ppo2.py:185][0m |          -0.0041 |          47.5619 |          12.6666 |
[32m[20221213 23:25:11 @agent_ppo2.py:185][0m |          -0.0107 |          46.6502 |          12.6687 |
[32m[20221213 23:25:11 @agent_ppo2.py:185][0m |           0.0031 |          51.3346 |          12.6945 |
[32m[20221213 23:25:11 @agent_ppo2.py:185][0m |           0.0056 |          52.7677 |          12.6738 |
[32m[20221213 23:25:12 @agent_ppo2.py:185][0m |          -0.0074 |          46.0072 |          12.6876 |
[32m[20221213 23:25:12 @agent_ppo2.py:185][0m |          -0.0109 |          45.5179 |          12.6857 |
[32m[20221213 23:25:12 @agent_ppo2.py:185][0m |          -0.0127 |          45.5153 |          12.6921 |
[32m[20221213 23:25:12 @agent_ppo2.py:185][0m |          -0.0152 |          45.2687 |          12.6934 |
[32m[20221213 23:25:12 @agent_ppo2.py:185][0m |          -0.0144 |          45.2286 |          12.6905 |
[32m[20221213 23:25:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.51
[32m[20221213 23:25:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.02
[32m[20221213 23:25:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.15
[32m[20221213 23:25:12 @agent_ppo2.py:143][0m Total time:      12.67 min
[32m[20221213 23:25:12 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221213 23:25:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2599 --------------------------#
[32m[20221213 23:25:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:12 @agent_ppo2.py:185][0m |           0.0053 |          39.5447 |          12.8733 |
[32m[20221213 23:25:12 @agent_ppo2.py:185][0m |          -0.0097 |          35.5527 |          12.8740 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0087 |          34.9248 |          12.8553 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0068 |          34.4655 |          12.8489 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0081 |          34.2577 |          12.8192 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0081 |          34.7403 |          12.8518 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0113 |          33.9803 |          12.8436 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0139 |          33.9263 |          12.8288 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0208 |          33.8274 |          12.8792 |
[32m[20221213 23:25:13 @agent_ppo2.py:185][0m |          -0.0133 |          33.7627 |          12.8439 |
[32m[20221213 23:25:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.55
[32m[20221213 23:25:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.99
[32m[20221213 23:25:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.42
[32m[20221213 23:25:13 @agent_ppo2.py:143][0m Total time:      12.69 min
[32m[20221213 23:25:13 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221213 23:25:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2600 --------------------------#
[32m[20221213 23:25:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:25:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0000 |          43.9224 |          12.4608 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |           0.0015 |          42.0612 |          12.4291 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |           0.0018 |          41.3003 |          12.4232 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0015 |          39.8533 |          12.3359 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0109 |          39.3206 |          12.3665 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0107 |          39.0834 |          12.3316 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0100 |          38.7523 |          12.3372 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0100 |          38.5040 |          12.2900 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0108 |          38.5214 |          12.2773 |
[32m[20221213 23:25:14 @agent_ppo2.py:185][0m |          -0.0149 |          38.4632 |          12.3002 |
[32m[20221213 23:25:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.12
[32m[20221213 23:25:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.36
[32m[20221213 23:25:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.18
[32m[20221213 23:25:15 @agent_ppo2.py:143][0m Total time:      12.71 min
[32m[20221213 23:25:15 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221213 23:25:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2601 --------------------------#
[32m[20221213 23:25:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0029 |          53.8630 |          12.2639 |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0035 |          51.5871 |          12.3077 |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0069 |          50.4633 |          12.2485 |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0065 |          49.5029 |          12.2288 |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0097 |          49.0562 |          12.2777 |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0078 |          48.5019 |          12.2695 |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0111 |          47.9966 |          12.2811 |
[32m[20221213 23:25:15 @agent_ppo2.py:185][0m |          -0.0034 |          49.3940 |          12.2702 |
[32m[20221213 23:25:16 @agent_ppo2.py:185][0m |          -0.0111 |          47.3277 |          12.2583 |
[32m[20221213 23:25:16 @agent_ppo2.py:185][0m |           0.0095 |          55.7014 |          12.3060 |
[32m[20221213 23:25:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:25:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.10
[32m[20221213 23:25:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.37
[32m[20221213 23:25:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.78
[32m[20221213 23:25:16 @agent_ppo2.py:143][0m Total time:      12.73 min
[32m[20221213 23:25:16 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221213 23:25:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2602 --------------------------#
[32m[20221213 23:25:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:16 @agent_ppo2.py:185][0m |          -0.0024 |          49.6688 |          12.3043 |
[32m[20221213 23:25:16 @agent_ppo2.py:185][0m |          -0.0050 |          45.2126 |          12.3227 |
[32m[20221213 23:25:16 @agent_ppo2.py:185][0m |          -0.0064 |          44.0717 |          12.2924 |
[32m[20221213 23:25:16 @agent_ppo2.py:185][0m |          -0.0003 |          45.3610 |          12.3315 |
[32m[20221213 23:25:16 @agent_ppo2.py:185][0m |          -0.0116 |          42.7769 |          12.3342 |
[32m[20221213 23:25:17 @agent_ppo2.py:185][0m |           0.0004 |          48.7095 |          12.2736 |
[32m[20221213 23:25:17 @agent_ppo2.py:185][0m |          -0.0105 |          42.7624 |          12.2590 |
[32m[20221213 23:25:17 @agent_ppo2.py:185][0m |          -0.0137 |          42.0921 |          12.2801 |
[32m[20221213 23:25:17 @agent_ppo2.py:185][0m |          -0.0146 |          42.0534 |          12.2579 |
[32m[20221213 23:25:17 @agent_ppo2.py:185][0m |          -0.0160 |          41.8742 |          12.2756 |
[32m[20221213 23:25:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.16
[32m[20221213 23:25:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.91
[32m[20221213 23:25:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.17
[32m[20221213 23:25:17 @agent_ppo2.py:143][0m Total time:      12.75 min
[32m[20221213 23:25:17 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221213 23:25:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2603 --------------------------#
[32m[20221213 23:25:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:17 @agent_ppo2.py:185][0m |          -0.0007 |          43.2402 |          12.2022 |
[32m[20221213 23:25:17 @agent_ppo2.py:185][0m |          -0.0038 |          41.4038 |          12.2223 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |          -0.0084 |          40.0841 |          12.2319 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |          -0.0106 |          39.4697 |          12.2549 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |          -0.0005 |          42.3611 |          12.2753 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |          -0.0122 |          39.0174 |          12.2661 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |          -0.0179 |          38.6794 |          12.2472 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |          -0.0130 |          38.3051 |          12.2594 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |           0.0035 |          42.4708 |          12.2600 |
[32m[20221213 23:25:18 @agent_ppo2.py:185][0m |          -0.0067 |          38.0067 |          12.2275 |
[32m[20221213 23:25:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.99
[32m[20221213 23:25:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.59
[32m[20221213 23:25:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.27
[32m[20221213 23:25:18 @agent_ppo2.py:143][0m Total time:      12.77 min
[32m[20221213 23:25:18 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221213 23:25:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2604 --------------------------#
[32m[20221213 23:25:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |           0.0030 |          51.5820 |          12.5712 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0063 |          50.2487 |          12.5302 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0072 |          49.9353 |          12.5211 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0086 |          49.7970 |          12.5222 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0108 |          49.6905 |          12.5048 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0098 |          49.5481 |          12.4894 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0112 |          49.4716 |          12.4582 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0121 |          49.3680 |          12.4804 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0043 |          52.4035 |          12.4643 |
[32m[20221213 23:25:19 @agent_ppo2.py:185][0m |          -0.0134 |          49.3241 |          12.4727 |
[32m[20221213 23:25:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:25:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.68
[32m[20221213 23:25:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.54
[32m[20221213 23:25:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.60
[32m[20221213 23:25:20 @agent_ppo2.py:143][0m Total time:      12.79 min
[32m[20221213 23:25:20 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221213 23:25:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2605 --------------------------#
[32m[20221213 23:25:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |           0.0035 |          41.4203 |          12.3414 |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |          -0.0065 |          37.0359 |          12.3607 |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |           0.0011 |          36.2985 |          12.3687 |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |          -0.0096 |          34.6438 |          12.3474 |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |          -0.0114 |          34.2881 |          12.3534 |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |          -0.0184 |          34.0728 |          12.3269 |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |          -0.0118 |          33.5961 |          12.3561 |
[32m[20221213 23:25:20 @agent_ppo2.py:185][0m |          -0.0150 |          33.3022 |          12.3603 |
[32m[20221213 23:25:21 @agent_ppo2.py:185][0m |          -0.0151 |          33.0796 |          12.3526 |
[32m[20221213 23:25:21 @agent_ppo2.py:185][0m |          -0.0171 |          33.0244 |          12.3402 |
[32m[20221213 23:25:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.02
[32m[20221213 23:25:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.31
[32m[20221213 23:25:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.44
[32m[20221213 23:25:21 @agent_ppo2.py:143][0m Total time:      12.81 min
[32m[20221213 23:25:21 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221213 23:25:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2606 --------------------------#
[32m[20221213 23:25:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:21 @agent_ppo2.py:185][0m |           0.0028 |          44.6068 |          12.3255 |
[32m[20221213 23:25:21 @agent_ppo2.py:185][0m |           0.0091 |          47.3259 |          12.3240 |
[32m[20221213 23:25:21 @agent_ppo2.py:185][0m |          -0.0042 |          43.6030 |          12.3105 |
[32m[20221213 23:25:21 @agent_ppo2.py:185][0m |          -0.0091 |          43.3095 |          12.2973 |
[32m[20221213 23:25:21 @agent_ppo2.py:185][0m |          -0.0077 |          43.2057 |          12.2853 |
[32m[20221213 23:25:22 @agent_ppo2.py:185][0m |          -0.0085 |          43.0549 |          12.2651 |
[32m[20221213 23:25:22 @agent_ppo2.py:185][0m |          -0.0100 |          42.9837 |          12.2404 |
[32m[20221213 23:25:22 @agent_ppo2.py:185][0m |          -0.0123 |          43.0595 |          12.2564 |
[32m[20221213 23:25:22 @agent_ppo2.py:185][0m |          -0.0115 |          42.9363 |          12.2742 |
[32m[20221213 23:25:22 @agent_ppo2.py:185][0m |          -0.0095 |          42.8473 |          12.2762 |
[32m[20221213 23:25:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:25:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.06
[32m[20221213 23:25:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.75
[32m[20221213 23:25:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.81
[32m[20221213 23:25:22 @agent_ppo2.py:143][0m Total time:      12.83 min
[32m[20221213 23:25:22 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221213 23:25:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2607 --------------------------#
[32m[20221213 23:25:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:22 @agent_ppo2.py:185][0m |           0.0015 |          43.4100 |          12.0537 |
[32m[20221213 23:25:22 @agent_ppo2.py:185][0m |          -0.0103 |          38.2017 |          11.9778 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0053 |          37.1990 |          11.9822 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0097 |          36.4981 |          11.9493 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0056 |          36.3641 |          11.8916 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0019 |          37.3368 |          11.9297 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0080 |          35.3417 |          11.8875 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0136 |          35.2653 |          11.8931 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0117 |          35.2247 |          11.8435 |
[32m[20221213 23:25:23 @agent_ppo2.py:185][0m |          -0.0139 |          34.9030 |          11.8417 |
[32m[20221213 23:25:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.63
[32m[20221213 23:25:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.31
[32m[20221213 23:25:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.75
[32m[20221213 23:25:23 @agent_ppo2.py:143][0m Total time:      12.85 min
[32m[20221213 23:25:23 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221213 23:25:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2608 --------------------------#
[32m[20221213 23:25:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |           0.0019 |          31.2835 |          12.2205 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0048 |          28.5594 |          12.2246 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0031 |          27.8151 |          12.2347 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0106 |          27.2794 |          12.2340 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0092 |          27.1278 |          12.2323 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0104 |          26.8145 |          12.2510 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0110 |          26.3856 |          12.2399 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0151 |          26.0214 |          12.2467 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0171 |          25.8806 |          12.2384 |
[32m[20221213 23:25:24 @agent_ppo2.py:185][0m |          -0.0111 |          25.7145 |          12.2555 |
[32m[20221213 23:25:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.91
[32m[20221213 23:25:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.22
[32m[20221213 23:25:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.06
[32m[20221213 23:25:25 @agent_ppo2.py:143][0m Total time:      12.88 min
[32m[20221213 23:25:25 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221213 23:25:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2609 --------------------------#
[32m[20221213 23:25:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |           0.0007 |          54.4213 |          12.2854 |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |          -0.0037 |          50.9202 |          12.3088 |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |          -0.0098 |          50.0173 |          12.3338 |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |          -0.0118 |          48.7456 |          12.3642 |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |          -0.0122 |          48.2064 |          12.3546 |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |          -0.0126 |          47.8813 |          12.3549 |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |          -0.0154 |          47.4682 |          12.3726 |
[32m[20221213 23:25:25 @agent_ppo2.py:185][0m |          -0.0144 |          47.2657 |          12.3561 |
[32m[20221213 23:25:26 @agent_ppo2.py:185][0m |          -0.0172 |          46.9927 |          12.3827 |
[32m[20221213 23:25:26 @agent_ppo2.py:185][0m |          -0.0115 |          47.0434 |          12.4005 |
[32m[20221213 23:25:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.26
[32m[20221213 23:25:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.35
[32m[20221213 23:25:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.75
[32m[20221213 23:25:26 @agent_ppo2.py:143][0m Total time:      12.90 min
[32m[20221213 23:25:26 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221213 23:25:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2610 --------------------------#
[32m[20221213 23:25:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:25:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:26 @agent_ppo2.py:185][0m |           0.0054 |          57.1491 |          12.2651 |
[32m[20221213 23:25:26 @agent_ppo2.py:185][0m |          -0.0058 |          53.7184 |          12.2743 |
[32m[20221213 23:25:26 @agent_ppo2.py:185][0m |          -0.0053 |          54.0081 |          12.2918 |
[32m[20221213 23:25:26 @agent_ppo2.py:185][0m |          -0.0069 |          53.0319 |          12.2873 |
[32m[20221213 23:25:26 @agent_ppo2.py:185][0m |          -0.0062 |          54.0770 |          12.2980 |
[32m[20221213 23:25:27 @agent_ppo2.py:185][0m |          -0.0072 |          52.7861 |          12.3023 |
[32m[20221213 23:25:27 @agent_ppo2.py:185][0m |          -0.0097 |          52.7065 |          12.3076 |
[32m[20221213 23:25:27 @agent_ppo2.py:185][0m |          -0.0119 |          52.5961 |          12.3209 |
[32m[20221213 23:25:27 @agent_ppo2.py:185][0m |          -0.0073 |          52.8460 |          12.3567 |
[32m[20221213 23:25:27 @agent_ppo2.py:185][0m |          -0.0146 |          52.3640 |          12.3431 |
[32m[20221213 23:25:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:25:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.25
[32m[20221213 23:25:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.59
[32m[20221213 23:25:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.03
[32m[20221213 23:25:27 @agent_ppo2.py:143][0m Total time:      12.92 min
[32m[20221213 23:25:27 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221213 23:25:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2611 --------------------------#
[32m[20221213 23:25:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:27 @agent_ppo2.py:185][0m |           0.0028 |          59.6246 |          12.3421 |
[32m[20221213 23:25:27 @agent_ppo2.py:185][0m |          -0.0044 |          57.2948 |          12.3546 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0064 |          56.3591 |          12.3110 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0103 |          55.8698 |          12.3646 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0092 |          55.4990 |          12.3563 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0120 |          54.9759 |          12.3864 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0118 |          54.8272 |          12.3644 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0130 |          54.3741 |          12.3675 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0127 |          54.3203 |          12.3876 |
[32m[20221213 23:25:28 @agent_ppo2.py:185][0m |          -0.0119 |          54.0943 |          12.3998 |
[32m[20221213 23:25:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:25:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.55
[32m[20221213 23:25:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.39
[32m[20221213 23:25:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.17
[32m[20221213 23:25:28 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221213 23:25:28 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221213 23:25:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2612 --------------------------#
[32m[20221213 23:25:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0035 |          28.3961 |          12.7744 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |           0.0014 |          26.7749 |          12.7067 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0080 |          25.5904 |          12.6866 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0107 |          25.3132 |          12.6804 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0135 |          24.9261 |          12.6460 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0114 |          24.7577 |          12.6625 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0116 |          24.6455 |          12.6253 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0143 |          24.4853 |          12.5913 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0115 |          24.3322 |          12.5568 |
[32m[20221213 23:25:29 @agent_ppo2.py:185][0m |          -0.0104 |          24.3890 |          12.5581 |
[32m[20221213 23:25:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:25:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.60
[32m[20221213 23:25:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.99
[32m[20221213 23:25:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.80
[32m[20221213 23:25:30 @agent_ppo2.py:143][0m Total time:      12.96 min
[32m[20221213 23:25:30 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221213 23:25:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2613 --------------------------#
[32m[20221213 23:25:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |           0.0047 |          50.1033 |          11.9354 |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |          -0.0064 |          47.8856 |          11.9214 |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |          -0.0075 |          46.9885 |          11.8887 |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |          -0.0055 |          47.1337 |          11.9464 |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |          -0.0119 |          46.0340 |          11.9421 |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |           0.0002 |          48.1905 |          11.9278 |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |          -0.0121 |          45.4027 |          11.9474 |
[32m[20221213 23:25:30 @agent_ppo2.py:185][0m |          -0.0097 |          45.0377 |          11.9486 |
[32m[20221213 23:25:31 @agent_ppo2.py:185][0m |          -0.0125 |          44.9273 |          11.9374 |
[32m[20221213 23:25:31 @agent_ppo2.py:185][0m |          -0.0141 |          44.7061 |          11.9386 |
[32m[20221213 23:25:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.93
[32m[20221213 23:25:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.43
[32m[20221213 23:25:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.07
[32m[20221213 23:25:31 @agent_ppo2.py:143][0m Total time:      12.98 min
[32m[20221213 23:25:31 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221213 23:25:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2614 --------------------------#
[32m[20221213 23:25:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:31 @agent_ppo2.py:185][0m |          -0.0046 |          52.7692 |          12.2612 |
[32m[20221213 23:25:31 @agent_ppo2.py:185][0m |          -0.0035 |          49.4133 |          12.2128 |
[32m[20221213 23:25:31 @agent_ppo2.py:185][0m |          -0.0067 |          48.8096 |          12.2724 |
[32m[20221213 23:25:31 @agent_ppo2.py:185][0m |           0.0017 |          53.6512 |          12.3009 |
[32m[20221213 23:25:32 @agent_ppo2.py:185][0m |          -0.0113 |          47.3449 |          12.2916 |
[32m[20221213 23:25:32 @agent_ppo2.py:185][0m |          -0.0131 |          46.8802 |          12.3713 |
[32m[20221213 23:25:32 @agent_ppo2.py:185][0m |          -0.0134 |          46.5752 |          12.3523 |
[32m[20221213 23:25:32 @agent_ppo2.py:185][0m |          -0.0182 |          46.3557 |          12.3474 |
[32m[20221213 23:25:32 @agent_ppo2.py:185][0m |          -0.0107 |          45.9624 |          12.3866 |
[32m[20221213 23:25:32 @agent_ppo2.py:185][0m |          -0.0159 |          45.8527 |          12.3995 |
[32m[20221213 23:25:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:25:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.65
[32m[20221213 23:25:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.27
[32m[20221213 23:25:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.87
[32m[20221213 23:25:32 @agent_ppo2.py:143][0m Total time:      13.00 min
[32m[20221213 23:25:32 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221213 23:25:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2615 --------------------------#
[32m[20221213 23:25:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:32 @agent_ppo2.py:185][0m |           0.0000 |          43.5564 |          12.4985 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0018 |          40.4903 |          12.4790 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0072 |          39.2647 |          12.4653 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0138 |          38.7699 |          12.5141 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0098 |          38.3847 |          12.4504 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0121 |          38.2161 |          12.4003 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0107 |          38.1651 |          12.3903 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0086 |          37.9202 |          12.4255 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0155 |          37.6228 |          12.4102 |
[32m[20221213 23:25:33 @agent_ppo2.py:185][0m |          -0.0001 |          39.4343 |          12.4058 |
[32m[20221213 23:25:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:25:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.28
[32m[20221213 23:25:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.18
[32m[20221213 23:25:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.61
[32m[20221213 23:25:33 @agent_ppo2.py:143][0m Total time:      13.02 min
[32m[20221213 23:25:33 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221213 23:25:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2616 --------------------------#
[32m[20221213 23:25:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |           0.0110 |          38.3590 |          12.1454 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0066 |          33.0664 |          12.1315 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0035 |          32.0286 |          12.0965 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0101 |          31.3730 |          12.0884 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0110 |          31.0617 |          12.0653 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0104 |          30.5784 |          12.0516 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0117 |          30.4158 |          12.0277 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0051 |          30.8477 |          12.0442 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0071 |          30.2012 |          12.0342 |
[32m[20221213 23:25:34 @agent_ppo2.py:185][0m |          -0.0129 |          29.6401 |          12.0299 |
[32m[20221213 23:25:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.16
[32m[20221213 23:25:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.59
[32m[20221213 23:25:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.06
[32m[20221213 23:25:35 @agent_ppo2.py:143][0m Total time:      13.04 min
[32m[20221213 23:25:35 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221213 23:25:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2617 --------------------------#
[32m[20221213 23:25:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:35 @agent_ppo2.py:185][0m |           0.0017 |          37.5743 |          12.0622 |
[32m[20221213 23:25:35 @agent_ppo2.py:185][0m |          -0.0078 |          34.3316 |          12.0806 |
[32m[20221213 23:25:35 @agent_ppo2.py:185][0m |          -0.0088 |          33.3958 |          12.0486 |
[32m[20221213 23:25:35 @agent_ppo2.py:185][0m |          -0.0092 |          32.5973 |          12.0654 |
[32m[20221213 23:25:35 @agent_ppo2.py:185][0m |          -0.0116 |          32.3042 |          12.0637 |
[32m[20221213 23:25:35 @agent_ppo2.py:185][0m |           0.0023 |          35.8703 |          12.0614 |
[32m[20221213 23:25:35 @agent_ppo2.py:185][0m |          -0.0072 |          31.6041 |          12.1283 |
[32m[20221213 23:25:36 @agent_ppo2.py:185][0m |          -0.0088 |          31.3235 |          12.0815 |
[32m[20221213 23:25:36 @agent_ppo2.py:185][0m |          -0.0148 |          31.0325 |          12.0932 |
[32m[20221213 23:25:36 @agent_ppo2.py:185][0m |          -0.0118 |          30.7922 |          12.1001 |
[32m[20221213 23:25:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.09
[32m[20221213 23:25:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.30
[32m[20221213 23:25:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.04
[32m[20221213 23:25:36 @agent_ppo2.py:143][0m Total time:      13.06 min
[32m[20221213 23:25:36 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221213 23:25:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2618 --------------------------#
[32m[20221213 23:25:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:36 @agent_ppo2.py:185][0m |           0.0002 |          39.1639 |          12.2698 |
[32m[20221213 23:25:36 @agent_ppo2.py:185][0m |           0.0088 |          41.1003 |          12.2775 |
[32m[20221213 23:25:36 @agent_ppo2.py:185][0m |          -0.0068 |          37.0030 |          12.2297 |
[32m[20221213 23:25:36 @agent_ppo2.py:185][0m |          -0.0044 |          36.6204 |          12.2329 |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |          -0.0095 |          36.4609 |          12.2868 |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |          -0.0095 |          36.3530 |          12.2351 |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |          -0.0043 |          36.2964 |          12.2722 |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |          -0.0124 |          36.2350 |          12.2547 |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |          -0.0064 |          36.6544 |          12.2307 |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |          -0.0076 |          36.1191 |          12.2656 |
[32m[20221213 23:25:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:25:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.86
[32m[20221213 23:25:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.98
[32m[20221213 23:25:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.15
[32m[20221213 23:25:37 @agent_ppo2.py:143][0m Total time:      13.08 min
[32m[20221213 23:25:37 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221213 23:25:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2619 --------------------------#
[32m[20221213 23:25:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |           0.0004 |          50.3579 |          12.1558 |
[32m[20221213 23:25:37 @agent_ppo2.py:185][0m |          -0.0044 |          47.4647 |          12.1439 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0076 |          46.7853 |          12.1721 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0094 |          46.3622 |          12.1432 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0108 |          46.0423 |          12.1571 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0115 |          45.8306 |          12.1627 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0137 |          45.5652 |          12.1083 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0157 |          45.5030 |          12.1140 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0139 |          45.3493 |          12.1139 |
[32m[20221213 23:25:38 @agent_ppo2.py:185][0m |          -0.0083 |          47.0271 |          12.1293 |
[32m[20221213 23:25:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.06
[32m[20221213 23:25:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.75
[32m[20221213 23:25:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.96
[32m[20221213 23:25:38 @agent_ppo2.py:143][0m Total time:      13.11 min
[32m[20221213 23:25:38 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221213 23:25:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2620 --------------------------#
[32m[20221213 23:25:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:25:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |           0.0005 |          44.3279 |          12.3679 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0021 |          42.6142 |          12.3418 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0054 |          41.4117 |          12.3876 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0077 |          41.0039 |          12.3558 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0083 |          40.7099 |          12.3392 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0099 |          40.3306 |          12.3473 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0035 |          41.5949 |          12.3435 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0116 |          39.8187 |          12.3565 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0113 |          39.6213 |          12.3628 |
[32m[20221213 23:25:39 @agent_ppo2.py:185][0m |          -0.0126 |          39.5117 |          12.3678 |
[32m[20221213 23:25:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:25:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.84
[32m[20221213 23:25:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.84
[32m[20221213 23:25:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.08
[32m[20221213 23:25:40 @agent_ppo2.py:143][0m Total time:      13.13 min
[32m[20221213 23:25:40 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221213 23:25:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2621 --------------------------#
[32m[20221213 23:25:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:40 @agent_ppo2.py:185][0m |           0.0024 |          48.8063 |          12.2445 |
[32m[20221213 23:25:40 @agent_ppo2.py:185][0m |          -0.0059 |          46.6554 |          12.2736 |
[32m[20221213 23:25:40 @agent_ppo2.py:185][0m |          -0.0096 |          45.0444 |          12.2830 |
[32m[20221213 23:25:40 @agent_ppo2.py:185][0m |          -0.0077 |          43.8737 |          12.2386 |
[32m[20221213 23:25:40 @agent_ppo2.py:185][0m |          -0.0077 |          43.2691 |          12.2627 |
[32m[20221213 23:25:40 @agent_ppo2.py:185][0m |          -0.0110 |          42.6774 |          12.2448 |
[32m[20221213 23:25:40 @agent_ppo2.py:185][0m |          -0.0158 |          42.2516 |          12.2251 |
[32m[20221213 23:25:41 @agent_ppo2.py:185][0m |          -0.0116 |          41.9522 |          12.2515 |
[32m[20221213 23:25:41 @agent_ppo2.py:185][0m |          -0.0148 |          41.6085 |          12.2524 |
[32m[20221213 23:25:41 @agent_ppo2.py:185][0m |          -0.0099 |          42.9954 |          12.2339 |
[32m[20221213 23:25:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:25:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.73
[32m[20221213 23:25:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.04
[32m[20221213 23:25:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.73
[32m[20221213 23:25:41 @agent_ppo2.py:143][0m Total time:      13.15 min
[32m[20221213 23:25:41 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221213 23:25:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2622 --------------------------#
[32m[20221213 23:25:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:41 @agent_ppo2.py:185][0m |          -0.0030 |          60.3414 |          12.0204 |
[32m[20221213 23:25:41 @agent_ppo2.py:185][0m |          -0.0060 |          58.4468 |          12.0385 |
[32m[20221213 23:25:41 @agent_ppo2.py:185][0m |          -0.0074 |          58.1651 |          12.0624 |
[32m[20221213 23:25:41 @agent_ppo2.py:185][0m |          -0.0068 |          57.7410 |          12.0764 |
[32m[20221213 23:25:42 @agent_ppo2.py:185][0m |          -0.0115 |          57.5627 |          12.0725 |
[32m[20221213 23:25:42 @agent_ppo2.py:185][0m |          -0.0106 |          57.4951 |          12.0672 |
[32m[20221213 23:25:42 @agent_ppo2.py:185][0m |          -0.0115 |          57.1184 |          12.1030 |
[32m[20221213 23:25:42 @agent_ppo2.py:185][0m |          -0.0097 |          57.0361 |          12.1150 |
[32m[20221213 23:25:42 @agent_ppo2.py:185][0m |          -0.0115 |          56.8887 |          12.1507 |
[32m[20221213 23:25:42 @agent_ppo2.py:185][0m |          -0.0081 |          57.1176 |          12.1250 |
[32m[20221213 23:25:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.73
[32m[20221213 23:25:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.01
[32m[20221213 23:25:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.51
[32m[20221213 23:25:42 @agent_ppo2.py:143][0m Total time:      13.17 min
[32m[20221213 23:25:42 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221213 23:25:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2623 --------------------------#
[32m[20221213 23:25:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:42 @agent_ppo2.py:185][0m |           0.0039 |          68.1185 |          12.4928 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0041 |          65.2455 |          12.4924 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0038 |          64.5848 |          12.4728 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0126 |          63.9116 |          12.4880 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0123 |          63.5329 |          12.4953 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0098 |          63.3257 |          12.5230 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0098 |          63.0419 |          12.4970 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0113 |          62.9292 |          12.4974 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0103 |          62.7130 |          12.4913 |
[32m[20221213 23:25:43 @agent_ppo2.py:185][0m |          -0.0069 |          63.0418 |          12.5039 |
[32m[20221213 23:25:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.52
[32m[20221213 23:25:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.81
[32m[20221213 23:25:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.59
[32m[20221213 23:25:43 @agent_ppo2.py:143][0m Total time:      13.19 min
[32m[20221213 23:25:43 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221213 23:25:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2624 --------------------------#
[32m[20221213 23:25:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0020 |          44.5130 |          12.5456 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0051 |          43.2972 |          12.5671 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0039 |          42.8538 |          12.5741 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0044 |          42.7626 |          12.5694 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0089 |          42.6670 |          12.5374 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0032 |          42.6136 |          12.5374 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0087 |          42.4972 |          12.5370 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0065 |          42.5644 |          12.5139 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0052 |          42.3922 |          12.5475 |
[32m[20221213 23:25:44 @agent_ppo2.py:185][0m |          -0.0090 |          42.2968 |          12.4980 |
[32m[20221213 23:25:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:25:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.56
[32m[20221213 23:25:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.95
[32m[20221213 23:25:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.30
[32m[20221213 23:25:45 @agent_ppo2.py:143][0m Total time:      13.21 min
[32m[20221213 23:25:45 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221213 23:25:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2625 --------------------------#
[32m[20221213 23:25:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:45 @agent_ppo2.py:185][0m |          -0.0020 |          51.2226 |          12.0489 |
[32m[20221213 23:25:45 @agent_ppo2.py:185][0m |          -0.0081 |          48.7116 |          12.0493 |
[32m[20221213 23:25:45 @agent_ppo2.py:185][0m |          -0.0070 |          48.0792 |          12.0684 |
[32m[20221213 23:25:45 @agent_ppo2.py:185][0m |          -0.0026 |          47.6301 |          12.0835 |
[32m[20221213 23:25:45 @agent_ppo2.py:185][0m |           0.0023 |          49.3124 |          11.9953 |
[32m[20221213 23:25:45 @agent_ppo2.py:185][0m |          -0.0033 |          47.3683 |          12.0454 |
[32m[20221213 23:25:45 @agent_ppo2.py:185][0m |          -0.0091 |          46.8426 |          12.0596 |
[32m[20221213 23:25:46 @agent_ppo2.py:185][0m |          -0.0112 |          46.7539 |          12.0311 |
[32m[20221213 23:25:46 @agent_ppo2.py:185][0m |          -0.0124 |          46.4067 |          12.0174 |
[32m[20221213 23:25:46 @agent_ppo2.py:185][0m |          -0.0132 |          46.9638 |          12.0175 |
[32m[20221213 23:25:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:25:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.01
[32m[20221213 23:25:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.55
[32m[20221213 23:25:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.27
[32m[20221213 23:25:46 @agent_ppo2.py:143][0m Total time:      13.23 min
[32m[20221213 23:25:46 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221213 23:25:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2626 --------------------------#
[32m[20221213 23:25:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:46 @agent_ppo2.py:185][0m |          -0.0006 |          51.6819 |          12.3339 |
[32m[20221213 23:25:46 @agent_ppo2.py:185][0m |          -0.0024 |          50.9623 |          12.3272 |
[32m[20221213 23:25:46 @agent_ppo2.py:185][0m |          -0.0062 |          50.7566 |          12.2877 |
[32m[20221213 23:25:46 @agent_ppo2.py:185][0m |          -0.0073 |          50.4541 |          12.3446 |
[32m[20221213 23:25:47 @agent_ppo2.py:185][0m |          -0.0089 |          50.4356 |          12.3456 |
[32m[20221213 23:25:47 @agent_ppo2.py:185][0m |          -0.0071 |          50.3116 |          12.3886 |
[32m[20221213 23:25:47 @agent_ppo2.py:185][0m |          -0.0056 |          50.2508 |          12.3803 |
[32m[20221213 23:25:47 @agent_ppo2.py:185][0m |          -0.0094 |          50.0709 |          12.4212 |
[32m[20221213 23:25:47 @agent_ppo2.py:185][0m |          -0.0103 |          50.0845 |          12.3691 |
[32m[20221213 23:25:47 @agent_ppo2.py:185][0m |          -0.0102 |          50.0970 |          12.4277 |
[32m[20221213 23:25:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.23
[32m[20221213 23:25:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.03
[32m[20221213 23:25:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.21
[32m[20221213 23:25:47 @agent_ppo2.py:143][0m Total time:      13.25 min
[32m[20221213 23:25:47 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221213 23:25:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2627 --------------------------#
[32m[20221213 23:25:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:47 @agent_ppo2.py:185][0m |           0.0050 |          53.6256 |          12.5073 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0073 |          50.0109 |          12.5549 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0098 |          49.3690 |          12.5547 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0125 |          49.0647 |          12.5131 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0104 |          48.9537 |          12.4734 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0144 |          48.6310 |          12.4877 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0150 |          48.5608 |          12.4792 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0109 |          48.6294 |          12.4733 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0150 |          48.3103 |          12.4777 |
[32m[20221213 23:25:48 @agent_ppo2.py:185][0m |          -0.0044 |          54.0425 |          12.4968 |
[32m[20221213 23:25:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.18
[32m[20221213 23:25:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.44
[32m[20221213 23:25:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.57
[32m[20221213 23:25:48 @agent_ppo2.py:143][0m Total time:      13.27 min
[32m[20221213 23:25:48 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221213 23:25:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2628 --------------------------#
[32m[20221213 23:25:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |           0.0001 |          52.9110 |          12.2605 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0057 |          50.4376 |          12.2426 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0074 |          49.6060 |          12.2886 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0069 |          49.1588 |          12.2663 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0068 |          49.0057 |          12.2726 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0090 |          48.7019 |          12.3237 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0073 |          48.4068 |          12.3164 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |           0.0011 |          54.1010 |          12.3346 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0041 |          48.1409 |          12.3442 |
[32m[20221213 23:25:49 @agent_ppo2.py:185][0m |          -0.0096 |          47.6840 |          12.3616 |
[32m[20221213 23:25:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:25:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.64
[32m[20221213 23:25:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.13
[32m[20221213 23:25:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.26
[32m[20221213 23:25:50 @agent_ppo2.py:143][0m Total time:      13.29 min
[32m[20221213 23:25:50 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221213 23:25:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2629 --------------------------#
[32m[20221213 23:25:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:50 @agent_ppo2.py:185][0m |           0.0007 |          57.8719 |          12.1702 |
[32m[20221213 23:25:50 @agent_ppo2.py:185][0m |           0.0011 |          51.0492 |          12.1263 |
[32m[20221213 23:25:50 @agent_ppo2.py:185][0m |          -0.0088 |          46.3270 |          12.1000 |
[32m[20221213 23:25:50 @agent_ppo2.py:185][0m |          -0.0078 |          45.6842 |          12.0672 |
[32m[20221213 23:25:50 @agent_ppo2.py:185][0m |           0.0052 |          51.9446 |          12.0816 |
[32m[20221213 23:25:50 @agent_ppo2.py:185][0m |          -0.0091 |          44.7970 |          12.0240 |
[32m[20221213 23:25:50 @agent_ppo2.py:185][0m |          -0.0080 |          44.1936 |          12.0472 |
[32m[20221213 23:25:51 @agent_ppo2.py:185][0m |          -0.0074 |          43.9839 |          12.0134 |
[32m[20221213 23:25:51 @agent_ppo2.py:185][0m |          -0.0099 |          43.6864 |          12.0622 |
[32m[20221213 23:25:51 @agent_ppo2.py:185][0m |          -0.0200 |          43.4427 |          12.0090 |
[32m[20221213 23:25:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.08
[32m[20221213 23:25:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.50
[32m[20221213 23:25:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.27
[32m[20221213 23:25:51 @agent_ppo2.py:143][0m Total time:      13.31 min
[32m[20221213 23:25:51 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221213 23:25:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2630 --------------------------#
[32m[20221213 23:25:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:25:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:51 @agent_ppo2.py:185][0m |           0.0085 |          56.8039 |          12.1002 |
[32m[20221213 23:25:51 @agent_ppo2.py:185][0m |          -0.0040 |          51.7801 |          12.0439 |
[32m[20221213 23:25:51 @agent_ppo2.py:185][0m |          -0.0058 |          50.9640 |          12.0713 |
[32m[20221213 23:25:51 @agent_ppo2.py:185][0m |          -0.0071 |          50.7014 |          12.0949 |
[32m[20221213 23:25:52 @agent_ppo2.py:185][0m |          -0.0102 |          50.3744 |          12.1054 |
[32m[20221213 23:25:52 @agent_ppo2.py:185][0m |          -0.0072 |          50.1399 |          12.1069 |
[32m[20221213 23:25:52 @agent_ppo2.py:185][0m |          -0.0101 |          50.0135 |          12.1127 |
[32m[20221213 23:25:52 @agent_ppo2.py:185][0m |          -0.0138 |          49.8234 |          12.1645 |
[32m[20221213 23:25:52 @agent_ppo2.py:185][0m |          -0.0079 |          49.8655 |          12.1734 |
[32m[20221213 23:25:52 @agent_ppo2.py:185][0m |          -0.0101 |          49.6773 |          12.1881 |
[32m[20221213 23:25:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:25:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.20
[32m[20221213 23:25:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.16
[32m[20221213 23:25:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.67
[32m[20221213 23:25:52 @agent_ppo2.py:143][0m Total time:      13.34 min
[32m[20221213 23:25:52 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221213 23:25:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2631 --------------------------#
[32m[20221213 23:25:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:52 @agent_ppo2.py:185][0m |           0.0112 |          51.3999 |          12.4272 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0051 |          44.9452 |          12.4266 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |           0.0026 |          45.8521 |          12.4096 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0104 |          43.0838 |          12.4290 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0118 |          42.6610 |          12.4472 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0119 |          42.4305 |          12.4840 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0086 |          42.0227 |          12.4805 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0081 |          43.7680 |          12.4922 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0097 |          41.7945 |          12.4966 |
[32m[20221213 23:25:53 @agent_ppo2.py:185][0m |          -0.0111 |          41.5909 |          12.5121 |
[32m[20221213 23:25:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:25:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.67
[32m[20221213 23:25:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.91
[32m[20221213 23:25:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.76
[32m[20221213 23:25:53 @agent_ppo2.py:143][0m Total time:      13.36 min
[32m[20221213 23:25:53 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221213 23:25:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2632 --------------------------#
[32m[20221213 23:25:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |           0.0046 |          61.4609 |          12.6407 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0031 |          56.2419 |          12.6530 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0062 |          54.7576 |          12.6483 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0085 |          53.8462 |          12.6340 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0101 |          53.2388 |          12.6528 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0107 |          52.7644 |          12.6406 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0109 |          52.4857 |          12.6357 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0146 |          52.3525 |          12.6508 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0136 |          51.8226 |          12.6435 |
[32m[20221213 23:25:54 @agent_ppo2.py:185][0m |          -0.0074 |          54.7572 |          12.6549 |
[32m[20221213 23:25:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 308.46
[32m[20221213 23:25:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.64
[32m[20221213 23:25:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.23
[32m[20221213 23:25:55 @agent_ppo2.py:143][0m Total time:      13.38 min
[32m[20221213 23:25:55 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221213 23:25:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2633 --------------------------#
[32m[20221213 23:25:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:55 @agent_ppo2.py:185][0m |           0.0012 |          54.4550 |          12.2411 |
[32m[20221213 23:25:55 @agent_ppo2.py:185][0m |           0.0030 |          52.2483 |          12.2776 |
[32m[20221213 23:25:55 @agent_ppo2.py:185][0m |          -0.0068 |          50.9621 |          12.2939 |
[32m[20221213 23:25:55 @agent_ppo2.py:185][0m |          -0.0053 |          50.2711 |          12.2797 |
[32m[20221213 23:25:55 @agent_ppo2.py:185][0m |          -0.0056 |          50.0273 |          12.2716 |
[32m[20221213 23:25:55 @agent_ppo2.py:185][0m |          -0.0063 |          49.6559 |          12.2757 |
[32m[20221213 23:25:55 @agent_ppo2.py:185][0m |          -0.0081 |          49.4912 |          12.2816 |
[32m[20221213 23:25:56 @agent_ppo2.py:185][0m |          -0.0080 |          49.2080 |          12.3113 |
[32m[20221213 23:25:56 @agent_ppo2.py:185][0m |          -0.0096 |          48.9538 |          12.2915 |
[32m[20221213 23:25:56 @agent_ppo2.py:185][0m |          -0.0076 |          49.1123 |          12.2871 |
[32m[20221213 23:25:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.75
[32m[20221213 23:25:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.95
[32m[20221213 23:25:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.86
[32m[20221213 23:25:56 @agent_ppo2.py:143][0m Total time:      13.40 min
[32m[20221213 23:25:56 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221213 23:25:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2634 --------------------------#
[32m[20221213 23:25:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:56 @agent_ppo2.py:185][0m |           0.0031 |          49.3663 |          12.4499 |
[32m[20221213 23:25:56 @agent_ppo2.py:185][0m |           0.0048 |          48.0095 |          12.4607 |
[32m[20221213 23:25:56 @agent_ppo2.py:185][0m |          -0.0030 |          45.6722 |          12.4497 |
[32m[20221213 23:25:56 @agent_ppo2.py:185][0m |          -0.0038 |          45.0185 |          12.5057 |
[32m[20221213 23:25:57 @agent_ppo2.py:185][0m |          -0.0105 |          44.3661 |          12.4920 |
[32m[20221213 23:25:57 @agent_ppo2.py:185][0m |          -0.0124 |          43.9523 |          12.4733 |
[32m[20221213 23:25:57 @agent_ppo2.py:185][0m |          -0.0092 |          43.5985 |          12.4607 |
[32m[20221213 23:25:57 @agent_ppo2.py:185][0m |          -0.0136 |          43.2588 |          12.4713 |
[32m[20221213 23:25:57 @agent_ppo2.py:185][0m |          -0.0142 |          42.8871 |          12.4860 |
[32m[20221213 23:25:57 @agent_ppo2.py:185][0m |          -0.0162 |          42.7287 |          12.4882 |
[32m[20221213 23:25:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:25:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.18
[32m[20221213 23:25:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.72
[32m[20221213 23:25:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.74
[32m[20221213 23:25:57 @agent_ppo2.py:143][0m Total time:      13.42 min
[32m[20221213 23:25:57 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221213 23:25:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2635 --------------------------#
[32m[20221213 23:25:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:25:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:57 @agent_ppo2.py:185][0m |           0.0021 |          55.0298 |          12.6820 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |           0.0057 |          55.1339 |          12.6177 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0003 |          54.2542 |          12.5887 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0048 |          50.7672 |          12.5573 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0064 |          50.1331 |          12.5451 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0087 |          50.4206 |          12.5214 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0083 |          49.5063 |          12.4910 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0065 |          49.1933 |          12.4791 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0119 |          48.9038 |          12.4688 |
[32m[20221213 23:25:58 @agent_ppo2.py:185][0m |          -0.0101 |          48.8673 |          12.4588 |
[32m[20221213 23:25:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:25:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.38
[32m[20221213 23:25:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.11
[32m[20221213 23:25:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.22
[32m[20221213 23:25:58 @agent_ppo2.py:143][0m Total time:      13.44 min
[32m[20221213 23:25:58 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221213 23:25:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2636 --------------------------#
[32m[20221213 23:25:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:25:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |           0.0246 |          64.7270 |          12.3418 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0025 |          52.6950 |          12.3971 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0078 |          50.6921 |          12.3359 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0094 |          49.6232 |          12.2873 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0092 |          49.2462 |          12.2874 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0116 |          48.5549 |          12.2819 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0143 |          48.4058 |          12.2586 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0128 |          48.1860 |          12.2466 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0154 |          48.0865 |          12.2184 |
[32m[20221213 23:25:59 @agent_ppo2.py:185][0m |          -0.0165 |          47.7652 |          12.2215 |
[32m[20221213 23:25:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.16
[32m[20221213 23:26:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.93
[32m[20221213 23:26:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.83
[32m[20221213 23:26:00 @agent_ppo2.py:143][0m Total time:      13.46 min
[32m[20221213 23:26:00 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221213 23:26:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2637 --------------------------#
[32m[20221213 23:26:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:00 @agent_ppo2.py:185][0m |           0.0065 |          51.1369 |          12.4587 |
[32m[20221213 23:26:00 @agent_ppo2.py:185][0m |          -0.0079 |          40.6355 |          12.4865 |
[32m[20221213 23:26:00 @agent_ppo2.py:185][0m |          -0.0073 |          39.1377 |          12.4265 |
[32m[20221213 23:26:00 @agent_ppo2.py:185][0m |          -0.0140 |          38.5693 |          12.4364 |
[32m[20221213 23:26:00 @agent_ppo2.py:185][0m |          -0.0106 |          38.2913 |          12.4289 |
[32m[20221213 23:26:00 @agent_ppo2.py:185][0m |          -0.0083 |          37.9196 |          12.4315 |
[32m[20221213 23:26:00 @agent_ppo2.py:185][0m |          -0.0147 |          37.6130 |          12.4225 |
[32m[20221213 23:26:01 @agent_ppo2.py:185][0m |          -0.0082 |          39.9833 |          12.4061 |
[32m[20221213 23:26:01 @agent_ppo2.py:185][0m |          -0.0147 |          37.1870 |          12.4093 |
[32m[20221213 23:26:01 @agent_ppo2.py:185][0m |          -0.0079 |          41.4011 |          12.3980 |
[32m[20221213 23:26:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.45
[32m[20221213 23:26:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.06
[32m[20221213 23:26:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.26
[32m[20221213 23:26:01 @agent_ppo2.py:143][0m Total time:      13.48 min
[32m[20221213 23:26:01 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221213 23:26:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2638 --------------------------#
[32m[20221213 23:26:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:01 @agent_ppo2.py:185][0m |           0.0055 |          42.6267 |          11.7161 |
[32m[20221213 23:26:01 @agent_ppo2.py:185][0m |          -0.0005 |          39.0730 |          11.7346 |
[32m[20221213 23:26:01 @agent_ppo2.py:185][0m |           0.0065 |          42.6127 |          11.7771 |
[32m[20221213 23:26:01 @agent_ppo2.py:185][0m |          -0.0104 |          37.8042 |          11.8017 |
[32m[20221213 23:26:02 @agent_ppo2.py:185][0m |          -0.0125 |          37.1295 |          11.7538 |
[32m[20221213 23:26:02 @agent_ppo2.py:185][0m |          -0.0174 |          36.8934 |          11.7748 |
[32m[20221213 23:26:02 @agent_ppo2.py:185][0m |          -0.0134 |          36.6060 |          11.7687 |
[32m[20221213 23:26:02 @agent_ppo2.py:185][0m |          -0.0178 |          36.4519 |          11.7822 |
[32m[20221213 23:26:02 @agent_ppo2.py:185][0m |          -0.0174 |          36.7279 |          11.7793 |
[32m[20221213 23:26:02 @agent_ppo2.py:185][0m |          -0.0148 |          36.1685 |          11.8014 |
[32m[20221213 23:26:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:26:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.48
[32m[20221213 23:26:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.49
[32m[20221213 23:26:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.02
[32m[20221213 23:26:02 @agent_ppo2.py:143][0m Total time:      13.50 min
[32m[20221213 23:26:02 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221213 23:26:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2639 --------------------------#
[32m[20221213 23:26:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:02 @agent_ppo2.py:185][0m |          -0.0034 |          56.1773 |          12.0767 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0080 |          53.8239 |          12.0865 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0116 |          52.5707 |          12.1814 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0024 |          54.4139 |          12.1747 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0107 |          51.3469 |          12.1749 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0126 |          50.8530 |          12.1895 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0121 |          50.5287 |          12.1735 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0134 |          50.3020 |          12.2100 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0134 |          50.0605 |          12.1822 |
[32m[20221213 23:26:03 @agent_ppo2.py:185][0m |          -0.0145 |          49.7988 |          12.1920 |
[32m[20221213 23:26:03 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:26:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.25
[32m[20221213 23:26:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.01
[32m[20221213 23:26:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.89
[32m[20221213 23:26:03 @agent_ppo2.py:143][0m Total time:      13.52 min
[32m[20221213 23:26:03 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221213 23:26:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2640 --------------------------#
[32m[20221213 23:26:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:26:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |           0.0001 |          42.2859 |          12.1807 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0039 |          33.2570 |          12.2088 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0050 |          31.6763 |          12.1649 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0052 |          30.8855 |          12.1858 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0053 |          30.3742 |          12.1787 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0091 |          29.7393 |          12.1734 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0085 |          29.8461 |          12.1880 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0123 |          29.4051 |          12.1846 |
[32m[20221213 23:26:04 @agent_ppo2.py:185][0m |          -0.0075 |          28.8955 |          12.1489 |
[32m[20221213 23:26:05 @agent_ppo2.py:185][0m |          -0.0059 |          28.8743 |          12.1791 |
[32m[20221213 23:26:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:26:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.37
[32m[20221213 23:26:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.50
[32m[20221213 23:26:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.10
[32m[20221213 23:26:05 @agent_ppo2.py:143][0m Total time:      13.55 min
[32m[20221213 23:26:05 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221213 23:26:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2641 --------------------------#
[32m[20221213 23:26:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:05 @agent_ppo2.py:185][0m |           0.0024 |          34.8842 |          12.5739 |
[32m[20221213 23:26:05 @agent_ppo2.py:185][0m |          -0.0032 |          30.3424 |          12.5459 |
[32m[20221213 23:26:05 @agent_ppo2.py:185][0m |          -0.0054 |          29.2273 |          12.5690 |
[32m[20221213 23:26:05 @agent_ppo2.py:185][0m |          -0.0046 |          28.9121 |          12.5343 |
[32m[20221213 23:26:05 @agent_ppo2.py:185][0m |          -0.0089 |          28.2110 |          12.5870 |
[32m[20221213 23:26:06 @agent_ppo2.py:185][0m |          -0.0097 |          27.9359 |          12.5602 |
[32m[20221213 23:26:06 @agent_ppo2.py:185][0m |          -0.0021 |          31.3135 |          12.5660 |
[32m[20221213 23:26:06 @agent_ppo2.py:185][0m |          -0.0118 |          27.5221 |          12.5595 |
[32m[20221213 23:26:06 @agent_ppo2.py:185][0m |          -0.0131 |          26.8489 |          12.5961 |
[32m[20221213 23:26:06 @agent_ppo2.py:185][0m |          -0.0101 |          27.0052 |          12.5777 |
[32m[20221213 23:26:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:26:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.90
[32m[20221213 23:26:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.79
[32m[20221213 23:26:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.96
[32m[20221213 23:26:06 @agent_ppo2.py:143][0m Total time:      13.57 min
[32m[20221213 23:26:06 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221213 23:26:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2642 --------------------------#
[32m[20221213 23:26:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:06 @agent_ppo2.py:185][0m |          -0.0000 |          31.3777 |          12.8747 |
[32m[20221213 23:26:06 @agent_ppo2.py:185][0m |          -0.0086 |          28.1484 |          12.9089 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0021 |          27.2859 |          12.9189 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0118 |          26.8801 |          12.9176 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0123 |          26.6253 |          12.9389 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0073 |          26.2688 |          12.9443 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0140 |          26.1029 |          12.9539 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0118 |          25.9586 |          12.9639 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0055 |          26.8347 |          12.9583 |
[32m[20221213 23:26:07 @agent_ppo2.py:185][0m |          -0.0140 |          25.6338 |          13.0069 |
[32m[20221213 23:26:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:26:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.53
[32m[20221213 23:26:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.76
[32m[20221213 23:26:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.00
[32m[20221213 23:26:07 @agent_ppo2.py:143][0m Total time:      13.59 min
[32m[20221213 23:26:07 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221213 23:26:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2643 --------------------------#
[32m[20221213 23:26:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |           0.0025 |          51.8131 |          12.2101 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0033 |          49.7960 |          12.2577 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0064 |          49.2521 |          12.2536 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0046 |          48.8160 |          12.2936 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0088 |          48.3588 |          12.2743 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0070 |          48.0976 |          12.2929 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0095 |          48.1773 |          12.3051 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0118 |          47.8144 |          12.3303 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0119 |          47.6466 |          12.3138 |
[32m[20221213 23:26:08 @agent_ppo2.py:185][0m |          -0.0079 |          47.4419 |          12.3424 |
[32m[20221213 23:26:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:26:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.09
[32m[20221213 23:26:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.70
[32m[20221213 23:26:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.49
[32m[20221213 23:26:09 @agent_ppo2.py:143][0m Total time:      13.61 min
[32m[20221213 23:26:09 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221213 23:26:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2644 --------------------------#
[32m[20221213 23:26:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:09 @agent_ppo2.py:185][0m |           0.0056 |          48.9035 |          12.4040 |
[32m[20221213 23:26:09 @agent_ppo2.py:185][0m |          -0.0064 |          45.9820 |          12.3854 |
[32m[20221213 23:26:09 @agent_ppo2.py:185][0m |           0.0069 |          49.0980 |          12.4173 |
[32m[20221213 23:26:09 @agent_ppo2.py:185][0m |          -0.0065 |          45.0146 |          12.4588 |
[32m[20221213 23:26:09 @agent_ppo2.py:185][0m |          -0.0058 |          44.7168 |          12.4850 |
[32m[20221213 23:26:09 @agent_ppo2.py:185][0m |          -0.0121 |          44.2688 |          12.4937 |
[32m[20221213 23:26:10 @agent_ppo2.py:185][0m |          -0.0103 |          44.2444 |          12.4823 |
[32m[20221213 23:26:10 @agent_ppo2.py:185][0m |          -0.0057 |          43.8588 |          12.5327 |
[32m[20221213 23:26:10 @agent_ppo2.py:185][0m |          -0.0116 |          43.9441 |          12.5180 |
[32m[20221213 23:26:10 @agent_ppo2.py:185][0m |          -0.0107 |          43.3419 |          12.5155 |
[32m[20221213 23:26:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:26:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.53
[32m[20221213 23:26:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.72
[32m[20221213 23:26:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.69
[32m[20221213 23:26:10 @agent_ppo2.py:143][0m Total time:      13.63 min
[32m[20221213 23:26:10 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221213 23:26:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2645 --------------------------#
[32m[20221213 23:26:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:10 @agent_ppo2.py:185][0m |           0.0001 |          44.1412 |          12.8771 |
[32m[20221213 23:26:10 @agent_ppo2.py:185][0m |          -0.0002 |          41.9060 |          12.9012 |
[32m[20221213 23:26:10 @agent_ppo2.py:185][0m |          -0.0026 |          41.1181 |          12.8332 |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |          -0.0096 |          40.7644 |          12.8979 |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |          -0.0076 |          40.2956 |          12.8372 |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |          -0.0125 |          39.9944 |          12.8538 |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |          -0.0108 |          39.6965 |          12.8687 |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |          -0.0097 |          39.4205 |          12.8288 |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |          -0.0151 |          39.2454 |          12.8356 |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |          -0.0082 |          42.5990 |          12.8248 |
[32m[20221213 23:26:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.24
[32m[20221213 23:26:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.76
[32m[20221213 23:26:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.69
[32m[20221213 23:26:11 @agent_ppo2.py:143][0m Total time:      13.65 min
[32m[20221213 23:26:11 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221213 23:26:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2646 --------------------------#
[32m[20221213 23:26:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:11 @agent_ppo2.py:185][0m |           0.0012 |          46.6179 |          12.7904 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0023 |          44.2503 |          12.7983 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0091 |          41.7828 |          12.8137 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0100 |          40.9104 |          12.8101 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0069 |          42.0453 |          12.8041 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0118 |          39.8236 |          12.8308 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0086 |          39.5910 |          12.8424 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0082 |          40.1009 |          12.8224 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0174 |          38.8071 |          12.8472 |
[32m[20221213 23:26:12 @agent_ppo2.py:185][0m |          -0.0162 |          38.4244 |          12.8579 |
[32m[20221213 23:26:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.40
[32m[20221213 23:26:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.25
[32m[20221213 23:26:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.49
[32m[20221213 23:26:12 @agent_ppo2.py:143][0m Total time:      13.67 min
[32m[20221213 23:26:12 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221213 23:26:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2647 --------------------------#
[32m[20221213 23:26:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |           0.0019 |          54.3450 |          12.6807 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |           0.0025 |          54.5073 |          12.6054 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |          -0.0063 |          51.4508 |          12.6519 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |          -0.0073 |          50.8934 |          12.6381 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |          -0.0041 |          50.7518 |          12.6350 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |          -0.0008 |          56.1822 |          12.6615 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |           0.0002 |          53.2597 |          12.6469 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |          -0.0137 |          50.1424 |          12.6141 |
[32m[20221213 23:26:13 @agent_ppo2.py:185][0m |          -0.0102 |          50.0562 |          12.6428 |
[32m[20221213 23:26:14 @agent_ppo2.py:185][0m |          -0.0156 |          49.8643 |          12.6421 |
[32m[20221213 23:26:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:26:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.13
[32m[20221213 23:26:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.41
[32m[20221213 23:26:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.82
[32m[20221213 23:26:14 @agent_ppo2.py:143][0m Total time:      13.69 min
[32m[20221213 23:26:14 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221213 23:26:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2648 --------------------------#
[32m[20221213 23:26:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:14 @agent_ppo2.py:185][0m |           0.0107 |          45.7937 |          12.6103 |
[32m[20221213 23:26:14 @agent_ppo2.py:185][0m |          -0.0052 |          39.8988 |          12.5920 |
[32m[20221213 23:26:14 @agent_ppo2.py:185][0m |          -0.0085 |          38.2407 |          12.6767 |
[32m[20221213 23:26:14 @agent_ppo2.py:185][0m |          -0.0131 |          37.4422 |          12.6889 |
[32m[20221213 23:26:14 @agent_ppo2.py:185][0m |          -0.0115 |          37.0309 |          12.7130 |
[32m[20221213 23:26:14 @agent_ppo2.py:185][0m |          -0.0108 |          36.7007 |          12.6951 |
[32m[20221213 23:26:15 @agent_ppo2.py:185][0m |          -0.0134 |          36.3182 |          12.7479 |
[32m[20221213 23:26:15 @agent_ppo2.py:185][0m |          -0.0094 |          36.3588 |          12.7675 |
[32m[20221213 23:26:15 @agent_ppo2.py:185][0m |          -0.0096 |          36.6833 |          12.7741 |
[32m[20221213 23:26:15 @agent_ppo2.py:185][0m |          -0.0131 |          36.2365 |          12.7720 |
[32m[20221213 23:26:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.98
[32m[20221213 23:26:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.01
[32m[20221213 23:26:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.99
[32m[20221213 23:26:15 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 539.99
[32m[20221213 23:26:15 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 539.99
[32m[20221213 23:26:15 @agent_ppo2.py:143][0m Total time:      13.71 min
[32m[20221213 23:26:15 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221213 23:26:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2649 --------------------------#
[32m[20221213 23:26:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:15 @agent_ppo2.py:185][0m |           0.0068 |          56.1777 |          12.4951 |
[32m[20221213 23:26:15 @agent_ppo2.py:185][0m |          -0.0033 |          49.7446 |          12.5377 |
[32m[20221213 23:26:15 @agent_ppo2.py:185][0m |          -0.0028 |          48.9015 |          12.5702 |
[32m[20221213 23:26:16 @agent_ppo2.py:185][0m |          -0.0064 |          47.9881 |          12.5551 |
[32m[20221213 23:26:16 @agent_ppo2.py:185][0m |          -0.0065 |          47.7494 |          12.5547 |
[32m[20221213 23:26:16 @agent_ppo2.py:185][0m |          -0.0088 |          47.3633 |          12.5596 |
[32m[20221213 23:26:16 @agent_ppo2.py:185][0m |          -0.0014 |          51.8467 |          12.5886 |
[32m[20221213 23:26:16 @agent_ppo2.py:185][0m |          -0.0113 |          46.9171 |          12.5614 |
[32m[20221213 23:26:16 @agent_ppo2.py:185][0m |          -0.0161 |          46.8193 |          12.5526 |
[32m[20221213 23:26:16 @agent_ppo2.py:185][0m |          -0.0119 |          46.6572 |          12.5691 |
[32m[20221213 23:26:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:26:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.77
[32m[20221213 23:26:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.06
[32m[20221213 23:26:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.56
[32m[20221213 23:26:16 @agent_ppo2.py:143][0m Total time:      13.74 min
[32m[20221213 23:26:16 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221213 23:26:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2650 --------------------------#
[32m[20221213 23:26:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:26:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |           0.0063 |          42.0994 |          12.9219 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0096 |          35.6538 |          12.9635 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0017 |          34.2641 |          12.9925 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0099 |          33.5560 |          12.9956 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0078 |          32.5667 |          13.0389 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0088 |          32.0642 |          13.0636 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0122 |          31.5689 |          13.0880 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0128 |          31.2157 |          13.1021 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0138 |          30.8215 |          13.1291 |
[32m[20221213 23:26:17 @agent_ppo2.py:185][0m |          -0.0137 |          30.4973 |          13.1394 |
[32m[20221213 23:26:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:26:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.49
[32m[20221213 23:26:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.82
[32m[20221213 23:26:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.68
[32m[20221213 23:26:17 @agent_ppo2.py:143][0m Total time:      13.76 min
[32m[20221213 23:26:17 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221213 23:26:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2651 --------------------------#
[32m[20221213 23:26:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |           0.0019 |          53.3279 |          12.9334 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |          -0.0060 |          49.9060 |          12.8934 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |           0.0041 |          50.6677 |          12.9454 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |          -0.0081 |          48.0055 |          12.9226 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |          -0.0140 |          47.2834 |          12.9441 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |          -0.0106 |          46.9975 |          12.9425 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |          -0.0133 |          46.6005 |          12.9440 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |          -0.0129 |          46.4487 |          12.9866 |
[32m[20221213 23:26:18 @agent_ppo2.py:185][0m |          -0.0089 |          46.0320 |          12.9705 |
[32m[20221213 23:26:19 @agent_ppo2.py:185][0m |          -0.0055 |          46.3646 |          12.9863 |
[32m[20221213 23:26:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.37
[32m[20221213 23:26:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.67
[32m[20221213 23:26:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.18
[32m[20221213 23:26:19 @agent_ppo2.py:143][0m Total time:      13.78 min
[32m[20221213 23:26:19 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221213 23:26:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2652 --------------------------#
[32m[20221213 23:26:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:19 @agent_ppo2.py:185][0m |           0.0007 |          62.8570 |          12.9604 |
[32m[20221213 23:26:19 @agent_ppo2.py:185][0m |          -0.0031 |          58.4660 |          12.9864 |
[32m[20221213 23:26:19 @agent_ppo2.py:185][0m |          -0.0081 |          56.9815 |          12.9985 |
[32m[20221213 23:26:19 @agent_ppo2.py:185][0m |           0.0030 |          65.0367 |          13.0037 |
[32m[20221213 23:26:19 @agent_ppo2.py:185][0m |          -0.0055 |          56.2595 |          13.0522 |
[32m[20221213 23:26:19 @agent_ppo2.py:185][0m |          -0.0071 |          54.9703 |          13.0566 |
[32m[20221213 23:26:20 @agent_ppo2.py:185][0m |          -0.0007 |          55.8810 |          13.0772 |
[32m[20221213 23:26:20 @agent_ppo2.py:185][0m |          -0.0008 |          55.8135 |          13.0660 |
[32m[20221213 23:26:20 @agent_ppo2.py:185][0m |          -0.0078 |          53.9440 |          13.1159 |
[32m[20221213 23:26:20 @agent_ppo2.py:185][0m |          -0.0147 |          53.6404 |          13.1179 |
[32m[20221213 23:26:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.74
[32m[20221213 23:26:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.84
[32m[20221213 23:26:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.80
[32m[20221213 23:26:20 @agent_ppo2.py:143][0m Total time:      13.80 min
[32m[20221213 23:26:20 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221213 23:26:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2653 --------------------------#
[32m[20221213 23:26:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:20 @agent_ppo2.py:185][0m |           0.0068 |          57.3169 |          13.1636 |
[32m[20221213 23:26:20 @agent_ppo2.py:185][0m |          -0.0061 |          53.0365 |          13.1912 |
[32m[20221213 23:26:20 @agent_ppo2.py:185][0m |          -0.0025 |          54.0928 |          13.1872 |
[32m[20221213 23:26:21 @agent_ppo2.py:185][0m |          -0.0106 |          51.8980 |          13.2083 |
[32m[20221213 23:26:21 @agent_ppo2.py:185][0m |          -0.0091 |          51.5763 |          13.2149 |
[32m[20221213 23:26:21 @agent_ppo2.py:185][0m |          -0.0124 |          51.2916 |          13.1978 |
[32m[20221213 23:26:21 @agent_ppo2.py:185][0m |          -0.0094 |          51.1728 |          13.2205 |
[32m[20221213 23:26:21 @agent_ppo2.py:185][0m |          -0.0127 |          50.9884 |          13.2054 |
[32m[20221213 23:26:21 @agent_ppo2.py:185][0m |          -0.0110 |          50.7116 |          13.2127 |
[32m[20221213 23:26:21 @agent_ppo2.py:185][0m |          -0.0170 |          50.7157 |          13.2362 |
[32m[20221213 23:26:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.53
[32m[20221213 23:26:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.17
[32m[20221213 23:26:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.58
[32m[20221213 23:26:21 @agent_ppo2.py:143][0m Total time:      13.82 min
[32m[20221213 23:26:21 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221213 23:26:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2654 --------------------------#
[32m[20221213 23:26:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |           0.0006 |          45.0142 |          13.2686 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0098 |          40.0170 |          13.2238 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0033 |          38.7159 |          13.2202 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0147 |          37.9252 |          13.2016 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0063 |          37.2295 |          13.2092 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0109 |          36.9822 |          13.2081 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0095 |          36.5065 |          13.1741 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0198 |          36.3297 |          13.1838 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0088 |          39.1028 |          13.1539 |
[32m[20221213 23:26:22 @agent_ppo2.py:185][0m |          -0.0192 |          35.8965 |          13.1558 |
[32m[20221213 23:26:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:26:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.31
[32m[20221213 23:26:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.94
[32m[20221213 23:26:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.40
[32m[20221213 23:26:22 @agent_ppo2.py:143][0m Total time:      13.84 min
[32m[20221213 23:26:22 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221213 23:26:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2655 --------------------------#
[32m[20221213 23:26:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |           0.0160 |          60.9696 |          13.2474 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0020 |          52.9265 |          13.1865 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0066 |          52.3281 |          13.1921 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0062 |          52.0092 |          13.1904 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0065 |          51.7246 |          13.1629 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0092 |          51.5378 |          13.1582 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0096 |          51.3045 |          13.1420 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0125 |          51.2435 |          13.1171 |
[32m[20221213 23:26:23 @agent_ppo2.py:185][0m |          -0.0098 |          51.0234 |          13.1173 |
[32m[20221213 23:26:24 @agent_ppo2.py:185][0m |          -0.0056 |          51.9561 |          13.1139 |
[32m[20221213 23:26:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:26:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.29
[32m[20221213 23:26:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.07
[32m[20221213 23:26:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.27
[32m[20221213 23:26:24 @agent_ppo2.py:143][0m Total time:      13.86 min
[32m[20221213 23:26:24 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221213 23:26:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2656 --------------------------#
[32m[20221213 23:26:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:24 @agent_ppo2.py:185][0m |          -0.0012 |          52.2309 |          13.1505 |
[32m[20221213 23:26:24 @agent_ppo2.py:185][0m |          -0.0096 |          51.7064 |          13.1470 |
[32m[20221213 23:26:24 @agent_ppo2.py:185][0m |          -0.0079 |          51.4710 |          13.1468 |
[32m[20221213 23:26:24 @agent_ppo2.py:185][0m |          -0.0060 |          51.4291 |          13.1270 |
[32m[20221213 23:26:24 @agent_ppo2.py:185][0m |          -0.0035 |          53.9382 |          13.1076 |
[32m[20221213 23:26:24 @agent_ppo2.py:185][0m |           0.0023 |          54.3192 |          13.1675 |
[32m[20221213 23:26:25 @agent_ppo2.py:185][0m |          -0.0093 |          51.1069 |          13.1871 |
[32m[20221213 23:26:25 @agent_ppo2.py:185][0m |          -0.0109 |          51.1005 |          13.1813 |
[32m[20221213 23:26:25 @agent_ppo2.py:185][0m |          -0.0100 |          51.1503 |          13.1673 |
[32m[20221213 23:26:25 @agent_ppo2.py:185][0m |          -0.0121 |          50.7740 |          13.1820 |
[32m[20221213 23:26:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:26:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.95
[32m[20221213 23:26:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.56
[32m[20221213 23:26:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.12
[32m[20221213 23:26:25 @agent_ppo2.py:143][0m Total time:      13.88 min
[32m[20221213 23:26:25 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221213 23:26:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2657 --------------------------#
[32m[20221213 23:26:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:25 @agent_ppo2.py:185][0m |           0.0041 |          58.0832 |          13.0265 |
[32m[20221213 23:26:25 @agent_ppo2.py:185][0m |          -0.0006 |          55.2439 |          13.0502 |
[32m[20221213 23:26:25 @agent_ppo2.py:185][0m |          -0.0032 |          53.6035 |          13.0334 |
[32m[20221213 23:26:26 @agent_ppo2.py:185][0m |          -0.0060 |          52.0821 |          13.0576 |
[32m[20221213 23:26:26 @agent_ppo2.py:185][0m |          -0.0076 |          51.0230 |          13.0644 |
[32m[20221213 23:26:26 @agent_ppo2.py:185][0m |          -0.0093 |          50.2883 |          13.0599 |
[32m[20221213 23:26:26 @agent_ppo2.py:185][0m |          -0.0089 |          49.7143 |          13.0444 |
[32m[20221213 23:26:26 @agent_ppo2.py:185][0m |          -0.0124 |          49.1427 |          13.0662 |
[32m[20221213 23:26:26 @agent_ppo2.py:185][0m |          -0.0092 |          48.8568 |          13.0596 |
[32m[20221213 23:26:26 @agent_ppo2.py:185][0m |          -0.0087 |          48.4188 |          13.0723 |
[32m[20221213 23:26:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:26:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.61
[32m[20221213 23:26:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.49
[32m[20221213 23:26:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.00
[32m[20221213 23:26:26 @agent_ppo2.py:143][0m Total time:      13.90 min
[32m[20221213 23:26:26 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221213 23:26:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2658 --------------------------#
[32m[20221213 23:26:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |           0.0006 |          64.1046 |          13.0810 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0059 |          57.5782 |          13.0679 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0142 |          55.4485 |          13.0388 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0007 |          57.5885 |          13.0524 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0067 |          53.9947 |          13.0643 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0126 |          53.0833 |          13.0644 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0024 |          55.9239 |          13.0761 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0039 |          58.7050 |          13.0884 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0124 |          51.5351 |          13.0912 |
[32m[20221213 23:26:27 @agent_ppo2.py:185][0m |          -0.0184 |          51.1237 |          13.0799 |
[32m[20221213 23:26:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:26:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.85
[32m[20221213 23:26:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.53
[32m[20221213 23:26:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.67
[32m[20221213 23:26:27 @agent_ppo2.py:143][0m Total time:      13.92 min
[32m[20221213 23:26:27 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221213 23:26:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2659 --------------------------#
[32m[20221213 23:26:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |           0.0048 |          49.8922 |          13.2021 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0042 |          38.1432 |          13.1868 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0034 |          35.3630 |          13.1505 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0051 |          33.4815 |          13.1148 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0110 |          32.4331 |          13.0418 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0128 |          31.4829 |          13.0079 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0107 |          30.8263 |          12.9999 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0173 |          30.4773 |          12.9704 |
[32m[20221213 23:26:28 @agent_ppo2.py:185][0m |          -0.0169 |          30.0692 |          12.9770 |
[32m[20221213 23:26:29 @agent_ppo2.py:185][0m |          -0.0164 |          29.5633 |          12.9138 |
[32m[20221213 23:26:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:26:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.86
[32m[20221213 23:26:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.38
[32m[20221213 23:26:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.68
[32m[20221213 23:26:29 @agent_ppo2.py:143][0m Total time:      13.94 min
[32m[20221213 23:26:29 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221213 23:26:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2660 --------------------------#
[32m[20221213 23:26:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:26:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:29 @agent_ppo2.py:185][0m |           0.0040 |          44.5852 |          12.7543 |
[32m[20221213 23:26:29 @agent_ppo2.py:185][0m |          -0.0052 |          39.9276 |          12.7753 |
[32m[20221213 23:26:29 @agent_ppo2.py:185][0m |          -0.0106 |          38.1385 |          12.7382 |
[32m[20221213 23:26:29 @agent_ppo2.py:185][0m |          -0.0063 |          36.5907 |          12.7851 |
[32m[20221213 23:26:29 @agent_ppo2.py:185][0m |          -0.0100 |          35.8663 |          12.8041 |
[32m[20221213 23:26:29 @agent_ppo2.py:185][0m |          -0.0089 |          35.1451 |          12.7857 |
[32m[20221213 23:26:30 @agent_ppo2.py:185][0m |          -0.0166 |          34.6241 |          12.8004 |
[32m[20221213 23:26:30 @agent_ppo2.py:185][0m |          -0.0146 |          34.0222 |          12.7982 |
[32m[20221213 23:26:30 @agent_ppo2.py:185][0m |          -0.0065 |          34.2375 |          12.8048 |
[32m[20221213 23:26:30 @agent_ppo2.py:185][0m |          -0.0188 |          33.3903 |          12.8041 |
[32m[20221213 23:26:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.63
[32m[20221213 23:26:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.93
[32m[20221213 23:26:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.01
[32m[20221213 23:26:30 @agent_ppo2.py:143][0m Total time:      13.97 min
[32m[20221213 23:26:30 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221213 23:26:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2661 --------------------------#
[32m[20221213 23:26:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:30 @agent_ppo2.py:185][0m |           0.0069 |          32.4882 |          13.1974 |
[32m[20221213 23:26:30 @agent_ppo2.py:185][0m |          -0.0008 |          29.6279 |          13.2016 |
[32m[20221213 23:26:30 @agent_ppo2.py:185][0m |          -0.0098 |          28.6681 |          13.1899 |
[32m[20221213 23:26:31 @agent_ppo2.py:185][0m |          -0.0066 |          28.4759 |          13.1356 |
[32m[20221213 23:26:31 @agent_ppo2.py:185][0m |          -0.0168 |          27.7355 |          13.1220 |
[32m[20221213 23:26:31 @agent_ppo2.py:185][0m |          -0.0137 |          27.4048 |          13.1042 |
[32m[20221213 23:26:31 @agent_ppo2.py:185][0m |          -0.0166 |          27.0865 |          13.0893 |
[32m[20221213 23:26:31 @agent_ppo2.py:185][0m |          -0.0143 |          26.8389 |          13.0654 |
[32m[20221213 23:26:31 @agent_ppo2.py:185][0m |          -0.0128 |          26.7384 |          13.0775 |
[32m[20221213 23:26:31 @agent_ppo2.py:185][0m |          -0.0145 |          26.5574 |          13.0542 |
[32m[20221213 23:26:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.47
[32m[20221213 23:26:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.45
[32m[20221213 23:26:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.86
[32m[20221213 23:26:31 @agent_ppo2.py:143][0m Total time:      13.99 min
[32m[20221213 23:26:31 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221213 23:26:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2662 --------------------------#
[32m[20221213 23:26:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |           0.0058 |          66.9415 |          12.7863 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0042 |          63.8183 |          12.7816 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0078 |          62.9402 |          12.7402 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0066 |          62.4135 |          12.7483 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0064 |          61.9492 |          12.7447 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0057 |          62.3248 |          12.7399 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0132 |          61.5860 |          12.7552 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0090 |          61.0745 |          12.7075 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0099 |          60.9357 |          12.7303 |
[32m[20221213 23:26:32 @agent_ppo2.py:185][0m |          -0.0144 |          60.6970 |          12.7148 |
[32m[20221213 23:26:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.26
[32m[20221213 23:26:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.78
[32m[20221213 23:26:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.54
[32m[20221213 23:26:32 @agent_ppo2.py:143][0m Total time:      14.01 min
[32m[20221213 23:26:32 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221213 23:26:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2663 --------------------------#
[32m[20221213 23:26:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |           0.0152 |          63.9667 |          12.6094 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |          -0.0039 |          56.7520 |          12.5397 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |          -0.0055 |          55.7149 |          12.5773 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |          -0.0073 |          54.9696 |          12.5487 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |          -0.0071 |          54.2880 |          12.5483 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |          -0.0049 |          54.3806 |          12.5493 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |           0.0036 |          57.6557 |          12.5606 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |          -0.0091 |          53.7632 |          12.5665 |
[32m[20221213 23:26:33 @agent_ppo2.py:185][0m |          -0.0103 |          53.3474 |          12.5304 |
[32m[20221213 23:26:34 @agent_ppo2.py:185][0m |          -0.0131 |          53.3285 |          12.5734 |
[32m[20221213 23:26:34 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:26:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.83
[32m[20221213 23:26:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.01
[32m[20221213 23:26:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.40
[32m[20221213 23:26:34 @agent_ppo2.py:143][0m Total time:      14.03 min
[32m[20221213 23:26:34 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221213 23:26:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2664 --------------------------#
[32m[20221213 23:26:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:34 @agent_ppo2.py:185][0m |           0.0044 |          62.3257 |          12.3171 |
[32m[20221213 23:26:34 @agent_ppo2.py:185][0m |          -0.0061 |          59.2773 |          12.2892 |
[32m[20221213 23:26:34 @agent_ppo2.py:185][0m |          -0.0059 |          58.7980 |          12.3032 |
[32m[20221213 23:26:34 @agent_ppo2.py:185][0m |          -0.0123 |          58.6467 |          12.3067 |
[32m[20221213 23:26:34 @agent_ppo2.py:185][0m |          -0.0097 |          58.3394 |          12.2972 |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |           0.0012 |          61.3616 |          12.2898 |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |          -0.0094 |          58.0501 |          12.3066 |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |           0.0013 |          63.3648 |          12.3182 |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |           0.0011 |          61.6361 |          12.2884 |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |          -0.0097 |          58.0069 |          12.2767 |
[32m[20221213 23:26:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.65
[32m[20221213 23:26:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.65
[32m[20221213 23:26:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.22
[32m[20221213 23:26:35 @agent_ppo2.py:143][0m Total time:      14.05 min
[32m[20221213 23:26:35 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221213 23:26:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2665 --------------------------#
[32m[20221213 23:26:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |          -0.0015 |          36.8629 |          12.5703 |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |          -0.0072 |          33.1946 |          12.5516 |
[32m[20221213 23:26:35 @agent_ppo2.py:185][0m |          -0.0071 |          33.9800 |          12.5589 |
[32m[20221213 23:26:36 @agent_ppo2.py:185][0m |           0.0005 |          31.9568 |          12.5531 |
[32m[20221213 23:26:36 @agent_ppo2.py:185][0m |          -0.0118 |          30.3107 |          12.5602 |
[32m[20221213 23:26:36 @agent_ppo2.py:185][0m |          -0.0059 |          30.3900 |          12.5352 |
[32m[20221213 23:26:36 @agent_ppo2.py:185][0m |          -0.0097 |          29.7055 |          12.5221 |
[32m[20221213 23:26:36 @agent_ppo2.py:185][0m |          -0.0096 |          29.2132 |          12.5402 |
[32m[20221213 23:26:36 @agent_ppo2.py:185][0m |          -0.0110 |          29.0859 |          12.5417 |
[32m[20221213 23:26:36 @agent_ppo2.py:185][0m |          -0.0107 |          28.9549 |          12.4952 |
[32m[20221213 23:26:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.00
[32m[20221213 23:26:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.48
[32m[20221213 23:26:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.89
[32m[20221213 23:26:36 @agent_ppo2.py:143][0m Total time:      14.07 min
[32m[20221213 23:26:36 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221213 23:26:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2666 --------------------------#
[32m[20221213 23:26:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |           0.0109 |          67.9171 |          12.4545 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |           0.0084 |          66.0757 |          12.4368 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0028 |          63.3728 |          12.4939 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0087 |          61.0471 |          12.4746 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0064 |          60.6612 |          12.4771 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0094 |          60.2981 |          12.5100 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0092 |          60.1078 |          12.5081 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0096 |          60.1078 |          12.5149 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0088 |          59.7464 |          12.5388 |
[32m[20221213 23:26:37 @agent_ppo2.py:185][0m |          -0.0047 |          60.2015 |          12.5395 |
[32m[20221213 23:26:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:26:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.12
[32m[20221213 23:26:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.35
[32m[20221213 23:26:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.01
[32m[20221213 23:26:37 @agent_ppo2.py:143][0m Total time:      14.09 min
[32m[20221213 23:26:37 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221213 23:26:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2667 --------------------------#
[32m[20221213 23:26:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |           0.0034 |          50.8171 |          12.6466 |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |          -0.0079 |          45.8612 |          12.6153 |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |          -0.0092 |          44.4004 |          12.6726 |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |          -0.0137 |          44.2277 |          12.6838 |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |          -0.0071 |          43.6037 |          12.6693 |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |          -0.0121 |          42.8505 |          12.6688 |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |          -0.0101 |          42.0925 |          12.6760 |
[32m[20221213 23:26:38 @agent_ppo2.py:185][0m |          -0.0102 |          41.9086 |          12.6752 |
[32m[20221213 23:26:39 @agent_ppo2.py:185][0m |          -0.0112 |          41.4605 |          12.6949 |
[32m[20221213 23:26:39 @agent_ppo2.py:185][0m |          -0.0145 |          41.3378 |          12.6712 |
[32m[20221213 23:26:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.25
[32m[20221213 23:26:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.84
[32m[20221213 23:26:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.58
[32m[20221213 23:26:39 @agent_ppo2.py:143][0m Total time:      14.11 min
[32m[20221213 23:26:39 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221213 23:26:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2668 --------------------------#
[32m[20221213 23:26:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:39 @agent_ppo2.py:185][0m |           0.0004 |          51.3129 |          12.7827 |
[32m[20221213 23:26:39 @agent_ppo2.py:185][0m |          -0.0046 |          49.1591 |          12.7777 |
[32m[20221213 23:26:39 @agent_ppo2.py:185][0m |          -0.0089 |          48.4950 |          12.7665 |
[32m[20221213 23:26:39 @agent_ppo2.py:185][0m |          -0.0097 |          48.1699 |          12.7134 |
[32m[20221213 23:26:39 @agent_ppo2.py:185][0m |          -0.0055 |          47.9928 |          12.7218 |
[32m[20221213 23:26:40 @agent_ppo2.py:185][0m |          -0.0083 |          47.5028 |          12.6963 |
[32m[20221213 23:26:40 @agent_ppo2.py:185][0m |          -0.0121 |          47.2989 |          12.6717 |
[32m[20221213 23:26:40 @agent_ppo2.py:185][0m |          -0.0082 |          47.2204 |          12.6864 |
[32m[20221213 23:26:40 @agent_ppo2.py:185][0m |          -0.0124 |          46.9918 |          12.6424 |
[32m[20221213 23:26:40 @agent_ppo2.py:185][0m |          -0.0124 |          46.8753 |          12.6351 |
[32m[20221213 23:26:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:26:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.96
[32m[20221213 23:26:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.27
[32m[20221213 23:26:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.45
[32m[20221213 23:26:40 @agent_ppo2.py:143][0m Total time:      14.13 min
[32m[20221213 23:26:40 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221213 23:26:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2669 --------------------------#
[32m[20221213 23:26:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:40 @agent_ppo2.py:185][0m |          -0.0003 |          42.8111 |          12.3931 |
[32m[20221213 23:26:40 @agent_ppo2.py:185][0m |          -0.0059 |          40.6981 |          12.4137 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |          -0.0009 |          39.7950 |          12.3363 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |           0.0050 |          42.5720 |          12.3760 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |           0.0087 |          44.1729 |          12.4308 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |          -0.0074 |          38.4981 |          12.4217 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |          -0.0089 |          37.9419 |          12.4245 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |          -0.0071 |          37.7759 |          12.4310 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |          -0.0058 |          37.5352 |          12.3994 |
[32m[20221213 23:26:41 @agent_ppo2.py:185][0m |          -0.0102 |          37.1780 |          12.4005 |
[32m[20221213 23:26:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.93
[32m[20221213 23:26:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.25
[32m[20221213 23:26:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.04
[32m[20221213 23:26:41 @agent_ppo2.py:143][0m Total time:      14.15 min
[32m[20221213 23:26:41 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221213 23:26:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2670 --------------------------#
[32m[20221213 23:26:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:26:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0023 |          53.5700 |          12.8668 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |           0.0024 |          51.0693 |          12.8502 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0085 |          49.2304 |          12.8625 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0125 |          48.6320 |          12.8462 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0120 |          48.2938 |          12.8284 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0089 |          47.9711 |          12.8419 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0101 |          48.1789 |          12.8659 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0129 |          47.3568 |          12.8226 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0161 |          47.1909 |          12.8420 |
[32m[20221213 23:26:42 @agent_ppo2.py:185][0m |          -0.0077 |          47.5392 |          12.8441 |
[32m[20221213 23:26:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.34
[32m[20221213 23:26:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.52
[32m[20221213 23:26:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.70
[32m[20221213 23:26:43 @agent_ppo2.py:143][0m Total time:      14.18 min
[32m[20221213 23:26:43 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221213 23:26:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2671 --------------------------#
[32m[20221213 23:26:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0012 |          65.6058 |          12.8267 |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0028 |          60.4198 |          12.7998 |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0058 |          58.0389 |          12.8119 |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0075 |          56.1670 |          12.7962 |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0022 |          57.2264 |          12.8072 |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0076 |          54.3919 |          12.8175 |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0112 |          53.5950 |          12.8547 |
[32m[20221213 23:26:43 @agent_ppo2.py:185][0m |          -0.0121 |          52.7224 |          12.8607 |
[32m[20221213 23:26:44 @agent_ppo2.py:185][0m |           0.0020 |          56.5221 |          12.8234 |
[32m[20221213 23:26:44 @agent_ppo2.py:185][0m |          -0.0110 |          52.0290 |          12.8059 |
[32m[20221213 23:26:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.91
[32m[20221213 23:26:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.86
[32m[20221213 23:26:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.61
[32m[20221213 23:26:44 @agent_ppo2.py:143][0m Total time:      14.20 min
[32m[20221213 23:26:44 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221213 23:26:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2672 --------------------------#
[32m[20221213 23:26:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:44 @agent_ppo2.py:185][0m |           0.0010 |          49.0404 |          12.7082 |
[32m[20221213 23:26:44 @agent_ppo2.py:185][0m |          -0.0046 |          38.3471 |          12.7292 |
[32m[20221213 23:26:44 @agent_ppo2.py:185][0m |          -0.0078 |          36.8243 |          12.7205 |
[32m[20221213 23:26:44 @agent_ppo2.py:185][0m |          -0.0051 |          35.7056 |          12.6996 |
[32m[20221213 23:26:44 @agent_ppo2.py:185][0m |          -0.0087 |          35.2949 |          12.6808 |
[32m[20221213 23:26:45 @agent_ppo2.py:185][0m |          -0.0029 |          36.8686 |          12.6943 |
[32m[20221213 23:26:45 @agent_ppo2.py:185][0m |          -0.0098 |          34.9013 |          12.6977 |
[32m[20221213 23:26:45 @agent_ppo2.py:185][0m |          -0.0053 |          35.7446 |          12.6782 |
[32m[20221213 23:26:45 @agent_ppo2.py:185][0m |          -0.0114 |          34.2492 |          12.6952 |
[32m[20221213 23:26:45 @agent_ppo2.py:185][0m |          -0.0163 |          33.8735 |          12.6898 |
[32m[20221213 23:26:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.43
[32m[20221213 23:26:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.48
[32m[20221213 23:26:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.08
[32m[20221213 23:26:45 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 573.08
[32m[20221213 23:26:45 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 573.08
[32m[20221213 23:26:45 @agent_ppo2.py:143][0m Total time:      14.22 min
[32m[20221213 23:26:45 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221213 23:26:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2673 --------------------------#
[32m[20221213 23:26:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:45 @agent_ppo2.py:185][0m |           0.0011 |          53.6187 |          12.6728 |
[32m[20221213 23:26:45 @agent_ppo2.py:185][0m |          -0.0034 |          52.1374 |          12.6888 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |          -0.0048 |          51.8998 |          12.6895 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |          -0.0016 |          52.9501 |          12.6845 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |          -0.0021 |          52.9372 |          12.6944 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |           0.0039 |          56.5593 |          12.6633 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |          -0.0035 |          51.8516 |          12.7419 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |          -0.0091 |          50.9964 |          12.7109 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |          -0.0020 |          51.8773 |          12.7340 |
[32m[20221213 23:26:46 @agent_ppo2.py:185][0m |          -0.0096 |          50.7640 |          12.8109 |
[32m[20221213 23:26:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:26:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.96
[32m[20221213 23:26:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.08
[32m[20221213 23:26:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.53
[32m[20221213 23:26:46 @agent_ppo2.py:143][0m Total time:      14.24 min
[32m[20221213 23:26:46 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221213 23:26:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2674 --------------------------#
[32m[20221213 23:26:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |           0.0214 |          66.5827 |          12.7542 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0052 |          46.1677 |          12.8601 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |           0.0007 |          47.4107 |          12.8673 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0070 |          43.8228 |          12.8734 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0072 |          43.0537 |          12.8718 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0098 |          42.5894 |          12.8992 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0102 |          42.1154 |          12.9163 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0091 |          41.8906 |          12.9580 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0123 |          44.6663 |          12.9674 |
[32m[20221213 23:26:47 @agent_ppo2.py:185][0m |          -0.0124 |          41.3807 |          12.9762 |
[32m[20221213 23:26:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.99
[32m[20221213 23:26:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.58
[32m[20221213 23:26:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.42
[32m[20221213 23:26:48 @agent_ppo2.py:143][0m Total time:      14.26 min
[32m[20221213 23:26:48 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221213 23:26:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2675 --------------------------#
[32m[20221213 23:26:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |           0.0029 |          59.0354 |          13.0966 |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |           0.0033 |          60.1277 |          13.1743 |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |          -0.0061 |          57.3690 |          13.1940 |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |          -0.0087 |          56.8762 |          13.2022 |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |          -0.0072 |          56.9387 |          13.2071 |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |           0.0023 |          61.2517 |          13.2494 |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |          -0.0105 |          56.4902 |          13.2658 |
[32m[20221213 23:26:48 @agent_ppo2.py:185][0m |          -0.0111 |          56.2648 |          13.2820 |
[32m[20221213 23:26:49 @agent_ppo2.py:185][0m |          -0.0126 |          56.1047 |          13.3405 |
[32m[20221213 23:26:49 @agent_ppo2.py:185][0m |          -0.0119 |          55.9202 |          13.3438 |
[32m[20221213 23:26:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.10
[32m[20221213 23:26:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.29
[32m[20221213 23:26:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.42
[32m[20221213 23:26:49 @agent_ppo2.py:143][0m Total time:      14.28 min
[32m[20221213 23:26:49 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221213 23:26:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2676 --------------------------#
[32m[20221213 23:26:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:49 @agent_ppo2.py:185][0m |          -0.0017 |          55.5991 |          13.4140 |
[32m[20221213 23:26:49 @agent_ppo2.py:185][0m |          -0.0053 |          54.6252 |          13.4616 |
[32m[20221213 23:26:49 @agent_ppo2.py:185][0m |          -0.0079 |          54.1417 |          13.4888 |
[32m[20221213 23:26:49 @agent_ppo2.py:185][0m |          -0.0074 |          54.3150 |          13.4858 |
[32m[20221213 23:26:49 @agent_ppo2.py:185][0m |          -0.0078 |          53.8106 |          13.4143 |
[32m[20221213 23:26:50 @agent_ppo2.py:185][0m |          -0.0093 |          53.6305 |          13.4714 |
[32m[20221213 23:26:50 @agent_ppo2.py:185][0m |          -0.0095 |          53.4259 |          13.4762 |
[32m[20221213 23:26:50 @agent_ppo2.py:185][0m |          -0.0104 |          53.3716 |          13.4937 |
[32m[20221213 23:26:50 @agent_ppo2.py:185][0m |          -0.0113 |          53.3277 |          13.5058 |
[32m[20221213 23:26:50 @agent_ppo2.py:185][0m |          -0.0100 |          53.2411 |          13.4900 |
[32m[20221213 23:26:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.29
[32m[20221213 23:26:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.91
[32m[20221213 23:26:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.32
[32m[20221213 23:26:50 @agent_ppo2.py:143][0m Total time:      14.30 min
[32m[20221213 23:26:50 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221213 23:26:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2677 --------------------------#
[32m[20221213 23:26:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:50 @agent_ppo2.py:185][0m |           0.0004 |          58.9372 |          13.2052 |
[32m[20221213 23:26:50 @agent_ppo2.py:185][0m |          -0.0046 |          56.2871 |          13.2504 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0078 |          55.3234 |          13.2760 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0067 |          54.7679 |          13.3002 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0096 |          54.3266 |          13.3409 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0088 |          54.0695 |          13.3301 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0087 |          54.0714 |          13.3450 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0081 |          53.5698 |          13.3824 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0118 |          53.4308 |          13.4034 |
[32m[20221213 23:26:51 @agent_ppo2.py:185][0m |          -0.0082 |          54.3528 |          13.3989 |
[32m[20221213 23:26:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.22
[32m[20221213 23:26:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.15
[32m[20221213 23:26:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.71
[32m[20221213 23:26:51 @agent_ppo2.py:143][0m Total time:      14.32 min
[32m[20221213 23:26:51 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221213 23:26:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2678 --------------------------#
[32m[20221213 23:26:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0008 |          50.8198 |          13.3527 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0048 |          48.4130 |          13.3330 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0053 |          47.5972 |          13.3250 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0069 |          47.1590 |          13.3333 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0073 |          46.7854 |          13.3350 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0094 |          46.4729 |          13.3190 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0080 |          46.2736 |          13.3117 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0109 |          46.1078 |          13.3095 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0048 |          46.2576 |          13.3388 |
[32m[20221213 23:26:52 @agent_ppo2.py:185][0m |          -0.0114 |          45.8509 |          13.3112 |
[32m[20221213 23:26:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:26:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.21
[32m[20221213 23:26:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.56
[32m[20221213 23:26:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.32
[32m[20221213 23:26:53 @agent_ppo2.py:143][0m Total time:      14.34 min
[32m[20221213 23:26:53 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221213 23:26:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2679 --------------------------#
[32m[20221213 23:26:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |           0.0021 |          30.3447 |          13.6101 |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |          -0.0131 |          25.4245 |          13.6072 |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |          -0.0036 |          25.7162 |          13.6067 |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |          -0.0096 |          23.6092 |          13.6110 |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |          -0.0108 |          22.9724 |          13.6106 |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |          -0.0093 |          22.8661 |          13.5797 |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |          -0.0169 |          22.3145 |          13.5675 |
[32m[20221213 23:26:53 @agent_ppo2.py:185][0m |          -0.0049 |          27.2316 |          13.5810 |
[32m[20221213 23:26:54 @agent_ppo2.py:185][0m |          -0.0160 |          22.0413 |          13.5799 |
[32m[20221213 23:26:54 @agent_ppo2.py:185][0m |          -0.0152 |          21.8816 |          13.5431 |
[32m[20221213 23:26:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.22
[32m[20221213 23:26:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.04
[32m[20221213 23:26:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.78
[32m[20221213 23:26:54 @agent_ppo2.py:143][0m Total time:      14.36 min
[32m[20221213 23:26:54 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221213 23:26:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2680 --------------------------#
[32m[20221213 23:26:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:26:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:54 @agent_ppo2.py:185][0m |           0.0026 |          24.8772 |          13.6041 |
[32m[20221213 23:26:54 @agent_ppo2.py:185][0m |          -0.0101 |          21.9705 |          13.6190 |
[32m[20221213 23:26:54 @agent_ppo2.py:185][0m |          -0.0059 |          20.2328 |          13.5746 |
[32m[20221213 23:26:54 @agent_ppo2.py:185][0m |          -0.0097 |          19.5926 |          13.5955 |
[32m[20221213 23:26:54 @agent_ppo2.py:185][0m |          -0.0134 |          19.0237 |          13.5929 |
[32m[20221213 23:26:55 @agent_ppo2.py:185][0m |          -0.0137 |          19.1234 |          13.5585 |
[32m[20221213 23:26:55 @agent_ppo2.py:185][0m |          -0.0129 |          18.4505 |          13.5520 |
[32m[20221213 23:26:55 @agent_ppo2.py:185][0m |          -0.0112 |          18.5471 |          13.5263 |
[32m[20221213 23:26:55 @agent_ppo2.py:185][0m |          -0.0153 |          18.2998 |          13.5197 |
[32m[20221213 23:26:55 @agent_ppo2.py:185][0m |          -0.0163 |          18.0375 |          13.5164 |
[32m[20221213 23:26:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.08
[32m[20221213 23:26:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.23
[32m[20221213 23:26:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.15
[32m[20221213 23:26:55 @agent_ppo2.py:143][0m Total time:      14.38 min
[32m[20221213 23:26:55 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221213 23:26:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2681 --------------------------#
[32m[20221213 23:26:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:55 @agent_ppo2.py:185][0m |           0.0018 |          46.1956 |          13.4581 |
[32m[20221213 23:26:55 @agent_ppo2.py:185][0m |          -0.0057 |          43.3611 |          13.4076 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0057 |          42.4338 |          13.3888 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0072 |          42.1155 |          13.4046 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0082 |          41.3862 |          13.3760 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0083 |          41.1208 |          13.3727 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0013 |          46.6091 |          13.3369 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0097 |          40.5025 |          13.3644 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0103 |          40.2864 |          13.3517 |
[32m[20221213 23:26:56 @agent_ppo2.py:185][0m |          -0.0126 |          40.1984 |          13.3399 |
[32m[20221213 23:26:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.95
[32m[20221213 23:26:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.09
[32m[20221213 23:26:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.25
[32m[20221213 23:26:56 @agent_ppo2.py:143][0m Total time:      14.40 min
[32m[20221213 23:26:56 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221213 23:26:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2682 --------------------------#
[32m[20221213 23:26:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |           0.0011 |          55.3003 |          12.9075 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |          -0.0044 |          54.4679 |          12.9274 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |          -0.0038 |          54.0623 |          12.9074 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |          -0.0024 |          54.0891 |          12.9577 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |          -0.0054 |          54.3607 |          12.8990 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |           0.0104 |          60.5065 |          12.9079 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |           0.0020 |          57.0291 |          12.9461 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |          -0.0067 |          53.3890 |          12.9137 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |          -0.0100 |          53.3141 |          12.8985 |
[32m[20221213 23:26:57 @agent_ppo2.py:185][0m |          -0.0050 |          53.8472 |          12.9046 |
[32m[20221213 23:26:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:26:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.91
[32m[20221213 23:26:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.47
[32m[20221213 23:26:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.33
[32m[20221213 23:26:58 @agent_ppo2.py:143][0m Total time:      14.43 min
[32m[20221213 23:26:58 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221213 23:26:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2683 --------------------------#
[32m[20221213 23:26:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:26:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |          -0.0031 |          41.9841 |          13.3503 |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |          -0.0010 |          36.3529 |          13.3440 |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |          -0.0029 |          34.7187 |          13.3497 |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |          -0.0053 |          33.8455 |          13.3474 |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |          -0.0080 |          33.3521 |          13.3468 |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |          -0.0055 |          33.3074 |          13.3263 |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |          -0.0101 |          32.1997 |          13.3311 |
[32m[20221213 23:26:58 @agent_ppo2.py:185][0m |           0.0011 |          34.0545 |          13.3319 |
[32m[20221213 23:26:59 @agent_ppo2.py:185][0m |          -0.0091 |          31.4558 |          13.3649 |
[32m[20221213 23:26:59 @agent_ppo2.py:185][0m |          -0.0086 |          31.0669 |          13.3365 |
[32m[20221213 23:26:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:26:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.62
[32m[20221213 23:26:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.20
[32m[20221213 23:26:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.28
[32m[20221213 23:26:59 @agent_ppo2.py:143][0m Total time:      14.45 min
[32m[20221213 23:26:59 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221213 23:26:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2684 --------------------------#
[32m[20221213 23:26:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:26:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:26:59 @agent_ppo2.py:185][0m |           0.0012 |          53.3105 |          13.2383 |
[32m[20221213 23:26:59 @agent_ppo2.py:185][0m |          -0.0079 |          51.9709 |          13.2197 |
[32m[20221213 23:26:59 @agent_ppo2.py:185][0m |          -0.0056 |          51.5751 |          13.2522 |
[32m[20221213 23:26:59 @agent_ppo2.py:185][0m |          -0.0006 |          53.2189 |          13.2774 |
[32m[20221213 23:26:59 @agent_ppo2.py:185][0m |          -0.0057 |          51.0538 |          13.1837 |
[32m[20221213 23:27:00 @agent_ppo2.py:185][0m |          -0.0099 |          50.8166 |          13.2013 |
[32m[20221213 23:27:00 @agent_ppo2.py:185][0m |          -0.0090 |          50.8701 |          13.2025 |
[32m[20221213 23:27:00 @agent_ppo2.py:185][0m |          -0.0028 |          53.1684 |          13.2055 |
[32m[20221213 23:27:00 @agent_ppo2.py:185][0m |          -0.0077 |          50.5751 |          13.1951 |
[32m[20221213 23:27:00 @agent_ppo2.py:185][0m |          -0.0105 |          50.3916 |          13.1926 |
[32m[20221213 23:27:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.56
[32m[20221213 23:27:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.62
[32m[20221213 23:27:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.72
[32m[20221213 23:27:00 @agent_ppo2.py:143][0m Total time:      14.47 min
[32m[20221213 23:27:00 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221213 23:27:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2685 --------------------------#
[32m[20221213 23:27:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:00 @agent_ppo2.py:185][0m |          -0.0016 |          47.5922 |          12.9954 |
[32m[20221213 23:27:00 @agent_ppo2.py:185][0m |          -0.0041 |          45.5660 |          13.0484 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |          -0.0069 |          44.1622 |          13.0334 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |          -0.0082 |          43.2013 |          12.9857 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |          -0.0096 |          42.5394 |          13.0159 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |          -0.0053 |          42.1713 |          12.9754 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |           0.0005 |          43.9293 |          12.9331 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |          -0.0112 |          41.5699 |          12.9833 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |          -0.0116 |          41.2664 |          12.9807 |
[32m[20221213 23:27:01 @agent_ppo2.py:185][0m |          -0.0145 |          41.0557 |          12.9267 |
[32m[20221213 23:27:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.98
[32m[20221213 23:27:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.02
[32m[20221213 23:27:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.99
[32m[20221213 23:27:01 @agent_ppo2.py:143][0m Total time:      14.49 min
[32m[20221213 23:27:01 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221213 23:27:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2686 --------------------------#
[32m[20221213 23:27:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |           0.0058 |          44.8777 |          12.9572 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0064 |          39.1527 |          12.9437 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0017 |          38.4582 |          12.9404 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0136 |          36.6514 |          12.9393 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0029 |          37.5423 |          12.9365 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0139 |          35.7754 |          12.9615 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0122 |          36.2348 |          12.9747 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0160 |          34.8717 |          12.9836 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0139 |          34.4516 |          12.9573 |
[32m[20221213 23:27:02 @agent_ppo2.py:185][0m |          -0.0065 |          34.6815 |          12.9751 |
[32m[20221213 23:27:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.20
[32m[20221213 23:27:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.51
[32m[20221213 23:27:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.24
[32m[20221213 23:27:03 @agent_ppo2.py:143][0m Total time:      14.51 min
[32m[20221213 23:27:03 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221213 23:27:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2687 --------------------------#
[32m[20221213 23:27:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:03 @agent_ppo2.py:185][0m |           0.0026 |          51.6880 |          13.2385 |
[32m[20221213 23:27:03 @agent_ppo2.py:185][0m |          -0.0089 |          46.1261 |          13.2356 |
[32m[20221213 23:27:03 @agent_ppo2.py:185][0m |          -0.0056 |          44.9434 |          13.2547 |
[32m[20221213 23:27:03 @agent_ppo2.py:185][0m |          -0.0126 |          43.6469 |          13.2181 |
[32m[20221213 23:27:03 @agent_ppo2.py:185][0m |          -0.0138 |          43.1932 |          13.2392 |
[32m[20221213 23:27:03 @agent_ppo2.py:185][0m |          -0.0098 |          43.2380 |          13.2103 |
[32m[20221213 23:27:03 @agent_ppo2.py:185][0m |          -0.0120 |          42.3677 |          13.1854 |
[32m[20221213 23:27:04 @agent_ppo2.py:185][0m |          -0.0127 |          42.1065 |          13.1858 |
[32m[20221213 23:27:04 @agent_ppo2.py:185][0m |          -0.0134 |          42.4626 |          13.1833 |
[32m[20221213 23:27:04 @agent_ppo2.py:185][0m |          -0.0138 |          41.9968 |          13.1452 |
[32m[20221213 23:27:04 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:27:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.52
[32m[20221213 23:27:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.45
[32m[20221213 23:27:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.74
[32m[20221213 23:27:04 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221213 23:27:04 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221213 23:27:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2688 --------------------------#
[32m[20221213 23:27:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:04 @agent_ppo2.py:185][0m |           0.0027 |          48.4383 |          12.8342 |
[32m[20221213 23:27:04 @agent_ppo2.py:185][0m |          -0.0056 |          43.2782 |          12.7718 |
[32m[20221213 23:27:04 @agent_ppo2.py:185][0m |          -0.0082 |          41.6474 |          12.7533 |
[32m[20221213 23:27:04 @agent_ppo2.py:185][0m |          -0.0070 |          40.4792 |          12.7679 |
[32m[20221213 23:27:05 @agent_ppo2.py:185][0m |          -0.0057 |          39.9041 |          12.7321 |
[32m[20221213 23:27:05 @agent_ppo2.py:185][0m |          -0.0076 |          39.0813 |          12.7406 |
[32m[20221213 23:27:05 @agent_ppo2.py:185][0m |          -0.0084 |          38.7066 |          12.7350 |
[32m[20221213 23:27:05 @agent_ppo2.py:185][0m |          -0.0183 |          38.7317 |          12.7006 |
[32m[20221213 23:27:05 @agent_ppo2.py:185][0m |          -0.0068 |          38.2196 |          12.6932 |
[32m[20221213 23:27:05 @agent_ppo2.py:185][0m |          -0.0162 |          38.1136 |          12.6808 |
[32m[20221213 23:27:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:27:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.80
[32m[20221213 23:27:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.24
[32m[20221213 23:27:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.44
[32m[20221213 23:27:05 @agent_ppo2.py:143][0m Total time:      14.55 min
[32m[20221213 23:27:05 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221213 23:27:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2689 --------------------------#
[32m[20221213 23:27:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:05 @agent_ppo2.py:185][0m |          -0.0011 |          52.1604 |          13.0131 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0036 |          49.5716 |          13.0273 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0065 |          48.0704 |          13.0138 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0121 |          47.0536 |          13.0210 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0097 |          46.2284 |          12.9735 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0125 |          45.6217 |          12.9677 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0100 |          45.0637 |          12.9569 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0078 |          44.6017 |          12.9152 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0122 |          44.1659 |          12.9112 |
[32m[20221213 23:27:06 @agent_ppo2.py:185][0m |          -0.0124 |          43.7864 |          12.9318 |
[32m[20221213 23:27:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:27:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.30
[32m[20221213 23:27:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.74
[32m[20221213 23:27:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.53
[32m[20221213 23:27:06 @agent_ppo2.py:143][0m Total time:      14.57 min
[32m[20221213 23:27:06 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221213 23:27:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2690 --------------------------#
[32m[20221213 23:27:07 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:27:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0025 |          63.6197 |          12.4286 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0017 |          61.1112 |          12.4547 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0073 |          60.5325 |          12.4129 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |           0.0046 |          64.1584 |          12.4341 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0133 |          59.7302 |          12.4450 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0125 |          59.3564 |          12.4298 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0098 |          59.0694 |          12.4059 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0023 |          62.8639 |          12.3973 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0132 |          58.9460 |          12.3857 |
[32m[20221213 23:27:07 @agent_ppo2.py:185][0m |          -0.0015 |          60.9509 |          12.4047 |
[32m[20221213 23:27:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:27:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.57
[32m[20221213 23:27:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.02
[32m[20221213 23:27:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.17
[32m[20221213 23:27:08 @agent_ppo2.py:143][0m Total time:      14.59 min
[32m[20221213 23:27:08 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221213 23:27:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2691 --------------------------#
[32m[20221213 23:27:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:08 @agent_ppo2.py:185][0m |          -0.0011 |          61.5639 |          12.4441 |
[32m[20221213 23:27:08 @agent_ppo2.py:185][0m |          -0.0104 |          59.6229 |          12.4790 |
[32m[20221213 23:27:08 @agent_ppo2.py:185][0m |          -0.0123 |          58.8356 |          12.4801 |
[32m[20221213 23:27:08 @agent_ppo2.py:185][0m |          -0.0102 |          58.1705 |          12.4519 |
[32m[20221213 23:27:08 @agent_ppo2.py:185][0m |          -0.0086 |          57.9809 |          12.4490 |
[32m[20221213 23:27:08 @agent_ppo2.py:185][0m |          -0.0125 |          57.5993 |          12.4429 |
[32m[20221213 23:27:08 @agent_ppo2.py:185][0m |          -0.0116 |          57.5711 |          12.4326 |
[32m[20221213 23:27:09 @agent_ppo2.py:185][0m |          -0.0090 |          58.1010 |          12.4470 |
[32m[20221213 23:27:09 @agent_ppo2.py:185][0m |          -0.0149 |          57.1558 |          12.4605 |
[32m[20221213 23:27:09 @agent_ppo2.py:185][0m |          -0.0110 |          57.2692 |          12.4190 |
[32m[20221213 23:27:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.11
[32m[20221213 23:27:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.35
[32m[20221213 23:27:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.31
[32m[20221213 23:27:09 @agent_ppo2.py:143][0m Total time:      14.61 min
[32m[20221213 23:27:09 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221213 23:27:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2692 --------------------------#
[32m[20221213 23:27:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:09 @agent_ppo2.py:185][0m |           0.0030 |          47.3839 |          12.5665 |
[32m[20221213 23:27:09 @agent_ppo2.py:185][0m |           0.0023 |          43.0338 |          12.5578 |
[32m[20221213 23:27:09 @agent_ppo2.py:185][0m |           0.0038 |          43.0826 |          12.5436 |
[32m[20221213 23:27:09 @agent_ppo2.py:185][0m |          -0.0039 |          40.7773 |          12.5290 |
[32m[20221213 23:27:10 @agent_ppo2.py:185][0m |          -0.0124 |          39.5352 |          12.5093 |
[32m[20221213 23:27:10 @agent_ppo2.py:185][0m |          -0.0131 |          39.3100 |          12.5175 |
[32m[20221213 23:27:10 @agent_ppo2.py:185][0m |          -0.0140 |          38.6701 |          12.4930 |
[32m[20221213 23:27:10 @agent_ppo2.py:185][0m |          -0.0132 |          38.2238 |          12.4770 |
[32m[20221213 23:27:10 @agent_ppo2.py:185][0m |          -0.0143 |          37.8456 |          12.5012 |
[32m[20221213 23:27:10 @agent_ppo2.py:185][0m |          -0.0163 |          37.7840 |          12.4518 |
[32m[20221213 23:27:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:27:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.10
[32m[20221213 23:27:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.82
[32m[20221213 23:27:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.32
[32m[20221213 23:27:10 @agent_ppo2.py:143][0m Total time:      14.63 min
[32m[20221213 23:27:10 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221213 23:27:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2693 --------------------------#
[32m[20221213 23:27:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:10 @agent_ppo2.py:185][0m |           0.0166 |          39.4389 |          12.5986 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0015 |          29.5141 |          12.5688 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0106 |          28.6028 |          12.5715 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |           0.0164 |          31.1926 |          12.5720 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0120 |          27.7977 |          12.5096 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0107 |          27.3853 |          12.5129 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0129 |          27.1320 |          12.5384 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0178 |          26.9930 |          12.4943 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0132 |          26.7772 |          12.5492 |
[32m[20221213 23:27:11 @agent_ppo2.py:185][0m |          -0.0152 |          26.5655 |          12.4989 |
[32m[20221213 23:27:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.05
[32m[20221213 23:27:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.66
[32m[20221213 23:27:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.50
[32m[20221213 23:27:11 @agent_ppo2.py:143][0m Total time:      14.66 min
[32m[20221213 23:27:11 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221213 23:27:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2694 --------------------------#
[32m[20221213 23:27:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0033 |          53.3003 |          12.2269 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0054 |          48.5348 |          12.2018 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0119 |          46.7174 |          12.2290 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0092 |          45.6559 |          12.2138 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0115 |          44.4520 |          12.2442 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0127 |          43.5754 |          12.2554 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0161 |          43.1680 |          12.2466 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0168 |          43.0347 |          12.2823 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0172 |          42.5539 |          12.2837 |
[32m[20221213 23:27:12 @agent_ppo2.py:185][0m |          -0.0170 |          42.3373 |          12.3092 |
[32m[20221213 23:27:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.12
[32m[20221213 23:27:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.83
[32m[20221213 23:27:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.15
[32m[20221213 23:27:13 @agent_ppo2.py:143][0m Total time:      14.68 min
[32m[20221213 23:27:13 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221213 23:27:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2695 --------------------------#
[32m[20221213 23:27:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:13 @agent_ppo2.py:185][0m |           0.0029 |          60.9849 |          12.5184 |
[32m[20221213 23:27:13 @agent_ppo2.py:185][0m |          -0.0060 |          51.5054 |          12.4885 |
[32m[20221213 23:27:13 @agent_ppo2.py:185][0m |          -0.0073 |          49.9638 |          12.4818 |
[32m[20221213 23:27:13 @agent_ppo2.py:185][0m |          -0.0083 |          49.1614 |          12.4703 |
[32m[20221213 23:27:13 @agent_ppo2.py:185][0m |          -0.0018 |          57.3744 |          12.4785 |
[32m[20221213 23:27:13 @agent_ppo2.py:185][0m |          -0.0109 |          48.1904 |          12.4540 |
[32m[20221213 23:27:13 @agent_ppo2.py:185][0m |          -0.0140 |          47.7623 |          12.4599 |
[32m[20221213 23:27:14 @agent_ppo2.py:185][0m |          -0.0140 |          47.5439 |          12.4877 |
[32m[20221213 23:27:14 @agent_ppo2.py:185][0m |          -0.0154 |          47.1745 |          12.4988 |
[32m[20221213 23:27:14 @agent_ppo2.py:185][0m |          -0.0151 |          46.9562 |          12.4668 |
[32m[20221213 23:27:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.64
[32m[20221213 23:27:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.96
[32m[20221213 23:27:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.21
[32m[20221213 23:27:14 @agent_ppo2.py:143][0m Total time:      14.70 min
[32m[20221213 23:27:14 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221213 23:27:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2696 --------------------------#
[32m[20221213 23:27:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:14 @agent_ppo2.py:185][0m |           0.0032 |          64.1244 |          12.6408 |
[32m[20221213 23:27:14 @agent_ppo2.py:185][0m |          -0.0091 |          59.7063 |          12.6335 |
[32m[20221213 23:27:14 @agent_ppo2.py:185][0m |          -0.0137 |          58.1203 |          12.6244 |
[32m[20221213 23:27:14 @agent_ppo2.py:185][0m |          -0.0135 |          57.4768 |          12.5985 |
[32m[20221213 23:27:15 @agent_ppo2.py:185][0m |          -0.0132 |          56.4875 |          12.6178 |
[32m[20221213 23:27:15 @agent_ppo2.py:185][0m |          -0.0123 |          56.1867 |          12.6390 |
[32m[20221213 23:27:15 @agent_ppo2.py:185][0m |          -0.0133 |          55.9201 |          12.6379 |
[32m[20221213 23:27:15 @agent_ppo2.py:185][0m |          -0.0127 |          55.6175 |          12.6447 |
[32m[20221213 23:27:15 @agent_ppo2.py:185][0m |          -0.0136 |          55.2594 |          12.6553 |
[32m[20221213 23:27:15 @agent_ppo2.py:185][0m |          -0.0185 |          54.9583 |          12.6416 |
[32m[20221213 23:27:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.57
[32m[20221213 23:27:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.46
[32m[20221213 23:27:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.77
[32m[20221213 23:27:15 @agent_ppo2.py:143][0m Total time:      14.72 min
[32m[20221213 23:27:15 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221213 23:27:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2697 --------------------------#
[32m[20221213 23:27:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:15 @agent_ppo2.py:185][0m |           0.0017 |          58.2981 |          12.7333 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0020 |          55.4556 |          12.6931 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0117 |          51.5142 |          12.6748 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0137 |          50.0889 |          12.6997 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0117 |          49.1264 |          12.6956 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0148 |          48.3889 |          12.6988 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0154 |          47.6781 |          12.7080 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0152 |          47.2732 |          12.7221 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0149 |          46.7028 |          12.7461 |
[32m[20221213 23:27:16 @agent_ppo2.py:185][0m |          -0.0162 |          46.3067 |          12.7369 |
[32m[20221213 23:27:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:27:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.89
[32m[20221213 23:27:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.35
[32m[20221213 23:27:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.02
[32m[20221213 23:27:16 @agent_ppo2.py:143][0m Total time:      14.74 min
[32m[20221213 23:27:16 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221213 23:27:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2698 --------------------------#
[32m[20221213 23:27:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |           0.0039 |          63.2903 |          12.6444 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0049 |          60.8243 |          12.6541 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0043 |          60.6625 |          12.6561 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |           0.0006 |          62.0773 |          12.7245 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0097 |          59.5295 |          12.6847 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0098 |          59.3574 |          12.7072 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0092 |          59.2301 |          12.7201 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0100 |          59.2318 |          12.7304 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0118 |          59.0440 |          12.7071 |
[32m[20221213 23:27:17 @agent_ppo2.py:185][0m |          -0.0099 |          59.2636 |          12.7415 |
[32m[20221213 23:27:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:27:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.46
[32m[20221213 23:27:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.25
[32m[20221213 23:27:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.35
[32m[20221213 23:27:18 @agent_ppo2.py:143][0m Total time:      14.76 min
[32m[20221213 23:27:18 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221213 23:27:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2699 --------------------------#
[32m[20221213 23:27:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:18 @agent_ppo2.py:185][0m |          -0.0044 |          58.9732 |          12.6077 |
[32m[20221213 23:27:18 @agent_ppo2.py:185][0m |          -0.0065 |          58.3806 |          12.6291 |
[32m[20221213 23:27:18 @agent_ppo2.py:185][0m |          -0.0072 |          58.0821 |          12.6276 |
[32m[20221213 23:27:18 @agent_ppo2.py:185][0m |          -0.0082 |          57.9707 |          12.6482 |
[32m[20221213 23:27:18 @agent_ppo2.py:185][0m |          -0.0122 |          58.0559 |          12.6474 |
[32m[20221213 23:27:18 @agent_ppo2.py:185][0m |          -0.0104 |          57.6624 |          12.6539 |
[32m[20221213 23:27:18 @agent_ppo2.py:185][0m |          -0.0109 |          57.5241 |          12.6543 |
[32m[20221213 23:27:19 @agent_ppo2.py:185][0m |          -0.0088 |          57.4066 |          12.6482 |
[32m[20221213 23:27:19 @agent_ppo2.py:185][0m |          -0.0090 |          57.3744 |          12.6474 |
[32m[20221213 23:27:19 @agent_ppo2.py:185][0m |          -0.0096 |          57.2373 |          12.6231 |
[32m[20221213 23:27:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:27:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.49
[32m[20221213 23:27:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.21
[32m[20221213 23:27:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.65
[32m[20221213 23:27:19 @agent_ppo2.py:143][0m Total time:      14.78 min
[32m[20221213 23:27:19 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221213 23:27:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2700 --------------------------#
[32m[20221213 23:27:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:27:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:19 @agent_ppo2.py:185][0m |           0.0017 |          49.6608 |          12.7308 |
[32m[20221213 23:27:19 @agent_ppo2.py:185][0m |          -0.0033 |          44.9870 |          12.6458 |
[32m[20221213 23:27:19 @agent_ppo2.py:185][0m |          -0.0073 |          43.7316 |          12.6619 |
[32m[20221213 23:27:19 @agent_ppo2.py:185][0m |          -0.0066 |          42.8744 |          12.6329 |
[32m[20221213 23:27:20 @agent_ppo2.py:185][0m |          -0.0100 |          42.3447 |          12.6414 |
[32m[20221213 23:27:20 @agent_ppo2.py:185][0m |           0.0034 |          45.3551 |          12.6478 |
[32m[20221213 23:27:20 @agent_ppo2.py:185][0m |          -0.0103 |          41.8198 |          12.6271 |
[32m[20221213 23:27:20 @agent_ppo2.py:185][0m |          -0.0080 |          41.4774 |          12.6350 |
[32m[20221213 23:27:20 @agent_ppo2.py:185][0m |          -0.0051 |          41.8279 |          12.5832 |
[32m[20221213 23:27:20 @agent_ppo2.py:185][0m |          -0.0118 |          40.9854 |          12.6242 |
[32m[20221213 23:27:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.28
[32m[20221213 23:27:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.91
[32m[20221213 23:27:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.57
[32m[20221213 23:27:20 @agent_ppo2.py:143][0m Total time:      14.80 min
[32m[20221213 23:27:20 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221213 23:27:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2701 --------------------------#
[32m[20221213 23:27:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:20 @agent_ppo2.py:185][0m |          -0.0009 |          46.4516 |          12.3420 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0077 |          40.0198 |          12.3535 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0085 |          37.7250 |          12.3701 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0093 |          36.7796 |          12.4012 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0071 |          36.6336 |          12.4072 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0094 |          35.4199 |          12.4392 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0138 |          34.8409 |          12.4234 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0115 |          34.5229 |          12.4710 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0155 |          34.1651 |          12.4549 |
[32m[20221213 23:27:21 @agent_ppo2.py:185][0m |          -0.0167 |          33.7179 |          12.4921 |
[32m[20221213 23:27:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:27:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.64
[32m[20221213 23:27:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.99
[32m[20221213 23:27:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.52
[32m[20221213 23:27:21 @agent_ppo2.py:143][0m Total time:      14.82 min
[32m[20221213 23:27:21 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221213 23:27:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2702 --------------------------#
[32m[20221213 23:27:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0003 |          53.7087 |          12.4036 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0015 |          50.2075 |          12.3555 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0023 |          49.4957 |          12.3404 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0067 |          48.5742 |          12.3530 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0062 |          48.0280 |          12.2878 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0095 |          47.7054 |          12.3032 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |           0.0040 |          52.2912 |          12.2683 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |           0.0046 |          51.5815 |          12.2327 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0078 |          46.8507 |          12.2160 |
[32m[20221213 23:27:22 @agent_ppo2.py:185][0m |          -0.0087 |          47.2428 |          12.2171 |
[32m[20221213 23:27:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:27:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.04
[32m[20221213 23:27:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.28
[32m[20221213 23:27:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.38
[32m[20221213 23:27:23 @agent_ppo2.py:143][0m Total time:      14.84 min
[32m[20221213 23:27:23 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221213 23:27:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2703 --------------------------#
[32m[20221213 23:27:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:23 @agent_ppo2.py:185][0m |           0.0070 |          70.2030 |          12.3568 |
[32m[20221213 23:27:23 @agent_ppo2.py:185][0m |          -0.0041 |          65.4663 |          12.3779 |
[32m[20221213 23:27:23 @agent_ppo2.py:185][0m |          -0.0049 |          64.5614 |          12.3407 |
[32m[20221213 23:27:23 @agent_ppo2.py:185][0m |          -0.0096 |          63.3259 |          12.3419 |
[32m[20221213 23:27:23 @agent_ppo2.py:185][0m |          -0.0046 |          63.3897 |          12.3182 |
[32m[20221213 23:27:23 @agent_ppo2.py:185][0m |          -0.0105 |          62.0759 |          12.2898 |
[32m[20221213 23:27:23 @agent_ppo2.py:185][0m |          -0.0123 |          61.4564 |          12.3057 |
[32m[20221213 23:27:24 @agent_ppo2.py:185][0m |          -0.0112 |          61.0834 |          12.2823 |
[32m[20221213 23:27:24 @agent_ppo2.py:185][0m |          -0.0096 |          60.7515 |          12.3440 |
[32m[20221213 23:27:24 @agent_ppo2.py:185][0m |          -0.0155 |          60.6133 |          12.3148 |
[32m[20221213 23:27:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.30
[32m[20221213 23:27:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.55
[32m[20221213 23:27:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.62
[32m[20221213 23:27:24 @agent_ppo2.py:143][0m Total time:      14.86 min
[32m[20221213 23:27:24 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221213 23:27:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2704 --------------------------#
[32m[20221213 23:27:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:24 @agent_ppo2.py:185][0m |           0.0102 |          50.4524 |          12.0777 |
[32m[20221213 23:27:24 @agent_ppo2.py:185][0m |           0.0010 |          45.2429 |          12.1426 |
[32m[20221213 23:27:24 @agent_ppo2.py:185][0m |          -0.0024 |          44.5344 |          12.1248 |
[32m[20221213 23:27:24 @agent_ppo2.py:185][0m |           0.0041 |          44.9303 |          12.1176 |
[32m[20221213 23:27:25 @agent_ppo2.py:185][0m |          -0.0026 |          42.9760 |          12.0892 |
[32m[20221213 23:27:25 @agent_ppo2.py:185][0m |          -0.0097 |          41.8598 |          12.0705 |
[32m[20221213 23:27:25 @agent_ppo2.py:185][0m |          -0.0123 |          41.5226 |          12.0379 |
[32m[20221213 23:27:25 @agent_ppo2.py:185][0m |          -0.0116 |          41.3310 |          12.1066 |
[32m[20221213 23:27:25 @agent_ppo2.py:185][0m |          -0.0043 |          42.1980 |          12.0232 |
[32m[20221213 23:27:25 @agent_ppo2.py:185][0m |           0.0015 |          46.4205 |          12.0157 |
[32m[20221213 23:27:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.90
[32m[20221213 23:27:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.03
[32m[20221213 23:27:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.89
[32m[20221213 23:27:25 @agent_ppo2.py:143][0m Total time:      14.89 min
[32m[20221213 23:27:25 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221213 23:27:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2705 --------------------------#
[32m[20221213 23:27:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:25 @agent_ppo2.py:185][0m |          -0.0007 |          54.2574 |          12.2011 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0027 |          51.2081 |          12.1903 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0081 |          50.1767 |          12.1304 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0089 |          49.3117 |          12.1396 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0061 |          48.9373 |          12.1172 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0118 |          48.4197 |          12.0991 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0084 |          48.6663 |          12.0947 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0107 |          47.9810 |          12.0926 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0132 |          47.6059 |          12.0408 |
[32m[20221213 23:27:26 @agent_ppo2.py:185][0m |          -0.0159 |          47.5655 |          12.0417 |
[32m[20221213 23:27:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.05
[32m[20221213 23:27:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.11
[32m[20221213 23:27:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.40
[32m[20221213 23:27:26 @agent_ppo2.py:143][0m Total time:      14.91 min
[32m[20221213 23:27:26 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221213 23:27:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2706 --------------------------#
[32m[20221213 23:27:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |           0.0038 |          63.1609 |          12.1992 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0005 |          60.6856 |          12.2069 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0052 |          58.0562 |          12.1647 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0098 |          57.1901 |          12.1636 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0073 |          56.5822 |          12.1671 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0097 |          56.3118 |          12.1565 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0085 |          55.8027 |          12.1771 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0096 |          55.7506 |          12.1798 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |           0.0014 |          62.9752 |          12.1613 |
[32m[20221213 23:27:27 @agent_ppo2.py:185][0m |          -0.0020 |          58.8733 |          12.1768 |
[32m[20221213 23:27:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:27:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.44
[32m[20221213 23:27:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.25
[32m[20221213 23:27:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.00
[32m[20221213 23:27:28 @agent_ppo2.py:143][0m Total time:      14.93 min
[32m[20221213 23:27:28 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221213 23:27:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2707 --------------------------#
[32m[20221213 23:27:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:28 @agent_ppo2.py:185][0m |           0.0028 |          61.0865 |          12.2678 |
[32m[20221213 23:27:28 @agent_ppo2.py:185][0m |          -0.0018 |          57.8523 |          12.2812 |
[32m[20221213 23:27:28 @agent_ppo2.py:185][0m |          -0.0044 |          56.9733 |          12.2814 |
[32m[20221213 23:27:28 @agent_ppo2.py:185][0m |          -0.0064 |          56.3493 |          12.2480 |
[32m[20221213 23:27:28 @agent_ppo2.py:185][0m |          -0.0085 |          56.0283 |          12.3226 |
[32m[20221213 23:27:28 @agent_ppo2.py:185][0m |          -0.0064 |          55.9677 |          12.2280 |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |          -0.0100 |          55.5891 |          12.2678 |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |          -0.0102 |          55.5298 |          12.2700 |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |          -0.0103 |          55.2423 |          12.2811 |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |          -0.0076 |          55.0733 |          12.2804 |
[32m[20221213 23:27:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.02
[32m[20221213 23:27:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.56
[32m[20221213 23:27:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.78
[32m[20221213 23:27:29 @agent_ppo2.py:143][0m Total time:      14.95 min
[32m[20221213 23:27:29 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221213 23:27:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2708 --------------------------#
[32m[20221213 23:27:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |           0.0095 |          56.1140 |          11.9992 |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |           0.0061 |          52.6277 |          12.0614 |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |          -0.0030 |          50.2876 |          12.0086 |
[32m[20221213 23:27:29 @agent_ppo2.py:185][0m |          -0.0041 |          49.2146 |          12.0449 |
[32m[20221213 23:27:30 @agent_ppo2.py:185][0m |          -0.0082 |          48.2924 |          12.0210 |
[32m[20221213 23:27:30 @agent_ppo2.py:185][0m |          -0.0090 |          47.5235 |          12.0428 |
[32m[20221213 23:27:30 @agent_ppo2.py:185][0m |           0.0001 |          50.7582 |          12.0098 |
[32m[20221213 23:27:30 @agent_ppo2.py:185][0m |          -0.0100 |          46.3894 |          12.0027 |
[32m[20221213 23:27:30 @agent_ppo2.py:185][0m |          -0.0049 |          49.2447 |          12.0103 |
[32m[20221213 23:27:30 @agent_ppo2.py:185][0m |          -0.0103 |          45.4448 |          12.0049 |
[32m[20221213 23:27:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.73
[32m[20221213 23:27:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.16
[32m[20221213 23:27:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.45
[32m[20221213 23:27:30 @agent_ppo2.py:143][0m Total time:      14.97 min
[32m[20221213 23:27:30 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221213 23:27:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2709 --------------------------#
[32m[20221213 23:27:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:30 @agent_ppo2.py:185][0m |          -0.0017 |          68.0903 |          11.7061 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0060 |          65.4481 |          11.6578 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0046 |          65.3638 |          11.6995 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0095 |          63.8296 |          11.7005 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0104 |          63.9484 |          11.7015 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0113 |          63.3786 |          11.6936 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0130 |          62.9853 |          11.6939 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0052 |          67.3401 |          11.6718 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0128 |          62.6036 |          11.7214 |
[32m[20221213 23:27:31 @agent_ppo2.py:185][0m |          -0.0112 |          62.6595 |          11.7281 |
[32m[20221213 23:27:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.09
[32m[20221213 23:27:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.55
[32m[20221213 23:27:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.90
[32m[20221213 23:27:31 @agent_ppo2.py:143][0m Total time:      14.99 min
[32m[20221213 23:27:31 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221213 23:27:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2710 --------------------------#
[32m[20221213 23:27:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:27:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |           0.0003 |          66.9108 |          12.0425 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0053 |          60.8798 |          12.1064 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0111 |          59.4236 |          12.1308 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0092 |          58.7841 |          12.1955 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0048 |          58.8083 |          12.2030 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0111 |          58.0116 |          12.2030 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0131 |          57.2023 |          12.2034 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0142 |          56.8461 |          12.2065 |
[32m[20221213 23:27:32 @agent_ppo2.py:185][0m |          -0.0184 |          56.8527 |          12.2196 |
[32m[20221213 23:27:33 @agent_ppo2.py:185][0m |          -0.0167 |          56.6054 |          12.2625 |
[32m[20221213 23:27:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.26
[32m[20221213 23:27:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.80
[32m[20221213 23:27:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.80
[32m[20221213 23:27:33 @agent_ppo2.py:143][0m Total time:      15.01 min
[32m[20221213 23:27:33 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221213 23:27:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2711 --------------------------#
[32m[20221213 23:27:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:33 @agent_ppo2.py:185][0m |           0.0002 |          36.8612 |          12.1814 |
[32m[20221213 23:27:33 @agent_ppo2.py:185][0m |          -0.0026 |          33.7207 |          12.1680 |
[32m[20221213 23:27:33 @agent_ppo2.py:185][0m |          -0.0062 |          31.4624 |          12.1676 |
[32m[20221213 23:27:33 @agent_ppo2.py:185][0m |          -0.0084 |          30.6596 |          12.2039 |
[32m[20221213 23:27:33 @agent_ppo2.py:185][0m |          -0.0131 |          29.8277 |          12.1641 |
[32m[20221213 23:27:33 @agent_ppo2.py:185][0m |           0.0041 |          32.2125 |          12.1394 |
[32m[20221213 23:27:34 @agent_ppo2.py:185][0m |          -0.0107 |          29.1305 |          12.1133 |
[32m[20221213 23:27:34 @agent_ppo2.py:185][0m |          -0.0157 |          28.2985 |          12.1305 |
[32m[20221213 23:27:34 @agent_ppo2.py:185][0m |          -0.0179 |          27.8607 |          12.0908 |
[32m[20221213 23:27:34 @agent_ppo2.py:185][0m |          -0.0184 |          27.6112 |          12.0837 |
[32m[20221213 23:27:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:27:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.24
[32m[20221213 23:27:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.93
[32m[20221213 23:27:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.59
[32m[20221213 23:27:34 @agent_ppo2.py:143][0m Total time:      15.03 min
[32m[20221213 23:27:34 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221213 23:27:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2712 --------------------------#
[32m[20221213 23:27:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:34 @agent_ppo2.py:185][0m |           0.0020 |          61.2934 |          12.0971 |
[32m[20221213 23:27:34 @agent_ppo2.py:185][0m |           0.0032 |          55.5181 |          12.0840 |
[32m[20221213 23:27:34 @agent_ppo2.py:185][0m |          -0.0038 |          53.8510 |          12.0680 |
[32m[20221213 23:27:35 @agent_ppo2.py:185][0m |          -0.0102 |          52.8456 |          12.0268 |
[32m[20221213 23:27:35 @agent_ppo2.py:185][0m |           0.0057 |          53.9187 |          12.0512 |
[32m[20221213 23:27:35 @agent_ppo2.py:185][0m |          -0.0072 |          51.4251 |          12.0415 |
[32m[20221213 23:27:35 @agent_ppo2.py:185][0m |          -0.0073 |          50.9141 |          12.0108 |
[32m[20221213 23:27:35 @agent_ppo2.py:185][0m |          -0.0057 |          50.7882 |          11.9942 |
[32m[20221213 23:27:35 @agent_ppo2.py:185][0m |          -0.0063 |          50.8547 |          12.0043 |
[32m[20221213 23:27:35 @agent_ppo2.py:185][0m |          -0.0117 |          50.5211 |          12.0327 |
[32m[20221213 23:27:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.35
[32m[20221213 23:27:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.20
[32m[20221213 23:27:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.96
[32m[20221213 23:27:35 @agent_ppo2.py:143][0m Total time:      15.05 min
[32m[20221213 23:27:35 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221213 23:27:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2713 --------------------------#
[32m[20221213 23:27:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |           0.0013 |          63.4240 |          11.8709 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |           0.0109 |          65.8692 |          11.9112 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0063 |          60.6158 |          11.8750 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0101 |          60.2584 |          11.8833 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0081 |          60.1220 |          11.8386 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0090 |          59.8429 |          11.8295 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0110 |          59.7580 |          11.8501 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0106 |          59.5215 |          11.8098 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0100 |          59.4179 |          11.7759 |
[32m[20221213 23:27:36 @agent_ppo2.py:185][0m |          -0.0096 |          59.4442 |          11.7544 |
[32m[20221213 23:27:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.71
[32m[20221213 23:27:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.79
[32m[20221213 23:27:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.63
[32m[20221213 23:27:36 @agent_ppo2.py:143][0m Total time:      15.07 min
[32m[20221213 23:27:36 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221213 23:27:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2714 --------------------------#
[32m[20221213 23:27:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |           0.0003 |          55.3807 |          11.7008 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |           0.0067 |          50.8319 |          11.7014 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |          -0.0061 |          46.1185 |          11.6229 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |          -0.0070 |          44.2803 |          11.6658 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |          -0.0075 |          43.1391 |          11.5518 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |          -0.0047 |          42.5713 |          11.5714 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |          -0.0120 |          41.6265 |          11.5155 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |          -0.0084 |          41.0416 |          11.5392 |
[32m[20221213 23:27:37 @agent_ppo2.py:185][0m |          -0.0101 |          40.6361 |          11.5338 |
[32m[20221213 23:27:38 @agent_ppo2.py:185][0m |          -0.0082 |          40.5297 |          11.5201 |
[32m[20221213 23:27:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.18
[32m[20221213 23:27:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.99
[32m[20221213 23:27:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.35
[32m[20221213 23:27:38 @agent_ppo2.py:143][0m Total time:      15.09 min
[32m[20221213 23:27:38 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221213 23:27:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2715 --------------------------#
[32m[20221213 23:27:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:38 @agent_ppo2.py:185][0m |           0.0011 |          55.3281 |          11.6202 |
[32m[20221213 23:27:38 @agent_ppo2.py:185][0m |          -0.0007 |          50.3644 |          11.6200 |
[32m[20221213 23:27:38 @agent_ppo2.py:185][0m |          -0.0056 |          48.7206 |          11.6610 |
[32m[20221213 23:27:38 @agent_ppo2.py:185][0m |           0.0052 |          49.1295 |          11.6762 |
[32m[20221213 23:27:38 @agent_ppo2.py:185][0m |           0.0014 |          48.3716 |          11.7594 |
[32m[20221213 23:27:38 @agent_ppo2.py:185][0m |          -0.0038 |          46.4897 |          11.7285 |
[32m[20221213 23:27:39 @agent_ppo2.py:185][0m |          -0.0050 |          46.1065 |          11.7092 |
[32m[20221213 23:27:39 @agent_ppo2.py:185][0m |          -0.0051 |          45.9044 |          11.7022 |
[32m[20221213 23:27:39 @agent_ppo2.py:185][0m |          -0.0048 |          46.1592 |          11.7221 |
[32m[20221213 23:27:39 @agent_ppo2.py:185][0m |          -0.0061 |          45.3785 |          11.7571 |
[32m[20221213 23:27:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.51
[32m[20221213 23:27:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.76
[32m[20221213 23:27:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.02
[32m[20221213 23:27:39 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221213 23:27:39 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221213 23:27:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2716 --------------------------#
[32m[20221213 23:27:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:39 @agent_ppo2.py:185][0m |           0.0010 |          58.7358 |          11.6156 |
[32m[20221213 23:27:39 @agent_ppo2.py:185][0m |          -0.0050 |          55.3556 |          11.6105 |
[32m[20221213 23:27:39 @agent_ppo2.py:185][0m |          -0.0085 |          53.0560 |          11.6451 |
[32m[20221213 23:27:40 @agent_ppo2.py:185][0m |          -0.0114 |          52.0624 |          11.6397 |
[32m[20221213 23:27:40 @agent_ppo2.py:185][0m |          -0.0087 |          51.0535 |          11.6539 |
[32m[20221213 23:27:40 @agent_ppo2.py:185][0m |           0.0025 |          59.3322 |          11.7100 |
[32m[20221213 23:27:40 @agent_ppo2.py:185][0m |          -0.0127 |          50.4838 |          11.7305 |
[32m[20221213 23:27:40 @agent_ppo2.py:185][0m |          -0.0080 |          51.2280 |          11.7160 |
[32m[20221213 23:27:40 @agent_ppo2.py:185][0m |          -0.0119 |          49.4064 |          11.7214 |
[32m[20221213 23:27:40 @agent_ppo2.py:185][0m |          -0.0127 |          49.3472 |          11.7156 |
[32m[20221213 23:27:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:27:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.52
[32m[20221213 23:27:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.11
[32m[20221213 23:27:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.32
[32m[20221213 23:27:40 @agent_ppo2.py:143][0m Total time:      15.14 min
[32m[20221213 23:27:40 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221213 23:27:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2717 --------------------------#
[32m[20221213 23:27:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |           0.0007 |          61.5053 |          11.8708 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0092 |          59.3016 |          11.8200 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0083 |          58.3779 |          11.7816 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0058 |          58.4507 |          11.7882 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0097 |          57.6300 |          11.7892 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0123 |          57.2169 |          11.8018 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0085 |          59.5922 |          11.8071 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0096 |          57.1026 |          11.8148 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0116 |          56.5605 |          11.8591 |
[32m[20221213 23:27:41 @agent_ppo2.py:185][0m |          -0.0149 |          56.3171 |          11.8534 |
[32m[20221213 23:27:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.92
[32m[20221213 23:27:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.89
[32m[20221213 23:27:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.51
[32m[20221213 23:27:41 @agent_ppo2.py:143][0m Total time:      15.16 min
[32m[20221213 23:27:41 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221213 23:27:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2718 --------------------------#
[32m[20221213 23:27:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0019 |          42.8142 |          12.1027 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0071 |          40.1715 |          12.1218 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0002 |          41.1717 |          12.1478 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0088 |          38.5931 |          12.1299 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0096 |          37.9396 |          12.1841 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0082 |          37.6241 |          12.1902 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0115 |          37.2490 |          12.1922 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0171 |          37.1317 |          12.2200 |
[32m[20221213 23:27:42 @agent_ppo2.py:185][0m |          -0.0157 |          37.0919 |          12.2008 |
[32m[20221213 23:27:43 @agent_ppo2.py:185][0m |          -0.0152 |          36.7566 |          12.2158 |
[32m[20221213 23:27:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:27:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.48
[32m[20221213 23:27:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.91
[32m[20221213 23:27:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.27
[32m[20221213 23:27:43 @agent_ppo2.py:143][0m Total time:      15.18 min
[32m[20221213 23:27:43 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221213 23:27:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2719 --------------------------#
[32m[20221213 23:27:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:43 @agent_ppo2.py:185][0m |           0.0055 |          43.8485 |          12.2460 |
[32m[20221213 23:27:43 @agent_ppo2.py:185][0m |          -0.0054 |          38.7053 |          12.2429 |
[32m[20221213 23:27:43 @agent_ppo2.py:185][0m |          -0.0107 |          36.7501 |          12.2929 |
[32m[20221213 23:27:43 @agent_ppo2.py:185][0m |          -0.0111 |          35.4950 |          12.3513 |
[32m[20221213 23:27:43 @agent_ppo2.py:185][0m |          -0.0138 |          34.8097 |          12.3815 |
[32m[20221213 23:27:43 @agent_ppo2.py:185][0m |          -0.0200 |          34.1143 |          12.3710 |
[32m[20221213 23:27:44 @agent_ppo2.py:185][0m |          -0.0143 |          33.6122 |          12.3905 |
[32m[20221213 23:27:44 @agent_ppo2.py:185][0m |          -0.0135 |          33.3658 |          12.3950 |
[32m[20221213 23:27:44 @agent_ppo2.py:185][0m |          -0.0133 |          32.9227 |          12.4041 |
[32m[20221213 23:27:44 @agent_ppo2.py:185][0m |          -0.0167 |          32.9004 |          12.4817 |
[32m[20221213 23:27:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.64
[32m[20221213 23:27:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.62
[32m[20221213 23:27:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.09
[32m[20221213 23:27:44 @agent_ppo2.py:143][0m Total time:      15.20 min
[32m[20221213 23:27:44 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221213 23:27:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2720 --------------------------#
[32m[20221213 23:27:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:27:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:44 @agent_ppo2.py:185][0m |           0.0023 |          58.7371 |          12.2134 |
[32m[20221213 23:27:44 @agent_ppo2.py:185][0m |          -0.0059 |          56.1289 |          12.2290 |
[32m[20221213 23:27:44 @agent_ppo2.py:185][0m |          -0.0119 |          55.1965 |          12.2364 |
[32m[20221213 23:27:45 @agent_ppo2.py:185][0m |          -0.0084 |          54.5528 |          12.2557 |
[32m[20221213 23:27:45 @agent_ppo2.py:185][0m |          -0.0096 |          54.1738 |          12.1671 |
[32m[20221213 23:27:45 @agent_ppo2.py:185][0m |          -0.0127 |          53.9630 |          12.2039 |
[32m[20221213 23:27:45 @agent_ppo2.py:185][0m |          -0.0116 |          53.7469 |          12.2273 |
[32m[20221213 23:27:45 @agent_ppo2.py:185][0m |          -0.0108 |          53.5267 |          12.2249 |
[32m[20221213 23:27:45 @agent_ppo2.py:185][0m |          -0.0135 |          53.3827 |          12.2407 |
[32m[20221213 23:27:45 @agent_ppo2.py:185][0m |          -0.0146 |          53.1972 |          12.2351 |
[32m[20221213 23:27:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.29
[32m[20221213 23:27:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.76
[32m[20221213 23:27:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.54
[32m[20221213 23:27:45 @agent_ppo2.py:143][0m Total time:      15.22 min
[32m[20221213 23:27:45 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221213 23:27:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2721 --------------------------#
[32m[20221213 23:27:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0037 |          71.4084 |          12.2433 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0078 |          67.8314 |          12.1912 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0097 |          66.7078 |          12.2300 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0139 |          65.5492 |          12.2322 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0141 |          65.0742 |          12.2556 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0157 |          64.4094 |          12.2856 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0163 |          64.1086 |          12.3065 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0159 |          63.7705 |          12.3301 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0151 |          63.2050 |          12.3572 |
[32m[20221213 23:27:46 @agent_ppo2.py:185][0m |          -0.0166 |          63.3577 |          12.3664 |
[32m[20221213 23:27:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:27:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.99
[32m[20221213 23:27:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.99
[32m[20221213 23:27:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.55
[32m[20221213 23:27:46 @agent_ppo2.py:143][0m Total time:      15.24 min
[32m[20221213 23:27:46 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221213 23:27:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2722 --------------------------#
[32m[20221213 23:27:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |           0.0050 |          60.7053 |          12.6750 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0060 |          56.2293 |          12.7128 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0004 |          58.2479 |          12.6917 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0071 |          54.6970 |          12.6400 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0064 |          54.1305 |          12.7467 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0132 |          53.7876 |          12.6711 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0103 |          53.4804 |          12.6948 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0088 |          53.1709 |          12.7087 |
[32m[20221213 23:27:47 @agent_ppo2.py:185][0m |          -0.0141 |          52.9642 |          12.7150 |
[32m[20221213 23:27:48 @agent_ppo2.py:185][0m |          -0.0119 |          52.8028 |          12.7041 |
[32m[20221213 23:27:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.24
[32m[20221213 23:27:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.74
[32m[20221213 23:27:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.30
[32m[20221213 23:27:48 @agent_ppo2.py:143][0m Total time:      15.26 min
[32m[20221213 23:27:48 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221213 23:27:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2723 --------------------------#
[32m[20221213 23:27:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:48 @agent_ppo2.py:185][0m |           0.0024 |          51.7819 |          12.0903 |
[32m[20221213 23:27:48 @agent_ppo2.py:185][0m |          -0.0091 |          46.4263 |          12.0960 |
[32m[20221213 23:27:48 @agent_ppo2.py:185][0m |          -0.0028 |          45.0392 |          12.1229 |
[32m[20221213 23:27:48 @agent_ppo2.py:185][0m |          -0.0090 |          43.7502 |          12.1407 |
[32m[20221213 23:27:48 @agent_ppo2.py:185][0m |          -0.0122 |          43.0536 |          12.1771 |
[32m[20221213 23:27:48 @agent_ppo2.py:185][0m |          -0.0102 |          42.4814 |          12.1992 |
[32m[20221213 23:27:49 @agent_ppo2.py:185][0m |          -0.0097 |          42.2594 |          12.1801 |
[32m[20221213 23:27:49 @agent_ppo2.py:185][0m |          -0.0080 |          41.8935 |          12.1989 |
[32m[20221213 23:27:49 @agent_ppo2.py:185][0m |          -0.0113 |          41.8754 |          12.1931 |
[32m[20221213 23:27:49 @agent_ppo2.py:185][0m |          -0.0176 |          41.1738 |          12.2253 |
[32m[20221213 23:27:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:27:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.33
[32m[20221213 23:27:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.64
[32m[20221213 23:27:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.40
[32m[20221213 23:27:49 @agent_ppo2.py:143][0m Total time:      15.28 min
[32m[20221213 23:27:49 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221213 23:27:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2724 --------------------------#
[32m[20221213 23:27:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:49 @agent_ppo2.py:185][0m |          -0.0025 |          59.8984 |          13.2116 |
[32m[20221213 23:27:49 @agent_ppo2.py:185][0m |          -0.0070 |          55.3340 |          13.1676 |
[32m[20221213 23:27:49 @agent_ppo2.py:185][0m |          -0.0089 |          53.1979 |          13.1823 |
[32m[20221213 23:27:50 @agent_ppo2.py:185][0m |          -0.0090 |          52.2987 |          13.1844 |
[32m[20221213 23:27:50 @agent_ppo2.py:185][0m |          -0.0095 |          51.3231 |          13.1544 |
[32m[20221213 23:27:50 @agent_ppo2.py:185][0m |          -0.0145 |          50.9129 |          13.1493 |
[32m[20221213 23:27:50 @agent_ppo2.py:185][0m |          -0.0034 |          51.4294 |          13.1524 |
[32m[20221213 23:27:50 @agent_ppo2.py:185][0m |          -0.0111 |          49.8020 |          13.1557 |
[32m[20221213 23:27:50 @agent_ppo2.py:185][0m |          -0.0179 |          49.3781 |          13.0939 |
[32m[20221213 23:27:50 @agent_ppo2.py:185][0m |          -0.0142 |          48.9336 |          13.1469 |
[32m[20221213 23:27:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.40
[32m[20221213 23:27:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.38
[32m[20221213 23:27:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.92
[32m[20221213 23:27:50 @agent_ppo2.py:143][0m Total time:      15.30 min
[32m[20221213 23:27:50 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221213 23:27:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2725 --------------------------#
[32m[20221213 23:27:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |           0.0029 |          56.4185 |          12.2566 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0073 |          51.7932 |          12.2275 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0075 |          50.7296 |          12.2189 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0077 |          50.5543 |          12.2018 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0111 |          49.6091 |          12.2374 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0092 |          49.1349 |          12.2247 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0089 |          49.1333 |          12.2592 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0127 |          48.6121 |          12.2578 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0116 |          48.2554 |          12.2239 |
[32m[20221213 23:27:51 @agent_ppo2.py:185][0m |          -0.0119 |          47.9761 |          12.2365 |
[32m[20221213 23:27:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:27:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.87
[32m[20221213 23:27:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.89
[32m[20221213 23:27:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.41
[32m[20221213 23:27:51 @agent_ppo2.py:143][0m Total time:      15.32 min
[32m[20221213 23:27:51 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221213 23:27:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2726 --------------------------#
[32m[20221213 23:27:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0003 |          59.1386 |          12.9315 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0063 |          57.7187 |          12.9179 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0070 |          57.3607 |          12.9186 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0108 |          57.2647 |          12.9439 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0117 |          57.0931 |          12.9542 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0105 |          57.1009 |          12.9060 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0126 |          57.0345 |          12.9536 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0127 |          56.8040 |          12.8881 |
[32m[20221213 23:27:52 @agent_ppo2.py:185][0m |          -0.0076 |          57.6228 |          12.9141 |
[32m[20221213 23:27:53 @agent_ppo2.py:185][0m |          -0.0128 |          56.6905 |          12.8910 |
[32m[20221213 23:27:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.20
[32m[20221213 23:27:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.48
[32m[20221213 23:27:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.91
[32m[20221213 23:27:53 @agent_ppo2.py:143][0m Total time:      15.35 min
[32m[20221213 23:27:53 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221213 23:27:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2727 --------------------------#
[32m[20221213 23:27:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:53 @agent_ppo2.py:185][0m |           0.0010 |          45.3080 |          12.7506 |
[32m[20221213 23:27:53 @agent_ppo2.py:185][0m |          -0.0070 |          43.5664 |          12.7801 |
[32m[20221213 23:27:53 @agent_ppo2.py:185][0m |          -0.0079 |          43.0686 |          12.7391 |
[32m[20221213 23:27:53 @agent_ppo2.py:185][0m |          -0.0095 |          42.7861 |          12.7391 |
[32m[20221213 23:27:53 @agent_ppo2.py:185][0m |          -0.0035 |          42.8827 |          12.7286 |
[32m[20221213 23:27:53 @agent_ppo2.py:185][0m |          -0.0127 |          42.3692 |          12.7201 |
[32m[20221213 23:27:54 @agent_ppo2.py:185][0m |          -0.0100 |          42.1755 |          12.7036 |
[32m[20221213 23:27:54 @agent_ppo2.py:185][0m |          -0.0132 |          42.0298 |          12.7279 |
[32m[20221213 23:27:54 @agent_ppo2.py:185][0m |          -0.0139 |          41.9957 |          12.7322 |
[32m[20221213 23:27:54 @agent_ppo2.py:185][0m |          -0.0123 |          41.7215 |          12.7422 |
[32m[20221213 23:27:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:27:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.61
[32m[20221213 23:27:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.56
[32m[20221213 23:27:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.36
[32m[20221213 23:27:54 @agent_ppo2.py:143][0m Total time:      15.37 min
[32m[20221213 23:27:54 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221213 23:27:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2728 --------------------------#
[32m[20221213 23:27:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:54 @agent_ppo2.py:185][0m |          -0.0001 |          56.9919 |          12.6252 |
[32m[20221213 23:27:54 @agent_ppo2.py:185][0m |          -0.0040 |          52.3708 |          12.6487 |
[32m[20221213 23:27:54 @agent_ppo2.py:185][0m |          -0.0076 |          51.2652 |          12.6096 |
[32m[20221213 23:27:55 @agent_ppo2.py:185][0m |          -0.0125 |          50.2959 |          12.6147 |
[32m[20221213 23:27:55 @agent_ppo2.py:185][0m |          -0.0153 |          49.6662 |          12.6133 |
[32m[20221213 23:27:55 @agent_ppo2.py:185][0m |          -0.0102 |          49.0992 |          12.5976 |
[32m[20221213 23:27:55 @agent_ppo2.py:185][0m |          -0.0100 |          49.1243 |          12.5699 |
[32m[20221213 23:27:55 @agent_ppo2.py:185][0m |          -0.0153 |          48.3594 |          12.5893 |
[32m[20221213 23:27:55 @agent_ppo2.py:185][0m |          -0.0153 |          48.0174 |          12.5662 |
[32m[20221213 23:27:55 @agent_ppo2.py:185][0m |          -0.0192 |          47.5887 |          12.5507 |
[32m[20221213 23:27:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:27:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.12
[32m[20221213 23:27:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.68
[32m[20221213 23:27:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.08
[32m[20221213 23:27:55 @agent_ppo2.py:143][0m Total time:      15.39 min
[32m[20221213 23:27:55 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221213 23:27:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2729 --------------------------#
[32m[20221213 23:27:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |           0.0040 |          61.5571 |          12.5749 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0050 |          54.8202 |          12.5896 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0053 |          53.0096 |          12.5760 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0081 |          51.6443 |          12.5972 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0091 |          51.1978 |          12.5867 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0093 |          50.5233 |          12.5943 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0088 |          49.7177 |          12.5932 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0131 |          49.7462 |          12.6023 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0079 |          48.9458 |          12.5864 |
[32m[20221213 23:27:56 @agent_ppo2.py:185][0m |          -0.0174 |          48.6531 |          12.6130 |
[32m[20221213 23:27:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:27:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.36
[32m[20221213 23:27:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.17
[32m[20221213 23:27:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.37
[32m[20221213 23:27:57 @agent_ppo2.py:143][0m Total time:      15.41 min
[32m[20221213 23:27:57 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221213 23:27:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2730 --------------------------#
[32m[20221213 23:27:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:27:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:57 @agent_ppo2.py:185][0m |           0.0090 |          57.4160 |          12.3787 |
[32m[20221213 23:27:57 @agent_ppo2.py:185][0m |          -0.0050 |          53.1636 |          12.3678 |
[32m[20221213 23:27:57 @agent_ppo2.py:185][0m |          -0.0079 |          51.9675 |          12.3871 |
[32m[20221213 23:27:57 @agent_ppo2.py:185][0m |          -0.0102 |          51.2688 |          12.3766 |
[32m[20221213 23:27:57 @agent_ppo2.py:185][0m |          -0.0043 |          52.9496 |          12.3819 |
[32m[20221213 23:27:57 @agent_ppo2.py:185][0m |          -0.0131 |          50.4436 |          12.4081 |
[32m[20221213 23:27:57 @agent_ppo2.py:185][0m |          -0.0001 |          56.9643 |          12.3818 |
[32m[20221213 23:27:58 @agent_ppo2.py:185][0m |          -0.0131 |          49.9170 |          12.4095 |
[32m[20221213 23:27:58 @agent_ppo2.py:185][0m |          -0.0155 |          49.7842 |          12.4149 |
[32m[20221213 23:27:58 @agent_ppo2.py:185][0m |          -0.0175 |          49.3671 |          12.3901 |
[32m[20221213 23:27:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:27:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.40
[32m[20221213 23:27:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.21
[32m[20221213 23:27:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.09
[32m[20221213 23:27:58 @agent_ppo2.py:143][0m Total time:      15.43 min
[32m[20221213 23:27:58 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221213 23:27:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2731 --------------------------#
[32m[20221213 23:27:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:27:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:58 @agent_ppo2.py:185][0m |           0.0005 |          50.8830 |          12.3086 |
[32m[20221213 23:27:58 @agent_ppo2.py:185][0m |          -0.0019 |          50.3013 |          12.2877 |
[32m[20221213 23:27:58 @agent_ppo2.py:185][0m |          -0.0033 |          49.9773 |          12.2643 |
[32m[20221213 23:27:58 @agent_ppo2.py:185][0m |          -0.0017 |          49.9126 |          12.2669 |
[32m[20221213 23:27:59 @agent_ppo2.py:185][0m |          -0.0004 |          50.6209 |          12.2544 |
[32m[20221213 23:27:59 @agent_ppo2.py:185][0m |          -0.0041 |          49.7627 |          12.2373 |
[32m[20221213 23:27:59 @agent_ppo2.py:185][0m |          -0.0077 |          49.7729 |          12.2679 |
[32m[20221213 23:27:59 @agent_ppo2.py:185][0m |          -0.0028 |          50.5149 |          12.2416 |
[32m[20221213 23:27:59 @agent_ppo2.py:185][0m |          -0.0046 |          49.5367 |          12.2056 |
[32m[20221213 23:27:59 @agent_ppo2.py:185][0m |          -0.0038 |          49.5195 |          12.2081 |
[32m[20221213 23:27:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:27:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.50
[32m[20221213 23:27:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.54
[32m[20221213 23:27:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.05
[32m[20221213 23:27:59 @agent_ppo2.py:143][0m Total time:      15.45 min
[32m[20221213 23:27:59 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221213 23:27:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2732 --------------------------#
[32m[20221213 23:27:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:27:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:27:59 @agent_ppo2.py:185][0m |          -0.0045 |          54.9503 |          12.3363 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0080 |          54.0664 |          12.3867 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0096 |          53.7272 |          12.3906 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0095 |          53.6141 |          12.4149 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0123 |          53.1729 |          12.4302 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0130 |          53.1811 |          12.3909 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0115 |          53.2012 |          12.4063 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0153 |          52.8139 |          12.4034 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0153 |          53.0375 |          12.3940 |
[32m[20221213 23:28:00 @agent_ppo2.py:185][0m |          -0.0148 |          52.5526 |          12.3799 |
[32m[20221213 23:28:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:28:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.46
[32m[20221213 23:28:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.84
[32m[20221213 23:28:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.52
[32m[20221213 23:28:00 @agent_ppo2.py:143][0m Total time:      15.47 min
[32m[20221213 23:28:00 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221213 23:28:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2733 --------------------------#
[32m[20221213 23:28:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |           0.0033 |          57.9060 |          12.1460 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0053 |          53.8324 |          12.1366 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0041 |          52.8157 |          12.1481 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0014 |          53.4150 |          12.1573 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0085 |          51.6553 |          12.1666 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0104 |          51.2729 |          12.1883 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |           0.0000 |          53.1806 |          12.1499 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0092 |          50.8362 |          12.1555 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0118 |          50.6165 |          12.1830 |
[32m[20221213 23:28:01 @agent_ppo2.py:185][0m |          -0.0086 |          50.3981 |          12.1947 |
[32m[20221213 23:28:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:28:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.63
[32m[20221213 23:28:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.50
[32m[20221213 23:28:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.12
[32m[20221213 23:28:02 @agent_ppo2.py:143][0m Total time:      15.49 min
[32m[20221213 23:28:02 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221213 23:28:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2734 --------------------------#
[32m[20221213 23:28:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:02 @agent_ppo2.py:185][0m |          -0.0035 |          64.7227 |          12.5223 |
[32m[20221213 23:28:02 @agent_ppo2.py:185][0m |          -0.0032 |          62.9024 |          12.4972 |
[32m[20221213 23:28:02 @agent_ppo2.py:185][0m |          -0.0067 |          61.9504 |          12.4670 |
[32m[20221213 23:28:02 @agent_ppo2.py:185][0m |          -0.0080 |          61.3966 |          12.4460 |
[32m[20221213 23:28:02 @agent_ppo2.py:185][0m |           0.0042 |          65.5338 |          12.4503 |
[32m[20221213 23:28:02 @agent_ppo2.py:185][0m |          -0.0083 |          60.7507 |          12.3951 |
[32m[20221213 23:28:02 @agent_ppo2.py:185][0m |          -0.0078 |          60.8757 |          12.4190 |
[32m[20221213 23:28:03 @agent_ppo2.py:185][0m |          -0.0128 |          60.1806 |          12.4297 |
[32m[20221213 23:28:03 @agent_ppo2.py:185][0m |          -0.0113 |          60.2698 |          12.4157 |
[32m[20221213 23:28:03 @agent_ppo2.py:185][0m |          -0.0104 |          60.4787 |          12.3675 |
[32m[20221213 23:28:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:28:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.97
[32m[20221213 23:28:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.31
[32m[20221213 23:28:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.21
[32m[20221213 23:28:03 @agent_ppo2.py:143][0m Total time:      15.51 min
[32m[20221213 23:28:03 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221213 23:28:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2735 --------------------------#
[32m[20221213 23:28:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:03 @agent_ppo2.py:185][0m |           0.0001 |          62.0091 |          12.3146 |
[32m[20221213 23:28:03 @agent_ppo2.py:185][0m |           0.0023 |          60.8319 |          12.2990 |
[32m[20221213 23:28:03 @agent_ppo2.py:185][0m |          -0.0070 |          57.2947 |          12.2683 |
[32m[20221213 23:28:03 @agent_ppo2.py:185][0m |          -0.0059 |          56.8564 |          12.2919 |
[32m[20221213 23:28:04 @agent_ppo2.py:185][0m |          -0.0098 |          56.3520 |          12.2727 |
[32m[20221213 23:28:04 @agent_ppo2.py:185][0m |          -0.0131 |          56.0763 |          12.2644 |
[32m[20221213 23:28:04 @agent_ppo2.py:185][0m |          -0.0114 |          55.8318 |          12.2905 |
[32m[20221213 23:28:04 @agent_ppo2.py:185][0m |          -0.0128 |          55.8449 |          12.2518 |
[32m[20221213 23:28:04 @agent_ppo2.py:185][0m |          -0.0042 |          59.6254 |          12.2260 |
[32m[20221213 23:28:04 @agent_ppo2.py:185][0m |          -0.0078 |          55.3827 |          12.2776 |
[32m[20221213 23:28:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.16
[32m[20221213 23:28:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.11
[32m[20221213 23:28:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.68
[32m[20221213 23:28:04 @agent_ppo2.py:143][0m Total time:      15.54 min
[32m[20221213 23:28:04 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221213 23:28:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2736 --------------------------#
[32m[20221213 23:28:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:04 @agent_ppo2.py:185][0m |          -0.0007 |          61.1619 |          12.0203 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0040 |          58.8406 |          12.0207 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0026 |          58.4601 |          12.0405 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0034 |          57.8919 |          12.0255 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0116 |          56.1144 |          12.0083 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0128 |          55.9428 |          12.0684 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0066 |          59.2902 |          12.0543 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0149 |          56.0380 |          12.0393 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0127 |          55.6227 |          12.0040 |
[32m[20221213 23:28:05 @agent_ppo2.py:185][0m |          -0.0148 |          55.3197 |          12.0091 |
[32m[20221213 23:28:05 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.17
[32m[20221213 23:28:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.27
[32m[20221213 23:28:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.76
[32m[20221213 23:28:05 @agent_ppo2.py:143][0m Total time:      15.56 min
[32m[20221213 23:28:05 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221213 23:28:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2737 --------------------------#
[32m[20221213 23:28:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0013 |          56.7206 |          12.3390 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0045 |          55.6761 |          12.3852 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0055 |          55.1995 |          12.3537 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0035 |          55.1645 |          12.3654 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0077 |          54.9841 |          12.3524 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0077 |          54.8455 |          12.3828 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0077 |          54.9278 |          12.3765 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0078 |          54.7571 |          12.3761 |
[32m[20221213 23:28:06 @agent_ppo2.py:185][0m |          -0.0071 |          54.5356 |          12.3940 |
[32m[20221213 23:28:07 @agent_ppo2.py:185][0m |          -0.0020 |          55.2780 |          12.3685 |
[32m[20221213 23:28:07 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.75
[32m[20221213 23:28:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.83
[32m[20221213 23:28:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.17
[32m[20221213 23:28:07 @agent_ppo2.py:143][0m Total time:      15.58 min
[32m[20221213 23:28:07 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221213 23:28:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2738 --------------------------#
[32m[20221213 23:28:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:28:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:07 @agent_ppo2.py:185][0m |           0.0024 |          68.8948 |          12.2623 |
[32m[20221213 23:28:07 @agent_ppo2.py:185][0m |          -0.0046 |          66.7891 |          12.3200 |
[32m[20221213 23:28:07 @agent_ppo2.py:185][0m |          -0.0077 |          65.8184 |          12.2854 |
[32m[20221213 23:28:07 @agent_ppo2.py:185][0m |          -0.0104 |          65.2231 |          12.3225 |
[32m[20221213 23:28:07 @agent_ppo2.py:185][0m |          -0.0069 |          64.8230 |          12.2769 |
[32m[20221213 23:28:08 @agent_ppo2.py:185][0m |          -0.0112 |          64.1674 |          12.2952 |
[32m[20221213 23:28:08 @agent_ppo2.py:185][0m |          -0.0104 |          63.6211 |          12.2822 |
[32m[20221213 23:28:08 @agent_ppo2.py:185][0m |          -0.0068 |          64.0635 |          12.2462 |
[32m[20221213 23:28:08 @agent_ppo2.py:185][0m |           0.0034 |          70.3846 |          12.2811 |
[32m[20221213 23:28:08 @agent_ppo2.py:185][0m |           0.0115 |          75.8450 |          12.2625 |
[32m[20221213 23:28:08 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.78
[32m[20221213 23:28:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.27
[32m[20221213 23:28:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.10
[32m[20221213 23:28:08 @agent_ppo2.py:143][0m Total time:      15.60 min
[32m[20221213 23:28:08 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221213 23:28:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2739 --------------------------#
[32m[20221213 23:28:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:08 @agent_ppo2.py:185][0m |           0.0018 |          56.2559 |          11.5295 |
[32m[20221213 23:28:08 @agent_ppo2.py:185][0m |          -0.0056 |          53.4524 |          11.5165 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0078 |          52.3798 |          11.4949 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0074 |          51.5344 |          11.4650 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0079 |          51.2101 |          11.4301 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0104 |          50.5928 |          11.4114 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0108 |          50.3029 |          11.4222 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0114 |          49.9235 |          11.4534 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0091 |          49.8876 |          11.4309 |
[32m[20221213 23:28:09 @agent_ppo2.py:185][0m |          -0.0130 |          49.6063 |          11.4052 |
[32m[20221213 23:28:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:28:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.75
[32m[20221213 23:28:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.77
[32m[20221213 23:28:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.17
[32m[20221213 23:28:09 @agent_ppo2.py:143][0m Total time:      15.62 min
[32m[20221213 23:28:09 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221213 23:28:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2740 --------------------------#
[32m[20221213 23:28:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:28:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |           0.0005 |          58.9521 |          12.2595 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0056 |          51.3903 |          12.2508 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0046 |          50.2729 |          12.2758 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0049 |          49.3329 |          12.2524 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0079 |          48.7994 |          12.2011 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0071 |          48.3315 |          12.2424 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0069 |          47.8847 |          12.2694 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0039 |          47.6501 |          12.2073 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0117 |          47.3592 |          12.2318 |
[32m[20221213 23:28:10 @agent_ppo2.py:185][0m |          -0.0112 |          47.1684 |          12.2224 |
[32m[20221213 23:28:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.23
[32m[20221213 23:28:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.73
[32m[20221213 23:28:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.03
[32m[20221213 23:28:11 @agent_ppo2.py:143][0m Total time:      15.64 min
[32m[20221213 23:28:11 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221213 23:28:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2741 --------------------------#
[32m[20221213 23:28:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:11 @agent_ppo2.py:185][0m |          -0.0012 |          61.9962 |          11.6696 |
[32m[20221213 23:28:11 @agent_ppo2.py:185][0m |          -0.0078 |          60.3629 |          11.7037 |
[32m[20221213 23:28:11 @agent_ppo2.py:185][0m |           0.0038 |          65.3641 |          11.7356 |
[32m[20221213 23:28:11 @agent_ppo2.py:185][0m |           0.0027 |          67.3488 |          11.7356 |
[32m[20221213 23:28:11 @agent_ppo2.py:185][0m |          -0.0118 |          59.6660 |          11.7854 |
[32m[20221213 23:28:11 @agent_ppo2.py:185][0m |          -0.0129 |          59.4078 |          11.7573 |
[32m[20221213 23:28:12 @agent_ppo2.py:185][0m |          -0.0114 |          59.2269 |          11.7506 |
[32m[20221213 23:28:12 @agent_ppo2.py:185][0m |          -0.0106 |          59.1455 |          11.7609 |
[32m[20221213 23:28:12 @agent_ppo2.py:185][0m |          -0.0104 |          59.4075 |          11.7654 |
[32m[20221213 23:28:12 @agent_ppo2.py:185][0m |          -0.0126 |          59.0259 |          11.7918 |
[32m[20221213 23:28:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.39
[32m[20221213 23:28:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.09
[32m[20221213 23:28:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.25
[32m[20221213 23:28:12 @agent_ppo2.py:143][0m Total time:      15.67 min
[32m[20221213 23:28:12 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221213 23:28:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2742 --------------------------#
[32m[20221213 23:28:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:12 @agent_ppo2.py:185][0m |          -0.0025 |          55.6141 |          11.7293 |
[32m[20221213 23:28:12 @agent_ppo2.py:185][0m |          -0.0081 |          54.8621 |          11.8436 |
[32m[20221213 23:28:12 @agent_ppo2.py:185][0m |           0.0017 |          57.0965 |          11.7805 |
[32m[20221213 23:28:13 @agent_ppo2.py:185][0m |          -0.0101 |          54.5175 |          11.8032 |
[32m[20221213 23:28:13 @agent_ppo2.py:185][0m |          -0.0076 |          54.1998 |          11.8047 |
[32m[20221213 23:28:13 @agent_ppo2.py:185][0m |          -0.0084 |          54.0975 |          11.8112 |
[32m[20221213 23:28:13 @agent_ppo2.py:185][0m |          -0.0042 |          55.0128 |          11.8139 |
[32m[20221213 23:28:13 @agent_ppo2.py:185][0m |          -0.0089 |          53.8474 |          11.8160 |
[32m[20221213 23:28:13 @agent_ppo2.py:185][0m |          -0.0107 |          53.7897 |          11.8284 |
[32m[20221213 23:28:13 @agent_ppo2.py:185][0m |           0.0003 |          57.2463 |          11.8234 |
[32m[20221213 23:28:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.51
[32m[20221213 23:28:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.25
[32m[20221213 23:28:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.56
[32m[20221213 23:28:13 @agent_ppo2.py:143][0m Total time:      15.69 min
[32m[20221213 23:28:13 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221213 23:28:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2743 --------------------------#
[32m[20221213 23:28:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0003 |          49.3653 |          11.6654 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |           0.0032 |          49.6349 |          11.6258 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0038 |          48.5905 |          11.5854 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0077 |          48.4222 |          11.6106 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0066 |          48.3211 |          11.6149 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0064 |          48.2180 |          11.6414 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0007 |          50.3332 |          11.6549 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0099 |          48.0660 |          11.6584 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0105 |          47.9265 |          11.6487 |
[32m[20221213 23:28:14 @agent_ppo2.py:185][0m |          -0.0114 |          47.9487 |          11.6404 |
[32m[20221213 23:28:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.21
[32m[20221213 23:28:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.63
[32m[20221213 23:28:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.06
[32m[20221213 23:28:15 @agent_ppo2.py:143][0m Total time:      15.71 min
[32m[20221213 23:28:15 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221213 23:28:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2744 --------------------------#
[32m[20221213 23:28:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0019 |          39.9776 |          11.7366 |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0053 |          35.9950 |          11.7346 |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0114 |          34.4658 |          11.7429 |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0046 |          33.3162 |          11.6585 |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0100 |          32.6684 |          11.6729 |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0142 |          32.1320 |          11.6342 |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0151 |          31.6593 |          11.6116 |
[32m[20221213 23:28:15 @agent_ppo2.py:185][0m |          -0.0153 |          31.3064 |          11.5796 |
[32m[20221213 23:28:16 @agent_ppo2.py:185][0m |          -0.0170 |          31.0737 |          11.5920 |
[32m[20221213 23:28:16 @agent_ppo2.py:185][0m |          -0.0070 |          30.9110 |          11.5467 |
[32m[20221213 23:28:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.08
[32m[20221213 23:28:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.87
[32m[20221213 23:28:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.53
[32m[20221213 23:28:16 @agent_ppo2.py:143][0m Total time:      15.73 min
[32m[20221213 23:28:16 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221213 23:28:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2745 --------------------------#
[32m[20221213 23:28:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:16 @agent_ppo2.py:185][0m |          -0.0006 |          60.1184 |          11.7928 |
[32m[20221213 23:28:16 @agent_ppo2.py:185][0m |          -0.0041 |          58.7118 |          11.7877 |
[32m[20221213 23:28:16 @agent_ppo2.py:185][0m |          -0.0088 |          58.5117 |          11.7674 |
[32m[20221213 23:28:16 @agent_ppo2.py:185][0m |          -0.0069 |          58.4362 |          11.7964 |
[32m[20221213 23:28:17 @agent_ppo2.py:185][0m |          -0.0077 |          57.9360 |          11.7741 |
[32m[20221213 23:28:17 @agent_ppo2.py:185][0m |          -0.0107 |          57.9833 |          11.7604 |
[32m[20221213 23:28:17 @agent_ppo2.py:185][0m |          -0.0076 |          58.1199 |          11.7800 |
[32m[20221213 23:28:17 @agent_ppo2.py:185][0m |          -0.0084 |          57.8410 |          11.7771 |
[32m[20221213 23:28:17 @agent_ppo2.py:185][0m |          -0.0100 |          57.2689 |          11.7468 |
[32m[20221213 23:28:17 @agent_ppo2.py:185][0m |          -0.0105 |          57.0724 |          11.7272 |
[32m[20221213 23:28:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.85
[32m[20221213 23:28:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.37
[32m[20221213 23:28:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.25
[32m[20221213 23:28:17 @agent_ppo2.py:143][0m Total time:      15.75 min
[32m[20221213 23:28:17 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221213 23:28:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2746 --------------------------#
[32m[20221213 23:28:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:17 @agent_ppo2.py:185][0m |          -0.0021 |          53.5111 |          12.0047 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0075 |          49.6459 |          11.9823 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0077 |          48.0899 |          12.0272 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0118 |          46.9920 |          12.0350 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0125 |          46.5919 |          12.0096 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0144 |          46.0120 |          12.0572 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0143 |          45.5779 |          12.0281 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0096 |          46.1171 |          12.0274 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0051 |          49.8362 |          12.0264 |
[32m[20221213 23:28:18 @agent_ppo2.py:185][0m |          -0.0149 |          44.7071 |          12.0434 |
[32m[20221213 23:28:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.67
[32m[20221213 23:28:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.70
[32m[20221213 23:28:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.75
[32m[20221213 23:28:18 @agent_ppo2.py:143][0m Total time:      15.77 min
[32m[20221213 23:28:18 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221213 23:28:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2747 --------------------------#
[32m[20221213 23:28:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |           0.0002 |          57.8653 |          11.7926 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0059 |          53.4247 |          11.7686 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0118 |          52.4573 |          11.7909 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0087 |          51.8017 |          11.7978 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0080 |          51.4218 |          11.7700 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0070 |          51.0459 |          11.7519 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0062 |          51.4928 |          11.7717 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0112 |          50.4821 |          11.8036 |
[32m[20221213 23:28:19 @agent_ppo2.py:185][0m |          -0.0165 |          50.3045 |          11.8074 |
[32m[20221213 23:28:20 @agent_ppo2.py:185][0m |          -0.0166 |          50.0941 |          11.8054 |
[32m[20221213 23:28:20 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.16
[32m[20221213 23:28:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.82
[32m[20221213 23:28:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.24
[32m[20221213 23:28:20 @agent_ppo2.py:143][0m Total time:      15.79 min
[32m[20221213 23:28:20 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221213 23:28:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2748 --------------------------#
[32m[20221213 23:28:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:20 @agent_ppo2.py:185][0m |          -0.0064 |          56.9833 |          11.7681 |
[32m[20221213 23:28:20 @agent_ppo2.py:185][0m |          -0.0020 |          49.5846 |          11.7994 |
[32m[20221213 23:28:20 @agent_ppo2.py:185][0m |          -0.0100 |          47.3549 |          11.7411 |
[32m[20221213 23:28:20 @agent_ppo2.py:185][0m |          -0.0046 |          45.9072 |          11.7525 |
[32m[20221213 23:28:20 @agent_ppo2.py:185][0m |           0.0115 |          47.3608 |          11.7567 |
[32m[20221213 23:28:20 @agent_ppo2.py:185][0m |          -0.0075 |          44.1156 |          11.7711 |
[32m[20221213 23:28:21 @agent_ppo2.py:185][0m |          -0.0101 |          43.2352 |          11.7600 |
[32m[20221213 23:28:21 @agent_ppo2.py:185][0m |          -0.0148 |          42.8936 |          11.7810 |
[32m[20221213 23:28:21 @agent_ppo2.py:185][0m |          -0.0117 |          42.3616 |          11.8061 |
[32m[20221213 23:28:21 @agent_ppo2.py:185][0m |          -0.0125 |          42.0436 |          11.7915 |
[32m[20221213 23:28:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.69
[32m[20221213 23:28:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.87
[32m[20221213 23:28:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.12
[32m[20221213 23:28:21 @agent_ppo2.py:143][0m Total time:      15.82 min
[32m[20221213 23:28:21 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221213 23:28:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2749 --------------------------#
[32m[20221213 23:28:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:28:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:21 @agent_ppo2.py:185][0m |           0.0058 |          52.4824 |          11.5918 |
[32m[20221213 23:28:21 @agent_ppo2.py:185][0m |          -0.0055 |          43.7428 |          11.5842 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0105 |          41.8978 |          11.6135 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0056 |          41.4149 |          11.6873 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0130 |          40.7218 |          11.7346 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0142 |          40.0073 |          11.7148 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0132 |          39.3509 |          11.7413 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0187 |          39.1819 |          11.7833 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0089 |          39.8821 |          11.7889 |
[32m[20221213 23:28:22 @agent_ppo2.py:185][0m |          -0.0232 |          38.3552 |          11.8316 |
[32m[20221213 23:28:22 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.28
[32m[20221213 23:28:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.81
[32m[20221213 23:28:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.68
[32m[20221213 23:28:22 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221213 23:28:22 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221213 23:28:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2750 --------------------------#
[32m[20221213 23:28:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:28:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0035 |          61.1838 |          12.1638 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0093 |          53.9049 |          12.1686 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0084 |          50.8338 |          12.1777 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0070 |          49.6654 |          12.1597 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0144 |          48.2711 |          12.1985 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0129 |          46.8574 |          12.1775 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0151 |          46.2232 |          12.1764 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0131 |          45.4878 |          12.2164 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0182 |          45.1334 |          12.2010 |
[32m[20221213 23:28:23 @agent_ppo2.py:185][0m |          -0.0108 |          44.8723 |          12.2114 |
[32m[20221213 23:28:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.31
[32m[20221213 23:28:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.51
[32m[20221213 23:28:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.09
[32m[20221213 23:28:24 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221213 23:28:24 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221213 23:28:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2751 --------------------------#
[32m[20221213 23:28:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:24 @agent_ppo2.py:185][0m |           0.0071 |          62.1120 |          12.1145 |
[32m[20221213 23:28:24 @agent_ppo2.py:185][0m |          -0.0050 |          57.7150 |          12.0165 |
[32m[20221213 23:28:24 @agent_ppo2.py:185][0m |          -0.0065 |          57.0826 |          11.9936 |
[32m[20221213 23:28:24 @agent_ppo2.py:185][0m |          -0.0104 |          56.5653 |          12.0262 |
[32m[20221213 23:28:24 @agent_ppo2.py:185][0m |          -0.0100 |          56.2679 |          11.9704 |
[32m[20221213 23:28:24 @agent_ppo2.py:185][0m |          -0.0082 |          56.1184 |          11.9473 |
[32m[20221213 23:28:24 @agent_ppo2.py:185][0m |          -0.0091 |          55.8732 |          11.9380 |
[32m[20221213 23:28:25 @agent_ppo2.py:185][0m |          -0.0105 |          55.7243 |          11.8742 |
[32m[20221213 23:28:25 @agent_ppo2.py:185][0m |          -0.0085 |          55.8056 |          11.9059 |
[32m[20221213 23:28:25 @agent_ppo2.py:185][0m |          -0.0098 |          55.6433 |          11.8864 |
[32m[20221213 23:28:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.72
[32m[20221213 23:28:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.16
[32m[20221213 23:28:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.85
[32m[20221213 23:28:25 @agent_ppo2.py:143][0m Total time:      15.88 min
[32m[20221213 23:28:25 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221213 23:28:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2752 --------------------------#
[32m[20221213 23:28:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:25 @agent_ppo2.py:185][0m |          -0.0015 |          43.8064 |          11.9795 |
[32m[20221213 23:28:25 @agent_ppo2.py:185][0m |          -0.0034 |          40.6220 |          11.9880 |
[32m[20221213 23:28:25 @agent_ppo2.py:185][0m |          -0.0088 |          39.4634 |          12.0125 |
[32m[20221213 23:28:25 @agent_ppo2.py:185][0m |          -0.0096 |          38.7898 |          12.0046 |
[32m[20221213 23:28:26 @agent_ppo2.py:185][0m |          -0.0144 |          38.2683 |          12.0607 |
[32m[20221213 23:28:26 @agent_ppo2.py:185][0m |          -0.0113 |          38.3738 |          12.0849 |
[32m[20221213 23:28:26 @agent_ppo2.py:185][0m |          -0.0140 |          37.7347 |          12.0653 |
[32m[20221213 23:28:26 @agent_ppo2.py:185][0m |          -0.0134 |          37.4176 |          12.1141 |
[32m[20221213 23:28:26 @agent_ppo2.py:185][0m |          -0.0172 |          37.4061 |          12.1315 |
[32m[20221213 23:28:26 @agent_ppo2.py:185][0m |          -0.0152 |          37.2702 |          12.1503 |
[32m[20221213 23:28:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:28:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.89
[32m[20221213 23:28:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.86
[32m[20221213 23:28:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.14
[32m[20221213 23:28:26 @agent_ppo2.py:143][0m Total time:      15.90 min
[32m[20221213 23:28:26 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221213 23:28:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2753 --------------------------#
[32m[20221213 23:28:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:28:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |           0.0055 |          48.2934 |          12.2157 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0017 |          40.6210 |          12.2051 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0097 |          38.4145 |          12.2218 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0055 |          36.8823 |          12.2048 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0110 |          35.8931 |          12.2126 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0119 |          34.9440 |          12.1660 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0035 |          35.3152 |          12.2172 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0126 |          33.7261 |          12.2053 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0119 |          33.2599 |          12.2340 |
[32m[20221213 23:28:27 @agent_ppo2.py:185][0m |          -0.0157 |          32.7906 |          12.2027 |
[32m[20221213 23:28:27 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:28:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.32
[32m[20221213 23:28:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.27
[32m[20221213 23:28:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.58
[32m[20221213 23:28:28 @agent_ppo2.py:143][0m Total time:      15.93 min
[32m[20221213 23:28:28 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221213 23:28:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2754 --------------------------#
[32m[20221213 23:28:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:28:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |           0.0024 |          58.1617 |          11.6337 |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |           0.0008 |          54.2999 |          11.5907 |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |          -0.0054 |          52.1300 |          11.5926 |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |          -0.0062 |          51.5121 |          11.5964 |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |          -0.0086 |          50.7267 |          11.6257 |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |          -0.0083 |          50.4009 |          11.6146 |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |          -0.0091 |          50.0474 |          11.6428 |
[32m[20221213 23:28:28 @agent_ppo2.py:185][0m |          -0.0078 |          49.7825 |          11.6509 |
[32m[20221213 23:28:29 @agent_ppo2.py:185][0m |          -0.0099 |          49.6081 |          11.6602 |
[32m[20221213 23:28:29 @agent_ppo2.py:185][0m |          -0.0094 |          49.2618 |          11.6565 |
[32m[20221213 23:28:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:28:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.17
[32m[20221213 23:28:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.57
[32m[20221213 23:28:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.34
[32m[20221213 23:28:29 @agent_ppo2.py:143][0m Total time:      15.95 min
[32m[20221213 23:28:29 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221213 23:28:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2755 --------------------------#
[32m[20221213 23:28:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:29 @agent_ppo2.py:185][0m |          -0.0020 |          60.2603 |          12.2643 |
[32m[20221213 23:28:29 @agent_ppo2.py:185][0m |          -0.0039 |          56.6134 |          12.2799 |
[32m[20221213 23:28:29 @agent_ppo2.py:185][0m |          -0.0077 |          55.1157 |          12.3007 |
[32m[20221213 23:28:29 @agent_ppo2.py:185][0m |          -0.0062 |          54.2282 |          12.2582 |
[32m[20221213 23:28:29 @agent_ppo2.py:185][0m |          -0.0090 |          53.6921 |          12.3134 |
[32m[20221213 23:28:30 @agent_ppo2.py:185][0m |          -0.0106 |          53.4019 |          12.3222 |
[32m[20221213 23:28:30 @agent_ppo2.py:185][0m |          -0.0088 |          52.9124 |          12.3264 |
[32m[20221213 23:28:30 @agent_ppo2.py:185][0m |          -0.0116 |          52.3404 |          12.3239 |
[32m[20221213 23:28:30 @agent_ppo2.py:185][0m |          -0.0076 |          52.1129 |          12.3440 |
[32m[20221213 23:28:30 @agent_ppo2.py:185][0m |          -0.0098 |          52.0619 |          12.3376 |
[32m[20221213 23:28:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.06
[32m[20221213 23:28:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.38
[32m[20221213 23:28:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.93
[32m[20221213 23:28:30 @agent_ppo2.py:143][0m Total time:      15.97 min
[32m[20221213 23:28:30 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221213 23:28:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2756 --------------------------#
[32m[20221213 23:28:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:30 @agent_ppo2.py:185][0m |           0.0012 |          56.2040 |          11.9356 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0095 |          51.5200 |          11.9002 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0058 |          50.3103 |          11.9263 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0102 |          49.5610 |          11.9023 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0129 |          49.2055 |          11.9705 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0147 |          48.8373 |          11.9434 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0145 |          48.5092 |          11.9995 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0147 |          48.2724 |          11.9641 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0185 |          48.0352 |          11.9716 |
[32m[20221213 23:28:31 @agent_ppo2.py:185][0m |          -0.0050 |          51.3172 |          11.9792 |
[32m[20221213 23:28:31 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.03
[32m[20221213 23:28:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.90
[32m[20221213 23:28:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.19
[32m[20221213 23:28:31 @agent_ppo2.py:143][0m Total time:      15.99 min
[32m[20221213 23:28:31 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221213 23:28:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2757 --------------------------#
[32m[20221213 23:28:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:28:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |           0.0008 |          60.8031 |          12.2572 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0056 |          55.0726 |          12.3047 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0136 |          52.6598 |          12.3483 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0094 |          51.1730 |          12.3607 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0105 |          49.8888 |          12.4122 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0150 |          49.3923 |          12.4146 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0139 |          48.6164 |          12.4577 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0167 |          48.3626 |          12.4953 |
[32m[20221213 23:28:32 @agent_ppo2.py:185][0m |          -0.0174 |          48.0724 |          12.5117 |
[32m[20221213 23:28:33 @agent_ppo2.py:185][0m |          -0.0182 |          47.6768 |          12.5628 |
[32m[20221213 23:28:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:28:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.15
[32m[20221213 23:28:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.60
[32m[20221213 23:28:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.45
[32m[20221213 23:28:33 @agent_ppo2.py:143][0m Total time:      16.01 min
[32m[20221213 23:28:33 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221213 23:28:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2758 --------------------------#
[32m[20221213 23:28:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:33 @agent_ppo2.py:185][0m |          -0.0007 |          63.5469 |          12.8224 |
[32m[20221213 23:28:33 @agent_ppo2.py:185][0m |           0.0007 |          49.1147 |          12.8440 |
[32m[20221213 23:28:33 @agent_ppo2.py:185][0m |          -0.0076 |          47.7035 |          12.8517 |
[32m[20221213 23:28:33 @agent_ppo2.py:185][0m |          -0.0095 |          47.2807 |          12.8599 |
[32m[20221213 23:28:33 @agent_ppo2.py:185][0m |          -0.0147 |          46.6776 |          12.8227 |
[32m[20221213 23:28:33 @agent_ppo2.py:185][0m |          -0.0104 |          49.9287 |          12.8335 |
[32m[20221213 23:28:34 @agent_ppo2.py:185][0m |          -0.0135 |          46.1930 |          12.8133 |
[32m[20221213 23:28:34 @agent_ppo2.py:185][0m |          -0.0131 |          45.9939 |          12.8103 |
[32m[20221213 23:28:34 @agent_ppo2.py:185][0m |          -0.0132 |          45.7470 |          12.8343 |
[32m[20221213 23:28:34 @agent_ppo2.py:185][0m |          -0.0124 |          45.7827 |          12.8131 |
[32m[20221213 23:28:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:28:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.48
[32m[20221213 23:28:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.26
[32m[20221213 23:28:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.63
[32m[20221213 23:28:34 @agent_ppo2.py:143][0m Total time:      16.03 min
[32m[20221213 23:28:34 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221213 23:28:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2759 --------------------------#
[32m[20221213 23:28:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:34 @agent_ppo2.py:185][0m |          -0.0004 |          50.3458 |          12.5396 |
[32m[20221213 23:28:34 @agent_ppo2.py:185][0m |          -0.0051 |          45.0344 |          12.4956 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0106 |          43.9288 |          12.5127 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0101 |          43.2274 |          12.5275 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0139 |          42.6908 |          12.5028 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0136 |          42.1669 |          12.4924 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0042 |          45.4674 |          12.5545 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0157 |          41.6568 |          12.5541 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0129 |          41.4746 |          12.5042 |
[32m[20221213 23:28:35 @agent_ppo2.py:185][0m |          -0.0125 |          41.1094 |          12.5500 |
[32m[20221213 23:28:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.20
[32m[20221213 23:28:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.82
[32m[20221213 23:28:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.04
[32m[20221213 23:28:35 @agent_ppo2.py:143][0m Total time:      16.05 min
[32m[20221213 23:28:35 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221213 23:28:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2760 --------------------------#
[32m[20221213 23:28:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:28:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0015 |          54.0631 |          12.6060 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0025 |          51.2039 |          12.5722 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0077 |          50.0752 |          12.6195 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |           0.0040 |          54.5621 |          12.5999 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0087 |          48.9602 |          12.5803 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0081 |          48.4806 |          12.5735 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0106 |          48.0409 |          12.5783 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0096 |          47.6379 |          12.5628 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0112 |          47.7751 |          12.5525 |
[32m[20221213 23:28:36 @agent_ppo2.py:185][0m |          -0.0114 |          47.3225 |          12.5504 |
[32m[20221213 23:28:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.86
[32m[20221213 23:28:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.31
[32m[20221213 23:28:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.05
[32m[20221213 23:28:37 @agent_ppo2.py:143][0m Total time:      16.08 min
[32m[20221213 23:28:37 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221213 23:28:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2761 --------------------------#
[32m[20221213 23:28:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:28:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:37 @agent_ppo2.py:185][0m |           0.0096 |          49.1865 |          12.3644 |
[32m[20221213 23:28:37 @agent_ppo2.py:185][0m |           0.0043 |          44.3928 |          12.2724 |
[32m[20221213 23:28:37 @agent_ppo2.py:185][0m |           0.0068 |          45.2463 |          12.2992 |
[32m[20221213 23:28:37 @agent_ppo2.py:185][0m |          -0.0070 |          43.0860 |          12.2550 |
[32m[20221213 23:28:37 @agent_ppo2.py:185][0m |          -0.0048 |          42.7795 |          12.2545 |
[32m[20221213 23:28:37 @agent_ppo2.py:185][0m |          -0.0053 |          42.4580 |          12.2784 |
[32m[20221213 23:28:37 @agent_ppo2.py:185][0m |          -0.0067 |          42.1641 |          12.2663 |
[32m[20221213 23:28:38 @agent_ppo2.py:185][0m |          -0.0098 |          41.9114 |          12.2732 |
[32m[20221213 23:28:38 @agent_ppo2.py:185][0m |          -0.0066 |          42.3525 |          12.2344 |
[32m[20221213 23:28:38 @agent_ppo2.py:185][0m |          -0.0107 |          41.5988 |          12.2546 |
[32m[20221213 23:28:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:28:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.00
[32m[20221213 23:28:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.85
[32m[20221213 23:28:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.55
[32m[20221213 23:28:38 @agent_ppo2.py:143][0m Total time:      16.10 min
[32m[20221213 23:28:38 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221213 23:28:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2762 --------------------------#
[32m[20221213 23:28:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:38 @agent_ppo2.py:185][0m |          -0.0013 |          60.6798 |          12.6252 |
[32m[20221213 23:28:38 @agent_ppo2.py:185][0m |          -0.0049 |          59.3711 |          12.5910 |
[32m[20221213 23:28:38 @agent_ppo2.py:185][0m |          -0.0056 |          58.5940 |          12.6491 |
[32m[20221213 23:28:39 @agent_ppo2.py:185][0m |           0.0067 |          64.6071 |          12.5994 |
[32m[20221213 23:28:39 @agent_ppo2.py:185][0m |          -0.0086 |          58.0544 |          12.6724 |
[32m[20221213 23:28:39 @agent_ppo2.py:185][0m |          -0.0068 |          57.8261 |          12.6446 |
[32m[20221213 23:28:39 @agent_ppo2.py:185][0m |          -0.0094 |          57.7929 |          12.6523 |
[32m[20221213 23:28:39 @agent_ppo2.py:185][0m |          -0.0094 |          57.5394 |          12.6315 |
[32m[20221213 23:28:39 @agent_ppo2.py:185][0m |          -0.0128 |          57.6514 |          12.6643 |
[32m[20221213 23:28:39 @agent_ppo2.py:185][0m |          -0.0115 |          57.3753 |          12.6687 |
[32m[20221213 23:28:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:28:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.11
[32m[20221213 23:28:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.27
[32m[20221213 23:28:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.71
[32m[20221213 23:28:39 @agent_ppo2.py:143][0m Total time:      16.12 min
[32m[20221213 23:28:39 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221213 23:28:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2763 --------------------------#
[32m[20221213 23:28:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |           0.0152 |          67.5569 |          12.5198 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0039 |          59.5386 |          12.5952 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0011 |          59.3156 |          12.6112 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0072 |          57.8711 |          12.6071 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0092 |          56.9935 |          12.6309 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0111 |          56.5771 |          12.6044 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |           0.0027 |          62.1547 |          12.5955 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0088 |          56.4244 |          12.6396 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0106 |          55.4882 |          12.6591 |
[32m[20221213 23:28:40 @agent_ppo2.py:185][0m |          -0.0020 |          58.4206 |          12.6896 |
[32m[20221213 23:28:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.04
[32m[20221213 23:28:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.67
[32m[20221213 23:28:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.43
[32m[20221213 23:28:41 @agent_ppo2.py:143][0m Total time:      16.14 min
[32m[20221213 23:28:41 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221213 23:28:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2764 --------------------------#
[32m[20221213 23:28:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |          -0.0010 |          46.7767 |          12.5605 |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |          -0.0070 |          41.3568 |          12.5692 |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |           0.0060 |          45.7317 |          12.5926 |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |          -0.0068 |          39.7275 |          12.5485 |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |          -0.0051 |          39.1331 |          12.5961 |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |          -0.0111 |          39.0782 |          12.5593 |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |          -0.0125 |          38.6344 |          12.5505 |
[32m[20221213 23:28:41 @agent_ppo2.py:185][0m |          -0.0102 |          38.3221 |          12.5412 |
[32m[20221213 23:28:42 @agent_ppo2.py:185][0m |          -0.0137 |          38.2426 |          12.5141 |
[32m[20221213 23:28:42 @agent_ppo2.py:185][0m |          -0.0158 |          37.9567 |          12.5068 |
[32m[20221213 23:28:42 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.32
[32m[20221213 23:28:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.74
[32m[20221213 23:28:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.15
[32m[20221213 23:28:42 @agent_ppo2.py:143][0m Total time:      16.16 min
[32m[20221213 23:28:42 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221213 23:28:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2765 --------------------------#
[32m[20221213 23:28:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:42 @agent_ppo2.py:185][0m |          -0.0005 |          49.0125 |          12.3908 |
[32m[20221213 23:28:42 @agent_ppo2.py:185][0m |          -0.0048 |          46.4353 |          12.3557 |
[32m[20221213 23:28:42 @agent_ppo2.py:185][0m |          -0.0042 |          45.3599 |          12.3233 |
[32m[20221213 23:28:42 @agent_ppo2.py:185][0m |          -0.0060 |          44.8248 |          12.3366 |
[32m[20221213 23:28:42 @agent_ppo2.py:185][0m |          -0.0102 |          44.2947 |          12.3633 |
[32m[20221213 23:28:43 @agent_ppo2.py:185][0m |          -0.0110 |          44.0106 |          12.3490 |
[32m[20221213 23:28:43 @agent_ppo2.py:185][0m |          -0.0103 |          43.8861 |          12.3049 |
[32m[20221213 23:28:43 @agent_ppo2.py:185][0m |          -0.0133 |          43.5285 |          12.3112 |
[32m[20221213 23:28:43 @agent_ppo2.py:185][0m |          -0.0091 |          44.1246 |          12.2905 |
[32m[20221213 23:28:43 @agent_ppo2.py:185][0m |          -0.0170 |          43.3679 |          12.3051 |
[32m[20221213 23:28:43 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:28:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.19
[32m[20221213 23:28:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.77
[32m[20221213 23:28:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.30
[32m[20221213 23:28:43 @agent_ppo2.py:143][0m Total time:      16.18 min
[32m[20221213 23:28:43 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221213 23:28:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2766 --------------------------#
[32m[20221213 23:28:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:43 @agent_ppo2.py:185][0m |          -0.0020 |          36.7916 |          12.4704 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0070 |          32.2052 |          12.4925 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0040 |          29.7158 |          12.4871 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0098 |          29.0008 |          12.5030 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0114 |          27.7792 |          12.4753 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0107 |          27.3373 |          12.5119 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0167 |          26.5732 |          12.4795 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0147 |          26.0847 |          12.5138 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0176 |          25.5860 |          12.4957 |
[32m[20221213 23:28:44 @agent_ppo2.py:185][0m |          -0.0187 |          25.3880 |          12.4954 |
[32m[20221213 23:28:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.27
[32m[20221213 23:28:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.71
[32m[20221213 23:28:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.15
[32m[20221213 23:28:44 @agent_ppo2.py:143][0m Total time:      16.21 min
[32m[20221213 23:28:44 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221213 23:28:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2767 --------------------------#
[32m[20221213 23:28:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0027 |          56.2110 |          12.7838 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0051 |          54.3087 |          12.7915 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0044 |          53.3695 |          12.7894 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0081 |          52.8596 |          12.7693 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |           0.0043 |          57.8423 |          12.7674 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0108 |          51.8590 |          12.7682 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0097 |          51.4589 |          12.7400 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0071 |          51.1715 |          12.7492 |
[32m[20221213 23:28:45 @agent_ppo2.py:185][0m |          -0.0018 |          52.5290 |          12.7636 |
[32m[20221213 23:28:46 @agent_ppo2.py:185][0m |          -0.0107 |          50.4563 |          12.7265 |
[32m[20221213 23:28:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.11
[32m[20221213 23:28:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.59
[32m[20221213 23:28:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.42
[32m[20221213 23:28:46 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221213 23:28:46 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221213 23:28:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2768 --------------------------#
[32m[20221213 23:28:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:46 @agent_ppo2.py:185][0m |           0.0028 |          55.7028 |          12.3619 |
[32m[20221213 23:28:46 @agent_ppo2.py:185][0m |          -0.0044 |          53.5571 |          12.3571 |
[32m[20221213 23:28:46 @agent_ppo2.py:185][0m |          -0.0046 |          52.8971 |          12.3604 |
[32m[20221213 23:28:46 @agent_ppo2.py:185][0m |          -0.0069 |          52.6817 |          12.3701 |
[32m[20221213 23:28:46 @agent_ppo2.py:185][0m |          -0.0007 |          53.8021 |          12.3966 |
[32m[20221213 23:28:46 @agent_ppo2.py:185][0m |          -0.0060 |          52.4406 |          12.3905 |
[32m[20221213 23:28:47 @agent_ppo2.py:185][0m |          -0.0044 |          52.6668 |          12.3640 |
[32m[20221213 23:28:47 @agent_ppo2.py:185][0m |          -0.0100 |          52.1091 |          12.3648 |
[32m[20221213 23:28:47 @agent_ppo2.py:185][0m |          -0.0084 |          52.2931 |          12.3930 |
[32m[20221213 23:28:47 @agent_ppo2.py:185][0m |          -0.0088 |          52.0441 |          12.3720 |
[32m[20221213 23:28:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.93
[32m[20221213 23:28:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.26
[32m[20221213 23:28:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.74
[32m[20221213 23:28:47 @agent_ppo2.py:143][0m Total time:      16.25 min
[32m[20221213 23:28:47 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221213 23:28:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2769 --------------------------#
[32m[20221213 23:28:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:47 @agent_ppo2.py:185][0m |          -0.0013 |          48.3151 |          12.6279 |
[32m[20221213 23:28:47 @agent_ppo2.py:185][0m |          -0.0104 |          44.7115 |          12.6427 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0026 |          46.1421 |          12.6100 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0026 |          44.1107 |          12.5730 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0071 |          42.3606 |          12.5831 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0104 |          43.5908 |          12.5167 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0167 |          41.4964 |          12.4644 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0190 |          41.3379 |          12.4738 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0127 |          40.9643 |          12.4729 |
[32m[20221213 23:28:48 @agent_ppo2.py:185][0m |          -0.0112 |          46.9572 |          12.4562 |
[32m[20221213 23:28:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:28:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.34
[32m[20221213 23:28:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.56
[32m[20221213 23:28:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.01
[32m[20221213 23:28:48 @agent_ppo2.py:143][0m Total time:      16.27 min
[32m[20221213 23:28:48 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221213 23:28:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2770 --------------------------#
[32m[20221213 23:28:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:28:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0023 |          41.6882 |          12.4464 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0096 |          35.1115 |          12.4447 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0126 |          33.1950 |          12.4443 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0131 |          32.5092 |          12.4351 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0134 |          31.6421 |          12.4409 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0143 |          31.1009 |          12.3921 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0073 |          30.9400 |          12.4185 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0144 |          30.2849 |          12.4202 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0111 |          29.8299 |          12.4171 |
[32m[20221213 23:28:49 @agent_ppo2.py:185][0m |          -0.0149 |          29.7068 |          12.4162 |
[32m[20221213 23:28:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:28:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.11
[32m[20221213 23:28:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.63
[32m[20221213 23:28:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.10
[32m[20221213 23:28:50 @agent_ppo2.py:143][0m Total time:      16.29 min
[32m[20221213 23:28:50 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221213 23:28:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2771 --------------------------#
[32m[20221213 23:28:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:50 @agent_ppo2.py:185][0m |          -0.0014 |          65.5393 |          12.4689 |
[32m[20221213 23:28:50 @agent_ppo2.py:185][0m |          -0.0040 |          62.2656 |          12.5359 |
[32m[20221213 23:28:50 @agent_ppo2.py:185][0m |          -0.0097 |          60.8209 |          12.5143 |
[32m[20221213 23:28:50 @agent_ppo2.py:185][0m |          -0.0109 |          60.0446 |          12.5582 |
[32m[20221213 23:28:50 @agent_ppo2.py:185][0m |          -0.0104 |          59.4927 |          12.5538 |
[32m[20221213 23:28:50 @agent_ppo2.py:185][0m |          -0.0133 |          59.1888 |          12.5486 |
[32m[20221213 23:28:50 @agent_ppo2.py:185][0m |          -0.0136 |          58.8628 |          12.5823 |
[32m[20221213 23:28:51 @agent_ppo2.py:185][0m |          -0.0148 |          58.7602 |          12.5747 |
[32m[20221213 23:28:51 @agent_ppo2.py:185][0m |          -0.0155 |          58.3397 |          12.5751 |
[32m[20221213 23:28:51 @agent_ppo2.py:185][0m |          -0.0170 |          58.1154 |          12.5951 |
[32m[20221213 23:28:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:28:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.64
[32m[20221213 23:28:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.29
[32m[20221213 23:28:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.87
[32m[20221213 23:28:51 @agent_ppo2.py:143][0m Total time:      16.31 min
[32m[20221213 23:28:51 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221213 23:28:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2772 --------------------------#
[32m[20221213 23:28:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:28:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:51 @agent_ppo2.py:185][0m |           0.0118 |          53.9002 |          12.6530 |
[32m[20221213 23:28:51 @agent_ppo2.py:185][0m |           0.0071 |          48.0828 |          12.6608 |
[32m[20221213 23:28:51 @agent_ppo2.py:185][0m |          -0.0049 |          47.4129 |          12.6502 |
[32m[20221213 23:28:52 @agent_ppo2.py:185][0m |          -0.0020 |          46.6360 |          12.6864 |
[32m[20221213 23:28:52 @agent_ppo2.py:185][0m |          -0.0020 |          46.2510 |          12.6343 |
[32m[20221213 23:28:52 @agent_ppo2.py:185][0m |          -0.0036 |          46.1956 |          12.6430 |
[32m[20221213 23:28:52 @agent_ppo2.py:185][0m |          -0.0077 |          45.7578 |          12.6549 |
[32m[20221213 23:28:52 @agent_ppo2.py:185][0m |          -0.0101 |          45.9538 |          12.6301 |
[32m[20221213 23:28:52 @agent_ppo2.py:185][0m |          -0.0085 |          45.3984 |          12.6304 |
[32m[20221213 23:28:52 @agent_ppo2.py:185][0m |          -0.0090 |          45.1313 |          12.6179 |
[32m[20221213 23:28:52 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.84
[32m[20221213 23:28:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.83
[32m[20221213 23:28:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.83
[32m[20221213 23:28:52 @agent_ppo2.py:143][0m Total time:      16.34 min
[32m[20221213 23:28:52 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221213 23:28:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2773 --------------------------#
[32m[20221213 23:28:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |           0.0136 |          40.1038 |          12.1940 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0033 |          33.7853 |          12.2250 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0093 |          32.3305 |          12.2040 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0080 |          31.6545 |          12.2384 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0110 |          31.3807 |          12.2302 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0123 |          30.7941 |          12.1884 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0096 |          30.2678 |          12.2408 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0121 |          30.0050 |          12.2087 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0140 |          29.8531 |          12.2086 |
[32m[20221213 23:28:53 @agent_ppo2.py:185][0m |          -0.0023 |          35.5330 |          12.2276 |
[32m[20221213 23:28:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.35
[32m[20221213 23:28:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.45
[32m[20221213 23:28:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.17
[32m[20221213 23:28:54 @agent_ppo2.py:143][0m Total time:      16.36 min
[32m[20221213 23:28:54 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221213 23:28:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2774 --------------------------#
[32m[20221213 23:28:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |           0.0034 |          34.0980 |          12.2798 |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |          -0.0061 |          30.2595 |          12.2012 |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |          -0.0094 |          29.1454 |          12.1844 |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |          -0.0115 |          28.5738 |          12.1337 |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |          -0.0060 |          30.5748 |          12.1286 |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |          -0.0101 |          27.8475 |          12.1305 |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |          -0.0160 |          27.5478 |          12.1235 |
[32m[20221213 23:28:54 @agent_ppo2.py:185][0m |          -0.0113 |          27.6785 |          12.0797 |
[32m[20221213 23:28:55 @agent_ppo2.py:185][0m |          -0.0181 |          27.0178 |          12.0644 |
[32m[20221213 23:28:55 @agent_ppo2.py:185][0m |          -0.0184 |          26.8141 |          12.0798 |
[32m[20221213 23:28:55 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.19
[32m[20221213 23:28:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.03
[32m[20221213 23:28:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.91
[32m[20221213 23:28:55 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221213 23:28:55 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221213 23:28:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2775 --------------------------#
[32m[20221213 23:28:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:55 @agent_ppo2.py:185][0m |          -0.0016 |          59.4402 |          12.3221 |
[32m[20221213 23:28:55 @agent_ppo2.py:185][0m |           0.0073 |          60.2131 |          12.2619 |
[32m[20221213 23:28:55 @agent_ppo2.py:185][0m |           0.0047 |          60.7657 |          12.2772 |
[32m[20221213 23:28:55 @agent_ppo2.py:185][0m |           0.0091 |          60.8019 |          12.2894 |
[32m[20221213 23:28:55 @agent_ppo2.py:185][0m |           0.0036 |          59.2337 |          12.2969 |
[32m[20221213 23:28:56 @agent_ppo2.py:185][0m |          -0.0077 |          55.8955 |          12.2796 |
[32m[20221213 23:28:56 @agent_ppo2.py:185][0m |          -0.0110 |          55.7698 |          12.3072 |
[32m[20221213 23:28:56 @agent_ppo2.py:185][0m |          -0.0081 |          55.6956 |          12.2978 |
[32m[20221213 23:28:56 @agent_ppo2.py:185][0m |          -0.0091 |          55.4744 |          12.3055 |
[32m[20221213 23:28:56 @agent_ppo2.py:185][0m |          -0.0139 |          55.6803 |          12.3510 |
[32m[20221213 23:28:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:28:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.65
[32m[20221213 23:28:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.55
[32m[20221213 23:28:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.82
[32m[20221213 23:28:56 @agent_ppo2.py:143][0m Total time:      16.40 min
[32m[20221213 23:28:56 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221213 23:28:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2776 --------------------------#
[32m[20221213 23:28:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:56 @agent_ppo2.py:185][0m |           0.0174 |          70.9210 |          12.1977 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |          -0.0028 |          63.3600 |          12.1457 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |          -0.0062 |          62.4738 |          12.1500 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |          -0.0067 |          62.0741 |          12.1647 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |          -0.0086 |          61.6276 |          12.1823 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |           0.0047 |          67.8717 |          12.2210 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |           0.0081 |          67.6041 |          12.2300 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |          -0.0107 |          60.9045 |          12.2306 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |          -0.0106 |          60.5630 |          12.2087 |
[32m[20221213 23:28:57 @agent_ppo2.py:185][0m |          -0.0099 |          60.4975 |          12.2078 |
[32m[20221213 23:28:57 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:28:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.97
[32m[20221213 23:28:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.58
[32m[20221213 23:28:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.44
[32m[20221213 23:28:57 @agent_ppo2.py:143][0m Total time:      16.42 min
[32m[20221213 23:28:57 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221213 23:28:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2777 --------------------------#
[32m[20221213 23:28:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0009 |          64.4143 |          12.3199 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0040 |          62.9096 |          12.3019 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0047 |          62.5695 |          12.2804 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0071 |          62.0623 |          12.2563 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0069 |          61.9101 |          12.2498 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0130 |          61.4601 |          12.1989 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0009 |          66.4553 |          12.2766 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0112 |          61.2319 |          12.2280 |
[32m[20221213 23:28:58 @agent_ppo2.py:185][0m |          -0.0134 |          60.9807 |          12.2434 |
[32m[20221213 23:28:59 @agent_ppo2.py:185][0m |          -0.0060 |          61.4900 |          12.2458 |
[32m[20221213 23:28:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:28:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.64
[32m[20221213 23:28:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.58
[32m[20221213 23:28:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 100.34
[32m[20221213 23:28:59 @agent_ppo2.py:143][0m Total time:      16.44 min
[32m[20221213 23:28:59 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221213 23:28:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2778 --------------------------#
[32m[20221213 23:28:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:28:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:28:59 @agent_ppo2.py:185][0m |          -0.0010 |          55.9905 |          12.3376 |
[32m[20221213 23:28:59 @agent_ppo2.py:185][0m |          -0.0110 |          52.8283 |          12.3444 |
[32m[20221213 23:28:59 @agent_ppo2.py:185][0m |          -0.0113 |          51.9802 |          12.3831 |
[32m[20221213 23:28:59 @agent_ppo2.py:185][0m |          -0.0104 |          51.1482 |          12.3592 |
[32m[20221213 23:28:59 @agent_ppo2.py:185][0m |          -0.0035 |          51.1724 |          12.3424 |
[32m[20221213 23:28:59 @agent_ppo2.py:185][0m |          -0.0114 |          49.9249 |          12.3473 |
[32m[20221213 23:29:00 @agent_ppo2.py:185][0m |          -0.0109 |          49.6698 |          12.3579 |
[32m[20221213 23:29:00 @agent_ppo2.py:185][0m |          -0.0124 |          49.2560 |          12.3201 |
[32m[20221213 23:29:00 @agent_ppo2.py:185][0m |          -0.0149 |          49.1540 |          12.3153 |
[32m[20221213 23:29:00 @agent_ppo2.py:185][0m |          -0.0137 |          48.9980 |          12.3031 |
[32m[20221213 23:29:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:29:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.37
[32m[20221213 23:29:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.59
[32m[20221213 23:29:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.27
[32m[20221213 23:29:00 @agent_ppo2.py:143][0m Total time:      16.47 min
[32m[20221213 23:29:00 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221213 23:29:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2779 --------------------------#
[32m[20221213 23:29:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:00 @agent_ppo2.py:185][0m |           0.0008 |          56.4826 |          12.0411 |
[32m[20221213 23:29:00 @agent_ppo2.py:185][0m |          -0.0046 |          54.2257 |          11.9944 |
[32m[20221213 23:29:00 @agent_ppo2.py:185][0m |          -0.0047 |          53.6218 |          12.0121 |
[32m[20221213 23:29:01 @agent_ppo2.py:185][0m |           0.0049 |          59.4138 |          12.0242 |
[32m[20221213 23:29:01 @agent_ppo2.py:185][0m |          -0.0064 |          53.1879 |          12.0540 |
[32m[20221213 23:29:01 @agent_ppo2.py:185][0m |          -0.0033 |          52.5551 |          12.0849 |
[32m[20221213 23:29:01 @agent_ppo2.py:185][0m |           0.0039 |          57.6077 |          12.0729 |
[32m[20221213 23:29:01 @agent_ppo2.py:185][0m |          -0.0113 |          52.2483 |          12.1190 |
[32m[20221213 23:29:01 @agent_ppo2.py:185][0m |          -0.0057 |          52.0110 |          12.0676 |
[32m[20221213 23:29:01 @agent_ppo2.py:185][0m |          -0.0098 |          51.8432 |          12.1075 |
[32m[20221213 23:29:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.20
[32m[20221213 23:29:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.62
[32m[20221213 23:29:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.83
[32m[20221213 23:29:01 @agent_ppo2.py:143][0m Total time:      16.49 min
[32m[20221213 23:29:01 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221213 23:29:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2780 --------------------------#
[32m[20221213 23:29:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:29:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |           0.0110 |          61.3467 |          12.0414 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |           0.0002 |          51.5700 |          12.0123 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0058 |          50.4874 |          12.0145 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0127 |          49.5593 |          12.0202 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0090 |          48.9867 |          12.0195 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0033 |          49.5674 |          11.9918 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0091 |          48.4429 |          11.9811 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0152 |          48.0329 |          12.0027 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0121 |          47.5591 |          11.9757 |
[32m[20221213 23:29:02 @agent_ppo2.py:185][0m |          -0.0124 |          47.1824 |          11.9960 |
[32m[20221213 23:29:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.74
[32m[20221213 23:29:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.43
[32m[20221213 23:29:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.04
[32m[20221213 23:29:03 @agent_ppo2.py:143][0m Total time:      16.51 min
[32m[20221213 23:29:03 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221213 23:29:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2781 --------------------------#
[32m[20221213 23:29:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:03 @agent_ppo2.py:185][0m |           0.0019 |          55.5019 |          12.4892 |
[32m[20221213 23:29:03 @agent_ppo2.py:185][0m |          -0.0039 |          52.0659 |          12.4545 |
[32m[20221213 23:29:03 @agent_ppo2.py:185][0m |          -0.0101 |          51.1060 |          12.4361 |
[32m[20221213 23:29:03 @agent_ppo2.py:185][0m |          -0.0137 |          50.4014 |          12.4074 |
[32m[20221213 23:29:03 @agent_ppo2.py:185][0m |          -0.0108 |          49.8634 |          12.3830 |
[32m[20221213 23:29:03 @agent_ppo2.py:185][0m |          -0.0144 |          49.5310 |          12.3679 |
[32m[20221213 23:29:03 @agent_ppo2.py:185][0m |          -0.0105 |          49.2559 |          12.3633 |
[32m[20221213 23:29:04 @agent_ppo2.py:185][0m |          -0.0127 |          48.8621 |          12.3311 |
[32m[20221213 23:29:04 @agent_ppo2.py:185][0m |          -0.0170 |          48.6374 |          12.3336 |
[32m[20221213 23:29:04 @agent_ppo2.py:185][0m |          -0.0014 |          54.9129 |          12.3067 |
[32m[20221213 23:29:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.58
[32m[20221213 23:29:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.37
[32m[20221213 23:29:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.94
[32m[20221213 23:29:04 @agent_ppo2.py:143][0m Total time:      16.53 min
[32m[20221213 23:29:04 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221213 23:29:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2782 --------------------------#
[32m[20221213 23:29:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:04 @agent_ppo2.py:185][0m |           0.0019 |          56.6667 |          12.1226 |
[32m[20221213 23:29:04 @agent_ppo2.py:185][0m |          -0.0057 |          52.9802 |          12.0662 |
[32m[20221213 23:29:04 @agent_ppo2.py:185][0m |          -0.0121 |          51.7081 |          12.1039 |
[32m[20221213 23:29:04 @agent_ppo2.py:185][0m |          -0.0119 |          50.9770 |          12.0461 |
[32m[20221213 23:29:05 @agent_ppo2.py:185][0m |          -0.0129 |          50.5058 |          12.1026 |
[32m[20221213 23:29:05 @agent_ppo2.py:185][0m |          -0.0079 |          50.2161 |          12.0555 |
[32m[20221213 23:29:05 @agent_ppo2.py:185][0m |          -0.0057 |          51.9410 |          12.0365 |
[32m[20221213 23:29:05 @agent_ppo2.py:185][0m |          -0.0148 |          49.7418 |          12.0189 |
[32m[20221213 23:29:05 @agent_ppo2.py:185][0m |          -0.0136 |          49.3303 |          11.9660 |
[32m[20221213 23:29:05 @agent_ppo2.py:185][0m |          -0.0132 |          49.1920 |          11.9813 |
[32m[20221213 23:29:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.07
[32m[20221213 23:29:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.18
[32m[20221213 23:29:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.46
[32m[20221213 23:29:05 @agent_ppo2.py:143][0m Total time:      16.55 min
[32m[20221213 23:29:05 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221213 23:29:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2783 --------------------------#
[32m[20221213 23:29:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0019 |          54.6854 |          11.8979 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0027 |          51.4514 |          11.9140 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |           0.0172 |          58.4515 |          11.9218 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |           0.0002 |          50.5725 |          11.9162 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0065 |          49.7577 |          11.9335 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0034 |          49.5088 |          11.9061 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0079 |          49.3517 |          11.9218 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0054 |          49.2552 |          11.8752 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0104 |          48.8960 |          11.9019 |
[32m[20221213 23:29:06 @agent_ppo2.py:185][0m |          -0.0107 |          48.9149 |          11.9153 |
[32m[20221213 23:29:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.86
[32m[20221213 23:29:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.54
[32m[20221213 23:29:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.37
[32m[20221213 23:29:06 @agent_ppo2.py:143][0m Total time:      16.57 min
[32m[20221213 23:29:06 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221213 23:29:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2784 --------------------------#
[32m[20221213 23:29:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |           0.0001 |          46.1265 |          11.2335 |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |           0.0027 |          45.2484 |          11.2242 |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |          -0.0013 |          42.5266 |          11.2317 |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |          -0.0098 |          39.3880 |          11.2025 |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |          -0.0032 |          40.8100 |          11.1847 |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |          -0.0139 |          38.2611 |          11.1868 |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |          -0.0137 |          37.6423 |          11.1812 |
[32m[20221213 23:29:07 @agent_ppo2.py:185][0m |           0.0006 |          40.9293 |          11.1751 |
[32m[20221213 23:29:08 @agent_ppo2.py:185][0m |          -0.0114 |          38.2709 |          11.1485 |
[32m[20221213 23:29:08 @agent_ppo2.py:185][0m |          -0.0148 |          36.7993 |          11.1372 |
[32m[20221213 23:29:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:29:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.89
[32m[20221213 23:29:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.60
[32m[20221213 23:29:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.53
[32m[20221213 23:29:08 @agent_ppo2.py:143][0m Total time:      16.60 min
[32m[20221213 23:29:08 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221213 23:29:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2785 --------------------------#
[32m[20221213 23:29:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:08 @agent_ppo2.py:185][0m |           0.0018 |          51.5380 |          11.4662 |
[32m[20221213 23:29:08 @agent_ppo2.py:185][0m |          -0.0064 |          48.8772 |          11.4908 |
[32m[20221213 23:29:08 @agent_ppo2.py:185][0m |          -0.0049 |          47.9326 |          11.4964 |
[32m[20221213 23:29:08 @agent_ppo2.py:185][0m |          -0.0118 |          47.1384 |          11.5023 |
[32m[20221213 23:29:08 @agent_ppo2.py:185][0m |          -0.0077 |          47.3090 |          11.4530 |
[32m[20221213 23:29:09 @agent_ppo2.py:185][0m |          -0.0146 |          46.1438 |          11.5102 |
[32m[20221213 23:29:09 @agent_ppo2.py:185][0m |          -0.0155 |          45.8069 |          11.5334 |
[32m[20221213 23:29:09 @agent_ppo2.py:185][0m |          -0.0151 |          45.4546 |          11.5340 |
[32m[20221213 23:29:09 @agent_ppo2.py:185][0m |          -0.0143 |          46.6673 |          11.5305 |
[32m[20221213 23:29:09 @agent_ppo2.py:185][0m |          -0.0162 |          45.0968 |          11.5394 |
[32m[20221213 23:29:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:29:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.85
[32m[20221213 23:29:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.65
[32m[20221213 23:29:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.39
[32m[20221213 23:29:09 @agent_ppo2.py:143][0m Total time:      16.62 min
[32m[20221213 23:29:09 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221213 23:29:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2786 --------------------------#
[32m[20221213 23:29:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:09 @agent_ppo2.py:185][0m |          -0.0008 |          56.7496 |          11.3650 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |          -0.0015 |          55.3190 |          11.3373 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |           0.0036 |          56.1033 |          11.3781 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |          -0.0030 |          54.1659 |          11.3356 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |          -0.0120 |          53.5002 |          11.3908 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |           0.0015 |          54.6735 |          11.3732 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |          -0.0057 |          52.9811 |          11.3842 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |          -0.0098 |          52.6943 |          11.3794 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |           0.0013 |          56.7760 |          11.4015 |
[32m[20221213 23:29:10 @agent_ppo2.py:185][0m |          -0.0057 |          52.9458 |          11.4228 |
[32m[20221213 23:29:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.65
[32m[20221213 23:29:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.10
[32m[20221213 23:29:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.50
[32m[20221213 23:29:10 @agent_ppo2.py:143][0m Total time:      16.64 min
[32m[20221213 23:29:10 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221213 23:29:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2787 --------------------------#
[32m[20221213 23:29:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |           0.0028 |          56.3163 |          11.6004 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0024 |          53.3933 |          11.6378 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0089 |          51.4774 |          11.5967 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0108 |          50.9445 |          11.5944 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0087 |          50.6678 |          11.5783 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0132 |          49.8384 |          11.5903 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0114 |          49.9481 |          11.5711 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0175 |          49.3361 |          11.5971 |
[32m[20221213 23:29:11 @agent_ppo2.py:185][0m |          -0.0109 |          49.0725 |          11.5637 |
[32m[20221213 23:29:12 @agent_ppo2.py:185][0m |          -0.0160 |          48.9399 |          11.5779 |
[32m[20221213 23:29:12 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:29:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.97
[32m[20221213 23:29:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.88
[32m[20221213 23:29:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.68
[32m[20221213 23:29:12 @agent_ppo2.py:143][0m Total time:      16.66 min
[32m[20221213 23:29:12 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221213 23:29:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2788 --------------------------#
[32m[20221213 23:29:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:29:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:12 @agent_ppo2.py:185][0m |           0.0049 |          10.3092 |          12.0472 |
[32m[20221213 23:29:12 @agent_ppo2.py:185][0m |          -0.0002 |           9.4131 |          12.0205 |
[32m[20221213 23:29:12 @agent_ppo2.py:185][0m |          -0.0011 |           9.3831 |          11.9587 |
[32m[20221213 23:29:12 @agent_ppo2.py:185][0m |          -0.0029 |           9.3686 |          11.9987 |
[32m[20221213 23:29:12 @agent_ppo2.py:185][0m |          -0.0015 |           9.3689 |          11.9956 |
[32m[20221213 23:29:12 @agent_ppo2.py:185][0m |          -0.0040 |           9.3418 |          11.9493 |
[32m[20221213 23:29:13 @agent_ppo2.py:185][0m |          -0.0037 |           9.3500 |          11.9340 |
[32m[20221213 23:29:13 @agent_ppo2.py:185][0m |           0.0081 |           9.8119 |          11.9595 |
[32m[20221213 23:29:13 @agent_ppo2.py:185][0m |          -0.0031 |           9.3341 |          11.8938 |
[32m[20221213 23:29:13 @agent_ppo2.py:185][0m |          -0.0014 |           9.3530 |          11.8862 |
[32m[20221213 23:29:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:29:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:29:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:29:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.41
[32m[20221213 23:29:13 @agent_ppo2.py:143][0m Total time:      16.68 min
[32m[20221213 23:29:13 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221213 23:29:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2789 --------------------------#
[32m[20221213 23:29:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:13 @agent_ppo2.py:185][0m |          -0.0000 |          59.1511 |          11.7611 |
[32m[20221213 23:29:13 @agent_ppo2.py:185][0m |          -0.0070 |          57.1806 |          11.7541 |
[32m[20221213 23:29:13 @agent_ppo2.py:185][0m |          -0.0064 |          56.4319 |          11.7480 |
[32m[20221213 23:29:14 @agent_ppo2.py:185][0m |          -0.0062 |          55.9384 |          11.7513 |
[32m[20221213 23:29:14 @agent_ppo2.py:185][0m |          -0.0079 |          55.6896 |          11.6965 |
[32m[20221213 23:29:14 @agent_ppo2.py:185][0m |          -0.0109 |          55.6867 |          11.7021 |
[32m[20221213 23:29:14 @agent_ppo2.py:185][0m |          -0.0128 |          55.2689 |          11.6743 |
[32m[20221213 23:29:14 @agent_ppo2.py:185][0m |          -0.0058 |          55.1084 |          11.6827 |
[32m[20221213 23:29:14 @agent_ppo2.py:185][0m |          -0.0151 |          55.0792 |          11.6351 |
[32m[20221213 23:29:14 @agent_ppo2.py:185][0m |          -0.0088 |          55.0336 |          11.7070 |
[32m[20221213 23:29:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.53
[32m[20221213 23:29:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.14
[32m[20221213 23:29:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.14
[32m[20221213 23:29:14 @agent_ppo2.py:143][0m Total time:      16.70 min
[32m[20221213 23:29:14 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221213 23:29:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2790 --------------------------#
[32m[20221213 23:29:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:29:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |           0.0038 |          63.0330 |          11.6793 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |           0.0037 |          60.8984 |          11.6809 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0075 |          56.7569 |          11.6932 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0096 |          55.0309 |          11.6404 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0145 |          53.6167 |          11.6757 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0064 |          52.9593 |          11.6590 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0163 |          51.8277 |          11.6771 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0160 |          51.5839 |          11.6588 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0178 |          50.7950 |          11.6373 |
[32m[20221213 23:29:15 @agent_ppo2.py:185][0m |          -0.0192 |          50.4560 |          11.6854 |
[32m[20221213 23:29:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:29:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.24
[32m[20221213 23:29:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.51
[32m[20221213 23:29:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.61
[32m[20221213 23:29:16 @agent_ppo2.py:143][0m Total time:      16.73 min
[32m[20221213 23:29:16 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221213 23:29:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2791 --------------------------#
[32m[20221213 23:29:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:16 @agent_ppo2.py:185][0m |          -0.0013 |          57.9054 |          11.2633 |
[32m[20221213 23:29:16 @agent_ppo2.py:185][0m |          -0.0052 |          53.5212 |          11.2513 |
[32m[20221213 23:29:16 @agent_ppo2.py:185][0m |          -0.0064 |          51.9180 |          11.2428 |
[32m[20221213 23:29:16 @agent_ppo2.py:185][0m |          -0.0079 |          51.1844 |          11.1856 |
[32m[20221213 23:29:16 @agent_ppo2.py:185][0m |          -0.0121 |          50.4959 |          11.1554 |
[32m[20221213 23:29:16 @agent_ppo2.py:185][0m |           0.0024 |          54.6634 |          11.1397 |
[32m[20221213 23:29:16 @agent_ppo2.py:185][0m |          -0.0093 |          50.1590 |          11.0908 |
[32m[20221213 23:29:17 @agent_ppo2.py:185][0m |          -0.0210 |          50.1708 |          11.0789 |
[32m[20221213 23:29:17 @agent_ppo2.py:185][0m |          -0.0115 |          49.4403 |          11.0521 |
[32m[20221213 23:29:17 @agent_ppo2.py:185][0m |          -0.0131 |          48.8746 |          11.0386 |
[32m[20221213 23:29:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:29:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.37
[32m[20221213 23:29:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.33
[32m[20221213 23:29:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.50
[32m[20221213 23:29:17 @agent_ppo2.py:143][0m Total time:      16.75 min
[32m[20221213 23:29:17 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221213 23:29:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2792 --------------------------#
[32m[20221213 23:29:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:17 @agent_ppo2.py:185][0m |           0.0012 |          63.0498 |          11.1015 |
[32m[20221213 23:29:17 @agent_ppo2.py:185][0m |          -0.0018 |          58.3831 |          11.1026 |
[32m[20221213 23:29:17 @agent_ppo2.py:185][0m |           0.0034 |          60.6509 |          11.1264 |
[32m[20221213 23:29:17 @agent_ppo2.py:185][0m |          -0.0073 |          54.9975 |          11.0633 |
[32m[20221213 23:29:18 @agent_ppo2.py:185][0m |          -0.0080 |          54.1003 |          11.0528 |
[32m[20221213 23:29:18 @agent_ppo2.py:185][0m |          -0.0065 |          54.8153 |          11.0750 |
[32m[20221213 23:29:18 @agent_ppo2.py:185][0m |           0.0005 |          56.1879 |          11.0934 |
[32m[20221213 23:29:18 @agent_ppo2.py:185][0m |          -0.0100 |          52.7080 |          11.0747 |
[32m[20221213 23:29:18 @agent_ppo2.py:185][0m |          -0.0075 |          52.2112 |          11.0792 |
[32m[20221213 23:29:18 @agent_ppo2.py:185][0m |          -0.0113 |          52.2791 |          11.0657 |
[32m[20221213 23:29:18 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:29:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.26
[32m[20221213 23:29:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.31
[32m[20221213 23:29:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.20
[32m[20221213 23:29:18 @agent_ppo2.py:143][0m Total time:      16.77 min
[32m[20221213 23:29:18 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221213 23:29:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2793 --------------------------#
[32m[20221213 23:29:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |           0.0010 |          55.6470 |          11.2112 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0029 |          53.8718 |          11.2203 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |           0.0007 |          55.1335 |          11.2975 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0018 |          53.3081 |          11.2979 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0061 |          52.7195 |          11.2909 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0044 |          52.5508 |          11.2851 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0119 |          52.5265 |          11.3453 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0054 |          52.9661 |          11.3657 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0121 |          52.2501 |          11.3826 |
[32m[20221213 23:29:19 @agent_ppo2.py:185][0m |          -0.0101 |          52.1592 |          11.3966 |
[32m[20221213 23:29:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.80
[32m[20221213 23:29:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.45
[32m[20221213 23:29:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.63
[32m[20221213 23:29:19 @agent_ppo2.py:143][0m Total time:      16.79 min
[32m[20221213 23:29:19 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221213 23:29:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2794 --------------------------#
[32m[20221213 23:29:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |          -0.0029 |          60.8450 |          11.4774 |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |           0.0048 |          60.1810 |          11.5017 |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |          -0.0103 |          54.5827 |          11.4827 |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |          -0.0112 |          53.2346 |          11.4397 |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |          -0.0108 |          52.7394 |          11.4445 |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |          -0.0117 |          52.0683 |          11.3891 |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |          -0.0078 |          51.6147 |          11.3513 |
[32m[20221213 23:29:20 @agent_ppo2.py:185][0m |          -0.0095 |          51.4634 |          11.3530 |
[32m[20221213 23:29:21 @agent_ppo2.py:185][0m |          -0.0034 |          52.2449 |          11.3130 |
[32m[20221213 23:29:21 @agent_ppo2.py:185][0m |          -0.0125 |          51.6043 |          11.2854 |
[32m[20221213 23:29:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.58
[32m[20221213 23:29:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.88
[32m[20221213 23:29:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.70
[32m[20221213 23:29:21 @agent_ppo2.py:143][0m Total time:      16.81 min
[32m[20221213 23:29:21 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221213 23:29:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2795 --------------------------#
[32m[20221213 23:29:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:21 @agent_ppo2.py:185][0m |          -0.0001 |          51.7928 |          11.1630 |
[32m[20221213 23:29:21 @agent_ppo2.py:185][0m |          -0.0085 |          47.8496 |          11.1709 |
[32m[20221213 23:29:21 @agent_ppo2.py:185][0m |          -0.0068 |          46.3476 |          11.1700 |
[32m[20221213 23:29:21 @agent_ppo2.py:185][0m |          -0.0088 |          45.0570 |          11.1489 |
[32m[20221213 23:29:21 @agent_ppo2.py:185][0m |          -0.0080 |          44.7014 |          11.1858 |
[32m[20221213 23:29:22 @agent_ppo2.py:185][0m |          -0.0063 |          44.8021 |          11.1156 |
[32m[20221213 23:29:22 @agent_ppo2.py:185][0m |          -0.0110 |          44.4849 |          11.1282 |
[32m[20221213 23:29:22 @agent_ppo2.py:185][0m |          -0.0108 |          43.8332 |          11.1450 |
[32m[20221213 23:29:22 @agent_ppo2.py:185][0m |          -0.0129 |          42.9363 |          11.1030 |
[32m[20221213 23:29:22 @agent_ppo2.py:185][0m |          -0.0121 |          42.8751 |          11.0876 |
[32m[20221213 23:29:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:29:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.20
[32m[20221213 23:29:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.28
[32m[20221213 23:29:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.63
[32m[20221213 23:29:22 @agent_ppo2.py:143][0m Total time:      16.83 min
[32m[20221213 23:29:22 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221213 23:29:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2796 --------------------------#
[32m[20221213 23:29:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:22 @agent_ppo2.py:185][0m |           0.0002 |          49.9633 |          11.1766 |
[32m[20221213 23:29:22 @agent_ppo2.py:185][0m |          -0.0105 |          43.5764 |          11.1395 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0126 |          40.5965 |          11.1539 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0127 |          39.1665 |          11.1030 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0163 |          38.3453 |          11.1532 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0159 |          37.6890 |          11.0750 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0175 |          37.0637 |          11.0663 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0190 |          36.4763 |          11.1052 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0175 |          36.3378 |          11.0862 |
[32m[20221213 23:29:23 @agent_ppo2.py:185][0m |          -0.0205 |          35.8067 |          11.0512 |
[32m[20221213 23:29:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:29:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.02
[32m[20221213 23:29:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.81
[32m[20221213 23:29:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.28
[32m[20221213 23:29:23 @agent_ppo2.py:143][0m Total time:      16.86 min
[32m[20221213 23:29:23 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221213 23:29:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2797 --------------------------#
[32m[20221213 23:29:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |           0.0015 |          31.7885 |          10.9856 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0054 |          27.7856 |          11.0086 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0023 |          26.4955 |          11.0404 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0046 |          24.8301 |          11.0331 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0086 |          25.0256 |          11.0094 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0151 |          23.7867 |          11.0373 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0169 |          23.1679 |          11.0490 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0193 |          22.5363 |          11.0189 |
[32m[20221213 23:29:24 @agent_ppo2.py:185][0m |          -0.0157 |          22.3021 |          11.0037 |
[32m[20221213 23:29:25 @agent_ppo2.py:185][0m |          -0.0213 |          21.8065 |          11.0026 |
[32m[20221213 23:29:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.39
[32m[20221213 23:29:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.30
[32m[20221213 23:29:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.43
[32m[20221213 23:29:25 @agent_ppo2.py:143][0m Total time:      16.88 min
[32m[20221213 23:29:25 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221213 23:29:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2798 --------------------------#
[32m[20221213 23:29:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:25 @agent_ppo2.py:185][0m |          -0.0011 |          59.9434 |          11.0536 |
[32m[20221213 23:29:25 @agent_ppo2.py:185][0m |          -0.0044 |          55.2670 |          11.0805 |
[32m[20221213 23:29:25 @agent_ppo2.py:185][0m |          -0.0087 |          53.8470 |          11.0267 |
[32m[20221213 23:29:25 @agent_ppo2.py:185][0m |          -0.0123 |          52.9600 |          11.0163 |
[32m[20221213 23:29:25 @agent_ppo2.py:185][0m |          -0.0132 |          52.2807 |          10.9901 |
[32m[20221213 23:29:25 @agent_ppo2.py:185][0m |           0.0012 |          59.0664 |          10.9777 |
[32m[20221213 23:29:26 @agent_ppo2.py:185][0m |          -0.0156 |          51.5436 |          11.0067 |
[32m[20221213 23:29:26 @agent_ppo2.py:185][0m |          -0.0127 |          51.2235 |          10.9955 |
[32m[20221213 23:29:26 @agent_ppo2.py:185][0m |          -0.0155 |          50.8106 |          11.0005 |
[32m[20221213 23:29:26 @agent_ppo2.py:185][0m |          -0.0157 |          50.7651 |          11.0228 |
[32m[20221213 23:29:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.00
[32m[20221213 23:29:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.59
[32m[20221213 23:29:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.59
[32m[20221213 23:29:26 @agent_ppo2.py:143][0m Total time:      16.90 min
[32m[20221213 23:29:26 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221213 23:29:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2799 --------------------------#
[32m[20221213 23:29:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:26 @agent_ppo2.py:185][0m |           0.0023 |          63.4282 |          10.6397 |
[32m[20221213 23:29:26 @agent_ppo2.py:185][0m |           0.0007 |          58.9362 |          10.7047 |
[32m[20221213 23:29:26 @agent_ppo2.py:185][0m |          -0.0052 |          57.6826 |          10.7308 |
[32m[20221213 23:29:27 @agent_ppo2.py:185][0m |          -0.0000 |          59.0739 |          10.7376 |
[32m[20221213 23:29:27 @agent_ppo2.py:185][0m |          -0.0058 |          56.4160 |          10.7750 |
[32m[20221213 23:29:27 @agent_ppo2.py:185][0m |          -0.0079 |          56.1486 |          10.8349 |
[32m[20221213 23:29:27 @agent_ppo2.py:185][0m |          -0.0008 |          58.8900 |          10.8281 |
[32m[20221213 23:29:27 @agent_ppo2.py:185][0m |          -0.0077 |          55.3483 |          10.9003 |
[32m[20221213 23:29:27 @agent_ppo2.py:185][0m |          -0.0096 |          55.4391 |          10.8589 |
[32m[20221213 23:29:27 @agent_ppo2.py:185][0m |          -0.0116 |          55.0775 |          10.8920 |
[32m[20221213 23:29:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:29:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.22
[32m[20221213 23:29:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.84
[32m[20221213 23:29:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.83
[32m[20221213 23:29:27 @agent_ppo2.py:143][0m Total time:      16.92 min
[32m[20221213 23:29:27 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221213 23:29:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2800 --------------------------#
[32m[20221213 23:29:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:29:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |           0.0013 |          60.8805 |          10.9228 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0066 |          56.5697 |          10.9522 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0087 |          55.0871 |          10.9295 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0089 |          54.3425 |          10.9369 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0098 |          53.7466 |          10.9413 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0055 |          53.4011 |          10.9721 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0136 |          53.0656 |          10.9882 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0154 |          52.6816 |          10.9749 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0141 |          52.4099 |          11.0362 |
[32m[20221213 23:29:28 @agent_ppo2.py:185][0m |          -0.0173 |          52.2246 |          10.9890 |
[32m[20221213 23:29:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:29:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.21
[32m[20221213 23:29:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.08
[32m[20221213 23:29:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.56
[32m[20221213 23:29:29 @agent_ppo2.py:143][0m Total time:      16.94 min
[32m[20221213 23:29:29 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221213 23:29:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2801 --------------------------#
[32m[20221213 23:29:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:29 @agent_ppo2.py:185][0m |          -0.0013 |          40.8381 |          11.3237 |
[32m[20221213 23:29:29 @agent_ppo2.py:185][0m |          -0.0039 |          37.2865 |          11.3104 |
[32m[20221213 23:29:29 @agent_ppo2.py:185][0m |          -0.0069 |          35.7960 |          11.2689 |
[32m[20221213 23:29:29 @agent_ppo2.py:185][0m |          -0.0096 |          35.1050 |          11.3092 |
[32m[20221213 23:29:29 @agent_ppo2.py:185][0m |          -0.0026 |          36.2725 |          11.2850 |
[32m[20221213 23:29:29 @agent_ppo2.py:185][0m |          -0.0087 |          35.4304 |          11.2685 |
[32m[20221213 23:29:29 @agent_ppo2.py:185][0m |          -0.0151 |          34.0674 |          11.2586 |
[32m[20221213 23:29:30 @agent_ppo2.py:185][0m |          -0.0116 |          33.9854 |          11.2611 |
[32m[20221213 23:29:30 @agent_ppo2.py:185][0m |          -0.0116 |          33.7190 |          11.2813 |
[32m[20221213 23:29:30 @agent_ppo2.py:185][0m |          -0.0145 |          33.4771 |          11.2915 |
[32m[20221213 23:29:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:29:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.64
[32m[20221213 23:29:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.15
[32m[20221213 23:29:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 179.48
[32m[20221213 23:29:30 @agent_ppo2.py:143][0m Total time:      16.96 min
[32m[20221213 23:29:30 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221213 23:29:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2802 --------------------------#
[32m[20221213 23:29:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:30 @agent_ppo2.py:185][0m |          -0.0024 |          41.9572 |          11.1784 |
[32m[20221213 23:29:30 @agent_ppo2.py:185][0m |          -0.0080 |          39.0889 |          11.1660 |
[32m[20221213 23:29:30 @agent_ppo2.py:185][0m |          -0.0091 |          38.0159 |          11.2034 |
[32m[20221213 23:29:30 @agent_ppo2.py:185][0m |          -0.0107 |          37.3918 |          11.2264 |
[32m[20221213 23:29:31 @agent_ppo2.py:185][0m |          -0.0162 |          36.9621 |          11.1874 |
[32m[20221213 23:29:31 @agent_ppo2.py:185][0m |          -0.0189 |          36.5281 |          11.2077 |
[32m[20221213 23:29:31 @agent_ppo2.py:185][0m |          -0.0144 |          36.1514 |          11.2267 |
[32m[20221213 23:29:31 @agent_ppo2.py:185][0m |          -0.0099 |          36.0565 |          11.2118 |
[32m[20221213 23:29:31 @agent_ppo2.py:185][0m |          -0.0113 |          35.6024 |          11.2415 |
[32m[20221213 23:29:31 @agent_ppo2.py:185][0m |          -0.0188 |          35.4806 |          11.2914 |
[32m[20221213 23:29:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:29:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.97
[32m[20221213 23:29:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.13
[32m[20221213 23:29:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.41
[32m[20221213 23:29:31 @agent_ppo2.py:143][0m Total time:      16.99 min
[32m[20221213 23:29:31 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221213 23:29:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2803 --------------------------#
[32m[20221213 23:29:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:31 @agent_ppo2.py:185][0m |           0.0092 |          46.3080 |          10.7625 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0084 |          42.7014 |          10.7686 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0091 |          41.3678 |          10.7202 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0098 |          40.6985 |          10.7636 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0146 |          40.1175 |          10.7414 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0146 |          39.6293 |          10.7843 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0150 |          39.0137 |          10.8059 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0144 |          38.7482 |          10.8140 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0010 |          46.8455 |          10.7646 |
[32m[20221213 23:29:32 @agent_ppo2.py:185][0m |          -0.0192 |          38.5192 |          10.7902 |
[32m[20221213 23:29:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:29:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.89
[32m[20221213 23:29:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.85
[32m[20221213 23:29:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.80
[32m[20221213 23:29:32 @agent_ppo2.py:143][0m Total time:      17.01 min
[32m[20221213 23:29:32 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221213 23:29:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2804 --------------------------#
[32m[20221213 23:29:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |           0.0015 |          37.0110 |          10.6412 |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |          -0.0033 |          31.4816 |          10.7024 |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |          -0.0030 |          30.3270 |          10.7699 |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |          -0.0062 |          29.0486 |          10.7713 |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |          -0.0099 |          28.7629 |          10.7370 |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |           0.0030 |          29.3472 |          10.7852 |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |          -0.0144 |          27.9732 |          10.7435 |
[32m[20221213 23:29:33 @agent_ppo2.py:185][0m |          -0.0163 |          27.4270 |          10.7138 |
[32m[20221213 23:29:34 @agent_ppo2.py:185][0m |          -0.0096 |          27.2247 |          10.7538 |
[32m[20221213 23:29:34 @agent_ppo2.py:185][0m |          -0.0152 |          26.9136 |          10.7076 |
[32m[20221213 23:29:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.12
[32m[20221213 23:29:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.26
[32m[20221213 23:29:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.41
[32m[20221213 23:29:34 @agent_ppo2.py:143][0m Total time:      17.03 min
[32m[20221213 23:29:34 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221213 23:29:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2805 --------------------------#
[32m[20221213 23:29:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:34 @agent_ppo2.py:185][0m |           0.0031 |          67.8382 |          11.2048 |
[32m[20221213 23:29:34 @agent_ppo2.py:185][0m |          -0.0081 |          61.5584 |          11.1996 |
[32m[20221213 23:29:34 @agent_ppo2.py:185][0m |          -0.0105 |          58.7181 |          11.2663 |
[32m[20221213 23:29:34 @agent_ppo2.py:185][0m |           0.0040 |          61.0438 |          11.2583 |
[32m[20221213 23:29:34 @agent_ppo2.py:185][0m |          -0.0127 |          55.3058 |          11.2654 |
[32m[20221213 23:29:35 @agent_ppo2.py:185][0m |          -0.0143 |          54.6254 |          11.2891 |
[32m[20221213 23:29:35 @agent_ppo2.py:185][0m |          -0.0029 |          59.2521 |          11.3240 |
[32m[20221213 23:29:35 @agent_ppo2.py:185][0m |          -0.0101 |          53.4906 |          11.3126 |
[32m[20221213 23:29:35 @agent_ppo2.py:185][0m |          -0.0158 |          53.1171 |          11.3954 |
[32m[20221213 23:29:35 @agent_ppo2.py:185][0m |          -0.0037 |          60.6287 |          11.3970 |
[32m[20221213 23:29:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:29:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.84
[32m[20221213 23:29:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.24
[32m[20221213 23:29:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.45
[32m[20221213 23:29:35 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221213 23:29:35 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221213 23:29:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2806 --------------------------#
[32m[20221213 23:29:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:35 @agent_ppo2.py:185][0m |           0.0026 |          54.5357 |          11.7200 |
[32m[20221213 23:29:35 @agent_ppo2.py:185][0m |          -0.0082 |          44.6073 |          11.6965 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0083 |          42.4265 |          11.7250 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0070 |          41.3009 |          11.7209 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0050 |          39.8621 |          11.7139 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0118 |          39.4267 |          11.7264 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0109 |          38.5059 |          11.7650 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0097 |          38.2449 |          11.6949 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0085 |          37.7712 |          11.7380 |
[32m[20221213 23:29:36 @agent_ppo2.py:185][0m |          -0.0082 |          37.2756 |          11.7508 |
[32m[20221213 23:29:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.86
[32m[20221213 23:29:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.84
[32m[20221213 23:29:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.39
[32m[20221213 23:29:36 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221213 23:29:36 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221213 23:29:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2807 --------------------------#
[32m[20221213 23:29:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0017 |          46.9592 |          11.2129 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0014 |          39.4421 |          11.2212 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0052 |          38.2941 |          11.1777 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0039 |          36.6685 |          11.1704 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0047 |          35.7961 |          11.1527 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0088 |          35.3467 |          11.1408 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0041 |          34.6597 |          11.1275 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0079 |          34.5815 |          11.0768 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0101 |          33.9199 |          11.0904 |
[32m[20221213 23:29:37 @agent_ppo2.py:185][0m |          -0.0155 |          33.6035 |          11.0861 |
[32m[20221213 23:29:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.24
[32m[20221213 23:29:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.43
[32m[20221213 23:29:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.28
[32m[20221213 23:29:38 @agent_ppo2.py:143][0m Total time:      17.09 min
[32m[20221213 23:29:38 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221213 23:29:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2808 --------------------------#
[32m[20221213 23:29:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:38 @agent_ppo2.py:185][0m |           0.0017 |          40.0105 |          11.5949 |
[32m[20221213 23:29:38 @agent_ppo2.py:185][0m |           0.0074 |          41.0208 |          11.5854 |
[32m[20221213 23:29:38 @agent_ppo2.py:185][0m |          -0.0066 |          33.0632 |          11.5395 |
[32m[20221213 23:29:38 @agent_ppo2.py:185][0m |          -0.0035 |          31.7322 |          11.5256 |
[32m[20221213 23:29:38 @agent_ppo2.py:185][0m |          -0.0084 |          31.3446 |          11.5083 |
[32m[20221213 23:29:38 @agent_ppo2.py:185][0m |          -0.0084 |          30.8094 |          11.4814 |
[32m[20221213 23:29:38 @agent_ppo2.py:185][0m |          -0.0105 |          30.2894 |          11.4431 |
[32m[20221213 23:29:39 @agent_ppo2.py:185][0m |          -0.0149 |          29.9700 |          11.4536 |
[32m[20221213 23:29:39 @agent_ppo2.py:185][0m |          -0.0140 |          29.5555 |          11.4314 |
[32m[20221213 23:29:39 @agent_ppo2.py:185][0m |          -0.0094 |          29.3884 |          11.4237 |
[32m[20221213 23:29:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.51
[32m[20221213 23:29:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.96
[32m[20221213 23:29:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.55
[32m[20221213 23:29:39 @agent_ppo2.py:143][0m Total time:      17.12 min
[32m[20221213 23:29:39 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221213 23:29:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2809 --------------------------#
[32m[20221213 23:29:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:39 @agent_ppo2.py:185][0m |           0.0067 |          36.2183 |          10.7844 |
[32m[20221213 23:29:39 @agent_ppo2.py:185][0m |          -0.0063 |          32.6249 |          10.7686 |
[32m[20221213 23:29:39 @agent_ppo2.py:185][0m |          -0.0020 |          30.9023 |          10.8086 |
[32m[20221213 23:29:40 @agent_ppo2.py:185][0m |          -0.0077 |          29.9495 |          10.7928 |
[32m[20221213 23:29:40 @agent_ppo2.py:185][0m |          -0.0040 |          29.2628 |          10.7903 |
[32m[20221213 23:29:40 @agent_ppo2.py:185][0m |          -0.0117 |          28.7697 |          10.7835 |
[32m[20221213 23:29:40 @agent_ppo2.py:185][0m |          -0.0117 |          28.1645 |          10.8012 |
[32m[20221213 23:29:40 @agent_ppo2.py:185][0m |          -0.0099 |          27.6726 |          10.7671 |
[32m[20221213 23:29:40 @agent_ppo2.py:185][0m |          -0.0165 |          27.3451 |          10.7653 |
[32m[20221213 23:29:40 @agent_ppo2.py:185][0m |          -0.0155 |          26.8865 |          10.7944 |
[32m[20221213 23:29:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.19
[32m[20221213 23:29:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.07
[32m[20221213 23:29:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.55
[32m[20221213 23:29:40 @agent_ppo2.py:143][0m Total time:      17.14 min
[32m[20221213 23:29:40 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221213 23:29:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2810 --------------------------#
[32m[20221213 23:29:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:29:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |           0.0008 |          54.4107 |          11.0052 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0045 |          52.7681 |          10.9837 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0017 |          52.6343 |          10.9597 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |           0.0010 |          54.5669 |          10.9574 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0033 |          52.3920 |          10.9530 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0095 |          51.6380 |          10.9593 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0070 |          51.3409 |          10.8996 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0119 |          51.2803 |          10.9306 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0104 |          51.2376 |          10.8926 |
[32m[20221213 23:29:41 @agent_ppo2.py:185][0m |          -0.0083 |          51.1212 |          10.8978 |
[32m[20221213 23:29:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.32
[32m[20221213 23:29:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.55
[32m[20221213 23:29:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.18
[32m[20221213 23:29:42 @agent_ppo2.py:143][0m Total time:      17.16 min
[32m[20221213 23:29:42 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221213 23:29:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2811 --------------------------#
[32m[20221213 23:29:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |          -0.0040 |          58.3795 |          10.2510 |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |          -0.0099 |          56.1417 |          10.2362 |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |          -0.0103 |          55.3724 |          10.2715 |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |           0.0007 |          60.8771 |          10.2796 |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |          -0.0100 |          54.8493 |          10.2992 |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |          -0.0137 |          54.6008 |          10.3111 |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |          -0.0010 |          62.0082 |          10.3473 |
[32m[20221213 23:29:42 @agent_ppo2.py:185][0m |          -0.0089 |          54.4655 |          10.4535 |
[32m[20221213 23:29:43 @agent_ppo2.py:185][0m |          -0.0127 |          54.0124 |          10.3624 |
[32m[20221213 23:29:43 @agent_ppo2.py:185][0m |          -0.0154 |          53.9097 |          10.4087 |
[32m[20221213 23:29:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.14
[32m[20221213 23:29:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.70
[32m[20221213 23:29:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.31
[32m[20221213 23:29:43 @agent_ppo2.py:143][0m Total time:      17.18 min
[32m[20221213 23:29:43 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221213 23:29:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2812 --------------------------#
[32m[20221213 23:29:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:43 @agent_ppo2.py:185][0m |           0.0007 |          55.1069 |          10.6156 |
[32m[20221213 23:29:43 @agent_ppo2.py:185][0m |          -0.0062 |          51.9165 |          10.6001 |
[32m[20221213 23:29:43 @agent_ppo2.py:185][0m |          -0.0083 |          51.4707 |          10.6402 |
[32m[20221213 23:29:43 @agent_ppo2.py:185][0m |          -0.0134 |          51.1514 |          10.6518 |
[32m[20221213 23:29:44 @agent_ppo2.py:185][0m |          -0.0067 |          50.8575 |          10.7188 |
[32m[20221213 23:29:44 @agent_ppo2.py:185][0m |          -0.0096 |          50.7447 |          10.6312 |
[32m[20221213 23:29:44 @agent_ppo2.py:185][0m |           0.0012 |          53.3149 |          10.6653 |
[32m[20221213 23:29:44 @agent_ppo2.py:185][0m |          -0.0031 |          51.2550 |          10.7106 |
[32m[20221213 23:29:44 @agent_ppo2.py:185][0m |          -0.0059 |          51.4738 |          10.6545 |
[32m[20221213 23:29:44 @agent_ppo2.py:185][0m |          -0.0113 |          50.3674 |          10.6732 |
[32m[20221213 23:29:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.98
[32m[20221213 23:29:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.60
[32m[20221213 23:29:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.26
[32m[20221213 23:29:44 @agent_ppo2.py:143][0m Total time:      17.20 min
[32m[20221213 23:29:44 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221213 23:29:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2813 --------------------------#
[32m[20221213 23:29:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:44 @agent_ppo2.py:185][0m |           0.0009 |          56.5131 |          11.1468 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |          -0.0005 |          55.0111 |          11.1779 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |           0.0030 |          56.7474 |          11.1489 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |           0.0036 |          54.3574 |          11.2236 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |          -0.0093 |          52.7461 |          11.2075 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |          -0.0105 |          52.5112 |          11.2233 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |          -0.0100 |          52.3719 |          11.2330 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |          -0.0104 |          52.3847 |          11.2490 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |          -0.0125 |          52.2617 |          11.2208 |
[32m[20221213 23:29:45 @agent_ppo2.py:185][0m |          -0.0132 |          51.9291 |          11.2578 |
[32m[20221213 23:29:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:29:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.53
[32m[20221213 23:29:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.07
[32m[20221213 23:29:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.65
[32m[20221213 23:29:45 @agent_ppo2.py:143][0m Total time:      17.22 min
[32m[20221213 23:29:45 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221213 23:29:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2814 --------------------------#
[32m[20221213 23:29:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |           0.0003 |          63.5044 |          11.1426 |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |          -0.0003 |          63.1871 |          11.1359 |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |           0.0016 |          63.8292 |          11.1258 |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |          -0.0066 |          61.9872 |          11.1846 |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |          -0.0070 |          61.4706 |          11.1316 |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |          -0.0076 |          61.4110 |          11.1056 |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |          -0.0067 |          61.2266 |          11.1318 |
[32m[20221213 23:29:46 @agent_ppo2.py:185][0m |          -0.0108 |          61.2283 |          11.0815 |
[32m[20221213 23:29:47 @agent_ppo2.py:185][0m |          -0.0126 |          61.0430 |          11.0768 |
[32m[20221213 23:29:47 @agent_ppo2.py:185][0m |          -0.0107 |          60.8805 |          11.1052 |
[32m[20221213 23:29:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:29:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.90
[32m[20221213 23:29:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.20
[32m[20221213 23:29:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.96
[32m[20221213 23:29:47 @agent_ppo2.py:143][0m Total time:      17.25 min
[32m[20221213 23:29:47 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221213 23:29:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2815 --------------------------#
[32m[20221213 23:29:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:47 @agent_ppo2.py:185][0m |          -0.0059 |          43.8844 |          11.1868 |
[32m[20221213 23:29:47 @agent_ppo2.py:185][0m |          -0.0043 |          39.1006 |          11.2809 |
[32m[20221213 23:29:47 @agent_ppo2.py:185][0m |          -0.0122 |          37.7552 |          11.2426 |
[32m[20221213 23:29:47 @agent_ppo2.py:185][0m |          -0.0153 |          37.0406 |          11.3412 |
[32m[20221213 23:29:47 @agent_ppo2.py:185][0m |          -0.0083 |          36.4332 |          11.3169 |
[32m[20221213 23:29:48 @agent_ppo2.py:185][0m |          -0.0101 |          36.1752 |          11.3741 |
[32m[20221213 23:29:48 @agent_ppo2.py:185][0m |          -0.0088 |          35.5847 |          11.3995 |
[32m[20221213 23:29:48 @agent_ppo2.py:185][0m |          -0.0128 |          35.2635 |          11.3936 |
[32m[20221213 23:29:48 @agent_ppo2.py:185][0m |          -0.0138 |          35.6058 |          11.4398 |
[32m[20221213 23:29:48 @agent_ppo2.py:185][0m |          -0.0182 |          34.6980 |          11.4354 |
[32m[20221213 23:29:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:29:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.39
[32m[20221213 23:29:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.32
[32m[20221213 23:29:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.73
[32m[20221213 23:29:48 @agent_ppo2.py:143][0m Total time:      17.27 min
[32m[20221213 23:29:48 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221213 23:29:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2816 --------------------------#
[32m[20221213 23:29:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:48 @agent_ppo2.py:185][0m |          -0.0011 |          43.8476 |          11.2462 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0062 |          37.2160 |          11.2846 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0086 |          35.8824 |          11.2995 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0110 |          35.3210 |          11.2852 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0122 |          34.8857 |          11.3187 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0119 |          34.4762 |          11.2641 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0120 |          34.1114 |          11.2899 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0111 |          33.9743 |          11.3155 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0083 |          34.3584 |          11.3207 |
[32m[20221213 23:29:49 @agent_ppo2.py:185][0m |          -0.0118 |          33.4474 |          11.3189 |
[32m[20221213 23:29:49 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 23:29:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.98
[32m[20221213 23:29:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.94
[32m[20221213 23:29:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.19
[32m[20221213 23:29:50 @agent_ppo2.py:143][0m Total time:      17.30 min
[32m[20221213 23:29:50 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221213 23:29:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2817 --------------------------#
[32m[20221213 23:29:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 23:29:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:50 @agent_ppo2.py:185][0m |          -0.0068 |          32.8879 |          11.2855 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0011 |          27.7968 |          11.2703 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0102 |          26.1158 |          11.1992 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0057 |          25.3291 |          11.2424 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0069 |          24.7682 |          11.1896 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0130 |          24.5341 |          11.1485 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0112 |          23.9758 |          11.1255 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0118 |          23.7380 |          11.1154 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0147 |          23.6133 |          11.0920 |
[32m[20221213 23:29:51 @agent_ppo2.py:185][0m |          -0.0183 |          23.2829 |          11.0964 |
[32m[20221213 23:29:51 @agent_ppo2.py:130][0m Policy update time: 1.53 s
[32m[20221213 23:29:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.52
[32m[20221213 23:29:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.85
[32m[20221213 23:29:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.66
[32m[20221213 23:29:52 @agent_ppo2.py:143][0m Total time:      17.33 min
[32m[20221213 23:29:52 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221213 23:29:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2818 --------------------------#
[32m[20221213 23:29:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:52 @agent_ppo2.py:185][0m |           0.0001 |          57.3776 |          11.0303 |
[32m[20221213 23:29:52 @agent_ppo2.py:185][0m |          -0.0002 |          52.6240 |          10.9268 |
[32m[20221213 23:29:52 @agent_ppo2.py:185][0m |          -0.0064 |          50.3720 |          10.8929 |
[32m[20221213 23:29:52 @agent_ppo2.py:185][0m |          -0.0052 |          49.3733 |          10.8680 |
[32m[20221213 23:29:52 @agent_ppo2.py:185][0m |          -0.0074 |          48.4290 |          10.8952 |
[32m[20221213 23:29:52 @agent_ppo2.py:185][0m |          -0.0121 |          47.8263 |          10.8112 |
[32m[20221213 23:29:52 @agent_ppo2.py:185][0m |          -0.0089 |          47.0993 |          10.7775 |
[32m[20221213 23:29:53 @agent_ppo2.py:185][0m |          -0.0040 |          47.0571 |          10.7104 |
[32m[20221213 23:29:53 @agent_ppo2.py:185][0m |          -0.0000 |          50.0047 |          10.6909 |
[32m[20221213 23:29:53 @agent_ppo2.py:185][0m |          -0.0106 |          46.0717 |          10.7258 |
[32m[20221213 23:29:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.04
[32m[20221213 23:29:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.36
[32m[20221213 23:29:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.03
[32m[20221213 23:29:53 @agent_ppo2.py:143][0m Total time:      17.35 min
[32m[20221213 23:29:53 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221213 23:29:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2819 --------------------------#
[32m[20221213 23:29:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:29:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:53 @agent_ppo2.py:185][0m |           0.0002 |          40.2732 |          10.9919 |
[32m[20221213 23:29:53 @agent_ppo2.py:185][0m |          -0.0043 |          34.5730 |          11.0664 |
[32m[20221213 23:29:53 @agent_ppo2.py:185][0m |          -0.0103 |          32.5491 |          11.0637 |
[32m[20221213 23:29:53 @agent_ppo2.py:185][0m |          -0.0113 |          31.5152 |          11.1504 |
[32m[20221213 23:29:54 @agent_ppo2.py:185][0m |          -0.0118 |          30.8388 |          11.1520 |
[32m[20221213 23:29:54 @agent_ppo2.py:185][0m |          -0.0080 |          30.0848 |          11.1756 |
[32m[20221213 23:29:54 @agent_ppo2.py:185][0m |          -0.0000 |          29.9363 |          11.2423 |
[32m[20221213 23:29:54 @agent_ppo2.py:185][0m |          -0.0143 |          29.2544 |          11.2573 |
[32m[20221213 23:29:54 @agent_ppo2.py:185][0m |          -0.0165 |          28.8139 |          11.2528 |
[32m[20221213 23:29:54 @agent_ppo2.py:185][0m |          -0.0065 |          28.5531 |          11.2931 |
[32m[20221213 23:29:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:29:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.04
[32m[20221213 23:29:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.97
[32m[20221213 23:29:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.34
[32m[20221213 23:29:54 @agent_ppo2.py:143][0m Total time:      17.37 min
[32m[20221213 23:29:54 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221213 23:29:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2820 --------------------------#
[32m[20221213 23:29:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:29:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0017 |          47.4018 |          10.7380 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |           0.0030 |          44.0792 |          10.7811 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0087 |          40.3319 |          10.7826 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0037 |          39.5145 |          10.7975 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0071 |          38.4268 |          10.7909 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0088 |          37.6889 |          10.8123 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0156 |          37.1104 |          10.7935 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0091 |          36.9504 |          10.7983 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0137 |          36.5035 |          10.8229 |
[32m[20221213 23:29:55 @agent_ppo2.py:185][0m |          -0.0119 |          36.4935 |          10.8584 |
[32m[20221213 23:29:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:29:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.29
[32m[20221213 23:29:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.27
[32m[20221213 23:29:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.21
[32m[20221213 23:29:55 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221213 23:29:55 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221213 23:29:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2821 --------------------------#
[32m[20221213 23:29:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |           0.0034 |          46.9954 |          11.1634 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0094 |          41.4038 |          11.1715 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0085 |          39.9371 |          11.2411 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0096 |          39.0779 |          11.2014 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0112 |          38.4117 |          11.1671 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0107 |          37.7893 |          11.1764 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0119 |          37.9699 |          11.1357 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0208 |          37.5128 |          11.1689 |
[32m[20221213 23:29:56 @agent_ppo2.py:185][0m |          -0.0169 |          37.0420 |          11.1760 |
[32m[20221213 23:29:57 @agent_ppo2.py:185][0m |          -0.0137 |          37.1837 |          11.1334 |
[32m[20221213 23:29:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:29:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.78
[32m[20221213 23:29:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.60
[32m[20221213 23:29:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.85
[32m[20221213 23:29:57 @agent_ppo2.py:143][0m Total time:      17.41 min
[32m[20221213 23:29:57 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221213 23:29:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2822 --------------------------#
[32m[20221213 23:29:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:29:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:57 @agent_ppo2.py:185][0m |           0.0068 |          63.2320 |          10.9555 |
[32m[20221213 23:29:57 @agent_ppo2.py:185][0m |          -0.0017 |          59.0514 |          10.9933 |
[32m[20221213 23:29:57 @agent_ppo2.py:185][0m |          -0.0061 |          58.0958 |          10.9883 |
[32m[20221213 23:29:57 @agent_ppo2.py:185][0m |           0.0139 |          65.4704 |          11.0482 |
[32m[20221213 23:29:58 @agent_ppo2.py:185][0m |          -0.0099 |          57.4351 |          11.0653 |
[32m[20221213 23:29:58 @agent_ppo2.py:185][0m |          -0.0110 |          56.8440 |          11.0546 |
[32m[20221213 23:29:58 @agent_ppo2.py:185][0m |          -0.0020 |          62.4311 |          11.0658 |
[32m[20221213 23:29:58 @agent_ppo2.py:185][0m |          -0.0110 |          56.6585 |          11.1288 |
[32m[20221213 23:29:58 @agent_ppo2.py:185][0m |          -0.0066 |          57.2354 |          11.0941 |
[32m[20221213 23:29:58 @agent_ppo2.py:185][0m |          -0.0097 |          56.2671 |          11.1222 |
[32m[20221213 23:29:58 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 23:29:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.96
[32m[20221213 23:29:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.69
[32m[20221213 23:29:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.12
[32m[20221213 23:29:58 @agent_ppo2.py:143][0m Total time:      17.44 min
[32m[20221213 23:29:58 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221213 23:29:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2823 --------------------------#
[32m[20221213 23:29:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:29:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:29:58 @agent_ppo2.py:185][0m |           0.0026 |          50.6437 |          11.2527 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0108 |          44.3692 |          11.1333 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |           0.0032 |          47.9829 |          11.1454 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0109 |          42.1571 |          11.1150 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0114 |          41.0259 |          11.0719 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0048 |          46.5939 |          11.0511 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0159 |          41.3240 |          11.0765 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0180 |          40.5264 |          11.0182 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0211 |          39.6608 |          11.0424 |
[32m[20221213 23:29:59 @agent_ppo2.py:185][0m |          -0.0164 |          39.1982 |          10.9854 |
[32m[20221213 23:29:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:29:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.44
[32m[20221213 23:29:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.31
[32m[20221213 23:29:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.61
[32m[20221213 23:29:59 @agent_ppo2.py:143][0m Total time:      17.46 min
[32m[20221213 23:29:59 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221213 23:29:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2824 --------------------------#
[32m[20221213 23:30:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |           0.0080 |          52.8271 |          10.7517 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0044 |          49.1328 |          10.7288 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0053 |          48.5276 |          10.8186 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0038 |          48.2184 |          10.8781 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0050 |          47.7722 |          10.8372 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0021 |          48.5855 |          10.8951 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0060 |          47.0580 |          10.9074 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0081 |          46.9847 |          10.9117 |
[32m[20221213 23:30:00 @agent_ppo2.py:185][0m |          -0.0088 |          46.7357 |          10.9555 |
[32m[20221213 23:30:01 @agent_ppo2.py:185][0m |          -0.0084 |          46.5419 |          10.9377 |
[32m[20221213 23:30:01 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:30:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.85
[32m[20221213 23:30:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.35
[32m[20221213 23:30:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.24
[32m[20221213 23:30:01 @agent_ppo2.py:143][0m Total time:      17.48 min
[32m[20221213 23:30:01 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221213 23:30:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2825 --------------------------#
[32m[20221213 23:30:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:01 @agent_ppo2.py:185][0m |           0.0001 |          61.4646 |          10.5905 |
[32m[20221213 23:30:01 @agent_ppo2.py:185][0m |          -0.0028 |          59.3875 |          10.6037 |
[32m[20221213 23:30:01 @agent_ppo2.py:185][0m |           0.0017 |          58.6795 |          10.6110 |
[32m[20221213 23:30:01 @agent_ppo2.py:185][0m |          -0.0068 |          56.2362 |          10.6400 |
[32m[20221213 23:30:01 @agent_ppo2.py:185][0m |          -0.0082 |          55.6110 |          10.6644 |
[32m[20221213 23:30:02 @agent_ppo2.py:185][0m |          -0.0069 |          54.9024 |          10.6271 |
[32m[20221213 23:30:02 @agent_ppo2.py:185][0m |          -0.0083 |          53.9266 |          10.5963 |
[32m[20221213 23:30:02 @agent_ppo2.py:185][0m |          -0.0094 |          53.1199 |          10.6118 |
[32m[20221213 23:30:02 @agent_ppo2.py:185][0m |          -0.0075 |          52.6047 |          10.5803 |
[32m[20221213 23:30:02 @agent_ppo2.py:185][0m |          -0.0113 |          51.9884 |          10.6753 |
[32m[20221213 23:30:02 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:30:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.52
[32m[20221213 23:30:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.41
[32m[20221213 23:30:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.52
[32m[20221213 23:30:02 @agent_ppo2.py:143][0m Total time:      17.50 min
[32m[20221213 23:30:02 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221213 23:30:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2826 --------------------------#
[32m[20221213 23:30:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:02 @agent_ppo2.py:185][0m |           0.0022 |          55.6782 |          11.2323 |
[32m[20221213 23:30:02 @agent_ppo2.py:185][0m |          -0.0063 |          49.2869 |          11.2164 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0069 |          47.4600 |          11.2222 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0056 |          46.5428 |          11.1930 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0085 |          45.8643 |          11.2049 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0102 |          45.4021 |          11.1413 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0058 |          45.1193 |          11.1612 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0081 |          44.5940 |          11.1676 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0156 |          44.3519 |          11.1876 |
[32m[20221213 23:30:03 @agent_ppo2.py:185][0m |          -0.0074 |          44.1420 |          11.1468 |
[32m[20221213 23:30:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:30:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.24
[32m[20221213 23:30:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.25
[32m[20221213 23:30:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.66
[32m[20221213 23:30:03 @agent_ppo2.py:143][0m Total time:      17.52 min
[32m[20221213 23:30:03 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221213 23:30:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2827 --------------------------#
[32m[20221213 23:30:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0005 |          43.7632 |          11.2914 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0053 |          38.8475 |          11.3144 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0056 |          36.7121 |          11.3263 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0026 |          37.4169 |          11.3112 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0064 |          35.0404 |          11.3039 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0084 |          34.5094 |          11.3221 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0078 |          34.6640 |          11.3523 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0107 |          33.7308 |          11.3099 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0054 |          33.3212 |          11.3239 |
[32m[20221213 23:30:04 @agent_ppo2.py:185][0m |          -0.0099 |          33.2545 |          11.3294 |
[32m[20221213 23:30:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.13
[32m[20221213 23:30:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.23
[32m[20221213 23:30:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.39
[32m[20221213 23:30:05 @agent_ppo2.py:143][0m Total time:      17.54 min
[32m[20221213 23:30:05 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221213 23:30:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2828 --------------------------#
[32m[20221213 23:30:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:05 @agent_ppo2.py:185][0m |           0.0040 |          34.2206 |          11.2786 |
[32m[20221213 23:30:05 @agent_ppo2.py:185][0m |          -0.0025 |          31.5959 |          11.2416 |
[32m[20221213 23:30:05 @agent_ppo2.py:185][0m |          -0.0056 |          30.6543 |          11.2529 |
[32m[20221213 23:30:05 @agent_ppo2.py:185][0m |          -0.0047 |          29.9687 |          11.2052 |
[32m[20221213 23:30:05 @agent_ppo2.py:185][0m |          -0.0080 |          29.5199 |          11.1924 |
[32m[20221213 23:30:05 @agent_ppo2.py:185][0m |          -0.0044 |          29.1654 |          11.1977 |
[32m[20221213 23:30:06 @agent_ppo2.py:185][0m |          -0.0112 |          28.6222 |          11.2456 |
[32m[20221213 23:30:06 @agent_ppo2.py:185][0m |          -0.0116 |          28.3677 |          11.1668 |
[32m[20221213 23:30:06 @agent_ppo2.py:185][0m |          -0.0117 |          28.0308 |          11.1816 |
[32m[20221213 23:30:06 @agent_ppo2.py:185][0m |          -0.0130 |          27.8306 |          11.1445 |
[32m[20221213 23:30:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:30:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.00
[32m[20221213 23:30:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.48
[32m[20221213 23:30:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.40
[32m[20221213 23:30:06 @agent_ppo2.py:143][0m Total time:      17.57 min
[32m[20221213 23:30:06 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221213 23:30:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2829 --------------------------#
[32m[20221213 23:30:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:06 @agent_ppo2.py:185][0m |           0.0192 |          66.0335 |          10.9874 |
[32m[20221213 23:30:06 @agent_ppo2.py:185][0m |          -0.0040 |          56.6407 |          11.0508 |
[32m[20221213 23:30:06 @agent_ppo2.py:185][0m |          -0.0063 |          55.8982 |          10.9701 |
[32m[20221213 23:30:07 @agent_ppo2.py:185][0m |          -0.0112 |          55.4700 |          10.9998 |
[32m[20221213 23:30:07 @agent_ppo2.py:185][0m |          -0.0020 |          58.0348 |          11.0779 |
[32m[20221213 23:30:07 @agent_ppo2.py:185][0m |          -0.0105 |          55.0060 |          11.0336 |
[32m[20221213 23:30:07 @agent_ppo2.py:185][0m |          -0.0124 |          54.8438 |          11.0536 |
[32m[20221213 23:30:07 @agent_ppo2.py:185][0m |          -0.0067 |          55.7774 |          11.0738 |
[32m[20221213 23:30:07 @agent_ppo2.py:185][0m |          -0.0122 |          54.6441 |          11.0966 |
[32m[20221213 23:30:07 @agent_ppo2.py:185][0m |          -0.0105 |          54.8283 |          11.0109 |
[32m[20221213 23:30:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:30:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.43
[32m[20221213 23:30:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.02
[32m[20221213 23:30:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.28
[32m[20221213 23:30:07 @agent_ppo2.py:143][0m Total time:      17.59 min
[32m[20221213 23:30:07 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221213 23:30:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2830 --------------------------#
[32m[20221213 23:30:07 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:30:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |           0.0131 |          54.3587 |          11.4307 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0046 |          48.0786 |          11.4720 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0072 |          46.4631 |          11.5578 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0051 |          45.5680 |          11.6094 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0079 |          45.0636 |          11.6250 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0077 |          44.9548 |          11.6695 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0088 |          44.2617 |          11.7230 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0104 |          43.8214 |          11.7445 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0144 |          43.4227 |          11.7882 |
[32m[20221213 23:30:08 @agent_ppo2.py:185][0m |          -0.0113 |          43.1789 |          11.7988 |
[32m[20221213 23:30:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:30:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.87
[32m[20221213 23:30:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.90
[32m[20221213 23:30:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.88
[32m[20221213 23:30:09 @agent_ppo2.py:143][0m Total time:      17.61 min
[32m[20221213 23:30:09 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221213 23:30:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2831 --------------------------#
[32m[20221213 23:30:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:09 @agent_ppo2.py:185][0m |           0.0176 |          51.4706 |          11.2483 |
[32m[20221213 23:30:09 @agent_ppo2.py:185][0m |          -0.0027 |          40.9938 |          11.2515 |
[32m[20221213 23:30:09 @agent_ppo2.py:185][0m |          -0.0031 |          40.3238 |          11.2573 |
[32m[20221213 23:30:09 @agent_ppo2.py:185][0m |          -0.0066 |          38.6088 |          11.2569 |
[32m[20221213 23:30:09 @agent_ppo2.py:185][0m |          -0.0082 |          37.9563 |          11.2856 |
[32m[20221213 23:30:09 @agent_ppo2.py:185][0m |          -0.0123 |          37.3503 |          11.2534 |
[32m[20221213 23:30:10 @agent_ppo2.py:185][0m |          -0.0105 |          37.0675 |          11.2894 |
[32m[20221213 23:30:10 @agent_ppo2.py:185][0m |          -0.0133 |          36.6922 |          11.2569 |
[32m[20221213 23:30:10 @agent_ppo2.py:185][0m |          -0.0060 |          36.5217 |          11.2467 |
[32m[20221213 23:30:10 @agent_ppo2.py:185][0m |          -0.0145 |          35.8520 |          11.3047 |
[32m[20221213 23:30:10 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 23:30:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.59
[32m[20221213 23:30:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.32
[32m[20221213 23:30:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.19
[32m[20221213 23:30:10 @agent_ppo2.py:143][0m Total time:      17.63 min
[32m[20221213 23:30:10 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221213 23:30:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2832 --------------------------#
[32m[20221213 23:30:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:10 @agent_ppo2.py:185][0m |           0.0004 |          49.0495 |          11.0113 |
[32m[20221213 23:30:10 @agent_ppo2.py:185][0m |          -0.0057 |          44.8961 |          10.9742 |
[32m[20221213 23:30:10 @agent_ppo2.py:185][0m |           0.0006 |          43.5264 |          10.9565 |
[32m[20221213 23:30:11 @agent_ppo2.py:185][0m |          -0.0051 |          41.7754 |          10.9306 |
[32m[20221213 23:30:11 @agent_ppo2.py:185][0m |          -0.0093 |          41.0956 |          10.9715 |
[32m[20221213 23:30:11 @agent_ppo2.py:185][0m |          -0.0098 |          40.2375 |          10.9786 |
[32m[20221213 23:30:11 @agent_ppo2.py:185][0m |          -0.0121 |          39.5370 |          10.9779 |
[32m[20221213 23:30:11 @agent_ppo2.py:185][0m |          -0.0094 |          38.9514 |          10.9584 |
[32m[20221213 23:30:11 @agent_ppo2.py:185][0m |          -0.0085 |          38.8930 |          10.9597 |
[32m[20221213 23:30:11 @agent_ppo2.py:185][0m |          -0.0152 |          38.2956 |          10.9578 |
[32m[20221213 23:30:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:30:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.65
[32m[20221213 23:30:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.04
[32m[20221213 23:30:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.75
[32m[20221213 23:30:11 @agent_ppo2.py:143][0m Total time:      17.65 min
[32m[20221213 23:30:11 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221213 23:30:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2833 --------------------------#
[32m[20221213 23:30:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 23:30:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |           0.0091 |          42.0498 |          11.5798 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0002 |          36.5937 |          11.6452 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0070 |          34.4238 |          11.6253 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0048 |          33.3270 |          11.6319 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0077 |          32.5824 |          11.6421 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0111 |          31.6592 |          11.6549 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0177 |          31.1765 |          11.6408 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0204 |          30.7927 |          11.6264 |
[32m[20221213 23:30:12 @agent_ppo2.py:185][0m |          -0.0180 |          30.4166 |          11.6468 |
[32m[20221213 23:30:13 @agent_ppo2.py:185][0m |          -0.0155 |          30.1958 |          11.6452 |
[32m[20221213 23:30:13 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 23:30:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.49
[32m[20221213 23:30:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.40
[32m[20221213 23:30:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.63
[32m[20221213 23:30:13 @agent_ppo2.py:143][0m Total time:      17.68 min
[32m[20221213 23:30:13 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221213 23:30:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2834 --------------------------#
[32m[20221213 23:30:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:13 @agent_ppo2.py:185][0m |           0.0017 |          57.3817 |          11.3567 |
[32m[20221213 23:30:13 @agent_ppo2.py:185][0m |           0.0008 |          50.2416 |          11.3690 |
[32m[20221213 23:30:13 @agent_ppo2.py:185][0m |          -0.0040 |          48.1509 |          11.3687 |
[32m[20221213 23:30:13 @agent_ppo2.py:185][0m |          -0.0088 |          47.1576 |          11.4131 |
[32m[20221213 23:30:13 @agent_ppo2.py:185][0m |          -0.0084 |          46.6449 |          11.4469 |
[32m[20221213 23:30:14 @agent_ppo2.py:185][0m |          -0.0117 |          46.2007 |          11.4481 |
[32m[20221213 23:30:14 @agent_ppo2.py:185][0m |          -0.0060 |          46.7621 |          11.4379 |
[32m[20221213 23:30:14 @agent_ppo2.py:185][0m |          -0.0161 |          45.2555 |          11.4748 |
[32m[20221213 23:30:14 @agent_ppo2.py:185][0m |          -0.0044 |          49.8766 |          11.4988 |
[32m[20221213 23:30:14 @agent_ppo2.py:185][0m |          -0.0193 |          45.0459 |          11.5357 |
[32m[20221213 23:30:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:30:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.52
[32m[20221213 23:30:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.50
[32m[20221213 23:30:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.53
[32m[20221213 23:30:14 @agent_ppo2.py:143][0m Total time:      17.70 min
[32m[20221213 23:30:14 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221213 23:30:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2835 --------------------------#
[32m[20221213 23:30:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:14 @agent_ppo2.py:185][0m |           0.0015 |          46.6022 |          11.7571 |
[32m[20221213 23:30:14 @agent_ppo2.py:185][0m |          -0.0105 |          41.6795 |          11.7294 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0084 |          40.0021 |          11.7891 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0074 |          39.1432 |          11.7540 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0070 |          38.7748 |          11.7141 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0143 |          38.0880 |          11.7492 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0141 |          37.7578 |          11.7669 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0150 |          37.5899 |          11.7515 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0098 |          37.8118 |          11.7280 |
[32m[20221213 23:30:15 @agent_ppo2.py:185][0m |          -0.0190 |          37.0950 |          11.7276 |
[32m[20221213 23:30:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:30:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.86
[32m[20221213 23:30:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.97
[32m[20221213 23:30:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.17
[32m[20221213 23:30:15 @agent_ppo2.py:143][0m Total time:      17.72 min
[32m[20221213 23:30:15 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221213 23:30:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2836 --------------------------#
[32m[20221213 23:30:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |           0.0027 |          55.7856 |          11.4654 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0092 |          52.6002 |          11.3586 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0070 |          52.1552 |          11.3736 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0120 |          51.5409 |          11.3852 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0089 |          51.4948 |          11.3606 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0115 |          51.1392 |          11.3424 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0083 |          51.0494 |          11.3084 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0139 |          50.8245 |          11.3145 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0145 |          50.7000 |          11.2845 |
[32m[20221213 23:30:16 @agent_ppo2.py:185][0m |          -0.0093 |          53.0805 |          11.1969 |
[32m[20221213 23:30:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:30:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.74
[32m[20221213 23:30:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.62
[32m[20221213 23:30:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.13
[32m[20221213 23:30:17 @agent_ppo2.py:143][0m Total time:      17.74 min
[32m[20221213 23:30:17 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221213 23:30:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2837 --------------------------#
[32m[20221213 23:30:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:17 @agent_ppo2.py:185][0m |           0.0046 |          45.3311 |          11.2447 |
[32m[20221213 23:30:17 @agent_ppo2.py:185][0m |          -0.0021 |          39.9379 |          11.2149 |
[32m[20221213 23:30:17 @agent_ppo2.py:185][0m |          -0.0043 |          38.4028 |          11.2144 |
[32m[20221213 23:30:17 @agent_ppo2.py:185][0m |          -0.0051 |          37.7707 |          11.1729 |
[32m[20221213 23:30:17 @agent_ppo2.py:185][0m |          -0.0047 |          37.8621 |          11.1933 |
[32m[20221213 23:30:17 @agent_ppo2.py:185][0m |          -0.0103 |          36.4074 |          11.1921 |
[32m[20221213 23:30:17 @agent_ppo2.py:185][0m |          -0.0146 |          36.0182 |          11.1596 |
[32m[20221213 23:30:18 @agent_ppo2.py:185][0m |          -0.0162 |          35.8798 |          11.1507 |
[32m[20221213 23:30:18 @agent_ppo2.py:185][0m |          -0.0124 |          35.3443 |          11.1157 |
[32m[20221213 23:30:18 @agent_ppo2.py:185][0m |          -0.0051 |          38.8309 |          11.1198 |
[32m[20221213 23:30:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.87
[32m[20221213 23:30:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.74
[32m[20221213 23:30:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.27
[32m[20221213 23:30:18 @agent_ppo2.py:143][0m Total time:      17.76 min
[32m[20221213 23:30:18 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221213 23:30:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2838 --------------------------#
[32m[20221213 23:30:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:18 @agent_ppo2.py:185][0m |           0.0006 |          59.3580 |          11.1585 |
[32m[20221213 23:30:18 @agent_ppo2.py:185][0m |          -0.0045 |          57.8120 |          11.1067 |
[32m[20221213 23:30:18 @agent_ppo2.py:185][0m |          -0.0055 |          57.3635 |          11.1874 |
[32m[20221213 23:30:18 @agent_ppo2.py:185][0m |           0.0035 |          60.6035 |          11.2032 |
[32m[20221213 23:30:19 @agent_ppo2.py:185][0m |          -0.0089 |          57.0618 |          11.2098 |
[32m[20221213 23:30:19 @agent_ppo2.py:185][0m |          -0.0104 |          56.8175 |          11.2309 |
[32m[20221213 23:30:19 @agent_ppo2.py:185][0m |           0.0056 |          60.8473 |          11.2532 |
[32m[20221213 23:30:19 @agent_ppo2.py:185][0m |          -0.0097 |          56.8645 |          11.2770 |
[32m[20221213 23:30:19 @agent_ppo2.py:185][0m |          -0.0116 |          56.3394 |          11.2966 |
[32m[20221213 23:30:19 @agent_ppo2.py:185][0m |          -0.0107 |          56.2433 |          11.3203 |
[32m[20221213 23:30:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.40
[32m[20221213 23:30:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.79
[32m[20221213 23:30:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.85
[32m[20221213 23:30:19 @agent_ppo2.py:143][0m Total time:      17.79 min
[32m[20221213 23:30:19 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221213 23:30:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2839 --------------------------#
[32m[20221213 23:30:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:19 @agent_ppo2.py:185][0m |           0.0060 |          44.2965 |          11.4260 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |           0.0085 |          44.7214 |          11.4132 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |          -0.0092 |          39.5582 |          11.4921 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |          -0.0081 |          38.2824 |          11.4427 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |          -0.0100 |          37.6950 |          11.4688 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |          -0.0076 |          37.2510 |          11.4578 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |           0.0045 |          37.0055 |          11.4846 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |          -0.0138 |          36.6266 |          11.4845 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |          -0.0156 |          36.3085 |          11.4919 |
[32m[20221213 23:30:20 @agent_ppo2.py:185][0m |          -0.0134 |          36.0207 |          11.4746 |
[32m[20221213 23:30:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.41
[32m[20221213 23:30:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.00
[32m[20221213 23:30:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.60
[32m[20221213 23:30:20 @agent_ppo2.py:143][0m Total time:      17.81 min
[32m[20221213 23:30:20 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221213 23:30:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2840 --------------------------#
[32m[20221213 23:30:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:30:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |           0.0026 |          63.4381 |          11.1587 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |           0.0014 |          62.5220 |          11.1962 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |          -0.0043 |          61.7192 |          11.1271 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |          -0.0052 |          61.4380 |          11.1371 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |          -0.0071 |          61.0473 |          11.1285 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |           0.0067 |          63.5161 |          11.1164 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |          -0.0095 |          61.0323 |          11.1199 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |          -0.0098 |          60.7943 |          11.0784 |
[32m[20221213 23:30:21 @agent_ppo2.py:185][0m |          -0.0086 |          60.5407 |          11.0700 |
[32m[20221213 23:30:22 @agent_ppo2.py:185][0m |           0.0033 |          67.2881 |          11.0925 |
[32m[20221213 23:30:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:30:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.93
[32m[20221213 23:30:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.93
[32m[20221213 23:30:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.12
[32m[20221213 23:30:22 @agent_ppo2.py:143][0m Total time:      17.83 min
[32m[20221213 23:30:22 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221213 23:30:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2841 --------------------------#
[32m[20221213 23:30:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:22 @agent_ppo2.py:185][0m |           0.0034 |          28.3506 |          11.1347 |
[32m[20221213 23:30:22 @agent_ppo2.py:185][0m |          -0.0091 |          23.8196 |          11.1070 |
[32m[20221213 23:30:22 @agent_ppo2.py:185][0m |          -0.0064 |          22.5456 |          11.1015 |
[32m[20221213 23:30:22 @agent_ppo2.py:185][0m |          -0.0138 |          21.8172 |          11.1032 |
[32m[20221213 23:30:22 @agent_ppo2.py:185][0m |          -0.0080 |          21.1218 |          11.0947 |
[32m[20221213 23:30:22 @agent_ppo2.py:185][0m |          -0.0178 |          20.7478 |          11.0717 |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0145 |          20.3456 |          11.0646 |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0144 |          20.2240 |          11.0970 |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0200 |          19.7763 |          11.0658 |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0140 |          19.5275 |          11.0948 |
[32m[20221213 23:30:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.84
[32m[20221213 23:30:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.92
[32m[20221213 23:30:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.18
[32m[20221213 23:30:23 @agent_ppo2.py:143][0m Total time:      17.85 min
[32m[20221213 23:30:23 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221213 23:30:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2842 --------------------------#
[32m[20221213 23:30:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0008 |          22.3392 |          10.9491 |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0053 |          16.5507 |          10.9574 |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0072 |          14.9460 |          10.9182 |
[32m[20221213 23:30:23 @agent_ppo2.py:185][0m |          -0.0077 |          13.9986 |          10.9124 |
[32m[20221213 23:30:24 @agent_ppo2.py:185][0m |          -0.0151 |          13.4218 |          10.8792 |
[32m[20221213 23:30:24 @agent_ppo2.py:185][0m |          -0.0113 |          12.8560 |          10.8843 |
[32m[20221213 23:30:24 @agent_ppo2.py:185][0m |          -0.0105 |          12.4634 |          10.8226 |
[32m[20221213 23:30:24 @agent_ppo2.py:185][0m |          -0.0083 |          12.3105 |          10.8363 |
[32m[20221213 23:30:24 @agent_ppo2.py:185][0m |          -0.0116 |          12.0310 |          10.7814 |
[32m[20221213 23:30:24 @agent_ppo2.py:185][0m |          -0.0182 |          11.8166 |          10.7888 |
[32m[20221213 23:30:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.69
[32m[20221213 23:30:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.71
[32m[20221213 23:30:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.74
[32m[20221213 23:30:24 @agent_ppo2.py:143][0m Total time:      17.87 min
[32m[20221213 23:30:24 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221213 23:30:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2843 --------------------------#
[32m[20221213 23:30:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:24 @agent_ppo2.py:185][0m |           0.0018 |          50.6113 |          10.9236 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0051 |          46.1396 |          10.9702 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0046 |          44.7849 |          10.9394 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0103 |          43.8639 |          10.9760 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0089 |          43.1565 |          11.0058 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0091 |          42.6465 |          10.9293 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0117 |          42.2530 |          10.9605 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0111 |          42.1305 |          10.9486 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0126 |          41.5418 |          10.9533 |
[32m[20221213 23:30:25 @agent_ppo2.py:185][0m |          -0.0168 |          41.3963 |          10.9170 |
[32m[20221213 23:30:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.98
[32m[20221213 23:30:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.15
[32m[20221213 23:30:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.79
[32m[20221213 23:30:25 @agent_ppo2.py:143][0m Total time:      17.89 min
[32m[20221213 23:30:25 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221213 23:30:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2844 --------------------------#
[32m[20221213 23:30:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |           0.0084 |          72.9460 |          10.5136 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |           0.0025 |          67.5769 |          10.5114 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |          -0.0083 |          65.0400 |          10.5640 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |          -0.0047 |          64.0088 |          10.5381 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |          -0.0087 |          63.7955 |          10.5914 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |          -0.0076 |          62.8511 |          10.6158 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |          -0.0079 |          62.5497 |          10.6262 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |          -0.0074 |          62.1351 |          10.6108 |
[32m[20221213 23:30:26 @agent_ppo2.py:185][0m |          -0.0067 |          61.8257 |          10.6441 |
[32m[20221213 23:30:27 @agent_ppo2.py:185][0m |          -0.0095 |          61.8764 |          10.6301 |
[32m[20221213 23:30:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.39
[32m[20221213 23:30:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.23
[32m[20221213 23:30:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.70
[32m[20221213 23:30:27 @agent_ppo2.py:143][0m Total time:      17.91 min
[32m[20221213 23:30:27 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221213 23:30:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2845 --------------------------#
[32m[20221213 23:30:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:27 @agent_ppo2.py:185][0m |           0.0136 |          67.6901 |          11.2204 |
[32m[20221213 23:30:27 @agent_ppo2.py:185][0m |          -0.0041 |          57.1445 |          11.3596 |
[32m[20221213 23:30:27 @agent_ppo2.py:185][0m |          -0.0076 |          54.1773 |          11.3906 |
[32m[20221213 23:30:27 @agent_ppo2.py:185][0m |          -0.0088 |          52.6857 |          11.3706 |
[32m[20221213 23:30:27 @agent_ppo2.py:185][0m |          -0.0128 |          51.7609 |          11.4065 |
[32m[20221213 23:30:27 @agent_ppo2.py:185][0m |          -0.0106 |          51.0681 |          11.4327 |
[32m[20221213 23:30:28 @agent_ppo2.py:185][0m |          -0.0119 |          49.9544 |          11.4263 |
[32m[20221213 23:30:28 @agent_ppo2.py:185][0m |          -0.0083 |          49.6200 |          11.4409 |
[32m[20221213 23:30:28 @agent_ppo2.py:185][0m |          -0.0097 |          48.9030 |          11.5018 |
[32m[20221213 23:30:28 @agent_ppo2.py:185][0m |          -0.0128 |          48.3689 |          11.5100 |
[32m[20221213 23:30:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.18
[32m[20221213 23:30:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.83
[32m[20221213 23:30:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.03
[32m[20221213 23:30:28 @agent_ppo2.py:143][0m Total time:      17.93 min
[32m[20221213 23:30:28 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221213 23:30:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2846 --------------------------#
[32m[20221213 23:30:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:28 @agent_ppo2.py:185][0m |           0.0035 |          65.0404 |          11.3317 |
[32m[20221213 23:30:28 @agent_ppo2.py:185][0m |          -0.0070 |          63.1536 |          11.3181 |
[32m[20221213 23:30:28 @agent_ppo2.py:185][0m |          -0.0021 |          63.4882 |          11.3729 |
[32m[20221213 23:30:29 @agent_ppo2.py:185][0m |          -0.0040 |          63.0985 |          11.3937 |
[32m[20221213 23:30:29 @agent_ppo2.py:185][0m |          -0.0084 |          62.0211 |          11.3930 |
[32m[20221213 23:30:29 @agent_ppo2.py:185][0m |          -0.0094 |          61.9815 |          11.4215 |
[32m[20221213 23:30:29 @agent_ppo2.py:185][0m |          -0.0088 |          61.6320 |          11.4518 |
[32m[20221213 23:30:29 @agent_ppo2.py:185][0m |          -0.0139 |          61.5388 |          11.4574 |
[32m[20221213 23:30:29 @agent_ppo2.py:185][0m |          -0.0140 |          61.2680 |          11.4535 |
[32m[20221213 23:30:29 @agent_ppo2.py:185][0m |          -0.0073 |          62.1247 |          11.4652 |
[32m[20221213 23:30:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:30:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.28
[32m[20221213 23:30:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.55
[32m[20221213 23:30:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.35
[32m[20221213 23:30:29 @agent_ppo2.py:143][0m Total time:      17.95 min
[32m[20221213 23:30:29 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221213 23:30:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2847 --------------------------#
[32m[20221213 23:30:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |           0.0033 |          60.0041 |          11.1774 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0069 |          51.0141 |          11.1357 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0089 |          47.7648 |          11.1734 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0013 |          47.4039 |          11.1526 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0073 |          44.5781 |          11.1662 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0113 |          43.6991 |          11.1481 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0061 |          45.4396 |          11.1568 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0134 |          42.8699 |          11.1273 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0153 |          42.4291 |          11.1038 |
[32m[20221213 23:30:30 @agent_ppo2.py:185][0m |          -0.0091 |          42.2719 |          11.1359 |
[32m[20221213 23:30:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.17
[32m[20221213 23:30:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.14
[32m[20221213 23:30:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.75
[32m[20221213 23:30:30 @agent_ppo2.py:143][0m Total time:      17.97 min
[32m[20221213 23:30:30 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221213 23:30:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2848 --------------------------#
[32m[20221213 23:30:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0012 |          37.2445 |          11.1558 |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0111 |          30.2280 |          11.1541 |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0105 |          28.2352 |          11.1215 |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0069 |          27.3546 |          11.1066 |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0139 |          26.7011 |          11.0940 |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0159 |          26.1747 |          11.0595 |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0134 |          25.7228 |          11.0367 |
[32m[20221213 23:30:31 @agent_ppo2.py:185][0m |          -0.0175 |          25.4821 |          11.0471 |
[32m[20221213 23:30:32 @agent_ppo2.py:185][0m |          -0.0149 |          25.1597 |          11.0362 |
[32m[20221213 23:30:32 @agent_ppo2.py:185][0m |          -0.0168 |          24.8551 |          11.0138 |
[32m[20221213 23:30:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.66
[32m[20221213 23:30:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.49
[32m[20221213 23:30:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.48
[32m[20221213 23:30:32 @agent_ppo2.py:143][0m Total time:      18.00 min
[32m[20221213 23:30:32 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221213 23:30:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2849 --------------------------#
[32m[20221213 23:30:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:32 @agent_ppo2.py:185][0m |           0.0004 |          42.6045 |          11.2593 |
[32m[20221213 23:30:32 @agent_ppo2.py:185][0m |          -0.0092 |          36.9330 |          11.1545 |
[32m[20221213 23:30:32 @agent_ppo2.py:185][0m |          -0.0120 |          34.3532 |          11.1486 |
[32m[20221213 23:30:32 @agent_ppo2.py:185][0m |          -0.0078 |          32.5647 |          11.1568 |
[32m[20221213 23:30:32 @agent_ppo2.py:185][0m |          -0.0152 |          31.0381 |          11.1851 |
[32m[20221213 23:30:33 @agent_ppo2.py:185][0m |          -0.0131 |          29.9858 |          11.1692 |
[32m[20221213 23:30:33 @agent_ppo2.py:185][0m |          -0.0118 |          29.4465 |          11.1859 |
[32m[20221213 23:30:33 @agent_ppo2.py:185][0m |          -0.0072 |          29.8364 |          11.2180 |
[32m[20221213 23:30:33 @agent_ppo2.py:185][0m |          -0.0110 |          28.1344 |          11.2007 |
[32m[20221213 23:30:33 @agent_ppo2.py:185][0m |          -0.0113 |          27.7791 |          11.2081 |
[32m[20221213 23:30:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:30:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.75
[32m[20221213 23:30:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.94
[32m[20221213 23:30:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.85
[32m[20221213 23:30:33 @agent_ppo2.py:143][0m Total time:      18.02 min
[32m[20221213 23:30:33 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221213 23:30:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2850 --------------------------#
[32m[20221213 23:30:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:30:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:33 @agent_ppo2.py:185][0m |          -0.0035 |          58.3336 |          11.1870 |
[32m[20221213 23:30:33 @agent_ppo2.py:185][0m |          -0.0031 |          54.9511 |          11.1764 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0063 |          53.8348 |          11.2291 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0094 |          53.1794 |          11.1692 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0104 |          52.6136 |          11.2185 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0114 |          52.2805 |          11.2219 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0132 |          51.9780 |          11.2083 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0133 |          51.7611 |          11.2094 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0121 |          51.5247 |          11.2113 |
[32m[20221213 23:30:34 @agent_ppo2.py:185][0m |          -0.0103 |          51.5251 |          11.2096 |
[32m[20221213 23:30:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.02
[32m[20221213 23:30:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.64
[32m[20221213 23:30:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.01
[32m[20221213 23:30:34 @agent_ppo2.py:143][0m Total time:      18.04 min
[32m[20221213 23:30:34 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221213 23:30:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2851 --------------------------#
[32m[20221213 23:30:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |           0.0022 |          55.6727 |          11.1043 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0086 |          50.3808 |          11.1114 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0092 |          48.5463 |          11.0833 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0011 |          49.6167 |          11.0547 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0071 |          46.6801 |          11.0149 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0160 |          45.6973 |          10.9606 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0121 |          45.5686 |          11.0159 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0175 |          44.9491 |          10.9787 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |           0.0015 |          53.1817 |          10.9714 |
[32m[20221213 23:30:35 @agent_ppo2.py:185][0m |          -0.0138 |          44.5301 |          10.9737 |
[32m[20221213 23:30:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.40
[32m[20221213 23:30:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.58
[32m[20221213 23:30:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.03
[32m[20221213 23:30:36 @agent_ppo2.py:143][0m Total time:      18.06 min
[32m[20221213 23:30:36 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221213 23:30:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2852 --------------------------#
[32m[20221213 23:30:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |           0.0049 |          26.1990 |          10.9183 |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |          -0.0013 |          20.6673 |          10.9105 |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |          -0.0092 |          18.8195 |          10.9483 |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |          -0.0076 |          17.6843 |          10.9167 |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |          -0.0109 |          17.0005 |          10.9147 |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |          -0.0101 |          16.4291 |          10.9369 |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |          -0.0127 |          15.9630 |          10.9755 |
[32m[20221213 23:30:36 @agent_ppo2.py:185][0m |          -0.0155 |          15.6279 |          10.9287 |
[32m[20221213 23:30:37 @agent_ppo2.py:185][0m |          -0.0198 |          15.4068 |          10.9745 |
[32m[20221213 23:30:37 @agent_ppo2.py:185][0m |          -0.0098 |          15.1940 |          10.9481 |
[32m[20221213 23:30:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.00
[32m[20221213 23:30:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.95
[32m[20221213 23:30:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.46
[32m[20221213 23:30:37 @agent_ppo2.py:143][0m Total time:      18.08 min
[32m[20221213 23:30:37 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221213 23:30:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2853 --------------------------#
[32m[20221213 23:30:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:30:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:37 @agent_ppo2.py:185][0m |           0.0033 |          26.3535 |          11.1310 |
[32m[20221213 23:30:37 @agent_ppo2.py:185][0m |          -0.0062 |          23.3518 |          11.1087 |
[32m[20221213 23:30:37 @agent_ppo2.py:185][0m |          -0.0002 |          23.3286 |          11.0912 |
[32m[20221213 23:30:37 @agent_ppo2.py:185][0m |          -0.0020 |          23.1537 |          11.0547 |
[32m[20221213 23:30:37 @agent_ppo2.py:185][0m |          -0.0035 |          23.2458 |          11.0589 |
[32m[20221213 23:30:38 @agent_ppo2.py:185][0m |           0.0025 |          23.2485 |          11.0234 |
[32m[20221213 23:30:38 @agent_ppo2.py:185][0m |          -0.0056 |          23.0078 |          11.0196 |
[32m[20221213 23:30:38 @agent_ppo2.py:185][0m |          -0.0028 |          23.1019 |          11.0098 |
[32m[20221213 23:30:38 @agent_ppo2.py:185][0m |          -0.0048 |          22.9383 |          11.0140 |
[32m[20221213 23:30:38 @agent_ppo2.py:185][0m |          -0.0057 |          22.9352 |          10.9735 |
[32m[20221213 23:30:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:30:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:30:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:30:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.85
[32m[20221213 23:30:38 @agent_ppo2.py:143][0m Total time:      18.10 min
[32m[20221213 23:30:38 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221213 23:30:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2854 --------------------------#
[32m[20221213 23:30:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:38 @agent_ppo2.py:185][0m |           0.0016 |          48.5990 |          10.8430 |
[32m[20221213 23:30:38 @agent_ppo2.py:185][0m |          -0.0116 |          44.9781 |          10.8838 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |           0.0005 |          46.6502 |          10.8567 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |          -0.0103 |          43.4451 |          10.8571 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |          -0.0112 |          42.7947 |          10.8472 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |          -0.0122 |          41.8021 |          10.8803 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |          -0.0156 |          41.3388 |          10.8966 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |          -0.0139 |          40.9979 |          10.9138 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |          -0.0190 |          40.6074 |          10.9090 |
[32m[20221213 23:30:39 @agent_ppo2.py:185][0m |          -0.0165 |          40.4932 |          10.9120 |
[32m[20221213 23:30:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:30:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.20
[32m[20221213 23:30:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.06
[32m[20221213 23:30:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.30
[32m[20221213 23:30:39 @agent_ppo2.py:143][0m Total time:      18.12 min
[32m[20221213 23:30:39 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221213 23:30:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2855 --------------------------#
[32m[20221213 23:30:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |           0.0036 |          20.4833 |          10.6737 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0115 |          16.1980 |          10.7235 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0053 |          14.8129 |          10.6971 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0077 |          13.9620 |          10.7014 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0117 |          13.3212 |          10.6474 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0084 |          12.9051 |          10.5800 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0111 |          12.4377 |          10.5725 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0169 |          12.1355 |          10.6105 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0242 |          11.9254 |          10.5984 |
[32m[20221213 23:30:40 @agent_ppo2.py:185][0m |          -0.0348 |          11.6730 |          10.5663 |
[32m[20221213 23:30:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.69
[32m[20221213 23:30:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.78
[32m[20221213 23:30:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.09
[32m[20221213 23:30:41 @agent_ppo2.py:143][0m Total time:      18.14 min
[32m[20221213 23:30:41 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221213 23:30:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2856 --------------------------#
[32m[20221213 23:30:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:41 @agent_ppo2.py:185][0m |          -0.0015 |          59.1981 |          10.7522 |
[32m[20221213 23:30:41 @agent_ppo2.py:185][0m |          -0.0018 |          55.8007 |          10.7258 |
[32m[20221213 23:30:41 @agent_ppo2.py:185][0m |          -0.0016 |          55.0944 |          10.7445 |
[32m[20221213 23:30:41 @agent_ppo2.py:185][0m |          -0.0008 |          53.9652 |          10.7447 |
[32m[20221213 23:30:41 @agent_ppo2.py:185][0m |          -0.0060 |          53.0864 |          10.7392 |
[32m[20221213 23:30:41 @agent_ppo2.py:185][0m |          -0.0071 |          52.4004 |          10.7784 |
[32m[20221213 23:30:41 @agent_ppo2.py:185][0m |          -0.0075 |          51.7007 |          10.7435 |
[32m[20221213 23:30:42 @agent_ppo2.py:185][0m |          -0.0102 |          51.5465 |          10.7505 |
[32m[20221213 23:30:42 @agent_ppo2.py:185][0m |          -0.0143 |          51.4348 |          10.7755 |
[32m[20221213 23:30:42 @agent_ppo2.py:185][0m |          -0.0110 |          51.0881 |          10.8010 |
[32m[20221213 23:30:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.80
[32m[20221213 23:30:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.41
[32m[20221213 23:30:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.22
[32m[20221213 23:30:42 @agent_ppo2.py:143][0m Total time:      18.16 min
[32m[20221213 23:30:42 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221213 23:30:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2857 --------------------------#
[32m[20221213 23:30:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:42 @agent_ppo2.py:185][0m |          -0.0014 |          52.7008 |          10.4497 |
[32m[20221213 23:30:42 @agent_ppo2.py:185][0m |           0.0119 |          55.9663 |          10.3969 |
[32m[20221213 23:30:42 @agent_ppo2.py:185][0m |          -0.0084 |          47.6845 |          10.3699 |
[32m[20221213 23:30:42 @agent_ppo2.py:185][0m |          -0.0087 |          46.8366 |          10.3845 |
[32m[20221213 23:30:43 @agent_ppo2.py:185][0m |          -0.0144 |          46.5209 |          10.3444 |
[32m[20221213 23:30:43 @agent_ppo2.py:185][0m |           0.0040 |          49.6977 |          10.3441 |
[32m[20221213 23:30:43 @agent_ppo2.py:185][0m |          -0.0139 |          46.0578 |          10.2770 |
[32m[20221213 23:30:43 @agent_ppo2.py:185][0m |          -0.0127 |          45.6502 |          10.3178 |
[32m[20221213 23:30:43 @agent_ppo2.py:185][0m |          -0.0117 |          45.4431 |          10.3384 |
[32m[20221213 23:30:43 @agent_ppo2.py:185][0m |          -0.0182 |          45.2264 |          10.3327 |
[32m[20221213 23:30:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.38
[32m[20221213 23:30:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.01
[32m[20221213 23:30:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.39
[32m[20221213 23:30:43 @agent_ppo2.py:143][0m Total time:      18.19 min
[32m[20221213 23:30:43 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221213 23:30:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2858 --------------------------#
[32m[20221213 23:30:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:43 @agent_ppo2.py:185][0m |           0.0018 |          53.0897 |          10.4492 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0033 |          46.9835 |          10.4472 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0067 |          45.2362 |          10.4691 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0125 |          44.5906 |          10.4562 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0053 |          44.0628 |          10.4573 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0102 |          43.9963 |          10.4117 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0125 |          43.1568 |          10.4937 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0129 |          43.0380 |          10.4274 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0114 |          42.9704 |          10.4696 |
[32m[20221213 23:30:44 @agent_ppo2.py:185][0m |          -0.0097 |          42.8816 |          10.4314 |
[32m[20221213 23:30:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.12
[32m[20221213 23:30:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.15
[32m[20221213 23:30:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.25
[32m[20221213 23:30:44 @agent_ppo2.py:143][0m Total time:      18.21 min
[32m[20221213 23:30:44 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221213 23:30:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2859 --------------------------#
[32m[20221213 23:30:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0014 |          47.5939 |          10.4031 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0036 |          42.2269 |          10.3793 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0078 |          40.6668 |          10.3844 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0105 |          39.7937 |          10.4130 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0132 |          39.3786 |          10.4046 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0160 |          38.9942 |          10.4333 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0085 |          39.1477 |          10.4018 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0168 |          38.4393 |          10.4035 |
[32m[20221213 23:30:45 @agent_ppo2.py:185][0m |          -0.0178 |          38.2302 |          10.4504 |
[32m[20221213 23:30:46 @agent_ppo2.py:185][0m |          -0.0175 |          37.9866 |          10.4493 |
[32m[20221213 23:30:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:30:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.96
[32m[20221213 23:30:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.17
[32m[20221213 23:30:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.64
[32m[20221213 23:30:46 @agent_ppo2.py:143][0m Total time:      18.23 min
[32m[20221213 23:30:46 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221213 23:30:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2860 --------------------------#
[32m[20221213 23:30:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:30:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:46 @agent_ppo2.py:185][0m |          -0.0002 |          42.8609 |          10.7082 |
[32m[20221213 23:30:46 @agent_ppo2.py:185][0m |          -0.0097 |          37.8720 |          10.7445 |
[32m[20221213 23:30:46 @agent_ppo2.py:185][0m |          -0.0099 |          36.9467 |          10.8169 |
[32m[20221213 23:30:46 @agent_ppo2.py:185][0m |          -0.0131 |          35.9746 |          10.8297 |
[32m[20221213 23:30:46 @agent_ppo2.py:185][0m |          -0.0141 |          35.3293 |          10.8091 |
[32m[20221213 23:30:46 @agent_ppo2.py:185][0m |          -0.0145 |          34.9418 |          10.8703 |
[32m[20221213 23:30:47 @agent_ppo2.py:185][0m |          -0.0117 |          34.4690 |          10.8779 |
[32m[20221213 23:30:47 @agent_ppo2.py:185][0m |          -0.0148 |          34.0134 |          10.8943 |
[32m[20221213 23:30:47 @agent_ppo2.py:185][0m |          -0.0122 |          33.7540 |          10.8750 |
[32m[20221213 23:30:47 @agent_ppo2.py:185][0m |          -0.0171 |          33.4282 |          10.9121 |
[32m[20221213 23:30:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.92
[32m[20221213 23:30:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.45
[32m[20221213 23:30:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.78
[32m[20221213 23:30:47 @agent_ppo2.py:143][0m Total time:      18.25 min
[32m[20221213 23:30:47 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221213 23:30:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2861 --------------------------#
[32m[20221213 23:30:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:47 @agent_ppo2.py:185][0m |           0.0007 |          55.0137 |          11.2969 |
[32m[20221213 23:30:47 @agent_ppo2.py:185][0m |          -0.0039 |          49.6789 |          11.3563 |
[32m[20221213 23:30:47 @agent_ppo2.py:185][0m |          -0.0063 |          47.8778 |          11.3342 |
[32m[20221213 23:30:48 @agent_ppo2.py:185][0m |          -0.0075 |          47.2231 |          11.3674 |
[32m[20221213 23:30:48 @agent_ppo2.py:185][0m |          -0.0100 |          46.6296 |          11.3115 |
[32m[20221213 23:30:48 @agent_ppo2.py:185][0m |          -0.0055 |          46.2030 |          11.3235 |
[32m[20221213 23:30:48 @agent_ppo2.py:185][0m |          -0.0115 |          45.8157 |          11.3723 |
[32m[20221213 23:30:48 @agent_ppo2.py:185][0m |          -0.0117 |          45.3298 |          11.3188 |
[32m[20221213 23:30:48 @agent_ppo2.py:185][0m |          -0.0147 |          44.9346 |          11.3406 |
[32m[20221213 23:30:48 @agent_ppo2.py:185][0m |          -0.0032 |          46.8571 |          11.3526 |
[32m[20221213 23:30:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.23
[32m[20221213 23:30:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.80
[32m[20221213 23:30:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.86
[32m[20221213 23:30:48 @agent_ppo2.py:143][0m Total time:      18.27 min
[32m[20221213 23:30:48 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221213 23:30:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2862 --------------------------#
[32m[20221213 23:30:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |           0.0130 |          60.5531 |          10.8964 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |           0.0149 |          57.9320 |          11.0039 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0069 |          50.9049 |          11.0229 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0011 |          50.0836 |          10.9736 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0053 |          49.8702 |          10.9709 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0016 |          49.4359 |          10.9603 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0028 |          49.3034 |          10.9591 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0032 |          49.3411 |          10.9576 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0086 |          48.8641 |          10.9997 |
[32m[20221213 23:30:49 @agent_ppo2.py:185][0m |          -0.0076 |          48.7456 |          10.9262 |
[32m[20221213 23:30:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.12
[32m[20221213 23:30:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.61
[32m[20221213 23:30:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.22
[32m[20221213 23:30:49 @agent_ppo2.py:143][0m Total time:      18.29 min
[32m[20221213 23:30:49 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221213 23:30:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2863 --------------------------#
[32m[20221213 23:30:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |           0.0005 |          59.6688 |          11.1782 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |          -0.0071 |          58.6201 |          11.1306 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |           0.0049 |          62.2574 |          11.1308 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |          -0.0117 |          57.7714 |          11.1197 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |          -0.0125 |          57.2932 |          11.1126 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |          -0.0131 |          56.6878 |          11.1434 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |          -0.0122 |          56.5200 |          11.0849 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |          -0.0136 |          56.1740 |          11.1187 |
[32m[20221213 23:30:50 @agent_ppo2.py:185][0m |          -0.0136 |          56.3547 |          11.1056 |
[32m[20221213 23:30:51 @agent_ppo2.py:185][0m |          -0.0143 |          56.1495 |          11.1145 |
[32m[20221213 23:30:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:30:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.98
[32m[20221213 23:30:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.58
[32m[20221213 23:30:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.95
[32m[20221213 23:30:51 @agent_ppo2.py:143][0m Total time:      18.31 min
[32m[20221213 23:30:51 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221213 23:30:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2864 --------------------------#
[32m[20221213 23:30:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:30:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:51 @agent_ppo2.py:185][0m |          -0.0010 |          56.5185 |          10.9477 |
[32m[20221213 23:30:51 @agent_ppo2.py:185][0m |          -0.0021 |          54.4353 |          11.0028 |
[32m[20221213 23:30:51 @agent_ppo2.py:185][0m |          -0.0055 |          53.8704 |          11.0684 |
[32m[20221213 23:30:51 @agent_ppo2.py:185][0m |          -0.0056 |          53.4558 |          10.9222 |
[32m[20221213 23:30:51 @agent_ppo2.py:185][0m |          -0.0044 |          53.3393 |          10.9778 |
[32m[20221213 23:30:51 @agent_ppo2.py:185][0m |          -0.0068 |          53.0294 |          10.9817 |
[32m[20221213 23:30:52 @agent_ppo2.py:185][0m |          -0.0078 |          52.6774 |          10.9527 |
[32m[20221213 23:30:52 @agent_ppo2.py:185][0m |          -0.0111 |          52.4057 |          10.9482 |
[32m[20221213 23:30:52 @agent_ppo2.py:185][0m |          -0.0095 |          52.2436 |          10.9589 |
[32m[20221213 23:30:52 @agent_ppo2.py:185][0m |          -0.0108 |          52.1248 |          10.9628 |
[32m[20221213 23:30:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.41
[32m[20221213 23:30:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.99
[32m[20221213 23:30:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.71
[32m[20221213 23:30:52 @agent_ppo2.py:143][0m Total time:      18.33 min
[32m[20221213 23:30:52 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221213 23:30:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2865 --------------------------#
[32m[20221213 23:30:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:52 @agent_ppo2.py:185][0m |           0.0005 |          54.4433 |          10.7869 |
[32m[20221213 23:30:52 @agent_ppo2.py:185][0m |          -0.0074 |          50.1306 |          10.8037 |
[32m[20221213 23:30:52 @agent_ppo2.py:185][0m |          -0.0025 |          49.9086 |          10.8221 |
[32m[20221213 23:30:53 @agent_ppo2.py:185][0m |          -0.0088 |          46.5665 |          10.7859 |
[32m[20221213 23:30:53 @agent_ppo2.py:185][0m |          -0.0101 |          45.5523 |          10.8194 |
[32m[20221213 23:30:53 @agent_ppo2.py:185][0m |          -0.0107 |          44.9416 |          10.8223 |
[32m[20221213 23:30:53 @agent_ppo2.py:185][0m |          -0.0114 |          44.4931 |          10.8221 |
[32m[20221213 23:30:53 @agent_ppo2.py:185][0m |           0.0011 |          44.9291 |          10.8669 |
[32m[20221213 23:30:53 @agent_ppo2.py:185][0m |          -0.0113 |          43.8959 |          10.8460 |
[32m[20221213 23:30:53 @agent_ppo2.py:185][0m |          -0.0117 |          43.6785 |          10.8562 |
[32m[20221213 23:30:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.60
[32m[20221213 23:30:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.70
[32m[20221213 23:30:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.62
[32m[20221213 23:30:53 @agent_ppo2.py:143][0m Total time:      18.35 min
[32m[20221213 23:30:53 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221213 23:30:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2866 --------------------------#
[32m[20221213 23:30:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |           0.0047 |          59.4342 |          11.0003 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0039 |          57.7070 |          11.0029 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0078 |          57.3164 |          10.9833 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0088 |          56.9840 |          11.0018 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0085 |          56.8906 |          11.0072 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0095 |          56.7084 |          10.9495 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0107 |          56.6168 |          10.9599 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0105 |          56.6300 |          10.9465 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0106 |          56.4017 |          10.9498 |
[32m[20221213 23:30:54 @agent_ppo2.py:185][0m |          -0.0083 |          57.3313 |          10.9798 |
[32m[20221213 23:30:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.45
[32m[20221213 23:30:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.41
[32m[20221213 23:30:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.31
[32m[20221213 23:30:54 @agent_ppo2.py:143][0m Total time:      18.37 min
[32m[20221213 23:30:54 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221213 23:30:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2867 --------------------------#
[32m[20221213 23:30:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |           0.0018 |          60.8591 |          10.6471 |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |          -0.0011 |          60.1915 |          10.6153 |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |          -0.0054 |          59.7695 |          10.5343 |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |          -0.0048 |          59.5314 |          10.5373 |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |          -0.0078 |          59.5078 |          10.5278 |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |          -0.0079 |          59.3655 |          10.4877 |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |          -0.0094 |          59.0941 |          10.3917 |
[32m[20221213 23:30:55 @agent_ppo2.py:185][0m |          -0.0111 |          59.0138 |          10.4149 |
[32m[20221213 23:30:56 @agent_ppo2.py:185][0m |          -0.0040 |          60.5830 |          10.3991 |
[32m[20221213 23:30:56 @agent_ppo2.py:185][0m |          -0.0131 |          58.8737 |          10.3917 |
[32m[20221213 23:30:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:30:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.69
[32m[20221213 23:30:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.49
[32m[20221213 23:30:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.67
[32m[20221213 23:30:56 @agent_ppo2.py:143][0m Total time:      18.40 min
[32m[20221213 23:30:56 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221213 23:30:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2868 --------------------------#
[32m[20221213 23:30:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:56 @agent_ppo2.py:185][0m |          -0.0044 |          69.7968 |          10.2915 |
[32m[20221213 23:30:56 @agent_ppo2.py:185][0m |          -0.0031 |          66.1798 |          10.2975 |
[32m[20221213 23:30:56 @agent_ppo2.py:185][0m |          -0.0063 |          65.2594 |          10.3175 |
[32m[20221213 23:30:56 @agent_ppo2.py:185][0m |           0.0037 |          68.2476 |          10.3123 |
[32m[20221213 23:30:56 @agent_ppo2.py:185][0m |          -0.0082 |          63.9019 |          10.3320 |
[32m[20221213 23:30:57 @agent_ppo2.py:185][0m |          -0.0125 |          63.2066 |          10.3486 |
[32m[20221213 23:30:57 @agent_ppo2.py:185][0m |           0.0011 |          67.7076 |          10.3517 |
[32m[20221213 23:30:57 @agent_ppo2.py:185][0m |          -0.0097 |          62.9532 |          10.3954 |
[32m[20221213 23:30:57 @agent_ppo2.py:185][0m |          -0.0090 |          62.8054 |          10.4275 |
[32m[20221213 23:30:57 @agent_ppo2.py:185][0m |          -0.0155 |          62.3369 |          10.3714 |
[32m[20221213 23:30:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:30:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.26
[32m[20221213 23:30:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.10
[32m[20221213 23:30:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.16
[32m[20221213 23:30:57 @agent_ppo2.py:143][0m Total time:      18.42 min
[32m[20221213 23:30:57 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221213 23:30:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2869 --------------------------#
[32m[20221213 23:30:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:30:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:57 @agent_ppo2.py:185][0m |           0.0024 |          42.1919 |          10.4301 |
[32m[20221213 23:30:57 @agent_ppo2.py:185][0m |          -0.0039 |          37.5521 |          10.4710 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0089 |          36.1968 |          10.4803 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0084 |          35.0015 |          10.4386 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0096 |          34.3343 |          10.4561 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0095 |          33.9482 |          10.4898 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0061 |          35.1991 |          10.4320 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0152 |          32.8267 |          10.4204 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0141 |          32.5981 |          10.4853 |
[32m[20221213 23:30:58 @agent_ppo2.py:185][0m |          -0.0130 |          32.2754 |          10.4891 |
[32m[20221213 23:30:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:30:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.47
[32m[20221213 23:30:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.58
[32m[20221213 23:30:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.01
[32m[20221213 23:30:58 @agent_ppo2.py:143][0m Total time:      18.44 min
[32m[20221213 23:30:58 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221213 23:30:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2870 --------------------------#
[32m[20221213 23:30:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:30:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0003 |          61.3736 |          10.4847 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0064 |          58.8050 |          10.4694 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0083 |          57.9794 |          10.4582 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0104 |          57.7163 |          10.4992 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0039 |          57.9731 |          10.4535 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0069 |          59.0920 |          10.4591 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0116 |          56.6779 |          10.4643 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0128 |          56.5314 |          10.4447 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0123 |          56.3722 |          10.4216 |
[32m[20221213 23:30:59 @agent_ppo2.py:185][0m |          -0.0127 |          56.3569 |          10.4749 |
[32m[20221213 23:30:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.05
[32m[20221213 23:31:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.47
[32m[20221213 23:31:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.43
[32m[20221213 23:31:00 @agent_ppo2.py:143][0m Total time:      18.46 min
[32m[20221213 23:31:00 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221213 23:31:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2871 --------------------------#
[32m[20221213 23:31:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |           0.0030 |          58.8635 |          10.6513 |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |          -0.0072 |          55.0831 |          10.6896 |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |          -0.0075 |          53.7652 |          10.7165 |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |          -0.0129 |          52.7092 |          10.7541 |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |          -0.0113 |          52.2298 |          10.7508 |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |          -0.0135 |          51.5054 |          10.7597 |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |          -0.0131 |          51.1877 |          10.7873 |
[32m[20221213 23:31:00 @agent_ppo2.py:185][0m |          -0.0137 |          50.9324 |          10.7830 |
[32m[20221213 23:31:01 @agent_ppo2.py:185][0m |          -0.0157 |          50.6216 |          10.7962 |
[32m[20221213 23:31:01 @agent_ppo2.py:185][0m |          -0.0081 |          52.1402 |          10.8478 |
[32m[20221213 23:31:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.98
[32m[20221213 23:31:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.57
[32m[20221213 23:31:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.96
[32m[20221213 23:31:01 @agent_ppo2.py:143][0m Total time:      18.48 min
[32m[20221213 23:31:01 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221213 23:31:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2872 --------------------------#
[32m[20221213 23:31:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:01 @agent_ppo2.py:185][0m |           0.0095 |          62.2898 |          10.6152 |
[32m[20221213 23:31:01 @agent_ppo2.py:185][0m |          -0.0095 |          55.7674 |          10.6072 |
[32m[20221213 23:31:01 @agent_ppo2.py:185][0m |          -0.0074 |          54.1829 |          10.6292 |
[32m[20221213 23:31:01 @agent_ppo2.py:185][0m |          -0.0094 |          53.8819 |          10.6775 |
[32m[20221213 23:31:01 @agent_ppo2.py:185][0m |          -0.0058 |          53.0284 |          10.6997 |
[32m[20221213 23:31:02 @agent_ppo2.py:185][0m |          -0.0123 |          52.4004 |          10.6810 |
[32m[20221213 23:31:02 @agent_ppo2.py:185][0m |          -0.0092 |          51.9739 |          10.6964 |
[32m[20221213 23:31:02 @agent_ppo2.py:185][0m |          -0.0124 |          51.7000 |          10.7452 |
[32m[20221213 23:31:02 @agent_ppo2.py:185][0m |          -0.0116 |          51.4002 |          10.7391 |
[32m[20221213 23:31:02 @agent_ppo2.py:185][0m |          -0.0058 |          53.6883 |          10.7266 |
[32m[20221213 23:31:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.11
[32m[20221213 23:31:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.06
[32m[20221213 23:31:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.66
[32m[20221213 23:31:02 @agent_ppo2.py:143][0m Total time:      18.50 min
[32m[20221213 23:31:02 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221213 23:31:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2873 --------------------------#
[32m[20221213 23:31:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:02 @agent_ppo2.py:185][0m |           0.0013 |          54.0298 |          10.9311 |
[32m[20221213 23:31:02 @agent_ppo2.py:185][0m |           0.0043 |          49.6238 |          10.8621 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0045 |          46.8648 |          10.8951 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0079 |          45.6763 |          10.8532 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0106 |          44.9451 |          10.8427 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0148 |          44.1630 |          10.8679 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0158 |          43.6121 |          10.8404 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0158 |          43.1756 |          10.8136 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0121 |          42.5777 |          10.8181 |
[32m[20221213 23:31:03 @agent_ppo2.py:185][0m |          -0.0159 |          42.3529 |          10.8433 |
[32m[20221213 23:31:03 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:31:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.49
[32m[20221213 23:31:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.55
[32m[20221213 23:31:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.59
[32m[20221213 23:31:03 @agent_ppo2.py:143][0m Total time:      18.52 min
[32m[20221213 23:31:03 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221213 23:31:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2874 --------------------------#
[32m[20221213 23:31:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |           0.0015 |          57.5355 |          10.9232 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0093 |          54.1320 |          10.8476 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0126 |          53.0806 |          10.8238 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0119 |          52.2745 |          10.7940 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0109 |          51.6290 |          10.8122 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0039 |          52.0142 |          10.7578 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0025 |          52.0214 |          10.7296 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0136 |          49.8239 |          10.7711 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0110 |          49.4115 |          10.7763 |
[32m[20221213 23:31:04 @agent_ppo2.py:185][0m |          -0.0147 |          48.7211 |          10.7536 |
[32m[20221213 23:31:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.07
[32m[20221213 23:31:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.38
[32m[20221213 23:31:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.03
[32m[20221213 23:31:05 @agent_ppo2.py:143][0m Total time:      18.54 min
[32m[20221213 23:31:05 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221213 23:31:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2875 --------------------------#
[32m[20221213 23:31:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:05 @agent_ppo2.py:185][0m |          -0.0007 |          49.2755 |          10.6315 |
[32m[20221213 23:31:05 @agent_ppo2.py:185][0m |          -0.0008 |          45.0478 |          10.6418 |
[32m[20221213 23:31:05 @agent_ppo2.py:185][0m |          -0.0122 |          42.5158 |          10.6454 |
[32m[20221213 23:31:05 @agent_ppo2.py:185][0m |          -0.0131 |          41.5375 |          10.6474 |
[32m[20221213 23:31:05 @agent_ppo2.py:185][0m |          -0.0022 |          46.5880 |          10.6425 |
[32m[20221213 23:31:05 @agent_ppo2.py:185][0m |          -0.0095 |          40.5529 |          10.6971 |
[32m[20221213 23:31:05 @agent_ppo2.py:185][0m |          -0.0139 |          40.1500 |          10.6682 |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |          -0.0115 |          39.7620 |          10.6578 |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |          -0.0201 |          39.5841 |          10.6901 |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |          -0.0159 |          39.3402 |          10.6763 |
[32m[20221213 23:31:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.68
[32m[20221213 23:31:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.15
[32m[20221213 23:31:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.71
[32m[20221213 23:31:06 @agent_ppo2.py:143][0m Total time:      18.56 min
[32m[20221213 23:31:06 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221213 23:31:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2876 --------------------------#
[32m[20221213 23:31:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |          -0.0001 |          69.6684 |          10.4636 |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |          -0.0056 |          63.8837 |          10.4490 |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |           0.0028 |          67.2327 |          10.4589 |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |          -0.0086 |          61.5548 |          10.4943 |
[32m[20221213 23:31:06 @agent_ppo2.py:185][0m |          -0.0081 |          60.9038 |          10.4858 |
[32m[20221213 23:31:07 @agent_ppo2.py:185][0m |          -0.0088 |          61.1866 |          10.5141 |
[32m[20221213 23:31:07 @agent_ppo2.py:185][0m |          -0.0120 |          60.1901 |          10.5270 |
[32m[20221213 23:31:07 @agent_ppo2.py:185][0m |          -0.0021 |          63.6438 |          10.5387 |
[32m[20221213 23:31:07 @agent_ppo2.py:185][0m |          -0.0115 |          59.6105 |          10.5254 |
[32m[20221213 23:31:07 @agent_ppo2.py:185][0m |          -0.0120 |          59.1030 |          10.5416 |
[32m[20221213 23:31:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.31
[32m[20221213 23:31:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.11
[32m[20221213 23:31:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.46
[32m[20221213 23:31:07 @agent_ppo2.py:143][0m Total time:      18.58 min
[32m[20221213 23:31:07 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221213 23:31:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2877 --------------------------#
[32m[20221213 23:31:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:07 @agent_ppo2.py:185][0m |           0.0030 |          44.8906 |          10.8174 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0033 |          41.3918 |          10.8221 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0081 |          40.4718 |          10.8643 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0076 |          39.9715 |          10.8654 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0126 |          39.6041 |          10.8827 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0086 |          39.1735 |          10.8742 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0148 |          38.9232 |          10.8791 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0140 |          38.7636 |          10.9096 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0151 |          38.3104 |          10.9112 |
[32m[20221213 23:31:08 @agent_ppo2.py:185][0m |          -0.0145 |          38.1782 |          10.9075 |
[32m[20221213 23:31:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.20
[32m[20221213 23:31:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.00
[32m[20221213 23:31:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.20
[32m[20221213 23:31:08 @agent_ppo2.py:143][0m Total time:      18.61 min
[32m[20221213 23:31:08 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221213 23:31:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2878 --------------------------#
[32m[20221213 23:31:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |           0.0034 |          49.4697 |          10.7821 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0022 |          45.2640 |          10.8432 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0044 |          43.4841 |          10.8879 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0076 |          42.2318 |          10.8775 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0029 |          46.5632 |          10.9147 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0131 |          41.1143 |          10.9276 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0168 |          40.5303 |          10.9193 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0140 |          40.1165 |          10.8819 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0184 |          39.7547 |          10.9086 |
[32m[20221213 23:31:09 @agent_ppo2.py:185][0m |          -0.0191 |          39.6483 |          10.9208 |
[32m[20221213 23:31:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:31:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.81
[32m[20221213 23:31:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.19
[32m[20221213 23:31:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.84
[32m[20221213 23:31:10 @agent_ppo2.py:143][0m Total time:      18.63 min
[32m[20221213 23:31:10 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221213 23:31:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2879 --------------------------#
[32m[20221213 23:31:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:10 @agent_ppo2.py:185][0m |          -0.0006 |          62.0432 |          10.6859 |
[32m[20221213 23:31:10 @agent_ppo2.py:185][0m |          -0.0066 |          56.9260 |          10.7521 |
[32m[20221213 23:31:10 @agent_ppo2.py:185][0m |          -0.0057 |          54.9706 |          10.7325 |
[32m[20221213 23:31:10 @agent_ppo2.py:185][0m |          -0.0017 |          56.7823 |          10.7145 |
[32m[20221213 23:31:10 @agent_ppo2.py:185][0m |          -0.0096 |          53.6314 |          10.7507 |
[32m[20221213 23:31:10 @agent_ppo2.py:185][0m |           0.0002 |          56.7741 |          10.7722 |
[32m[20221213 23:31:10 @agent_ppo2.py:185][0m |          -0.0096 |          52.7161 |          10.7917 |
[32m[20221213 23:31:11 @agent_ppo2.py:185][0m |          -0.0021 |          55.0328 |          10.7401 |
[32m[20221213 23:31:11 @agent_ppo2.py:185][0m |          -0.0111 |          52.3710 |          10.7466 |
[32m[20221213 23:31:11 @agent_ppo2.py:185][0m |          -0.0155 |          52.2438 |          10.7699 |
[32m[20221213 23:31:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.06
[32m[20221213 23:31:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.51
[32m[20221213 23:31:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.64
[32m[20221213 23:31:11 @agent_ppo2.py:143][0m Total time:      18.65 min
[32m[20221213 23:31:11 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221213 23:31:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2880 --------------------------#
[32m[20221213 23:31:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:31:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:11 @agent_ppo2.py:185][0m |          -0.0020 |          55.9743 |          10.9819 |
[32m[20221213 23:31:11 @agent_ppo2.py:185][0m |          -0.0005 |          54.2823 |          11.0066 |
[32m[20221213 23:31:11 @agent_ppo2.py:185][0m |          -0.0070 |          53.4886 |          11.0429 |
[32m[20221213 23:31:11 @agent_ppo2.py:185][0m |          -0.0072 |          53.1865 |          11.0374 |
[32m[20221213 23:31:12 @agent_ppo2.py:185][0m |          -0.0079 |          53.0384 |          11.0449 |
[32m[20221213 23:31:12 @agent_ppo2.py:185][0m |           0.0028 |          56.6617 |          11.0409 |
[32m[20221213 23:31:12 @agent_ppo2.py:185][0m |          -0.0064 |          53.1851 |          11.0748 |
[32m[20221213 23:31:12 @agent_ppo2.py:185][0m |          -0.0107 |          52.2896 |          11.0465 |
[32m[20221213 23:31:12 @agent_ppo2.py:185][0m |          -0.0120 |          52.2814 |          11.0591 |
[32m[20221213 23:31:12 @agent_ppo2.py:185][0m |          -0.0122 |          52.0262 |          11.0430 |
[32m[20221213 23:31:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.98
[32m[20221213 23:31:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.59
[32m[20221213 23:31:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.68
[32m[20221213 23:31:12 @agent_ppo2.py:143][0m Total time:      18.67 min
[32m[20221213 23:31:12 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221213 23:31:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2881 --------------------------#
[32m[20221213 23:31:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:12 @agent_ppo2.py:185][0m |           0.0019 |          63.6480 |          10.6258 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0047 |          59.9362 |          10.6449 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0085 |          58.3706 |          10.6749 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0048 |          57.4533 |          10.5930 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0095 |          56.9510 |          10.6766 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0080 |          56.3234 |          10.6328 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0139 |          55.8515 |          10.6502 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0127 |          55.5960 |          10.6482 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0023 |          60.5920 |          10.6122 |
[32m[20221213 23:31:13 @agent_ppo2.py:185][0m |          -0.0123 |          55.1041 |          10.6420 |
[32m[20221213 23:31:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.68
[32m[20221213 23:31:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.51
[32m[20221213 23:31:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.94
[32m[20221213 23:31:13 @agent_ppo2.py:143][0m Total time:      18.69 min
[32m[20221213 23:31:13 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221213 23:31:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2882 --------------------------#
[32m[20221213 23:31:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |           0.0060 |          53.9378 |          10.8121 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |          -0.0008 |          48.4251 |          10.8076 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |          -0.0043 |          46.8589 |          10.8464 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |           0.0015 |          46.8246 |          10.8498 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |          -0.0074 |          45.6791 |          10.8518 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |          -0.0041 |          45.1592 |          10.8986 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |          -0.0066 |          44.9901 |          10.8464 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |           0.0018 |          51.6412 |          10.8965 |
[32m[20221213 23:31:14 @agent_ppo2.py:185][0m |          -0.0103 |          44.4329 |          10.8507 |
[32m[20221213 23:31:15 @agent_ppo2.py:185][0m |          -0.0050 |          44.3630 |          10.8620 |
[32m[20221213 23:31:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.00
[32m[20221213 23:31:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.54
[32m[20221213 23:31:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.90
[32m[20221213 23:31:15 @agent_ppo2.py:143][0m Total time:      18.71 min
[32m[20221213 23:31:15 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221213 23:31:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2883 --------------------------#
[32m[20221213 23:31:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:15 @agent_ppo2.py:185][0m |          -0.0002 |          39.5823 |          10.6596 |
[32m[20221213 23:31:15 @agent_ppo2.py:185][0m |           0.0052 |          35.1112 |          10.7460 |
[32m[20221213 23:31:15 @agent_ppo2.py:185][0m |          -0.0049 |          33.0370 |          10.7529 |
[32m[20221213 23:31:15 @agent_ppo2.py:185][0m |          -0.0081 |          32.4039 |          10.7757 |
[32m[20221213 23:31:15 @agent_ppo2.py:185][0m |          -0.0119 |          32.0372 |          10.8033 |
[32m[20221213 23:31:15 @agent_ppo2.py:185][0m |          -0.0106 |          31.8028 |          10.7944 |
[32m[20221213 23:31:16 @agent_ppo2.py:185][0m |          -0.0042 |          32.1211 |          10.8164 |
[32m[20221213 23:31:16 @agent_ppo2.py:185][0m |           0.0030 |          34.7102 |          10.8320 |
[32m[20221213 23:31:16 @agent_ppo2.py:185][0m |          -0.0089 |          31.1522 |          10.8659 |
[32m[20221213 23:31:16 @agent_ppo2.py:185][0m |          -0.0186 |          30.9400 |          10.8990 |
[32m[20221213 23:31:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.41
[32m[20221213 23:31:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.62
[32m[20221213 23:31:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.88
[32m[20221213 23:31:16 @agent_ppo2.py:143][0m Total time:      18.73 min
[32m[20221213 23:31:16 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221213 23:31:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2884 --------------------------#
[32m[20221213 23:31:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:16 @agent_ppo2.py:185][0m |          -0.0007 |          46.2978 |          11.0351 |
[32m[20221213 23:31:16 @agent_ppo2.py:185][0m |           0.0072 |          48.5856 |          10.9393 |
[32m[20221213 23:31:16 @agent_ppo2.py:185][0m |          -0.0008 |          45.1850 |          10.9125 |
[32m[20221213 23:31:17 @agent_ppo2.py:185][0m |          -0.0125 |          38.9135 |          10.9596 |
[32m[20221213 23:31:17 @agent_ppo2.py:185][0m |          -0.0140 |          38.2115 |          10.8895 |
[32m[20221213 23:31:17 @agent_ppo2.py:185][0m |          -0.0186 |          38.0743 |          10.9156 |
[32m[20221213 23:31:17 @agent_ppo2.py:185][0m |          -0.0077 |          37.7983 |          10.9158 |
[32m[20221213 23:31:17 @agent_ppo2.py:185][0m |          -0.0137 |          37.3917 |          10.8594 |
[32m[20221213 23:31:17 @agent_ppo2.py:185][0m |          -0.0148 |          37.0431 |          10.8154 |
[32m[20221213 23:31:17 @agent_ppo2.py:185][0m |          -0.0054 |          41.8367 |          10.8085 |
[32m[20221213 23:31:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.77
[32m[20221213 23:31:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.10
[32m[20221213 23:31:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.94
[32m[20221213 23:31:17 @agent_ppo2.py:143][0m Total time:      18.75 min
[32m[20221213 23:31:17 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221213 23:31:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2885 --------------------------#
[32m[20221213 23:31:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |           0.0002 |          50.6620 |          10.7744 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0071 |          45.9021 |          10.7613 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0062 |          44.7288 |          10.7788 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0058 |          43.9087 |          10.7485 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0110 |          42.6643 |          10.7637 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0134 |          42.0647 |          10.7374 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0132 |          41.7874 |          10.7632 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0153 |          41.6408 |          10.7227 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0145 |          41.2683 |          10.7239 |
[32m[20221213 23:31:18 @agent_ppo2.py:185][0m |          -0.0223 |          40.8081 |          10.6766 |
[32m[20221213 23:31:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.05
[32m[20221213 23:31:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.56
[32m[20221213 23:31:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.86
[32m[20221213 23:31:18 @agent_ppo2.py:143][0m Total time:      18.77 min
[32m[20221213 23:31:18 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221213 23:31:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2886 --------------------------#
[32m[20221213 23:31:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |           0.0015 |          68.4583 |          11.2586 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0044 |          67.3880 |          11.3052 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0047 |          66.9246 |          11.3045 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0005 |          67.5113 |          11.2903 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0051 |          66.4906 |          11.3361 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0040 |          66.2085 |          11.3197 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0077 |          66.1575 |          11.3179 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0069 |          65.8615 |          11.2996 |
[32m[20221213 23:31:19 @agent_ppo2.py:185][0m |          -0.0079 |          65.8101 |          11.3198 |
[32m[20221213 23:31:20 @agent_ppo2.py:185][0m |          -0.0045 |          65.9347 |          11.3766 |
[32m[20221213 23:31:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.87
[32m[20221213 23:31:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.04
[32m[20221213 23:31:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.37
[32m[20221213 23:31:20 @agent_ppo2.py:143][0m Total time:      18.80 min
[32m[20221213 23:31:20 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221213 23:31:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2887 --------------------------#
[32m[20221213 23:31:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:20 @agent_ppo2.py:185][0m |           0.0084 |          65.8260 |          11.1833 |
[32m[20221213 23:31:20 @agent_ppo2.py:185][0m |          -0.0061 |          58.0531 |          11.1935 |
[32m[20221213 23:31:20 @agent_ppo2.py:185][0m |          -0.0121 |          56.3282 |          11.2009 |
[32m[20221213 23:31:20 @agent_ppo2.py:185][0m |          -0.0113 |          55.0741 |          11.1270 |
[32m[20221213 23:31:20 @agent_ppo2.py:185][0m |          -0.0112 |          54.5080 |          11.1332 |
[32m[20221213 23:31:21 @agent_ppo2.py:185][0m |          -0.0144 |          53.5943 |          11.1001 |
[32m[20221213 23:31:21 @agent_ppo2.py:185][0m |          -0.0115 |          53.2381 |          11.1039 |
[32m[20221213 23:31:21 @agent_ppo2.py:185][0m |          -0.0164 |          53.0717 |          11.0075 |
[32m[20221213 23:31:21 @agent_ppo2.py:185][0m |          -0.0136 |          52.5769 |          11.0708 |
[32m[20221213 23:31:21 @agent_ppo2.py:185][0m |          -0.0171 |          52.0659 |          11.0632 |
[32m[20221213 23:31:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:31:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.28
[32m[20221213 23:31:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.95
[32m[20221213 23:31:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.65
[32m[20221213 23:31:21 @agent_ppo2.py:143][0m Total time:      18.82 min
[32m[20221213 23:31:21 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221213 23:31:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2888 --------------------------#
[32m[20221213 23:31:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:21 @agent_ppo2.py:185][0m |           0.0050 |          62.9778 |          10.5011 |
[32m[20221213 23:31:21 @agent_ppo2.py:185][0m |          -0.0054 |          60.0407 |          10.5285 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0037 |          59.5253 |          10.5798 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0092 |          58.2633 |          10.5554 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0050 |          58.5076 |          10.5797 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0108 |          57.1186 |          10.6183 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0089 |          56.6638 |          10.6554 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0110 |          56.6510 |          10.6707 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0146 |          56.3918 |          10.7244 |
[32m[20221213 23:31:22 @agent_ppo2.py:185][0m |          -0.0116 |          56.3016 |          10.7191 |
[32m[20221213 23:31:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.02
[32m[20221213 23:31:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.32
[32m[20221213 23:31:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.81
[32m[20221213 23:31:22 @agent_ppo2.py:143][0m Total time:      18.84 min
[32m[20221213 23:31:22 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221213 23:31:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2889 --------------------------#
[32m[20221213 23:31:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |           0.0008 |          62.4969 |          11.1640 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0018 |          54.7230 |          11.1420 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0038 |          52.2606 |          11.1133 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0063 |          51.0148 |          11.1747 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0075 |          50.2156 |          11.1888 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0053 |          49.5610 |          11.2202 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0096 |          48.9970 |          11.2369 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0057 |          49.4079 |          11.2345 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0045 |          48.3379 |          11.2503 |
[32m[20221213 23:31:23 @agent_ppo2.py:185][0m |          -0.0073 |          48.2932 |          11.2880 |
[32m[20221213 23:31:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.03
[32m[20221213 23:31:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.05
[32m[20221213 23:31:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.82
[32m[20221213 23:31:24 @agent_ppo2.py:143][0m Total time:      18.86 min
[32m[20221213 23:31:24 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221213 23:31:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2890 --------------------------#
[32m[20221213 23:31:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:31:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |           0.0027 |          68.1321 |          11.4663 |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |           0.0039 |          61.6840 |          11.4277 |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |          -0.0070 |          56.0068 |          11.4275 |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |          -0.0149 |          54.3983 |          11.3931 |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |          -0.0110 |          53.7543 |          11.4016 |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |          -0.0081 |          53.5963 |          11.4056 |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |          -0.0141 |          52.8121 |          11.4096 |
[32m[20221213 23:31:24 @agent_ppo2.py:185][0m |          -0.0117 |          52.6742 |          11.4123 |
[32m[20221213 23:31:25 @agent_ppo2.py:185][0m |          -0.0146 |          51.5932 |          11.3924 |
[32m[20221213 23:31:25 @agent_ppo2.py:185][0m |          -0.0149 |          51.4776 |          11.3801 |
[32m[20221213 23:31:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.54
[32m[20221213 23:31:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.51
[32m[20221213 23:31:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.25
[32m[20221213 23:31:25 @agent_ppo2.py:143][0m Total time:      18.88 min
[32m[20221213 23:31:25 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221213 23:31:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2891 --------------------------#
[32m[20221213 23:31:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:25 @agent_ppo2.py:185][0m |           0.0016 |          45.7106 |          10.7284 |
[32m[20221213 23:31:25 @agent_ppo2.py:185][0m |          -0.0034 |          40.4423 |          10.7222 |
[32m[20221213 23:31:25 @agent_ppo2.py:185][0m |          -0.0093 |          38.7179 |          10.7359 |
[32m[20221213 23:31:25 @agent_ppo2.py:185][0m |          -0.0073 |          37.7766 |          10.7377 |
[32m[20221213 23:31:25 @agent_ppo2.py:185][0m |          -0.0084 |          37.1841 |          10.7001 |
[32m[20221213 23:31:26 @agent_ppo2.py:185][0m |          -0.0051 |          36.8278 |          10.7267 |
[32m[20221213 23:31:26 @agent_ppo2.py:185][0m |          -0.0049 |          38.9516 |          10.6796 |
[32m[20221213 23:31:26 @agent_ppo2.py:185][0m |          -0.0178 |          35.7444 |          10.7026 |
[32m[20221213 23:31:26 @agent_ppo2.py:185][0m |          -0.0116 |          35.2882 |          10.7075 |
[32m[20221213 23:31:26 @agent_ppo2.py:185][0m |          -0.0125 |          35.2890 |          10.7292 |
[32m[20221213 23:31:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.85
[32m[20221213 23:31:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.18
[32m[20221213 23:31:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.80
[32m[20221213 23:31:26 @agent_ppo2.py:143][0m Total time:      18.90 min
[32m[20221213 23:31:26 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221213 23:31:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2892 --------------------------#
[32m[20221213 23:31:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:26 @agent_ppo2.py:185][0m |           0.0019 |          49.4994 |          10.4900 |
[32m[20221213 23:31:26 @agent_ppo2.py:185][0m |          -0.0066 |          46.1017 |          10.5039 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0054 |          45.0543 |          10.4267 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0121 |          44.4736 |          10.4861 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0097 |          43.8930 |          10.4223 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0103 |          43.6169 |          10.4705 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0119 |          43.1740 |          10.4684 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0100 |          42.7763 |          10.4493 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0124 |          42.6416 |          10.4334 |
[32m[20221213 23:31:27 @agent_ppo2.py:185][0m |          -0.0169 |          42.2839 |          10.4551 |
[32m[20221213 23:31:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:31:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.21
[32m[20221213 23:31:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.43
[32m[20221213 23:31:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.06
[32m[20221213 23:31:27 @agent_ppo2.py:143][0m Total time:      18.92 min
[32m[20221213 23:31:27 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221213 23:31:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2893 --------------------------#
[32m[20221213 23:31:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |           0.0012 |          58.0753 |          10.6701 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0061 |          54.5924 |          10.6747 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0088 |          54.1736 |          10.6752 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0076 |          53.5507 |          10.6951 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0101 |          53.2510 |          10.6639 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0104 |          53.0062 |          10.7020 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0086 |          53.1514 |          10.7047 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0048 |          53.8368 |          10.7311 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0093 |          52.5338 |          10.7895 |
[32m[20221213 23:31:28 @agent_ppo2.py:185][0m |          -0.0076 |          52.4318 |          10.7473 |
[32m[20221213 23:31:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.66
[32m[20221213 23:31:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.29
[32m[20221213 23:31:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.72
[32m[20221213 23:31:29 @agent_ppo2.py:143][0m Total time:      18.94 min
[32m[20221213 23:31:29 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221213 23:31:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2894 --------------------------#
[32m[20221213 23:31:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:29 @agent_ppo2.py:185][0m |           0.0009 |          58.3368 |          11.0674 |
[32m[20221213 23:31:29 @agent_ppo2.py:185][0m |          -0.0033 |          52.6331 |          11.0106 |
[32m[20221213 23:31:29 @agent_ppo2.py:185][0m |          -0.0035 |          51.1037 |          11.0928 |
[32m[20221213 23:31:29 @agent_ppo2.py:185][0m |          -0.0068 |          50.4568 |          11.0321 |
[32m[20221213 23:31:29 @agent_ppo2.py:185][0m |          -0.0070 |          49.8312 |          11.0078 |
[32m[20221213 23:31:29 @agent_ppo2.py:185][0m |          -0.0108 |          49.6866 |          11.0232 |
[32m[20221213 23:31:29 @agent_ppo2.py:185][0m |          -0.0123 |          49.2904 |          11.0425 |
[32m[20221213 23:31:30 @agent_ppo2.py:185][0m |          -0.0072 |          50.0380 |          11.0221 |
[32m[20221213 23:31:30 @agent_ppo2.py:185][0m |          -0.0103 |          49.0932 |          11.0649 |
[32m[20221213 23:31:30 @agent_ppo2.py:185][0m |          -0.0127 |          48.5573 |          11.0369 |
[32m[20221213 23:31:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.08
[32m[20221213 23:31:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.69
[32m[20221213 23:31:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.39
[32m[20221213 23:31:30 @agent_ppo2.py:143][0m Total time:      18.96 min
[32m[20221213 23:31:30 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221213 23:31:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2895 --------------------------#
[32m[20221213 23:31:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:30 @agent_ppo2.py:185][0m |           0.0006 |          51.7877 |          10.6501 |
[32m[20221213 23:31:30 @agent_ppo2.py:185][0m |          -0.0058 |          49.0822 |          10.7006 |
[32m[20221213 23:31:30 @agent_ppo2.py:185][0m |          -0.0034 |          48.5367 |          10.7414 |
[32m[20221213 23:31:30 @agent_ppo2.py:185][0m |          -0.0074 |          47.9870 |          10.7906 |
[32m[20221213 23:31:31 @agent_ppo2.py:185][0m |          -0.0095 |          47.9247 |          10.7745 |
[32m[20221213 23:31:31 @agent_ppo2.py:185][0m |          -0.0075 |          47.3682 |          10.8021 |
[32m[20221213 23:31:31 @agent_ppo2.py:185][0m |          -0.0147 |          47.4417 |          10.8032 |
[32m[20221213 23:31:31 @agent_ppo2.py:185][0m |          -0.0120 |          47.1650 |          10.8308 |
[32m[20221213 23:31:31 @agent_ppo2.py:185][0m |           0.0069 |          55.8430 |          10.8487 |
[32m[20221213 23:31:31 @agent_ppo2.py:185][0m |          -0.0106 |          47.0743 |          10.8879 |
[32m[20221213 23:31:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.89
[32m[20221213 23:31:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.24
[32m[20221213 23:31:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.81
[32m[20221213 23:31:31 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221213 23:31:31 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221213 23:31:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2896 --------------------------#
[32m[20221213 23:31:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:31 @agent_ppo2.py:185][0m |           0.0001 |          40.9333 |          10.7164 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |           0.0059 |          38.5043 |          10.7257 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |          -0.0017 |          34.6590 |          10.7079 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |           0.0004 |          34.2254 |          10.7344 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |           0.0050 |          38.7931 |          10.6673 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |          -0.0129 |          33.2533 |          10.6597 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |          -0.0069 |          32.8363 |          10.6787 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |          -0.0095 |          32.5087 |          10.6599 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |          -0.0090 |          32.1807 |          10.6085 |
[32m[20221213 23:31:32 @agent_ppo2.py:185][0m |          -0.0148 |          32.0411 |          10.6480 |
[32m[20221213 23:31:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.71
[32m[20221213 23:31:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.66
[32m[20221213 23:31:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.37
[32m[20221213 23:31:32 @agent_ppo2.py:143][0m Total time:      19.01 min
[32m[20221213 23:31:32 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221213 23:31:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2897 --------------------------#
[32m[20221213 23:31:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0025 |          55.8272 |          10.5457 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0035 |          51.4928 |          10.5569 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0056 |          49.4720 |          10.5390 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |           0.0058 |          53.8393 |          10.6003 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0080 |          47.0143 |          10.5891 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0034 |          47.8641 |          10.5302 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0090 |          45.3858 |          10.5646 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0126 |          44.8701 |          10.5502 |
[32m[20221213 23:31:33 @agent_ppo2.py:185][0m |          -0.0078 |          45.1917 |          10.5257 |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |          -0.0136 |          44.1786 |          10.5343 |
[32m[20221213 23:31:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:31:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.16
[32m[20221213 23:31:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.63
[32m[20221213 23:31:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.99
[32m[20221213 23:31:34 @agent_ppo2.py:143][0m Total time:      19.03 min
[32m[20221213 23:31:34 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221213 23:31:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2898 --------------------------#
[32m[20221213 23:31:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |           0.0016 |          46.6419 |          10.6907 |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |           0.0045 |          40.4425 |          10.7163 |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |          -0.0086 |          38.6537 |          10.7709 |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |          -0.0068 |          37.5860 |          10.7501 |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |          -0.0063 |          36.9823 |          10.7216 |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |          -0.0070 |          36.3346 |          10.7571 |
[32m[20221213 23:31:34 @agent_ppo2.py:185][0m |          -0.0090 |          35.7934 |          10.8111 |
[32m[20221213 23:31:35 @agent_ppo2.py:185][0m |          -0.0038 |          36.3835 |          10.8054 |
[32m[20221213 23:31:35 @agent_ppo2.py:185][0m |          -0.0081 |          35.2073 |          10.8396 |
[32m[20221213 23:31:35 @agent_ppo2.py:185][0m |          -0.0092 |          34.9973 |          10.8410 |
[32m[20221213 23:31:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.71
[32m[20221213 23:31:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.39
[32m[20221213 23:31:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.46
[32m[20221213 23:31:35 @agent_ppo2.py:143][0m Total time:      19.05 min
[32m[20221213 23:31:35 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221213 23:31:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2899 --------------------------#
[32m[20221213 23:31:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:35 @agent_ppo2.py:185][0m |           0.0137 |          62.8282 |          11.0765 |
[32m[20221213 23:31:35 @agent_ppo2.py:185][0m |           0.0028 |          56.4115 |          11.0504 |
[32m[20221213 23:31:35 @agent_ppo2.py:185][0m |          -0.0082 |          55.1679 |          11.1125 |
[32m[20221213 23:31:35 @agent_ppo2.py:185][0m |          -0.0064 |          55.8676 |          11.0990 |
[32m[20221213 23:31:36 @agent_ppo2.py:185][0m |          -0.0120 |          54.7157 |          11.0965 |
[32m[20221213 23:31:36 @agent_ppo2.py:185][0m |          -0.0119 |          54.8045 |          11.1248 |
[32m[20221213 23:31:36 @agent_ppo2.py:185][0m |          -0.0129 |          54.6005 |          11.1446 |
[32m[20221213 23:31:36 @agent_ppo2.py:185][0m |          -0.0120 |          54.4334 |          11.1307 |
[32m[20221213 23:31:36 @agent_ppo2.py:185][0m |          -0.0128 |          54.4799 |          11.1847 |
[32m[20221213 23:31:36 @agent_ppo2.py:185][0m |          -0.0124 |          54.4250 |          11.1377 |
[32m[20221213 23:31:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.67
[32m[20221213 23:31:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.27
[32m[20221213 23:31:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.13
[32m[20221213 23:31:36 @agent_ppo2.py:143][0m Total time:      19.07 min
[32m[20221213 23:31:36 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221213 23:31:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2900 --------------------------#
[32m[20221213 23:31:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:31:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:36 @agent_ppo2.py:185][0m |           0.0031 |          54.0332 |          10.9377 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |           0.0012 |          50.0708 |          10.9208 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0053 |          49.5444 |          10.8596 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0057 |          49.3086 |          10.8424 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0067 |          49.2609 |          10.7803 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0078 |          49.2594 |          10.7096 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0078 |          48.7586 |          10.7406 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0088 |          48.8994 |          10.6275 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0069 |          48.6939 |          10.6643 |
[32m[20221213 23:31:37 @agent_ppo2.py:185][0m |          -0.0071 |          48.6221 |          10.6578 |
[32m[20221213 23:31:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:31:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.42
[32m[20221213 23:31:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.72
[32m[20221213 23:31:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.08
[32m[20221213 23:31:37 @agent_ppo2.py:143][0m Total time:      19.09 min
[32m[20221213 23:31:37 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221213 23:31:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2901 --------------------------#
[32m[20221213 23:31:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |           0.0017 |          60.6641 |          10.2433 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0054 |          58.4443 |          10.3005 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0072 |          57.9150 |          10.3032 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0097 |          57.5751 |          10.3531 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0069 |          57.7573 |          10.3137 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0111 |          57.0546 |          10.3064 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0106 |          56.9358 |          10.3065 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0145 |          56.7775 |          10.3125 |
[32m[20221213 23:31:38 @agent_ppo2.py:185][0m |          -0.0116 |          56.6796 |          10.3051 |
[32m[20221213 23:31:39 @agent_ppo2.py:185][0m |          -0.0111 |          56.6912 |          10.3480 |
[32m[20221213 23:31:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.90
[32m[20221213 23:31:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.98
[32m[20221213 23:31:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.54
[32m[20221213 23:31:39 @agent_ppo2.py:143][0m Total time:      19.11 min
[32m[20221213 23:31:39 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221213 23:31:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2902 --------------------------#
[32m[20221213 23:31:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:39 @agent_ppo2.py:185][0m |           0.0034 |          48.7763 |          11.0838 |
[32m[20221213 23:31:39 @agent_ppo2.py:185][0m |          -0.0084 |          44.4958 |          11.0876 |
[32m[20221213 23:31:39 @agent_ppo2.py:185][0m |          -0.0082 |          42.6808 |          11.0539 |
[32m[20221213 23:31:39 @agent_ppo2.py:185][0m |          -0.0115 |          41.6142 |          11.0168 |
[32m[20221213 23:31:39 @agent_ppo2.py:185][0m |          -0.0067 |          41.7207 |          10.9938 |
[32m[20221213 23:31:39 @agent_ppo2.py:185][0m |          -0.0135 |          39.9186 |          10.9733 |
[32m[20221213 23:31:40 @agent_ppo2.py:185][0m |          -0.0072 |          41.0932 |          10.9682 |
[32m[20221213 23:31:40 @agent_ppo2.py:185][0m |          -0.0125 |          38.8821 |          10.9470 |
[32m[20221213 23:31:40 @agent_ppo2.py:185][0m |          -0.0169 |          38.7104 |          10.9503 |
[32m[20221213 23:31:40 @agent_ppo2.py:185][0m |          -0.0155 |          38.0927 |          10.9272 |
[32m[20221213 23:31:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.24
[32m[20221213 23:31:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.59
[32m[20221213 23:31:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.61
[32m[20221213 23:31:40 @agent_ppo2.py:143][0m Total time:      19.13 min
[32m[20221213 23:31:40 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221213 23:31:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2903 --------------------------#
[32m[20221213 23:31:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:40 @agent_ppo2.py:185][0m |          -0.0057 |          28.1087 |          10.4655 |
[32m[20221213 23:31:40 @agent_ppo2.py:185][0m |          -0.0034 |          23.5810 |          10.5059 |
[32m[20221213 23:31:40 @agent_ppo2.py:185][0m |          -0.0098 |          21.6029 |          10.5346 |
[32m[20221213 23:31:41 @agent_ppo2.py:185][0m |           0.0047 |          22.8734 |          10.5228 |
[32m[20221213 23:31:41 @agent_ppo2.py:185][0m |          -0.0064 |          20.0116 |          10.5226 |
[32m[20221213 23:31:41 @agent_ppo2.py:185][0m |          -0.0083 |          19.8506 |          10.5687 |
[32m[20221213 23:31:41 @agent_ppo2.py:185][0m |          -0.0103 |          19.1530 |          10.5230 |
[32m[20221213 23:31:41 @agent_ppo2.py:185][0m |          -0.0137 |          18.8013 |          10.5229 |
[32m[20221213 23:31:41 @agent_ppo2.py:185][0m |          -0.0097 |          18.6146 |          10.5555 |
[32m[20221213 23:31:41 @agent_ppo2.py:185][0m |          -0.0189 |          18.1468 |          10.5297 |
[32m[20221213 23:31:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.37
[32m[20221213 23:31:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.08
[32m[20221213 23:31:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.88
[32m[20221213 23:31:41 @agent_ppo2.py:143][0m Total time:      19.15 min
[32m[20221213 23:31:41 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221213 23:31:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2904 --------------------------#
[32m[20221213 23:31:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |           0.0008 |          37.6639 |          10.6212 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0019 |          33.0714 |          10.6518 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0069 |          32.1256 |          10.6580 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0049 |          31.3570 |          10.6737 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0061 |          31.0414 |          10.6402 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0072 |          31.3263 |          10.6739 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0123 |          30.1359 |          10.7119 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0152 |          29.9583 |          10.7147 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0162 |          29.6411 |          10.7041 |
[32m[20221213 23:31:42 @agent_ppo2.py:185][0m |          -0.0073 |          29.6918 |          10.7080 |
[32m[20221213 23:31:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.49
[32m[20221213 23:31:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.91
[32m[20221213 23:31:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.81
[32m[20221213 23:31:42 @agent_ppo2.py:143][0m Total time:      19.17 min
[32m[20221213 23:31:42 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221213 23:31:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2905 --------------------------#
[32m[20221213 23:31:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |           0.0032 |          47.7526 |          10.7036 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0075 |          45.0128 |          10.7821 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0033 |          44.2792 |          10.7603 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0120 |          43.4316 |          10.8032 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0094 |          42.9685 |          10.7650 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0076 |          42.4356 |          10.7598 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0098 |          42.2866 |          10.7544 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0124 |          41.9841 |          10.7614 |
[32m[20221213 23:31:43 @agent_ppo2.py:185][0m |          -0.0050 |          42.2860 |          10.7669 |
[32m[20221213 23:31:44 @agent_ppo2.py:185][0m |          -0.0077 |          41.5839 |          10.7676 |
[32m[20221213 23:31:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.13
[32m[20221213 23:31:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.86
[32m[20221213 23:31:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.92
[32m[20221213 23:31:44 @agent_ppo2.py:143][0m Total time:      19.20 min
[32m[20221213 23:31:44 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221213 23:31:44 @agent_ppo2.py:121][0m #------------------------ Iteration 2906 --------------------------#
[32m[20221213 23:31:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:44 @agent_ppo2.py:185][0m |           0.0017 |          46.2796 |          10.4307 |
[32m[20221213 23:31:44 @agent_ppo2.py:185][0m |          -0.0069 |          42.6952 |          10.3675 |
[32m[20221213 23:31:44 @agent_ppo2.py:185][0m |          -0.0066 |          41.1236 |          10.4596 |
[32m[20221213 23:31:44 @agent_ppo2.py:185][0m |          -0.0050 |          40.3751 |          10.4866 |
[32m[20221213 23:31:44 @agent_ppo2.py:185][0m |          -0.0010 |          41.8729 |          10.4549 |
[32m[20221213 23:31:44 @agent_ppo2.py:185][0m |          -0.0071 |          39.2949 |          10.4689 |
[32m[20221213 23:31:45 @agent_ppo2.py:185][0m |           0.0013 |          43.8416 |          10.5192 |
[32m[20221213 23:31:45 @agent_ppo2.py:185][0m |          -0.0131 |          38.6654 |          10.5634 |
[32m[20221213 23:31:45 @agent_ppo2.py:185][0m |          -0.0126 |          38.2971 |          10.5533 |
[32m[20221213 23:31:45 @agent_ppo2.py:185][0m |          -0.0117 |          38.0711 |          10.5568 |
[32m[20221213 23:31:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:31:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.92
[32m[20221213 23:31:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.29
[32m[20221213 23:31:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.01
[32m[20221213 23:31:45 @agent_ppo2.py:143][0m Total time:      19.22 min
[32m[20221213 23:31:45 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221213 23:31:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2907 --------------------------#
[32m[20221213 23:31:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:45 @agent_ppo2.py:185][0m |           0.0011 |          58.0750 |          10.9769 |
[32m[20221213 23:31:45 @agent_ppo2.py:185][0m |          -0.0047 |          53.7044 |          10.9571 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0097 |          51.8564 |          10.9613 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0126 |          50.6457 |          10.8905 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0086 |          49.4863 |          10.9344 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0114 |          48.8392 |          10.9712 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0125 |          48.4072 |          10.9413 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0144 |          48.0527 |          10.9400 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0139 |          47.8170 |          10.9321 |
[32m[20221213 23:31:46 @agent_ppo2.py:185][0m |          -0.0166 |          47.4250 |          10.8761 |
[32m[20221213 23:31:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.62
[32m[20221213 23:31:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.78
[32m[20221213 23:31:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.86
[32m[20221213 23:31:46 @agent_ppo2.py:143][0m Total time:      19.24 min
[32m[20221213 23:31:46 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221213 23:31:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2908 --------------------------#
[32m[20221213 23:31:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0014 |          44.2043 |          10.7741 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0063 |          40.6116 |          10.7850 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0088 |          39.5230 |          10.7244 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0105 |          38.6550 |          10.7026 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0129 |          38.2630 |          10.7452 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0133 |          37.6910 |          10.7403 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0092 |          37.4882 |          10.6953 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0129 |          36.8940 |          10.6992 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |          -0.0134 |          36.6648 |          10.6671 |
[32m[20221213 23:31:47 @agent_ppo2.py:185][0m |           0.0088 |          46.1448 |          10.6727 |
[32m[20221213 23:31:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:31:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.36
[32m[20221213 23:31:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.64
[32m[20221213 23:31:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.88
[32m[20221213 23:31:48 @agent_ppo2.py:143][0m Total time:      19.26 min
[32m[20221213 23:31:48 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221213 23:31:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2909 --------------------------#
[32m[20221213 23:31:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:48 @agent_ppo2.py:185][0m |           0.0119 |          39.2173 |          10.0902 |
[32m[20221213 23:31:48 @agent_ppo2.py:185][0m |          -0.0046 |          35.1111 |          10.0609 |
[32m[20221213 23:31:48 @agent_ppo2.py:185][0m |          -0.0041 |          33.4856 |          10.0704 |
[32m[20221213 23:31:48 @agent_ppo2.py:185][0m |          -0.0044 |          32.4867 |          10.0806 |
[32m[20221213 23:31:48 @agent_ppo2.py:185][0m |          -0.0045 |          32.1402 |          10.1157 |
[32m[20221213 23:31:48 @agent_ppo2.py:185][0m |          -0.0072 |          31.4720 |          10.1358 |
[32m[20221213 23:31:48 @agent_ppo2.py:185][0m |          -0.0078 |          31.5784 |          10.1125 |
[32m[20221213 23:31:49 @agent_ppo2.py:185][0m |          -0.0050 |          30.6360 |          10.0849 |
[32m[20221213 23:31:49 @agent_ppo2.py:185][0m |          -0.0081 |          32.5426 |          10.1243 |
[32m[20221213 23:31:49 @agent_ppo2.py:185][0m |          -0.0120 |          29.9794 |          10.1234 |
[32m[20221213 23:31:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:31:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.54
[32m[20221213 23:31:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.24
[32m[20221213 23:31:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.20
[32m[20221213 23:31:49 @agent_ppo2.py:143][0m Total time:      19.28 min
[32m[20221213 23:31:49 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221213 23:31:49 @agent_ppo2.py:121][0m #------------------------ Iteration 2910 --------------------------#
[32m[20221213 23:31:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:31:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:49 @agent_ppo2.py:185][0m |          -0.0023 |          32.8152 |          10.3158 |
[32m[20221213 23:31:49 @agent_ppo2.py:185][0m |          -0.0005 |          27.1890 |          10.2367 |
[32m[20221213 23:31:49 @agent_ppo2.py:185][0m |          -0.0100 |          25.8021 |          10.1970 |
[32m[20221213 23:31:49 @agent_ppo2.py:185][0m |          -0.0114 |          25.1660 |          10.1810 |
[32m[20221213 23:31:50 @agent_ppo2.py:185][0m |          -0.0071 |          24.3426 |          10.1595 |
[32m[20221213 23:31:50 @agent_ppo2.py:185][0m |          -0.0167 |          23.6020 |          10.1205 |
[32m[20221213 23:31:50 @agent_ppo2.py:185][0m |          -0.0163 |          23.0670 |          10.1399 |
[32m[20221213 23:31:50 @agent_ppo2.py:185][0m |          -0.0157 |          22.8086 |          10.1042 |
[32m[20221213 23:31:50 @agent_ppo2.py:185][0m |          -0.0127 |          22.9219 |          10.0768 |
[32m[20221213 23:31:50 @agent_ppo2.py:185][0m |          -0.0144 |          22.2614 |          10.0454 |
[32m[20221213 23:31:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:31:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.82
[32m[20221213 23:31:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.85
[32m[20221213 23:31:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.73
[32m[20221213 23:31:50 @agent_ppo2.py:143][0m Total time:      19.30 min
[32m[20221213 23:31:50 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221213 23:31:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2911 --------------------------#
[32m[20221213 23:31:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:50 @agent_ppo2.py:185][0m |           0.0005 |          35.1698 |          10.5869 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0067 |          29.5736 |          10.5535 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0043 |          27.2469 |          10.5556 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0059 |          26.8267 |          10.5217 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0110 |          25.2720 |          10.5033 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0079 |          24.8558 |          10.5217 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0180 |          24.0981 |          10.5175 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0146 |          23.8514 |          10.5106 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0213 |          23.6431 |          10.4629 |
[32m[20221213 23:31:51 @agent_ppo2.py:185][0m |          -0.0143 |          23.6193 |          10.4784 |
[32m[20221213 23:31:51 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 23:31:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.82
[32m[20221213 23:31:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.84
[32m[20221213 23:31:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.77
[32m[20221213 23:31:52 @agent_ppo2.py:143][0m Total time:      19.32 min
[32m[20221213 23:31:52 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221213 23:31:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2912 --------------------------#
[32m[20221213 23:31:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |           0.0091 |          56.2348 |          10.5801 |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |          -0.0014 |          52.0155 |          10.6396 |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |           0.0067 |          53.4916 |          10.6136 |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |          -0.0062 |          49.9787 |          10.5913 |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |          -0.0111 |          48.3581 |          10.6313 |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |          -0.0056 |          48.3141 |          10.5829 |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |          -0.0114 |          47.4042 |          10.5960 |
[32m[20221213 23:31:52 @agent_ppo2.py:185][0m |          -0.0121 |          46.8897 |          10.5436 |
[32m[20221213 23:31:53 @agent_ppo2.py:185][0m |          -0.0132 |          46.5138 |          10.5746 |
[32m[20221213 23:31:53 @agent_ppo2.py:185][0m |          -0.0120 |          46.2572 |          10.5650 |
[32m[20221213 23:31:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:31:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.23
[32m[20221213 23:31:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.20
[32m[20221213 23:31:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.64
[32m[20221213 23:31:53 @agent_ppo2.py:143][0m Total time:      19.35 min
[32m[20221213 23:31:53 @agent_ppo2.py:145][0m 1869824 total steps have happened
[32m[20221213 23:31:53 @agent_ppo2.py:121][0m #------------------------ Iteration 2913 --------------------------#
[32m[20221213 23:31:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:53 @agent_ppo2.py:185][0m |          -0.0010 |          48.9260 |           9.9179 |
[32m[20221213 23:31:53 @agent_ppo2.py:185][0m |           0.0024 |          53.0720 |           9.9092 |
[32m[20221213 23:31:53 @agent_ppo2.py:185][0m |          -0.0046 |          44.6919 |           9.9278 |
[32m[20221213 23:31:53 @agent_ppo2.py:185][0m |          -0.0054 |          43.6866 |           9.9861 |
[32m[20221213 23:31:53 @agent_ppo2.py:185][0m |          -0.0118 |          43.0312 |           9.9618 |
[32m[20221213 23:31:54 @agent_ppo2.py:185][0m |          -0.0064 |          42.8609 |           9.9554 |
[32m[20221213 23:31:54 @agent_ppo2.py:185][0m |          -0.0172 |          42.2378 |           9.9384 |
[32m[20221213 23:31:54 @agent_ppo2.py:185][0m |          -0.0143 |          41.8541 |           9.9598 |
[32m[20221213 23:31:54 @agent_ppo2.py:185][0m |          -0.0119 |          42.5486 |           9.9416 |
[32m[20221213 23:31:54 @agent_ppo2.py:185][0m |          -0.0125 |          41.3092 |           9.9600 |
[32m[20221213 23:31:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:31:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.32
[32m[20221213 23:31:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.36
[32m[20221213 23:31:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.57
[32m[20221213 23:31:54 @agent_ppo2.py:143][0m Total time:      19.37 min
[32m[20221213 23:31:54 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221213 23:31:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2914 --------------------------#
[32m[20221213 23:31:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:54 @agent_ppo2.py:185][0m |          -0.0001 |          67.7137 |          10.4469 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0017 |          60.8598 |          10.3697 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0042 |          59.2958 |          10.3404 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0097 |          58.6986 |          10.2896 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0090 |          58.0379 |          10.2910 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0127 |          57.8123 |          10.2714 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0059 |          60.2102 |          10.2681 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0006 |          67.0648 |          10.2623 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0127 |          57.3378 |          10.2757 |
[32m[20221213 23:31:55 @agent_ppo2.py:185][0m |          -0.0127 |          57.7378 |          10.2383 |
[32m[20221213 23:31:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.26
[32m[20221213 23:31:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.25
[32m[20221213 23:31:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.24
[32m[20221213 23:31:55 @agent_ppo2.py:143][0m Total time:      19.39 min
[32m[20221213 23:31:55 @agent_ppo2.py:145][0m 1873920 total steps have happened
[32m[20221213 23:31:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2915 --------------------------#
[32m[20221213 23:31:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |           0.0140 |          68.1741 |          10.0629 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0014 |          58.8933 |          10.0662 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0076 |          58.2319 |          10.0358 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0094 |          57.6855 |          10.0025 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0128 |          57.5660 |          10.0804 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0119 |          57.1223 |           9.9768 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0102 |          56.9834 |           9.9912 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0114 |          56.6364 |          10.0028 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0109 |          56.4760 |           9.9451 |
[32m[20221213 23:31:56 @agent_ppo2.py:185][0m |          -0.0153 |          56.3074 |           9.9829 |
[32m[20221213 23:31:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.20
[32m[20221213 23:31:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.16
[32m[20221213 23:31:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.88
[32m[20221213 23:31:57 @agent_ppo2.py:143][0m Total time:      19.41 min
[32m[20221213 23:31:57 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221213 23:31:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2916 --------------------------#
[32m[20221213 23:31:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:31:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:57 @agent_ppo2.py:185][0m |           0.0037 |          55.8185 |          10.1584 |
[32m[20221213 23:31:57 @agent_ppo2.py:185][0m |          -0.0054 |          50.7975 |          10.1951 |
[32m[20221213 23:31:57 @agent_ppo2.py:185][0m |          -0.0046 |          49.4029 |          10.1336 |
[32m[20221213 23:31:57 @agent_ppo2.py:185][0m |          -0.0143 |          48.6073 |          10.1463 |
[32m[20221213 23:31:57 @agent_ppo2.py:185][0m |          -0.0100 |          48.0808 |          10.1500 |
[32m[20221213 23:31:57 @agent_ppo2.py:185][0m |          -0.0125 |          47.8056 |          10.1313 |
[32m[20221213 23:31:58 @agent_ppo2.py:185][0m |          -0.0057 |          48.6457 |          10.1264 |
[32m[20221213 23:31:58 @agent_ppo2.py:185][0m |          -0.0150 |          47.2419 |          10.1608 |
[32m[20221213 23:31:58 @agent_ppo2.py:185][0m |          -0.0146 |          46.6423 |          10.1693 |
[32m[20221213 23:31:58 @agent_ppo2.py:185][0m |          -0.0157 |          46.3970 |          10.1389 |
[32m[20221213 23:31:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.69
[32m[20221213 23:31:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.82
[32m[20221213 23:31:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.56
[32m[20221213 23:31:58 @agent_ppo2.py:143][0m Total time:      19.43 min
[32m[20221213 23:31:58 @agent_ppo2.py:145][0m 1878016 total steps have happened
[32m[20221213 23:31:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2917 --------------------------#
[32m[20221213 23:31:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:31:58 @agent_ppo2.py:185][0m |          -0.0035 |          62.5165 |           9.7599 |
[32m[20221213 23:31:58 @agent_ppo2.py:185][0m |          -0.0062 |          59.4068 |           9.8354 |
[32m[20221213 23:31:58 @agent_ppo2.py:185][0m |          -0.0075 |          58.4988 |           9.7921 |
[32m[20221213 23:31:59 @agent_ppo2.py:185][0m |          -0.0021 |          60.8055 |           9.7334 |
[32m[20221213 23:31:59 @agent_ppo2.py:185][0m |          -0.0083 |          57.1309 |           9.7895 |
[32m[20221213 23:31:59 @agent_ppo2.py:185][0m |          -0.0112 |          56.6925 |           9.7661 |
[32m[20221213 23:31:59 @agent_ppo2.py:185][0m |          -0.0064 |          56.7364 |           9.7987 |
[32m[20221213 23:31:59 @agent_ppo2.py:185][0m |           0.0027 |          60.5528 |           9.7855 |
[32m[20221213 23:31:59 @agent_ppo2.py:185][0m |          -0.0115 |          55.6570 |           9.8123 |
[32m[20221213 23:31:59 @agent_ppo2.py:185][0m |          -0.0006 |          60.4454 |           9.8023 |
[32m[20221213 23:31:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:31:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.73
[32m[20221213 23:31:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.92
[32m[20221213 23:31:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.05
[32m[20221213 23:31:59 @agent_ppo2.py:143][0m Total time:      19.45 min
[32m[20221213 23:31:59 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221213 23:31:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2918 --------------------------#
[32m[20221213 23:31:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:31:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0003 |          60.0614 |           9.8364 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0054 |          57.6485 |           9.7449 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0059 |          56.7757 |           9.7514 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0114 |          56.3883 |           9.7214 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0122 |          55.8763 |           9.7036 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0122 |          55.5980 |           9.6945 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0088 |          55.4801 |           9.6797 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0124 |          55.3441 |           9.6693 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0148 |          55.1247 |           9.6552 |
[32m[20221213 23:32:00 @agent_ppo2.py:185][0m |          -0.0132 |          55.1986 |           9.6708 |
[32m[20221213 23:32:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.95
[32m[20221213 23:32:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.21
[32m[20221213 23:32:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.78
[32m[20221213 23:32:00 @agent_ppo2.py:143][0m Total time:      19.47 min
[32m[20221213 23:32:00 @agent_ppo2.py:145][0m 1882112 total steps have happened
[32m[20221213 23:32:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2919 --------------------------#
[32m[20221213 23:32:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |           0.0005 |          65.8579 |           9.7395 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0009 |          63.8082 |           9.7448 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0081 |          61.5058 |           9.7507 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0089 |          60.9963 |           9.7517 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0115 |          60.4070 |           9.7593 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0086 |          59.8265 |           9.7603 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0065 |          60.0104 |           9.7740 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0042 |          60.3963 |           9.7786 |
[32m[20221213 23:32:01 @agent_ppo2.py:185][0m |          -0.0138 |          58.8015 |           9.7692 |
[32m[20221213 23:32:02 @agent_ppo2.py:185][0m |          -0.0149 |          58.6879 |           9.8064 |
[32m[20221213 23:32:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.99
[32m[20221213 23:32:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.85
[32m[20221213 23:32:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.56
[32m[20221213 23:32:02 @agent_ppo2.py:143][0m Total time:      19.49 min
[32m[20221213 23:32:02 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221213 23:32:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2920 --------------------------#
[32m[20221213 23:32:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:32:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:02 @agent_ppo2.py:185][0m |          -0.0012 |          53.9048 |           9.9689 |
[32m[20221213 23:32:02 @agent_ppo2.py:185][0m |          -0.0073 |          49.3375 |          10.0196 |
[32m[20221213 23:32:02 @agent_ppo2.py:185][0m |          -0.0102 |          48.2680 |           9.9686 |
[32m[20221213 23:32:02 @agent_ppo2.py:185][0m |          -0.0083 |          47.4900 |          10.0199 |
[32m[20221213 23:32:02 @agent_ppo2.py:185][0m |          -0.0062 |          48.3700 |           9.9963 |
[32m[20221213 23:32:02 @agent_ppo2.py:185][0m |          -0.0100 |          46.8989 |           9.9901 |
[32m[20221213 23:32:03 @agent_ppo2.py:185][0m |          -0.0158 |          46.4975 |           9.9883 |
[32m[20221213 23:32:03 @agent_ppo2.py:185][0m |          -0.0134 |          46.2614 |          10.0089 |
[32m[20221213 23:32:03 @agent_ppo2.py:185][0m |          -0.0103 |          46.2542 |          10.0184 |
[32m[20221213 23:32:03 @agent_ppo2.py:185][0m |          -0.0040 |          48.0530 |           9.9899 |
[32m[20221213 23:32:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:32:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.89
[32m[20221213 23:32:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.44
[32m[20221213 23:32:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.41
[32m[20221213 23:32:03 @agent_ppo2.py:143][0m Total time:      19.52 min
[32m[20221213 23:32:03 @agent_ppo2.py:145][0m 1886208 total steps have happened
[32m[20221213 23:32:03 @agent_ppo2.py:121][0m #------------------------ Iteration 2921 --------------------------#
[32m[20221213 23:32:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:03 @agent_ppo2.py:185][0m |           0.0176 |          74.1412 |           9.3237 |
[32m[20221213 23:32:03 @agent_ppo2.py:185][0m |          -0.0050 |          65.1605 |           9.3744 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |           0.0073 |          68.6340 |           9.3843 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |          -0.0073 |          64.0618 |           9.4033 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |          -0.0044 |          63.8635 |           9.4453 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |          -0.0044 |          63.2518 |           9.4784 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |          -0.0062 |          62.6648 |           9.4548 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |          -0.0077 |          62.4648 |           9.5138 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |          -0.0134 |          62.4395 |           9.5225 |
[32m[20221213 23:32:04 @agent_ppo2.py:185][0m |          -0.0076 |          62.2577 |           9.5635 |
[32m[20221213 23:32:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.89
[32m[20221213 23:32:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.84
[32m[20221213 23:32:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.09
[32m[20221213 23:32:04 @agent_ppo2.py:143][0m Total time:      19.54 min
[32m[20221213 23:32:04 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221213 23:32:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2922 --------------------------#
[32m[20221213 23:32:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |           0.0059 |          54.1143 |          10.2144 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0031 |          47.6855 |          10.1895 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0073 |          45.7418 |          10.1323 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0004 |          48.7843 |          10.1630 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0097 |          43.7841 |          10.0852 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0109 |          42.8742 |          10.0929 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |           0.0035 |          47.7767 |          10.0592 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0168 |          41.9619 |          10.0780 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0126 |          41.2956 |          10.0602 |
[32m[20221213 23:32:05 @agent_ppo2.py:185][0m |          -0.0139 |          41.0148 |           9.9895 |
[32m[20221213 23:32:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.59
[32m[20221213 23:32:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.52
[32m[20221213 23:32:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.00
[32m[20221213 23:32:06 @agent_ppo2.py:143][0m Total time:      19.56 min
[32m[20221213 23:32:06 @agent_ppo2.py:145][0m 1890304 total steps have happened
[32m[20221213 23:32:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2923 --------------------------#
[32m[20221213 23:32:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |           0.0140 |          77.4906 |           9.8009 |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |          -0.0004 |          64.7814 |           9.8124 |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |          -0.0051 |          63.6028 |           9.8751 |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |          -0.0071 |          63.0256 |           9.7983 |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |          -0.0053 |          63.5725 |           9.8128 |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |          -0.0039 |          62.1183 |           9.8429 |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |          -0.0090 |          61.7778 |           9.8066 |
[32m[20221213 23:32:06 @agent_ppo2.py:185][0m |          -0.0100 |          61.6328 |           9.8377 |
[32m[20221213 23:32:07 @agent_ppo2.py:185][0m |          -0.0087 |          61.7316 |           9.8356 |
[32m[20221213 23:32:07 @agent_ppo2.py:185][0m |          -0.0104 |          61.1933 |           9.8335 |
[32m[20221213 23:32:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.26
[32m[20221213 23:32:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.62
[32m[20221213 23:32:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.84
[32m[20221213 23:32:07 @agent_ppo2.py:143][0m Total time:      19.58 min
[32m[20221213 23:32:07 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221213 23:32:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2924 --------------------------#
[32m[20221213 23:32:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:07 @agent_ppo2.py:185][0m |           0.0028 |          56.1763 |          10.2154 |
[32m[20221213 23:32:07 @agent_ppo2.py:185][0m |          -0.0006 |          53.4932 |          10.2253 |
[32m[20221213 23:32:07 @agent_ppo2.py:185][0m |           0.0044 |          56.1637 |          10.2516 |
[32m[20221213 23:32:07 @agent_ppo2.py:185][0m |          -0.0056 |          52.1001 |          10.2367 |
[32m[20221213 23:32:07 @agent_ppo2.py:185][0m |          -0.0071 |          51.9131 |          10.2843 |
[32m[20221213 23:32:08 @agent_ppo2.py:185][0m |          -0.0098 |          51.6508 |          10.2997 |
[32m[20221213 23:32:08 @agent_ppo2.py:185][0m |          -0.0111 |          51.3628 |          10.2674 |
[32m[20221213 23:32:08 @agent_ppo2.py:185][0m |          -0.0123 |          51.3359 |          10.2908 |
[32m[20221213 23:32:08 @agent_ppo2.py:185][0m |          -0.0091 |          51.0922 |          10.2646 |
[32m[20221213 23:32:08 @agent_ppo2.py:185][0m |          -0.0132 |          50.8487 |          10.3301 |
[32m[20221213 23:32:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.69
[32m[20221213 23:32:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.89
[32m[20221213 23:32:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.07
[32m[20221213 23:32:08 @agent_ppo2.py:143][0m Total time:      19.60 min
[32m[20221213 23:32:08 @agent_ppo2.py:145][0m 1894400 total steps have happened
[32m[20221213 23:32:08 @agent_ppo2.py:121][0m #------------------------ Iteration 2925 --------------------------#
[32m[20221213 23:32:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:08 @agent_ppo2.py:185][0m |           0.0052 |          66.2287 |           9.8230 |
[32m[20221213 23:32:08 @agent_ppo2.py:185][0m |          -0.0060 |          62.5354 |           9.8886 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0062 |          62.2641 |           9.8446 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0074 |          61.1378 |           9.8791 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0130 |          60.7230 |           9.8307 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0126 |          60.5520 |           9.8626 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0112 |          60.2493 |           9.8436 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0043 |          62.6069 |           9.8241 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0109 |          59.9837 |           9.8242 |
[32m[20221213 23:32:09 @agent_ppo2.py:185][0m |          -0.0123 |          59.8481 |           9.8581 |
[32m[20221213 23:32:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:32:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.02
[32m[20221213 23:32:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.63
[32m[20221213 23:32:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.52
[32m[20221213 23:32:09 @agent_ppo2.py:143][0m Total time:      19.62 min
[32m[20221213 23:32:09 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221213 23:32:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2926 --------------------------#
[32m[20221213 23:32:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0019 |          69.0071 |          10.1636 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0091 |          67.2409 |          10.1326 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0032 |          68.7313 |          10.1053 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0085 |          66.0680 |          10.1590 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0128 |          65.6254 |          10.1333 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0138 |          65.4974 |          10.1495 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0132 |          65.1702 |          10.1716 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0171 |          64.8382 |          10.1691 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0154 |          64.6970 |          10.2253 |
[32m[20221213 23:32:10 @agent_ppo2.py:185][0m |          -0.0141 |          64.3800 |          10.1718 |
[32m[20221213 23:32:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.22
[32m[20221213 23:32:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.59
[32m[20221213 23:32:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.01
[32m[20221213 23:32:11 @agent_ppo2.py:143][0m Total time:      19.64 min
[32m[20221213 23:32:11 @agent_ppo2.py:145][0m 1898496 total steps have happened
[32m[20221213 23:32:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2927 --------------------------#
[32m[20221213 23:32:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:11 @agent_ppo2.py:185][0m |           0.0018 |          66.6930 |           9.8926 |
[32m[20221213 23:32:11 @agent_ppo2.py:185][0m |          -0.0016 |          59.8613 |           9.8547 |
[32m[20221213 23:32:11 @agent_ppo2.py:185][0m |          -0.0030 |          57.3252 |           9.9039 |
[32m[20221213 23:32:11 @agent_ppo2.py:185][0m |          -0.0079 |          56.2013 |           9.9503 |
[32m[20221213 23:32:11 @agent_ppo2.py:185][0m |          -0.0053 |          55.6338 |           9.9176 |
[32m[20221213 23:32:11 @agent_ppo2.py:185][0m |          -0.0099 |          55.5558 |           9.9593 |
[32m[20221213 23:32:11 @agent_ppo2.py:185][0m |          -0.0135 |          54.4413 |           9.9301 |
[32m[20221213 23:32:12 @agent_ppo2.py:185][0m |          -0.0052 |          55.3534 |           9.9507 |
[32m[20221213 23:32:12 @agent_ppo2.py:185][0m |          -0.0144 |          53.6541 |          10.0002 |
[32m[20221213 23:32:12 @agent_ppo2.py:185][0m |          -0.0055 |          53.1000 |           9.9829 |
[32m[20221213 23:32:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.04
[32m[20221213 23:32:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.07
[32m[20221213 23:32:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.30
[32m[20221213 23:32:12 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221213 23:32:12 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221213 23:32:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2928 --------------------------#
[32m[20221213 23:32:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:12 @agent_ppo2.py:185][0m |          -0.0026 |          65.1547 |          10.3770 |
[32m[20221213 23:32:12 @agent_ppo2.py:185][0m |          -0.0076 |          63.0640 |          10.3369 |
[32m[20221213 23:32:12 @agent_ppo2.py:185][0m |          -0.0061 |          62.9879 |          10.2720 |
[32m[20221213 23:32:12 @agent_ppo2.py:185][0m |          -0.0114 |          62.3723 |          10.2722 |
[32m[20221213 23:32:13 @agent_ppo2.py:185][0m |          -0.0145 |          61.9382 |          10.2276 |
[32m[20221213 23:32:13 @agent_ppo2.py:185][0m |          -0.0136 |          61.8137 |          10.1767 |
[32m[20221213 23:32:13 @agent_ppo2.py:185][0m |          -0.0092 |          62.1592 |          10.1431 |
[32m[20221213 23:32:13 @agent_ppo2.py:185][0m |          -0.0128 |          61.7023 |          10.1023 |
[32m[20221213 23:32:13 @agent_ppo2.py:185][0m |          -0.0056 |          63.4031 |          10.0868 |
[32m[20221213 23:32:13 @agent_ppo2.py:185][0m |          -0.0139 |          61.6216 |          10.0847 |
[32m[20221213 23:32:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:32:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.76
[32m[20221213 23:32:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.37
[32m[20221213 23:32:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.90
[32m[20221213 23:32:13 @agent_ppo2.py:143][0m Total time:      19.68 min
[32m[20221213 23:32:13 @agent_ppo2.py:145][0m 1902592 total steps have happened
[32m[20221213 23:32:13 @agent_ppo2.py:121][0m #------------------------ Iteration 2929 --------------------------#
[32m[20221213 23:32:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:13 @agent_ppo2.py:185][0m |           0.0078 |          65.6187 |           9.7088 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0015 |          56.3679 |           9.7367 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0055 |          54.1189 |           9.7104 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0095 |          52.3739 |           9.7351 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0107 |          51.4976 |           9.7132 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0114 |          50.7196 |           9.7340 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0112 |          50.2604 |           9.7415 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0165 |          49.7900 |           9.7570 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0144 |          49.4760 |           9.8093 |
[32m[20221213 23:32:14 @agent_ppo2.py:185][0m |          -0.0128 |          49.1018 |           9.7375 |
[32m[20221213 23:32:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.55
[32m[20221213 23:32:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.05
[32m[20221213 23:32:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.51
[32m[20221213 23:32:14 @agent_ppo2.py:143][0m Total time:      19.71 min
[32m[20221213 23:32:14 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221213 23:32:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2930 --------------------------#
[32m[20221213 23:32:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:32:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |           0.0112 |          69.3422 |           9.3649 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0045 |          59.1233 |           9.4502 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0075 |          57.7954 |           9.4902 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0094 |          56.8959 |           9.5179 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0096 |          56.1481 |           9.5270 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0112 |          55.7286 |           9.5499 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0113 |          55.2538 |           9.5634 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0124 |          54.8424 |           9.5931 |
[32m[20221213 23:32:15 @agent_ppo2.py:185][0m |          -0.0153 |          54.7790 |           9.6214 |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |          -0.0163 |          54.6460 |           9.6431 |
[32m[20221213 23:32:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:32:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.11
[32m[20221213 23:32:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.08
[32m[20221213 23:32:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.97
[32m[20221213 23:32:16 @agent_ppo2.py:143][0m Total time:      19.73 min
[32m[20221213 23:32:16 @agent_ppo2.py:145][0m 1906688 total steps have happened
[32m[20221213 23:32:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2931 --------------------------#
[32m[20221213 23:32:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |           0.0043 |          69.7484 |           9.7435 |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |          -0.0029 |          66.4122 |           9.7183 |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |          -0.0075 |          64.6954 |           9.7387 |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |          -0.0074 |          63.0657 |           9.7267 |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |          -0.0100 |          61.9681 |           9.7406 |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |          -0.0119 |          61.9963 |           9.7565 |
[32m[20221213 23:32:16 @agent_ppo2.py:185][0m |          -0.0117 |          61.1260 |           9.7502 |
[32m[20221213 23:32:17 @agent_ppo2.py:185][0m |          -0.0075 |          60.0910 |           9.7081 |
[32m[20221213 23:32:17 @agent_ppo2.py:185][0m |          -0.0082 |          59.7976 |           9.7646 |
[32m[20221213 23:32:17 @agent_ppo2.py:185][0m |          -0.0054 |          60.6666 |           9.7672 |
[32m[20221213 23:32:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.86
[32m[20221213 23:32:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.63
[32m[20221213 23:32:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.27
[32m[20221213 23:32:17 @agent_ppo2.py:143][0m Total time:      19.75 min
[32m[20221213 23:32:17 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221213 23:32:17 @agent_ppo2.py:121][0m #------------------------ Iteration 2932 --------------------------#
[32m[20221213 23:32:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:17 @agent_ppo2.py:185][0m |           0.0008 |          65.2785 |           9.7565 |
[32m[20221213 23:32:17 @agent_ppo2.py:185][0m |           0.0007 |          63.2075 |           9.7468 |
[32m[20221213 23:32:17 @agent_ppo2.py:185][0m |          -0.0057 |          62.2195 |           9.8019 |
[32m[20221213 23:32:17 @agent_ppo2.py:185][0m |          -0.0075 |          61.2410 |           9.8556 |
[32m[20221213 23:32:18 @agent_ppo2.py:185][0m |          -0.0071 |          60.8177 |           9.8798 |
[32m[20221213 23:32:18 @agent_ppo2.py:185][0m |          -0.0088 |          60.5606 |           9.9480 |
[32m[20221213 23:32:18 @agent_ppo2.py:185][0m |          -0.0118 |          60.2387 |           9.9608 |
[32m[20221213 23:32:18 @agent_ppo2.py:185][0m |          -0.0038 |          61.2237 |          10.0225 |
[32m[20221213 23:32:18 @agent_ppo2.py:185][0m |          -0.0031 |          65.2592 |          10.1020 |
[32m[20221213 23:32:18 @agent_ppo2.py:185][0m |           0.0025 |          64.0427 |          10.1147 |
[32m[20221213 23:32:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.07
[32m[20221213 23:32:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.29
[32m[20221213 23:32:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.44
[32m[20221213 23:32:18 @agent_ppo2.py:143][0m Total time:      19.77 min
[32m[20221213 23:32:18 @agent_ppo2.py:145][0m 1910784 total steps have happened
[32m[20221213 23:32:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2933 --------------------------#
[32m[20221213 23:32:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:18 @agent_ppo2.py:185][0m |          -0.0014 |          59.0108 |          10.3855 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0077 |          52.6199 |          10.4214 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0084 |          50.8729 |          10.4164 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0060 |          49.7163 |          10.4270 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0059 |          49.3029 |          10.3917 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0116 |          48.2163 |          10.3976 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0085 |          48.4720 |          10.4108 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0160 |          47.3302 |          10.4100 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0124 |          47.4164 |          10.3827 |
[32m[20221213 23:32:19 @agent_ppo2.py:185][0m |          -0.0134 |          47.0962 |          10.4058 |
[32m[20221213 23:32:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.25
[32m[20221213 23:32:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.06
[32m[20221213 23:32:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.08
[32m[20221213 23:32:19 @agent_ppo2.py:143][0m Total time:      19.79 min
[32m[20221213 23:32:19 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221213 23:32:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2934 --------------------------#
[32m[20221213 23:32:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |           0.0008 |          60.6792 |           9.8178 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0052 |          58.3254 |           9.7853 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0085 |          56.9526 |           9.7739 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0104 |          56.6788 |           9.8062 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0078 |          55.9101 |           9.7937 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0119 |          55.3035 |           9.7658 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0118 |          54.8170 |           9.7510 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0139 |          54.5171 |           9.7339 |
[32m[20221213 23:32:20 @agent_ppo2.py:185][0m |          -0.0125 |          54.1411 |           9.7873 |
[32m[20221213 23:32:21 @agent_ppo2.py:185][0m |          -0.0138 |          54.2899 |           9.7854 |
[32m[20221213 23:32:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:32:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.42
[32m[20221213 23:32:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.49
[32m[20221213 23:32:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.03
[32m[20221213 23:32:21 @agent_ppo2.py:143][0m Total time:      19.81 min
[32m[20221213 23:32:21 @agent_ppo2.py:145][0m 1914880 total steps have happened
[32m[20221213 23:32:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2935 --------------------------#
[32m[20221213 23:32:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:21 @agent_ppo2.py:185][0m |           0.0020 |          49.1882 |          10.1741 |
[32m[20221213 23:32:21 @agent_ppo2.py:185][0m |          -0.0005 |          45.3184 |          10.2437 |
[32m[20221213 23:32:21 @agent_ppo2.py:185][0m |          -0.0052 |          44.1027 |          10.2532 |
[32m[20221213 23:32:21 @agent_ppo2.py:185][0m |           0.0050 |          44.2501 |          10.2488 |
[32m[20221213 23:32:21 @agent_ppo2.py:185][0m |          -0.0044 |          42.7663 |          10.2719 |
[32m[20221213 23:32:21 @agent_ppo2.py:185][0m |          -0.0108 |          42.2360 |          10.2688 |
[32m[20221213 23:32:22 @agent_ppo2.py:185][0m |          -0.0105 |          41.9008 |          10.2654 |
[32m[20221213 23:32:22 @agent_ppo2.py:185][0m |          -0.0073 |          42.4051 |          10.2945 |
[32m[20221213 23:32:22 @agent_ppo2.py:185][0m |          -0.0133 |          41.3881 |          10.2733 |
[32m[20221213 23:32:22 @agent_ppo2.py:185][0m |          -0.0017 |          44.8499 |          10.2002 |
[32m[20221213 23:32:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.97
[32m[20221213 23:32:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.68
[32m[20221213 23:32:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.71
[32m[20221213 23:32:22 @agent_ppo2.py:143][0m Total time:      19.83 min
[32m[20221213 23:32:22 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221213 23:32:22 @agent_ppo2.py:121][0m #------------------------ Iteration 2936 --------------------------#
[32m[20221213 23:32:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:22 @agent_ppo2.py:185][0m |           0.0028 |          59.7655 |           9.6368 |
[32m[20221213 23:32:22 @agent_ppo2.py:185][0m |          -0.0015 |          54.7724 |           9.6867 |
[32m[20221213 23:32:22 @agent_ppo2.py:185][0m |          -0.0051 |          54.0125 |           9.6368 |
[32m[20221213 23:32:23 @agent_ppo2.py:185][0m |           0.0043 |          59.2559 |           9.6156 |
[32m[20221213 23:32:23 @agent_ppo2.py:185][0m |          -0.0060 |          53.3725 |           9.6637 |
[32m[20221213 23:32:23 @agent_ppo2.py:185][0m |          -0.0171 |          53.3337 |           9.6851 |
[32m[20221213 23:32:23 @agent_ppo2.py:185][0m |          -0.0106 |          52.8279 |           9.6812 |
[32m[20221213 23:32:23 @agent_ppo2.py:185][0m |          -0.0135 |          52.7026 |           9.6724 |
[32m[20221213 23:32:23 @agent_ppo2.py:185][0m |          -0.0127 |          52.3762 |           9.6578 |
[32m[20221213 23:32:23 @agent_ppo2.py:185][0m |          -0.0153 |          52.2551 |           9.6840 |
[32m[20221213 23:32:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.69
[32m[20221213 23:32:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.46
[32m[20221213 23:32:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.64
[32m[20221213 23:32:23 @agent_ppo2.py:143][0m Total time:      19.85 min
[32m[20221213 23:32:23 @agent_ppo2.py:145][0m 1918976 total steps have happened
[32m[20221213 23:32:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2937 --------------------------#
[32m[20221213 23:32:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |           0.0004 |          58.5195 |          10.0321 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0046 |          55.4239 |           9.9781 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0098 |          54.2705 |           9.9586 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0101 |          53.8458 |           9.9174 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0032 |          57.9971 |           9.9432 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0075 |          53.2352 |           9.9696 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0165 |          52.7465 |           9.8809 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0158 |          52.3977 |           9.8613 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0159 |          52.1335 |           9.8710 |
[32m[20221213 23:32:24 @agent_ppo2.py:185][0m |          -0.0167 |          52.1677 |           9.8065 |
[32m[20221213 23:32:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.30
[32m[20221213 23:32:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.38
[32m[20221213 23:32:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.90
[32m[20221213 23:32:24 @agent_ppo2.py:143][0m Total time:      19.87 min
[32m[20221213 23:32:24 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221213 23:32:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2938 --------------------------#
[32m[20221213 23:32:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |           0.0031 |          63.6706 |           9.2877 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0008 |          59.7430 |           9.2826 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0012 |          58.6569 |           9.3309 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0025 |          58.4023 |           9.3537 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0055 |          57.5839 |           9.3699 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0053 |          57.1499 |           9.3887 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0057 |          56.8215 |           9.3727 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0140 |          56.8691 |           9.4380 |
[32m[20221213 23:32:25 @agent_ppo2.py:185][0m |          -0.0088 |          56.6061 |           9.4018 |
[32m[20221213 23:32:26 @agent_ppo2.py:185][0m |           0.0053 |          63.3052 |           9.4010 |
[32m[20221213 23:32:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.23
[32m[20221213 23:32:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.52
[32m[20221213 23:32:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.05
[32m[20221213 23:32:26 @agent_ppo2.py:143][0m Total time:      19.89 min
[32m[20221213 23:32:26 @agent_ppo2.py:145][0m 1923072 total steps have happened
[32m[20221213 23:32:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2939 --------------------------#
[32m[20221213 23:32:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:26 @agent_ppo2.py:185][0m |          -0.0003 |          48.4630 |           9.8072 |
[32m[20221213 23:32:26 @agent_ppo2.py:185][0m |          -0.0052 |          46.7026 |           9.7777 |
[32m[20221213 23:32:26 @agent_ppo2.py:185][0m |          -0.0020 |          46.7698 |           9.7589 |
[32m[20221213 23:32:26 @agent_ppo2.py:185][0m |           0.0004 |          48.3771 |           9.7177 |
[32m[20221213 23:32:26 @agent_ppo2.py:185][0m |          -0.0127 |          45.8716 |           9.7661 |
[32m[20221213 23:32:26 @agent_ppo2.py:185][0m |          -0.0080 |          45.5335 |           9.7595 |
[32m[20221213 23:32:27 @agent_ppo2.py:185][0m |          -0.0115 |          45.3741 |           9.6923 |
[32m[20221213 23:32:27 @agent_ppo2.py:185][0m |          -0.0136 |          45.3106 |           9.6898 |
[32m[20221213 23:32:27 @agent_ppo2.py:185][0m |          -0.0102 |          45.0909 |           9.6295 |
[32m[20221213 23:32:27 @agent_ppo2.py:185][0m |          -0.0098 |          45.0205 |           9.6805 |
[32m[20221213 23:32:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:32:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.44
[32m[20221213 23:32:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.44
[32m[20221213 23:32:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.70
[32m[20221213 23:32:27 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221213 23:32:27 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221213 23:32:27 @agent_ppo2.py:121][0m #------------------------ Iteration 2940 --------------------------#
[32m[20221213 23:32:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:32:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:27 @agent_ppo2.py:185][0m |           0.0002 |          61.4421 |           9.6762 |
[32m[20221213 23:32:27 @agent_ppo2.py:185][0m |           0.0154 |          69.5979 |           9.6420 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0022 |          59.3331 |           9.6851 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0067 |          57.9319 |           9.6517 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0085 |          57.4019 |           9.6695 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0084 |          57.2915 |           9.6776 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0117 |          57.1828 |           9.6515 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0090 |          56.8700 |           9.6837 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0126 |          56.7505 |           9.6475 |
[32m[20221213 23:32:28 @agent_ppo2.py:185][0m |          -0.0133 |          56.5500 |           9.6853 |
[32m[20221213 23:32:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.18
[32m[20221213 23:32:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.70
[32m[20221213 23:32:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.90
[32m[20221213 23:32:28 @agent_ppo2.py:143][0m Total time:      19.94 min
[32m[20221213 23:32:28 @agent_ppo2.py:145][0m 1927168 total steps have happened
[32m[20221213 23:32:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2941 --------------------------#
[32m[20221213 23:32:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |           0.0082 |          61.3100 |           9.6009 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0116 |          52.5696 |           9.6049 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |           0.0150 |          62.7789 |           9.5915 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0130 |          49.8689 |           9.5414 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0166 |          49.0325 |           9.6072 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0148 |          48.3022 |           9.6232 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0166 |          47.8784 |           9.6241 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0150 |          47.7923 |           9.6392 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0221 |          47.3156 |           9.6202 |
[32m[20221213 23:32:29 @agent_ppo2.py:185][0m |          -0.0247 |          47.0823 |           9.6417 |
[32m[20221213 23:32:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:32:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.69
[32m[20221213 23:32:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.54
[32m[20221213 23:32:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.43
[32m[20221213 23:32:30 @agent_ppo2.py:143][0m Total time:      19.96 min
[32m[20221213 23:32:30 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221213 23:32:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2942 --------------------------#
[32m[20221213 23:32:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0034 |          65.6946 |           9.7396 |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0079 |          63.6538 |           9.7359 |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0120 |          62.5281 |           9.7936 |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0070 |          62.7058 |           9.8123 |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0098 |          61.5191 |           9.8212 |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0006 |          69.0553 |           9.8488 |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0143 |          60.9912 |           9.8342 |
[32m[20221213 23:32:30 @agent_ppo2.py:185][0m |          -0.0155 |          60.4307 |           9.8507 |
[32m[20221213 23:32:31 @agent_ppo2.py:185][0m |          -0.0130 |          60.1686 |           9.8561 |
[32m[20221213 23:32:31 @agent_ppo2.py:185][0m |          -0.0121 |          60.0461 |           9.8675 |
[32m[20221213 23:32:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.67
[32m[20221213 23:32:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.56
[32m[20221213 23:32:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.46
[32m[20221213 23:32:31 @agent_ppo2.py:143][0m Total time:      19.98 min
[32m[20221213 23:32:31 @agent_ppo2.py:145][0m 1931264 total steps have happened
[32m[20221213 23:32:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2943 --------------------------#
[32m[20221213 23:32:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:31 @agent_ppo2.py:185][0m |           0.0013 |          60.7992 |           9.7431 |
[32m[20221213 23:32:31 @agent_ppo2.py:185][0m |          -0.0037 |          57.8968 |           9.7641 |
[32m[20221213 23:32:31 @agent_ppo2.py:185][0m |          -0.0075 |          57.0293 |           9.7362 |
[32m[20221213 23:32:31 @agent_ppo2.py:185][0m |          -0.0061 |          56.2779 |           9.7635 |
[32m[20221213 23:32:31 @agent_ppo2.py:185][0m |          -0.0053 |          56.0587 |           9.6975 |
[32m[20221213 23:32:32 @agent_ppo2.py:185][0m |          -0.0092 |          55.5612 |           9.7086 |
[32m[20221213 23:32:32 @agent_ppo2.py:185][0m |          -0.0103 |          55.1064 |           9.6807 |
[32m[20221213 23:32:32 @agent_ppo2.py:185][0m |          -0.0139 |          54.9287 |           9.6960 |
[32m[20221213 23:32:32 @agent_ppo2.py:185][0m |          -0.0021 |          60.1932 |           9.7011 |
[32m[20221213 23:32:32 @agent_ppo2.py:185][0m |          -0.0099 |          54.5269 |           9.6453 |
[32m[20221213 23:32:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.32
[32m[20221213 23:32:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.78
[32m[20221213 23:32:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.05
[32m[20221213 23:32:32 @agent_ppo2.py:143][0m Total time:      20.00 min
[32m[20221213 23:32:32 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221213 23:32:32 @agent_ppo2.py:121][0m #------------------------ Iteration 2944 --------------------------#
[32m[20221213 23:32:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:32 @agent_ppo2.py:185][0m |           0.0017 |          45.7589 |          10.0083 |
[32m[20221213 23:32:32 @agent_ppo2.py:185][0m |          -0.0041 |          41.8639 |          10.0307 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0046 |          40.7538 |          10.0480 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0038 |          39.8443 |          10.0454 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0118 |          39.2166 |          10.0237 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0099 |          38.9200 |          10.0125 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0117 |          38.4635 |          10.0454 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0172 |          38.2522 |          10.0471 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0152 |          37.8845 |          10.0670 |
[32m[20221213 23:32:33 @agent_ppo2.py:185][0m |          -0.0144 |          38.0904 |          10.0356 |
[32m[20221213 23:32:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:32:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.71
[32m[20221213 23:32:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.27
[32m[20221213 23:32:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.83
[32m[20221213 23:32:33 @agent_ppo2.py:143][0m Total time:      20.02 min
[32m[20221213 23:32:33 @agent_ppo2.py:145][0m 1935360 total steps have happened
[32m[20221213 23:32:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2945 --------------------------#
[32m[20221213 23:32:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |           0.0012 |          69.9431 |          10.1318 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0046 |          65.8408 |          10.1351 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0059 |          64.5568 |          10.1205 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0107 |          63.9620 |          10.0941 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0105 |          63.4914 |          10.1181 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0069 |          62.8702 |          10.1197 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0069 |          62.7638 |          10.0880 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0090 |          62.2186 |          10.1023 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0138 |          62.0111 |          10.1180 |
[32m[20221213 23:32:34 @agent_ppo2.py:185][0m |          -0.0124 |          61.6400 |          10.0987 |
[32m[20221213 23:32:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.66
[32m[20221213 23:32:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.13
[32m[20221213 23:32:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.90
[32m[20221213 23:32:35 @agent_ppo2.py:143][0m Total time:      20.04 min
[32m[20221213 23:32:35 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221213 23:32:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2946 --------------------------#
[32m[20221213 23:32:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:35 @agent_ppo2.py:185][0m |           0.0047 |          55.4039 |           9.6503 |
[32m[20221213 23:32:35 @agent_ppo2.py:185][0m |           0.0078 |          55.4058 |           9.6091 |
[32m[20221213 23:32:35 @agent_ppo2.py:185][0m |          -0.0051 |          47.9754 |           9.6461 |
[32m[20221213 23:32:35 @agent_ppo2.py:185][0m |          -0.0022 |          46.3611 |           9.6185 |
[32m[20221213 23:32:35 @agent_ppo2.py:185][0m |          -0.0018 |          45.8212 |           9.5725 |
[32m[20221213 23:32:35 @agent_ppo2.py:185][0m |          -0.0072 |          45.1442 |           9.5910 |
[32m[20221213 23:32:35 @agent_ppo2.py:185][0m |          -0.0099 |          44.8952 |           9.5621 |
[32m[20221213 23:32:36 @agent_ppo2.py:185][0m |          -0.0111 |          44.4044 |           9.5244 |
[32m[20221213 23:32:36 @agent_ppo2.py:185][0m |          -0.0114 |          43.9407 |           9.5142 |
[32m[20221213 23:32:36 @agent_ppo2.py:185][0m |          -0.0106 |          43.7944 |           9.4974 |
[32m[20221213 23:32:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.62
[32m[20221213 23:32:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.20
[32m[20221213 23:32:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.61
[32m[20221213 23:32:36 @agent_ppo2.py:143][0m Total time:      20.06 min
[32m[20221213 23:32:36 @agent_ppo2.py:145][0m 1939456 total steps have happened
[32m[20221213 23:32:36 @agent_ppo2.py:121][0m #------------------------ Iteration 2947 --------------------------#
[32m[20221213 23:32:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:36 @agent_ppo2.py:185][0m |           0.0098 |          36.0583 |           9.7776 |
[32m[20221213 23:32:36 @agent_ppo2.py:185][0m |          -0.0053 |          28.4581 |           9.7142 |
[32m[20221213 23:32:36 @agent_ppo2.py:185][0m |          -0.0062 |          26.4289 |           9.7320 |
[32m[20221213 23:32:36 @agent_ppo2.py:185][0m |          -0.0142 |          25.1325 |           9.7207 |
[32m[20221213 23:32:37 @agent_ppo2.py:185][0m |          -0.0160 |          24.4917 |           9.6811 |
[32m[20221213 23:32:37 @agent_ppo2.py:185][0m |          -0.0151 |          23.7078 |           9.7130 |
[32m[20221213 23:32:37 @agent_ppo2.py:185][0m |          -0.0166 |          24.4811 |           9.6325 |
[32m[20221213 23:32:37 @agent_ppo2.py:185][0m |          -0.0145 |          23.0521 |           9.6237 |
[32m[20221213 23:32:37 @agent_ppo2.py:185][0m |          -0.0207 |          22.5672 |           9.6383 |
[32m[20221213 23:32:37 @agent_ppo2.py:185][0m |          -0.0142 |          22.5017 |           9.6091 |
[32m[20221213 23:32:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.30
[32m[20221213 23:32:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.15
[32m[20221213 23:32:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.63
[32m[20221213 23:32:37 @agent_ppo2.py:143][0m Total time:      20.08 min
[32m[20221213 23:32:37 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221213 23:32:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2948 --------------------------#
[32m[20221213 23:32:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:37 @agent_ppo2.py:185][0m |           0.0011 |          63.8351 |           9.2329 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0085 |          59.5336 |           9.2669 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0090 |          58.4310 |           9.1890 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0080 |          59.4094 |           9.1359 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0089 |          57.7156 |           9.1691 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0068 |          57.3670 |           9.1258 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0128 |          57.1462 |           9.1082 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0079 |          57.5262 |           9.0963 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0196 |          56.7144 |           9.1231 |
[32m[20221213 23:32:38 @agent_ppo2.py:185][0m |          -0.0166 |          56.8839 |           9.1249 |
[32m[20221213 23:32:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.32
[32m[20221213 23:32:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.05
[32m[20221213 23:32:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.95
[32m[20221213 23:32:38 @agent_ppo2.py:143][0m Total time:      20.11 min
[32m[20221213 23:32:38 @agent_ppo2.py:145][0m 1943552 total steps have happened
[32m[20221213 23:32:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2949 --------------------------#
[32m[20221213 23:32:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0025 |          56.0286 |           9.2399 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0099 |          52.4379 |           9.2755 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0104 |          50.7146 |           9.2096 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0104 |          49.8133 |           9.2583 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0095 |          48.9627 |           9.2506 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0081 |          48.4971 |           9.2753 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0143 |          48.2001 |           9.3240 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0020 |          53.7738 |           9.3305 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0080 |          47.7232 |           9.3388 |
[32m[20221213 23:32:39 @agent_ppo2.py:185][0m |          -0.0046 |          49.7897 |           9.4135 |
[32m[20221213 23:32:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:32:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.75
[32m[20221213 23:32:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.30
[32m[20221213 23:32:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.18
[32m[20221213 23:32:40 @agent_ppo2.py:143][0m Total time:      20.13 min
[32m[20221213 23:32:40 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221213 23:32:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2950 --------------------------#
[32m[20221213 23:32:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:32:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:40 @agent_ppo2.py:185][0m |          -0.0007 |          56.0402 |           9.6012 |
[32m[20221213 23:32:40 @agent_ppo2.py:185][0m |          -0.0039 |          52.5287 |           9.6447 |
[32m[20221213 23:32:40 @agent_ppo2.py:185][0m |          -0.0048 |          51.9637 |           9.6411 |
[32m[20221213 23:32:40 @agent_ppo2.py:185][0m |          -0.0092 |          51.0429 |           9.6403 |
[32m[20221213 23:32:40 @agent_ppo2.py:185][0m |          -0.0092 |          50.7678 |           9.7075 |
[32m[20221213 23:32:40 @agent_ppo2.py:185][0m |          -0.0034 |          51.7405 |           9.6798 |
[32m[20221213 23:32:40 @agent_ppo2.py:185][0m |          -0.0097 |          49.9043 |           9.7598 |
[32m[20221213 23:32:41 @agent_ppo2.py:185][0m |          -0.0117 |          49.7361 |           9.7594 |
[32m[20221213 23:32:41 @agent_ppo2.py:185][0m |          -0.0074 |          51.2255 |           9.7466 |
[32m[20221213 23:32:41 @agent_ppo2.py:185][0m |          -0.0148 |          49.4846 |           9.7508 |
[32m[20221213 23:32:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.78
[32m[20221213 23:32:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.81
[32m[20221213 23:32:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.51
[32m[20221213 23:32:41 @agent_ppo2.py:143][0m Total time:      20.15 min
[32m[20221213 23:32:41 @agent_ppo2.py:145][0m 1947648 total steps have happened
[32m[20221213 23:32:41 @agent_ppo2.py:121][0m #------------------------ Iteration 2951 --------------------------#
[32m[20221213 23:32:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:41 @agent_ppo2.py:185][0m |           0.0145 |          57.4262 |           9.5985 |
[32m[20221213 23:32:41 @agent_ppo2.py:185][0m |          -0.0032 |          51.9414 |           9.6238 |
[32m[20221213 23:32:41 @agent_ppo2.py:185][0m |          -0.0053 |          51.3197 |           9.6731 |
[32m[20221213 23:32:41 @agent_ppo2.py:185][0m |          -0.0070 |          50.6344 |           9.6435 |
[32m[20221213 23:32:42 @agent_ppo2.py:185][0m |          -0.0085 |          50.3045 |           9.6405 |
[32m[20221213 23:32:42 @agent_ppo2.py:185][0m |          -0.0068 |          50.3434 |           9.6593 |
[32m[20221213 23:32:42 @agent_ppo2.py:185][0m |          -0.0064 |          49.7280 |           9.6396 |
[32m[20221213 23:32:42 @agent_ppo2.py:185][0m |          -0.0108 |          49.4599 |           9.6857 |
[32m[20221213 23:32:42 @agent_ppo2.py:185][0m |          -0.0116 |          49.4294 |           9.6682 |
[32m[20221213 23:32:42 @agent_ppo2.py:185][0m |          -0.0107 |          49.1634 |           9.6629 |
[32m[20221213 23:32:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.66
[32m[20221213 23:32:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.72
[32m[20221213 23:32:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.63
[32m[20221213 23:32:42 @agent_ppo2.py:143][0m Total time:      20.17 min
[32m[20221213 23:32:42 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221213 23:32:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2952 --------------------------#
[32m[20221213 23:32:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:42 @agent_ppo2.py:185][0m |           0.0019 |          44.5274 |           9.4073 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |           0.0013 |          39.9622 |           9.3905 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0085 |          38.1199 |           9.3401 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0140 |          37.0424 |           9.3639 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0157 |          36.3627 |           9.3250 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0096 |          35.7990 |           9.3140 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0166 |          35.2546 |           9.3316 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0144 |          35.0403 |           9.2753 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0100 |          34.6849 |           9.2630 |
[32m[20221213 23:32:43 @agent_ppo2.py:185][0m |          -0.0143 |          34.1550 |           9.2283 |
[32m[20221213 23:32:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:32:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.46
[32m[20221213 23:32:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.63
[32m[20221213 23:32:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.47
[32m[20221213 23:32:43 @agent_ppo2.py:143][0m Total time:      20.19 min
[32m[20221213 23:32:43 @agent_ppo2.py:145][0m 1951744 total steps have happened
[32m[20221213 23:32:43 @agent_ppo2.py:121][0m #------------------------ Iteration 2953 --------------------------#
[32m[20221213 23:32:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |           0.0024 |          48.0673 |           8.7158 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0048 |          44.1909 |           8.7540 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0003 |          44.8390 |           8.7046 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0075 |          42.0565 |           8.7206 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0138 |          41.6840 |           8.7182 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0052 |          41.4792 |           8.7007 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0155 |          40.7627 |           8.7259 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0144 |          40.7197 |           8.7376 |
[32m[20221213 23:32:44 @agent_ppo2.py:185][0m |          -0.0148 |          40.3195 |           8.6841 |
[32m[20221213 23:32:45 @agent_ppo2.py:185][0m |          -0.0176 |          40.2570 |           8.7153 |
[32m[20221213 23:32:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:32:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.40
[32m[20221213 23:32:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.57
[32m[20221213 23:32:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.60
[32m[20221213 23:32:45 @agent_ppo2.py:143][0m Total time:      20.21 min
[32m[20221213 23:32:45 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221213 23:32:45 @agent_ppo2.py:121][0m #------------------------ Iteration 2954 --------------------------#
[32m[20221213 23:32:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:45 @agent_ppo2.py:185][0m |          -0.0027 |          55.0305 |           9.0900 |
[32m[20221213 23:32:45 @agent_ppo2.py:185][0m |          -0.0071 |          50.8560 |           9.0117 |
[32m[20221213 23:32:45 @agent_ppo2.py:185][0m |          -0.0105 |          49.4749 |           8.9979 |
[32m[20221213 23:32:45 @agent_ppo2.py:185][0m |          -0.0128 |          48.9446 |           8.9859 |
[32m[20221213 23:32:45 @agent_ppo2.py:185][0m |          -0.0140 |          48.1602 |           8.9637 |
[32m[20221213 23:32:45 @agent_ppo2.py:185][0m |          -0.0124 |          47.8079 |           8.9723 |
[32m[20221213 23:32:46 @agent_ppo2.py:185][0m |          -0.0090 |          47.4390 |           8.9387 |
[32m[20221213 23:32:46 @agent_ppo2.py:185][0m |          -0.0175 |          47.1239 |           8.9612 |
[32m[20221213 23:32:46 @agent_ppo2.py:185][0m |          -0.0112 |          46.8056 |           8.9634 |
[32m[20221213 23:32:46 @agent_ppo2.py:185][0m |          -0.0145 |          46.6141 |           8.9277 |
[32m[20221213 23:32:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:32:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.30
[32m[20221213 23:32:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.74
[32m[20221213 23:32:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.32
[32m[20221213 23:32:46 @agent_ppo2.py:143][0m Total time:      20.23 min
[32m[20221213 23:32:46 @agent_ppo2.py:145][0m 1955840 total steps have happened
[32m[20221213 23:32:46 @agent_ppo2.py:121][0m #------------------------ Iteration 2955 --------------------------#
[32m[20221213 23:32:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:46 @agent_ppo2.py:185][0m |          -0.0001 |          65.3152 |           8.6664 |
[32m[20221213 23:32:46 @agent_ppo2.py:185][0m |           0.0000 |          59.3671 |           8.6375 |
[32m[20221213 23:32:46 @agent_ppo2.py:185][0m |          -0.0035 |          55.9817 |           8.7006 |
[32m[20221213 23:32:47 @agent_ppo2.py:185][0m |          -0.0035 |          54.7102 |           8.6937 |
[32m[20221213 23:32:47 @agent_ppo2.py:185][0m |          -0.0083 |          53.8116 |           8.6951 |
[32m[20221213 23:32:47 @agent_ppo2.py:185][0m |          -0.0142 |          53.2806 |           8.6569 |
[32m[20221213 23:32:47 @agent_ppo2.py:185][0m |          -0.0142 |          53.0266 |           8.6494 |
[32m[20221213 23:32:47 @agent_ppo2.py:185][0m |          -0.0137 |          52.4226 |           8.6851 |
[32m[20221213 23:32:47 @agent_ppo2.py:185][0m |          -0.0136 |          51.9325 |           8.6815 |
[32m[20221213 23:32:47 @agent_ppo2.py:185][0m |          -0.0112 |          51.8837 |           8.6548 |
[32m[20221213 23:32:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.40
[32m[20221213 23:32:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.31
[32m[20221213 23:32:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.52
[32m[20221213 23:32:47 @agent_ppo2.py:143][0m Total time:      20.25 min
[32m[20221213 23:32:47 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221213 23:32:47 @agent_ppo2.py:121][0m #------------------------ Iteration 2956 --------------------------#
[32m[20221213 23:32:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |           0.0029 |          65.7072 |           9.2558 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0035 |          62.6447 |           9.3060 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0017 |          61.4919 |           9.3275 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0010 |          60.7476 |           9.3008 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0084 |          60.0124 |           9.3566 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0103 |          60.4563 |           9.3659 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0125 |          59.1699 |           9.4822 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0107 |          59.2074 |           9.4708 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0088 |          58.9808 |           9.4586 |
[32m[20221213 23:32:48 @agent_ppo2.py:185][0m |          -0.0131 |          58.1930 |           9.5200 |
[32m[20221213 23:32:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.47
[32m[20221213 23:32:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.26
[32m[20221213 23:32:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.76
[32m[20221213 23:32:48 @agent_ppo2.py:143][0m Total time:      20.27 min
[32m[20221213 23:32:48 @agent_ppo2.py:145][0m 1959936 total steps have happened
[32m[20221213 23:32:48 @agent_ppo2.py:121][0m #------------------------ Iteration 2957 --------------------------#
[32m[20221213 23:32:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |          -0.0022 |          48.7959 |           9.0314 |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |          -0.0039 |          45.0344 |           9.0165 |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |           0.0048 |          48.7094 |           9.0381 |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |          -0.0051 |          43.7374 |           9.0599 |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |          -0.0053 |          43.6949 |           9.0966 |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |          -0.0128 |          42.1172 |           9.0387 |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |          -0.0076 |          41.8333 |           9.0605 |
[32m[20221213 23:32:49 @agent_ppo2.py:185][0m |          -0.0139 |          41.0674 |           9.0679 |
[32m[20221213 23:32:50 @agent_ppo2.py:185][0m |          -0.0127 |          40.6385 |           9.0587 |
[32m[20221213 23:32:50 @agent_ppo2.py:185][0m |          -0.0155 |          40.3405 |           9.0585 |
[32m[20221213 23:32:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.31
[32m[20221213 23:32:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.09
[32m[20221213 23:32:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.59
[32m[20221213 23:32:50 @agent_ppo2.py:143][0m Total time:      20.30 min
[32m[20221213 23:32:50 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221213 23:32:50 @agent_ppo2.py:121][0m #------------------------ Iteration 2958 --------------------------#
[32m[20221213 23:32:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:50 @agent_ppo2.py:185][0m |           0.0106 |          64.1354 |           9.2977 |
[32m[20221213 23:32:50 @agent_ppo2.py:185][0m |          -0.0029 |          53.0664 |           9.3177 |
[32m[20221213 23:32:50 @agent_ppo2.py:185][0m |          -0.0049 |          51.9403 |           9.2629 |
[32m[20221213 23:32:50 @agent_ppo2.py:185][0m |           0.0009 |          51.5023 |           9.2268 |
[32m[20221213 23:32:50 @agent_ppo2.py:185][0m |          -0.0089 |          50.2993 |           9.2011 |
[32m[20221213 23:32:51 @agent_ppo2.py:185][0m |          -0.0002 |          55.6793 |           9.2244 |
[32m[20221213 23:32:51 @agent_ppo2.py:185][0m |          -0.0125 |          49.3913 |           9.2382 |
[32m[20221213 23:32:51 @agent_ppo2.py:185][0m |          -0.0113 |          49.1864 |           9.2123 |
[32m[20221213 23:32:51 @agent_ppo2.py:185][0m |          -0.0111 |          48.7559 |           9.2164 |
[32m[20221213 23:32:51 @agent_ppo2.py:185][0m |          -0.0106 |          48.3546 |           9.1752 |
[32m[20221213 23:32:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:32:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.61
[32m[20221213 23:32:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.37
[32m[20221213 23:32:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.44
[32m[20221213 23:32:51 @agent_ppo2.py:143][0m Total time:      20.32 min
[32m[20221213 23:32:51 @agent_ppo2.py:145][0m 1964032 total steps have happened
[32m[20221213 23:32:51 @agent_ppo2.py:121][0m #------------------------ Iteration 2959 --------------------------#
[32m[20221213 23:32:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:51 @agent_ppo2.py:185][0m |          -0.0010 |          46.9681 |           9.1257 |
[32m[20221213 23:32:51 @agent_ppo2.py:185][0m |           0.0021 |          44.0182 |           9.0844 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0051 |          42.8628 |           9.0609 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0024 |          42.0255 |           9.0602 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0108 |          41.4139 |           9.0690 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0090 |          41.1121 |           9.0432 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0131 |          40.7559 |           9.0420 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0129 |          40.5149 |           9.0346 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0140 |          40.2271 |           9.0226 |
[32m[20221213 23:32:52 @agent_ppo2.py:185][0m |          -0.0095 |          40.0416 |           9.0234 |
[32m[20221213 23:32:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.12
[32m[20221213 23:32:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.45
[32m[20221213 23:32:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.01
[32m[20221213 23:32:52 @agent_ppo2.py:143][0m Total time:      20.34 min
[32m[20221213 23:32:52 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221213 23:32:52 @agent_ppo2.py:121][0m #------------------------ Iteration 2960 --------------------------#
[32m[20221213 23:32:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:32:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |           0.0005 |          69.0118 |           9.0003 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0064 |          66.3055 |           9.0545 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0094 |          65.3149 |           9.0179 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0024 |          65.5221 |           9.0128 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0098 |          64.0469 |           9.0102 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0119 |          63.6376 |           9.0016 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0110 |          63.2274 |           9.0041 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0141 |          63.1401 |           8.9965 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0151 |          62.5986 |           8.9510 |
[32m[20221213 23:32:53 @agent_ppo2.py:185][0m |          -0.0142 |          62.3338 |           8.9736 |
[32m[20221213 23:32:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.35
[32m[20221213 23:32:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.43
[32m[20221213 23:32:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.22
[32m[20221213 23:32:54 @agent_ppo2.py:143][0m Total time:      20.36 min
[32m[20221213 23:32:54 @agent_ppo2.py:145][0m 1968128 total steps have happened
[32m[20221213 23:32:54 @agent_ppo2.py:121][0m #------------------------ Iteration 2961 --------------------------#
[32m[20221213 23:32:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |           0.0033 |          57.2118 |           8.9938 |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |          -0.0011 |          54.0436 |           8.9996 |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |          -0.0075 |          53.2921 |           9.0242 |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |          -0.0086 |          52.8995 |           8.9220 |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |          -0.0082 |          52.5060 |           8.9246 |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |           0.0015 |          57.3429 |           8.9321 |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |          -0.0096 |          51.9135 |           8.9478 |
[32m[20221213 23:32:54 @agent_ppo2.py:185][0m |          -0.0118 |          51.4950 |           8.9364 |
[32m[20221213 23:32:55 @agent_ppo2.py:185][0m |          -0.0126 |          51.4075 |           8.9692 |
[32m[20221213 23:32:55 @agent_ppo2.py:185][0m |          -0.0176 |          51.3829 |           8.9242 |
[32m[20221213 23:32:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.79
[32m[20221213 23:32:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.36
[32m[20221213 23:32:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.30
[32m[20221213 23:32:55 @agent_ppo2.py:143][0m Total time:      20.38 min
[32m[20221213 23:32:55 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221213 23:32:55 @agent_ppo2.py:121][0m #------------------------ Iteration 2962 --------------------------#
[32m[20221213 23:32:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:55 @agent_ppo2.py:185][0m |          -0.0036 |          58.3152 |           9.1116 |
[32m[20221213 23:32:55 @agent_ppo2.py:185][0m |          -0.0078 |          54.4544 |           9.1305 |
[32m[20221213 23:32:55 @agent_ppo2.py:185][0m |          -0.0094 |          53.4092 |           9.1242 |
[32m[20221213 23:32:55 @agent_ppo2.py:185][0m |          -0.0089 |          52.7218 |           9.1076 |
[32m[20221213 23:32:55 @agent_ppo2.py:185][0m |          -0.0165 |          52.4291 |           9.0667 |
[32m[20221213 23:32:56 @agent_ppo2.py:185][0m |          -0.0140 |          51.8703 |           9.0818 |
[32m[20221213 23:32:56 @agent_ppo2.py:185][0m |          -0.0140 |          51.5049 |           9.0681 |
[32m[20221213 23:32:56 @agent_ppo2.py:185][0m |          -0.0114 |          51.2561 |           9.0881 |
[32m[20221213 23:32:56 @agent_ppo2.py:185][0m |          -0.0097 |          52.1098 |           9.0244 |
[32m[20221213 23:32:56 @agent_ppo2.py:185][0m |          -0.0187 |          50.7902 |           9.0635 |
[32m[20221213 23:32:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:32:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.68
[32m[20221213 23:32:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.69
[32m[20221213 23:32:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.11
[32m[20221213 23:32:56 @agent_ppo2.py:143][0m Total time:      20.40 min
[32m[20221213 23:32:56 @agent_ppo2.py:145][0m 1972224 total steps have happened
[32m[20221213 23:32:56 @agent_ppo2.py:121][0m #------------------------ Iteration 2963 --------------------------#
[32m[20221213 23:32:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:56 @agent_ppo2.py:185][0m |           0.0013 |          57.1912 |           8.3013 |
[32m[20221213 23:32:56 @agent_ppo2.py:185][0m |          -0.0006 |          51.1523 |           8.3473 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0075 |          48.3675 |           8.3729 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0128 |          46.7507 |           8.3957 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0133 |          45.4248 |           8.3950 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0114 |          44.0306 |           8.4312 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0122 |          43.5693 |           8.4562 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0189 |          43.1949 |           8.4580 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0173 |          42.6868 |           8.5047 |
[32m[20221213 23:32:57 @agent_ppo2.py:185][0m |          -0.0176 |          42.3155 |           8.5113 |
[32m[20221213 23:32:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:32:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.87
[32m[20221213 23:32:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.88
[32m[20221213 23:32:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.07
[32m[20221213 23:32:57 @agent_ppo2.py:143][0m Total time:      20.42 min
[32m[20221213 23:32:57 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221213 23:32:57 @agent_ppo2.py:121][0m #------------------------ Iteration 2964 --------------------------#
[32m[20221213 23:32:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:32:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |          -0.0008 |          73.3737 |           8.9074 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |          -0.0051 |          68.9768 |           8.8296 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |           0.0134 |          78.0535 |           8.8303 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |          -0.0072 |          66.5066 |           8.8686 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |          -0.0002 |          71.9396 |           8.9326 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |           0.0003 |          70.2528 |           8.9375 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |          -0.0111 |          63.4473 |           8.9892 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |          -0.0110 |          63.1579 |           8.8875 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |          -0.0122 |          62.7644 |           8.9090 |
[32m[20221213 23:32:58 @agent_ppo2.py:185][0m |           0.0022 |          69.5334 |           8.9654 |
[32m[20221213 23:32:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:32:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.73
[32m[20221213 23:32:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.50
[32m[20221213 23:32:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.11
[32m[20221213 23:32:59 @agent_ppo2.py:143][0m Total time:      20.44 min
[32m[20221213 23:32:59 @agent_ppo2.py:145][0m 1976320 total steps have happened
[32m[20221213 23:32:59 @agent_ppo2.py:121][0m #------------------------ Iteration 2965 --------------------------#
[32m[20221213 23:32:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:32:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:32:59 @agent_ppo2.py:185][0m |           0.0009 |          64.5581 |           8.9037 |
[32m[20221213 23:32:59 @agent_ppo2.py:185][0m |           0.0101 |          68.1775 |           8.8666 |
[32m[20221213 23:32:59 @agent_ppo2.py:185][0m |          -0.0036 |          55.0760 |           8.8750 |
[32m[20221213 23:32:59 @agent_ppo2.py:185][0m |          -0.0046 |          52.5616 |           8.8856 |
[32m[20221213 23:32:59 @agent_ppo2.py:185][0m |          -0.0059 |          50.7765 |           8.8794 |
[32m[20221213 23:32:59 @agent_ppo2.py:185][0m |          -0.0057 |          50.0574 |           8.8508 |
[32m[20221213 23:32:59 @agent_ppo2.py:185][0m |          -0.0095 |          48.8714 |           8.8335 |
[32m[20221213 23:33:00 @agent_ppo2.py:185][0m |          -0.0048 |          48.7236 |           8.8594 |
[32m[20221213 23:33:00 @agent_ppo2.py:185][0m |          -0.0119 |          47.8238 |           8.8985 |
[32m[20221213 23:33:00 @agent_ppo2.py:185][0m |          -0.0071 |          47.5476 |           8.8207 |
[32m[20221213 23:33:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.40
[32m[20221213 23:33:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.52
[32m[20221213 23:33:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.08
[32m[20221213 23:33:00 @agent_ppo2.py:143][0m Total time:      20.46 min
[32m[20221213 23:33:00 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221213 23:33:00 @agent_ppo2.py:121][0m #------------------------ Iteration 2966 --------------------------#
[32m[20221213 23:33:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:00 @agent_ppo2.py:185][0m |           0.0025 |          58.8896 |           8.9342 |
[32m[20221213 23:33:00 @agent_ppo2.py:185][0m |          -0.0031 |          46.0076 |           8.9381 |
[32m[20221213 23:33:00 @agent_ppo2.py:185][0m |          -0.0054 |          44.7529 |           8.8988 |
[32m[20221213 23:33:00 @agent_ppo2.py:185][0m |          -0.0122 |          43.8528 |           8.8915 |
[32m[20221213 23:33:01 @agent_ppo2.py:185][0m |          -0.0083 |          43.2401 |           8.8434 |
[32m[20221213 23:33:01 @agent_ppo2.py:185][0m |          -0.0137 |          43.0154 |           8.8307 |
[32m[20221213 23:33:01 @agent_ppo2.py:185][0m |          -0.0026 |          43.8999 |           8.8234 |
[32m[20221213 23:33:01 @agent_ppo2.py:185][0m |          -0.0070 |          42.2858 |           8.7979 |
[32m[20221213 23:33:01 @agent_ppo2.py:185][0m |          -0.0119 |          42.0535 |           8.7662 |
[32m[20221213 23:33:01 @agent_ppo2.py:185][0m |          -0.0126 |          41.4096 |           8.7096 |
[32m[20221213 23:33:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.78
[32m[20221213 23:33:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.93
[32m[20221213 23:33:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.57
[32m[20221213 23:33:01 @agent_ppo2.py:143][0m Total time:      20.49 min
[32m[20221213 23:33:01 @agent_ppo2.py:145][0m 1980416 total steps have happened
[32m[20221213 23:33:01 @agent_ppo2.py:121][0m #------------------------ Iteration 2967 --------------------------#
[32m[20221213 23:33:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:01 @agent_ppo2.py:185][0m |           0.0031 |          47.2352 |           8.3854 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |           0.0007 |          40.5914 |           8.3590 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0059 |          38.7026 |           8.3542 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0047 |          37.9473 |           8.4083 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0094 |          37.4362 |           8.3788 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0053 |          37.7711 |           8.4493 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0121 |          36.4229 |           8.4438 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0104 |          35.7979 |           8.4247 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0178 |          35.3261 |           8.4704 |
[32m[20221213 23:33:02 @agent_ppo2.py:185][0m |          -0.0142 |          35.2002 |           8.4691 |
[32m[20221213 23:33:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.59
[32m[20221213 23:33:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.62
[32m[20221213 23:33:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.47
[32m[20221213 23:33:02 @agent_ppo2.py:143][0m Total time:      20.51 min
[32m[20221213 23:33:02 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221213 23:33:02 @agent_ppo2.py:121][0m #------------------------ Iteration 2968 --------------------------#
[32m[20221213 23:33:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |           0.0004 |          47.5521 |           8.2847 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0076 |          43.2089 |           8.1979 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0067 |          42.4868 |           8.1716 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0118 |          40.8280 |           8.2308 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0123 |          40.3717 |           8.2294 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0156 |          40.1474 |           8.2341 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0143 |          39.3386 |           8.2589 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0139 |          39.1286 |           8.2108 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0152 |          38.5756 |           8.2328 |
[32m[20221213 23:33:03 @agent_ppo2.py:185][0m |          -0.0141 |          38.5524 |           8.2133 |
[32m[20221213 23:33:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:33:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.75
[32m[20221213 23:33:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.45
[32m[20221213 23:33:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.87
[32m[20221213 23:33:04 @agent_ppo2.py:143][0m Total time:      20.53 min
[32m[20221213 23:33:04 @agent_ppo2.py:145][0m 1984512 total steps have happened
[32m[20221213 23:33:04 @agent_ppo2.py:121][0m #------------------------ Iteration 2969 --------------------------#
[32m[20221213 23:33:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:04 @agent_ppo2.py:185][0m |           0.0018 |          62.9147 |           8.9194 |
[32m[20221213 23:33:04 @agent_ppo2.py:185][0m |          -0.0013 |          57.1350 |           8.9663 |
[32m[20221213 23:33:04 @agent_ppo2.py:185][0m |          -0.0101 |          54.7045 |           8.8888 |
[32m[20221213 23:33:04 @agent_ppo2.py:185][0m |          -0.0132 |          53.5585 |           8.8612 |
[32m[20221213 23:33:04 @agent_ppo2.py:185][0m |          -0.0107 |          52.3016 |           8.9053 |
[32m[20221213 23:33:04 @agent_ppo2.py:185][0m |          -0.0137 |          51.7419 |           8.9513 |
[32m[20221213 23:33:04 @agent_ppo2.py:185][0m |          -0.0140 |          51.4605 |           8.8668 |
[32m[20221213 23:33:05 @agent_ppo2.py:185][0m |          -0.0160 |          50.9850 |           8.8799 |
[32m[20221213 23:33:05 @agent_ppo2.py:185][0m |          -0.0158 |          50.4857 |           8.9171 |
[32m[20221213 23:33:05 @agent_ppo2.py:185][0m |          -0.0200 |          50.0302 |           8.8920 |
[32m[20221213 23:33:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.06
[32m[20221213 23:33:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.62
[32m[20221213 23:33:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.35
[32m[20221213 23:33:05 @agent_ppo2.py:143][0m Total time:      20.55 min
[32m[20221213 23:33:05 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221213 23:33:05 @agent_ppo2.py:121][0m #------------------------ Iteration 2970 --------------------------#
[32m[20221213 23:33:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:33:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:05 @agent_ppo2.py:185][0m |           0.0007 |          62.8867 |           8.7260 |
[32m[20221213 23:33:05 @agent_ppo2.py:185][0m |          -0.0057 |          56.8611 |           8.6987 |
[32m[20221213 23:33:05 @agent_ppo2.py:185][0m |           0.0026 |          60.8367 |           8.7355 |
[32m[20221213 23:33:06 @agent_ppo2.py:185][0m |          -0.0105 |          51.5436 |           8.7646 |
[32m[20221213 23:33:06 @agent_ppo2.py:185][0m |          -0.0098 |          49.6235 |           8.7513 |
[32m[20221213 23:33:06 @agent_ppo2.py:185][0m |          -0.0123 |          49.0290 |           8.7714 |
[32m[20221213 23:33:06 @agent_ppo2.py:185][0m |          -0.0087 |          48.4751 |           8.7708 |
[32m[20221213 23:33:06 @agent_ppo2.py:185][0m |          -0.0100 |          48.0045 |           8.8003 |
[32m[20221213 23:33:06 @agent_ppo2.py:185][0m |          -0.0117 |          47.7556 |           8.7334 |
[32m[20221213 23:33:06 @agent_ppo2.py:185][0m |          -0.0136 |          47.4419 |           8.7424 |
[32m[20221213 23:33:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.78
[32m[20221213 23:33:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.80
[32m[20221213 23:33:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.87
[32m[20221213 23:33:06 @agent_ppo2.py:143][0m Total time:      20.57 min
[32m[20221213 23:33:06 @agent_ppo2.py:145][0m 1988608 total steps have happened
[32m[20221213 23:33:06 @agent_ppo2.py:121][0m #------------------------ Iteration 2971 --------------------------#
[32m[20221213 23:33:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |           0.0080 |          69.9140 |           8.7607 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0012 |          65.0773 |           8.6851 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0053 |          64.0879 |           8.7077 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0059 |          64.1738 |           8.6539 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0023 |          65.2311 |           8.6935 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0094 |          63.6224 |           8.6753 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0068 |          62.9859 |           8.6694 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0066 |          62.8456 |           8.5968 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0074 |          62.7030 |           8.6172 |
[32m[20221213 23:33:07 @agent_ppo2.py:185][0m |          -0.0087 |          62.6286 |           8.6288 |
[32m[20221213 23:33:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.61
[32m[20221213 23:33:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.14
[32m[20221213 23:33:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.68
[32m[20221213 23:33:07 @agent_ppo2.py:143][0m Total time:      20.59 min
[32m[20221213 23:33:07 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221213 23:33:07 @agent_ppo2.py:121][0m #------------------------ Iteration 2972 --------------------------#
[32m[20221213 23:33:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |           0.0048 |          60.7500 |           8.4913 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0061 |          57.2686 |           8.5093 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0068 |          56.4977 |           8.5387 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0104 |          56.2921 |           8.5251 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0123 |          55.8988 |           8.5603 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0097 |          55.7415 |           8.6234 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0122 |          55.4048 |           8.6197 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0107 |          55.2394 |           8.6485 |
[32m[20221213 23:33:08 @agent_ppo2.py:185][0m |          -0.0095 |          55.1539 |           8.6667 |
[32m[20221213 23:33:09 @agent_ppo2.py:185][0m |          -0.0163 |          55.0378 |           8.6982 |
[32m[20221213 23:33:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:33:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.59
[32m[20221213 23:33:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.22
[32m[20221213 23:33:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.37
[32m[20221213 23:33:09 @agent_ppo2.py:143][0m Total time:      20.61 min
[32m[20221213 23:33:09 @agent_ppo2.py:145][0m 1992704 total steps have happened
[32m[20221213 23:33:09 @agent_ppo2.py:121][0m #------------------------ Iteration 2973 --------------------------#
[32m[20221213 23:33:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:09 @agent_ppo2.py:185][0m |           0.0008 |          69.6423 |           8.8707 |
[32m[20221213 23:33:09 @agent_ppo2.py:185][0m |           0.0008 |          60.2888 |           8.8377 |
[32m[20221213 23:33:09 @agent_ppo2.py:185][0m |          -0.0014 |          59.1333 |           8.7479 |
[32m[20221213 23:33:09 @agent_ppo2.py:185][0m |          -0.0066 |          57.5521 |           8.8072 |
[32m[20221213 23:33:09 @agent_ppo2.py:185][0m |          -0.0081 |          56.4701 |           8.7739 |
[32m[20221213 23:33:09 @agent_ppo2.py:185][0m |          -0.0150 |          56.5971 |           8.7746 |
[32m[20221213 23:33:10 @agent_ppo2.py:185][0m |          -0.0093 |          55.9491 |           8.7302 |
[32m[20221213 23:33:10 @agent_ppo2.py:185][0m |          -0.0048 |          60.0121 |           8.7805 |
[32m[20221213 23:33:10 @agent_ppo2.py:185][0m |          -0.0036 |          58.2163 |           8.7461 |
[32m[20221213 23:33:10 @agent_ppo2.py:185][0m |          -0.0136 |          54.1630 |           8.7704 |
[32m[20221213 23:33:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:33:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.30
[32m[20221213 23:33:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.12
[32m[20221213 23:33:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.12
[32m[20221213 23:33:10 @agent_ppo2.py:143][0m Total time:      20.63 min
[32m[20221213 23:33:10 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221213 23:33:10 @agent_ppo2.py:121][0m #------------------------ Iteration 2974 --------------------------#
[32m[20221213 23:33:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:10 @agent_ppo2.py:185][0m |           0.0001 |          55.6209 |           8.5416 |
[32m[20221213 23:33:10 @agent_ppo2.py:185][0m |          -0.0028 |          52.0659 |           8.6304 |
[32m[20221213 23:33:10 @agent_ppo2.py:185][0m |          -0.0030 |          50.7435 |           8.6233 |
[32m[20221213 23:33:11 @agent_ppo2.py:185][0m |          -0.0065 |          49.9096 |           8.6439 |
[32m[20221213 23:33:11 @agent_ppo2.py:185][0m |          -0.0055 |          48.9681 |           8.6511 |
[32m[20221213 23:33:11 @agent_ppo2.py:185][0m |          -0.0052 |          48.7554 |           8.6898 |
[32m[20221213 23:33:11 @agent_ppo2.py:185][0m |          -0.0007 |          49.0533 |           8.6772 |
[32m[20221213 23:33:11 @agent_ppo2.py:185][0m |          -0.0023 |          48.4011 |           8.6408 |
[32m[20221213 23:33:11 @agent_ppo2.py:185][0m |          -0.0063 |          48.1435 |           8.6851 |
[32m[20221213 23:33:11 @agent_ppo2.py:185][0m |          -0.0101 |          47.7531 |           8.6904 |
[32m[20221213 23:33:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.64
[32m[20221213 23:33:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.17
[32m[20221213 23:33:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.39
[32m[20221213 23:33:11 @agent_ppo2.py:143][0m Total time:      20.65 min
[32m[20221213 23:33:11 @agent_ppo2.py:145][0m 1996800 total steps have happened
[32m[20221213 23:33:11 @agent_ppo2.py:121][0m #------------------------ Iteration 2975 --------------------------#
[32m[20221213 23:33:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |           0.0066 |          63.8404 |           8.6164 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0100 |          55.1112 |           8.6529 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0071 |          52.5259 |           8.6317 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0076 |          51.0249 |           8.6494 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0075 |          50.0522 |           8.7021 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0102 |          49.3832 |           8.6451 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0124 |          49.0353 |           8.7381 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0151 |          48.7385 |           8.6937 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0124 |          48.5048 |           8.6975 |
[32m[20221213 23:33:12 @agent_ppo2.py:185][0m |          -0.0138 |          48.3310 |           8.7450 |
[32m[20221213 23:33:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.66
[32m[20221213 23:33:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.81
[32m[20221213 23:33:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.57
[32m[20221213 23:33:12 @agent_ppo2.py:143][0m Total time:      20.67 min
[32m[20221213 23:33:12 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221213 23:33:12 @agent_ppo2.py:121][0m #------------------------ Iteration 2976 --------------------------#
[32m[20221213 23:33:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |           0.0021 |          58.2062 |           8.4735 |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |          -0.0072 |          51.0370 |           8.4333 |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |          -0.0126 |          49.7554 |           8.4546 |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |          -0.0112 |          49.2900 |           8.4690 |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |          -0.0081 |          49.2374 |           8.4246 |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |          -0.0124 |          48.3555 |           8.3710 |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |          -0.0091 |          48.3107 |           8.4061 |
[32m[20221213 23:33:13 @agent_ppo2.py:185][0m |          -0.0147 |          47.8483 |           8.3326 |
[32m[20221213 23:33:14 @agent_ppo2.py:185][0m |          -0.0181 |          47.7402 |           8.2863 |
[32m[20221213 23:33:14 @agent_ppo2.py:185][0m |          -0.0081 |          51.8707 |           8.2737 |
[32m[20221213 23:33:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:33:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.55
[32m[20221213 23:33:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.34
[32m[20221213 23:33:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.55
[32m[20221213 23:33:14 @agent_ppo2.py:143][0m Total time:      20.70 min
[32m[20221213 23:33:14 @agent_ppo2.py:145][0m 2000896 total steps have happened
[32m[20221213 23:33:14 @agent_ppo2.py:121][0m #------------------------ Iteration 2977 --------------------------#
[32m[20221213 23:33:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:14 @agent_ppo2.py:185][0m |           0.0067 |          50.0746 |           8.4514 |
[32m[20221213 23:33:14 @agent_ppo2.py:185][0m |          -0.0038 |          42.1262 |           8.4787 |
[32m[20221213 23:33:14 @agent_ppo2.py:185][0m |          -0.0073 |          40.4306 |           8.4892 |
[32m[20221213 23:33:14 @agent_ppo2.py:185][0m |          -0.0070 |          39.3620 |           8.4467 |
[32m[20221213 23:33:14 @agent_ppo2.py:185][0m |          -0.0098 |          38.8046 |           8.4147 |
[32m[20221213 23:33:15 @agent_ppo2.py:185][0m |          -0.0107 |          38.1183 |           8.3838 |
[32m[20221213 23:33:15 @agent_ppo2.py:185][0m |          -0.0112 |          38.0740 |           8.2914 |
[32m[20221213 23:33:15 @agent_ppo2.py:185][0m |          -0.0127 |          37.3135 |           8.2432 |
[32m[20221213 23:33:15 @agent_ppo2.py:185][0m |          -0.0111 |          37.0713 |           8.2513 |
[32m[20221213 23:33:15 @agent_ppo2.py:185][0m |          -0.0105 |          36.9374 |           8.2244 |
[32m[20221213 23:33:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:33:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.70
[32m[20221213 23:33:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.05
[32m[20221213 23:33:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.31
[32m[20221213 23:33:15 @agent_ppo2.py:143][0m Total time:      20.72 min
[32m[20221213 23:33:15 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221213 23:33:15 @agent_ppo2.py:121][0m #------------------------ Iteration 2978 --------------------------#
[32m[20221213 23:33:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:15 @agent_ppo2.py:185][0m |           0.0029 |          61.8279 |           7.7084 |
[32m[20221213 23:33:15 @agent_ppo2.py:185][0m |          -0.0010 |          55.1514 |           7.6974 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0091 |          52.5283 |           7.7283 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0027 |          51.4354 |           7.6815 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0065 |          49.9506 |           7.6799 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0078 |          49.7788 |           7.6877 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0127 |          48.1531 |           7.6612 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0147 |          47.3902 |           7.6589 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0149 |          46.9579 |           7.6386 |
[32m[20221213 23:33:16 @agent_ppo2.py:185][0m |          -0.0152 |          46.3591 |           7.6460 |
[32m[20221213 23:33:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.47
[32m[20221213 23:33:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.52
[32m[20221213 23:33:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.93
[32m[20221213 23:33:16 @agent_ppo2.py:143][0m Total time:      20.74 min
[32m[20221213 23:33:16 @agent_ppo2.py:145][0m 2004992 total steps have happened
[32m[20221213 23:33:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2979 --------------------------#
[32m[20221213 23:33:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0006 |          54.2059 |           8.3983 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0033 |          50.6128 |           8.4654 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0122 |          49.7798 |           8.4784 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0072 |          49.1610 |           8.5091 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0121 |          48.7788 |           8.4976 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0106 |          48.3613 |           8.4945 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0073 |          48.1133 |           8.5307 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0020 |          49.8581 |           8.5116 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0122 |          47.6362 |           8.5656 |
[32m[20221213 23:33:17 @agent_ppo2.py:185][0m |          -0.0165 |          47.6922 |           8.5709 |
[32m[20221213 23:33:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.93
[32m[20221213 23:33:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.23
[32m[20221213 23:33:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.12
[32m[20221213 23:33:18 @agent_ppo2.py:143][0m Total time:      20.76 min
[32m[20221213 23:33:18 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221213 23:33:18 @agent_ppo2.py:121][0m #------------------------ Iteration 2980 --------------------------#
[32m[20221213 23:33:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:33:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |           0.0079 |          43.3046 |           7.5992 |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |          -0.0069 |          39.0494 |           7.6029 |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |          -0.0046 |          37.9532 |           7.5866 |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |          -0.0126 |          37.5196 |           7.5618 |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |          -0.0110 |          36.8013 |           7.5455 |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |          -0.0132 |          36.6382 |           7.5492 |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |          -0.0146 |          35.9511 |           7.4673 |
[32m[20221213 23:33:18 @agent_ppo2.py:185][0m |          -0.0139 |          35.7290 |           7.4921 |
[32m[20221213 23:33:19 @agent_ppo2.py:185][0m |          -0.0170 |          35.5877 |           7.4567 |
[32m[20221213 23:33:19 @agent_ppo2.py:185][0m |          -0.0171 |          35.4218 |           7.5119 |
[32m[20221213 23:33:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.88
[32m[20221213 23:33:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.60
[32m[20221213 23:33:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.44
[32m[20221213 23:33:19 @agent_ppo2.py:143][0m Total time:      20.78 min
[32m[20221213 23:33:19 @agent_ppo2.py:145][0m 2009088 total steps have happened
[32m[20221213 23:33:19 @agent_ppo2.py:121][0m #------------------------ Iteration 2981 --------------------------#
[32m[20221213 23:33:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:19 @agent_ppo2.py:185][0m |           0.0020 |          60.2021 |           8.2034 |
[32m[20221213 23:33:19 @agent_ppo2.py:185][0m |          -0.0052 |          55.1798 |           8.2378 |
[32m[20221213 23:33:19 @agent_ppo2.py:185][0m |          -0.0081 |          52.9526 |           8.2460 |
[32m[20221213 23:33:19 @agent_ppo2.py:185][0m |          -0.0119 |          51.6567 |           8.2615 |
[32m[20221213 23:33:19 @agent_ppo2.py:185][0m |          -0.0107 |          50.9467 |           8.2326 |
[32m[20221213 23:33:20 @agent_ppo2.py:185][0m |          -0.0082 |          50.5292 |           8.3275 |
[32m[20221213 23:33:20 @agent_ppo2.py:185][0m |          -0.0130 |          49.9845 |           8.3721 |
[32m[20221213 23:33:20 @agent_ppo2.py:185][0m |          -0.0106 |          49.3493 |           8.3601 |
[32m[20221213 23:33:20 @agent_ppo2.py:185][0m |          -0.0126 |          49.0087 |           8.4240 |
[32m[20221213 23:33:20 @agent_ppo2.py:185][0m |          -0.0095 |          49.0994 |           8.3852 |
[32m[20221213 23:33:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.88
[32m[20221213 23:33:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.47
[32m[20221213 23:33:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.09
[32m[20221213 23:33:20 @agent_ppo2.py:143][0m Total time:      20.80 min
[32m[20221213 23:33:20 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221213 23:33:20 @agent_ppo2.py:121][0m #------------------------ Iteration 2982 --------------------------#
[32m[20221213 23:33:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:20 @agent_ppo2.py:185][0m |           0.0007 |          59.1429 |           7.8843 |
[32m[20221213 23:33:20 @agent_ppo2.py:185][0m |           0.0028 |          56.4562 |           7.9048 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0080 |          53.7447 |           7.8897 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0087 |          53.1280 |           7.8819 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0069 |          52.4702 |           7.8907 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0112 |          52.0553 |           7.9265 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0044 |          52.3428 |           7.9183 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0126 |          51.5523 |           7.8932 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0153 |          51.2907 |           7.8966 |
[32m[20221213 23:33:21 @agent_ppo2.py:185][0m |          -0.0082 |          51.1596 |           7.9270 |
[32m[20221213 23:33:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:33:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.60
[32m[20221213 23:33:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.52
[32m[20221213 23:33:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.39
[32m[20221213 23:33:21 @agent_ppo2.py:143][0m Total time:      20.82 min
[32m[20221213 23:33:21 @agent_ppo2.py:145][0m 2013184 total steps have happened
[32m[20221213 23:33:21 @agent_ppo2.py:121][0m #------------------------ Iteration 2983 --------------------------#
[32m[20221213 23:33:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0005 |          62.5642 |           8.6214 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0066 |          59.6030 |           8.6525 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0088 |          58.0649 |           8.6112 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0078 |          57.2768 |           8.6663 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0079 |          56.6829 |           8.6243 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0118 |          56.0531 |           8.6728 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0128 |          55.7278 |           8.6302 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0138 |          55.4070 |           8.6145 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0149 |          55.2035 |           8.5977 |
[32m[20221213 23:33:22 @agent_ppo2.py:185][0m |          -0.0139 |          54.8279 |           8.5927 |
[32m[20221213 23:33:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.77
[32m[20221213 23:33:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.84
[32m[20221213 23:33:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.48
[32m[20221213 23:33:23 @agent_ppo2.py:143][0m Total time:      20.84 min
[32m[20221213 23:33:23 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221213 23:33:23 @agent_ppo2.py:121][0m #------------------------ Iteration 2984 --------------------------#
[32m[20221213 23:33:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:23 @agent_ppo2.py:185][0m |           0.0022 |          60.4169 |           8.0812 |
[32m[20221213 23:33:23 @agent_ppo2.py:185][0m |          -0.0027 |          59.4472 |           8.0742 |
[32m[20221213 23:33:23 @agent_ppo2.py:185][0m |          -0.0048 |          59.1429 |           8.0738 |
[32m[20221213 23:33:23 @agent_ppo2.py:185][0m |          -0.0022 |          59.1278 |           8.0424 |
[32m[20221213 23:33:23 @agent_ppo2.py:185][0m |          -0.0080 |          58.6881 |           8.0738 |
[32m[20221213 23:33:23 @agent_ppo2.py:185][0m |          -0.0044 |          59.5875 |           7.9848 |
[32m[20221213 23:33:23 @agent_ppo2.py:185][0m |          -0.0051 |          58.6168 |           7.9738 |
[32m[20221213 23:33:24 @agent_ppo2.py:185][0m |          -0.0057 |          59.0412 |           7.9886 |
[32m[20221213 23:33:24 @agent_ppo2.py:185][0m |          -0.0075 |          58.2480 |           7.9454 |
[32m[20221213 23:33:24 @agent_ppo2.py:185][0m |          -0.0085 |          58.1378 |           7.9612 |
[32m[20221213 23:33:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.35
[32m[20221213 23:33:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.15
[32m[20221213 23:33:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.87
[32m[20221213 23:33:24 @agent_ppo2.py:143][0m Total time:      20.86 min
[32m[20221213 23:33:24 @agent_ppo2.py:145][0m 2017280 total steps have happened
[32m[20221213 23:33:24 @agent_ppo2.py:121][0m #------------------------ Iteration 2985 --------------------------#
[32m[20221213 23:33:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:24 @agent_ppo2.py:185][0m |           0.0017 |          57.4255 |           7.5922 |
[32m[20221213 23:33:24 @agent_ppo2.py:185][0m |          -0.0075 |          53.1897 |           7.6058 |
[32m[20221213 23:33:24 @agent_ppo2.py:185][0m |          -0.0109 |          51.9179 |           7.7422 |
[32m[20221213 23:33:24 @agent_ppo2.py:185][0m |          -0.0102 |          51.2181 |           7.6677 |
[32m[20221213 23:33:25 @agent_ppo2.py:185][0m |          -0.0121 |          50.5702 |           7.6768 |
[32m[20221213 23:33:25 @agent_ppo2.py:185][0m |          -0.0172 |          50.1093 |           7.7211 |
[32m[20221213 23:33:25 @agent_ppo2.py:185][0m |          -0.0080 |          50.1566 |           7.6704 |
[32m[20221213 23:33:25 @agent_ppo2.py:185][0m |          -0.0127 |          49.6250 |           7.6315 |
[32m[20221213 23:33:25 @agent_ppo2.py:185][0m |          -0.0159 |          49.3726 |           7.6775 |
[32m[20221213 23:33:25 @agent_ppo2.py:185][0m |          -0.0096 |          50.7972 |           7.6990 |
[32m[20221213 23:33:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.40
[32m[20221213 23:33:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.91
[32m[20221213 23:33:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.15
[32m[20221213 23:33:25 @agent_ppo2.py:143][0m Total time:      20.89 min
[32m[20221213 23:33:25 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221213 23:33:25 @agent_ppo2.py:121][0m #------------------------ Iteration 2986 --------------------------#
[32m[20221213 23:33:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:25 @agent_ppo2.py:185][0m |          -0.0004 |          43.0528 |           7.0665 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0090 |          34.6401 |           7.0517 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0070 |          33.7624 |           7.0780 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0036 |          33.0572 |           7.0731 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0054 |          33.5941 |           7.0341 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0124 |          32.4924 |           7.0353 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0133 |          32.3317 |           7.0083 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0117 |          32.0991 |           6.9978 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0138 |          31.8581 |           6.9536 |
[32m[20221213 23:33:26 @agent_ppo2.py:185][0m |          -0.0153 |          31.7893 |           6.9356 |
[32m[20221213 23:33:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.00
[32m[20221213 23:33:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.48
[32m[20221213 23:33:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.70
[32m[20221213 23:33:26 @agent_ppo2.py:143][0m Total time:      20.91 min
[32m[20221213 23:33:26 @agent_ppo2.py:145][0m 2021376 total steps have happened
[32m[20221213 23:33:26 @agent_ppo2.py:121][0m #------------------------ Iteration 2987 --------------------------#
[32m[20221213 23:33:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0033 |          41.8960 |           7.6446 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0062 |          39.6045 |           7.6516 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0048 |          38.9419 |           7.6967 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0005 |          39.5005 |           7.6805 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0102 |          38.4987 |           7.6589 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0070 |          38.7379 |           7.6643 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0064 |          38.2083 |           7.6692 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0118 |          37.9916 |           7.6472 |
[32m[20221213 23:33:27 @agent_ppo2.py:185][0m |          -0.0133 |          37.8754 |           7.6291 |
[32m[20221213 23:33:28 @agent_ppo2.py:185][0m |          -0.0138 |          37.7041 |           7.6325 |
[32m[20221213 23:33:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:33:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.91
[32m[20221213 23:33:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.38
[32m[20221213 23:33:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.82
[32m[20221213 23:33:28 @agent_ppo2.py:143][0m Total time:      20.93 min
[32m[20221213 23:33:28 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221213 23:33:28 @agent_ppo2.py:121][0m #------------------------ Iteration 2988 --------------------------#
[32m[20221213 23:33:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:28 @agent_ppo2.py:185][0m |           0.0018 |          49.9290 |           7.9559 |
[32m[20221213 23:33:28 @agent_ppo2.py:185][0m |           0.0020 |          46.7923 |           7.9193 |
[32m[20221213 23:33:28 @agent_ppo2.py:185][0m |          -0.0009 |          45.7394 |           7.9245 |
[32m[20221213 23:33:28 @agent_ppo2.py:185][0m |          -0.0089 |          44.9734 |           7.9290 |
[32m[20221213 23:33:28 @agent_ppo2.py:185][0m |          -0.0099 |          44.2341 |           7.9312 |
[32m[20221213 23:33:28 @agent_ppo2.py:185][0m |          -0.0139 |          44.3564 |           7.9180 |
[32m[20221213 23:33:29 @agent_ppo2.py:185][0m |          -0.0135 |          43.9054 |           7.9280 |
[32m[20221213 23:33:29 @agent_ppo2.py:185][0m |          -0.0136 |          43.6789 |           7.9294 |
[32m[20221213 23:33:29 @agent_ppo2.py:185][0m |          -0.0164 |          43.4325 |           7.9157 |
[32m[20221213 23:33:29 @agent_ppo2.py:185][0m |          -0.0164 |          43.3391 |           7.9398 |
[32m[20221213 23:33:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:33:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.18
[32m[20221213 23:33:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.33
[32m[20221213 23:33:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.92
[32m[20221213 23:33:29 @agent_ppo2.py:143][0m Total time:      20.95 min
[32m[20221213 23:33:29 @agent_ppo2.py:145][0m 2025472 total steps have happened
[32m[20221213 23:33:29 @agent_ppo2.py:121][0m #------------------------ Iteration 2989 --------------------------#
[32m[20221213 23:33:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:29 @agent_ppo2.py:185][0m |           0.0031 |          48.7446 |           7.1332 |
[32m[20221213 23:33:29 @agent_ppo2.py:185][0m |          -0.0001 |          44.8240 |           7.1781 |
[32m[20221213 23:33:29 @agent_ppo2.py:185][0m |          -0.0123 |          43.7661 |           7.1417 |
[32m[20221213 23:33:30 @agent_ppo2.py:185][0m |          -0.0057 |          43.0003 |           7.0804 |
[32m[20221213 23:33:30 @agent_ppo2.py:185][0m |          -0.0081 |          42.7614 |           7.1337 |
[32m[20221213 23:33:30 @agent_ppo2.py:185][0m |          -0.0066 |          43.2914 |           7.1248 |
[32m[20221213 23:33:30 @agent_ppo2.py:185][0m |          -0.0144 |          41.8942 |           7.1071 |
[32m[20221213 23:33:30 @agent_ppo2.py:185][0m |          -0.0110 |          41.4531 |           7.0978 |
[32m[20221213 23:33:30 @agent_ppo2.py:185][0m |          -0.0138 |          41.1436 |           7.0902 |
[32m[20221213 23:33:30 @agent_ppo2.py:185][0m |          -0.0177 |          40.9603 |           7.0848 |
[32m[20221213 23:33:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.61
[32m[20221213 23:33:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.99
[32m[20221213 23:33:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.00
[32m[20221213 23:33:30 @agent_ppo2.py:143][0m Total time:      20.97 min
[32m[20221213 23:33:30 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221213 23:33:30 @agent_ppo2.py:121][0m #------------------------ Iteration 2990 --------------------------#
[32m[20221213 23:33:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:33:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0027 |          49.4003 |           6.9100 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0009 |          45.4013 |           6.9203 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0069 |          44.6004 |           6.9341 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0045 |          43.8483 |           6.9857 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0062 |          43.4408 |           7.0169 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0086 |          43.0814 |           6.9861 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |           0.0043 |          45.6001 |           7.0337 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0123 |          42.5680 |           7.0096 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0119 |          42.1594 |           7.0221 |
[32m[20221213 23:33:31 @agent_ppo2.py:185][0m |          -0.0106 |          42.2335 |           6.9311 |
[32m[20221213 23:33:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.49
[32m[20221213 23:33:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.35
[32m[20221213 23:33:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.21
[32m[20221213 23:33:31 @agent_ppo2.py:143][0m Total time:      20.99 min
[32m[20221213 23:33:31 @agent_ppo2.py:145][0m 2029568 total steps have happened
[32m[20221213 23:33:31 @agent_ppo2.py:121][0m #------------------------ Iteration 2991 --------------------------#
[32m[20221213 23:33:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |           0.0103 |          50.1316 |           7.0726 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |          -0.0065 |          40.5982 |           6.9976 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |           0.0012 |          43.7840 |           6.9792 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |          -0.0121 |          38.3570 |           7.0015 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |          -0.0108 |          37.7132 |           6.9801 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |          -0.0176 |          37.4321 |           6.9326 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |          -0.0132 |          37.0620 |           6.9622 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |          -0.0121 |          36.9671 |           6.8917 |
[32m[20221213 23:33:32 @agent_ppo2.py:185][0m |          -0.0117 |          36.9445 |           6.8996 |
[32m[20221213 23:33:33 @agent_ppo2.py:185][0m |          -0.0141 |          36.6544 |           6.8846 |
[32m[20221213 23:33:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:33:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.87
[32m[20221213 23:33:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.03
[32m[20221213 23:33:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.33
[32m[20221213 23:33:33 @agent_ppo2.py:143][0m Total time:      21.01 min
[32m[20221213 23:33:33 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221213 23:33:33 @agent_ppo2.py:121][0m #------------------------ Iteration 2992 --------------------------#
[32m[20221213 23:33:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:33 @agent_ppo2.py:185][0m |           0.0084 |          57.6965 |           7.2073 |
[32m[20221213 23:33:33 @agent_ppo2.py:185][0m |           0.0053 |          58.6462 |           7.3439 |
[32m[20221213 23:33:33 @agent_ppo2.py:185][0m |          -0.0091 |          51.3387 |           7.3841 |
[32m[20221213 23:33:33 @agent_ppo2.py:185][0m |          -0.0095 |          50.0113 |           7.3483 |
[32m[20221213 23:33:33 @agent_ppo2.py:185][0m |          -0.0120 |          49.3697 |           7.4022 |
[32m[20221213 23:33:33 @agent_ppo2.py:185][0m |          -0.0117 |          48.9460 |           7.4175 |
[32m[20221213 23:33:34 @agent_ppo2.py:185][0m |          -0.0099 |          48.8928 |           7.4117 |
[32m[20221213 23:33:34 @agent_ppo2.py:185][0m |          -0.0103 |          48.3718 |           7.4705 |
[32m[20221213 23:33:34 @agent_ppo2.py:185][0m |          -0.0028 |          51.2923 |           7.4645 |
[32m[20221213 23:33:34 @agent_ppo2.py:185][0m |          -0.0076 |          48.2678 |           7.4617 |
[32m[20221213 23:33:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.86
[32m[20221213 23:33:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.46
[32m[20221213 23:33:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.83
[32m[20221213 23:33:34 @agent_ppo2.py:143][0m Total time:      21.03 min
[32m[20221213 23:33:34 @agent_ppo2.py:145][0m 2033664 total steps have happened
[32m[20221213 23:33:34 @agent_ppo2.py:121][0m #------------------------ Iteration 2993 --------------------------#
[32m[20221213 23:33:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:34 @agent_ppo2.py:185][0m |          -0.0008 |          41.8954 |           7.3847 |
[32m[20221213 23:33:34 @agent_ppo2.py:185][0m |          -0.0033 |          36.0619 |           7.5025 |
[32m[20221213 23:33:34 @agent_ppo2.py:185][0m |          -0.0120 |          33.6378 |           7.4834 |
[32m[20221213 23:33:35 @agent_ppo2.py:185][0m |          -0.0066 |          33.0376 |           7.4932 |
[32m[20221213 23:33:35 @agent_ppo2.py:185][0m |          -0.0124 |          31.6541 |           7.5203 |
[32m[20221213 23:33:35 @agent_ppo2.py:185][0m |          -0.0145 |          30.9299 |           7.5746 |
[32m[20221213 23:33:35 @agent_ppo2.py:185][0m |          -0.0175 |          30.7495 |           7.5824 |
[32m[20221213 23:33:35 @agent_ppo2.py:185][0m |          -0.0122 |          30.2449 |           7.5534 |
[32m[20221213 23:33:35 @agent_ppo2.py:185][0m |          -0.0187 |          29.7597 |           7.6023 |
[32m[20221213 23:33:35 @agent_ppo2.py:185][0m |          -0.0181 |          29.3692 |           7.6841 |
[32m[20221213 23:33:35 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 23:33:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.87
[32m[20221213 23:33:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.50
[32m[20221213 23:33:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.40
[32m[20221213 23:33:35 @agent_ppo2.py:143][0m Total time:      21.06 min
[32m[20221213 23:33:35 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221213 23:33:35 @agent_ppo2.py:121][0m #------------------------ Iteration 2994 --------------------------#
[32m[20221213 23:33:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0042 |          50.0904 |           7.8429 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0067 |          44.6110 |           7.8342 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0072 |          42.8320 |           7.8965 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0033 |          44.5535 |           7.8635 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0115 |          41.4599 |           7.9251 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0074 |          41.6889 |           7.9086 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0113 |          40.6773 |           7.9238 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0136 |          40.1835 |           7.9215 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0127 |          40.0291 |           7.9038 |
[32m[20221213 23:33:36 @agent_ppo2.py:185][0m |          -0.0189 |          39.5462 |           7.9234 |
[32m[20221213 23:33:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.24
[32m[20221213 23:33:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.56
[32m[20221213 23:33:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.62
[32m[20221213 23:33:37 @agent_ppo2.py:143][0m Total time:      21.08 min
[32m[20221213 23:33:37 @agent_ppo2.py:145][0m 2037760 total steps have happened
[32m[20221213 23:33:37 @agent_ppo2.py:121][0m #------------------------ Iteration 2995 --------------------------#
[32m[20221213 23:33:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:37 @agent_ppo2.py:185][0m |          -0.0007 |          49.3594 |           8.0270 |
[32m[20221213 23:33:37 @agent_ppo2.py:185][0m |           0.0035 |          51.4246 |           7.9246 |
[32m[20221213 23:33:37 @agent_ppo2.py:185][0m |          -0.0094 |          45.2801 |           7.8827 |
[32m[20221213 23:33:37 @agent_ppo2.py:185][0m |          -0.0125 |          44.8467 |           7.8604 |
[32m[20221213 23:33:37 @agent_ppo2.py:185][0m |          -0.0124 |          44.6623 |           7.8083 |
[32m[20221213 23:33:37 @agent_ppo2.py:185][0m |          -0.0147 |          44.3792 |           7.7746 |
[32m[20221213 23:33:37 @agent_ppo2.py:185][0m |          -0.0096 |          44.1914 |           7.7524 |
[32m[20221213 23:33:38 @agent_ppo2.py:185][0m |          -0.0171 |          43.8277 |           7.7283 |
[32m[20221213 23:33:38 @agent_ppo2.py:185][0m |          -0.0167 |          43.4889 |           7.6910 |
[32m[20221213 23:33:38 @agent_ppo2.py:185][0m |          -0.0189 |          43.4501 |           7.7076 |
[32m[20221213 23:33:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.39
[32m[20221213 23:33:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.51
[32m[20221213 23:33:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.68
[32m[20221213 23:33:38 @agent_ppo2.py:143][0m Total time:      21.10 min
[32m[20221213 23:33:38 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221213 23:33:38 @agent_ppo2.py:121][0m #------------------------ Iteration 2996 --------------------------#
[32m[20221213 23:33:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:38 @agent_ppo2.py:185][0m |          -0.0001 |          59.4962 |           7.2169 |
[32m[20221213 23:33:38 @agent_ppo2.py:185][0m |          -0.0012 |          55.7543 |           7.2844 |
[32m[20221213 23:33:38 @agent_ppo2.py:185][0m |          -0.0015 |          55.2501 |           7.1993 |
[32m[20221213 23:33:38 @agent_ppo2.py:185][0m |           0.0033 |          58.1707 |           7.1569 |
[32m[20221213 23:33:39 @agent_ppo2.py:185][0m |          -0.0068 |          54.3704 |           7.1350 |
[32m[20221213 23:33:39 @agent_ppo2.py:185][0m |          -0.0095 |          53.7784 |           7.0914 |
[32m[20221213 23:33:39 @agent_ppo2.py:185][0m |          -0.0113 |          53.6783 |           7.1346 |
[32m[20221213 23:33:39 @agent_ppo2.py:185][0m |          -0.0101 |          53.7682 |           7.0765 |
[32m[20221213 23:33:39 @agent_ppo2.py:185][0m |          -0.0094 |          53.1959 |           7.0382 |
[32m[20221213 23:33:39 @agent_ppo2.py:185][0m |          -0.0102 |          53.1017 |           7.0640 |
[32m[20221213 23:33:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:33:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.60
[32m[20221213 23:33:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.00
[32m[20221213 23:33:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.45
[32m[20221213 23:33:39 @agent_ppo2.py:143][0m Total time:      21.12 min
[32m[20221213 23:33:39 @agent_ppo2.py:145][0m 2041856 total steps have happened
[32m[20221213 23:33:39 @agent_ppo2.py:121][0m #------------------------ Iteration 2997 --------------------------#
[32m[20221213 23:33:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:39 @agent_ppo2.py:185][0m |           0.0090 |          50.8743 |           7.2680 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0066 |          44.8555 |           7.3652 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0109 |          43.4870 |           7.3763 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0114 |          42.7499 |           7.3923 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0142 |          42.1769 |           7.3979 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0114 |          41.6547 |           7.3978 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0159 |          41.2744 |           7.4581 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0146 |          41.0549 |           7.4388 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0155 |          40.6945 |           7.4214 |
[32m[20221213 23:33:40 @agent_ppo2.py:185][0m |          -0.0092 |          40.4481 |           7.4083 |
[32m[20221213 23:33:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.06
[32m[20221213 23:33:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.93
[32m[20221213 23:33:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.53
[32m[20221213 23:33:40 @agent_ppo2.py:143][0m Total time:      21.14 min
[32m[20221213 23:33:40 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221213 23:33:40 @agent_ppo2.py:121][0m #------------------------ Iteration 2998 --------------------------#
[32m[20221213 23:33:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |           0.0024 |          62.9578 |           6.6433 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0004 |          60.0183 |           6.6429 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0077 |          58.4340 |           6.6565 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0061 |          58.0078 |           6.6301 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0028 |          58.4497 |           6.6526 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0100 |          56.8143 |           6.6998 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0074 |          56.3998 |           6.6557 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0097 |          55.9875 |           6.6160 |
[32m[20221213 23:33:41 @agent_ppo2.py:185][0m |          -0.0072 |          56.3230 |           6.6218 |
[32m[20221213 23:33:42 @agent_ppo2.py:185][0m |          -0.0109 |          55.4345 |           6.6651 |
[32m[20221213 23:33:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.47
[32m[20221213 23:33:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.21
[32m[20221213 23:33:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.33
[32m[20221213 23:33:42 @agent_ppo2.py:143][0m Total time:      21.16 min
[32m[20221213 23:33:42 @agent_ppo2.py:145][0m 2045952 total steps have happened
[32m[20221213 23:33:42 @agent_ppo2.py:121][0m #------------------------ Iteration 2999 --------------------------#
[32m[20221213 23:33:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:42 @agent_ppo2.py:185][0m |           0.0019 |          64.9633 |           7.6370 |
[32m[20221213 23:33:42 @agent_ppo2.py:185][0m |          -0.0057 |          61.4082 |           7.6390 |
[32m[20221213 23:33:42 @agent_ppo2.py:185][0m |          -0.0019 |          60.1810 |           7.5862 |
[32m[20221213 23:33:42 @agent_ppo2.py:185][0m |          -0.0076 |          59.8422 |           7.5685 |
[32m[20221213 23:33:42 @agent_ppo2.py:185][0m |          -0.0046 |          59.4482 |           7.5671 |
[32m[20221213 23:33:42 @agent_ppo2.py:185][0m |          -0.0077 |          58.9320 |           7.5113 |
[32m[20221213 23:33:43 @agent_ppo2.py:185][0m |          -0.0042 |          58.8765 |           7.5348 |
[32m[20221213 23:33:43 @agent_ppo2.py:185][0m |          -0.0081 |          58.3543 |           7.5142 |
[32m[20221213 23:33:43 @agent_ppo2.py:185][0m |          -0.0111 |          58.1192 |           7.5367 |
[32m[20221213 23:33:43 @agent_ppo2.py:185][0m |          -0.0112 |          58.0789 |           7.4821 |
[32m[20221213 23:33:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.01
[32m[20221213 23:33:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.11
[32m[20221213 23:33:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.40
[32m[20221213 23:33:43 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 573.08
[32m[20221213 23:33:43 @agent_ppo2.py:143][0m Total time:      21.18 min
[32m[20221213 23:33:43 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221213 23:33:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3000 --------------------------#
[32m[20221213 23:33:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:33:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:43 @agent_ppo2.py:185][0m |          -0.0008 |          66.1009 |           6.7036 |
[32m[20221213 23:33:43 @agent_ppo2.py:185][0m |           0.0014 |          65.2996 |           6.7683 |
[32m[20221213 23:33:43 @agent_ppo2.py:185][0m |          -0.0036 |          62.2969 |           6.7675 |
[32m[20221213 23:33:44 @agent_ppo2.py:185][0m |          -0.0097 |          61.5773 |           6.8030 |
[32m[20221213 23:33:44 @agent_ppo2.py:185][0m |          -0.0106 |          61.1414 |           6.8457 |
[32m[20221213 23:33:44 @agent_ppo2.py:185][0m |          -0.0081 |          60.8232 |           6.7910 |
[32m[20221213 23:33:44 @agent_ppo2.py:185][0m |          -0.0114 |          60.6232 |           6.8257 |
[32m[20221213 23:33:44 @agent_ppo2.py:185][0m |          -0.0116 |          60.0802 |           6.7702 |
[32m[20221213 23:33:44 @agent_ppo2.py:185][0m |          -0.0112 |          60.1245 |           6.8178 |
[32m[20221213 23:33:44 @agent_ppo2.py:185][0m |          -0.0096 |          59.7111 |           6.7836 |
[32m[20221213 23:33:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.02
[32m[20221213 23:33:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.99
[32m[20221213 23:33:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.09
[32m[20221213 23:33:44 @agent_ppo2.py:143][0m Total time:      21.20 min
[32m[20221213 23:33:44 @agent_ppo2.py:145][0m 2050048 total steps have happened
[32m[20221213 23:33:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3001 --------------------------#
[32m[20221213 23:33:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |           0.0018 |          59.7890 |           7.0262 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0051 |          55.9545 |           6.9738 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0030 |          53.7573 |           6.9702 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0022 |          54.3954 |           6.9456 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0113 |          51.4240 |           6.9874 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0067 |          51.0908 |           7.0105 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0025 |          53.4329 |           6.9643 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0116 |          50.2856 |           7.0246 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0121 |          49.9638 |           6.9742 |
[32m[20221213 23:33:45 @agent_ppo2.py:185][0m |          -0.0144 |          49.7350 |           6.9710 |
[32m[20221213 23:33:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:33:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.01
[32m[20221213 23:33:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.74
[32m[20221213 23:33:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.38
[32m[20221213 23:33:45 @agent_ppo2.py:143][0m Total time:      21.22 min
[32m[20221213 23:33:45 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221213 23:33:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3002 --------------------------#
[32m[20221213 23:33:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0015 |          48.6372 |           6.9977 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0095 |          43.4291 |           7.0004 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0066 |          41.9956 |           7.0141 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0072 |          41.3954 |           7.0082 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0057 |          41.7003 |           7.0241 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0069 |          40.9008 |           7.0095 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |           0.0011 |          43.6045 |           7.0278 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0149 |          40.4533 |           6.9983 |
[32m[20221213 23:33:46 @agent_ppo2.py:185][0m |          -0.0068 |          40.4279 |           7.0537 |
[32m[20221213 23:33:47 @agent_ppo2.py:185][0m |          -0.0130 |          39.9851 |           7.0278 |
[32m[20221213 23:33:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.72
[32m[20221213 23:33:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.50
[32m[20221213 23:33:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.13
[32m[20221213 23:33:47 @agent_ppo2.py:143][0m Total time:      21.25 min
[32m[20221213 23:33:47 @agent_ppo2.py:145][0m 2054144 total steps have happened
[32m[20221213 23:33:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3003 --------------------------#
[32m[20221213 23:33:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:47 @agent_ppo2.py:185][0m |           0.0068 |          42.9139 |           6.6296 |
[32m[20221213 23:33:47 @agent_ppo2.py:185][0m |          -0.0062 |          34.6931 |           6.6520 |
[32m[20221213 23:33:47 @agent_ppo2.py:185][0m |          -0.0101 |          31.8861 |           6.5544 |
[32m[20221213 23:33:47 @agent_ppo2.py:185][0m |          -0.0074 |          30.0589 |           6.6001 |
[32m[20221213 23:33:47 @agent_ppo2.py:185][0m |          -0.0101 |          29.0138 |           6.6251 |
[32m[20221213 23:33:47 @agent_ppo2.py:185][0m |          -0.0111 |          28.1247 |           6.6003 |
[32m[20221213 23:33:48 @agent_ppo2.py:185][0m |          -0.0101 |          27.8559 |           6.5810 |
[32m[20221213 23:33:48 @agent_ppo2.py:185][0m |          -0.0119 |          27.1597 |           6.5290 |
[32m[20221213 23:33:48 @agent_ppo2.py:185][0m |          -0.0135 |          26.7019 |           6.6067 |
[32m[20221213 23:33:48 @agent_ppo2.py:185][0m |          -0.0116 |          26.4000 |           6.5375 |
[32m[20221213 23:33:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:33:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.84
[32m[20221213 23:33:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.28
[32m[20221213 23:33:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.17
[32m[20221213 23:33:48 @agent_ppo2.py:143][0m Total time:      21.27 min
[32m[20221213 23:33:48 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221213 23:33:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3004 --------------------------#
[32m[20221213 23:33:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:48 @agent_ppo2.py:185][0m |           0.0044 |          57.7347 |           7.4827 |
[32m[20221213 23:33:48 @agent_ppo2.py:185][0m |          -0.0027 |          53.7883 |           7.5663 |
[32m[20221213 23:33:48 @agent_ppo2.py:185][0m |          -0.0109 |          52.8782 |           7.5917 |
[32m[20221213 23:33:49 @agent_ppo2.py:185][0m |          -0.0117 |          52.3452 |           7.5935 |
[32m[20221213 23:33:49 @agent_ppo2.py:185][0m |          -0.0112 |          51.9527 |           7.6632 |
[32m[20221213 23:33:49 @agent_ppo2.py:185][0m |          -0.0099 |          51.9784 |           7.6407 |
[32m[20221213 23:33:49 @agent_ppo2.py:185][0m |          -0.0139 |          51.4914 |           7.6599 |
[32m[20221213 23:33:49 @agent_ppo2.py:185][0m |          -0.0156 |          51.1798 |           7.6430 |
[32m[20221213 23:33:49 @agent_ppo2.py:185][0m |          -0.0123 |          50.9219 |           7.7192 |
[32m[20221213 23:33:49 @agent_ppo2.py:185][0m |          -0.0123 |          51.8157 |           7.6909 |
[32m[20221213 23:33:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.08
[32m[20221213 23:33:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.85
[32m[20221213 23:33:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.66
[32m[20221213 23:33:49 @agent_ppo2.py:143][0m Total time:      21.29 min
[32m[20221213 23:33:49 @agent_ppo2.py:145][0m 2058240 total steps have happened
[32m[20221213 23:33:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3005 --------------------------#
[32m[20221213 23:33:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0032 |          70.4142 |           7.1217 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0030 |          62.3570 |           7.1928 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0037 |          61.3341 |           7.1393 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0032 |          59.9056 |           7.1126 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0044 |          59.6943 |           7.0347 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0035 |          58.4436 |           6.9734 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0070 |          57.7950 |           6.9701 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0079 |          57.3504 |           6.9465 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0078 |          57.3021 |           6.9340 |
[32m[20221213 23:33:50 @agent_ppo2.py:185][0m |          -0.0102 |          56.8216 |           6.9170 |
[32m[20221213 23:33:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.32
[32m[20221213 23:33:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.22
[32m[20221213 23:33:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.48
[32m[20221213 23:33:50 @agent_ppo2.py:143][0m Total time:      21.31 min
[32m[20221213 23:33:50 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221213 23:33:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3006 --------------------------#
[32m[20221213 23:33:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |           0.0099 |          62.6898 |           6.6352 |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |           0.0001 |          58.0448 |           6.5837 |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |          -0.0062 |          57.2204 |           6.5959 |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |          -0.0064 |          57.0126 |           6.6228 |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |          -0.0058 |          56.6764 |           6.5710 |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |          -0.0074 |          56.5885 |           6.5631 |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |          -0.0079 |          56.6217 |           6.5897 |
[32m[20221213 23:33:51 @agent_ppo2.py:185][0m |          -0.0082 |          56.3281 |           6.5296 |
[32m[20221213 23:33:52 @agent_ppo2.py:185][0m |          -0.0099 |          56.3090 |           6.5724 |
[32m[20221213 23:33:52 @agent_ppo2.py:185][0m |          -0.0081 |          56.2076 |           6.5632 |
[32m[20221213 23:33:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:33:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.53
[32m[20221213 23:33:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.51
[32m[20221213 23:33:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.50
[32m[20221213 23:33:52 @agent_ppo2.py:143][0m Total time:      21.33 min
[32m[20221213 23:33:52 @agent_ppo2.py:145][0m 2062336 total steps have happened
[32m[20221213 23:33:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3007 --------------------------#
[32m[20221213 23:33:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:52 @agent_ppo2.py:185][0m |          -0.0025 |          55.4200 |           6.4001 |
[32m[20221213 23:33:52 @agent_ppo2.py:185][0m |           0.0016 |          52.8019 |           6.4355 |
[32m[20221213 23:33:52 @agent_ppo2.py:185][0m |          -0.0105 |          50.4382 |           6.4549 |
[32m[20221213 23:33:52 @agent_ppo2.py:185][0m |          -0.0051 |          50.1308 |           6.4335 |
[32m[20221213 23:33:53 @agent_ppo2.py:185][0m |          -0.0065 |          50.2633 |           6.4046 |
[32m[20221213 23:33:53 @agent_ppo2.py:185][0m |          -0.0115 |          49.6380 |           6.4790 |
[32m[20221213 23:33:53 @agent_ppo2.py:185][0m |          -0.0063 |          50.6885 |           6.4330 |
[32m[20221213 23:33:53 @agent_ppo2.py:185][0m |          -0.0041 |          51.6046 |           6.4516 |
[32m[20221213 23:33:53 @agent_ppo2.py:185][0m |          -0.0097 |          49.2734 |           6.3493 |
[32m[20221213 23:33:53 @agent_ppo2.py:185][0m |          -0.0092 |          49.3104 |           6.3859 |
[32m[20221213 23:33:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:33:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.19
[32m[20221213 23:33:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.18
[32m[20221213 23:33:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.60
[32m[20221213 23:33:53 @agent_ppo2.py:143][0m Total time:      21.35 min
[32m[20221213 23:33:53 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221213 23:33:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3008 --------------------------#
[32m[20221213 23:33:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:53 @agent_ppo2.py:185][0m |          -0.0011 |          52.4651 |           6.2972 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |           0.0029 |          50.0314 |           6.3757 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |           0.0005 |          50.3376 |           6.3252 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |          -0.0063 |          49.0106 |           6.3520 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |          -0.0092 |          48.7713 |           6.3286 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |          -0.0062 |          48.8505 |           6.2640 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |          -0.0091 |          48.2769 |           6.3347 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |          -0.0043 |          48.4610 |           6.2410 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |          -0.0104 |          47.9912 |           6.2822 |
[32m[20221213 23:33:54 @agent_ppo2.py:185][0m |          -0.0068 |          48.4335 |           6.2543 |
[32m[20221213 23:33:54 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 23:33:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.77
[32m[20221213 23:33:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.88
[32m[20221213 23:33:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.92
[32m[20221213 23:33:54 @agent_ppo2.py:143][0m Total time:      21.37 min
[32m[20221213 23:33:54 @agent_ppo2.py:145][0m 2066432 total steps have happened
[32m[20221213 23:33:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3009 --------------------------#
[32m[20221213 23:33:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:33:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |           0.0023 |          57.8858 |           6.3352 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0073 |          54.4016 |           6.4140 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0083 |          53.7402 |           6.3913 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0128 |          53.3327 |           6.3915 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0123 |          52.9976 |           6.3726 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0135 |          52.8526 |           6.3579 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0146 |          52.6114 |           6.4184 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0143 |          52.3463 |           6.3371 |
[32m[20221213 23:33:55 @agent_ppo2.py:185][0m |          -0.0128 |          52.0530 |           6.4061 |
[32m[20221213 23:33:56 @agent_ppo2.py:185][0m |          -0.0134 |          52.2124 |           6.3287 |
[32m[20221213 23:33:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:33:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.75
[32m[20221213 23:33:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.82
[32m[20221213 23:33:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.64
[32m[20221213 23:33:56 @agent_ppo2.py:143][0m Total time:      21.40 min
[32m[20221213 23:33:56 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221213 23:33:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3010 --------------------------#
[32m[20221213 23:33:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:33:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:56 @agent_ppo2.py:185][0m |           0.0015 |          49.2344 |           6.6794 |
[32m[20221213 23:33:56 @agent_ppo2.py:185][0m |          -0.0090 |          45.6041 |           6.6676 |
[32m[20221213 23:33:56 @agent_ppo2.py:185][0m |          -0.0101 |          44.7119 |           6.7195 |
[32m[20221213 23:33:56 @agent_ppo2.py:185][0m |          -0.0115 |          43.9888 |           6.6858 |
[32m[20221213 23:33:56 @agent_ppo2.py:185][0m |          -0.0140 |          43.6112 |           6.7453 |
[32m[20221213 23:33:57 @agent_ppo2.py:185][0m |          -0.0118 |          43.2345 |           6.6227 |
[32m[20221213 23:33:57 @agent_ppo2.py:185][0m |          -0.0048 |          43.9117 |           6.6407 |
[32m[20221213 23:33:57 @agent_ppo2.py:185][0m |          -0.0139 |          42.3353 |           6.7091 |
[32m[20221213 23:33:57 @agent_ppo2.py:185][0m |          -0.0175 |          42.0386 |           6.7255 |
[32m[20221213 23:33:57 @agent_ppo2.py:185][0m |          -0.0170 |          41.6790 |           6.6698 |
[32m[20221213 23:33:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:33:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.83
[32m[20221213 23:33:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.20
[32m[20221213 23:33:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.43
[32m[20221213 23:33:57 @agent_ppo2.py:143][0m Total time:      21.42 min
[32m[20221213 23:33:57 @agent_ppo2.py:145][0m 2070528 total steps have happened
[32m[20221213 23:33:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3011 --------------------------#
[32m[20221213 23:33:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:57 @agent_ppo2.py:185][0m |           0.0063 |          62.0851 |           6.3527 |
[32m[20221213 23:33:57 @agent_ppo2.py:185][0m |          -0.0038 |          60.2610 |           6.4959 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0051 |          59.1023 |           6.4005 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0070 |          58.7712 |           6.4296 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0101 |          58.1649 |           6.4086 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0136 |          58.1401 |           6.4951 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0098 |          57.6336 |           6.3980 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0145 |          57.4765 |           6.4517 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0073 |          57.8222 |           6.4597 |
[32m[20221213 23:33:58 @agent_ppo2.py:185][0m |          -0.0126 |          57.3614 |           6.4489 |
[32m[20221213 23:33:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:33:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.78
[32m[20221213 23:33:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.24
[32m[20221213 23:33:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.57
[32m[20221213 23:33:58 @agent_ppo2.py:143][0m Total time:      21.44 min
[32m[20221213 23:33:58 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221213 23:33:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3012 --------------------------#
[32m[20221213 23:33:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:33:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0019 |          56.3426 |           6.4079 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0064 |          51.5855 |           6.4424 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0085 |          50.2082 |           6.5015 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0097 |          49.8802 |           6.5029 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0027 |          55.7749 |           6.5608 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0149 |          47.9373 |           6.4902 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0194 |          47.6552 |           6.5729 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0180 |          47.2725 |           6.5045 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0177 |          47.0208 |           6.5606 |
[32m[20221213 23:33:59 @agent_ppo2.py:185][0m |          -0.0226 |          46.8831 |           6.5360 |
[32m[20221213 23:33:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.70
[32m[20221213 23:34:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.31
[32m[20221213 23:34:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.51
[32m[20221213 23:34:00 @agent_ppo2.py:143][0m Total time:      21.46 min
[32m[20221213 23:34:00 @agent_ppo2.py:145][0m 2074624 total steps have happened
[32m[20221213 23:34:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3013 --------------------------#
[32m[20221213 23:34:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |           0.0046 |          58.9870 |           6.5008 |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |          -0.0051 |          56.2419 |           6.5002 |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |          -0.0064 |          56.0228 |           6.5072 |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |          -0.0023 |          57.2325 |           6.4951 |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |          -0.0080 |          55.3732 |           6.5480 |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |          -0.0082 |          54.9927 |           6.5063 |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |          -0.0035 |          55.7964 |           6.5270 |
[32m[20221213 23:34:00 @agent_ppo2.py:185][0m |          -0.0087 |          54.9738 |           6.4748 |
[32m[20221213 23:34:01 @agent_ppo2.py:185][0m |          -0.0109 |          54.4710 |           6.4731 |
[32m[20221213 23:34:01 @agent_ppo2.py:185][0m |          -0.0130 |          54.4120 |           6.4272 |
[32m[20221213 23:34:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.73
[32m[20221213 23:34:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.24
[32m[20221213 23:34:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.58
[32m[20221213 23:34:01 @agent_ppo2.py:143][0m Total time:      21.48 min
[32m[20221213 23:34:01 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221213 23:34:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3014 --------------------------#
[32m[20221213 23:34:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:01 @agent_ppo2.py:185][0m |           0.0020 |          51.5523 |           6.6207 |
[32m[20221213 23:34:01 @agent_ppo2.py:185][0m |          -0.0040 |          46.4017 |           6.7421 |
[32m[20221213 23:34:01 @agent_ppo2.py:185][0m |          -0.0093 |          43.9070 |           6.7225 |
[32m[20221213 23:34:01 @agent_ppo2.py:185][0m |          -0.0072 |          42.0848 |           6.7059 |
[32m[20221213 23:34:02 @agent_ppo2.py:185][0m |          -0.0086 |          41.6231 |           6.7405 |
[32m[20221213 23:34:02 @agent_ppo2.py:185][0m |          -0.0133 |          40.9708 |           6.7640 |
[32m[20221213 23:34:02 @agent_ppo2.py:185][0m |          -0.0119 |          41.5742 |           6.7742 |
[32m[20221213 23:34:02 @agent_ppo2.py:185][0m |          -0.0129 |          40.2032 |           6.7504 |
[32m[20221213 23:34:02 @agent_ppo2.py:185][0m |          -0.0145 |          40.0499 |           6.7291 |
[32m[20221213 23:34:02 @agent_ppo2.py:185][0m |          -0.0109 |          40.0773 |           6.8040 |
[32m[20221213 23:34:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.23
[32m[20221213 23:34:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.59
[32m[20221213 23:34:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.94
[32m[20221213 23:34:02 @agent_ppo2.py:143][0m Total time:      21.50 min
[32m[20221213 23:34:02 @agent_ppo2.py:145][0m 2078720 total steps have happened
[32m[20221213 23:34:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3015 --------------------------#
[32m[20221213 23:34:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:02 @agent_ppo2.py:185][0m |          -0.0023 |          64.6754 |           6.9720 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0099 |          61.8726 |           6.9327 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0120 |          61.0252 |           6.9972 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0073 |          61.9878 |           7.0386 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0126 |          60.1082 |           7.0460 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0147 |          59.7192 |           7.0556 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0158 |          59.3375 |           7.0609 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0135 |          59.1288 |           6.9686 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0147 |          59.0326 |           7.1131 |
[32m[20221213 23:34:03 @agent_ppo2.py:185][0m |          -0.0170 |          58.8091 |           7.1578 |
[32m[20221213 23:34:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:34:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.99
[32m[20221213 23:34:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.96
[32m[20221213 23:34:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.79
[32m[20221213 23:34:03 @agent_ppo2.py:143][0m Total time:      21.52 min
[32m[20221213 23:34:03 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221213 23:34:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3016 --------------------------#
[32m[20221213 23:34:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0032 |          61.0902 |           6.5154 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0092 |          58.3550 |           6.6001 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0076 |          57.3028 |           6.6105 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0083 |          56.3632 |           6.6088 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0104 |          55.7312 |           6.6254 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0105 |          55.0521 |           6.6356 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0002 |          59.6403 |           6.6516 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0107 |          54.1028 |           6.6699 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0096 |          53.6877 |           6.6606 |
[32m[20221213 23:34:04 @agent_ppo2.py:185][0m |          -0.0139 |          53.2265 |           6.6750 |
[32m[20221213 23:34:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.27
[32m[20221213 23:34:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.02
[32m[20221213 23:34:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.24
[32m[20221213 23:34:05 @agent_ppo2.py:143][0m Total time:      21.54 min
[32m[20221213 23:34:05 @agent_ppo2.py:145][0m 2082816 total steps have happened
[32m[20221213 23:34:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3017 --------------------------#
[32m[20221213 23:34:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:05 @agent_ppo2.py:185][0m |           0.0027 |          54.5645 |           7.0956 |
[32m[20221213 23:34:05 @agent_ppo2.py:185][0m |          -0.0052 |          51.4079 |           7.0743 |
[32m[20221213 23:34:05 @agent_ppo2.py:185][0m |          -0.0094 |          50.4912 |           7.1229 |
[32m[20221213 23:34:05 @agent_ppo2.py:185][0m |          -0.0073 |          49.9448 |           7.0742 |
[32m[20221213 23:34:05 @agent_ppo2.py:185][0m |          -0.0105 |          49.4867 |           7.0996 |
[32m[20221213 23:34:05 @agent_ppo2.py:185][0m |          -0.0106 |          49.1937 |           7.0917 |
[32m[20221213 23:34:05 @agent_ppo2.py:185][0m |          -0.0088 |          48.9917 |           7.0266 |
[32m[20221213 23:34:06 @agent_ppo2.py:185][0m |          -0.0090 |          48.8099 |           7.1773 |
[32m[20221213 23:34:06 @agent_ppo2.py:185][0m |          -0.0149 |          48.6773 |           7.0778 |
[32m[20221213 23:34:06 @agent_ppo2.py:185][0m |          -0.0086 |          49.1172 |           7.0895 |
[32m[20221213 23:34:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.23
[32m[20221213 23:34:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.00
[32m[20221213 23:34:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.21
[32m[20221213 23:34:06 @agent_ppo2.py:143][0m Total time:      21.56 min
[32m[20221213 23:34:06 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221213 23:34:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3018 --------------------------#
[32m[20221213 23:34:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:06 @agent_ppo2.py:185][0m |           0.0036 |          60.0768 |           7.7003 |
[32m[20221213 23:34:06 @agent_ppo2.py:185][0m |          -0.0036 |          55.1005 |           7.6242 |
[32m[20221213 23:34:06 @agent_ppo2.py:185][0m |          -0.0071 |          53.7393 |           7.6750 |
[32m[20221213 23:34:06 @agent_ppo2.py:185][0m |          -0.0103 |          53.2600 |           7.5918 |
[32m[20221213 23:34:07 @agent_ppo2.py:185][0m |          -0.0097 |          52.8473 |           7.5675 |
[32m[20221213 23:34:07 @agent_ppo2.py:185][0m |          -0.0090 |          52.4030 |           7.5968 |
[32m[20221213 23:34:07 @agent_ppo2.py:185][0m |          -0.0172 |          52.2641 |           7.5618 |
[32m[20221213 23:34:07 @agent_ppo2.py:185][0m |          -0.0148 |          51.8188 |           7.5818 |
[32m[20221213 23:34:07 @agent_ppo2.py:185][0m |          -0.0136 |          51.5626 |           7.5133 |
[32m[20221213 23:34:07 @agent_ppo2.py:185][0m |          -0.0154 |          51.3001 |           7.5310 |
[32m[20221213 23:34:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.73
[32m[20221213 23:34:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.21
[32m[20221213 23:34:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.06
[32m[20221213 23:34:07 @agent_ppo2.py:143][0m Total time:      21.59 min
[32m[20221213 23:34:07 @agent_ppo2.py:145][0m 2086912 total steps have happened
[32m[20221213 23:34:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3019 --------------------------#
[32m[20221213 23:34:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:07 @agent_ppo2.py:185][0m |           0.0040 |          62.3382 |           6.8274 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0037 |          59.9193 |           6.8073 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |           0.0025 |          60.9952 |           6.7832 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0074 |          58.0394 |           6.7782 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0080 |          57.5944 |           6.7528 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0080 |          57.3916 |           6.8390 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0082 |          56.9706 |           6.7800 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0071 |          58.5418 |           6.8088 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0108 |          56.5009 |           6.7652 |
[32m[20221213 23:34:08 @agent_ppo2.py:185][0m |          -0.0145 |          56.4972 |           6.7166 |
[32m[20221213 23:34:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.88
[32m[20221213 23:34:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.84
[32m[20221213 23:34:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.73
[32m[20221213 23:34:08 @agent_ppo2.py:143][0m Total time:      21.61 min
[32m[20221213 23:34:08 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221213 23:34:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3020 --------------------------#
[32m[20221213 23:34:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:34:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |           0.0133 |          56.9026 |           6.3110 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |           0.0115 |          52.3028 |           6.3850 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |          -0.0031 |          45.3858 |           6.3481 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |          -0.0075 |          44.3871 |           6.3949 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |          -0.0124 |          44.0475 |           6.3299 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |          -0.0133 |          43.2404 |           6.3846 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |          -0.0093 |          43.0143 |           6.3891 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |          -0.0129 |          42.6281 |           6.3763 |
[32m[20221213 23:34:09 @agent_ppo2.py:185][0m |          -0.0128 |          42.4884 |           6.4038 |
[32m[20221213 23:34:10 @agent_ppo2.py:185][0m |          -0.0144 |          42.2435 |           6.4494 |
[32m[20221213 23:34:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:34:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.36
[32m[20221213 23:34:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.29
[32m[20221213 23:34:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.92
[32m[20221213 23:34:10 @agent_ppo2.py:143][0m Total time:      21.63 min
[32m[20221213 23:34:10 @agent_ppo2.py:145][0m 2091008 total steps have happened
[32m[20221213 23:34:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3021 --------------------------#
[32m[20221213 23:34:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:10 @agent_ppo2.py:185][0m |           0.0101 |          60.4831 |           7.1835 |
[32m[20221213 23:34:10 @agent_ppo2.py:185][0m |          -0.0052 |          47.7074 |           7.1035 |
[32m[20221213 23:34:10 @agent_ppo2.py:185][0m |          -0.0057 |          45.5481 |           7.1391 |
[32m[20221213 23:34:10 @agent_ppo2.py:185][0m |          -0.0049 |          44.1485 |           7.2024 |
[32m[20221213 23:34:10 @agent_ppo2.py:185][0m |          -0.0066 |          43.3390 |           7.0996 |
[32m[20221213 23:34:10 @agent_ppo2.py:185][0m |          -0.0045 |          42.4635 |           7.0873 |
[32m[20221213 23:34:11 @agent_ppo2.py:185][0m |          -0.0121 |          42.1458 |           7.1149 |
[32m[20221213 23:34:11 @agent_ppo2.py:185][0m |          -0.0098 |          41.6067 |           7.0993 |
[32m[20221213 23:34:11 @agent_ppo2.py:185][0m |          -0.0072 |          41.0316 |           7.0822 |
[32m[20221213 23:34:11 @agent_ppo2.py:185][0m |          -0.0112 |          40.5698 |           7.0846 |
[32m[20221213 23:34:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.73
[32m[20221213 23:34:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.75
[32m[20221213 23:34:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.44
[32m[20221213 23:34:11 @agent_ppo2.py:143][0m Total time:      21.65 min
[32m[20221213 23:34:11 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221213 23:34:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3022 --------------------------#
[32m[20221213 23:34:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:11 @agent_ppo2.py:185][0m |           0.0019 |          58.4135 |           6.4500 |
[32m[20221213 23:34:11 @agent_ppo2.py:185][0m |          -0.0023 |          53.4118 |           6.3780 |
[32m[20221213 23:34:11 @agent_ppo2.py:185][0m |          -0.0078 |          51.9736 |           6.4271 |
[32m[20221213 23:34:12 @agent_ppo2.py:185][0m |           0.0013 |          54.0315 |           6.3705 |
[32m[20221213 23:34:12 @agent_ppo2.py:185][0m |          -0.0095 |          50.6897 |           6.3469 |
[32m[20221213 23:34:12 @agent_ppo2.py:185][0m |          -0.0086 |          50.5198 |           6.3174 |
[32m[20221213 23:34:12 @agent_ppo2.py:185][0m |          -0.0104 |          50.0304 |           6.3316 |
[32m[20221213 23:34:12 @agent_ppo2.py:185][0m |          -0.0133 |          49.8190 |           6.3838 |
[32m[20221213 23:34:12 @agent_ppo2.py:185][0m |          -0.0164 |          49.3770 |           6.3720 |
[32m[20221213 23:34:12 @agent_ppo2.py:185][0m |          -0.0138 |          49.2580 |           6.3251 |
[32m[20221213 23:34:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.50
[32m[20221213 23:34:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.25
[32m[20221213 23:34:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.12
[32m[20221213 23:34:12 @agent_ppo2.py:143][0m Total time:      21.67 min
[32m[20221213 23:34:12 @agent_ppo2.py:145][0m 2095104 total steps have happened
[32m[20221213 23:34:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3023 --------------------------#
[32m[20221213 23:34:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |           0.0063 |          62.5329 |           5.8948 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0031 |          53.2715 |           5.9352 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0016 |          52.3084 |           5.8911 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0117 |          50.7244 |           5.9576 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0132 |          49.8736 |           5.9511 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0117 |          49.4359 |           5.9465 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0114 |          48.8953 |           5.9769 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0037 |          50.6889 |           6.0258 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0088 |          48.5917 |           6.0420 |
[32m[20221213 23:34:13 @agent_ppo2.py:185][0m |          -0.0087 |          48.0296 |           6.0107 |
[32m[20221213 23:34:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:34:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.18
[32m[20221213 23:34:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.61
[32m[20221213 23:34:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.60
[32m[20221213 23:34:13 @agent_ppo2.py:143][0m Total time:      21.69 min
[32m[20221213 23:34:13 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221213 23:34:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3024 --------------------------#
[32m[20221213 23:34:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |           0.0047 |          67.6669 |           6.6319 |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |          -0.0053 |          64.9461 |           6.5781 |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |           0.0006 |          65.9750 |           6.6034 |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |          -0.0022 |          62.8115 |           6.6336 |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |          -0.0082 |          62.3097 |           6.6251 |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |          -0.0111 |          61.7071 |           6.5787 |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |          -0.0109 |          61.4940 |           6.5839 |
[32m[20221213 23:34:14 @agent_ppo2.py:185][0m |          -0.0133 |          61.2428 |           6.5690 |
[32m[20221213 23:34:15 @agent_ppo2.py:185][0m |          -0.0125 |          61.0560 |           6.5405 |
[32m[20221213 23:34:15 @agent_ppo2.py:185][0m |          -0.0061 |          63.6342 |           6.5519 |
[32m[20221213 23:34:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:34:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.42
[32m[20221213 23:34:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.34
[32m[20221213 23:34:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.78
[32m[20221213 23:34:15 @agent_ppo2.py:143][0m Total time:      21.71 min
[32m[20221213 23:34:15 @agent_ppo2.py:145][0m 2099200 total steps have happened
[32m[20221213 23:34:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3025 --------------------------#
[32m[20221213 23:34:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:15 @agent_ppo2.py:185][0m |          -0.0018 |          55.7775 |           6.1294 |
[32m[20221213 23:34:15 @agent_ppo2.py:185][0m |          -0.0035 |          50.9600 |           6.2386 |
[32m[20221213 23:34:15 @agent_ppo2.py:185][0m |          -0.0056 |          49.6335 |           6.1897 |
[32m[20221213 23:34:15 @agent_ppo2.py:185][0m |          -0.0033 |          49.1666 |           6.2263 |
[32m[20221213 23:34:15 @agent_ppo2.py:185][0m |          -0.0125 |          48.1029 |           6.2302 |
[32m[20221213 23:34:16 @agent_ppo2.py:185][0m |          -0.0133 |          47.7677 |           6.2589 |
[32m[20221213 23:34:16 @agent_ppo2.py:185][0m |          -0.0077 |          47.5436 |           6.2745 |
[32m[20221213 23:34:16 @agent_ppo2.py:185][0m |          -0.0105 |          47.1122 |           6.3116 |
[32m[20221213 23:34:16 @agent_ppo2.py:185][0m |          -0.0086 |          46.8278 |           6.3011 |
[32m[20221213 23:34:16 @agent_ppo2.py:185][0m |          -0.0073 |          46.5660 |           6.3389 |
[32m[20221213 23:34:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.61
[32m[20221213 23:34:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.41
[32m[20221213 23:34:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.57
[32m[20221213 23:34:16 @agent_ppo2.py:143][0m Total time:      21.73 min
[32m[20221213 23:34:16 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221213 23:34:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3026 --------------------------#
[32m[20221213 23:34:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:16 @agent_ppo2.py:185][0m |           0.0006 |          53.4189 |           6.1517 |
[32m[20221213 23:34:16 @agent_ppo2.py:185][0m |           0.0035 |          53.1263 |           6.2593 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |           0.0001 |          46.7936 |           6.2941 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |          -0.0100 |          44.3618 |           6.3823 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |          -0.0128 |          43.7258 |           6.3830 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |          -0.0134 |          43.3314 |           6.4442 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |          -0.0133 |          42.8031 |           6.4818 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |          -0.0129 |          43.7170 |           6.5157 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |          -0.0138 |          42.2363 |           6.5818 |
[32m[20221213 23:34:17 @agent_ppo2.py:185][0m |          -0.0157 |          41.8888 |           6.6361 |
[32m[20221213 23:34:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.88
[32m[20221213 23:34:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.28
[32m[20221213 23:34:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.14
[32m[20221213 23:34:17 @agent_ppo2.py:143][0m Total time:      21.75 min
[32m[20221213 23:34:17 @agent_ppo2.py:145][0m 2103296 total steps have happened
[32m[20221213 23:34:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3027 --------------------------#
[32m[20221213 23:34:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0006 |          63.5467 |           7.3529 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0047 |          62.3817 |           7.3682 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0067 |          61.9868 |           7.3770 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0087 |          61.7474 |           7.4158 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0152 |          61.6266 |           7.3978 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0120 |          61.4836 |           7.4471 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0133 |          61.0538 |           7.5186 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0168 |          60.8998 |           7.4769 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0134 |          60.8403 |           7.5478 |
[32m[20221213 23:34:18 @agent_ppo2.py:185][0m |          -0.0152 |          60.7695 |           7.5212 |
[32m[20221213 23:34:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.84
[32m[20221213 23:34:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.83
[32m[20221213 23:34:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.51
[32m[20221213 23:34:19 @agent_ppo2.py:143][0m Total time:      21.78 min
[32m[20221213 23:34:19 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221213 23:34:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3028 --------------------------#
[32m[20221213 23:34:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |           0.0014 |          64.9458 |           7.6919 |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |          -0.0086 |          63.2670 |           7.7204 |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |          -0.0090 |          62.2371 |           7.6981 |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |          -0.0090 |          61.5951 |           7.7916 |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |          -0.0093 |          61.0929 |           7.8075 |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |          -0.0123 |          60.8156 |           7.8469 |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |          -0.0092 |          60.8269 |           7.8807 |
[32m[20221213 23:34:19 @agent_ppo2.py:185][0m |          -0.0148 |          60.2400 |           7.9130 |
[32m[20221213 23:34:20 @agent_ppo2.py:185][0m |          -0.0127 |          59.9426 |           7.9477 |
[32m[20221213 23:34:20 @agent_ppo2.py:185][0m |          -0.0139 |          59.9350 |           7.9497 |
[32m[20221213 23:34:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.60
[32m[20221213 23:34:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.17
[32m[20221213 23:34:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.08
[32m[20221213 23:34:20 @agent_ppo2.py:143][0m Total time:      21.80 min
[32m[20221213 23:34:20 @agent_ppo2.py:145][0m 2107392 total steps have happened
[32m[20221213 23:34:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3029 --------------------------#
[32m[20221213 23:34:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:20 @agent_ppo2.py:185][0m |          -0.0017 |          55.5433 |           7.9455 |
[32m[20221213 23:34:20 @agent_ppo2.py:185][0m |          -0.0038 |          46.7238 |           7.9669 |
[32m[20221213 23:34:20 @agent_ppo2.py:185][0m |          -0.0005 |          45.8828 |           7.9371 |
[32m[20221213 23:34:20 @agent_ppo2.py:185][0m |          -0.0113 |          44.6964 |           7.9334 |
[32m[20221213 23:34:20 @agent_ppo2.py:185][0m |          -0.0112 |          44.1261 |           7.8916 |
[32m[20221213 23:34:21 @agent_ppo2.py:185][0m |          -0.0118 |          43.9218 |           7.9126 |
[32m[20221213 23:34:21 @agent_ppo2.py:185][0m |          -0.0085 |          43.2806 |           7.8670 |
[32m[20221213 23:34:21 @agent_ppo2.py:185][0m |          -0.0150 |          43.2983 |           7.8572 |
[32m[20221213 23:34:21 @agent_ppo2.py:185][0m |          -0.0143 |          43.3498 |           7.8303 |
[32m[20221213 23:34:21 @agent_ppo2.py:185][0m |          -0.0129 |          42.8798 |           7.8741 |
[32m[20221213 23:34:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:34:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.83
[32m[20221213 23:34:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.84
[32m[20221213 23:34:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.54
[32m[20221213 23:34:21 @agent_ppo2.py:143][0m Total time:      21.82 min
[32m[20221213 23:34:21 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221213 23:34:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3030 --------------------------#
[32m[20221213 23:34:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:34:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:21 @agent_ppo2.py:185][0m |           0.0012 |          51.0651 |           7.2972 |
[32m[20221213 23:34:21 @agent_ppo2.py:185][0m |          -0.0049 |          48.2881 |           7.3416 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |          -0.0060 |          47.6723 |           7.4064 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |          -0.0016 |          47.1982 |           7.3461 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |          -0.0091 |          46.9243 |           7.3254 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |          -0.0065 |          46.6107 |           7.3290 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |          -0.0111 |          46.2082 |           7.2828 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |          -0.0110 |          46.0143 |           7.2462 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |           0.0011 |          47.4016 |           7.2599 |
[32m[20221213 23:34:22 @agent_ppo2.py:185][0m |          -0.0104 |          46.0475 |           7.2276 |
[32m[20221213 23:34:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.55
[32m[20221213 23:34:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.95
[32m[20221213 23:34:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.44
[32m[20221213 23:34:22 @agent_ppo2.py:143][0m Total time:      21.84 min
[32m[20221213 23:34:22 @agent_ppo2.py:145][0m 2111488 total steps have happened
[32m[20221213 23:34:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3031 --------------------------#
[32m[20221213 23:34:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |           0.0008 |          68.8719 |           7.0097 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0066 |          63.0775 |           7.0051 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0074 |          61.0363 |           7.0038 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0122 |          60.1377 |           6.9837 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0083 |          59.7891 |           6.9801 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0131 |          59.2310 |           6.9584 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0070 |          59.1055 |           6.9152 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0178 |          58.8903 |           6.9078 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0159 |          58.4271 |           6.9188 |
[32m[20221213 23:34:23 @agent_ppo2.py:185][0m |          -0.0167 |          58.8294 |           6.8805 |
[32m[20221213 23:34:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.82
[32m[20221213 23:34:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.38
[32m[20221213 23:34:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.73
[32m[20221213 23:34:24 @agent_ppo2.py:143][0m Total time:      21.86 min
[32m[20221213 23:34:24 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221213 23:34:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3032 --------------------------#
[32m[20221213 23:34:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:24 @agent_ppo2.py:185][0m |           0.0062 |          54.1264 |           6.6851 |
[32m[20221213 23:34:24 @agent_ppo2.py:185][0m |          -0.0048 |          48.0602 |           6.6972 |
[32m[20221213 23:34:24 @agent_ppo2.py:185][0m |          -0.0091 |          46.0508 |           6.6517 |
[32m[20221213 23:34:24 @agent_ppo2.py:185][0m |          -0.0119 |          44.9975 |           6.6840 |
[32m[20221213 23:34:24 @agent_ppo2.py:185][0m |          -0.0083 |          44.3014 |           6.6900 |
[32m[20221213 23:34:24 @agent_ppo2.py:185][0m |          -0.0117 |          43.8212 |           6.7015 |
[32m[20221213 23:34:24 @agent_ppo2.py:185][0m |          -0.0070 |          44.9257 |           6.7536 |
[32m[20221213 23:34:25 @agent_ppo2.py:185][0m |          -0.0164 |          43.0523 |           6.7541 |
[32m[20221213 23:34:25 @agent_ppo2.py:185][0m |          -0.0135 |          42.8988 |           6.7476 |
[32m[20221213 23:34:25 @agent_ppo2.py:185][0m |          -0.0148 |          42.5919 |           6.8071 |
[32m[20221213 23:34:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.41
[32m[20221213 23:34:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.12
[32m[20221213 23:34:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.68
[32m[20221213 23:34:25 @agent_ppo2.py:143][0m Total time:      21.88 min
[32m[20221213 23:34:25 @agent_ppo2.py:145][0m 2115584 total steps have happened
[32m[20221213 23:34:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3033 --------------------------#
[32m[20221213 23:34:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:25 @agent_ppo2.py:185][0m |           0.0007 |          47.6701 |           6.5252 |
[32m[20221213 23:34:25 @agent_ppo2.py:185][0m |          -0.0029 |          45.5534 |           6.4857 |
[32m[20221213 23:34:25 @agent_ppo2.py:185][0m |          -0.0060 |          43.6856 |           6.4532 |
[32m[20221213 23:34:25 @agent_ppo2.py:185][0m |          -0.0110 |          42.9432 |           6.4108 |
[32m[20221213 23:34:26 @agent_ppo2.py:185][0m |          -0.0137 |          42.4598 |           6.3691 |
[32m[20221213 23:34:26 @agent_ppo2.py:185][0m |          -0.0119 |          42.1777 |           6.3023 |
[32m[20221213 23:34:26 @agent_ppo2.py:185][0m |          -0.0131 |          41.8368 |           6.3365 |
[32m[20221213 23:34:26 @agent_ppo2.py:185][0m |          -0.0097 |          41.5323 |           6.3205 |
[32m[20221213 23:34:26 @agent_ppo2.py:185][0m |          -0.0141 |          41.4303 |           6.2783 |
[32m[20221213 23:34:26 @agent_ppo2.py:185][0m |          -0.0119 |          41.2201 |           6.3143 |
[32m[20221213 23:34:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.98
[32m[20221213 23:34:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.71
[32m[20221213 23:34:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.84
[32m[20221213 23:34:26 @agent_ppo2.py:143][0m Total time:      21.90 min
[32m[20221213 23:34:26 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221213 23:34:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3034 --------------------------#
[32m[20221213 23:34:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:26 @agent_ppo2.py:185][0m |           0.0043 |          48.5528 |           6.3474 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0022 |          43.6289 |           6.3402 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0031 |          42.0551 |           6.3614 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0048 |          40.9403 |           6.3123 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0047 |          40.0533 |           6.3196 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0128 |          39.6269 |           6.2885 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0094 |          39.1380 |           6.3204 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0111 |          38.6915 |           6.2459 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0118 |          38.2830 |           6.2534 |
[32m[20221213 23:34:27 @agent_ppo2.py:185][0m |          -0.0137 |          38.0376 |           6.3051 |
[32m[20221213 23:34:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:34:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.20
[32m[20221213 23:34:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.73
[32m[20221213 23:34:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.01
[32m[20221213 23:34:27 @agent_ppo2.py:143][0m Total time:      21.92 min
[32m[20221213 23:34:27 @agent_ppo2.py:145][0m 2119680 total steps have happened
[32m[20221213 23:34:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3035 --------------------------#
[32m[20221213 23:34:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |           0.0059 |          47.6171 |           7.0812 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0069 |          42.9727 |           7.0095 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0063 |          41.3106 |           7.0513 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0082 |          40.2998 |           7.0105 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0183 |          39.8812 |           7.0223 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0110 |          38.8907 |           6.9786 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0113 |          38.4633 |           6.9969 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0151 |          38.0036 |           7.0237 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0063 |          37.8212 |           6.9528 |
[32m[20221213 23:34:28 @agent_ppo2.py:185][0m |          -0.0149 |          37.3384 |           6.9686 |
[32m[20221213 23:34:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.02
[32m[20221213 23:34:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.89
[32m[20221213 23:34:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.98
[32m[20221213 23:34:29 @agent_ppo2.py:143][0m Total time:      21.94 min
[32m[20221213 23:34:29 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221213 23:34:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3036 --------------------------#
[32m[20221213 23:34:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:29 @agent_ppo2.py:185][0m |           0.0027 |          59.0455 |           5.8778 |
[32m[20221213 23:34:29 @agent_ppo2.py:185][0m |          -0.0115 |          55.0156 |           5.8395 |
[32m[20221213 23:34:29 @agent_ppo2.py:185][0m |          -0.0115 |          54.4875 |           5.7652 |
[32m[20221213 23:34:29 @agent_ppo2.py:185][0m |          -0.0078 |          53.9954 |           5.7431 |
[32m[20221213 23:34:29 @agent_ppo2.py:185][0m |          -0.0116 |          53.7437 |           5.7306 |
[32m[20221213 23:34:29 @agent_ppo2.py:185][0m |          -0.0122 |          53.6826 |           5.6938 |
[32m[20221213 23:34:29 @agent_ppo2.py:185][0m |          -0.0114 |          53.5288 |           5.7067 |
[32m[20221213 23:34:30 @agent_ppo2.py:185][0m |          -0.0123 |          53.3476 |           5.6879 |
[32m[20221213 23:34:30 @agent_ppo2.py:185][0m |          -0.0131 |          53.2094 |           5.7143 |
[32m[20221213 23:34:30 @agent_ppo2.py:185][0m |          -0.0161 |          53.2279 |           5.7046 |
[32m[20221213 23:34:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.32
[32m[20221213 23:34:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.98
[32m[20221213 23:34:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.52
[32m[20221213 23:34:30 @agent_ppo2.py:143][0m Total time:      21.96 min
[32m[20221213 23:34:30 @agent_ppo2.py:145][0m 2123776 total steps have happened
[32m[20221213 23:34:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3037 --------------------------#
[32m[20221213 23:34:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:30 @agent_ppo2.py:185][0m |          -0.0051 |          28.7648 |           7.0855 |
[32m[20221213 23:34:30 @agent_ppo2.py:185][0m |           0.0008 |          24.3374 |           7.1355 |
[32m[20221213 23:34:30 @agent_ppo2.py:185][0m |          -0.0069 |          22.7940 |           7.1559 |
[32m[20221213 23:34:30 @agent_ppo2.py:185][0m |          -0.0062 |          21.5695 |           7.1887 |
[32m[20221213 23:34:31 @agent_ppo2.py:185][0m |          -0.0093 |          20.9000 |           7.2419 |
[32m[20221213 23:34:31 @agent_ppo2.py:185][0m |          -0.0109 |          20.8617 |           7.2644 |
[32m[20221213 23:34:31 @agent_ppo2.py:185][0m |          -0.0129 |          19.8044 |           7.2857 |
[32m[20221213 23:34:31 @agent_ppo2.py:185][0m |          -0.0154 |          19.5252 |           7.3427 |
[32m[20221213 23:34:31 @agent_ppo2.py:185][0m |          -0.0196 |          19.1005 |           7.3724 |
[32m[20221213 23:34:31 @agent_ppo2.py:185][0m |          -0.0117 |          18.8016 |           7.4136 |
[32m[20221213 23:34:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.94
[32m[20221213 23:34:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.69
[32m[20221213 23:34:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.48
[32m[20221213 23:34:31 @agent_ppo2.py:143][0m Total time:      21.99 min
[32m[20221213 23:34:31 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221213 23:34:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3038 --------------------------#
[32m[20221213 23:34:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:31 @agent_ppo2.py:185][0m |           0.0044 |          56.1070 |           6.7557 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |           0.0022 |          53.3564 |           6.7289 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |          -0.0046 |          51.8145 |           6.7288 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |          -0.0052 |          50.8522 |           6.7618 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |          -0.0061 |          50.3361 |           6.8188 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |          -0.0126 |          50.1811 |           6.7952 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |           0.0006 |          53.8576 |           6.7727 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |           0.0056 |          52.0309 |           6.7912 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |          -0.0073 |          49.4605 |           6.7779 |
[32m[20221213 23:34:32 @agent_ppo2.py:185][0m |          -0.0104 |          48.8199 |           6.7907 |
[32m[20221213 23:34:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.39
[32m[20221213 23:34:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.42
[32m[20221213 23:34:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 221.02
[32m[20221213 23:34:32 @agent_ppo2.py:143][0m Total time:      22.01 min
[32m[20221213 23:34:32 @agent_ppo2.py:145][0m 2127872 total steps have happened
[32m[20221213 23:34:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3039 --------------------------#
[32m[20221213 23:34:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |           0.0105 |          67.3953 |           6.7464 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0058 |          58.0999 |           6.7153 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0063 |          57.0561 |           6.7518 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0046 |          56.3724 |           6.7277 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0095 |          55.5025 |           6.7254 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0060 |          55.1086 |           6.6937 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0112 |          54.7835 |           6.8439 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0104 |          54.7583 |           6.7087 |
[32m[20221213 23:34:33 @agent_ppo2.py:185][0m |          -0.0101 |          54.4969 |           6.7457 |
[32m[20221213 23:34:34 @agent_ppo2.py:185][0m |          -0.0126 |          54.0700 |           6.7456 |
[32m[20221213 23:34:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:34:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.16
[32m[20221213 23:34:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.96
[32m[20221213 23:34:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.87
[32m[20221213 23:34:34 @agent_ppo2.py:143][0m Total time:      22.03 min
[32m[20221213 23:34:34 @agent_ppo2.py:145][0m 2129920 total steps have happened
[32m[20221213 23:34:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3040 --------------------------#
[32m[20221213 23:34:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:34:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:34 @agent_ppo2.py:185][0m |           0.0040 |          55.4254 |           6.4765 |
[32m[20221213 23:34:34 @agent_ppo2.py:185][0m |          -0.0068 |          52.3978 |           6.4904 |
[32m[20221213 23:34:34 @agent_ppo2.py:185][0m |          -0.0125 |          51.1415 |           6.5407 |
[32m[20221213 23:34:34 @agent_ppo2.py:185][0m |          -0.0134 |          50.2015 |           6.5226 |
[32m[20221213 23:34:34 @agent_ppo2.py:185][0m |          -0.0089 |          49.8181 |           6.5759 |
[32m[20221213 23:34:34 @agent_ppo2.py:185][0m |          -0.0018 |          55.6144 |           6.5809 |
[32m[20221213 23:34:35 @agent_ppo2.py:185][0m |          -0.0123 |          48.7410 |           6.6144 |
[32m[20221213 23:34:35 @agent_ppo2.py:185][0m |          -0.0139 |          48.1986 |           6.6679 |
[32m[20221213 23:34:35 @agent_ppo2.py:185][0m |          -0.0153 |          47.9399 |           6.6852 |
[32m[20221213 23:34:35 @agent_ppo2.py:185][0m |          -0.0178 |          47.6583 |           6.6750 |
[32m[20221213 23:34:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.38
[32m[20221213 23:34:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.53
[32m[20221213 23:34:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.98
[32m[20221213 23:34:35 @agent_ppo2.py:143][0m Total time:      22.05 min
[32m[20221213 23:34:35 @agent_ppo2.py:145][0m 2131968 total steps have happened
[32m[20221213 23:34:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3041 --------------------------#
[32m[20221213 23:34:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:35 @agent_ppo2.py:185][0m |          -0.0003 |          37.6090 |           6.5985 |
[32m[20221213 23:34:35 @agent_ppo2.py:185][0m |          -0.0077 |          31.4801 |           6.5312 |
[32m[20221213 23:34:35 @agent_ppo2.py:185][0m |          -0.0078 |          29.4860 |           6.5309 |
[32m[20221213 23:34:36 @agent_ppo2.py:185][0m |          -0.0135 |          28.2555 |           6.5419 |
[32m[20221213 23:34:36 @agent_ppo2.py:185][0m |          -0.0113 |          27.7014 |           6.4801 |
[32m[20221213 23:34:36 @agent_ppo2.py:185][0m |          -0.0116 |          27.0014 |           6.4927 |
[32m[20221213 23:34:36 @agent_ppo2.py:185][0m |          -0.0167 |          26.3795 |           6.4875 |
[32m[20221213 23:34:36 @agent_ppo2.py:185][0m |          -0.0178 |          25.9189 |           6.5093 |
[32m[20221213 23:34:36 @agent_ppo2.py:185][0m |          -0.0151 |          25.4464 |           6.5684 |
[32m[20221213 23:34:36 @agent_ppo2.py:185][0m |          -0.0015 |          31.0527 |           6.4019 |
[32m[20221213 23:34:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.87
[32m[20221213 23:34:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.32
[32m[20221213 23:34:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.06
[32m[20221213 23:34:36 @agent_ppo2.py:143][0m Total time:      22.07 min
[32m[20221213 23:34:36 @agent_ppo2.py:145][0m 2134016 total steps have happened
[32m[20221213 23:34:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3042 --------------------------#
[32m[20221213 23:34:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |           0.0001 |          47.4924 |           6.7785 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0023 |          43.1823 |           6.7815 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0055 |          41.6172 |           6.7882 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0071 |          41.0224 |           6.7955 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0073 |          40.3703 |           6.7867 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0126 |          39.8028 |           6.7937 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0099 |          39.4814 |           6.8016 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0090 |          39.0889 |           6.8844 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0094 |          39.5279 |           6.8092 |
[32m[20221213 23:34:37 @agent_ppo2.py:185][0m |          -0.0124 |          38.7175 |           6.8424 |
[32m[20221213 23:34:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.07
[32m[20221213 23:34:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.86
[32m[20221213 23:34:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.83
[32m[20221213 23:34:37 @agent_ppo2.py:143][0m Total time:      22.09 min
[32m[20221213 23:34:37 @agent_ppo2.py:145][0m 2136064 total steps have happened
[32m[20221213 23:34:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3043 --------------------------#
[32m[20221213 23:34:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |           0.0019 |          62.3558 |           6.6692 |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |          -0.0075 |          57.7453 |           6.5892 |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |          -0.0037 |          55.9470 |           6.6271 |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |           0.0005 |          56.4383 |           6.6998 |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |          -0.0066 |          54.3912 |           6.6557 |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |          -0.0086 |          53.9573 |           6.6361 |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |          -0.0113 |          53.6849 |           6.6552 |
[32m[20221213 23:34:38 @agent_ppo2.py:185][0m |          -0.0090 |          53.2590 |           6.6904 |
[32m[20221213 23:34:39 @agent_ppo2.py:185][0m |          -0.0083 |          53.0158 |           6.7168 |
[32m[20221213 23:34:39 @agent_ppo2.py:185][0m |          -0.0104 |          52.9196 |           6.7508 |
[32m[20221213 23:34:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:34:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.47
[32m[20221213 23:34:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.29
[32m[20221213 23:34:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.72
[32m[20221213 23:34:39 @agent_ppo2.py:143][0m Total time:      22.11 min
[32m[20221213 23:34:39 @agent_ppo2.py:145][0m 2138112 total steps have happened
[32m[20221213 23:34:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3044 --------------------------#
[32m[20221213 23:34:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:39 @agent_ppo2.py:185][0m |           0.0012 |          65.5765 |           7.1680 |
[32m[20221213 23:34:39 @agent_ppo2.py:185][0m |           0.0071 |          62.8241 |           7.2145 |
[32m[20221213 23:34:39 @agent_ppo2.py:185][0m |          -0.0024 |          58.1967 |           7.3554 |
[32m[20221213 23:34:39 @agent_ppo2.py:185][0m |          -0.0069 |          57.5424 |           7.4059 |
[32m[20221213 23:34:39 @agent_ppo2.py:185][0m |          -0.0127 |          55.8991 |           7.4176 |
[32m[20221213 23:34:40 @agent_ppo2.py:185][0m |          -0.0097 |          55.3724 |           7.4814 |
[32m[20221213 23:34:40 @agent_ppo2.py:185][0m |          -0.0090 |          54.2550 |           7.5297 |
[32m[20221213 23:34:40 @agent_ppo2.py:185][0m |          -0.0035 |          54.7349 |           7.5591 |
[32m[20221213 23:34:40 @agent_ppo2.py:185][0m |          -0.0109 |          53.7551 |           7.6015 |
[32m[20221213 23:34:40 @agent_ppo2.py:185][0m |          -0.0100 |          52.9439 |           7.6167 |
[32m[20221213 23:34:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.67
[32m[20221213 23:34:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.38
[32m[20221213 23:34:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.75
[32m[20221213 23:34:40 @agent_ppo2.py:143][0m Total time:      22.13 min
[32m[20221213 23:34:40 @agent_ppo2.py:145][0m 2140160 total steps have happened
[32m[20221213 23:34:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3045 --------------------------#
[32m[20221213 23:34:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:40 @agent_ppo2.py:185][0m |           0.0138 |          67.4124 |           7.1605 |
[32m[20221213 23:34:40 @agent_ppo2.py:185][0m |          -0.0057 |          51.1164 |           7.2021 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0110 |          47.9060 |           7.2072 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0067 |          46.7732 |           7.2253 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0120 |          46.1405 |           7.2338 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0133 |          45.4128 |           7.2485 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0224 |          45.0373 |           7.2526 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0153 |          44.5989 |           7.2554 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0213 |          44.3041 |           7.2425 |
[32m[20221213 23:34:41 @agent_ppo2.py:185][0m |          -0.0122 |          43.9529 |           7.2652 |
[32m[20221213 23:34:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:34:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.55
[32m[20221213 23:34:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.59
[32m[20221213 23:34:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.19
[32m[20221213 23:34:41 @agent_ppo2.py:143][0m Total time:      22.16 min
[32m[20221213 23:34:41 @agent_ppo2.py:145][0m 2142208 total steps have happened
[32m[20221213 23:34:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3046 --------------------------#
[32m[20221213 23:34:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0024 |          59.4986 |           7.5027 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0016 |          54.7872 |           7.5568 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0029 |          53.3159 |           7.6614 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0121 |          52.3172 |           7.6530 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0067 |          51.3734 |           7.6775 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0111 |          50.4796 |           7.7435 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0123 |          50.0773 |           7.7733 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |           0.0020 |          57.8733 |           7.7543 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0105 |          49.7411 |           7.8250 |
[32m[20221213 23:34:42 @agent_ppo2.py:185][0m |          -0.0188 |          49.1193 |           7.8430 |
[32m[20221213 23:34:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:34:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.75
[32m[20221213 23:34:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.45
[32m[20221213 23:34:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.47
[32m[20221213 23:34:43 @agent_ppo2.py:143][0m Total time:      22.18 min
[32m[20221213 23:34:43 @agent_ppo2.py:145][0m 2144256 total steps have happened
[32m[20221213 23:34:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3047 --------------------------#
[32m[20221213 23:34:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:43 @agent_ppo2.py:185][0m |           0.0025 |          57.4989 |           8.3043 |
[32m[20221213 23:34:43 @agent_ppo2.py:185][0m |          -0.0059 |          55.8199 |           8.3281 |
[32m[20221213 23:34:43 @agent_ppo2.py:185][0m |          -0.0038 |          55.3416 |           8.4161 |
[32m[20221213 23:34:43 @agent_ppo2.py:185][0m |          -0.0047 |          54.9959 |           8.4320 |
[32m[20221213 23:34:43 @agent_ppo2.py:185][0m |          -0.0083 |          54.8891 |           8.4445 |
[32m[20221213 23:34:43 @agent_ppo2.py:185][0m |          -0.0068 |          54.6778 |           8.5368 |
[32m[20221213 23:34:44 @agent_ppo2.py:185][0m |          -0.0076 |          54.5096 |           8.5690 |
[32m[20221213 23:34:44 @agent_ppo2.py:185][0m |          -0.0092 |          54.3801 |           8.6008 |
[32m[20221213 23:34:44 @agent_ppo2.py:185][0m |          -0.0095 |          54.3594 |           8.6337 |
[32m[20221213 23:34:44 @agent_ppo2.py:185][0m |          -0.0083 |          54.1989 |           8.6552 |
[32m[20221213 23:34:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:34:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.80
[32m[20221213 23:34:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.67
[32m[20221213 23:34:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.29
[32m[20221213 23:34:44 @agent_ppo2.py:143][0m Total time:      22.20 min
[32m[20221213 23:34:44 @agent_ppo2.py:145][0m 2146304 total steps have happened
[32m[20221213 23:34:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3048 --------------------------#
[32m[20221213 23:34:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:44 @agent_ppo2.py:185][0m |           0.0012 |          61.7603 |           8.4912 |
[32m[20221213 23:34:44 @agent_ppo2.py:185][0m |          -0.0023 |          58.2902 |           8.4015 |
[32m[20221213 23:34:44 @agent_ppo2.py:185][0m |          -0.0040 |          57.0460 |           8.4070 |
[32m[20221213 23:34:45 @agent_ppo2.py:185][0m |          -0.0014 |          57.1456 |           8.4067 |
[32m[20221213 23:34:45 @agent_ppo2.py:185][0m |          -0.0081 |          55.2513 |           8.3911 |
[32m[20221213 23:34:45 @agent_ppo2.py:185][0m |          -0.0128 |          54.7655 |           8.4082 |
[32m[20221213 23:34:45 @agent_ppo2.py:185][0m |          -0.0134 |          54.2982 |           8.4077 |
[32m[20221213 23:34:45 @agent_ppo2.py:185][0m |          -0.0113 |          53.9978 |           8.4064 |
[32m[20221213 23:34:45 @agent_ppo2.py:185][0m |          -0.0111 |          53.6520 |           8.3747 |
[32m[20221213 23:34:45 @agent_ppo2.py:185][0m |          -0.0102 |          53.0861 |           8.3924 |
[32m[20221213 23:34:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:34:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.73
[32m[20221213 23:34:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.14
[32m[20221213 23:34:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.65
[32m[20221213 23:34:45 @agent_ppo2.py:143][0m Total time:      22.22 min
[32m[20221213 23:34:45 @agent_ppo2.py:145][0m 2148352 total steps have happened
[32m[20221213 23:34:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3049 --------------------------#
[32m[20221213 23:34:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |           0.0046 |          62.1128 |           8.1368 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |           0.0014 |          55.8419 |           8.1458 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |          -0.0016 |          54.4081 |           8.1745 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |          -0.0062 |          53.2883 |           8.1592 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |          -0.0084 |          52.5166 |           8.1545 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |          -0.0088 |          52.5443 |           8.1986 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |           0.0024 |          55.4823 |           8.1492 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |          -0.0094 |          52.2110 |           8.2071 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |          -0.0016 |          56.1336 |           8.2412 |
[32m[20221213 23:34:46 @agent_ppo2.py:185][0m |          -0.0066 |          53.4962 |           8.2504 |
[32m[20221213 23:34:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.70
[32m[20221213 23:34:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.17
[32m[20221213 23:34:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.91
[32m[20221213 23:34:46 @agent_ppo2.py:143][0m Total time:      22.24 min
[32m[20221213 23:34:46 @agent_ppo2.py:145][0m 2150400 total steps have happened
[32m[20221213 23:34:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3050 --------------------------#
[32m[20221213 23:34:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:34:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |           0.0037 |          55.8301 |           8.9024 |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |          -0.0029 |          51.2053 |           8.8817 |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |          -0.0049 |          50.2556 |           8.8560 |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |           0.0018 |          50.5605 |           8.8673 |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |          -0.0019 |          50.3357 |           8.8041 |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |          -0.0079 |          48.8918 |           8.8239 |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |          -0.0087 |          48.4829 |           8.7605 |
[32m[20221213 23:34:47 @agent_ppo2.py:185][0m |           0.0051 |          53.1588 |           8.7914 |
[32m[20221213 23:34:48 @agent_ppo2.py:185][0m |          -0.0110 |          48.2679 |           8.8016 |
[32m[20221213 23:34:48 @agent_ppo2.py:185][0m |          -0.0068 |          48.1724 |           8.7688 |
[32m[20221213 23:34:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.46
[32m[20221213 23:34:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.38
[32m[20221213 23:34:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.59
[32m[20221213 23:34:48 @agent_ppo2.py:143][0m Total time:      22.26 min
[32m[20221213 23:34:48 @agent_ppo2.py:145][0m 2152448 total steps have happened
[32m[20221213 23:34:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3051 --------------------------#
[32m[20221213 23:34:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:48 @agent_ppo2.py:185][0m |          -0.0024 |          63.0347 |           8.2869 |
[32m[20221213 23:34:48 @agent_ppo2.py:185][0m |           0.0004 |          60.0154 |           8.3017 |
[32m[20221213 23:34:48 @agent_ppo2.py:185][0m |          -0.0050 |          58.4477 |           8.3422 |
[32m[20221213 23:34:48 @agent_ppo2.py:185][0m |           0.0010 |          61.0847 |           8.3912 |
[32m[20221213 23:34:48 @agent_ppo2.py:185][0m |          -0.0030 |          58.8517 |           8.3555 |
[32m[20221213 23:34:49 @agent_ppo2.py:185][0m |          -0.0032 |          56.5490 |           8.4323 |
[32m[20221213 23:34:49 @agent_ppo2.py:185][0m |          -0.0111 |          55.6538 |           8.3681 |
[32m[20221213 23:34:49 @agent_ppo2.py:185][0m |          -0.0152 |          55.2213 |           8.4117 |
[32m[20221213 23:34:49 @agent_ppo2.py:185][0m |          -0.0122 |          54.6641 |           8.4267 |
[32m[20221213 23:34:49 @agent_ppo2.py:185][0m |          -0.0051 |          57.3842 |           8.4541 |
[32m[20221213 23:34:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.84
[32m[20221213 23:34:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.28
[32m[20221213 23:34:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.02
[32m[20221213 23:34:49 @agent_ppo2.py:143][0m Total time:      22.28 min
[32m[20221213 23:34:49 @agent_ppo2.py:145][0m 2154496 total steps have happened
[32m[20221213 23:34:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3052 --------------------------#
[32m[20221213 23:34:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:49 @agent_ppo2.py:185][0m |           0.0011 |          68.4645 |           8.3848 |
[32m[20221213 23:34:49 @agent_ppo2.py:185][0m |           0.0071 |          73.6734 |           8.4818 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0049 |          66.1576 |           8.4763 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0064 |          66.0127 |           8.4814 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0079 |          65.5490 |           8.5146 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0088 |          65.4002 |           8.5379 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0099 |          65.4129 |           8.5017 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0041 |          65.8448 |           8.4990 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0008 |          68.0529 |           8.5302 |
[32m[20221213 23:34:50 @agent_ppo2.py:185][0m |          -0.0135 |          64.9554 |           8.5281 |
[32m[20221213 23:34:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.84
[32m[20221213 23:34:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.51
[32m[20221213 23:34:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.81
[32m[20221213 23:34:50 @agent_ppo2.py:143][0m Total time:      22.30 min
[32m[20221213 23:34:50 @agent_ppo2.py:145][0m 2156544 total steps have happened
[32m[20221213 23:34:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3053 --------------------------#
[32m[20221213 23:34:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0002 |          65.1015 |           8.5793 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0013 |          62.8069 |           8.5849 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |           0.0128 |          67.1173 |           8.5825 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0066 |          60.0843 |           8.6249 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0106 |          59.4044 |           8.6239 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0086 |          59.6011 |           8.6316 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0136 |          58.5944 |           8.6296 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0149 |          58.2874 |           8.6149 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0170 |          58.0948 |           8.6544 |
[32m[20221213 23:34:51 @agent_ppo2.py:185][0m |          -0.0117 |          58.1770 |           8.6937 |
[32m[20221213 23:34:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:34:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.25
[32m[20221213 23:34:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.73
[32m[20221213 23:34:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.05
[32m[20221213 23:34:52 @agent_ppo2.py:143][0m Total time:      22.33 min
[32m[20221213 23:34:52 @agent_ppo2.py:145][0m 2158592 total steps have happened
[32m[20221213 23:34:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3054 --------------------------#
[32m[20221213 23:34:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |           0.0016 |          67.3429 |           8.6126 |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |          -0.0106 |          63.4113 |           8.6275 |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |          -0.0089 |          62.5048 |           8.6246 |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |          -0.0093 |          61.5623 |           8.6741 |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |          -0.0122 |          60.8585 |           8.6318 |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |          -0.0042 |          64.3981 |           8.6417 |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |          -0.0157 |          60.6854 |           8.6592 |
[32m[20221213 23:34:52 @agent_ppo2.py:185][0m |          -0.0145 |          59.7848 |           8.6793 |
[32m[20221213 23:34:53 @agent_ppo2.py:185][0m |          -0.0173 |          59.5248 |           8.7076 |
[32m[20221213 23:34:53 @agent_ppo2.py:185][0m |          -0.0160 |          59.2595 |           8.7868 |
[32m[20221213 23:34:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:34:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.78
[32m[20221213 23:34:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.36
[32m[20221213 23:34:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.40
[32m[20221213 23:34:53 @agent_ppo2.py:143][0m Total time:      22.35 min
[32m[20221213 23:34:53 @agent_ppo2.py:145][0m 2160640 total steps have happened
[32m[20221213 23:34:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3055 --------------------------#
[32m[20221213 23:34:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:53 @agent_ppo2.py:185][0m |          -0.0003 |          65.2058 |           8.6619 |
[32m[20221213 23:34:53 @agent_ppo2.py:185][0m |           0.0067 |          65.8620 |           8.6017 |
[32m[20221213 23:34:53 @agent_ppo2.py:185][0m |          -0.0048 |          56.9547 |           8.6224 |
[32m[20221213 23:34:53 @agent_ppo2.py:185][0m |          -0.0120 |          54.4908 |           8.6194 |
[32m[20221213 23:34:53 @agent_ppo2.py:185][0m |          -0.0112 |          52.7259 |           8.6698 |
[32m[20221213 23:34:54 @agent_ppo2.py:185][0m |          -0.0154 |          51.5881 |           8.5958 |
[32m[20221213 23:34:54 @agent_ppo2.py:185][0m |          -0.0146 |          50.7495 |           8.5860 |
[32m[20221213 23:34:54 @agent_ppo2.py:185][0m |          -0.0146 |          50.0902 |           8.5877 |
[32m[20221213 23:34:54 @agent_ppo2.py:185][0m |           0.0012 |          52.6106 |           8.6091 |
[32m[20221213 23:34:54 @agent_ppo2.py:185][0m |          -0.0142 |          48.5785 |           8.5530 |
[32m[20221213 23:34:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.25
[32m[20221213 23:34:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.17
[32m[20221213 23:34:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.51
[32m[20221213 23:34:54 @agent_ppo2.py:143][0m Total time:      22.37 min
[32m[20221213 23:34:54 @agent_ppo2.py:145][0m 2162688 total steps have happened
[32m[20221213 23:34:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3056 --------------------------#
[32m[20221213 23:34:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:34:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:54 @agent_ppo2.py:185][0m |           0.0054 |          55.7407 |           8.2198 |
[32m[20221213 23:34:54 @agent_ppo2.py:185][0m |          -0.0044 |          48.2793 |           8.2011 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0050 |          46.1502 |           8.2635 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0141 |          44.9800 |           8.1982 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0168 |          44.1900 |           8.2221 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0095 |          43.5143 |           8.1824 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0030 |          43.8572 |           8.1521 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0153 |          42.0994 |           8.1941 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0168 |          41.6563 |           8.1056 |
[32m[20221213 23:34:55 @agent_ppo2.py:185][0m |          -0.0135 |          41.3060 |           8.0786 |
[32m[20221213 23:34:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.44
[32m[20221213 23:34:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.17
[32m[20221213 23:34:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.02
[32m[20221213 23:34:55 @agent_ppo2.py:143][0m Total time:      22.39 min
[32m[20221213 23:34:55 @agent_ppo2.py:145][0m 2164736 total steps have happened
[32m[20221213 23:34:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3057 --------------------------#
[32m[20221213 23:34:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |           0.0003 |          59.9796 |           8.1578 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |           0.0009 |          58.2079 |           8.1707 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0022 |          57.0493 |           8.1655 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0031 |          56.1541 |           8.1343 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0056 |          55.8089 |           8.1242 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0067 |          55.4458 |           8.1154 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0051 |          55.3469 |           8.1100 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0060 |          55.0977 |           8.0773 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0060 |          54.9764 |           8.0892 |
[32m[20221213 23:34:56 @agent_ppo2.py:185][0m |          -0.0095 |          54.6123 |           8.0951 |
[32m[20221213 23:34:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:34:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.56
[32m[20221213 23:34:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.45
[32m[20221213 23:34:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.01
[32m[20221213 23:34:57 @agent_ppo2.py:143][0m Total time:      22.41 min
[32m[20221213 23:34:57 @agent_ppo2.py:145][0m 2166784 total steps have happened
[32m[20221213 23:34:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3058 --------------------------#
[32m[20221213 23:34:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:57 @agent_ppo2.py:185][0m |          -0.0042 |          64.9618 |           7.5520 |
[32m[20221213 23:34:57 @agent_ppo2.py:185][0m |          -0.0025 |          63.9837 |           7.6157 |
[32m[20221213 23:34:57 @agent_ppo2.py:185][0m |          -0.0088 |          63.8359 |           7.5834 |
[32m[20221213 23:34:57 @agent_ppo2.py:185][0m |          -0.0084 |          63.5259 |           7.6957 |
[32m[20221213 23:34:57 @agent_ppo2.py:185][0m |          -0.0075 |          63.2774 |           7.6588 |
[32m[20221213 23:34:57 @agent_ppo2.py:185][0m |          -0.0108 |          63.1671 |           7.6572 |
[32m[20221213 23:34:57 @agent_ppo2.py:185][0m |          -0.0093 |          63.0406 |           7.6955 |
[32m[20221213 23:34:58 @agent_ppo2.py:185][0m |          -0.0115 |          62.9543 |           7.7077 |
[32m[20221213 23:34:58 @agent_ppo2.py:185][0m |          -0.0086 |          62.7673 |           7.7368 |
[32m[20221213 23:34:58 @agent_ppo2.py:185][0m |           0.0014 |          66.7922 |           7.6946 |
[32m[20221213 23:34:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.70
[32m[20221213 23:34:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.69
[32m[20221213 23:34:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.03
[32m[20221213 23:34:58 @agent_ppo2.py:143][0m Total time:      22.43 min
[32m[20221213 23:34:58 @agent_ppo2.py:145][0m 2168832 total steps have happened
[32m[20221213 23:34:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3059 --------------------------#
[32m[20221213 23:34:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:34:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:58 @agent_ppo2.py:185][0m |          -0.0006 |          67.8326 |           8.3197 |
[32m[20221213 23:34:58 @agent_ppo2.py:185][0m |          -0.0094 |          65.9718 |           8.3105 |
[32m[20221213 23:34:58 @agent_ppo2.py:185][0m |          -0.0020 |          66.3305 |           8.4013 |
[32m[20221213 23:34:58 @agent_ppo2.py:185][0m |          -0.0093 |          64.0287 |           8.3964 |
[32m[20221213 23:34:59 @agent_ppo2.py:185][0m |          -0.0139 |          63.5434 |           8.3538 |
[32m[20221213 23:34:59 @agent_ppo2.py:185][0m |          -0.0130 |          63.2015 |           8.4129 |
[32m[20221213 23:34:59 @agent_ppo2.py:185][0m |          -0.0062 |          64.9265 |           8.4268 |
[32m[20221213 23:34:59 @agent_ppo2.py:185][0m |          -0.0144 |          62.6998 |           8.4902 |
[32m[20221213 23:34:59 @agent_ppo2.py:185][0m |          -0.0152 |          62.2755 |           8.4647 |
[32m[20221213 23:34:59 @agent_ppo2.py:185][0m |          -0.0161 |          62.1751 |           8.4726 |
[32m[20221213 23:34:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:34:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.95
[32m[20221213 23:34:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.60
[32m[20221213 23:34:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.24
[32m[20221213 23:34:59 @agent_ppo2.py:143][0m Total time:      22.45 min
[32m[20221213 23:34:59 @agent_ppo2.py:145][0m 2170880 total steps have happened
[32m[20221213 23:34:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3060 --------------------------#
[32m[20221213 23:34:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:34:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:34:59 @agent_ppo2.py:185][0m |           0.0005 |          80.6893 |           8.5747 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0012 |          77.6229 |           8.5702 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0050 |          76.1520 |           8.6146 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0080 |          75.8326 |           8.5640 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0087 |          75.2140 |           8.5980 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |           0.0049 |          87.1884 |           8.6051 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0076 |          74.8390 |           8.6768 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0102 |          74.7513 |           8.5784 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0133 |          74.2200 |           8.6271 |
[32m[20221213 23:35:00 @agent_ppo2.py:185][0m |          -0.0113 |          74.5662 |           8.6317 |
[32m[20221213 23:35:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.13
[32m[20221213 23:35:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.61
[32m[20221213 23:35:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.71
[32m[20221213 23:35:00 @agent_ppo2.py:143][0m Total time:      22.47 min
[32m[20221213 23:35:00 @agent_ppo2.py:145][0m 2172928 total steps have happened
[32m[20221213 23:35:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3061 --------------------------#
[32m[20221213 23:35:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0047 |          63.5679 |           8.6934 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0068 |          58.2307 |           8.7488 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0080 |          56.6322 |           8.7446 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0144 |          55.7438 |           8.8065 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0146 |          55.1196 |           8.8109 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0148 |          54.3833 |           8.8222 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0169 |          53.9037 |           8.8565 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0138 |          54.4806 |           8.8397 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0163 |          53.0069 |           8.8426 |
[32m[20221213 23:35:01 @agent_ppo2.py:185][0m |          -0.0200 |          52.7833 |           8.8647 |
[32m[20221213 23:35:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.66
[32m[20221213 23:35:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.47
[32m[20221213 23:35:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.07
[32m[20221213 23:35:02 @agent_ppo2.py:143][0m Total time:      22.49 min
[32m[20221213 23:35:02 @agent_ppo2.py:145][0m 2174976 total steps have happened
[32m[20221213 23:35:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3062 --------------------------#
[32m[20221213 23:35:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:02 @agent_ppo2.py:185][0m |           0.0018 |          64.3812 |           8.3967 |
[32m[20221213 23:35:02 @agent_ppo2.py:185][0m |           0.0005 |          60.3078 |           8.3363 |
[32m[20221213 23:35:02 @agent_ppo2.py:185][0m |          -0.0114 |          59.1391 |           8.3522 |
[32m[20221213 23:35:02 @agent_ppo2.py:185][0m |           0.0030 |          63.3672 |           8.3302 |
[32m[20221213 23:35:02 @agent_ppo2.py:185][0m |          -0.0110 |          58.1591 |           8.3241 |
[32m[20221213 23:35:02 @agent_ppo2.py:185][0m |          -0.0056 |          61.9116 |           8.3309 |
[32m[20221213 23:35:02 @agent_ppo2.py:185][0m |          -0.0102 |          56.9722 |           8.3803 |
[32m[20221213 23:35:03 @agent_ppo2.py:185][0m |          -0.0117 |          56.6779 |           8.3621 |
[32m[20221213 23:35:03 @agent_ppo2.py:185][0m |          -0.0132 |          56.4460 |           8.4054 |
[32m[20221213 23:35:03 @agent_ppo2.py:185][0m |          -0.0156 |          56.0633 |           8.3871 |
[32m[20221213 23:35:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:35:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.56
[32m[20221213 23:35:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.72
[32m[20221213 23:35:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.05
[32m[20221213 23:35:03 @agent_ppo2.py:143][0m Total time:      22.51 min
[32m[20221213 23:35:03 @agent_ppo2.py:145][0m 2177024 total steps have happened
[32m[20221213 23:35:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3063 --------------------------#
[32m[20221213 23:35:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:03 @agent_ppo2.py:185][0m |           0.0069 |          66.1658 |           8.6049 |
[32m[20221213 23:35:03 @agent_ppo2.py:185][0m |           0.0027 |          60.2243 |           8.6048 |
[32m[20221213 23:35:03 @agent_ppo2.py:185][0m |          -0.0038 |          58.9983 |           8.6011 |
[32m[20221213 23:35:03 @agent_ppo2.py:185][0m |           0.0033 |          63.6076 |           8.5864 |
[32m[20221213 23:35:04 @agent_ppo2.py:185][0m |          -0.0069 |          57.6594 |           8.6145 |
[32m[20221213 23:35:04 @agent_ppo2.py:185][0m |          -0.0067 |          56.8654 |           8.6291 |
[32m[20221213 23:35:04 @agent_ppo2.py:185][0m |          -0.0068 |          56.3690 |           8.6047 |
[32m[20221213 23:35:04 @agent_ppo2.py:185][0m |          -0.0114 |          56.0531 |           8.6174 |
[32m[20221213 23:35:04 @agent_ppo2.py:185][0m |          -0.0062 |          55.8696 |           8.6859 |
[32m[20221213 23:35:04 @agent_ppo2.py:185][0m |          -0.0110 |          55.3116 |           8.6342 |
[32m[20221213 23:35:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.67
[32m[20221213 23:35:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.39
[32m[20221213 23:35:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.47
[32m[20221213 23:35:04 @agent_ppo2.py:143][0m Total time:      22.54 min
[32m[20221213 23:35:04 @agent_ppo2.py:145][0m 2179072 total steps have happened
[32m[20221213 23:35:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3064 --------------------------#
[32m[20221213 23:35:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |           0.0007 |          67.8544 |           9.0273 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0066 |          64.4433 |           9.0376 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0063 |          63.4481 |           9.0617 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0063 |          62.7091 |           9.0446 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0107 |          62.1368 |           9.0859 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0134 |          61.7856 |           9.1168 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0136 |          61.6794 |           9.1413 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0175 |          61.5552 |           9.0916 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0135 |          61.2184 |           9.1441 |
[32m[20221213 23:35:05 @agent_ppo2.py:185][0m |          -0.0146 |          61.0376 |           9.1487 |
[32m[20221213 23:35:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.42
[32m[20221213 23:35:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.94
[32m[20221213 23:35:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.01
[32m[20221213 23:35:05 @agent_ppo2.py:143][0m Total time:      22.56 min
[32m[20221213 23:35:05 @agent_ppo2.py:145][0m 2181120 total steps have happened
[32m[20221213 23:35:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3065 --------------------------#
[32m[20221213 23:35:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |           0.0026 |          57.2774 |           9.1881 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0048 |          54.3276 |           9.2349 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0077 |          52.8207 |           9.2878 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0092 |          52.0992 |           9.2422 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0115 |          51.3263 |           9.2698 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0113 |          51.0884 |           9.2354 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0150 |          50.6474 |           9.2286 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0106 |          50.2763 |           9.2437 |
[32m[20221213 23:35:06 @agent_ppo2.py:185][0m |          -0.0133 |          49.7643 |           9.2533 |
[32m[20221213 23:35:07 @agent_ppo2.py:185][0m |          -0.0117 |          49.4021 |           9.2895 |
[32m[20221213 23:35:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.18
[32m[20221213 23:35:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.32
[32m[20221213 23:35:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.15
[32m[20221213 23:35:07 @agent_ppo2.py:143][0m Total time:      22.58 min
[32m[20221213 23:35:07 @agent_ppo2.py:145][0m 2183168 total steps have happened
[32m[20221213 23:35:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3066 --------------------------#
[32m[20221213 23:35:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:07 @agent_ppo2.py:185][0m |           0.0028 |          63.4946 |           9.0030 |
[32m[20221213 23:35:07 @agent_ppo2.py:185][0m |          -0.0027 |          61.8001 |           9.0380 |
[32m[20221213 23:35:07 @agent_ppo2.py:185][0m |           0.0024 |          61.9389 |           9.0466 |
[32m[20221213 23:35:07 @agent_ppo2.py:185][0m |          -0.0022 |          61.6220 |           9.0139 |
[32m[20221213 23:35:07 @agent_ppo2.py:185][0m |          -0.0064 |          60.7331 |           9.0079 |
[32m[20221213 23:35:07 @agent_ppo2.py:185][0m |          -0.0068 |          60.5256 |           9.0210 |
[32m[20221213 23:35:08 @agent_ppo2.py:185][0m |          -0.0077 |          60.3474 |           8.9855 |
[32m[20221213 23:35:08 @agent_ppo2.py:185][0m |          -0.0076 |          60.3581 |           9.0528 |
[32m[20221213 23:35:08 @agent_ppo2.py:185][0m |           0.0009 |          61.6379 |           9.0505 |
[32m[20221213 23:35:08 @agent_ppo2.py:185][0m |          -0.0079 |          60.0841 |           9.0380 |
[32m[20221213 23:35:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.47
[32m[20221213 23:35:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.06
[32m[20221213 23:35:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.33
[32m[20221213 23:35:08 @agent_ppo2.py:143][0m Total time:      22.60 min
[32m[20221213 23:35:08 @agent_ppo2.py:145][0m 2185216 total steps have happened
[32m[20221213 23:35:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3067 --------------------------#
[32m[20221213 23:35:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:08 @agent_ppo2.py:185][0m |          -0.0024 |          69.1926 |           9.2608 |
[32m[20221213 23:35:08 @agent_ppo2.py:185][0m |          -0.0044 |          66.9165 |           9.3597 |
[32m[20221213 23:35:08 @agent_ppo2.py:185][0m |          -0.0093 |          66.5736 |           9.2837 |
[32m[20221213 23:35:09 @agent_ppo2.py:185][0m |          -0.0063 |          65.8062 |           9.3475 |
[32m[20221213 23:35:09 @agent_ppo2.py:185][0m |          -0.0097 |          65.5411 |           9.3568 |
[32m[20221213 23:35:09 @agent_ppo2.py:185][0m |          -0.0116 |          65.0724 |           9.3464 |
[32m[20221213 23:35:09 @agent_ppo2.py:185][0m |          -0.0121 |          64.9962 |           9.2966 |
[32m[20221213 23:35:09 @agent_ppo2.py:185][0m |          -0.0084 |          65.3949 |           9.3144 |
[32m[20221213 23:35:09 @agent_ppo2.py:185][0m |          -0.0172 |          64.6124 |           9.3699 |
[32m[20221213 23:35:09 @agent_ppo2.py:185][0m |          -0.0094 |          64.5328 |           9.2802 |
[32m[20221213 23:35:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:35:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.74
[32m[20221213 23:35:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.49
[32m[20221213 23:35:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.67
[32m[20221213 23:35:09 @agent_ppo2.py:143][0m Total time:      22.62 min
[32m[20221213 23:35:09 @agent_ppo2.py:145][0m 2187264 total steps have happened
[32m[20221213 23:35:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3068 --------------------------#
[32m[20221213 23:35:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |           0.0018 |          48.6813 |           9.1575 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0051 |          44.9485 |           9.1240 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0073 |          43.7840 |           9.0516 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0086 |          43.0004 |           9.0846 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0121 |          42.3846 |           9.1012 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0113 |          42.1099 |           9.0448 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0040 |          41.9500 |           9.0308 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |           0.0007 |          49.1197 |           9.0478 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0154 |          41.5199 |           9.0563 |
[32m[20221213 23:35:10 @agent_ppo2.py:185][0m |          -0.0095 |          42.3919 |           9.0282 |
[32m[20221213 23:35:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.64
[32m[20221213 23:35:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.63
[32m[20221213 23:35:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.09
[32m[20221213 23:35:10 @agent_ppo2.py:143][0m Total time:      22.64 min
[32m[20221213 23:35:10 @agent_ppo2.py:145][0m 2189312 total steps have happened
[32m[20221213 23:35:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3069 --------------------------#
[32m[20221213 23:35:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |           0.0015 |          68.0937 |           8.4388 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |           0.0068 |          71.2887 |           8.5765 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |          -0.0048 |          60.9219 |           8.5814 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |          -0.0096 |          59.2797 |           8.5534 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |          -0.0011 |          61.5370 |           8.5874 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |          -0.0118 |          58.0061 |           8.5408 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |          -0.0092 |          57.5972 |           8.5340 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |          -0.0131 |          57.1732 |           8.5566 |
[32m[20221213 23:35:11 @agent_ppo2.py:185][0m |          -0.0145 |          57.1120 |           8.5747 |
[32m[20221213 23:35:12 @agent_ppo2.py:185][0m |          -0.0155 |          56.3569 |           8.6567 |
[32m[20221213 23:35:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.63
[32m[20221213 23:35:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.38
[32m[20221213 23:35:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.49
[32m[20221213 23:35:12 @agent_ppo2.py:143][0m Total time:      22.66 min
[32m[20221213 23:35:12 @agent_ppo2.py:145][0m 2191360 total steps have happened
[32m[20221213 23:35:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3070 --------------------------#
[32m[20221213 23:35:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:35:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:12 @agent_ppo2.py:185][0m |          -0.0004 |          66.1033 |           8.5643 |
[32m[20221213 23:35:12 @agent_ppo2.py:185][0m |           0.0099 |          65.8188 |           8.5541 |
[32m[20221213 23:35:12 @agent_ppo2.py:185][0m |          -0.0064 |          60.5172 |           8.5736 |
[32m[20221213 23:35:12 @agent_ppo2.py:185][0m |          -0.0042 |          59.5938 |           8.5687 |
[32m[20221213 23:35:12 @agent_ppo2.py:185][0m |          -0.0112 |          58.9854 |           8.5485 |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |          -0.0073 |          58.7511 |           8.5386 |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |          -0.0112 |          57.9604 |           8.5352 |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |          -0.0168 |          57.5411 |           8.5520 |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |          -0.0039 |          60.2333 |           8.5591 |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |          -0.0103 |          56.9707 |           8.4611 |
[32m[20221213 23:35:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.70
[32m[20221213 23:35:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.59
[32m[20221213 23:35:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.61
[32m[20221213 23:35:13 @agent_ppo2.py:143][0m Total time:      22.68 min
[32m[20221213 23:35:13 @agent_ppo2.py:145][0m 2193408 total steps have happened
[32m[20221213 23:35:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3071 --------------------------#
[32m[20221213 23:35:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |           0.0033 |          74.2247 |           8.9194 |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |          -0.0070 |          70.9273 |           8.9385 |
[32m[20221213 23:35:13 @agent_ppo2.py:185][0m |          -0.0024 |          76.0936 |           8.9466 |
[32m[20221213 23:35:14 @agent_ppo2.py:185][0m |           0.0026 |          76.9077 |           8.9228 |
[32m[20221213 23:35:14 @agent_ppo2.py:185][0m |          -0.0133 |          70.1225 |           8.9619 |
[32m[20221213 23:35:14 @agent_ppo2.py:185][0m |          -0.0102 |          69.2163 |           9.0097 |
[32m[20221213 23:35:14 @agent_ppo2.py:185][0m |          -0.0125 |          68.9216 |           9.0063 |
[32m[20221213 23:35:14 @agent_ppo2.py:185][0m |          -0.0100 |          68.8815 |           9.0527 |
[32m[20221213 23:35:14 @agent_ppo2.py:185][0m |          -0.0051 |          70.4411 |           9.0321 |
[32m[20221213 23:35:14 @agent_ppo2.py:185][0m |          -0.0081 |          68.6537 |           9.0146 |
[32m[20221213 23:35:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.46
[32m[20221213 23:35:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.52
[32m[20221213 23:35:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.47
[32m[20221213 23:35:14 @agent_ppo2.py:143][0m Total time:      22.70 min
[32m[20221213 23:35:14 @agent_ppo2.py:145][0m 2195456 total steps have happened
[32m[20221213 23:35:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3072 --------------------------#
[32m[20221213 23:35:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |           0.0051 |          65.8700 |           8.9139 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |           0.0030 |          61.8848 |           8.9620 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0058 |          59.4319 |           8.9378 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0114 |          58.5586 |           8.9086 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0099 |          57.5529 |           8.9329 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0091 |          56.9652 |           8.9020 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0112 |          56.8611 |           8.8926 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0151 |          56.0169 |           8.8637 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0148 |          55.4435 |           8.8482 |
[32m[20221213 23:35:15 @agent_ppo2.py:185][0m |          -0.0142 |          54.9447 |           8.7793 |
[32m[20221213 23:35:15 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:35:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.23
[32m[20221213 23:35:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.36
[32m[20221213 23:35:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.96
[32m[20221213 23:35:16 @agent_ppo2.py:143][0m Total time:      22.73 min
[32m[20221213 23:35:16 @agent_ppo2.py:145][0m 2197504 total steps have happened
[32m[20221213 23:35:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3073 --------------------------#
[32m[20221213 23:35:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |           0.0001 |          59.7711 |           8.9459 |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |          -0.0051 |          55.2701 |           8.9714 |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |          -0.0029 |          53.8592 |           8.9272 |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |          -0.0072 |          52.6527 |           8.9933 |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |          -0.0083 |          51.7936 |           8.9642 |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |          -0.0128 |          51.2399 |           8.9879 |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |          -0.0087 |          50.8531 |           8.9804 |
[32m[20221213 23:35:16 @agent_ppo2.py:185][0m |          -0.0114 |          50.2994 |           8.9887 |
[32m[20221213 23:35:17 @agent_ppo2.py:185][0m |          -0.0164 |          50.1265 |           9.0296 |
[32m[20221213 23:35:17 @agent_ppo2.py:185][0m |          -0.0180 |          49.9414 |           9.0204 |
[32m[20221213 23:35:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.65
[32m[20221213 23:35:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.42
[32m[20221213 23:35:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.75
[32m[20221213 23:35:17 @agent_ppo2.py:143][0m Total time:      22.75 min
[32m[20221213 23:35:17 @agent_ppo2.py:145][0m 2199552 total steps have happened
[32m[20221213 23:35:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3074 --------------------------#
[32m[20221213 23:35:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:17 @agent_ppo2.py:185][0m |           0.0025 |          75.5170 |           9.2821 |
[32m[20221213 23:35:17 @agent_ppo2.py:185][0m |          -0.0065 |          71.3167 |           9.3200 |
[32m[20221213 23:35:17 @agent_ppo2.py:185][0m |          -0.0078 |          69.9386 |           9.3639 |
[32m[20221213 23:35:17 @agent_ppo2.py:185][0m |          -0.0087 |          69.5084 |           9.3444 |
[32m[20221213 23:35:17 @agent_ppo2.py:185][0m |          -0.0096 |          68.8836 |           9.4144 |
[32m[20221213 23:35:18 @agent_ppo2.py:185][0m |          -0.0120 |          68.4430 |           9.4008 |
[32m[20221213 23:35:18 @agent_ppo2.py:185][0m |          -0.0093 |          68.1740 |           9.4095 |
[32m[20221213 23:35:18 @agent_ppo2.py:185][0m |          -0.0106 |          67.6561 |           9.4354 |
[32m[20221213 23:35:18 @agent_ppo2.py:185][0m |          -0.0139 |          67.4040 |           9.4690 |
[32m[20221213 23:35:18 @agent_ppo2.py:185][0m |          -0.0110 |          66.9706 |           9.5043 |
[32m[20221213 23:35:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.04
[32m[20221213 23:35:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.48
[32m[20221213 23:35:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.27
[32m[20221213 23:35:18 @agent_ppo2.py:143][0m Total time:      22.77 min
[32m[20221213 23:35:18 @agent_ppo2.py:145][0m 2201600 total steps have happened
[32m[20221213 23:35:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3075 --------------------------#
[32m[20221213 23:35:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:18 @agent_ppo2.py:185][0m |           0.0015 |          71.9998 |           8.8118 |
[32m[20221213 23:35:18 @agent_ppo2.py:185][0m |          -0.0085 |          69.1197 |           8.7352 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |           0.0071 |          75.0561 |           8.7256 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |          -0.0074 |          67.5987 |           8.7412 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |          -0.0065 |          67.1946 |           8.7035 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |          -0.0097 |          66.9474 |           8.6968 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |          -0.0123 |          66.4240 |           8.6082 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |          -0.0017 |          71.6933 |           8.6804 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |          -0.0092 |          66.5797 |           8.6200 |
[32m[20221213 23:35:19 @agent_ppo2.py:185][0m |          -0.0115 |          66.0370 |           8.6115 |
[32m[20221213 23:35:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.37
[32m[20221213 23:35:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.21
[32m[20221213 23:35:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.56
[32m[20221213 23:35:19 @agent_ppo2.py:143][0m Total time:      22.79 min
[32m[20221213 23:35:19 @agent_ppo2.py:145][0m 2203648 total steps have happened
[32m[20221213 23:35:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3076 --------------------------#
[32m[20221213 23:35:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |           0.0027 |          57.9365 |           9.2813 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0024 |          52.3846 |           9.3056 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0088 |          50.5483 |           9.3385 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0104 |          49.3143 |           9.3242 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0049 |          49.4324 |           9.3767 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0132 |          47.5680 |           9.3950 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0115 |          46.9915 |           9.4133 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0124 |          46.4388 |           9.4211 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0143 |          45.9531 |           9.4244 |
[32m[20221213 23:35:20 @agent_ppo2.py:185][0m |          -0.0120 |          45.7325 |           9.4380 |
[32m[20221213 23:35:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.91
[32m[20221213 23:35:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.48
[32m[20221213 23:35:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.03
[32m[20221213 23:35:21 @agent_ppo2.py:143][0m Total time:      22.81 min
[32m[20221213 23:35:21 @agent_ppo2.py:145][0m 2205696 total steps have happened
[32m[20221213 23:35:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3077 --------------------------#
[32m[20221213 23:35:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:21 @agent_ppo2.py:185][0m |          -0.0014 |          73.1825 |           8.9923 |
[32m[20221213 23:35:21 @agent_ppo2.py:185][0m |          -0.0046 |          69.6496 |           8.9199 |
[32m[20221213 23:35:21 @agent_ppo2.py:185][0m |          -0.0091 |          68.1750 |           8.9650 |
[32m[20221213 23:35:21 @agent_ppo2.py:185][0m |          -0.0087 |          67.3031 |           8.9822 |
[32m[20221213 23:35:21 @agent_ppo2.py:185][0m |          -0.0112 |          66.9893 |           8.9573 |
[32m[20221213 23:35:21 @agent_ppo2.py:185][0m |          -0.0082 |          66.4467 |           9.0092 |
[32m[20221213 23:35:21 @agent_ppo2.py:185][0m |          -0.0094 |          66.0537 |           9.0014 |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0045 |          67.8102 |           9.0296 |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0096 |          65.6474 |           9.0169 |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0090 |          65.3335 |           9.0620 |
[32m[20221213 23:35:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.79
[32m[20221213 23:35:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.93
[32m[20221213 23:35:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.89
[32m[20221213 23:35:22 @agent_ppo2.py:143][0m Total time:      22.83 min
[32m[20221213 23:35:22 @agent_ppo2.py:145][0m 2207744 total steps have happened
[32m[20221213 23:35:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3078 --------------------------#
[32m[20221213 23:35:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0060 |          71.9362 |           8.3088 |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0037 |          68.9746 |           8.2915 |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0077 |          68.2769 |           8.3032 |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0092 |          67.4145 |           8.2220 |
[32m[20221213 23:35:22 @agent_ppo2.py:185][0m |          -0.0104 |          67.2038 |           8.1823 |
[32m[20221213 23:35:23 @agent_ppo2.py:185][0m |          -0.0110 |          66.6646 |           8.1612 |
[32m[20221213 23:35:23 @agent_ppo2.py:185][0m |          -0.0105 |          66.4342 |           8.2288 |
[32m[20221213 23:35:23 @agent_ppo2.py:185][0m |          -0.0135 |          66.3427 |           8.1542 |
[32m[20221213 23:35:23 @agent_ppo2.py:185][0m |          -0.0121 |          65.9544 |           8.1919 |
[32m[20221213 23:35:23 @agent_ppo2.py:185][0m |          -0.0090 |          65.8493 |           8.1869 |
[32m[20221213 23:35:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.02
[32m[20221213 23:35:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.94
[32m[20221213 23:35:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.58
[32m[20221213 23:35:23 @agent_ppo2.py:143][0m Total time:      22.85 min
[32m[20221213 23:35:23 @agent_ppo2.py:145][0m 2209792 total steps have happened
[32m[20221213 23:35:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3079 --------------------------#
[32m[20221213 23:35:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:23 @agent_ppo2.py:185][0m |           0.0017 |          63.3441 |           8.9824 |
[32m[20221213 23:35:23 @agent_ppo2.py:185][0m |          -0.0040 |          56.4082 |           8.9735 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |          -0.0081 |          54.7387 |           8.9674 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |           0.0039 |          61.6468 |           8.9250 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |          -0.0015 |          56.5558 |           8.9461 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |          -0.0104 |          52.6502 |           8.8701 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |          -0.0184 |          52.3471 |           8.8440 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |          -0.0148 |          51.8810 |           8.8206 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |          -0.0141 |          51.6878 |           8.8447 |
[32m[20221213 23:35:24 @agent_ppo2.py:185][0m |          -0.0114 |          51.6599 |           8.8052 |
[32m[20221213 23:35:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.29
[32m[20221213 23:35:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.55
[32m[20221213 23:35:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.47
[32m[20221213 23:35:24 @agent_ppo2.py:143][0m Total time:      22.87 min
[32m[20221213 23:35:24 @agent_ppo2.py:145][0m 2211840 total steps have happened
[32m[20221213 23:35:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3080 --------------------------#
[32m[20221213 23:35:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:35:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0043 |          77.9734 |           8.6618 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0007 |          74.1696 |           8.6685 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0037 |          72.7274 |           8.6559 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0115 |          71.6406 |           8.6970 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0053 |          72.4937 |           8.7358 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0135 |          70.9506 |           8.6744 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0143 |          70.3501 |           8.7174 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0145 |          69.8302 |           8.7405 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0117 |          69.9093 |           8.7785 |
[32m[20221213 23:35:25 @agent_ppo2.py:185][0m |          -0.0065 |          75.9047 |           8.7683 |
[32m[20221213 23:35:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.34
[32m[20221213 23:35:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.01
[32m[20221213 23:35:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.45
[32m[20221213 23:35:26 @agent_ppo2.py:143][0m Total time:      22.89 min
[32m[20221213 23:35:26 @agent_ppo2.py:145][0m 2213888 total steps have happened
[32m[20221213 23:35:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3081 --------------------------#
[32m[20221213 23:35:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:26 @agent_ppo2.py:185][0m |           0.0043 |          47.8874 |           8.9314 |
[32m[20221213 23:35:26 @agent_ppo2.py:185][0m |          -0.0061 |          43.3714 |           8.9771 |
[32m[20221213 23:35:26 @agent_ppo2.py:185][0m |          -0.0067 |          41.9210 |           8.9656 |
[32m[20221213 23:35:26 @agent_ppo2.py:185][0m |          -0.0111 |          41.3175 |           8.9627 |
[32m[20221213 23:35:26 @agent_ppo2.py:185][0m |          -0.0118 |          40.8024 |           9.0457 |
[32m[20221213 23:35:26 @agent_ppo2.py:185][0m |          -0.0119 |          40.1342 |           9.0251 |
[32m[20221213 23:35:26 @agent_ppo2.py:185][0m |          -0.0153 |          39.7978 |           9.0344 |
[32m[20221213 23:35:27 @agent_ppo2.py:185][0m |          -0.0158 |          39.2867 |           9.0509 |
[32m[20221213 23:35:27 @agent_ppo2.py:185][0m |          -0.0142 |          39.1638 |           9.0465 |
[32m[20221213 23:35:27 @agent_ppo2.py:185][0m |          -0.0183 |          38.8758 |           9.0396 |
[32m[20221213 23:35:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:35:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.81
[32m[20221213 23:35:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.40
[32m[20221213 23:35:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.81
[32m[20221213 23:35:27 @agent_ppo2.py:143][0m Total time:      22.91 min
[32m[20221213 23:35:27 @agent_ppo2.py:145][0m 2215936 total steps have happened
[32m[20221213 23:35:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3082 --------------------------#
[32m[20221213 23:35:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:27 @agent_ppo2.py:185][0m |           0.0044 |          71.9007 |           8.4211 |
[32m[20221213 23:35:27 @agent_ppo2.py:185][0m |          -0.0015 |          69.7137 |           8.4511 |
[32m[20221213 23:35:27 @agent_ppo2.py:185][0m |          -0.0051 |          69.2977 |           8.4385 |
[32m[20221213 23:35:27 @agent_ppo2.py:185][0m |          -0.0055 |          69.1682 |           8.3771 |
[32m[20221213 23:35:28 @agent_ppo2.py:185][0m |          -0.0013 |          70.4765 |           8.4077 |
[32m[20221213 23:35:28 @agent_ppo2.py:185][0m |          -0.0124 |          68.0173 |           8.3862 |
[32m[20221213 23:35:28 @agent_ppo2.py:185][0m |          -0.0121 |          67.9729 |           8.4238 |
[32m[20221213 23:35:28 @agent_ppo2.py:185][0m |          -0.0104 |          68.1132 |           8.4386 |
[32m[20221213 23:35:28 @agent_ppo2.py:185][0m |          -0.0121 |          69.0663 |           8.4691 |
[32m[20221213 23:35:28 @agent_ppo2.py:185][0m |          -0.0140 |          67.4936 |           8.4238 |
[32m[20221213 23:35:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.86
[32m[20221213 23:35:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.49
[32m[20221213 23:35:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.25
[32m[20221213 23:35:28 @agent_ppo2.py:143][0m Total time:      22.93 min
[32m[20221213 23:35:28 @agent_ppo2.py:145][0m 2217984 total steps have happened
[32m[20221213 23:35:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3083 --------------------------#
[32m[20221213 23:35:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:28 @agent_ppo2.py:185][0m |           0.0086 |          73.6018 |           8.7817 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0014 |          65.8068 |           8.7484 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0080 |          64.9630 |           8.7589 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0095 |          64.2149 |           8.7800 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0062 |          64.1000 |           8.8653 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0134 |          63.2704 |           8.8460 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0100 |          62.9078 |           8.8580 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0127 |          62.6911 |           8.8496 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0101 |          62.2644 |           8.8922 |
[32m[20221213 23:35:29 @agent_ppo2.py:185][0m |          -0.0125 |          62.0469 |           8.8997 |
[32m[20221213 23:35:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.15
[32m[20221213 23:35:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.15
[32m[20221213 23:35:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.13
[32m[20221213 23:35:29 @agent_ppo2.py:143][0m Total time:      22.96 min
[32m[20221213 23:35:29 @agent_ppo2.py:145][0m 2220032 total steps have happened
[32m[20221213 23:35:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3084 --------------------------#
[32m[20221213 23:35:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |           0.0011 |          67.7701 |           9.3486 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |          -0.0050 |          65.9971 |           9.3498 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |          -0.0005 |          66.4985 |           9.3921 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |           0.0035 |          68.9254 |           9.3627 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |           0.0014 |          66.8446 |           9.4199 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |          -0.0089 |          64.7103 |           9.4021 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |          -0.0047 |          64.8549 |           9.4063 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |          -0.0112 |          64.3205 |           9.4565 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |          -0.0085 |          64.1935 |           9.4029 |
[32m[20221213 23:35:30 @agent_ppo2.py:185][0m |          -0.0108 |          64.0902 |           9.4858 |
[32m[20221213 23:35:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:35:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.71
[32m[20221213 23:35:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.46
[32m[20221213 23:35:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.99
[32m[20221213 23:35:31 @agent_ppo2.py:143][0m Total time:      22.98 min
[32m[20221213 23:35:31 @agent_ppo2.py:145][0m 2222080 total steps have happened
[32m[20221213 23:35:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3085 --------------------------#
[32m[20221213 23:35:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:31 @agent_ppo2.py:185][0m |          -0.0013 |          76.6341 |           8.7259 |
[32m[20221213 23:35:31 @agent_ppo2.py:185][0m |          -0.0048 |          74.7262 |           8.7282 |
[32m[20221213 23:35:31 @agent_ppo2.py:185][0m |          -0.0096 |          74.1728 |           8.7567 |
[32m[20221213 23:35:31 @agent_ppo2.py:185][0m |          -0.0060 |          73.6147 |           8.7433 |
[32m[20221213 23:35:31 @agent_ppo2.py:185][0m |          -0.0083 |          73.5042 |           8.7387 |
[32m[20221213 23:35:31 @agent_ppo2.py:185][0m |          -0.0075 |          73.5120 |           8.6796 |
[32m[20221213 23:35:31 @agent_ppo2.py:185][0m |          -0.0097 |          73.2249 |           8.7204 |
[32m[20221213 23:35:32 @agent_ppo2.py:185][0m |          -0.0138 |          72.7882 |           8.6663 |
[32m[20221213 23:35:32 @agent_ppo2.py:185][0m |          -0.0129 |          72.7317 |           8.6543 |
[32m[20221213 23:35:32 @agent_ppo2.py:185][0m |          -0.0118 |          72.4504 |           8.5496 |
[32m[20221213 23:35:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.55
[32m[20221213 23:35:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.51
[32m[20221213 23:35:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.59
[32m[20221213 23:35:32 @agent_ppo2.py:143][0m Total time:      23.00 min
[32m[20221213 23:35:32 @agent_ppo2.py:145][0m 2224128 total steps have happened
[32m[20221213 23:35:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3086 --------------------------#
[32m[20221213 23:35:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:32 @agent_ppo2.py:185][0m |          -0.0020 |          55.8351 |           8.8085 |
[32m[20221213 23:35:32 @agent_ppo2.py:185][0m |          -0.0060 |          50.6452 |           8.8409 |
[32m[20221213 23:35:32 @agent_ppo2.py:185][0m |          -0.0078 |          48.0899 |           8.8508 |
[32m[20221213 23:35:32 @agent_ppo2.py:185][0m |          -0.0058 |          46.9712 |           8.8312 |
[32m[20221213 23:35:33 @agent_ppo2.py:185][0m |          -0.0121 |          46.3307 |           8.8318 |
[32m[20221213 23:35:33 @agent_ppo2.py:185][0m |          -0.0143 |          45.8569 |           8.8378 |
[32m[20221213 23:35:33 @agent_ppo2.py:185][0m |          -0.0122 |          45.7310 |           8.8398 |
[32m[20221213 23:35:33 @agent_ppo2.py:185][0m |          -0.0109 |          45.3318 |           8.8752 |
[32m[20221213 23:35:33 @agent_ppo2.py:185][0m |          -0.0138 |          44.8803 |           8.8840 |
[32m[20221213 23:35:33 @agent_ppo2.py:185][0m |          -0.0125 |          44.7229 |           8.8538 |
[32m[20221213 23:35:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:35:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.73
[32m[20221213 23:35:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.46
[32m[20221213 23:35:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.59
[32m[20221213 23:35:33 @agent_ppo2.py:143][0m Total time:      23.02 min
[32m[20221213 23:35:33 @agent_ppo2.py:145][0m 2226176 total steps have happened
[32m[20221213 23:35:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3087 --------------------------#
[32m[20221213 23:35:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:33 @agent_ppo2.py:185][0m |          -0.0038 |          65.6346 |           8.9955 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0065 |          60.4490 |           9.0412 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0141 |          59.1637 |           9.0342 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0096 |          58.4291 |           9.0339 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0077 |          57.9939 |           9.0133 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0053 |          59.4586 |           9.1046 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0086 |          57.5553 |           9.1165 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0095 |          57.2877 |           9.1391 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0064 |          57.4858 |           9.2050 |
[32m[20221213 23:35:34 @agent_ppo2.py:185][0m |          -0.0126 |          57.2270 |           9.2272 |
[32m[20221213 23:35:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.99
[32m[20221213 23:35:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.08
[32m[20221213 23:35:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.62
[32m[20221213 23:35:34 @agent_ppo2.py:143][0m Total time:      23.04 min
[32m[20221213 23:35:34 @agent_ppo2.py:145][0m 2228224 total steps have happened
[32m[20221213 23:35:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3088 --------------------------#
[32m[20221213 23:35:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |           0.0021 |          55.4166 |           9.2929 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0062 |          51.9854 |           9.2626 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0057 |          51.1442 |           9.3113 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0079 |          50.8709 |           9.2938 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0087 |          50.4999 |           9.3005 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0136 |          50.3277 |           9.3219 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0104 |          49.7868 |           9.3336 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0147 |          49.7185 |           9.2856 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0092 |          49.7678 |           9.3115 |
[32m[20221213 23:35:35 @agent_ppo2.py:185][0m |          -0.0127 |          49.4982 |           9.3396 |
[32m[20221213 23:35:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.01
[32m[20221213 23:35:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.13
[32m[20221213 23:35:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.11
[32m[20221213 23:35:36 @agent_ppo2.py:143][0m Total time:      23.06 min
[32m[20221213 23:35:36 @agent_ppo2.py:145][0m 2230272 total steps have happened
[32m[20221213 23:35:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3089 --------------------------#
[32m[20221213 23:35:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:36 @agent_ppo2.py:185][0m |           0.0031 |          64.1523 |           9.1314 |
[32m[20221213 23:35:36 @agent_ppo2.py:185][0m |          -0.0024 |          62.7960 |           9.2151 |
[32m[20221213 23:35:36 @agent_ppo2.py:185][0m |          -0.0046 |          61.9318 |           9.2136 |
[32m[20221213 23:35:36 @agent_ppo2.py:185][0m |          -0.0026 |          62.5982 |           9.2307 |
[32m[20221213 23:35:36 @agent_ppo2.py:185][0m |          -0.0039 |          61.1759 |           9.2259 |
[32m[20221213 23:35:36 @agent_ppo2.py:185][0m |          -0.0093 |          60.4870 |           9.3197 |
[32m[20221213 23:35:36 @agent_ppo2.py:185][0m |          -0.0060 |          60.3374 |           9.2609 |
[32m[20221213 23:35:37 @agent_ppo2.py:185][0m |          -0.0087 |          59.7545 |           9.3378 |
[32m[20221213 23:35:37 @agent_ppo2.py:185][0m |          -0.0107 |          59.2843 |           9.3059 |
[32m[20221213 23:35:37 @agent_ppo2.py:185][0m |          -0.0088 |          59.0734 |           9.3149 |
[32m[20221213 23:35:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:35:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.60
[32m[20221213 23:35:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.69
[32m[20221213 23:35:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.64
[32m[20221213 23:35:37 @agent_ppo2.py:143][0m Total time:      23.08 min
[32m[20221213 23:35:37 @agent_ppo2.py:145][0m 2232320 total steps have happened
[32m[20221213 23:35:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3090 --------------------------#
[32m[20221213 23:35:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:35:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:37 @agent_ppo2.py:185][0m |           0.0017 |          69.4235 |           9.5856 |
[32m[20221213 23:35:37 @agent_ppo2.py:185][0m |          -0.0050 |          67.2710 |           9.5891 |
[32m[20221213 23:35:37 @agent_ppo2.py:185][0m |          -0.0032 |          66.5837 |           9.6008 |
[32m[20221213 23:35:37 @agent_ppo2.py:185][0m |          -0.0053 |          66.2250 |           9.6524 |
[32m[20221213 23:35:38 @agent_ppo2.py:185][0m |          -0.0040 |          66.2034 |           9.6537 |
[32m[20221213 23:35:38 @agent_ppo2.py:185][0m |          -0.0089 |          66.0959 |           9.7647 |
[32m[20221213 23:35:38 @agent_ppo2.py:185][0m |          -0.0078 |          65.8531 |           9.7487 |
[32m[20221213 23:35:38 @agent_ppo2.py:185][0m |          -0.0113 |          65.5806 |           9.7821 |
[32m[20221213 23:35:38 @agent_ppo2.py:185][0m |          -0.0108 |          65.3694 |           9.8171 |
[32m[20221213 23:35:38 @agent_ppo2.py:185][0m |          -0.0082 |          65.3595 |           9.8970 |
[32m[20221213 23:35:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:35:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.29
[32m[20221213 23:35:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.77
[32m[20221213 23:35:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.38
[32m[20221213 23:35:38 @agent_ppo2.py:143][0m Total time:      23.10 min
[32m[20221213 23:35:38 @agent_ppo2.py:145][0m 2234368 total steps have happened
[32m[20221213 23:35:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3091 --------------------------#
[32m[20221213 23:35:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:38 @agent_ppo2.py:185][0m |           0.0006 |          63.8308 |          10.1055 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |           0.0017 |          59.3657 |          10.1193 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0048 |          56.0281 |          10.1327 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0086 |          54.9424 |          10.1883 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0097 |          54.2156 |          10.1346 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0103 |          53.5801 |          10.1487 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0052 |          54.4969 |          10.2147 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0121 |          52.9960 |          10.1609 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0125 |          52.6879 |          10.1518 |
[32m[20221213 23:35:39 @agent_ppo2.py:185][0m |          -0.0132 |          52.4872 |          10.2056 |
[32m[20221213 23:35:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:35:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.19
[32m[20221213 23:35:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.43
[32m[20221213 23:35:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.25
[32m[20221213 23:35:39 @agent_ppo2.py:143][0m Total time:      23.12 min
[32m[20221213 23:35:39 @agent_ppo2.py:145][0m 2236416 total steps have happened
[32m[20221213 23:35:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3092 --------------------------#
[32m[20221213 23:35:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0038 |          52.5455 |           9.7449 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0069 |          37.5713 |           9.7821 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |           0.0007 |          36.4093 |           9.8076 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0079 |          34.5321 |           9.7863 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0097 |          33.8784 |           9.7739 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0139 |          33.6788 |           9.8339 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0054 |          34.4909 |           9.8730 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0140 |          33.1196 |           9.8540 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0128 |          32.7916 |           9.8748 |
[32m[20221213 23:35:40 @agent_ppo2.py:185][0m |          -0.0184 |          32.7376 |           9.8563 |
[32m[20221213 23:35:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.31
[32m[20221213 23:35:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.46
[32m[20221213 23:35:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 212.85
[32m[20221213 23:35:41 @agent_ppo2.py:143][0m Total time:      23.14 min
[32m[20221213 23:35:41 @agent_ppo2.py:145][0m 2238464 total steps have happened
[32m[20221213 23:35:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3093 --------------------------#
[32m[20221213 23:35:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:41 @agent_ppo2.py:185][0m |           0.0085 |          46.0753 |          10.1855 |
[32m[20221213 23:35:41 @agent_ppo2.py:185][0m |          -0.0074 |          38.4994 |          10.2407 |
[32m[20221213 23:35:41 @agent_ppo2.py:185][0m |          -0.0092 |          37.9353 |          10.2084 |
[32m[20221213 23:35:41 @agent_ppo2.py:185][0m |          -0.0054 |          37.0864 |          10.1875 |
[32m[20221213 23:35:41 @agent_ppo2.py:185][0m |          -0.0116 |          36.8493 |          10.2065 |
[32m[20221213 23:35:41 @agent_ppo2.py:185][0m |          -0.0079 |          36.5349 |          10.1978 |
[32m[20221213 23:35:41 @agent_ppo2.py:185][0m |          -0.0093 |          36.4489 |          10.1836 |
[32m[20221213 23:35:42 @agent_ppo2.py:185][0m |          -0.0059 |          36.2439 |          10.1766 |
[32m[20221213 23:35:42 @agent_ppo2.py:185][0m |          -0.0133 |          35.9019 |          10.2060 |
[32m[20221213 23:35:42 @agent_ppo2.py:185][0m |          -0.0092 |          35.7739 |          10.1741 |
[32m[20221213 23:35:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.15
[32m[20221213 23:35:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.53
[32m[20221213 23:35:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.46
[32m[20221213 23:35:42 @agent_ppo2.py:143][0m Total time:      23.16 min
[32m[20221213 23:35:42 @agent_ppo2.py:145][0m 2240512 total steps have happened
[32m[20221213 23:35:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3094 --------------------------#
[32m[20221213 23:35:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:42 @agent_ppo2.py:185][0m |           0.0017 |          67.9259 |          10.3476 |
[32m[20221213 23:35:42 @agent_ppo2.py:185][0m |          -0.0035 |          66.8520 |          10.3216 |
[32m[20221213 23:35:42 @agent_ppo2.py:185][0m |          -0.0080 |          65.9263 |          10.2917 |
[32m[20221213 23:35:42 @agent_ppo2.py:185][0m |          -0.0098 |          65.7240 |          10.3539 |
[32m[20221213 23:35:43 @agent_ppo2.py:185][0m |           0.0037 |          70.7807 |          10.2762 |
[32m[20221213 23:35:43 @agent_ppo2.py:185][0m |          -0.0067 |          65.3973 |          10.2825 |
[32m[20221213 23:35:43 @agent_ppo2.py:185][0m |          -0.0053 |          67.1814 |          10.2898 |
[32m[20221213 23:35:43 @agent_ppo2.py:185][0m |          -0.0049 |          68.3954 |          10.2716 |
[32m[20221213 23:35:43 @agent_ppo2.py:185][0m |          -0.0082 |          65.3648 |          10.2478 |
[32m[20221213 23:35:43 @agent_ppo2.py:185][0m |          -0.0082 |          65.8810 |          10.3229 |
[32m[20221213 23:35:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.54
[32m[20221213 23:35:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.53
[32m[20221213 23:35:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.82
[32m[20221213 23:35:43 @agent_ppo2.py:143][0m Total time:      23.19 min
[32m[20221213 23:35:43 @agent_ppo2.py:145][0m 2242560 total steps have happened
[32m[20221213 23:35:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3095 --------------------------#
[32m[20221213 23:35:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:43 @agent_ppo2.py:185][0m |           0.0021 |          58.7554 |           9.9627 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |           0.0001 |          57.3622 |           9.9311 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0043 |          56.6077 |           9.9859 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0038 |          56.2663 |           9.9270 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0044 |          56.0439 |           9.9670 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0057 |          55.8458 |           9.9315 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0099 |          55.7752 |           9.9294 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0094 |          55.7216 |          10.0130 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0085 |          55.4317 |           9.9952 |
[32m[20221213 23:35:44 @agent_ppo2.py:185][0m |          -0.0075 |          55.4418 |          10.0142 |
[32m[20221213 23:35:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.88
[32m[20221213 23:35:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.96
[32m[20221213 23:35:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.18
[32m[20221213 23:35:44 @agent_ppo2.py:143][0m Total time:      23.21 min
[32m[20221213 23:35:44 @agent_ppo2.py:145][0m 2244608 total steps have happened
[32m[20221213 23:35:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3096 --------------------------#
[32m[20221213 23:35:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |           0.0024 |          58.4879 |           9.9704 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |          -0.0021 |          54.5558 |          10.0124 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |           0.0090 |          58.0547 |           9.9977 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |          -0.0020 |          53.8827 |          10.0053 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |          -0.0061 |          52.4149 |          10.0342 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |          -0.0064 |          52.4527 |           9.9277 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |          -0.0118 |          51.6372 |           9.9759 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |          -0.0132 |          50.9652 |           9.9679 |
[32m[20221213 23:35:45 @agent_ppo2.py:185][0m |          -0.0119 |          50.6083 |           9.9466 |
[32m[20221213 23:35:46 @agent_ppo2.py:185][0m |          -0.0116 |          50.8915 |           9.9691 |
[32m[20221213 23:35:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:35:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.66
[32m[20221213 23:35:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.59
[32m[20221213 23:35:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.02
[32m[20221213 23:35:46 @agent_ppo2.py:143][0m Total time:      23.23 min
[32m[20221213 23:35:46 @agent_ppo2.py:145][0m 2246656 total steps have happened
[32m[20221213 23:35:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3097 --------------------------#
[32m[20221213 23:35:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:46 @agent_ppo2.py:185][0m |           0.0041 |          69.5996 |           9.9573 |
[32m[20221213 23:35:46 @agent_ppo2.py:185][0m |          -0.0002 |          67.5190 |           9.9090 |
[32m[20221213 23:35:46 @agent_ppo2.py:185][0m |          -0.0052 |          66.3403 |           9.8480 |
[32m[20221213 23:35:46 @agent_ppo2.py:185][0m |          -0.0085 |          65.7065 |           9.8305 |
[32m[20221213 23:35:46 @agent_ppo2.py:185][0m |          -0.0041 |          64.9536 |           9.8213 |
[32m[20221213 23:35:46 @agent_ppo2.py:185][0m |          -0.0105 |          64.5255 |           9.7627 |
[32m[20221213 23:35:47 @agent_ppo2.py:185][0m |          -0.0079 |          64.1179 |           9.7491 |
[32m[20221213 23:35:47 @agent_ppo2.py:185][0m |          -0.0070 |          64.1589 |           9.7084 |
[32m[20221213 23:35:47 @agent_ppo2.py:185][0m |          -0.0090 |          63.8506 |           9.6995 |
[32m[20221213 23:35:47 @agent_ppo2.py:185][0m |          -0.0118 |          63.8430 |           9.7061 |
[32m[20221213 23:35:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:35:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.44
[32m[20221213 23:35:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.89
[32m[20221213 23:35:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.30
[32m[20221213 23:35:47 @agent_ppo2.py:143][0m Total time:      23.25 min
[32m[20221213 23:35:47 @agent_ppo2.py:145][0m 2248704 total steps have happened
[32m[20221213 23:35:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3098 --------------------------#
[32m[20221213 23:35:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:47 @agent_ppo2.py:185][0m |           0.0010 |          67.9589 |           9.7146 |
[32m[20221213 23:35:47 @agent_ppo2.py:185][0m |          -0.0035 |          66.1081 |           9.7595 |
[32m[20221213 23:35:47 @agent_ppo2.py:185][0m |          -0.0051 |          65.7656 |           9.6856 |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0043 |          65.6001 |           9.6656 |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0057 |          65.5242 |           9.6793 |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0084 |          65.2030 |           9.6941 |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0070 |          65.2524 |           9.6651 |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0081 |          65.3562 |           9.6951 |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0031 |          66.2630 |           9.6682 |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0030 |          66.1034 |           9.6678 |
[32m[20221213 23:35:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:35:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.82
[32m[20221213 23:35:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.39
[32m[20221213 23:35:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.38
[32m[20221213 23:35:48 @agent_ppo2.py:143][0m Total time:      23.27 min
[32m[20221213 23:35:48 @agent_ppo2.py:145][0m 2250752 total steps have happened
[32m[20221213 23:35:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3099 --------------------------#
[32m[20221213 23:35:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:48 @agent_ppo2.py:185][0m |          -0.0005 |          60.3376 |           9.6247 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0058 |          56.7919 |           9.6879 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0063 |          56.0622 |           9.7418 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0076 |          55.6456 |           9.6584 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0082 |          55.3075 |           9.6638 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0100 |          55.2890 |           9.6510 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0122 |          54.9955 |           9.6764 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0109 |          54.9777 |           9.5985 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0117 |          54.7340 |           9.6035 |
[32m[20221213 23:35:49 @agent_ppo2.py:185][0m |          -0.0129 |          54.5908 |           9.6150 |
[32m[20221213 23:35:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.84
[32m[20221213 23:35:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.39
[32m[20221213 23:35:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.39
[32m[20221213 23:35:49 @agent_ppo2.py:143][0m Total time:      23.29 min
[32m[20221213 23:35:49 @agent_ppo2.py:145][0m 2252800 total steps have happened
[32m[20221213 23:35:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3100 --------------------------#
[32m[20221213 23:35:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:35:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |           0.0089 |          68.0472 |           9.8544 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0047 |          62.4648 |           9.8739 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0077 |          61.4215 |           9.8763 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0098 |          60.9873 |           9.8943 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0093 |          60.6297 |           9.9984 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0069 |          60.3686 |           9.9094 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0080 |          60.1542 |           9.9833 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0110 |          60.0996 |           9.9893 |
[32m[20221213 23:35:50 @agent_ppo2.py:185][0m |          -0.0080 |          60.4998 |          10.0474 |
[32m[20221213 23:35:51 @agent_ppo2.py:185][0m |          -0.0101 |          59.8319 |          10.0505 |
[32m[20221213 23:35:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:35:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.05
[32m[20221213 23:35:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.03
[32m[20221213 23:35:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.31
[32m[20221213 23:35:51 @agent_ppo2.py:143][0m Total time:      23.31 min
[32m[20221213 23:35:51 @agent_ppo2.py:145][0m 2254848 total steps have happened
[32m[20221213 23:35:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3101 --------------------------#
[32m[20221213 23:35:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:51 @agent_ppo2.py:185][0m |          -0.0001 |          66.1743 |          10.0580 |
[32m[20221213 23:35:51 @agent_ppo2.py:185][0m |           0.0032 |          65.6405 |          10.0547 |
[32m[20221213 23:35:51 @agent_ppo2.py:185][0m |          -0.0008 |          63.1703 |          10.0281 |
[32m[20221213 23:35:51 @agent_ppo2.py:185][0m |          -0.0088 |          61.5659 |          10.0580 |
[32m[20221213 23:35:51 @agent_ppo2.py:185][0m |          -0.0096 |          61.3116 |          10.1074 |
[32m[20221213 23:35:51 @agent_ppo2.py:185][0m |          -0.0080 |          60.9350 |          10.0607 |
[32m[20221213 23:35:52 @agent_ppo2.py:185][0m |          -0.0117 |          60.6401 |          10.1295 |
[32m[20221213 23:35:52 @agent_ppo2.py:185][0m |          -0.0053 |          61.0343 |          10.0672 |
[32m[20221213 23:35:52 @agent_ppo2.py:185][0m |          -0.0108 |          60.4814 |          10.0982 |
[32m[20221213 23:35:52 @agent_ppo2.py:185][0m |          -0.0116 |          60.1647 |          10.0510 |
[32m[20221213 23:35:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:35:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.35
[32m[20221213 23:35:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.39
[32m[20221213 23:35:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.42
[32m[20221213 23:35:52 @agent_ppo2.py:143][0m Total time:      23.33 min
[32m[20221213 23:35:52 @agent_ppo2.py:145][0m 2256896 total steps have happened
[32m[20221213 23:35:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3102 --------------------------#
[32m[20221213 23:35:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:52 @agent_ppo2.py:185][0m |           0.0011 |          54.0763 |           9.8913 |
[32m[20221213 23:35:52 @agent_ppo2.py:185][0m |          -0.0038 |          49.9255 |           9.9140 |
[32m[20221213 23:35:52 @agent_ppo2.py:185][0m |          -0.0117 |          49.2846 |           9.8932 |
[32m[20221213 23:35:53 @agent_ppo2.py:185][0m |          -0.0091 |          48.5997 |           9.9140 |
[32m[20221213 23:35:53 @agent_ppo2.py:185][0m |          -0.0041 |          48.4236 |           9.8586 |
[32m[20221213 23:35:53 @agent_ppo2.py:185][0m |          -0.0105 |          47.7813 |           9.9216 |
[32m[20221213 23:35:53 @agent_ppo2.py:185][0m |          -0.0066 |          51.4294 |           9.9016 |
[32m[20221213 23:35:53 @agent_ppo2.py:185][0m |          -0.0037 |          48.9317 |           9.8725 |
[32m[20221213 23:35:53 @agent_ppo2.py:185][0m |          -0.0111 |          46.7152 |           9.9062 |
[32m[20221213 23:35:53 @agent_ppo2.py:185][0m |          -0.0111 |          46.3234 |           9.8829 |
[32m[20221213 23:35:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:35:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.85
[32m[20221213 23:35:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.06
[32m[20221213 23:35:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.21
[32m[20221213 23:35:53 @agent_ppo2.py:143][0m Total time:      23.35 min
[32m[20221213 23:35:53 @agent_ppo2.py:145][0m 2258944 total steps have happened
[32m[20221213 23:35:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3103 --------------------------#
[32m[20221213 23:35:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0004 |          64.8991 |           9.6985 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0009 |          59.7019 |           9.6555 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0043 |          58.1631 |           9.6425 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0042 |          57.3816 |           9.6254 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0080 |          56.2798 |           9.5964 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0099 |          55.8161 |           9.5355 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0026 |          59.7183 |           9.5585 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0088 |          55.3504 |           9.5357 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0107 |          54.4488 |           9.5065 |
[32m[20221213 23:35:54 @agent_ppo2.py:185][0m |          -0.0137 |          54.0058 |           9.4323 |
[32m[20221213 23:35:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:35:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.04
[32m[20221213 23:35:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.15
[32m[20221213 23:35:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.11
[32m[20221213 23:35:54 @agent_ppo2.py:143][0m Total time:      23.37 min
[32m[20221213 23:35:54 @agent_ppo2.py:145][0m 2260992 total steps have happened
[32m[20221213 23:35:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3104 --------------------------#
[32m[20221213 23:35:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |           0.0068 |          58.4908 |           9.1492 |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |          -0.0017 |          51.9150 |           9.2252 |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |          -0.0057 |          50.2714 |           9.1109 |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |          -0.0039 |          49.5184 |           9.0693 |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |          -0.0061 |          48.6976 |           9.0663 |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |          -0.0105 |          48.1560 |           9.0364 |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |          -0.0118 |          47.8611 |           9.0168 |
[32m[20221213 23:35:55 @agent_ppo2.py:185][0m |          -0.0112 |          47.5219 |           9.0149 |
[32m[20221213 23:35:56 @agent_ppo2.py:185][0m |          -0.0126 |          47.3408 |           9.0095 |
[32m[20221213 23:35:56 @agent_ppo2.py:185][0m |          -0.0143 |          47.2060 |           8.9760 |
[32m[20221213 23:35:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:35:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.32
[32m[20221213 23:35:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.41
[32m[20221213 23:35:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.59
[32m[20221213 23:35:56 @agent_ppo2.py:143][0m Total time:      23.40 min
[32m[20221213 23:35:56 @agent_ppo2.py:145][0m 2263040 total steps have happened
[32m[20221213 23:35:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3105 --------------------------#
[32m[20221213 23:35:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:56 @agent_ppo2.py:185][0m |          -0.0006 |          79.8797 |           9.3406 |
[32m[20221213 23:35:56 @agent_ppo2.py:185][0m |          -0.0071 |          70.0635 |           9.3518 |
[32m[20221213 23:35:56 @agent_ppo2.py:185][0m |           0.0013 |          73.7174 |           9.3167 |
[32m[20221213 23:35:56 @agent_ppo2.py:185][0m |          -0.0079 |          68.0856 |           9.2999 |
[32m[20221213 23:35:57 @agent_ppo2.py:185][0m |          -0.0086 |          72.4164 |           9.3204 |
[32m[20221213 23:35:57 @agent_ppo2.py:185][0m |          -0.0056 |          66.7304 |           9.3201 |
[32m[20221213 23:35:57 @agent_ppo2.py:185][0m |          -0.0126 |          66.2940 |           9.3463 |
[32m[20221213 23:35:57 @agent_ppo2.py:185][0m |          -0.0156 |          65.9898 |           9.3054 |
[32m[20221213 23:35:57 @agent_ppo2.py:185][0m |          -0.0145 |          65.7154 |           9.3135 |
[32m[20221213 23:35:57 @agent_ppo2.py:185][0m |          -0.0065 |          72.0650 |           9.2867 |
[32m[20221213 23:35:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:35:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.87
[32m[20221213 23:35:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.95
[32m[20221213 23:35:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.75
[32m[20221213 23:35:57 @agent_ppo2.py:143][0m Total time:      23.42 min
[32m[20221213 23:35:57 @agent_ppo2.py:145][0m 2265088 total steps have happened
[32m[20221213 23:35:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3106 --------------------------#
[32m[20221213 23:35:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:35:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:57 @agent_ppo2.py:185][0m |          -0.0013 |          64.4523 |           9.4542 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0061 |          58.7756 |           9.4561 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |           0.0029 |          62.6908 |           9.5048 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0095 |          54.6895 |           9.4463 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0091 |          54.2900 |           9.5075 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0164 |          53.1156 |           9.5269 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0124 |          52.5792 |           9.4750 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0044 |          54.2647 |           9.5496 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0045 |          59.5010 |           9.5840 |
[32m[20221213 23:35:58 @agent_ppo2.py:185][0m |          -0.0130 |          51.8350 |           9.5298 |
[32m[20221213 23:35:58 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:35:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.43
[32m[20221213 23:35:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.31
[32m[20221213 23:35:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.63
[32m[20221213 23:35:58 @agent_ppo2.py:143][0m Total time:      23.44 min
[32m[20221213 23:35:58 @agent_ppo2.py:145][0m 2267136 total steps have happened
[32m[20221213 23:35:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3107 --------------------------#
[32m[20221213 23:35:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:35:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |           0.0011 |          57.9329 |           8.8075 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0020 |          43.0909 |           8.8201 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0072 |          40.6311 |           8.8497 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0048 |          39.3373 |           8.8718 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0073 |          38.8365 |           8.8818 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0091 |          37.7058 |           8.8905 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0118 |          37.2837 |           8.8923 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0164 |          36.6336 |           8.9183 |
[32m[20221213 23:35:59 @agent_ppo2.py:185][0m |          -0.0145 |          36.4254 |           8.9265 |
[32m[20221213 23:36:00 @agent_ppo2.py:185][0m |          -0.0195 |          36.1732 |           8.9445 |
[32m[20221213 23:36:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:36:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.85
[32m[20221213 23:36:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.17
[32m[20221213 23:36:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.20
[32m[20221213 23:36:00 @agent_ppo2.py:143][0m Total time:      23.46 min
[32m[20221213 23:36:00 @agent_ppo2.py:145][0m 2269184 total steps have happened
[32m[20221213 23:36:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3108 --------------------------#
[32m[20221213 23:36:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:00 @agent_ppo2.py:185][0m |          -0.0031 |          54.4434 |           9.0554 |
[32m[20221213 23:36:00 @agent_ppo2.py:185][0m |          -0.0047 |          49.2775 |           9.1471 |
[32m[20221213 23:36:00 @agent_ppo2.py:185][0m |           0.0030 |          50.1678 |           9.0993 |
[32m[20221213 23:36:00 @agent_ppo2.py:185][0m |          -0.0022 |          48.2559 |           9.0914 |
[32m[20221213 23:36:00 @agent_ppo2.py:185][0m |          -0.0028 |          47.9316 |           9.1430 |
[32m[20221213 23:36:00 @agent_ppo2.py:185][0m |          -0.0038 |          49.9365 |           9.1029 |
[32m[20221213 23:36:01 @agent_ppo2.py:185][0m |          -0.0115 |          45.4253 |           9.1343 |
[32m[20221213 23:36:01 @agent_ppo2.py:185][0m |          -0.0129 |          45.1248 |           9.0768 |
[32m[20221213 23:36:01 @agent_ppo2.py:185][0m |          -0.0090 |          44.8883 |           9.1050 |
[32m[20221213 23:36:01 @agent_ppo2.py:185][0m |          -0.0148 |          44.5038 |           9.0994 |
[32m[20221213 23:36:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.49
[32m[20221213 23:36:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.14
[32m[20221213 23:36:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.24
[32m[20221213 23:36:01 @agent_ppo2.py:143][0m Total time:      23.48 min
[32m[20221213 23:36:01 @agent_ppo2.py:145][0m 2271232 total steps have happened
[32m[20221213 23:36:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3109 --------------------------#
[32m[20221213 23:36:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:01 @agent_ppo2.py:185][0m |           0.0028 |          64.3596 |           8.9072 |
[32m[20221213 23:36:01 @agent_ppo2.py:185][0m |          -0.0014 |          56.8928 |           8.9132 |
[32m[20221213 23:36:01 @agent_ppo2.py:185][0m |           0.0021 |          53.7693 |           8.8745 |
[32m[20221213 23:36:02 @agent_ppo2.py:185][0m |           0.0004 |          52.1476 |           8.8561 |
[32m[20221213 23:36:02 @agent_ppo2.py:185][0m |          -0.0095 |          49.5897 |           8.9001 |
[32m[20221213 23:36:02 @agent_ppo2.py:185][0m |          -0.0110 |          48.4609 |           8.9235 |
[32m[20221213 23:36:02 @agent_ppo2.py:185][0m |          -0.0065 |          47.4252 |           8.9283 |
[32m[20221213 23:36:02 @agent_ppo2.py:185][0m |          -0.0127 |          46.4464 |           8.8844 |
[32m[20221213 23:36:02 @agent_ppo2.py:185][0m |          -0.0023 |          50.4864 |           8.9232 |
[32m[20221213 23:36:02 @agent_ppo2.py:185][0m |          -0.0075 |          45.8234 |           8.8688 |
[32m[20221213 23:36:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.40
[32m[20221213 23:36:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.00
[32m[20221213 23:36:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.85
[32m[20221213 23:36:02 @agent_ppo2.py:143][0m Total time:      23.50 min
[32m[20221213 23:36:02 @agent_ppo2.py:145][0m 2273280 total steps have happened
[32m[20221213 23:36:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3110 --------------------------#
[32m[20221213 23:36:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:36:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0025 |          71.6524 |           9.3700 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0002 |          71.3364 |           9.3221 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0114 |          66.1377 |           9.2500 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0110 |          65.1194 |           9.2981 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0136 |          64.3281 |           9.2636 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0129 |          63.7164 |           9.2357 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0136 |          63.3314 |           9.2496 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0100 |          62.9512 |           9.2381 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0129 |          62.7942 |           9.2232 |
[32m[20221213 23:36:03 @agent_ppo2.py:185][0m |          -0.0143 |          62.7226 |           9.1902 |
[32m[20221213 23:36:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:36:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.38
[32m[20221213 23:36:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.29
[32m[20221213 23:36:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.23
[32m[20221213 23:36:03 @agent_ppo2.py:143][0m Total time:      23.52 min
[32m[20221213 23:36:03 @agent_ppo2.py:145][0m 2275328 total steps have happened
[32m[20221213 23:36:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3111 --------------------------#
[32m[20221213 23:36:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |           0.0074 |          50.3019 |           8.8634 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0075 |          39.7188 |           8.9561 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0061 |          38.7021 |           8.9408 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0022 |          38.8711 |           8.9354 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0062 |          37.9015 |           8.9626 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0051 |          38.3841 |           8.9637 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0079 |          37.5297 |           8.9366 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0141 |          37.4359 |           8.9657 |
[32m[20221213 23:36:04 @agent_ppo2.py:185][0m |          -0.0131 |          37.3902 |           8.9270 |
[32m[20221213 23:36:05 @agent_ppo2.py:185][0m |          -0.0114 |          37.2004 |           8.9548 |
[32m[20221213 23:36:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.32
[32m[20221213 23:36:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.36
[32m[20221213 23:36:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.98
[32m[20221213 23:36:05 @agent_ppo2.py:143][0m Total time:      23.55 min
[32m[20221213 23:36:05 @agent_ppo2.py:145][0m 2277376 total steps have happened
[32m[20221213 23:36:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3112 --------------------------#
[32m[20221213 23:36:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:05 @agent_ppo2.py:185][0m |           0.0085 |          51.4154 |           8.7383 |
[32m[20221213 23:36:05 @agent_ppo2.py:185][0m |          -0.0061 |          44.7057 |           8.8015 |
[32m[20221213 23:36:05 @agent_ppo2.py:185][0m |          -0.0075 |          43.5755 |           8.7090 |
[32m[20221213 23:36:05 @agent_ppo2.py:185][0m |          -0.0049 |          43.5988 |           8.7484 |
[32m[20221213 23:36:05 @agent_ppo2.py:185][0m |          -0.0120 |          42.4106 |           8.6983 |
[32m[20221213 23:36:05 @agent_ppo2.py:185][0m |          -0.0122 |          42.0377 |           8.7055 |
[32m[20221213 23:36:06 @agent_ppo2.py:185][0m |          -0.0160 |          41.5367 |           8.7089 |
[32m[20221213 23:36:06 @agent_ppo2.py:185][0m |          -0.0116 |          41.5171 |           8.6586 |
[32m[20221213 23:36:06 @agent_ppo2.py:185][0m |          -0.0145 |          41.0853 |           8.7036 |
[32m[20221213 23:36:06 @agent_ppo2.py:185][0m |          -0.0120 |          40.7531 |           8.7160 |
[32m[20221213 23:36:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.11
[32m[20221213 23:36:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.70
[32m[20221213 23:36:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.54
[32m[20221213 23:36:06 @agent_ppo2.py:143][0m Total time:      23.57 min
[32m[20221213 23:36:06 @agent_ppo2.py:145][0m 2279424 total steps have happened
[32m[20221213 23:36:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3113 --------------------------#
[32m[20221213 23:36:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:06 @agent_ppo2.py:185][0m |          -0.0015 |          44.2636 |           9.0156 |
[32m[20221213 23:36:06 @agent_ppo2.py:185][0m |          -0.0040 |          41.2167 |           9.0801 |
[32m[20221213 23:36:06 @agent_ppo2.py:185][0m |          -0.0089 |          40.5363 |           9.1151 |
[32m[20221213 23:36:07 @agent_ppo2.py:185][0m |          -0.0087 |          40.0423 |           9.1011 |
[32m[20221213 23:36:07 @agent_ppo2.py:185][0m |          -0.0096 |          39.7836 |           9.1856 |
[32m[20221213 23:36:07 @agent_ppo2.py:185][0m |          -0.0094 |          40.0027 |           9.1749 |
[32m[20221213 23:36:07 @agent_ppo2.py:185][0m |          -0.0136 |          39.4193 |           9.2173 |
[32m[20221213 23:36:07 @agent_ppo2.py:185][0m |          -0.0117 |          39.1884 |           9.2295 |
[32m[20221213 23:36:07 @agent_ppo2.py:185][0m |          -0.0148 |          39.0743 |           9.2917 |
[32m[20221213 23:36:07 @agent_ppo2.py:185][0m |          -0.0173 |          38.9967 |           9.2686 |
[32m[20221213 23:36:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.35
[32m[20221213 23:36:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.08
[32m[20221213 23:36:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.42
[32m[20221213 23:36:07 @agent_ppo2.py:143][0m Total time:      23.59 min
[32m[20221213 23:36:07 @agent_ppo2.py:145][0m 2281472 total steps have happened
[32m[20221213 23:36:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3114 --------------------------#
[32m[20221213 23:36:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |           0.0068 |          76.7857 |           8.8342 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0016 |          74.7585 |           8.7776 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0085 |          70.1278 |           8.8794 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0103 |          69.6443 |           8.8078 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0128 |          68.9435 |           8.8372 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0113 |          68.7598 |           8.8253 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0071 |          69.2246 |           8.8361 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |           0.0022 |          74.8303 |           8.8191 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0114 |          68.2269 |           8.9098 |
[32m[20221213 23:36:08 @agent_ppo2.py:185][0m |          -0.0152 |          68.0758 |           8.8541 |
[32m[20221213 23:36:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.53
[32m[20221213 23:36:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.84
[32m[20221213 23:36:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.51
[32m[20221213 23:36:08 @agent_ppo2.py:143][0m Total time:      23.61 min
[32m[20221213 23:36:08 @agent_ppo2.py:145][0m 2283520 total steps have happened
[32m[20221213 23:36:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3115 --------------------------#
[32m[20221213 23:36:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0076 |          34.1688 |           8.6483 |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0048 |          30.0778 |           8.6420 |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0119 |          29.1761 |           8.6314 |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0062 |          28.3851 |           8.6499 |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0134 |          27.8704 |           8.6138 |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0120 |          27.8233 |           8.5956 |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0114 |          27.2107 |           8.6847 |
[32m[20221213 23:36:09 @agent_ppo2.py:185][0m |          -0.0138 |          26.9506 |           8.6758 |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |          -0.0113 |          26.6010 |           8.6757 |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |          -0.0152 |          26.3723 |           8.7100 |
[32m[20221213 23:36:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.18
[32m[20221213 23:36:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.50
[32m[20221213 23:36:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.23
[32m[20221213 23:36:10 @agent_ppo2.py:143][0m Total time:      23.63 min
[32m[20221213 23:36:10 @agent_ppo2.py:145][0m 2285568 total steps have happened
[32m[20221213 23:36:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3116 --------------------------#
[32m[20221213 23:36:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |          -0.0035 |          62.4695 |           8.7845 |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |          -0.0062 |          58.5948 |           8.8035 |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |          -0.0092 |          56.9300 |           8.8226 |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |          -0.0091 |          56.0161 |           8.7520 |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |           0.0032 |          58.9995 |           8.8346 |
[32m[20221213 23:36:10 @agent_ppo2.py:185][0m |          -0.0097 |          54.7275 |           8.8497 |
[32m[20221213 23:36:11 @agent_ppo2.py:185][0m |          -0.0076 |          57.7256 |           8.8818 |
[32m[20221213 23:36:11 @agent_ppo2.py:185][0m |          -0.0134 |          53.8082 |           8.8984 |
[32m[20221213 23:36:11 @agent_ppo2.py:185][0m |          -0.0075 |          54.8605 |           8.9299 |
[32m[20221213 23:36:11 @agent_ppo2.py:185][0m |          -0.0037 |          56.1497 |           8.8707 |
[32m[20221213 23:36:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.87
[32m[20221213 23:36:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.62
[32m[20221213 23:36:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.01
[32m[20221213 23:36:11 @agent_ppo2.py:143][0m Total time:      23.65 min
[32m[20221213 23:36:11 @agent_ppo2.py:145][0m 2287616 total steps have happened
[32m[20221213 23:36:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3117 --------------------------#
[32m[20221213 23:36:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:11 @agent_ppo2.py:185][0m |          -0.0004 |          54.5765 |           9.1727 |
[32m[20221213 23:36:11 @agent_ppo2.py:185][0m |          -0.0037 |          47.9185 |           9.2160 |
[32m[20221213 23:36:11 @agent_ppo2.py:185][0m |          -0.0116 |          45.9047 |           9.2617 |
[32m[20221213 23:36:12 @agent_ppo2.py:185][0m |          -0.0080 |          45.1440 |           9.3039 |
[32m[20221213 23:36:12 @agent_ppo2.py:185][0m |          -0.0104 |          44.8430 |           9.2938 |
[32m[20221213 23:36:12 @agent_ppo2.py:185][0m |          -0.0158 |          44.3967 |           9.3651 |
[32m[20221213 23:36:12 @agent_ppo2.py:185][0m |          -0.0002 |          47.7851 |           9.3490 |
[32m[20221213 23:36:12 @agent_ppo2.py:185][0m |          -0.0076 |          44.5556 |           9.3817 |
[32m[20221213 23:36:12 @agent_ppo2.py:185][0m |          -0.0136 |          43.6719 |           9.4932 |
[32m[20221213 23:36:12 @agent_ppo2.py:185][0m |          -0.0117 |          43.6293 |           9.4620 |
[32m[20221213 23:36:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.70
[32m[20221213 23:36:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.59
[32m[20221213 23:36:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.12
[32m[20221213 23:36:12 @agent_ppo2.py:143][0m Total time:      23.67 min
[32m[20221213 23:36:12 @agent_ppo2.py:145][0m 2289664 total steps have happened
[32m[20221213 23:36:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3118 --------------------------#
[32m[20221213 23:36:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0040 |          57.8903 |           9.0227 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0072 |          54.5294 |           9.1065 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0096 |          53.4215 |           9.1570 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0047 |          52.7915 |           9.1597 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0135 |          52.4059 |           9.1918 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0066 |          52.6388 |           9.2232 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0106 |          51.8737 |           9.1936 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0123 |          51.7428 |           9.2294 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0106 |          52.1139 |           9.2312 |
[32m[20221213 23:36:13 @agent_ppo2.py:185][0m |          -0.0051 |          54.0281 |           9.2627 |
[32m[20221213 23:36:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.18
[32m[20221213 23:36:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.83
[32m[20221213 23:36:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.02
[32m[20221213 23:36:13 @agent_ppo2.py:143][0m Total time:      23.69 min
[32m[20221213 23:36:13 @agent_ppo2.py:145][0m 2291712 total steps have happened
[32m[20221213 23:36:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3119 --------------------------#
[32m[20221213 23:36:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |           0.0009 |          46.9037 |           9.6855 |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |          -0.0041 |          40.3950 |           9.6040 |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |          -0.0039 |          38.7425 |           9.6290 |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |          -0.0135 |          37.9799 |           9.6697 |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |          -0.0051 |          37.3479 |           9.6192 |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |          -0.0102 |          36.6828 |           9.6632 |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |          -0.0104 |          36.3199 |           9.6087 |
[32m[20221213 23:36:14 @agent_ppo2.py:185][0m |          -0.0066 |          36.0468 |           9.6223 |
[32m[20221213 23:36:15 @agent_ppo2.py:185][0m |          -0.0101 |          35.6588 |           9.6192 |
[32m[20221213 23:36:15 @agent_ppo2.py:185][0m |          -0.0076 |          35.5336 |           9.5977 |
[32m[20221213 23:36:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:36:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.43
[32m[20221213 23:36:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.16
[32m[20221213 23:36:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.46
[32m[20221213 23:36:15 @agent_ppo2.py:143][0m Total time:      23.71 min
[32m[20221213 23:36:15 @agent_ppo2.py:145][0m 2293760 total steps have happened
[32m[20221213 23:36:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3120 --------------------------#
[32m[20221213 23:36:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:36:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:15 @agent_ppo2.py:185][0m |           0.0024 |          70.4459 |           9.1124 |
[32m[20221213 23:36:15 @agent_ppo2.py:185][0m |           0.0036 |          72.5737 |           9.0710 |
[32m[20221213 23:36:15 @agent_ppo2.py:185][0m |          -0.0058 |          66.8953 |           9.1385 |
[32m[20221213 23:36:15 @agent_ppo2.py:185][0m |          -0.0052 |          66.0910 |           9.0788 |
[32m[20221213 23:36:15 @agent_ppo2.py:185][0m |          -0.0068 |          65.8561 |           9.0760 |
[32m[20221213 23:36:16 @agent_ppo2.py:185][0m |          -0.0089 |          65.7996 |           9.0768 |
[32m[20221213 23:36:16 @agent_ppo2.py:185][0m |          -0.0049 |          65.5236 |           9.0918 |
[32m[20221213 23:36:16 @agent_ppo2.py:185][0m |          -0.0096 |          65.3744 |           9.0941 |
[32m[20221213 23:36:16 @agent_ppo2.py:185][0m |          -0.0097 |          65.0650 |           9.1219 |
[32m[20221213 23:36:16 @agent_ppo2.py:185][0m |          -0.0129 |          64.8203 |           9.1388 |
[32m[20221213 23:36:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.02
[32m[20221213 23:36:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.15
[32m[20221213 23:36:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.27
[32m[20221213 23:36:16 @agent_ppo2.py:143][0m Total time:      23.73 min
[32m[20221213 23:36:16 @agent_ppo2.py:145][0m 2295808 total steps have happened
[32m[20221213 23:36:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3121 --------------------------#
[32m[20221213 23:36:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:16 @agent_ppo2.py:185][0m |           0.0004 |          60.9484 |           9.3521 |
[32m[20221213 23:36:16 @agent_ppo2.py:185][0m |          -0.0024 |          56.9927 |           9.2937 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0045 |          56.1670 |           9.3416 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0037 |          55.2495 |           9.2993 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0060 |          54.6590 |           9.2945 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0098 |          54.3470 |           9.3097 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0141 |          54.3701 |           9.3465 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0146 |          54.1135 |           9.2748 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0111 |          53.8332 |           9.2698 |
[32m[20221213 23:36:17 @agent_ppo2.py:185][0m |          -0.0091 |          53.3556 |           9.2396 |
[32m[20221213 23:36:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.53
[32m[20221213 23:36:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.23
[32m[20221213 23:36:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.39
[32m[20221213 23:36:17 @agent_ppo2.py:143][0m Total time:      23.75 min
[32m[20221213 23:36:17 @agent_ppo2.py:145][0m 2297856 total steps have happened
[32m[20221213 23:36:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3122 --------------------------#
[32m[20221213 23:36:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |           0.0067 |          61.7263 |           9.3755 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0009 |          54.4793 |           9.3878 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0073 |          52.6542 |           9.3436 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0083 |          51.5169 |           9.3009 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0113 |          50.7799 |           9.3853 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0049 |          53.2992 |           9.3391 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0108 |          49.8906 |           9.3762 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0139 |          49.0740 |           9.2875 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0105 |          49.0261 |           9.3474 |
[32m[20221213 23:36:18 @agent_ppo2.py:185][0m |          -0.0099 |          48.8377 |           9.3577 |
[32m[20221213 23:36:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.34
[32m[20221213 23:36:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.53
[32m[20221213 23:36:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.78
[32m[20221213 23:36:19 @agent_ppo2.py:143][0m Total time:      23.77 min
[32m[20221213 23:36:19 @agent_ppo2.py:145][0m 2299904 total steps have happened
[32m[20221213 23:36:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3123 --------------------------#
[32m[20221213 23:36:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |          -0.0003 |          68.2395 |           8.4486 |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |           0.0063 |          68.7578 |           8.4564 |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |          -0.0079 |          61.1241 |           8.4673 |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |          -0.0095 |          59.6676 |           8.4260 |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |           0.0010 |          60.3885 |           8.4547 |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |          -0.0086 |          57.7275 |           8.3917 |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |          -0.0043 |          57.3388 |           8.3984 |
[32m[20221213 23:36:19 @agent_ppo2.py:185][0m |          -0.0011 |          57.6668 |           8.3432 |
[32m[20221213 23:36:20 @agent_ppo2.py:185][0m |          -0.0141 |          55.8671 |           8.2796 |
[32m[20221213 23:36:20 @agent_ppo2.py:185][0m |          -0.0117 |          55.3016 |           8.3034 |
[32m[20221213 23:36:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.00
[32m[20221213 23:36:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.51
[32m[20221213 23:36:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.50
[32m[20221213 23:36:20 @agent_ppo2.py:143][0m Total time:      23.80 min
[32m[20221213 23:36:20 @agent_ppo2.py:145][0m 2301952 total steps have happened
[32m[20221213 23:36:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3124 --------------------------#
[32m[20221213 23:36:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:20 @agent_ppo2.py:185][0m |          -0.0061 |          55.3117 |           8.8555 |
[32m[20221213 23:36:20 @agent_ppo2.py:185][0m |          -0.0051 |          49.3499 |           8.8749 |
[32m[20221213 23:36:20 @agent_ppo2.py:185][0m |          -0.0080 |          48.2965 |           8.8681 |
[32m[20221213 23:36:20 @agent_ppo2.py:185][0m |          -0.0072 |          49.4210 |           8.8777 |
[32m[20221213 23:36:20 @agent_ppo2.py:185][0m |          -0.0066 |          47.1152 |           8.8886 |
[32m[20221213 23:36:21 @agent_ppo2.py:185][0m |          -0.0153 |          47.0053 |           8.8650 |
[32m[20221213 23:36:21 @agent_ppo2.py:185][0m |          -0.0064 |          46.5284 |           8.8409 |
[32m[20221213 23:36:21 @agent_ppo2.py:185][0m |          -0.0115 |          46.2956 |           8.8703 |
[32m[20221213 23:36:21 @agent_ppo2.py:185][0m |          -0.0134 |          46.0322 |           8.8390 |
[32m[20221213 23:36:21 @agent_ppo2.py:185][0m |          -0.0013 |          52.8357 |           8.8182 |
[32m[20221213 23:36:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:36:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.53
[32m[20221213 23:36:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.67
[32m[20221213 23:36:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.03
[32m[20221213 23:36:21 @agent_ppo2.py:143][0m Total time:      23.82 min
[32m[20221213 23:36:21 @agent_ppo2.py:145][0m 2304000 total steps have happened
[32m[20221213 23:36:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3125 --------------------------#
[32m[20221213 23:36:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:21 @agent_ppo2.py:185][0m |           0.0004 |          52.9635 |           8.5909 |
[32m[20221213 23:36:21 @agent_ppo2.py:185][0m |          -0.0035 |          49.4833 |           8.7775 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0093 |          48.5425 |           8.7961 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0118 |          48.1862 |           8.8473 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0096 |          47.7926 |           8.8731 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0079 |          47.4132 |           8.8848 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0110 |          47.2405 |           8.9761 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0083 |          47.6905 |           8.9731 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0133 |          46.8725 |           9.0818 |
[32m[20221213 23:36:22 @agent_ppo2.py:185][0m |          -0.0135 |          46.6527 |           9.0860 |
[32m[20221213 23:36:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.60
[32m[20221213 23:36:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.33
[32m[20221213 23:36:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.28
[32m[20221213 23:36:22 @agent_ppo2.py:143][0m Total time:      23.84 min
[32m[20221213 23:36:22 @agent_ppo2.py:145][0m 2306048 total steps have happened
[32m[20221213 23:36:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3126 --------------------------#
[32m[20221213 23:36:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0001 |          58.1846 |           9.5920 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0044 |          55.6738 |           9.6983 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0036 |          54.3638 |           9.7415 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0072 |          53.5659 |           9.7376 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0036 |          53.2502 |           9.7380 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0061 |          52.5850 |           9.7926 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |           0.0034 |          54.7206 |           9.8119 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0063 |          52.0040 |           9.9295 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0043 |          51.2336 |           9.8506 |
[32m[20221213 23:36:23 @agent_ppo2.py:185][0m |          -0.0066 |          51.1468 |           9.9252 |
[32m[20221213 23:36:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:36:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.10
[32m[20221213 23:36:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.46
[32m[20221213 23:36:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.77
[32m[20221213 23:36:24 @agent_ppo2.py:143][0m Total time:      23.86 min
[32m[20221213 23:36:24 @agent_ppo2.py:145][0m 2308096 total steps have happened
[32m[20221213 23:36:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3127 --------------------------#
[32m[20221213 23:36:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0031 |          57.2666 |          10.1035 |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0071 |          53.4196 |          10.0689 |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0041 |          52.6240 |          10.0387 |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0101 |          51.3790 |          10.0084 |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0180 |          50.8582 |          10.0323 |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0151 |          50.6641 |          10.0517 |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0165 |          50.1113 |           9.9922 |
[32m[20221213 23:36:24 @agent_ppo2.py:185][0m |          -0.0164 |          49.8552 |           9.9708 |
[32m[20221213 23:36:25 @agent_ppo2.py:185][0m |          -0.0158 |          49.5463 |           9.9087 |
[32m[20221213 23:36:25 @agent_ppo2.py:185][0m |          -0.0154 |          49.5488 |           9.8788 |
[32m[20221213 23:36:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.80
[32m[20221213 23:36:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.39
[32m[20221213 23:36:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.25
[32m[20221213 23:36:25 @agent_ppo2.py:143][0m Total time:      23.88 min
[32m[20221213 23:36:25 @agent_ppo2.py:145][0m 2310144 total steps have happened
[32m[20221213 23:36:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3128 --------------------------#
[32m[20221213 23:36:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:25 @agent_ppo2.py:185][0m |           0.0052 |          71.0441 |           9.5767 |
[32m[20221213 23:36:25 @agent_ppo2.py:185][0m |           0.0006 |          68.5473 |           9.5850 |
[32m[20221213 23:36:25 @agent_ppo2.py:185][0m |          -0.0042 |          67.7484 |           9.4874 |
[32m[20221213 23:36:25 @agent_ppo2.py:185][0m |          -0.0012 |          66.8211 |           9.5194 |
[32m[20221213 23:36:25 @agent_ppo2.py:185][0m |          -0.0031 |          66.1492 |           9.5541 |
[32m[20221213 23:36:26 @agent_ppo2.py:185][0m |          -0.0034 |          65.7519 |           9.4549 |
[32m[20221213 23:36:26 @agent_ppo2.py:185][0m |          -0.0059 |          65.4477 |           9.4818 |
[32m[20221213 23:36:26 @agent_ppo2.py:185][0m |          -0.0080 |          65.2345 |           9.4111 |
[32m[20221213 23:36:26 @agent_ppo2.py:185][0m |          -0.0043 |          64.5559 |           9.3745 |
[32m[20221213 23:36:26 @agent_ppo2.py:185][0m |           0.0080 |          74.7394 |           9.4065 |
[32m[20221213 23:36:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.43
[32m[20221213 23:36:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.03
[32m[20221213 23:36:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.78
[32m[20221213 23:36:26 @agent_ppo2.py:143][0m Total time:      23.90 min
[32m[20221213 23:36:26 @agent_ppo2.py:145][0m 2312192 total steps have happened
[32m[20221213 23:36:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3129 --------------------------#
[32m[20221213 23:36:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:26 @agent_ppo2.py:185][0m |           0.0056 |          63.6801 |           8.6848 |
[32m[20221213 23:36:26 @agent_ppo2.py:185][0m |           0.0006 |          60.8683 |           8.6328 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0018 |          59.0168 |           8.6709 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0079 |          58.4887 |           8.5852 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0068 |          58.1900 |           8.5998 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0032 |          58.7249 |           8.5928 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0026 |          59.1371 |           8.5573 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0089 |          57.5711 |           8.5478 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0017 |          58.3622 |           8.5524 |
[32m[20221213 23:36:27 @agent_ppo2.py:185][0m |          -0.0111 |          57.2657 |           8.5578 |
[32m[20221213 23:36:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:36:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.56
[32m[20221213 23:36:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.47
[32m[20221213 23:36:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.87
[32m[20221213 23:36:27 @agent_ppo2.py:143][0m Total time:      23.92 min
[32m[20221213 23:36:27 @agent_ppo2.py:145][0m 2314240 total steps have happened
[32m[20221213 23:36:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3130 --------------------------#
[32m[20221213 23:36:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:36:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0036 |          62.8295 |           8.4283 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0018 |          60.1715 |           8.4826 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0036 |          59.2845 |           8.4320 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0015 |          58.9100 |           8.4885 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0030 |          58.4773 |           8.4616 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0087 |          58.1957 |           8.4850 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0030 |          58.3633 |           8.4914 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0029 |          58.2492 |           8.5011 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0093 |          57.4589 |           8.5193 |
[32m[20221213 23:36:28 @agent_ppo2.py:185][0m |          -0.0016 |          57.6710 |           8.5465 |
[32m[20221213 23:36:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.30
[32m[20221213 23:36:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.85
[32m[20221213 23:36:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.32
[32m[20221213 23:36:29 @agent_ppo2.py:143][0m Total time:      23.94 min
[32m[20221213 23:36:29 @agent_ppo2.py:145][0m 2316288 total steps have happened
[32m[20221213 23:36:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3131 --------------------------#
[32m[20221213 23:36:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |           0.0199 |          70.8508 |           9.2776 |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |          -0.0041 |          61.7620 |           9.2915 |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |          -0.0062 |          60.1891 |           9.2996 |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |          -0.0065 |          59.3853 |           9.2442 |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |          -0.0089 |          59.0568 |           9.2333 |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |          -0.0112 |          58.4542 |           9.2061 |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |          -0.0075 |          58.1696 |           9.2136 |
[32m[20221213 23:36:29 @agent_ppo2.py:185][0m |          -0.0062 |          60.2718 |           9.1566 |
[32m[20221213 23:36:30 @agent_ppo2.py:185][0m |          -0.0081 |          57.6427 |           9.2060 |
[32m[20221213 23:36:30 @agent_ppo2.py:185][0m |          -0.0110 |          57.1789 |           9.2454 |
[32m[20221213 23:36:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.43
[32m[20221213 23:36:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.16
[32m[20221213 23:36:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.94
[32m[20221213 23:36:30 @agent_ppo2.py:143][0m Total time:      23.96 min
[32m[20221213 23:36:30 @agent_ppo2.py:145][0m 2318336 total steps have happened
[32m[20221213 23:36:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3132 --------------------------#
[32m[20221213 23:36:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:30 @agent_ppo2.py:185][0m |           0.0076 |          57.5074 |           9.1390 |
[32m[20221213 23:36:30 @agent_ppo2.py:185][0m |          -0.0117 |          44.5881 |           9.1268 |
[32m[20221213 23:36:30 @agent_ppo2.py:185][0m |          -0.0037 |          42.7661 |           9.2017 |
[32m[20221213 23:36:30 @agent_ppo2.py:185][0m |          -0.0084 |          41.8096 |           9.1832 |
[32m[20221213 23:36:30 @agent_ppo2.py:185][0m |          -0.0049 |          41.3720 |           9.2141 |
[32m[20221213 23:36:31 @agent_ppo2.py:185][0m |          -0.0071 |          40.9672 |           9.1716 |
[32m[20221213 23:36:31 @agent_ppo2.py:185][0m |          -0.0123 |          40.4901 |           9.2101 |
[32m[20221213 23:36:31 @agent_ppo2.py:185][0m |          -0.0119 |          40.2805 |           9.2187 |
[32m[20221213 23:36:31 @agent_ppo2.py:185][0m |          -0.0107 |          40.1177 |           9.2636 |
[32m[20221213 23:36:31 @agent_ppo2.py:185][0m |          -0.0104 |          39.8967 |           9.2409 |
[32m[20221213 23:36:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:36:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.23
[32m[20221213 23:36:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.91
[32m[20221213 23:36:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.73
[32m[20221213 23:36:31 @agent_ppo2.py:143][0m Total time:      23.98 min
[32m[20221213 23:36:31 @agent_ppo2.py:145][0m 2320384 total steps have happened
[32m[20221213 23:36:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3133 --------------------------#
[32m[20221213 23:36:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:31 @agent_ppo2.py:185][0m |           0.0008 |          60.7719 |           8.8033 |
[32m[20221213 23:36:31 @agent_ppo2.py:185][0m |          -0.0031 |          55.3011 |           8.8729 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0111 |          54.1112 |           8.8616 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0126 |          53.4354 |           8.8360 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0116 |          52.9431 |           8.9290 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0168 |          52.7165 |           8.9328 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0113 |          52.6087 |           8.9935 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0171 |          52.0492 |           9.0362 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0169 |          51.7329 |           9.0109 |
[32m[20221213 23:36:32 @agent_ppo2.py:185][0m |          -0.0007 |          58.4376 |           8.9958 |
[32m[20221213 23:36:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.95
[32m[20221213 23:36:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.12
[32m[20221213 23:36:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.97
[32m[20221213 23:36:32 @agent_ppo2.py:143][0m Total time:      24.00 min
[32m[20221213 23:36:32 @agent_ppo2.py:145][0m 2322432 total steps have happened
[32m[20221213 23:36:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3134 --------------------------#
[32m[20221213 23:36:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |           0.0104 |          72.4507 |           8.7795 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |           0.0005 |          65.9363 |           8.7605 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0069 |          65.2340 |           8.7699 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0061 |          64.9955 |           8.7378 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0109 |          64.5378 |           8.7810 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0118 |          64.3841 |           8.8259 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0070 |          64.3533 |           8.8382 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0102 |          64.1075 |           8.8033 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0020 |          73.0557 |           8.8562 |
[32m[20221213 23:36:33 @agent_ppo2.py:185][0m |          -0.0128 |          64.1329 |           8.8513 |
[32m[20221213 23:36:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.64
[32m[20221213 23:36:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.15
[32m[20221213 23:36:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.41
[32m[20221213 23:36:34 @agent_ppo2.py:143][0m Total time:      24.03 min
[32m[20221213 23:36:34 @agent_ppo2.py:145][0m 2324480 total steps have happened
[32m[20221213 23:36:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3135 --------------------------#
[32m[20221213 23:36:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:36:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0039 |          20.7121 |           9.2061 |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0003 |          18.2243 |           9.1804 |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0063 |          17.9394 |           9.1202 |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0035 |          17.8069 |           8.9896 |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0026 |          17.9652 |           8.9644 |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0074 |          17.7348 |           8.9886 |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0051 |          17.7175 |           8.9173 |
[32m[20221213 23:36:34 @agent_ppo2.py:185][0m |          -0.0077 |          17.7006 |           8.8700 |
[32m[20221213 23:36:35 @agent_ppo2.py:185][0m |          -0.0080 |          17.6773 |           8.8499 |
[32m[20221213 23:36:35 @agent_ppo2.py:185][0m |           0.0052 |          20.0837 |           8.8612 |
[32m[20221213 23:36:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:36:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:36:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:36:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.27
[32m[20221213 23:36:35 @agent_ppo2.py:143][0m Total time:      24.05 min
[32m[20221213 23:36:35 @agent_ppo2.py:145][0m 2326528 total steps have happened
[32m[20221213 23:36:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3136 --------------------------#
[32m[20221213 23:36:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:35 @agent_ppo2.py:185][0m |           0.0062 |          62.0687 |           8.6349 |
[32m[20221213 23:36:35 @agent_ppo2.py:185][0m |          -0.0020 |          56.8138 |           8.6370 |
[32m[20221213 23:36:35 @agent_ppo2.py:185][0m |          -0.0051 |          56.2108 |           8.6695 |
[32m[20221213 23:36:35 @agent_ppo2.py:185][0m |          -0.0083 |          55.0903 |           8.6267 |
[32m[20221213 23:36:35 @agent_ppo2.py:185][0m |           0.0037 |          57.8720 |           8.6610 |
[32m[20221213 23:36:36 @agent_ppo2.py:185][0m |          -0.0062 |          54.3963 |           8.6192 |
[32m[20221213 23:36:36 @agent_ppo2.py:185][0m |          -0.0101 |          53.9625 |           8.6494 |
[32m[20221213 23:36:36 @agent_ppo2.py:185][0m |          -0.0114 |          53.6549 |           8.6795 |
[32m[20221213 23:36:36 @agent_ppo2.py:185][0m |          -0.0129 |          53.6273 |           8.6633 |
[32m[20221213 23:36:36 @agent_ppo2.py:185][0m |          -0.0160 |          53.1530 |           8.6644 |
[32m[20221213 23:36:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.44
[32m[20221213 23:36:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.35
[32m[20221213 23:36:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.13
[32m[20221213 23:36:36 @agent_ppo2.py:143][0m Total time:      24.07 min
[32m[20221213 23:36:36 @agent_ppo2.py:145][0m 2328576 total steps have happened
[32m[20221213 23:36:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3137 --------------------------#
[32m[20221213 23:36:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:36 @agent_ppo2.py:185][0m |           0.0032 |          60.7961 |           8.7382 |
[32m[20221213 23:36:36 @agent_ppo2.py:185][0m |          -0.0053 |          56.8831 |           8.7970 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0106 |          54.9567 |           8.7304 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0132 |          53.5027 |           8.8120 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0145 |          52.4466 |           8.8016 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0121 |          51.8077 |           8.8296 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0160 |          51.2362 |           8.9058 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0086 |          51.8297 |           8.8412 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0171 |          50.3436 |           8.8965 |
[32m[20221213 23:36:37 @agent_ppo2.py:185][0m |          -0.0172 |          49.9451 |           8.8740 |
[32m[20221213 23:36:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.79
[32m[20221213 23:36:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.24
[32m[20221213 23:36:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.70
[32m[20221213 23:36:37 @agent_ppo2.py:143][0m Total time:      24.09 min
[32m[20221213 23:36:37 @agent_ppo2.py:145][0m 2330624 total steps have happened
[32m[20221213 23:36:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3138 --------------------------#
[32m[20221213 23:36:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0017 |          68.5077 |           9.2447 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0075 |          66.3415 |           9.2200 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |           0.0001 |          68.6314 |           9.2249 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0067 |          65.9490 |           9.1473 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0060 |          63.8313 |           9.1421 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0019 |          64.5860 |           9.0959 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0044 |          63.3986 |           9.0921 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0047 |          63.0220 |           8.9609 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0059 |          65.3070 |           9.0064 |
[32m[20221213 23:36:38 @agent_ppo2.py:185][0m |          -0.0113 |          61.6502 |           8.9601 |
[32m[20221213 23:36:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.38
[32m[20221213 23:36:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.45
[32m[20221213 23:36:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.94
[32m[20221213 23:36:39 @agent_ppo2.py:143][0m Total time:      24.11 min
[32m[20221213 23:36:39 @agent_ppo2.py:145][0m 2332672 total steps have happened
[32m[20221213 23:36:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3139 --------------------------#
[32m[20221213 23:36:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |          -0.0028 |          70.5973 |           8.4701 |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |          -0.0022 |          63.7823 |           8.5213 |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |          -0.0077 |          62.7587 |           8.5509 |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |          -0.0096 |          62.2206 |           8.5806 |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |          -0.0109 |          62.0300 |           8.6079 |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |           0.0003 |          64.9906 |           8.6166 |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |          -0.0100 |          61.5176 |           8.5549 |
[32m[20221213 23:36:39 @agent_ppo2.py:185][0m |          -0.0123 |          61.1060 |           8.6215 |
[32m[20221213 23:36:40 @agent_ppo2.py:185][0m |          -0.0104 |          61.7743 |           8.6564 |
[32m[20221213 23:36:40 @agent_ppo2.py:185][0m |          -0.0090 |          64.1250 |           8.7069 |
[32m[20221213 23:36:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.83
[32m[20221213 23:36:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.45
[32m[20221213 23:36:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.03
[32m[20221213 23:36:40 @agent_ppo2.py:143][0m Total time:      24.13 min
[32m[20221213 23:36:40 @agent_ppo2.py:145][0m 2334720 total steps have happened
[32m[20221213 23:36:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3140 --------------------------#
[32m[20221213 23:36:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:36:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:40 @agent_ppo2.py:185][0m |           0.0034 |          53.9665 |           8.7636 |
[32m[20221213 23:36:40 @agent_ppo2.py:185][0m |          -0.0067 |          47.8631 |           8.7455 |
[32m[20221213 23:36:40 @agent_ppo2.py:185][0m |          -0.0071 |          45.1202 |           8.8041 |
[32m[20221213 23:36:40 @agent_ppo2.py:185][0m |          -0.0106 |          44.1491 |           8.7376 |
[32m[20221213 23:36:40 @agent_ppo2.py:185][0m |          -0.0094 |          42.9504 |           8.7435 |
[32m[20221213 23:36:41 @agent_ppo2.py:185][0m |          -0.0083 |          42.1877 |           8.8335 |
[32m[20221213 23:36:41 @agent_ppo2.py:185][0m |          -0.0110 |          41.6409 |           8.8285 |
[32m[20221213 23:36:41 @agent_ppo2.py:185][0m |          -0.0148 |          40.9398 |           8.7960 |
[32m[20221213 23:36:41 @agent_ppo2.py:185][0m |          -0.0141 |          40.3791 |           8.8254 |
[32m[20221213 23:36:41 @agent_ppo2.py:185][0m |          -0.0157 |          40.0453 |           8.7612 |
[32m[20221213 23:36:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:36:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.97
[32m[20221213 23:36:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.30
[32m[20221213 23:36:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.75
[32m[20221213 23:36:41 @agent_ppo2.py:143][0m Total time:      24.15 min
[32m[20221213 23:36:41 @agent_ppo2.py:145][0m 2336768 total steps have happened
[32m[20221213 23:36:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3141 --------------------------#
[32m[20221213 23:36:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:41 @agent_ppo2.py:185][0m |           0.0064 |          63.6095 |           8.7503 |
[32m[20221213 23:36:41 @agent_ppo2.py:185][0m |          -0.0070 |          60.1362 |           8.7894 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |          -0.0083 |          59.3830 |           8.8181 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |           0.0012 |          60.7042 |           8.8575 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |          -0.0073 |          58.5156 |           8.8604 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |          -0.0051 |          58.4426 |           8.8536 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |          -0.0066 |          57.8568 |           8.8799 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |          -0.0081 |          58.2900 |           8.8873 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |          -0.0117 |          57.5122 |           8.9150 |
[32m[20221213 23:36:42 @agent_ppo2.py:185][0m |          -0.0161 |          57.4452 |           8.8643 |
[32m[20221213 23:36:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.41
[32m[20221213 23:36:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.00
[32m[20221213 23:36:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.12
[32m[20221213 23:36:42 @agent_ppo2.py:143][0m Total time:      24.17 min
[32m[20221213 23:36:42 @agent_ppo2.py:145][0m 2338816 total steps have happened
[32m[20221213 23:36:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3142 --------------------------#
[32m[20221213 23:36:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0015 |          63.9038 |           8.8236 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0034 |          63.0332 |           8.8392 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0099 |          61.8835 |           8.9064 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0090 |          61.4170 |           8.9004 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0089 |          60.9965 |           8.9275 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0131 |          60.8476 |           8.9176 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0119 |          60.5879 |           8.9032 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0139 |          60.3418 |           8.9332 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0135 |          60.1274 |           8.9565 |
[32m[20221213 23:36:43 @agent_ppo2.py:185][0m |          -0.0150 |          60.0448 |           8.9757 |
[32m[20221213 23:36:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:36:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.38
[32m[20221213 23:36:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.62
[32m[20221213 23:36:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.65
[32m[20221213 23:36:44 @agent_ppo2.py:143][0m Total time:      24.19 min
[32m[20221213 23:36:44 @agent_ppo2.py:145][0m 2340864 total steps have happened
[32m[20221213 23:36:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3143 --------------------------#
[32m[20221213 23:36:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |          -0.0005 |          62.6587 |           8.7906 |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |          -0.0017 |          60.1721 |           8.7823 |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |           0.0008 |          59.3151 |           8.8132 |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |          -0.0047 |          58.3217 |           8.8113 |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |          -0.0046 |          58.5091 |           8.7495 |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |          -0.0114 |          57.5393 |           8.7652 |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |          -0.0053 |          59.1658 |           8.7521 |
[32m[20221213 23:36:44 @agent_ppo2.py:185][0m |          -0.0098 |          56.9509 |           8.7588 |
[32m[20221213 23:36:45 @agent_ppo2.py:185][0m |          -0.0138 |          56.8276 |           8.7998 |
[32m[20221213 23:36:45 @agent_ppo2.py:185][0m |          -0.0150 |          56.8778 |           8.8099 |
[32m[20221213 23:36:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:36:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.48
[32m[20221213 23:36:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.34
[32m[20221213 23:36:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.14
[32m[20221213 23:36:45 @agent_ppo2.py:143][0m Total time:      24.21 min
[32m[20221213 23:36:45 @agent_ppo2.py:145][0m 2342912 total steps have happened
[32m[20221213 23:36:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3144 --------------------------#
[32m[20221213 23:36:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:45 @agent_ppo2.py:185][0m |           0.0035 |          38.1789 |           8.5818 |
[32m[20221213 23:36:45 @agent_ppo2.py:185][0m |          -0.0034 |          31.8204 |           8.5157 |
[32m[20221213 23:36:45 @agent_ppo2.py:185][0m |          -0.0072 |          29.6888 |           8.4842 |
[32m[20221213 23:36:45 @agent_ppo2.py:185][0m |          -0.0100 |          28.8783 |           8.5424 |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |          -0.0098 |          28.2065 |           8.4864 |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |          -0.0133 |          27.7696 |           8.4627 |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |          -0.0046 |          27.3485 |           8.5284 |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |          -0.0110 |          26.9698 |           8.4135 |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |          -0.0161 |          26.8285 |           8.4380 |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |          -0.0125 |          26.7158 |           8.3528 |
[32m[20221213 23:36:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.76
[32m[20221213 23:36:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.86
[32m[20221213 23:36:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.29
[32m[20221213 23:36:46 @agent_ppo2.py:143][0m Total time:      24.23 min
[32m[20221213 23:36:46 @agent_ppo2.py:145][0m 2344960 total steps have happened
[32m[20221213 23:36:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3145 --------------------------#
[32m[20221213 23:36:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:36:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |           0.0043 |          47.2685 |           8.5084 |
[32m[20221213 23:36:46 @agent_ppo2.py:185][0m |          -0.0036 |          41.3739 |           8.5755 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |          -0.0100 |          39.9132 |           8.5994 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |          -0.0099 |          39.0618 |           8.5555 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |          -0.0123 |          38.4452 |           8.5550 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |          -0.0125 |          38.1566 |           8.5411 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |          -0.0105 |          37.7683 |           8.5864 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |           0.0026 |          41.8231 |           8.5963 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |          -0.0107 |          37.3402 |           8.6232 |
[32m[20221213 23:36:47 @agent_ppo2.py:185][0m |          -0.0073 |          37.1254 |           8.5857 |
[32m[20221213 23:36:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:36:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.77
[32m[20221213 23:36:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.36
[32m[20221213 23:36:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.14
[32m[20221213 23:36:47 @agent_ppo2.py:143][0m Total time:      24.26 min
[32m[20221213 23:36:47 @agent_ppo2.py:145][0m 2347008 total steps have happened
[32m[20221213 23:36:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3146 --------------------------#
[32m[20221213 23:36:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |           0.0002 |          38.7154 |           8.9410 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0027 |          34.3702 |           8.9777 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0106 |          32.3624 |           9.0264 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0028 |          31.7818 |           9.0324 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0088 |          30.4815 |           8.9743 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0163 |          29.7846 |           8.9810 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0129 |          29.1151 |           9.0215 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0144 |          28.8143 |           9.0363 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0177 |          28.4804 |           9.0696 |
[32m[20221213 23:36:48 @agent_ppo2.py:185][0m |          -0.0179 |          28.3063 |           9.0542 |
[32m[20221213 23:36:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.83
[32m[20221213 23:36:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.29
[32m[20221213 23:36:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.97
[32m[20221213 23:36:49 @agent_ppo2.py:143][0m Total time:      24.28 min
[32m[20221213 23:36:49 @agent_ppo2.py:145][0m 2349056 total steps have happened
[32m[20221213 23:36:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3147 --------------------------#
[32m[20221213 23:36:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:49 @agent_ppo2.py:185][0m |           0.0060 |          68.9362 |           8.9856 |
[32m[20221213 23:36:49 @agent_ppo2.py:185][0m |          -0.0026 |          64.1205 |           8.9504 |
[32m[20221213 23:36:49 @agent_ppo2.py:185][0m |          -0.0040 |          62.3388 |           8.9462 |
[32m[20221213 23:36:49 @agent_ppo2.py:185][0m |          -0.0043 |          61.5387 |           8.8834 |
[32m[20221213 23:36:49 @agent_ppo2.py:185][0m |          -0.0054 |          60.8804 |           8.8863 |
[32m[20221213 23:36:49 @agent_ppo2.py:185][0m |          -0.0055 |          60.4761 |           8.9235 |
[32m[20221213 23:36:49 @agent_ppo2.py:185][0m |          -0.0060 |          60.7028 |           8.9127 |
[32m[20221213 23:36:50 @agent_ppo2.py:185][0m |          -0.0098 |          60.2107 |           8.8407 |
[32m[20221213 23:36:50 @agent_ppo2.py:185][0m |          -0.0116 |          59.8123 |           8.8354 |
[32m[20221213 23:36:50 @agent_ppo2.py:185][0m |          -0.0098 |          59.6157 |           8.8579 |
[32m[20221213 23:36:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.66
[32m[20221213 23:36:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.25
[32m[20221213 23:36:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.77
[32m[20221213 23:36:50 @agent_ppo2.py:143][0m Total time:      24.30 min
[32m[20221213 23:36:50 @agent_ppo2.py:145][0m 2351104 total steps have happened
[32m[20221213 23:36:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3148 --------------------------#
[32m[20221213 23:36:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:50 @agent_ppo2.py:185][0m |           0.0005 |          55.1805 |           8.5642 |
[32m[20221213 23:36:50 @agent_ppo2.py:185][0m |          -0.0027 |          50.7293 |           8.5972 |
[32m[20221213 23:36:50 @agent_ppo2.py:185][0m |          -0.0078 |          49.3492 |           8.6339 |
[32m[20221213 23:36:50 @agent_ppo2.py:185][0m |          -0.0097 |          48.7089 |           8.6128 |
[32m[20221213 23:36:51 @agent_ppo2.py:185][0m |          -0.0091 |          48.3594 |           8.5994 |
[32m[20221213 23:36:51 @agent_ppo2.py:185][0m |          -0.0092 |          48.0531 |           8.5855 |
[32m[20221213 23:36:51 @agent_ppo2.py:185][0m |          -0.0117 |          47.6858 |           8.5860 |
[32m[20221213 23:36:51 @agent_ppo2.py:185][0m |           0.0033 |          51.1871 |           8.5840 |
[32m[20221213 23:36:51 @agent_ppo2.py:185][0m |          -0.0044 |          47.1849 |           8.6844 |
[32m[20221213 23:36:51 @agent_ppo2.py:185][0m |          -0.0118 |          46.9870 |           8.6049 |
[32m[20221213 23:36:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:36:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.99
[32m[20221213 23:36:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.41
[32m[20221213 23:36:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.54
[32m[20221213 23:36:51 @agent_ppo2.py:143][0m Total time:      24.32 min
[32m[20221213 23:36:51 @agent_ppo2.py:145][0m 2353152 total steps have happened
[32m[20221213 23:36:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3149 --------------------------#
[32m[20221213 23:36:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:51 @agent_ppo2.py:185][0m |           0.0002 |          47.4652 |           8.2880 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0087 |          44.1809 |           8.3103 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0045 |          43.1826 |           8.3309 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0068 |          42.5973 |           8.3337 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0123 |          42.3705 |           8.3425 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0115 |          42.1009 |           8.3468 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0146 |          41.9710 |           8.3267 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0167 |          41.8693 |           8.3433 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0160 |          41.6754 |           8.3365 |
[32m[20221213 23:36:52 @agent_ppo2.py:185][0m |          -0.0152 |          41.6349 |           8.3741 |
[32m[20221213 23:36:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.51
[32m[20221213 23:36:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.81
[32m[20221213 23:36:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.06
[32m[20221213 23:36:52 @agent_ppo2.py:143][0m Total time:      24.34 min
[32m[20221213 23:36:52 @agent_ppo2.py:145][0m 2355200 total steps have happened
[32m[20221213 23:36:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3150 --------------------------#
[32m[20221213 23:36:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:36:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |           0.0007 |          59.1204 |           8.4789 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0053 |          55.4858 |           8.5190 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0041 |          54.9411 |           8.4962 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0066 |          54.1122 |           8.4591 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0075 |          53.5908 |           8.4879 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0072 |          53.4460 |           8.4119 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0085 |          53.1724 |           8.4153 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0114 |          52.8725 |           8.3731 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0005 |          59.7325 |           8.4116 |
[32m[20221213 23:36:53 @agent_ppo2.py:185][0m |          -0.0117 |          53.0324 |           8.3309 |
[32m[20221213 23:36:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:36:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.14
[32m[20221213 23:36:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.82
[32m[20221213 23:36:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.67
[32m[20221213 23:36:54 @agent_ppo2.py:143][0m Total time:      24.36 min
[32m[20221213 23:36:54 @agent_ppo2.py:145][0m 2357248 total steps have happened
[32m[20221213 23:36:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3151 --------------------------#
[32m[20221213 23:36:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:54 @agent_ppo2.py:185][0m |          -0.0020 |          61.3966 |           8.6000 |
[32m[20221213 23:36:54 @agent_ppo2.py:185][0m |           0.0009 |          59.0456 |           8.5935 |
[32m[20221213 23:36:54 @agent_ppo2.py:185][0m |          -0.0036 |          57.4682 |           8.6383 |
[32m[20221213 23:36:54 @agent_ppo2.py:185][0m |          -0.0102 |          57.1433 |           8.5732 |
[32m[20221213 23:36:54 @agent_ppo2.py:185][0m |           0.0001 |          58.9889 |           8.6068 |
[32m[20221213 23:36:54 @agent_ppo2.py:185][0m |          -0.0105 |          56.2224 |           8.5965 |
[32m[20221213 23:36:54 @agent_ppo2.py:185][0m |          -0.0065 |          55.6227 |           8.5807 |
[32m[20221213 23:36:55 @agent_ppo2.py:185][0m |          -0.0097 |          55.5021 |           8.6094 |
[32m[20221213 23:36:55 @agent_ppo2.py:185][0m |          -0.0126 |          55.4066 |           8.5485 |
[32m[20221213 23:36:55 @agent_ppo2.py:185][0m |          -0.0120 |          54.9761 |           8.6803 |
[32m[20221213 23:36:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.07
[32m[20221213 23:36:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.69
[32m[20221213 23:36:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.08
[32m[20221213 23:36:55 @agent_ppo2.py:143][0m Total time:      24.38 min
[32m[20221213 23:36:55 @agent_ppo2.py:145][0m 2359296 total steps have happened
[32m[20221213 23:36:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3152 --------------------------#
[32m[20221213 23:36:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:55 @agent_ppo2.py:185][0m |           0.0006 |          65.6800 |           8.0545 |
[32m[20221213 23:36:55 @agent_ppo2.py:185][0m |          -0.0061 |          62.7367 |           8.0526 |
[32m[20221213 23:36:55 @agent_ppo2.py:185][0m |          -0.0054 |          61.7818 |           8.0334 |
[32m[20221213 23:36:55 @agent_ppo2.py:185][0m |          -0.0079 |          61.2748 |           8.0396 |
[32m[20221213 23:36:56 @agent_ppo2.py:185][0m |          -0.0086 |          60.9949 |           8.0006 |
[32m[20221213 23:36:56 @agent_ppo2.py:185][0m |          -0.0098 |          60.7194 |           8.0723 |
[32m[20221213 23:36:56 @agent_ppo2.py:185][0m |          -0.0084 |          60.2889 |           8.0758 |
[32m[20221213 23:36:56 @agent_ppo2.py:185][0m |          -0.0108 |          60.2354 |           8.0661 |
[32m[20221213 23:36:56 @agent_ppo2.py:185][0m |          -0.0111 |          59.8731 |           8.0493 |
[32m[20221213 23:36:56 @agent_ppo2.py:185][0m |          -0.0104 |          59.7172 |           8.1097 |
[32m[20221213 23:36:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.80
[32m[20221213 23:36:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.88
[32m[20221213 23:36:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.71
[32m[20221213 23:36:56 @agent_ppo2.py:143][0m Total time:      24.40 min
[32m[20221213 23:36:56 @agent_ppo2.py:145][0m 2361344 total steps have happened
[32m[20221213 23:36:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3153 --------------------------#
[32m[20221213 23:36:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:56 @agent_ppo2.py:185][0m |           0.0039 |          68.8760 |           8.2567 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0030 |          67.2875 |           8.1857 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0068 |          66.7862 |           8.2790 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0083 |          66.5562 |           8.2529 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0087 |          66.2875 |           8.2357 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0094 |          65.8449 |           8.2708 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0085 |          65.7068 |           8.2383 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0120 |          65.8923 |           8.2690 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0025 |          69.0752 |           8.2649 |
[32m[20221213 23:36:57 @agent_ppo2.py:185][0m |          -0.0126 |          65.5694 |           8.2785 |
[32m[20221213 23:36:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:36:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.13
[32m[20221213 23:36:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.63
[32m[20221213 23:36:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.67
[32m[20221213 23:36:57 @agent_ppo2.py:143][0m Total time:      24.42 min
[32m[20221213 23:36:57 @agent_ppo2.py:145][0m 2363392 total steps have happened
[32m[20221213 23:36:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3154 --------------------------#
[32m[20221213 23:36:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |           0.0069 |          57.7147 |           8.3133 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0044 |          50.2587 |           8.3553 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0082 |          48.5490 |           8.3200 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0076 |          47.6218 |           8.3389 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0093 |          47.4992 |           8.3893 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0025 |          52.1393 |           8.4103 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0124 |          46.3416 |           8.3999 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0145 |          45.4946 |           8.4007 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0042 |          47.1003 |           8.3763 |
[32m[20221213 23:36:58 @agent_ppo2.py:185][0m |          -0.0171 |          44.8805 |           8.4454 |
[32m[20221213 23:36:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:36:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.97
[32m[20221213 23:36:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.74
[32m[20221213 23:36:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.96
[32m[20221213 23:36:59 @agent_ppo2.py:143][0m Total time:      24.44 min
[32m[20221213 23:36:59 @agent_ppo2.py:145][0m 2365440 total steps have happened
[32m[20221213 23:36:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3155 --------------------------#
[32m[20221213 23:36:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:36:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:36:59 @agent_ppo2.py:185][0m |           0.0005 |          58.1845 |           8.5137 |
[32m[20221213 23:36:59 @agent_ppo2.py:185][0m |          -0.0093 |          53.4425 |           8.4994 |
[32m[20221213 23:36:59 @agent_ppo2.py:185][0m |          -0.0079 |          53.5471 |           8.4705 |
[32m[20221213 23:36:59 @agent_ppo2.py:185][0m |          -0.0144 |          51.1092 |           8.4558 |
[32m[20221213 23:36:59 @agent_ppo2.py:185][0m |          -0.0106 |          50.3642 |           8.4148 |
[32m[20221213 23:36:59 @agent_ppo2.py:185][0m |          -0.0065 |          53.7157 |           8.4269 |
[32m[20221213 23:36:59 @agent_ppo2.py:185][0m |          -0.0174 |          49.4039 |           8.4899 |
[32m[20221213 23:37:00 @agent_ppo2.py:185][0m |          -0.0172 |          48.8461 |           8.4376 |
[32m[20221213 23:37:00 @agent_ppo2.py:185][0m |          -0.0136 |          48.7621 |           8.4600 |
[32m[20221213 23:37:00 @agent_ppo2.py:185][0m |          -0.0210 |          48.5073 |           8.4223 |
[32m[20221213 23:37:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:37:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.86
[32m[20221213 23:37:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.67
[32m[20221213 23:37:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.52
[32m[20221213 23:37:00 @agent_ppo2.py:143][0m Total time:      24.46 min
[32m[20221213 23:37:00 @agent_ppo2.py:145][0m 2367488 total steps have happened
[32m[20221213 23:37:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3156 --------------------------#
[32m[20221213 23:37:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:00 @agent_ppo2.py:185][0m |          -0.0012 |          43.9247 |           7.5721 |
[32m[20221213 23:37:00 @agent_ppo2.py:185][0m |           0.0023 |          40.7668 |           7.6117 |
[32m[20221213 23:37:00 @agent_ppo2.py:185][0m |          -0.0050 |          39.6126 |           7.6435 |
[32m[20221213 23:37:00 @agent_ppo2.py:185][0m |          -0.0054 |          38.7483 |           7.6501 |
[32m[20221213 23:37:01 @agent_ppo2.py:185][0m |          -0.0069 |          38.2991 |           7.6836 |
[32m[20221213 23:37:01 @agent_ppo2.py:185][0m |          -0.0103 |          37.8062 |           7.6808 |
[32m[20221213 23:37:01 @agent_ppo2.py:185][0m |          -0.0102 |          37.6647 |           7.6870 |
[32m[20221213 23:37:01 @agent_ppo2.py:185][0m |          -0.0112 |          37.3159 |           7.7446 |
[32m[20221213 23:37:01 @agent_ppo2.py:185][0m |          -0.0079 |          36.8850 |           7.7570 |
[32m[20221213 23:37:01 @agent_ppo2.py:185][0m |          -0.0137 |          36.8695 |           7.8074 |
[32m[20221213 23:37:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.69
[32m[20221213 23:37:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.02
[32m[20221213 23:37:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.84
[32m[20221213 23:37:01 @agent_ppo2.py:143][0m Total time:      24.49 min
[32m[20221213 23:37:01 @agent_ppo2.py:145][0m 2369536 total steps have happened
[32m[20221213 23:37:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3157 --------------------------#
[32m[20221213 23:37:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:01 @agent_ppo2.py:185][0m |           0.0003 |          54.7541 |           8.7265 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0071 |          51.3547 |           8.7253 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |           0.0026 |          51.4515 |           8.6846 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0093 |          50.1794 |           8.6562 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0071 |          49.7524 |           8.6092 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0109 |          49.6110 |           8.6352 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0103 |          49.4402 |           8.6751 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0099 |          49.2719 |           8.6185 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0117 |          49.2141 |           8.5940 |
[32m[20221213 23:37:02 @agent_ppo2.py:185][0m |          -0.0097 |          49.1538 |           8.5433 |
[32m[20221213 23:37:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.00
[32m[20221213 23:37:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.33
[32m[20221213 23:37:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.95
[32m[20221213 23:37:02 @agent_ppo2.py:143][0m Total time:      24.51 min
[32m[20221213 23:37:02 @agent_ppo2.py:145][0m 2371584 total steps have happened
[32m[20221213 23:37:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3158 --------------------------#
[32m[20221213 23:37:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |           0.0001 |          60.3117 |           8.4161 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0064 |          58.0826 |           8.3621 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0034 |          58.2629 |           8.3125 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0116 |          57.4267 |           8.3820 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0088 |          57.1413 |           8.3162 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |           0.0001 |          61.7113 |           8.2782 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0125 |          56.8882 |           8.3295 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0084 |          56.6243 |           8.2986 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0147 |          56.5353 |           8.3115 |
[32m[20221213 23:37:03 @agent_ppo2.py:185][0m |          -0.0093 |          56.5226 |           8.3344 |
[32m[20221213 23:37:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:37:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.16
[32m[20221213 23:37:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.96
[32m[20221213 23:37:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.66
[32m[20221213 23:37:04 @agent_ppo2.py:143][0m Total time:      24.53 min
[32m[20221213 23:37:04 @agent_ppo2.py:145][0m 2373632 total steps have happened
[32m[20221213 23:37:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3159 --------------------------#
[32m[20221213 23:37:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:04 @agent_ppo2.py:185][0m |           0.0040 |          48.8397 |           8.0429 |
[32m[20221213 23:37:04 @agent_ppo2.py:185][0m |          -0.0075 |          45.2583 |           8.1560 |
[32m[20221213 23:37:04 @agent_ppo2.py:185][0m |          -0.0067 |          44.2136 |           8.1906 |
[32m[20221213 23:37:04 @agent_ppo2.py:185][0m |          -0.0087 |          43.4290 |           8.2183 |
[32m[20221213 23:37:04 @agent_ppo2.py:185][0m |          -0.0087 |          42.9327 |           8.1980 |
[32m[20221213 23:37:04 @agent_ppo2.py:185][0m |          -0.0158 |          42.6288 |           8.3164 |
[32m[20221213 23:37:04 @agent_ppo2.py:185][0m |          -0.0147 |          42.4130 |           8.3254 |
[32m[20221213 23:37:05 @agent_ppo2.py:185][0m |          -0.0167 |          41.9903 |           8.3085 |
[32m[20221213 23:37:05 @agent_ppo2.py:185][0m |          -0.0159 |          41.7505 |           8.3368 |
[32m[20221213 23:37:05 @agent_ppo2.py:185][0m |          -0.0169 |          41.5718 |           8.3018 |
[32m[20221213 23:37:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.28
[32m[20221213 23:37:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.90
[32m[20221213 23:37:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.59
[32m[20221213 23:37:05 @agent_ppo2.py:143][0m Total time:      24.55 min
[32m[20221213 23:37:05 @agent_ppo2.py:145][0m 2375680 total steps have happened
[32m[20221213 23:37:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3160 --------------------------#
[32m[20221213 23:37:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:37:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:05 @agent_ppo2.py:185][0m |           0.0070 |          44.6256 |           8.6516 |
[32m[20221213 23:37:05 @agent_ppo2.py:185][0m |          -0.0035 |          41.0170 |           8.5519 |
[32m[20221213 23:37:05 @agent_ppo2.py:185][0m |          -0.0024 |          40.0558 |           8.6186 |
[32m[20221213 23:37:05 @agent_ppo2.py:185][0m |          -0.0063 |          39.1291 |           8.5619 |
[32m[20221213 23:37:06 @agent_ppo2.py:185][0m |          -0.0092 |          38.4953 |           8.5067 |
[32m[20221213 23:37:06 @agent_ppo2.py:185][0m |          -0.0123 |          37.9784 |           8.5755 |
[32m[20221213 23:37:06 @agent_ppo2.py:185][0m |          -0.0090 |          37.5647 |           8.5700 |
[32m[20221213 23:37:06 @agent_ppo2.py:185][0m |          -0.0135 |          37.2662 |           8.5213 |
[32m[20221213 23:37:06 @agent_ppo2.py:185][0m |          -0.0128 |          36.8639 |           8.5146 |
[32m[20221213 23:37:06 @agent_ppo2.py:185][0m |          -0.0114 |          36.5197 |           8.5037 |
[32m[20221213 23:37:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.51
[32m[20221213 23:37:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.53
[32m[20221213 23:37:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.44
[32m[20221213 23:37:06 @agent_ppo2.py:143][0m Total time:      24.57 min
[32m[20221213 23:37:06 @agent_ppo2.py:145][0m 2377728 total steps have happened
[32m[20221213 23:37:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3161 --------------------------#
[32m[20221213 23:37:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:06 @agent_ppo2.py:185][0m |           0.0019 |          59.3523 |           8.6253 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0040 |          56.2717 |           8.5649 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0096 |          55.3266 |           8.5394 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0099 |          54.7790 |           8.4870 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0071 |          54.1155 |           8.4978 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0108 |          53.7450 |           8.4936 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0069 |          53.3643 |           8.4687 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |           0.0034 |          59.5623 |           8.4603 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0088 |          53.1125 |           8.4715 |
[32m[20221213 23:37:07 @agent_ppo2.py:185][0m |          -0.0133 |          52.5977 |           8.4514 |
[32m[20221213 23:37:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.73
[32m[20221213 23:37:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.25
[32m[20221213 23:37:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.48
[32m[20221213 23:37:07 @agent_ppo2.py:143][0m Total time:      24.59 min
[32m[20221213 23:37:07 @agent_ppo2.py:145][0m 2379776 total steps have happened
[32m[20221213 23:37:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3162 --------------------------#
[32m[20221213 23:37:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0026 |          56.5341 |           7.8682 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0036 |          50.0975 |           7.8253 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0068 |          47.3499 |           7.8906 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |           0.0075 |          50.4288 |           7.8213 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0121 |          44.3436 |           7.7936 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0087 |          43.2310 |           7.7906 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0075 |          42.4848 |           7.7652 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0086 |          41.7067 |           7.7258 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0119 |          41.2663 |           7.6799 |
[32m[20221213 23:37:08 @agent_ppo2.py:185][0m |          -0.0161 |          40.5724 |           7.6964 |
[32m[20221213 23:37:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:37:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.51
[32m[20221213 23:37:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.61
[32m[20221213 23:37:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.65
[32m[20221213 23:37:09 @agent_ppo2.py:143][0m Total time:      24.61 min
[32m[20221213 23:37:09 @agent_ppo2.py:145][0m 2381824 total steps have happened
[32m[20221213 23:37:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3163 --------------------------#
[32m[20221213 23:37:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:09 @agent_ppo2.py:185][0m |           0.0037 |          50.1516 |           7.9012 |
[32m[20221213 23:37:09 @agent_ppo2.py:185][0m |           0.0051 |          44.5085 |           7.9333 |
[32m[20221213 23:37:09 @agent_ppo2.py:185][0m |          -0.0083 |          40.9493 |           7.9677 |
[32m[20221213 23:37:09 @agent_ppo2.py:185][0m |          -0.0164 |          39.9983 |           7.9607 |
[32m[20221213 23:37:09 @agent_ppo2.py:185][0m |          -0.0150 |          39.5384 |           7.9378 |
[32m[20221213 23:37:09 @agent_ppo2.py:185][0m |           0.0012 |          43.4901 |           7.9363 |
[32m[20221213 23:37:09 @agent_ppo2.py:185][0m |          -0.0128 |          38.8061 |           7.9453 |
[32m[20221213 23:37:10 @agent_ppo2.py:185][0m |          -0.0147 |          38.4933 |           7.9147 |
[32m[20221213 23:37:10 @agent_ppo2.py:185][0m |          -0.0097 |          38.8789 |           7.8876 |
[32m[20221213 23:37:10 @agent_ppo2.py:185][0m |          -0.0011 |          42.1954 |           7.8865 |
[32m[20221213 23:37:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:37:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.48
[32m[20221213 23:37:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.01
[32m[20221213 23:37:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.27
[32m[20221213 23:37:10 @agent_ppo2.py:143][0m Total time:      24.63 min
[32m[20221213 23:37:10 @agent_ppo2.py:145][0m 2383872 total steps have happened
[32m[20221213 23:37:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3164 --------------------------#
[32m[20221213 23:37:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:37:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:10 @agent_ppo2.py:185][0m |           0.0025 |          19.8691 |           7.4490 |
[32m[20221213 23:37:10 @agent_ppo2.py:185][0m |          -0.0022 |          16.4485 |           7.3892 |
[32m[20221213 23:37:10 @agent_ppo2.py:185][0m |          -0.0018 |          16.2830 |           7.4128 |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |           0.0096 |          16.7338 |           7.3670 |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |          -0.0034 |          16.1864 |           7.3616 |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |           0.0083 |          17.2201 |           7.4095 |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |          -0.0054 |          16.1109 |           7.3801 |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |          -0.0026 |          16.0844 |           7.3251 |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |          -0.0054 |          16.0658 |           7.2782 |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |          -0.0019 |          16.1673 |           7.2713 |
[32m[20221213 23:37:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:37:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:37:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:37:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.06
[32m[20221213 23:37:11 @agent_ppo2.py:143][0m Total time:      24.65 min
[32m[20221213 23:37:11 @agent_ppo2.py:145][0m 2385920 total steps have happened
[32m[20221213 23:37:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3165 --------------------------#
[32m[20221213 23:37:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:11 @agent_ppo2.py:185][0m |          -0.0016 |          55.6944 |           7.8186 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |           0.0030 |          51.4156 |           7.8007 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0065 |          49.8660 |           7.7993 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0094 |          48.5285 |           7.8329 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0126 |          47.5781 |           7.7705 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0117 |          46.9974 |           7.7390 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0147 |          46.8390 |           7.8460 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0118 |          46.3957 |           7.7267 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0176 |          45.8131 |           7.7419 |
[32m[20221213 23:37:12 @agent_ppo2.py:185][0m |          -0.0131 |          45.4254 |           7.6795 |
[32m[20221213 23:37:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.15
[32m[20221213 23:37:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.27
[32m[20221213 23:37:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.06
[32m[20221213 23:37:12 @agent_ppo2.py:143][0m Total time:      24.67 min
[32m[20221213 23:37:12 @agent_ppo2.py:145][0m 2387968 total steps have happened
[32m[20221213 23:37:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3166 --------------------------#
[32m[20221213 23:37:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |           0.0006 |          57.2122 |           7.6371 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |           0.0032 |          53.5510 |           7.5471 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0035 |          51.3416 |           7.5792 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0091 |          50.5427 |           7.5750 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0111 |          49.9754 |           7.5508 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0116 |          48.8377 |           7.5713 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0029 |          55.9368 |           7.5490 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0116 |          47.8283 |           7.5792 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0131 |          47.4941 |           7.5002 |
[32m[20221213 23:37:13 @agent_ppo2.py:185][0m |          -0.0098 |          47.1297 |           7.5559 |
[32m[20221213 23:37:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.83
[32m[20221213 23:37:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.77
[32m[20221213 23:37:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.46
[32m[20221213 23:37:14 @agent_ppo2.py:143][0m Total time:      24.69 min
[32m[20221213 23:37:14 @agent_ppo2.py:145][0m 2390016 total steps have happened
[32m[20221213 23:37:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3167 --------------------------#
[32m[20221213 23:37:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:14 @agent_ppo2.py:185][0m |           0.0006 |          62.6950 |           7.3628 |
[32m[20221213 23:37:14 @agent_ppo2.py:185][0m |          -0.0047 |          59.7513 |           7.3554 |
[32m[20221213 23:37:14 @agent_ppo2.py:185][0m |          -0.0071 |          58.5581 |           7.3432 |
[32m[20221213 23:37:14 @agent_ppo2.py:185][0m |          -0.0078 |          57.5626 |           7.3557 |
[32m[20221213 23:37:14 @agent_ppo2.py:185][0m |          -0.0083 |          57.0773 |           7.4060 |
[32m[20221213 23:37:14 @agent_ppo2.py:185][0m |          -0.0104 |          56.3611 |           7.3094 |
[32m[20221213 23:37:15 @agent_ppo2.py:185][0m |          -0.0097 |          55.8104 |           7.3171 |
[32m[20221213 23:37:15 @agent_ppo2.py:185][0m |          -0.0135 |          55.3185 |           7.2596 |
[32m[20221213 23:37:15 @agent_ppo2.py:185][0m |          -0.0078 |          54.9227 |           7.3013 |
[32m[20221213 23:37:15 @agent_ppo2.py:185][0m |          -0.0094 |          54.7165 |           7.2246 |
[32m[20221213 23:37:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:37:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.03
[32m[20221213 23:37:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.96
[32m[20221213 23:37:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.36
[32m[20221213 23:37:15 @agent_ppo2.py:143][0m Total time:      24.72 min
[32m[20221213 23:37:15 @agent_ppo2.py:145][0m 2392064 total steps have happened
[32m[20221213 23:37:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3168 --------------------------#
[32m[20221213 23:37:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:15 @agent_ppo2.py:185][0m |           0.0055 |          63.0564 |           7.3318 |
[32m[20221213 23:37:15 @agent_ppo2.py:185][0m |          -0.0039 |          58.7406 |           7.3651 |
[32m[20221213 23:37:15 @agent_ppo2.py:185][0m |          -0.0095 |          57.5896 |           7.3612 |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |          -0.0083 |          56.7256 |           7.3451 |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |          -0.0012 |          57.0407 |           7.3989 |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |          -0.0058 |          55.4958 |           7.3829 |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |          -0.0123 |          55.3103 |           7.3652 |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |          -0.0115 |          54.7589 |           7.3852 |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |          -0.0090 |          54.5139 |           7.3900 |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |          -0.0042 |          57.4061 |           7.3754 |
[32m[20221213 23:37:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.87
[32m[20221213 23:37:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.79
[32m[20221213 23:37:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.30
[32m[20221213 23:37:16 @agent_ppo2.py:143][0m Total time:      24.74 min
[32m[20221213 23:37:16 @agent_ppo2.py:145][0m 2394112 total steps have happened
[32m[20221213 23:37:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3169 --------------------------#
[32m[20221213 23:37:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:16 @agent_ppo2.py:185][0m |           0.0025 |          63.3318 |           7.6573 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0011 |          58.2076 |           7.6895 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0030 |          56.8776 |           7.7087 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0099 |          56.3145 |           7.6323 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0092 |          55.9748 |           7.6021 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0087 |          55.5070 |           7.6431 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0065 |          55.3823 |           7.6128 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0120 |          55.1095 |           7.5966 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0098 |          54.8037 |           7.6184 |
[32m[20221213 23:37:17 @agent_ppo2.py:185][0m |          -0.0087 |          54.6026 |           7.6154 |
[32m[20221213 23:37:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:37:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.65
[32m[20221213 23:37:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.09
[32m[20221213 23:37:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.79
[32m[20221213 23:37:17 @agent_ppo2.py:143][0m Total time:      24.76 min
[32m[20221213 23:37:17 @agent_ppo2.py:145][0m 2396160 total steps have happened
[32m[20221213 23:37:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3170 --------------------------#
[32m[20221213 23:37:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:37:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0020 |          63.3177 |           7.4439 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0038 |          61.2055 |           7.4259 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0049 |          59.8042 |           7.3844 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0053 |          59.0744 |           7.3678 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0106 |          58.3069 |           7.4757 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0094 |          57.6325 |           7.4616 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0092 |          57.2968 |           7.4957 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0091 |          56.6031 |           7.4426 |
[32m[20221213 23:37:18 @agent_ppo2.py:185][0m |          -0.0107 |          56.3327 |           7.4935 |
[32m[20221213 23:37:19 @agent_ppo2.py:185][0m |          -0.0060 |          57.0538 |           7.5570 |
[32m[20221213 23:37:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:37:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.74
[32m[20221213 23:37:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.21
[32m[20221213 23:37:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.04
[32m[20221213 23:37:19 @agent_ppo2.py:143][0m Total time:      24.78 min
[32m[20221213 23:37:19 @agent_ppo2.py:145][0m 2398208 total steps have happened
[32m[20221213 23:37:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3171 --------------------------#
[32m[20221213 23:37:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:19 @agent_ppo2.py:185][0m |          -0.0017 |          58.6653 |           7.4772 |
[32m[20221213 23:37:19 @agent_ppo2.py:185][0m |          -0.0034 |          52.9907 |           7.4731 |
[32m[20221213 23:37:19 @agent_ppo2.py:185][0m |          -0.0026 |          51.8156 |           7.3997 |
[32m[20221213 23:37:19 @agent_ppo2.py:185][0m |          -0.0017 |          52.9542 |           7.3431 |
[32m[20221213 23:37:19 @agent_ppo2.py:185][0m |          -0.0092 |          49.8980 |           7.4093 |
[32m[20221213 23:37:19 @agent_ppo2.py:185][0m |          -0.0119 |          49.3470 |           7.3453 |
[32m[20221213 23:37:20 @agent_ppo2.py:185][0m |          -0.0097 |          49.3048 |           7.3042 |
[32m[20221213 23:37:20 @agent_ppo2.py:185][0m |          -0.0122 |          48.7925 |           7.2784 |
[32m[20221213 23:37:20 @agent_ppo2.py:185][0m |          -0.0126 |          48.2652 |           7.2337 |
[32m[20221213 23:37:20 @agent_ppo2.py:185][0m |          -0.0107 |          48.7824 |           7.2166 |
[32m[20221213 23:37:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.26
[32m[20221213 23:37:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.89
[32m[20221213 23:37:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.45
[32m[20221213 23:37:20 @agent_ppo2.py:143][0m Total time:      24.80 min
[32m[20221213 23:37:20 @agent_ppo2.py:145][0m 2400256 total steps have happened
[32m[20221213 23:37:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3172 --------------------------#
[32m[20221213 23:37:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:20 @agent_ppo2.py:185][0m |           0.0054 |          47.2439 |           7.2822 |
[32m[20221213 23:37:20 @agent_ppo2.py:185][0m |          -0.0057 |          42.3521 |           7.3874 |
[32m[20221213 23:37:20 @agent_ppo2.py:185][0m |          -0.0037 |          41.0444 |           7.3565 |
[32m[20221213 23:37:21 @agent_ppo2.py:185][0m |          -0.0070 |          40.5932 |           7.4318 |
[32m[20221213 23:37:21 @agent_ppo2.py:185][0m |          -0.0080 |          39.9839 |           7.3956 |
[32m[20221213 23:37:21 @agent_ppo2.py:185][0m |           0.0005 |          42.2522 |           7.3843 |
[32m[20221213 23:37:21 @agent_ppo2.py:185][0m |          -0.0151 |          39.4390 |           7.3956 |
[32m[20221213 23:37:21 @agent_ppo2.py:185][0m |          -0.0161 |          39.2632 |           7.4336 |
[32m[20221213 23:37:21 @agent_ppo2.py:185][0m |          -0.0117 |          39.2761 |           7.4087 |
[32m[20221213 23:37:21 @agent_ppo2.py:185][0m |          -0.0205 |          38.8466 |           7.4096 |
[32m[20221213 23:37:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:37:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.07
[32m[20221213 23:37:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.38
[32m[20221213 23:37:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.16
[32m[20221213 23:37:21 @agent_ppo2.py:143][0m Total time:      24.82 min
[32m[20221213 23:37:21 @agent_ppo2.py:145][0m 2402304 total steps have happened
[32m[20221213 23:37:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3173 --------------------------#
[32m[20221213 23:37:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |           0.0026 |          57.8146 |           7.4479 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0040 |          53.5941 |           7.5094 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |           0.0039 |          54.1336 |           7.4650 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0053 |          51.9495 |           7.4981 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0081 |          50.9452 |           7.4833 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0028 |          50.5495 |           7.4407 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0085 |          50.1869 |           7.4047 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0117 |          49.9673 |           7.3594 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0162 |          49.7579 |           7.3118 |
[32m[20221213 23:37:22 @agent_ppo2.py:185][0m |          -0.0144 |          49.2480 |           7.3535 |
[32m[20221213 23:37:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.52
[32m[20221213 23:37:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.20
[32m[20221213 23:37:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.10
[32m[20221213 23:37:22 @agent_ppo2.py:143][0m Total time:      24.84 min
[32m[20221213 23:37:22 @agent_ppo2.py:145][0m 2404352 total steps have happened
[32m[20221213 23:37:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3174 --------------------------#
[32m[20221213 23:37:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |           0.0025 |          72.3397 |           6.8425 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0068 |          66.7750 |           6.7238 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0043 |          65.4049 |           6.7052 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0094 |          63.6986 |           6.6573 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0064 |          63.7883 |           6.6006 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0128 |          62.5181 |           6.6376 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0102 |          62.8523 |           6.6650 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0113 |          61.3886 |           6.5345 |
[32m[20221213 23:37:23 @agent_ppo2.py:185][0m |          -0.0149 |          61.0700 |           6.5948 |
[32m[20221213 23:37:24 @agent_ppo2.py:185][0m |          -0.0019 |          62.2938 |           6.5678 |
[32m[20221213 23:37:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.77
[32m[20221213 23:37:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.08
[32m[20221213 23:37:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.41
[32m[20221213 23:37:24 @agent_ppo2.py:143][0m Total time:      24.86 min
[32m[20221213 23:37:24 @agent_ppo2.py:145][0m 2406400 total steps have happened
[32m[20221213 23:37:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3175 --------------------------#
[32m[20221213 23:37:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:24 @agent_ppo2.py:185][0m |           0.0037 |          63.1428 |           6.7665 |
[32m[20221213 23:37:24 @agent_ppo2.py:185][0m |          -0.0123 |          55.7751 |           6.8457 |
[32m[20221213 23:37:24 @agent_ppo2.py:185][0m |          -0.0104 |          53.8565 |           6.8742 |
[32m[20221213 23:37:24 @agent_ppo2.py:185][0m |          -0.0147 |          52.9622 |           6.9244 |
[32m[20221213 23:37:24 @agent_ppo2.py:185][0m |          -0.0190 |          52.2008 |           6.9287 |
[32m[20221213 23:37:24 @agent_ppo2.py:185][0m |          -0.0191 |          51.7315 |           6.9788 |
[32m[20221213 23:37:25 @agent_ppo2.py:185][0m |          -0.0134 |          51.1992 |           6.9822 |
[32m[20221213 23:37:25 @agent_ppo2.py:185][0m |          -0.0224 |          50.6843 |           6.9805 |
[32m[20221213 23:37:25 @agent_ppo2.py:185][0m |          -0.0154 |          50.3919 |           7.0665 |
[32m[20221213 23:37:25 @agent_ppo2.py:185][0m |          -0.0148 |          50.1839 |           7.0710 |
[32m[20221213 23:37:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.65
[32m[20221213 23:37:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.99
[32m[20221213 23:37:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.34
[32m[20221213 23:37:25 @agent_ppo2.py:143][0m Total time:      24.88 min
[32m[20221213 23:37:25 @agent_ppo2.py:145][0m 2408448 total steps have happened
[32m[20221213 23:37:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3176 --------------------------#
[32m[20221213 23:37:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:25 @agent_ppo2.py:185][0m |           0.0017 |          59.5819 |           6.4006 |
[32m[20221213 23:37:25 @agent_ppo2.py:185][0m |          -0.0060 |          53.0064 |           6.4252 |
[32m[20221213 23:37:25 @agent_ppo2.py:185][0m |          -0.0055 |          50.7543 |           6.3020 |
[32m[20221213 23:37:26 @agent_ppo2.py:185][0m |          -0.0090 |          49.0860 |           6.3253 |
[32m[20221213 23:37:26 @agent_ppo2.py:185][0m |          -0.0091 |          47.8546 |           6.2801 |
[32m[20221213 23:37:26 @agent_ppo2.py:185][0m |          -0.0121 |          47.0340 |           6.3138 |
[32m[20221213 23:37:26 @agent_ppo2.py:185][0m |          -0.0095 |          47.0772 |           6.3791 |
[32m[20221213 23:37:26 @agent_ppo2.py:185][0m |          -0.0136 |          46.0939 |           6.3408 |
[32m[20221213 23:37:26 @agent_ppo2.py:185][0m |          -0.0062 |          49.4273 |           6.3328 |
[32m[20221213 23:37:26 @agent_ppo2.py:185][0m |          -0.0090 |          45.0860 |           6.3481 |
[32m[20221213 23:37:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.48
[32m[20221213 23:37:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.48
[32m[20221213 23:37:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.02
[32m[20221213 23:37:26 @agent_ppo2.py:143][0m Total time:      24.90 min
[32m[20221213 23:37:26 @agent_ppo2.py:145][0m 2410496 total steps have happened
[32m[20221213 23:37:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3177 --------------------------#
[32m[20221213 23:37:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |           0.0050 |          46.3825 |           7.1894 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |           0.0036 |          44.1335 |           7.2178 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |          -0.0095 |          40.4243 |           7.2091 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |           0.0006 |          39.3749 |           7.2113 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |          -0.0116 |          38.5842 |           7.2109 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |          -0.0134 |          37.8740 |           7.2732 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |          -0.0150 |          37.4914 |           7.1940 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |          -0.0157 |          37.2304 |           7.1793 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |          -0.0142 |          36.9348 |           7.1697 |
[32m[20221213 23:37:27 @agent_ppo2.py:185][0m |          -0.0133 |          36.3600 |           7.1162 |
[32m[20221213 23:37:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:37:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.98
[32m[20221213 23:37:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.70
[32m[20221213 23:37:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.99
[32m[20221213 23:37:27 @agent_ppo2.py:143][0m Total time:      24.92 min
[32m[20221213 23:37:27 @agent_ppo2.py:145][0m 2412544 total steps have happened
[32m[20221213 23:37:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3178 --------------------------#
[32m[20221213 23:37:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |           0.0060 |          62.4697 |           6.8042 |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |          -0.0029 |          59.2947 |           6.7201 |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |          -0.0125 |          58.1232 |           6.7866 |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |          -0.0002 |          64.2293 |           6.8082 |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |          -0.0081 |          57.4836 |           6.8244 |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |          -0.0008 |          57.8034 |           6.8228 |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |           0.0017 |          61.6651 |           6.8548 |
[32m[20221213 23:37:28 @agent_ppo2.py:185][0m |          -0.0111 |          55.6932 |           6.8420 |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |          -0.0122 |          55.3960 |           6.8886 |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |          -0.0119 |          55.3197 |           6.8806 |
[32m[20221213 23:37:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.27
[32m[20221213 23:37:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.75
[32m[20221213 23:37:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.14
[32m[20221213 23:37:29 @agent_ppo2.py:143][0m Total time:      24.95 min
[32m[20221213 23:37:29 @agent_ppo2.py:145][0m 2414592 total steps have happened
[32m[20221213 23:37:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3179 --------------------------#
[32m[20221213 23:37:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |           0.0076 |          63.4141 |           7.1309 |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |          -0.0069 |          55.9139 |           7.0683 |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |           0.0107 |          60.5615 |           7.1042 |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |          -0.0036 |          56.2970 |           7.1511 |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |          -0.0143 |          52.5531 |           7.0662 |
[32m[20221213 23:37:29 @agent_ppo2.py:185][0m |          -0.0140 |          52.2678 |           7.0937 |
[32m[20221213 23:37:30 @agent_ppo2.py:185][0m |          -0.0167 |          51.7324 |           7.0782 |
[32m[20221213 23:37:30 @agent_ppo2.py:185][0m |          -0.0153 |          51.6826 |           7.0482 |
[32m[20221213 23:37:30 @agent_ppo2.py:185][0m |          -0.0171 |          51.1320 |           7.1053 |
[32m[20221213 23:37:30 @agent_ppo2.py:185][0m |          -0.0053 |          56.1995 |           7.0844 |
[32m[20221213 23:37:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.69
[32m[20221213 23:37:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.72
[32m[20221213 23:37:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.21
[32m[20221213 23:37:30 @agent_ppo2.py:143][0m Total time:      24.97 min
[32m[20221213 23:37:30 @agent_ppo2.py:145][0m 2416640 total steps have happened
[32m[20221213 23:37:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3180 --------------------------#
[32m[20221213 23:37:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:37:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:30 @agent_ppo2.py:185][0m |          -0.0003 |          72.5339 |           6.6928 |
[32m[20221213 23:37:30 @agent_ppo2.py:185][0m |          -0.0014 |          67.5988 |           6.6461 |
[32m[20221213 23:37:30 @agent_ppo2.py:185][0m |          -0.0041 |          66.9658 |           6.6244 |
[32m[20221213 23:37:31 @agent_ppo2.py:185][0m |          -0.0018 |          66.3120 |           6.5575 |
[32m[20221213 23:37:31 @agent_ppo2.py:185][0m |          -0.0068 |          66.0304 |           6.6528 |
[32m[20221213 23:37:31 @agent_ppo2.py:185][0m |          -0.0067 |          65.5357 |           6.5615 |
[32m[20221213 23:37:31 @agent_ppo2.py:185][0m |          -0.0075 |          65.3130 |           6.5982 |
[32m[20221213 23:37:31 @agent_ppo2.py:185][0m |          -0.0046 |          65.7908 |           6.5975 |
[32m[20221213 23:37:31 @agent_ppo2.py:185][0m |          -0.0061 |          65.6484 |           6.6529 |
[32m[20221213 23:37:31 @agent_ppo2.py:185][0m |          -0.0108 |          64.7843 |           6.6405 |
[32m[20221213 23:37:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.91
[32m[20221213 23:37:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.46
[32m[20221213 23:37:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.46
[32m[20221213 23:37:31 @agent_ppo2.py:143][0m Total time:      24.99 min
[32m[20221213 23:37:31 @agent_ppo2.py:145][0m 2418688 total steps have happened
[32m[20221213 23:37:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3181 --------------------------#
[32m[20221213 23:37:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |           0.0021 |          51.3704 |           6.7122 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0076 |          44.5702 |           6.6768 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0133 |          42.6584 |           6.6813 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0129 |          41.7907 |           6.6511 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0085 |          40.9652 |           6.5540 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0115 |          40.3649 |           6.6690 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0137 |          39.7755 |           6.5943 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0171 |          39.5711 |           6.5687 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0141 |          38.9254 |           6.5332 |
[32m[20221213 23:37:32 @agent_ppo2.py:185][0m |          -0.0149 |          38.8526 |           6.5283 |
[32m[20221213 23:37:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:37:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.76
[32m[20221213 23:37:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.73
[32m[20221213 23:37:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.11
[32m[20221213 23:37:32 @agent_ppo2.py:143][0m Total time:      25.01 min
[32m[20221213 23:37:32 @agent_ppo2.py:145][0m 2420736 total steps have happened
[32m[20221213 23:37:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3182 --------------------------#
[32m[20221213 23:37:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |           0.0008 |          64.7953 |           5.9818 |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |          -0.0015 |          62.5559 |           6.0057 |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |          -0.0052 |          61.9761 |           6.0000 |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |          -0.0064 |          61.6940 |           5.9869 |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |          -0.0073 |          61.4828 |           5.9549 |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |          -0.0049 |          61.2751 |           5.9751 |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |          -0.0096 |          61.1722 |           5.9304 |
[32m[20221213 23:37:33 @agent_ppo2.py:185][0m |           0.0004 |          64.2970 |           6.0063 |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0037 |          61.1942 |           6.0087 |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0090 |          60.7983 |           5.9686 |
[32m[20221213 23:37:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:37:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.63
[32m[20221213 23:37:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.16
[32m[20221213 23:37:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.35
[32m[20221213 23:37:34 @agent_ppo2.py:143][0m Total time:      25.03 min
[32m[20221213 23:37:34 @agent_ppo2.py:145][0m 2422784 total steps have happened
[32m[20221213 23:37:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3183 --------------------------#
[32m[20221213 23:37:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0033 |          55.6471 |           6.3313 |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0046 |          52.9946 |           6.3577 |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0108 |          51.7733 |           6.3425 |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0100 |          51.3004 |           6.3729 |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0109 |          50.8551 |           6.3035 |
[32m[20221213 23:37:34 @agent_ppo2.py:185][0m |          -0.0116 |          50.3357 |           6.2529 |
[32m[20221213 23:37:35 @agent_ppo2.py:185][0m |          -0.0119 |          50.0955 |           6.2823 |
[32m[20221213 23:37:35 @agent_ppo2.py:185][0m |          -0.0127 |          50.0363 |           6.2710 |
[32m[20221213 23:37:35 @agent_ppo2.py:185][0m |          -0.0146 |          49.8163 |           6.2499 |
[32m[20221213 23:37:35 @agent_ppo2.py:185][0m |          -0.0154 |          49.7220 |           6.2639 |
[32m[20221213 23:37:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.06
[32m[20221213 23:37:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.31
[32m[20221213 23:37:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.18
[32m[20221213 23:37:35 @agent_ppo2.py:143][0m Total time:      25.05 min
[32m[20221213 23:37:35 @agent_ppo2.py:145][0m 2424832 total steps have happened
[32m[20221213 23:37:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3184 --------------------------#
[32m[20221213 23:37:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:35 @agent_ppo2.py:185][0m |          -0.0000 |          67.4389 |           5.7325 |
[32m[20221213 23:37:35 @agent_ppo2.py:185][0m |          -0.0022 |          65.4399 |           5.8165 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0065 |          63.4559 |           5.8212 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0082 |          63.0058 |           5.8131 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0102 |          62.2968 |           5.8936 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0120 |          61.8217 |           5.9129 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0132 |          61.4812 |           5.9086 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0126 |          61.6666 |           5.8467 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0151 |          60.9302 |           5.8946 |
[32m[20221213 23:37:36 @agent_ppo2.py:185][0m |          -0.0140 |          60.8253 |           5.8942 |
[32m[20221213 23:37:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.10
[32m[20221213 23:37:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.75
[32m[20221213 23:37:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.94
[32m[20221213 23:37:36 @agent_ppo2.py:143][0m Total time:      25.07 min
[32m[20221213 23:37:36 @agent_ppo2.py:145][0m 2426880 total steps have happened
[32m[20221213 23:37:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3185 --------------------------#
[32m[20221213 23:37:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |           0.0002 |          46.3084 |           5.7948 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0040 |          43.1068 |           5.8463 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0149 |          41.6287 |           5.7438 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0089 |          40.6801 |           5.7138 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0105 |          40.0820 |           5.7212 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0129 |          39.5841 |           5.6973 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0114 |          39.2709 |           5.6385 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0155 |          38.8640 |           5.6707 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0139 |          38.8934 |           5.6208 |
[32m[20221213 23:37:37 @agent_ppo2.py:185][0m |          -0.0166 |          38.3202 |           5.6191 |
[32m[20221213 23:37:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.26
[32m[20221213 23:37:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.64
[32m[20221213 23:37:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.08
[32m[20221213 23:37:37 @agent_ppo2.py:143][0m Total time:      25.09 min
[32m[20221213 23:37:37 @agent_ppo2.py:145][0m 2428928 total steps have happened
[32m[20221213 23:37:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3186 --------------------------#
[32m[20221213 23:37:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |           0.0109 |          47.0157 |           5.8842 |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |          -0.0044 |          39.5053 |           5.8911 |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |           0.0016 |          42.0345 |           5.8136 |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |           0.0032 |          44.7414 |           5.8826 |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |          -0.0126 |          37.0244 |           5.7951 |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |          -0.0198 |          36.1110 |           5.7979 |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |          -0.0157 |          35.7525 |           5.8279 |
[32m[20221213 23:37:38 @agent_ppo2.py:185][0m |          -0.0157 |          35.5318 |           5.7764 |
[32m[20221213 23:37:39 @agent_ppo2.py:185][0m |          -0.0137 |          35.1623 |           5.7116 |
[32m[20221213 23:37:39 @agent_ppo2.py:185][0m |          -0.0168 |          34.9913 |           5.7080 |
[32m[20221213 23:37:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:37:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.69
[32m[20221213 23:37:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.45
[32m[20221213 23:37:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.14
[32m[20221213 23:37:39 @agent_ppo2.py:143][0m Total time:      25.11 min
[32m[20221213 23:37:39 @agent_ppo2.py:145][0m 2430976 total steps have happened
[32m[20221213 23:37:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3187 --------------------------#
[32m[20221213 23:37:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:39 @agent_ppo2.py:185][0m |          -0.0029 |          62.7025 |           5.2713 |
[32m[20221213 23:37:39 @agent_ppo2.py:185][0m |          -0.0080 |          58.4817 |           5.2976 |
[32m[20221213 23:37:39 @agent_ppo2.py:185][0m |          -0.0067 |          57.3748 |           5.2738 |
[32m[20221213 23:37:39 @agent_ppo2.py:185][0m |           0.0056 |          62.9778 |           5.2364 |
[32m[20221213 23:37:39 @agent_ppo2.py:185][0m |          -0.0083 |          55.8458 |           5.2411 |
[32m[20221213 23:37:40 @agent_ppo2.py:185][0m |          -0.0113 |          55.4011 |           5.2728 |
[32m[20221213 23:37:40 @agent_ppo2.py:185][0m |          -0.0121 |          54.8834 |           5.2054 |
[32m[20221213 23:37:40 @agent_ppo2.py:185][0m |          -0.0115 |          54.6195 |           5.2986 |
[32m[20221213 23:37:40 @agent_ppo2.py:185][0m |          -0.0122 |          54.5687 |           5.2642 |
[32m[20221213 23:37:40 @agent_ppo2.py:185][0m |          -0.0139 |          54.6885 |           5.2861 |
[32m[20221213 23:37:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.67
[32m[20221213 23:37:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.64
[32m[20221213 23:37:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.19
[32m[20221213 23:37:40 @agent_ppo2.py:143][0m Total time:      25.13 min
[32m[20221213 23:37:40 @agent_ppo2.py:145][0m 2433024 total steps have happened
[32m[20221213 23:37:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3188 --------------------------#
[32m[20221213 23:37:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:40 @agent_ppo2.py:185][0m |          -0.0032 |          64.6712 |           6.1951 |
[32m[20221213 23:37:40 @agent_ppo2.py:185][0m |          -0.0043 |          60.2202 |           6.1915 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0078 |          57.4415 |           6.1778 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0051 |          56.9206 |           6.2033 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0047 |          56.4465 |           6.1847 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0042 |          61.2767 |           6.1972 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0120 |          54.3554 |           6.1942 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0118 |          53.7972 |           6.1989 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0130 |          53.7142 |           6.2037 |
[32m[20221213 23:37:41 @agent_ppo2.py:185][0m |          -0.0149 |          53.2967 |           6.2550 |
[32m[20221213 23:37:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.39
[32m[20221213 23:37:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.61
[32m[20221213 23:37:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.92
[32m[20221213 23:37:41 @agent_ppo2.py:143][0m Total time:      25.15 min
[32m[20221213 23:37:41 @agent_ppo2.py:145][0m 2435072 total steps have happened
[32m[20221213 23:37:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3189 --------------------------#
[32m[20221213 23:37:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |           0.0045 |          60.9610 |           5.9264 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0054 |          54.8345 |           6.0139 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0073 |          53.0476 |           5.9270 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |           0.0021 |          57.9864 |           5.9082 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0099 |          51.5471 |           5.9251 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0140 |          51.0357 |           5.8650 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0120 |          50.5118 |           5.9365 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0135 |          50.2710 |           5.9306 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0170 |          49.9310 |           5.8577 |
[32m[20221213 23:37:42 @agent_ppo2.py:185][0m |          -0.0162 |          49.7183 |           5.8902 |
[32m[20221213 23:37:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.59
[32m[20221213 23:37:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.43
[32m[20221213 23:37:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.58
[32m[20221213 23:37:43 @agent_ppo2.py:143][0m Total time:      25.17 min
[32m[20221213 23:37:43 @agent_ppo2.py:145][0m 2437120 total steps have happened
[32m[20221213 23:37:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3190 --------------------------#
[32m[20221213 23:37:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:37:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |          -0.0021 |          72.4515 |           5.5618 |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |           0.0121 |          73.0077 |           5.6001 |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |          -0.0038 |          67.0878 |           5.5118 |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |          -0.0039 |          65.8839 |           5.4975 |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |          -0.0072 |          65.2961 |           5.4985 |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |          -0.0076 |          65.2290 |           5.5422 |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |          -0.0095 |          64.2257 |           5.4539 |
[32m[20221213 23:37:43 @agent_ppo2.py:185][0m |          -0.0103 |          64.1760 |           5.5424 |
[32m[20221213 23:37:44 @agent_ppo2.py:185][0m |          -0.0053 |          66.8317 |           5.4973 |
[32m[20221213 23:37:44 @agent_ppo2.py:185][0m |          -0.0076 |          64.2961 |           5.5626 |
[32m[20221213 23:37:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:37:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.47
[32m[20221213 23:37:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.22
[32m[20221213 23:37:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.92
[32m[20221213 23:37:44 @agent_ppo2.py:143][0m Total time:      25.20 min
[32m[20221213 23:37:44 @agent_ppo2.py:145][0m 2439168 total steps have happened
[32m[20221213 23:37:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3191 --------------------------#
[32m[20221213 23:37:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:44 @agent_ppo2.py:185][0m |          -0.0013 |          68.4749 |           5.6409 |
[32m[20221213 23:37:44 @agent_ppo2.py:185][0m |          -0.0083 |          61.7022 |           5.6662 |
[32m[20221213 23:37:44 @agent_ppo2.py:185][0m |          -0.0084 |          59.6225 |           5.6728 |
[32m[20221213 23:37:44 @agent_ppo2.py:185][0m |          -0.0079 |          58.3406 |           5.7161 |
[32m[20221213 23:37:44 @agent_ppo2.py:185][0m |          -0.0117 |          57.5524 |           5.7261 |
[32m[20221213 23:37:45 @agent_ppo2.py:185][0m |          -0.0146 |          57.0027 |           5.7723 |
[32m[20221213 23:37:45 @agent_ppo2.py:185][0m |          -0.0049 |          60.5493 |           5.7136 |
[32m[20221213 23:37:45 @agent_ppo2.py:185][0m |          -0.0118 |          56.1502 |           5.7255 |
[32m[20221213 23:37:45 @agent_ppo2.py:185][0m |          -0.0158 |          56.0530 |           5.7407 |
[32m[20221213 23:37:45 @agent_ppo2.py:185][0m |          -0.0179 |          55.2572 |           5.7264 |
[32m[20221213 23:37:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:37:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.09
[32m[20221213 23:37:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.48
[32m[20221213 23:37:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.55
[32m[20221213 23:37:45 @agent_ppo2.py:143][0m Total time:      25.22 min
[32m[20221213 23:37:45 @agent_ppo2.py:145][0m 2441216 total steps have happened
[32m[20221213 23:37:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3192 --------------------------#
[32m[20221213 23:37:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:45 @agent_ppo2.py:185][0m |           0.0044 |          66.6197 |           5.4682 |
[32m[20221213 23:37:45 @agent_ppo2.py:185][0m |          -0.0011 |          62.6193 |           5.5306 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0061 |          60.1162 |           5.4593 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0052 |          59.1107 |           5.4318 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0087 |          58.7023 |           5.4233 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0090 |          58.0827 |           5.3483 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0104 |          58.2191 |           5.3121 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0117 |          57.6053 |           5.3339 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0071 |          57.9646 |           5.3320 |
[32m[20221213 23:37:46 @agent_ppo2.py:185][0m |          -0.0048 |          59.2733 |           5.3784 |
[32m[20221213 23:37:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.89
[32m[20221213 23:37:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.31
[32m[20221213 23:37:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.23
[32m[20221213 23:37:46 @agent_ppo2.py:143][0m Total time:      25.24 min
[32m[20221213 23:37:46 @agent_ppo2.py:145][0m 2443264 total steps have happened
[32m[20221213 23:37:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3193 --------------------------#
[32m[20221213 23:37:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |           0.0060 |          53.8444 |           5.1370 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |           0.0064 |          53.6440 |           5.0293 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0029 |          48.6444 |           5.0955 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0004 |          49.7736 |           5.1322 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0099 |          47.4455 |           5.0496 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0124 |          47.0950 |           5.0565 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0129 |          46.8735 |           4.9970 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0122 |          46.3631 |           4.9632 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0137 |          46.2085 |           4.8983 |
[32m[20221213 23:37:47 @agent_ppo2.py:185][0m |          -0.0152 |          45.8881 |           4.8802 |
[32m[20221213 23:37:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.54
[32m[20221213 23:37:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.00
[32m[20221213 23:37:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.80
[32m[20221213 23:37:48 @agent_ppo2.py:143][0m Total time:      25.26 min
[32m[20221213 23:37:48 @agent_ppo2.py:145][0m 2445312 total steps have happened
[32m[20221213 23:37:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3194 --------------------------#
[32m[20221213 23:37:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0007 |          70.2012 |           4.9972 |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0003 |          68.3917 |           4.9792 |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0106 |          65.3718 |           4.9817 |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0102 |          64.8222 |           5.0088 |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0138 |          64.1209 |           5.0048 |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0118 |          63.9892 |           4.9814 |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0157 |          63.2788 |           4.9125 |
[32m[20221213 23:37:48 @agent_ppo2.py:185][0m |          -0.0155 |          62.8747 |           4.9898 |
[32m[20221213 23:37:49 @agent_ppo2.py:185][0m |          -0.0194 |          62.3433 |           5.0125 |
[32m[20221213 23:37:49 @agent_ppo2.py:185][0m |          -0.0179 |          62.1496 |           4.9189 |
[32m[20221213 23:37:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:37:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.79
[32m[20221213 23:37:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.31
[32m[20221213 23:37:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.56
[32m[20221213 23:37:49 @agent_ppo2.py:143][0m Total time:      25.28 min
[32m[20221213 23:37:49 @agent_ppo2.py:145][0m 2447360 total steps have happened
[32m[20221213 23:37:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3195 --------------------------#
[32m[20221213 23:37:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:49 @agent_ppo2.py:185][0m |           0.0026 |          65.8868 |           4.4207 |
[32m[20221213 23:37:49 @agent_ppo2.py:185][0m |           0.0046 |          61.9216 |           4.1872 |
[32m[20221213 23:37:49 @agent_ppo2.py:185][0m |          -0.0068 |          59.6638 |           4.1878 |
[32m[20221213 23:37:49 @agent_ppo2.py:185][0m |          -0.0066 |          58.3685 |           4.2413 |
[32m[20221213 23:37:49 @agent_ppo2.py:185][0m |          -0.0075 |          57.4934 |           4.1525 |
[32m[20221213 23:37:50 @agent_ppo2.py:185][0m |          -0.0087 |          57.0654 |           4.1135 |
[32m[20221213 23:37:50 @agent_ppo2.py:185][0m |          -0.0110 |          56.1459 |           4.0864 |
[32m[20221213 23:37:50 @agent_ppo2.py:185][0m |          -0.0159 |          55.7414 |           4.1315 |
[32m[20221213 23:37:50 @agent_ppo2.py:185][0m |          -0.0011 |          58.5577 |           4.1342 |
[32m[20221213 23:37:50 @agent_ppo2.py:185][0m |          -0.0127 |          54.8615 |           4.0145 |
[32m[20221213 23:37:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.40
[32m[20221213 23:37:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.24
[32m[20221213 23:37:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.13
[32m[20221213 23:37:50 @agent_ppo2.py:143][0m Total time:      25.30 min
[32m[20221213 23:37:50 @agent_ppo2.py:145][0m 2449408 total steps have happened
[32m[20221213 23:37:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3196 --------------------------#
[32m[20221213 23:37:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:37:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:50 @agent_ppo2.py:185][0m |           0.0011 |          62.3850 |           4.2992 |
[32m[20221213 23:37:50 @agent_ppo2.py:185][0m |           0.0019 |          58.3064 |           4.2157 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |          -0.0053 |          54.0105 |           4.1681 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |          -0.0065 |          51.3933 |           4.1519 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |          -0.0059 |          50.0793 |           4.1398 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |          -0.0083 |          49.3637 |           4.0875 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |          -0.0116 |          48.8620 |           4.0606 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |           0.0014 |          53.9993 |           4.1085 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |          -0.0071 |          48.1794 |           4.1048 |
[32m[20221213 23:37:51 @agent_ppo2.py:185][0m |          -0.0023 |          51.6181 |           4.0548 |
[32m[20221213 23:37:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:37:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.18
[32m[20221213 23:37:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.22
[32m[20221213 23:37:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.81
[32m[20221213 23:37:51 @agent_ppo2.py:143][0m Total time:      25.32 min
[32m[20221213 23:37:51 @agent_ppo2.py:145][0m 2451456 total steps have happened
[32m[20221213 23:37:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3197 --------------------------#
[32m[20221213 23:37:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |           0.0037 |          58.3563 |           4.1720 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0054 |          47.6066 |           4.2731 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0034 |          48.7121 |           4.2071 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0089 |          45.5552 |           4.2213 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0076 |          45.1941 |           4.2051 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0097 |          44.5058 |           4.2535 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0130 |          44.1964 |           4.2609 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0134 |          44.3473 |           4.2134 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0029 |          51.2497 |           4.2850 |
[32m[20221213 23:37:52 @agent_ppo2.py:185][0m |          -0.0091 |          43.5318 |           4.2271 |
[32m[20221213 23:37:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.85
[32m[20221213 23:37:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.72
[32m[20221213 23:37:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.33
[32m[20221213 23:37:53 @agent_ppo2.py:143][0m Total time:      25.34 min
[32m[20221213 23:37:53 @agent_ppo2.py:145][0m 2453504 total steps have happened
[32m[20221213 23:37:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3198 --------------------------#
[32m[20221213 23:37:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:53 @agent_ppo2.py:185][0m |           0.0012 |          55.5990 |           3.4831 |
[32m[20221213 23:37:53 @agent_ppo2.py:185][0m |          -0.0061 |          50.9082 |           3.4539 |
[32m[20221213 23:37:53 @agent_ppo2.py:185][0m |          -0.0053 |          49.6124 |           3.3898 |
[32m[20221213 23:37:53 @agent_ppo2.py:185][0m |          -0.0096 |          48.6432 |           3.4420 |
[32m[20221213 23:37:53 @agent_ppo2.py:185][0m |          -0.0096 |          47.8479 |           3.4755 |
[32m[20221213 23:37:53 @agent_ppo2.py:185][0m |          -0.0110 |          47.1905 |           3.4644 |
[32m[20221213 23:37:53 @agent_ppo2.py:185][0m |          -0.0083 |          46.6842 |           3.4783 |
[32m[20221213 23:37:54 @agent_ppo2.py:185][0m |          -0.0159 |          45.9196 |           3.5050 |
[32m[20221213 23:37:54 @agent_ppo2.py:185][0m |          -0.0107 |          45.6484 |           3.4978 |
[32m[20221213 23:37:54 @agent_ppo2.py:185][0m |          -0.0099 |          47.5002 |           3.4688 |
[32m[20221213 23:37:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.76
[32m[20221213 23:37:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.59
[32m[20221213 23:37:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.95
[32m[20221213 23:37:54 @agent_ppo2.py:143][0m Total time:      25.36 min
[32m[20221213 23:37:54 @agent_ppo2.py:145][0m 2455552 total steps have happened
[32m[20221213 23:37:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3199 --------------------------#
[32m[20221213 23:37:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:54 @agent_ppo2.py:185][0m |          -0.0009 |          57.3419 |           3.8793 |
[32m[20221213 23:37:54 @agent_ppo2.py:185][0m |          -0.0051 |          52.6832 |           3.9132 |
[32m[20221213 23:37:54 @agent_ppo2.py:185][0m |          -0.0114 |          51.5059 |           3.9811 |
[32m[20221213 23:37:54 @agent_ppo2.py:185][0m |          -0.0090 |          50.6248 |           3.9890 |
[32m[20221213 23:37:55 @agent_ppo2.py:185][0m |          -0.0100 |          50.3447 |           3.9780 |
[32m[20221213 23:37:55 @agent_ppo2.py:185][0m |          -0.0126 |          49.5590 |           3.9997 |
[32m[20221213 23:37:55 @agent_ppo2.py:185][0m |          -0.0023 |          53.1067 |           3.9440 |
[32m[20221213 23:37:55 @agent_ppo2.py:185][0m |          -0.0122 |          48.8419 |           3.9668 |
[32m[20221213 23:37:55 @agent_ppo2.py:185][0m |          -0.0077 |          49.0165 |           4.0043 |
[32m[20221213 23:37:55 @agent_ppo2.py:185][0m |          -0.0166 |          48.2981 |           3.9480 |
[32m[20221213 23:37:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:37:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.66
[32m[20221213 23:37:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.40
[32m[20221213 23:37:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.83
[32m[20221213 23:37:55 @agent_ppo2.py:143][0m Total time:      25.38 min
[32m[20221213 23:37:55 @agent_ppo2.py:145][0m 2457600 total steps have happened
[32m[20221213 23:37:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3200 --------------------------#
[32m[20221213 23:37:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:37:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:55 @agent_ppo2.py:185][0m |           0.0050 |          56.1555 |           4.0072 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0040 |          52.9351 |           4.0564 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0027 |          52.2913 |           4.0133 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0075 |          51.5901 |           4.1282 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0070 |          51.1742 |           4.0004 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0096 |          50.9731 |           4.0394 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0101 |          50.4593 |           4.0446 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0141 |          50.1520 |           4.0497 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0124 |          49.9306 |           4.0267 |
[32m[20221213 23:37:56 @agent_ppo2.py:185][0m |          -0.0120 |          49.6930 |           4.0233 |
[32m[20221213 23:37:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:37:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.50
[32m[20221213 23:37:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.83
[32m[20221213 23:37:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.17
[32m[20221213 23:37:56 @agent_ppo2.py:143][0m Total time:      25.41 min
[32m[20221213 23:37:56 @agent_ppo2.py:145][0m 2459648 total steps have happened
[32m[20221213 23:37:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3201 --------------------------#
[32m[20221213 23:37:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |           0.0021 |          71.0343 |           3.8739 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |           0.0066 |          70.4046 |           3.8811 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |          -0.0037 |          65.0318 |           3.8764 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |          -0.0085 |          63.1131 |           3.7943 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |           0.0044 |          69.6480 |           3.7594 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |          -0.0055 |          61.8280 |           3.7150 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |          -0.0077 |          61.6441 |           3.7573 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |          -0.0140 |          60.8120 |           3.7147 |
[32m[20221213 23:37:57 @agent_ppo2.py:185][0m |          -0.0030 |          62.3798 |           3.7720 |
[32m[20221213 23:37:58 @agent_ppo2.py:185][0m |          -0.0095 |          59.9970 |           3.6997 |
[32m[20221213 23:37:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:37:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.40
[32m[20221213 23:37:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.18
[32m[20221213 23:37:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.72
[32m[20221213 23:37:58 @agent_ppo2.py:143][0m Total time:      25.43 min
[32m[20221213 23:37:58 @agent_ppo2.py:145][0m 2461696 total steps have happened
[32m[20221213 23:37:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3202 --------------------------#
[32m[20221213 23:37:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:58 @agent_ppo2.py:185][0m |           0.0099 |          67.6367 |           4.0350 |
[32m[20221213 23:37:58 @agent_ppo2.py:185][0m |          -0.0034 |          55.8365 |           4.0817 |
[32m[20221213 23:37:58 @agent_ppo2.py:185][0m |          -0.0073 |          54.0669 |           4.0785 |
[32m[20221213 23:37:58 @agent_ppo2.py:185][0m |          -0.0017 |          55.2809 |           4.1311 |
[32m[20221213 23:37:58 @agent_ppo2.py:185][0m |          -0.0036 |          54.0059 |           4.1167 |
[32m[20221213 23:37:58 @agent_ppo2.py:185][0m |          -0.0065 |          52.0870 |           4.0957 |
[32m[20221213 23:37:59 @agent_ppo2.py:185][0m |          -0.0026 |          52.1172 |           4.1154 |
[32m[20221213 23:37:59 @agent_ppo2.py:185][0m |          -0.0088 |          51.4987 |           4.1916 |
[32m[20221213 23:37:59 @agent_ppo2.py:185][0m |          -0.0124 |          51.1460 |           4.1784 |
[32m[20221213 23:37:59 @agent_ppo2.py:185][0m |          -0.0109 |          50.8903 |           4.1977 |
[32m[20221213 23:37:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 23:37:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.77
[32m[20221213 23:37:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.76
[32m[20221213 23:37:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.64
[32m[20221213 23:37:59 @agent_ppo2.py:143][0m Total time:      25.45 min
[32m[20221213 23:37:59 @agent_ppo2.py:145][0m 2463744 total steps have happened
[32m[20221213 23:37:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3203 --------------------------#
[32m[20221213 23:37:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:37:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:37:59 @agent_ppo2.py:185][0m |           0.0004 |          59.0752 |           4.7435 |
[32m[20221213 23:37:59 @agent_ppo2.py:185][0m |          -0.0053 |          55.9756 |           4.7304 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0083 |          55.4802 |           4.6921 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0034 |          55.3068 |           4.8054 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0071 |          54.7183 |           4.9396 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0066 |          54.6068 |           4.8475 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0088 |          54.5196 |           4.9750 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0075 |          54.4256 |           4.9129 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0086 |          54.1693 |           4.9167 |
[32m[20221213 23:38:00 @agent_ppo2.py:185][0m |          -0.0083 |          54.5917 |           4.9348 |
[32m[20221213 23:38:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:38:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.43
[32m[20221213 23:38:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.33
[32m[20221213 23:38:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.08
[32m[20221213 23:38:00 @agent_ppo2.py:143][0m Total time:      25.47 min
[32m[20221213 23:38:00 @agent_ppo2.py:145][0m 2465792 total steps have happened
[32m[20221213 23:38:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3204 --------------------------#
[32m[20221213 23:38:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |           0.0110 |          69.4846 |           4.4955 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0052 |          50.7988 |           4.3626 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0067 |          47.4222 |           4.4006 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |           0.0027 |          46.8698 |           4.3971 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0094 |          43.3318 |           4.3383 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0103 |          42.2146 |           4.3727 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0105 |          41.5601 |           4.3650 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0102 |          41.1498 |           4.4056 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0119 |          41.2461 |           4.3779 |
[32m[20221213 23:38:01 @agent_ppo2.py:185][0m |          -0.0067 |          40.2627 |           4.4193 |
[32m[20221213 23:38:01 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:38:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.64
[32m[20221213 23:38:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.33
[32m[20221213 23:38:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.53
[32m[20221213 23:38:02 @agent_ppo2.py:143][0m Total time:      25.49 min
[32m[20221213 23:38:02 @agent_ppo2.py:145][0m 2467840 total steps have happened
[32m[20221213 23:38:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3205 --------------------------#
[32m[20221213 23:38:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:02 @agent_ppo2.py:185][0m |          -0.0026 |          54.5410 |           4.0995 |
[32m[20221213 23:38:02 @agent_ppo2.py:185][0m |          -0.0032 |          44.7228 |           4.1167 |
[32m[20221213 23:38:02 @agent_ppo2.py:185][0m |          -0.0087 |          42.3209 |           4.1573 |
[32m[20221213 23:38:02 @agent_ppo2.py:185][0m |          -0.0026 |          40.8474 |           4.1193 |
[32m[20221213 23:38:02 @agent_ppo2.py:185][0m |          -0.0074 |          39.8328 |           4.1125 |
[32m[20221213 23:38:02 @agent_ppo2.py:185][0m |          -0.0146 |          38.8464 |           4.1354 |
[32m[20221213 23:38:02 @agent_ppo2.py:185][0m |          -0.0105 |          38.4067 |           4.1203 |
[32m[20221213 23:38:03 @agent_ppo2.py:185][0m |          -0.0117 |          37.9879 |           4.1827 |
[32m[20221213 23:38:03 @agent_ppo2.py:185][0m |          -0.0114 |          37.8386 |           4.1363 |
[32m[20221213 23:38:03 @agent_ppo2.py:185][0m |          -0.0148 |          37.4032 |           4.1032 |
[32m[20221213 23:38:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:38:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.38
[32m[20221213 23:38:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.95
[32m[20221213 23:38:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.38
[32m[20221213 23:38:03 @agent_ppo2.py:143][0m Total time:      25.51 min
[32m[20221213 23:38:03 @agent_ppo2.py:145][0m 2469888 total steps have happened
[32m[20221213 23:38:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3206 --------------------------#
[32m[20221213 23:38:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:03 @agent_ppo2.py:185][0m |           0.0001 |          61.3308 |           3.8074 |
[32m[20221213 23:38:03 @agent_ppo2.py:185][0m |          -0.0071 |          55.8166 |           3.8406 |
[32m[20221213 23:38:03 @agent_ppo2.py:185][0m |          -0.0028 |          54.2773 |           3.7817 |
[32m[20221213 23:38:03 @agent_ppo2.py:185][0m |          -0.0088 |          53.8135 |           3.7943 |
[32m[20221213 23:38:04 @agent_ppo2.py:185][0m |          -0.0085 |          52.8716 |           3.9027 |
[32m[20221213 23:38:04 @agent_ppo2.py:185][0m |          -0.0128 |          52.4460 |           3.8072 |
[32m[20221213 23:38:04 @agent_ppo2.py:185][0m |          -0.0105 |          52.0190 |           3.8417 |
[32m[20221213 23:38:04 @agent_ppo2.py:185][0m |          -0.0152 |          51.6986 |           3.8717 |
[32m[20221213 23:38:04 @agent_ppo2.py:185][0m |          -0.0125 |          51.5443 |           3.8347 |
[32m[20221213 23:38:04 @agent_ppo2.py:185][0m |          -0.0133 |          51.1487 |           3.9297 |
[32m[20221213 23:38:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:38:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.25
[32m[20221213 23:38:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.39
[32m[20221213 23:38:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.27
[32m[20221213 23:38:04 @agent_ppo2.py:143][0m Total time:      25.54 min
[32m[20221213 23:38:04 @agent_ppo2.py:145][0m 2471936 total steps have happened
[32m[20221213 23:38:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3207 --------------------------#
[32m[20221213 23:38:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:04 @agent_ppo2.py:185][0m |           0.0049 |          56.8277 |           4.0533 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0039 |          51.7366 |           4.1114 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0053 |          50.4317 |           4.0299 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0062 |          49.4641 |           4.0766 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0040 |          48.6453 |           4.0966 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0060 |          49.3169 |           4.1692 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0114 |          47.8385 |           4.2294 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0150 |          47.8582 |           4.2201 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0173 |          47.0321 |           4.1871 |
[32m[20221213 23:38:05 @agent_ppo2.py:185][0m |          -0.0163 |          46.6948 |           4.2292 |
[32m[20221213 23:38:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.98
[32m[20221213 23:38:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.12
[32m[20221213 23:38:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.01
[32m[20221213 23:38:05 @agent_ppo2.py:143][0m Total time:      25.56 min
[32m[20221213 23:38:05 @agent_ppo2.py:145][0m 2473984 total steps have happened
[32m[20221213 23:38:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3208 --------------------------#
[32m[20221213 23:38:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |           0.0020 |          56.2143 |           3.8461 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0009 |          52.7759 |           3.7707 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0039 |          50.8507 |           3.8516 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0094 |          49.5141 |           3.7349 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0072 |          48.7218 |           3.8225 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0079 |          48.2985 |           3.7963 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0130 |          47.5899 |           3.7501 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0137 |          47.2754 |           3.7604 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0097 |          46.7135 |           3.7501 |
[32m[20221213 23:38:06 @agent_ppo2.py:185][0m |          -0.0157 |          46.4412 |           3.7422 |
[32m[20221213 23:38:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.68
[32m[20221213 23:38:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.76
[32m[20221213 23:38:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.80
[32m[20221213 23:38:07 @agent_ppo2.py:143][0m Total time:      25.58 min
[32m[20221213 23:38:07 @agent_ppo2.py:145][0m 2476032 total steps have happened
[32m[20221213 23:38:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3209 --------------------------#
[32m[20221213 23:38:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:07 @agent_ppo2.py:185][0m |           0.0036 |          65.5709 |           3.9612 |
[32m[20221213 23:38:07 @agent_ppo2.py:185][0m |          -0.0062 |          59.6522 |           4.0353 |
[32m[20221213 23:38:07 @agent_ppo2.py:185][0m |          -0.0039 |          58.8053 |           3.9831 |
[32m[20221213 23:38:07 @agent_ppo2.py:185][0m |          -0.0071 |          57.9593 |           4.0718 |
[32m[20221213 23:38:07 @agent_ppo2.py:185][0m |          -0.0102 |          57.6236 |           4.0875 |
[32m[20221213 23:38:07 @agent_ppo2.py:185][0m |          -0.0096 |          57.3174 |           4.0747 |
[32m[20221213 23:38:07 @agent_ppo2.py:185][0m |          -0.0123 |          57.0705 |           4.1121 |
[32m[20221213 23:38:08 @agent_ppo2.py:185][0m |          -0.0146 |          56.7380 |           4.0871 |
[32m[20221213 23:38:08 @agent_ppo2.py:185][0m |          -0.0196 |          56.6720 |           4.1396 |
[32m[20221213 23:38:08 @agent_ppo2.py:185][0m |          -0.0100 |          56.4560 |           4.1880 |
[32m[20221213 23:38:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.55
[32m[20221213 23:38:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.81
[32m[20221213 23:38:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.69
[32m[20221213 23:38:08 @agent_ppo2.py:143][0m Total time:      25.60 min
[32m[20221213 23:38:08 @agent_ppo2.py:145][0m 2478080 total steps have happened
[32m[20221213 23:38:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3210 --------------------------#
[32m[20221213 23:38:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:38:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:08 @agent_ppo2.py:185][0m |           0.0028 |          63.5407 |           4.1371 |
[32m[20221213 23:38:08 @agent_ppo2.py:185][0m |          -0.0070 |          60.8638 |           4.1757 |
[32m[20221213 23:38:08 @agent_ppo2.py:185][0m |          -0.0071 |          60.0030 |           4.1350 |
[32m[20221213 23:38:08 @agent_ppo2.py:185][0m |          -0.0125 |          59.2713 |           4.2269 |
[32m[20221213 23:38:09 @agent_ppo2.py:185][0m |          -0.0130 |          58.6835 |           4.1548 |
[32m[20221213 23:38:09 @agent_ppo2.py:185][0m |          -0.0143 |          58.3998 |           4.1854 |
[32m[20221213 23:38:09 @agent_ppo2.py:185][0m |          -0.0148 |          58.0761 |           4.1533 |
[32m[20221213 23:38:09 @agent_ppo2.py:185][0m |          -0.0155 |          57.6794 |           4.1703 |
[32m[20221213 23:38:09 @agent_ppo2.py:185][0m |          -0.0161 |          57.5954 |           4.1625 |
[32m[20221213 23:38:09 @agent_ppo2.py:185][0m |          -0.0164 |          57.4004 |           4.1429 |
[32m[20221213 23:38:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:38:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.52
[32m[20221213 23:38:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.53
[32m[20221213 23:38:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.35
[32m[20221213 23:38:09 @agent_ppo2.py:143][0m Total time:      25.62 min
[32m[20221213 23:38:09 @agent_ppo2.py:145][0m 2480128 total steps have happened
[32m[20221213 23:38:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3211 --------------------------#
[32m[20221213 23:38:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:09 @agent_ppo2.py:185][0m |           0.0029 |          54.8736 |           4.7834 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0052 |          48.5695 |           4.7393 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0087 |          46.2330 |           4.8610 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0079 |          44.5429 |           4.8723 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0102 |          42.7478 |           4.9139 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0130 |          41.7129 |           4.8466 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0028 |          41.8529 |           4.9252 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0132 |          40.7195 |           4.8751 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0160 |          40.3105 |           4.8379 |
[32m[20221213 23:38:10 @agent_ppo2.py:185][0m |          -0.0174 |          40.7659 |           4.8386 |
[32m[20221213 23:38:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.95
[32m[20221213 23:38:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.39
[32m[20221213 23:38:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.88
[32m[20221213 23:38:10 @agent_ppo2.py:143][0m Total time:      25.64 min
[32m[20221213 23:38:10 @agent_ppo2.py:145][0m 2482176 total steps have happened
[32m[20221213 23:38:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3212 --------------------------#
[32m[20221213 23:38:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |           0.0057 |          62.6943 |           4.0776 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0060 |          55.4404 |           4.1159 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0079 |          53.9100 |           4.0402 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0082 |          53.0820 |           4.0524 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0097 |          52.5644 |           4.0118 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0150 |          52.0300 |           3.9919 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0150 |          51.5621 |           4.0538 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0110 |          51.1886 |           3.9432 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0142 |          50.9491 |           4.0004 |
[32m[20221213 23:38:11 @agent_ppo2.py:185][0m |          -0.0165 |          50.7541 |           3.9888 |
[32m[20221213 23:38:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.82
[32m[20221213 23:38:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.15
[32m[20221213 23:38:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.40
[32m[20221213 23:38:12 @agent_ppo2.py:143][0m Total time:      25.66 min
[32m[20221213 23:38:12 @agent_ppo2.py:145][0m 2484224 total steps have happened
[32m[20221213 23:38:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3213 --------------------------#
[32m[20221213 23:38:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:12 @agent_ppo2.py:185][0m |           0.0052 |          57.6653 |           3.8418 |
[32m[20221213 23:38:12 @agent_ppo2.py:185][0m |           0.0080 |          61.0693 |           3.9514 |
[32m[20221213 23:38:12 @agent_ppo2.py:185][0m |          -0.0041 |          52.8162 |           3.9784 |
[32m[20221213 23:38:12 @agent_ppo2.py:185][0m |          -0.0075 |          51.2040 |           3.9686 |
[32m[20221213 23:38:12 @agent_ppo2.py:185][0m |          -0.0094 |          50.4652 |           3.9485 |
[32m[20221213 23:38:12 @agent_ppo2.py:185][0m |          -0.0107 |          49.7459 |           3.9261 |
[32m[20221213 23:38:12 @agent_ppo2.py:185][0m |          -0.0081 |          49.9520 |           3.8825 |
[32m[20221213 23:38:13 @agent_ppo2.py:185][0m |          -0.0112 |          48.9962 |           3.8647 |
[32m[20221213 23:38:13 @agent_ppo2.py:185][0m |          -0.0143 |          48.7124 |           3.8666 |
[32m[20221213 23:38:13 @agent_ppo2.py:185][0m |          -0.0172 |          48.7041 |           3.8841 |
[32m[20221213 23:38:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.59
[32m[20221213 23:38:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.51
[32m[20221213 23:38:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.50
[32m[20221213 23:38:13 @agent_ppo2.py:143][0m Total time:      25.68 min
[32m[20221213 23:38:13 @agent_ppo2.py:145][0m 2486272 total steps have happened
[32m[20221213 23:38:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3214 --------------------------#
[32m[20221213 23:38:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:13 @agent_ppo2.py:185][0m |           0.0058 |          67.3232 |           3.9561 |
[32m[20221213 23:38:13 @agent_ppo2.py:185][0m |          -0.0025 |          63.4278 |           4.0517 |
[32m[20221213 23:38:13 @agent_ppo2.py:185][0m |          -0.0085 |          60.8471 |           4.0259 |
[32m[20221213 23:38:13 @agent_ppo2.py:185][0m |          -0.0045 |          59.7417 |           3.9402 |
[32m[20221213 23:38:14 @agent_ppo2.py:185][0m |          -0.0102 |          58.5622 |           3.9199 |
[32m[20221213 23:38:14 @agent_ppo2.py:185][0m |           0.0089 |          68.9023 |           3.9183 |
[32m[20221213 23:38:14 @agent_ppo2.py:185][0m |           0.0033 |          60.0751 |           3.9415 |
[32m[20221213 23:38:14 @agent_ppo2.py:185][0m |          -0.0122 |          57.1878 |           4.0179 |
[32m[20221213 23:38:14 @agent_ppo2.py:185][0m |          -0.0162 |          56.4065 |           4.0051 |
[32m[20221213 23:38:14 @agent_ppo2.py:185][0m |          -0.0141 |          55.7362 |           4.0095 |
[32m[20221213 23:38:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.35
[32m[20221213 23:38:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.11
[32m[20221213 23:38:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.29
[32m[20221213 23:38:14 @agent_ppo2.py:143][0m Total time:      25.70 min
[32m[20221213 23:38:14 @agent_ppo2.py:145][0m 2488320 total steps have happened
[32m[20221213 23:38:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3215 --------------------------#
[32m[20221213 23:38:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:14 @agent_ppo2.py:185][0m |          -0.0019 |          71.5335 |           2.9192 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |          -0.0034 |          66.4914 |           3.0041 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |          -0.0069 |          65.6452 |           3.0516 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |          -0.0034 |          65.5813 |           3.0246 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |          -0.0095 |          63.8349 |           3.0924 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |          -0.0119 |          63.1056 |           3.1411 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |          -0.0123 |          62.6561 |           3.1678 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |           0.0004 |          67.5249 |           3.1798 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |           0.0025 |          69.3262 |           3.2470 |
[32m[20221213 23:38:15 @agent_ppo2.py:185][0m |          -0.0054 |          66.1653 |           3.2621 |
[32m[20221213 23:38:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:38:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.59
[32m[20221213 23:38:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.60
[32m[20221213 23:38:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.49
[32m[20221213 23:38:15 @agent_ppo2.py:143][0m Total time:      25.72 min
[32m[20221213 23:38:15 @agent_ppo2.py:145][0m 2490368 total steps have happened
[32m[20221213 23:38:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3216 --------------------------#
[32m[20221213 23:38:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0025 |          62.5476 |           5.0232 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0066 |          60.5388 |           5.0027 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0080 |          59.7042 |           5.0370 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0078 |          59.3893 |           5.1078 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0082 |          59.0559 |           5.0916 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0112 |          58.7538 |           5.1811 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0091 |          58.6351 |           5.1631 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0100 |          59.4556 |           5.0948 |
[32m[20221213 23:38:16 @agent_ppo2.py:185][0m |          -0.0110 |          58.2560 |           5.2353 |
[32m[20221213 23:38:17 @agent_ppo2.py:185][0m |          -0.0123 |          58.2374 |           5.1341 |
[32m[20221213 23:38:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.69
[32m[20221213 23:38:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.70
[32m[20221213 23:38:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.40
[32m[20221213 23:38:17 @agent_ppo2.py:143][0m Total time:      25.74 min
[32m[20221213 23:38:17 @agent_ppo2.py:145][0m 2492416 total steps have happened
[32m[20221213 23:38:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3217 --------------------------#
[32m[20221213 23:38:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:17 @agent_ppo2.py:185][0m |           0.0049 |          60.6314 |           4.0773 |
[32m[20221213 23:38:17 @agent_ppo2.py:185][0m |          -0.0050 |          56.2000 |           4.0868 |
[32m[20221213 23:38:17 @agent_ppo2.py:185][0m |          -0.0064 |          54.8321 |           4.0440 |
[32m[20221213 23:38:17 @agent_ppo2.py:185][0m |           0.0035 |          57.7439 |           4.2591 |
[32m[20221213 23:38:17 @agent_ppo2.py:185][0m |          -0.0121 |          52.6553 |           4.1995 |
[32m[20221213 23:38:17 @agent_ppo2.py:185][0m |          -0.0139 |          52.0035 |           4.1682 |
[32m[20221213 23:38:18 @agent_ppo2.py:185][0m |          -0.0114 |          51.5483 |           4.1521 |
[32m[20221213 23:38:18 @agent_ppo2.py:185][0m |          -0.0127 |          52.0863 |           4.2557 |
[32m[20221213 23:38:18 @agent_ppo2.py:185][0m |          -0.0163 |          50.8750 |           4.2714 |
[32m[20221213 23:38:18 @agent_ppo2.py:185][0m |          -0.0164 |          50.4742 |           4.2384 |
[32m[20221213 23:38:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.64
[32m[20221213 23:38:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.82
[32m[20221213 23:38:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.95
[32m[20221213 23:38:18 @agent_ppo2.py:143][0m Total time:      25.77 min
[32m[20221213 23:38:18 @agent_ppo2.py:145][0m 2494464 total steps have happened
[32m[20221213 23:38:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3218 --------------------------#
[32m[20221213 23:38:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:18 @agent_ppo2.py:185][0m |           0.0019 |          61.2528 |           4.3903 |
[32m[20221213 23:38:18 @agent_ppo2.py:185][0m |          -0.0062 |          54.3453 |           4.4618 |
[32m[20221213 23:38:18 @agent_ppo2.py:185][0m |          -0.0019 |          54.2568 |           4.4635 |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0097 |          52.5119 |           4.6048 |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0128 |          51.7698 |           4.6104 |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0107 |          51.3480 |           4.6234 |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0131 |          50.9831 |           4.6669 |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0126 |          50.6965 |           4.6554 |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0151 |          50.4783 |           4.6431 |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0115 |          50.3117 |           4.6757 |
[32m[20221213 23:38:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:38:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.89
[32m[20221213 23:38:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.16
[32m[20221213 23:38:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.91
[32m[20221213 23:38:19 @agent_ppo2.py:143][0m Total time:      25.79 min
[32m[20221213 23:38:19 @agent_ppo2.py:145][0m 2496512 total steps have happened
[32m[20221213 23:38:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3219 --------------------------#
[32m[20221213 23:38:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:19 @agent_ppo2.py:185][0m |          -0.0027 |          45.6905 |           4.1896 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0055 |          37.9073 |           4.1455 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0017 |          36.5579 |           4.1194 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0113 |          35.7923 |           4.1103 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0158 |          35.1444 |           4.1257 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0121 |          34.6612 |           4.1530 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0113 |          34.3324 |           4.0622 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0115 |          34.0653 |           4.0574 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0112 |          34.0235 |           4.0087 |
[32m[20221213 23:38:20 @agent_ppo2.py:185][0m |          -0.0201 |          33.7539 |           3.9914 |
[32m[20221213 23:38:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:38:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.27
[32m[20221213 23:38:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.04
[32m[20221213 23:38:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.31
[32m[20221213 23:38:20 @agent_ppo2.py:143][0m Total time:      25.81 min
[32m[20221213 23:38:20 @agent_ppo2.py:145][0m 2498560 total steps have happened
[32m[20221213 23:38:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3220 --------------------------#
[32m[20221213 23:38:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:38:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0017 |          67.9011 |           4.4151 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0065 |          64.6209 |           4.5198 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0078 |          63.4844 |           4.5433 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0114 |          62.7891 |           4.6246 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0029 |          65.4772 |           4.6394 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0113 |          61.8096 |           4.6617 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0109 |          61.2273 |           4.6549 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0165 |          61.1297 |           4.7088 |
[32m[20221213 23:38:21 @agent_ppo2.py:185][0m |          -0.0168 |          60.7100 |           4.7521 |
[32m[20221213 23:38:22 @agent_ppo2.py:185][0m |          -0.0165 |          60.4474 |           4.7450 |
[32m[20221213 23:38:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:38:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.93
[32m[20221213 23:38:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.23
[32m[20221213 23:38:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.50
[32m[20221213 23:38:22 @agent_ppo2.py:143][0m Total time:      25.83 min
[32m[20221213 23:38:22 @agent_ppo2.py:145][0m 2500608 total steps have happened
[32m[20221213 23:38:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3221 --------------------------#
[32m[20221213 23:38:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:22 @agent_ppo2.py:185][0m |          -0.0046 |          55.2875 |           4.2591 |
[32m[20221213 23:38:22 @agent_ppo2.py:185][0m |          -0.0111 |          50.7564 |           4.2559 |
[32m[20221213 23:38:22 @agent_ppo2.py:185][0m |          -0.0048 |          49.2579 |           4.2980 |
[32m[20221213 23:38:22 @agent_ppo2.py:185][0m |          -0.0069 |          48.1612 |           4.3642 |
[32m[20221213 23:38:22 @agent_ppo2.py:185][0m |          -0.0123 |          47.3910 |           4.3920 |
[32m[20221213 23:38:22 @agent_ppo2.py:185][0m |          -0.0081 |          46.8902 |           4.3634 |
[32m[20221213 23:38:23 @agent_ppo2.py:185][0m |          -0.0128 |          46.6089 |           4.3776 |
[32m[20221213 23:38:23 @agent_ppo2.py:185][0m |          -0.0140 |          46.1878 |           4.4602 |
[32m[20221213 23:38:23 @agent_ppo2.py:185][0m |          -0.0109 |          45.9840 |           4.4759 |
[32m[20221213 23:38:23 @agent_ppo2.py:185][0m |          -0.0139 |          45.6668 |           4.5069 |
[32m[20221213 23:38:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.11
[32m[20221213 23:38:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.58
[32m[20221213 23:38:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.22
[32m[20221213 23:38:23 @agent_ppo2.py:143][0m Total time:      25.85 min
[32m[20221213 23:38:23 @agent_ppo2.py:145][0m 2502656 total steps have happened
[32m[20221213 23:38:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3222 --------------------------#
[32m[20221213 23:38:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:23 @agent_ppo2.py:185][0m |           0.0022 |          49.1773 |           4.7922 |
[32m[20221213 23:38:23 @agent_ppo2.py:185][0m |          -0.0019 |          45.2344 |           4.7283 |
[32m[20221213 23:38:23 @agent_ppo2.py:185][0m |          -0.0077 |          44.0513 |           4.7864 |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |           0.0193 |          52.5540 |           4.8139 |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |          -0.0040 |          43.7928 |           4.7945 |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |           0.0088 |          49.5254 |           4.7447 |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |          -0.0107 |          42.4160 |           4.8482 |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |          -0.0108 |          42.0139 |           4.7438 |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |          -0.0130 |          41.8154 |           4.7782 |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |          -0.0132 |          41.6504 |           4.6508 |
[32m[20221213 23:38:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.76
[32m[20221213 23:38:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.59
[32m[20221213 23:38:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.07
[32m[20221213 23:38:24 @agent_ppo2.py:143][0m Total time:      25.87 min
[32m[20221213 23:38:24 @agent_ppo2.py:145][0m 2504704 total steps have happened
[32m[20221213 23:38:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3223 --------------------------#
[32m[20221213 23:38:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:24 @agent_ppo2.py:185][0m |           0.0105 |          81.8580 |           4.5455 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0038 |          73.6420 |           4.4361 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0092 |          72.8505 |           4.4867 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0086 |          72.5089 |           4.4423 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0137 |          72.2264 |           4.4029 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0124 |          71.8029 |           4.3642 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0127 |          71.4864 |           4.2688 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0040 |          73.1835 |           4.2610 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0128 |          71.3182 |           4.2563 |
[32m[20221213 23:38:25 @agent_ppo2.py:185][0m |          -0.0157 |          71.2652 |           4.1663 |
[32m[20221213 23:38:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.51
[32m[20221213 23:38:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.71
[32m[20221213 23:38:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.30
[32m[20221213 23:38:25 @agent_ppo2.py:143][0m Total time:      25.89 min
[32m[20221213 23:38:25 @agent_ppo2.py:145][0m 2506752 total steps have happened
[32m[20221213 23:38:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3224 --------------------------#
[32m[20221213 23:38:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |           0.0026 |          54.7659 |           4.5970 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |           0.0060 |          49.7241 |           4.7749 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |          -0.0045 |          46.0019 |           4.7518 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |          -0.0089 |          43.8719 |           4.8381 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |          -0.0119 |          42.3090 |           4.8775 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |          -0.0121 |          41.4665 |           4.9422 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |          -0.0165 |          40.8587 |           4.9216 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |          -0.0138 |          40.4129 |           4.9369 |
[32m[20221213 23:38:26 @agent_ppo2.py:185][0m |          -0.0125 |          40.2481 |           5.0703 |
[32m[20221213 23:38:27 @agent_ppo2.py:185][0m |          -0.0159 |          40.1427 |           5.1025 |
[32m[20221213 23:38:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:38:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.36
[32m[20221213 23:38:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.18
[32m[20221213 23:38:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.11
[32m[20221213 23:38:27 @agent_ppo2.py:143][0m Total time:      25.91 min
[32m[20221213 23:38:27 @agent_ppo2.py:145][0m 2508800 total steps have happened
[32m[20221213 23:38:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3225 --------------------------#
[32m[20221213 23:38:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:27 @agent_ppo2.py:185][0m |          -0.0013 |          70.3889 |           4.7151 |
[32m[20221213 23:38:27 @agent_ppo2.py:185][0m |          -0.0065 |          66.1679 |           4.7372 |
[32m[20221213 23:38:27 @agent_ppo2.py:185][0m |          -0.0050 |          64.3854 |           4.6938 |
[32m[20221213 23:38:27 @agent_ppo2.py:185][0m |          -0.0044 |          63.7749 |           4.7543 |
[32m[20221213 23:38:27 @agent_ppo2.py:185][0m |          -0.0103 |          62.7732 |           4.7884 |
[32m[20221213 23:38:27 @agent_ppo2.py:185][0m |          -0.0104 |          62.4845 |           4.8195 |
[32m[20221213 23:38:28 @agent_ppo2.py:185][0m |          -0.0141 |          62.0387 |           4.8334 |
[32m[20221213 23:38:28 @agent_ppo2.py:185][0m |          -0.0136 |          61.6709 |           4.8387 |
[32m[20221213 23:38:28 @agent_ppo2.py:185][0m |          -0.0125 |          61.3222 |           4.9163 |
[32m[20221213 23:38:28 @agent_ppo2.py:185][0m |          -0.0134 |          61.1103 |           4.8950 |
[32m[20221213 23:38:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:38:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.49
[32m[20221213 23:38:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.69
[32m[20221213 23:38:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.73
[32m[20221213 23:38:28 @agent_ppo2.py:143][0m Total time:      25.93 min
[32m[20221213 23:38:28 @agent_ppo2.py:145][0m 2510848 total steps have happened
[32m[20221213 23:38:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3226 --------------------------#
[32m[20221213 23:38:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:28 @agent_ppo2.py:185][0m |          -0.0016 |          38.3865 |           4.4647 |
[32m[20221213 23:38:28 @agent_ppo2.py:185][0m |          -0.0093 |          33.6275 |           4.5352 |
[32m[20221213 23:38:28 @agent_ppo2.py:185][0m |          -0.0150 |          31.4874 |           4.5407 |
[32m[20221213 23:38:29 @agent_ppo2.py:185][0m |          -0.0160 |          30.4857 |           4.5527 |
[32m[20221213 23:38:29 @agent_ppo2.py:185][0m |          -0.0112 |          29.6444 |           4.5521 |
[32m[20221213 23:38:29 @agent_ppo2.py:185][0m |          -0.0135 |          29.1907 |           4.4869 |
[32m[20221213 23:38:29 @agent_ppo2.py:185][0m |          -0.0166 |          28.6889 |           4.5467 |
[32m[20221213 23:38:29 @agent_ppo2.py:185][0m |          -0.0143 |          28.3529 |           4.5497 |
[32m[20221213 23:38:29 @agent_ppo2.py:185][0m |          -0.0149 |          28.2862 |           4.5771 |
[32m[20221213 23:38:29 @agent_ppo2.py:185][0m |          -0.0205 |          27.8694 |           4.6292 |
[32m[20221213 23:38:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.79
[32m[20221213 23:38:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.33
[32m[20221213 23:38:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.36
[32m[20221213 23:38:29 @agent_ppo2.py:143][0m Total time:      25.95 min
[32m[20221213 23:38:29 @agent_ppo2.py:145][0m 2512896 total steps have happened
[32m[20221213 23:38:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3227 --------------------------#
[32m[20221213 23:38:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0041 |          44.0677 |           5.4761 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0063 |          39.4016 |           5.4633 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0064 |          38.2726 |           5.4677 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0043 |          37.5046 |           5.4769 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0075 |          36.6094 |           5.4647 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0056 |          36.5851 |           5.4673 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0061 |          35.7887 |           5.4209 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0112 |          35.3456 |           5.3969 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |           0.0054 |          41.1252 |           5.3586 |
[32m[20221213 23:38:30 @agent_ppo2.py:185][0m |          -0.0093 |          35.0776 |           5.3900 |
[32m[20221213 23:38:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.02
[32m[20221213 23:38:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.75
[32m[20221213 23:38:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.87
[32m[20221213 23:38:30 @agent_ppo2.py:143][0m Total time:      25.97 min
[32m[20221213 23:38:30 @agent_ppo2.py:145][0m 2514944 total steps have happened
[32m[20221213 23:38:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3228 --------------------------#
[32m[20221213 23:38:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |           0.0008 |          54.7494 |           4.7555 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |          -0.0031 |          50.6649 |           4.8209 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |          -0.0102 |          49.1911 |           4.8797 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |           0.0001 |          51.8652 |           4.8011 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |          -0.0110 |          47.7919 |           4.8493 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |          -0.0142 |          46.8690 |           4.8386 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |          -0.0149 |          46.3389 |           4.8412 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |          -0.0083 |          49.0216 |           4.8336 |
[32m[20221213 23:38:31 @agent_ppo2.py:185][0m |          -0.0191 |          45.6987 |           4.9319 |
[32m[20221213 23:38:32 @agent_ppo2.py:185][0m |          -0.0155 |          45.1947 |           4.8576 |
[32m[20221213 23:38:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.46
[32m[20221213 23:38:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.49
[32m[20221213 23:38:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.28
[32m[20221213 23:38:32 @agent_ppo2.py:143][0m Total time:      26.00 min
[32m[20221213 23:38:32 @agent_ppo2.py:145][0m 2516992 total steps have happened
[32m[20221213 23:38:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3229 --------------------------#
[32m[20221213 23:38:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:32 @agent_ppo2.py:185][0m |           0.0008 |          51.8249 |           5.0936 |
[32m[20221213 23:38:32 @agent_ppo2.py:185][0m |           0.0029 |          44.5225 |           5.0293 |
[32m[20221213 23:38:32 @agent_ppo2.py:185][0m |          -0.0059 |          41.7488 |           5.0392 |
[32m[20221213 23:38:32 @agent_ppo2.py:185][0m |          -0.0063 |          40.1353 |           5.0868 |
[32m[20221213 23:38:32 @agent_ppo2.py:185][0m |          -0.0085 |          39.2368 |           5.1230 |
[32m[20221213 23:38:32 @agent_ppo2.py:185][0m |          -0.0062 |          39.3493 |           5.1427 |
[32m[20221213 23:38:33 @agent_ppo2.py:185][0m |          -0.0165 |          38.4928 |           5.1142 |
[32m[20221213 23:38:33 @agent_ppo2.py:185][0m |          -0.0063 |          37.8046 |           5.1051 |
[32m[20221213 23:38:33 @agent_ppo2.py:185][0m |          -0.0186 |          37.5685 |           5.1739 |
[32m[20221213 23:38:33 @agent_ppo2.py:185][0m |          -0.0170 |          36.3411 |           5.1574 |
[32m[20221213 23:38:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:38:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.47
[32m[20221213 23:38:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.81
[32m[20221213 23:38:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.64
[32m[20221213 23:38:33 @agent_ppo2.py:143][0m Total time:      26.02 min
[32m[20221213 23:38:33 @agent_ppo2.py:145][0m 2519040 total steps have happened
[32m[20221213 23:38:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3230 --------------------------#
[32m[20221213 23:38:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:38:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:33 @agent_ppo2.py:185][0m |           0.0061 |          60.1035 |           4.8064 |
[32m[20221213 23:38:33 @agent_ppo2.py:185][0m |          -0.0070 |          54.2108 |           4.9139 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |          -0.0037 |          52.9048 |           4.9014 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |          -0.0104 |          50.9967 |           4.8671 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |          -0.0091 |          50.4248 |           4.8817 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |          -0.0102 |          49.6934 |           4.9067 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |           0.0002 |          55.4310 |           4.8972 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |          -0.0133 |          49.3177 |           4.9445 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |          -0.0142 |          48.5604 |           4.9484 |
[32m[20221213 23:38:34 @agent_ppo2.py:185][0m |          -0.0155 |          48.1423 |           4.9414 |
[32m[20221213 23:38:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.69
[32m[20221213 23:38:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.74
[32m[20221213 23:38:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.66
[32m[20221213 23:38:34 @agent_ppo2.py:143][0m Total time:      26.04 min
[32m[20221213 23:38:34 @agent_ppo2.py:145][0m 2521088 total steps have happened
[32m[20221213 23:38:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3231 --------------------------#
[32m[20221213 23:38:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0008 |          47.7851 |           5.1081 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0011 |          41.1340 |           5.2041 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0095 |          39.4594 |           5.1964 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0095 |          38.1171 |           5.2211 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |           0.0009 |          41.1781 |           5.1205 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0123 |          36.8585 |           5.0753 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0059 |          37.1634 |           5.1450 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0014 |          39.4722 |           5.0636 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0126 |          36.0570 |           5.0995 |
[32m[20221213 23:38:35 @agent_ppo2.py:185][0m |          -0.0175 |          35.9742 |           5.0262 |
[32m[20221213 23:38:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.26
[32m[20221213 23:38:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.24
[32m[20221213 23:38:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.47
[32m[20221213 23:38:35 @agent_ppo2.py:143][0m Total time:      26.06 min
[32m[20221213 23:38:35 @agent_ppo2.py:145][0m 2523136 total steps have happened
[32m[20221213 23:38:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3232 --------------------------#
[32m[20221213 23:38:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0037 |          59.4411 |           4.0892 |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0072 |          53.8466 |           4.1440 |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0089 |          51.9646 |           4.2109 |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0110 |          50.6975 |           4.1778 |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0148 |          49.6915 |           4.1735 |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0145 |          49.0979 |           4.2664 |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0167 |          48.5177 |           4.2632 |
[32m[20221213 23:38:36 @agent_ppo2.py:185][0m |          -0.0071 |          51.7655 |           4.3108 |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |          -0.0131 |          47.7301 |           4.4138 |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |          -0.0136 |          47.3015 |           4.4131 |
[32m[20221213 23:38:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.11
[32m[20221213 23:38:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.75
[32m[20221213 23:38:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.38
[32m[20221213 23:38:37 @agent_ppo2.py:143][0m Total time:      26.08 min
[32m[20221213 23:38:37 @agent_ppo2.py:145][0m 2525184 total steps have happened
[32m[20221213 23:38:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3233 --------------------------#
[32m[20221213 23:38:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |           0.0044 |          45.0034 |           4.8644 |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |          -0.0036 |          37.9339 |           4.8876 |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |          -0.0073 |          36.3193 |           4.7173 |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |          -0.0070 |          35.3436 |           4.7937 |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |          -0.0024 |          34.7534 |           4.6942 |
[32m[20221213 23:38:37 @agent_ppo2.py:185][0m |          -0.0117 |          34.2070 |           4.7167 |
[32m[20221213 23:38:38 @agent_ppo2.py:185][0m |          -0.0153 |          33.8446 |           4.7128 |
[32m[20221213 23:38:38 @agent_ppo2.py:185][0m |          -0.0151 |          33.6491 |           4.7177 |
[32m[20221213 23:38:38 @agent_ppo2.py:185][0m |          -0.0096 |          34.3857 |           4.6259 |
[32m[20221213 23:38:38 @agent_ppo2.py:185][0m |          -0.0234 |          33.1580 |           4.5475 |
[32m[20221213 23:38:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.93
[32m[20221213 23:38:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.58
[32m[20221213 23:38:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.80
[32m[20221213 23:38:38 @agent_ppo2.py:143][0m Total time:      26.10 min
[32m[20221213 23:38:38 @agent_ppo2.py:145][0m 2527232 total steps have happened
[32m[20221213 23:38:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3234 --------------------------#
[32m[20221213 23:38:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:38 @agent_ppo2.py:185][0m |           0.0063 |          51.4413 |           4.1778 |
[32m[20221213 23:38:38 @agent_ppo2.py:185][0m |           0.0063 |          48.0737 |           4.1861 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |          -0.0058 |          44.6692 |           4.1786 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |          -0.0075 |          43.8621 |           4.1420 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |          -0.0081 |          43.4963 |           4.1312 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |          -0.0142 |          43.3292 |           4.1151 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |          -0.0107 |          43.0300 |           4.1695 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |           0.0017 |          50.4806 |           4.1855 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |          -0.0159 |          42.8408 |           4.1878 |
[32m[20221213 23:38:39 @agent_ppo2.py:185][0m |          -0.0166 |          42.2947 |           4.1915 |
[32m[20221213 23:38:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:38:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.38
[32m[20221213 23:38:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.96
[32m[20221213 23:38:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.42
[32m[20221213 23:38:39 @agent_ppo2.py:143][0m Total time:      26.12 min
[32m[20221213 23:38:39 @agent_ppo2.py:145][0m 2529280 total steps have happened
[32m[20221213 23:38:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3235 --------------------------#
[32m[20221213 23:38:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0002 |          51.7535 |           4.4240 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0040 |          48.1958 |           4.4007 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0035 |          47.1457 |           4.2990 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0110 |          46.3037 |           4.3241 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0082 |          45.8930 |           4.3999 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0069 |          45.3394 |           4.3883 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0092 |          44.8795 |           4.4253 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0061 |          46.3295 |           4.3909 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0115 |          44.3607 |           4.3713 |
[32m[20221213 23:38:40 @agent_ppo2.py:185][0m |          -0.0006 |          50.2962 |           4.3726 |
[32m[20221213 23:38:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.75
[32m[20221213 23:38:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.64
[32m[20221213 23:38:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.91
[32m[20221213 23:38:41 @agent_ppo2.py:143][0m Total time:      26.14 min
[32m[20221213 23:38:41 @agent_ppo2.py:145][0m 2531328 total steps have happened
[32m[20221213 23:38:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3236 --------------------------#
[32m[20221213 23:38:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |           0.0124 |          72.3130 |           4.4601 |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |          -0.0035 |          63.3100 |           4.5169 |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |          -0.0062 |          62.5681 |           4.4725 |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |          -0.0072 |          62.2300 |           4.4919 |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |          -0.0078 |          62.0258 |           4.5673 |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |          -0.0122 |          61.8527 |           4.4726 |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |          -0.0116 |          61.5184 |           4.4738 |
[32m[20221213 23:38:41 @agent_ppo2.py:185][0m |          -0.0058 |          62.3351 |           4.5004 |
[32m[20221213 23:38:42 @agent_ppo2.py:185][0m |          -0.0113 |          61.3656 |           4.5097 |
[32m[20221213 23:38:42 @agent_ppo2.py:185][0m |          -0.0060 |          63.2579 |           4.5263 |
[32m[20221213 23:38:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.19
[32m[20221213 23:38:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.07
[32m[20221213 23:38:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.90
[32m[20221213 23:38:42 @agent_ppo2.py:143][0m Total time:      26.16 min
[32m[20221213 23:38:42 @agent_ppo2.py:145][0m 2533376 total steps have happened
[32m[20221213 23:38:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3237 --------------------------#
[32m[20221213 23:38:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:42 @agent_ppo2.py:185][0m |           0.0020 |          46.7159 |           5.0539 |
[32m[20221213 23:38:42 @agent_ppo2.py:185][0m |          -0.0079 |          43.5391 |           5.0908 |
[32m[20221213 23:38:42 @agent_ppo2.py:185][0m |          -0.0038 |          43.4951 |           5.1737 |
[32m[20221213 23:38:42 @agent_ppo2.py:185][0m |          -0.0079 |          42.3048 |           5.1566 |
[32m[20221213 23:38:42 @agent_ppo2.py:185][0m |          -0.0144 |          41.8973 |           5.2494 |
[32m[20221213 23:38:43 @agent_ppo2.py:185][0m |          -0.0096 |          41.6633 |           5.2652 |
[32m[20221213 23:38:43 @agent_ppo2.py:185][0m |          -0.0120 |          41.4146 |           5.3333 |
[32m[20221213 23:38:43 @agent_ppo2.py:185][0m |          -0.0152 |          41.0195 |           5.3888 |
[32m[20221213 23:38:43 @agent_ppo2.py:185][0m |          -0.0165 |          40.9034 |           5.4088 |
[32m[20221213 23:38:43 @agent_ppo2.py:185][0m |          -0.0160 |          40.7376 |           5.4608 |
[32m[20221213 23:38:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.59
[32m[20221213 23:38:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.67
[32m[20221213 23:38:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.60
[32m[20221213 23:38:43 @agent_ppo2.py:143][0m Total time:      26.18 min
[32m[20221213 23:38:43 @agent_ppo2.py:145][0m 2535424 total steps have happened
[32m[20221213 23:38:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3238 --------------------------#
[32m[20221213 23:38:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:43 @agent_ppo2.py:185][0m |           0.0095 |          55.1904 |           5.1051 |
[32m[20221213 23:38:43 @agent_ppo2.py:185][0m |          -0.0025 |          48.9712 |           5.2164 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0024 |          47.2064 |           5.1474 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0057 |          45.9466 |           5.2641 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0065 |          45.2732 |           5.1412 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0026 |          44.7561 |           5.1581 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0087 |          44.6848 |           5.1499 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0079 |          43.7611 |           5.2261 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0168 |          43.3267 |           5.2054 |
[32m[20221213 23:38:44 @agent_ppo2.py:185][0m |          -0.0141 |          42.9949 |           5.2264 |
[32m[20221213 23:38:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.72
[32m[20221213 23:38:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.60
[32m[20221213 23:38:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.37
[32m[20221213 23:38:44 @agent_ppo2.py:143][0m Total time:      26.20 min
[32m[20221213 23:38:44 @agent_ppo2.py:145][0m 2537472 total steps have happened
[32m[20221213 23:38:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3239 --------------------------#
[32m[20221213 23:38:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0046 |          43.3261 |           4.6100 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0126 |          37.6152 |           4.7033 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0089 |          36.5818 |           4.6658 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0114 |          36.0586 |           4.6992 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0142 |          35.6579 |           4.6683 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0100 |          37.5508 |           4.6641 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0131 |          35.0518 |           4.7670 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0203 |          34.8114 |           4.7271 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0166 |          34.3149 |           4.7108 |
[32m[20221213 23:38:45 @agent_ppo2.py:185][0m |          -0.0127 |          34.2574 |           4.7092 |
[32m[20221213 23:38:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:38:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.72
[32m[20221213 23:38:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.72
[32m[20221213 23:38:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.96
[32m[20221213 23:38:46 @agent_ppo2.py:143][0m Total time:      26.23 min
[32m[20221213 23:38:46 @agent_ppo2.py:145][0m 2539520 total steps have happened
[32m[20221213 23:38:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3240 --------------------------#
[32m[20221213 23:38:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:38:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |           0.0006 |          61.0395 |           5.6826 |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |          -0.0062 |          57.3430 |           5.7285 |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |          -0.0059 |          56.1262 |           5.7631 |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |           0.0030 |          61.7659 |           5.7596 |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |          -0.0084 |          54.6552 |           5.7655 |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |          -0.0145 |          54.2019 |           5.7440 |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |          -0.0114 |          53.7722 |           5.8130 |
[32m[20221213 23:38:46 @agent_ppo2.py:185][0m |          -0.0094 |          53.3994 |           5.7810 |
[32m[20221213 23:38:47 @agent_ppo2.py:185][0m |          -0.0097 |          53.2751 |           5.8060 |
[32m[20221213 23:38:47 @agent_ppo2.py:185][0m |          -0.0106 |          52.7729 |           5.8865 |
[32m[20221213 23:38:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.50
[32m[20221213 23:38:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.65
[32m[20221213 23:38:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.40
[32m[20221213 23:38:47 @agent_ppo2.py:143][0m Total time:      26.25 min
[32m[20221213 23:38:47 @agent_ppo2.py:145][0m 2541568 total steps have happened
[32m[20221213 23:38:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3241 --------------------------#
[32m[20221213 23:38:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:47 @agent_ppo2.py:185][0m |          -0.0032 |          59.1911 |           5.6454 |
[32m[20221213 23:38:47 @agent_ppo2.py:185][0m |          -0.0034 |          55.3796 |           5.6460 |
[32m[20221213 23:38:47 @agent_ppo2.py:185][0m |          -0.0084 |          53.3150 |           5.6444 |
[32m[20221213 23:38:47 @agent_ppo2.py:185][0m |          -0.0061 |          52.3330 |           5.7052 |
[32m[20221213 23:38:47 @agent_ppo2.py:185][0m |          -0.0076 |          51.4491 |           5.7293 |
[32m[20221213 23:38:48 @agent_ppo2.py:185][0m |          -0.0118 |          50.9237 |           5.7162 |
[32m[20221213 23:38:48 @agent_ppo2.py:185][0m |          -0.0099 |          50.3552 |           5.7402 |
[32m[20221213 23:38:48 @agent_ppo2.py:185][0m |          -0.0117 |          50.2690 |           5.6457 |
[32m[20221213 23:38:48 @agent_ppo2.py:185][0m |          -0.0116 |          49.7148 |           5.7604 |
[32m[20221213 23:38:48 @agent_ppo2.py:185][0m |          -0.0153 |          49.4526 |           5.7940 |
[32m[20221213 23:38:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.95
[32m[20221213 23:38:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.43
[32m[20221213 23:38:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.94
[32m[20221213 23:38:48 @agent_ppo2.py:143][0m Total time:      26.27 min
[32m[20221213 23:38:48 @agent_ppo2.py:145][0m 2543616 total steps have happened
[32m[20221213 23:38:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3242 --------------------------#
[32m[20221213 23:38:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:48 @agent_ppo2.py:185][0m |           0.0098 |          47.4302 |           5.4996 |
[32m[20221213 23:38:48 @agent_ppo2.py:185][0m |          -0.0070 |          40.7378 |           5.5226 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0117 |          39.7160 |           5.7327 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0141 |          39.1642 |           5.7371 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0199 |          38.8533 |           5.7544 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0163 |          38.6388 |           5.7735 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0194 |          38.3180 |           5.8476 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0091 |          42.2576 |           5.8833 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0163 |          37.9670 |           5.9093 |
[32m[20221213 23:38:49 @agent_ppo2.py:185][0m |          -0.0233 |          37.7658 |           5.9326 |
[32m[20221213 23:38:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:38:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.89
[32m[20221213 23:38:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.78
[32m[20221213 23:38:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.39
[32m[20221213 23:38:49 @agent_ppo2.py:143][0m Total time:      26.29 min
[32m[20221213 23:38:49 @agent_ppo2.py:145][0m 2545664 total steps have happened
[32m[20221213 23:38:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3243 --------------------------#
[32m[20221213 23:38:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |           0.0044 |          54.5922 |           5.9994 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0063 |          50.7370 |           5.8614 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0093 |          49.4010 |           5.8726 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0110 |          49.3422 |           5.8867 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |           0.0209 |          57.1593 |           5.8901 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0149 |          47.7736 |           5.8693 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0133 |          47.4225 |           5.8793 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0148 |          47.0722 |           5.8369 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0166 |          46.7948 |           5.8263 |
[32m[20221213 23:38:50 @agent_ppo2.py:185][0m |          -0.0194 |          46.5321 |           5.8797 |
[32m[20221213 23:38:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:38:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.02
[32m[20221213 23:38:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.57
[32m[20221213 23:38:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.03
[32m[20221213 23:38:51 @agent_ppo2.py:143][0m Total time:      26.31 min
[32m[20221213 23:38:51 @agent_ppo2.py:145][0m 2547712 total steps have happened
[32m[20221213 23:38:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3244 --------------------------#
[32m[20221213 23:38:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:51 @agent_ppo2.py:185][0m |          -0.0008 |          66.0722 |           5.6700 |
[32m[20221213 23:38:51 @agent_ppo2.py:185][0m |          -0.0036 |          61.9686 |           5.7029 |
[32m[20221213 23:38:51 @agent_ppo2.py:185][0m |          -0.0106 |          60.1429 |           5.7129 |
[32m[20221213 23:38:51 @agent_ppo2.py:185][0m |          -0.0087 |          59.2100 |           5.6837 |
[32m[20221213 23:38:51 @agent_ppo2.py:185][0m |          -0.0133 |          58.4611 |           5.7497 |
[32m[20221213 23:38:51 @agent_ppo2.py:185][0m |          -0.0149 |          58.0638 |           5.7993 |
[32m[20221213 23:38:51 @agent_ppo2.py:185][0m |          -0.0053 |          61.5940 |           5.8284 |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |          -0.0118 |          57.2657 |           5.8695 |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |          -0.0068 |          57.9088 |           5.9456 |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |          -0.0140 |          56.5771 |           5.9002 |
[32m[20221213 23:38:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:38:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.07
[32m[20221213 23:38:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.75
[32m[20221213 23:38:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.16
[32m[20221213 23:38:52 @agent_ppo2.py:143][0m Total time:      26.33 min
[32m[20221213 23:38:52 @agent_ppo2.py:145][0m 2549760 total steps have happened
[32m[20221213 23:38:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3245 --------------------------#
[32m[20221213 23:38:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |           0.0036 |          50.3464 |           5.3890 |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |          -0.0117 |          42.4444 |           5.4207 |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |          -0.0096 |          39.9424 |           5.4061 |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |          -0.0164 |          38.7885 |           5.4109 |
[32m[20221213 23:38:52 @agent_ppo2.py:185][0m |          -0.0169 |          37.9102 |           5.3641 |
[32m[20221213 23:38:53 @agent_ppo2.py:185][0m |          -0.0142 |          37.8955 |           5.3823 |
[32m[20221213 23:38:53 @agent_ppo2.py:185][0m |          -0.0123 |          36.5505 |           5.4111 |
[32m[20221213 23:38:53 @agent_ppo2.py:185][0m |          -0.0156 |          36.0391 |           5.3199 |
[32m[20221213 23:38:53 @agent_ppo2.py:185][0m |          -0.0177 |          35.5810 |           5.3180 |
[32m[20221213 23:38:53 @agent_ppo2.py:185][0m |          -0.0177 |          35.2763 |           5.3303 |
[32m[20221213 23:38:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.15
[32m[20221213 23:38:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.38
[32m[20221213 23:38:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.31
[32m[20221213 23:38:53 @agent_ppo2.py:143][0m Total time:      26.35 min
[32m[20221213 23:38:53 @agent_ppo2.py:145][0m 2551808 total steps have happened
[32m[20221213 23:38:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3246 --------------------------#
[32m[20221213 23:38:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:53 @agent_ppo2.py:185][0m |          -0.0059 |          60.9855 |           5.6759 |
[32m[20221213 23:38:53 @agent_ppo2.py:185][0m |          -0.0012 |          59.0121 |           5.6453 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0073 |          58.1861 |           5.6669 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0058 |          57.9963 |           5.6826 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0032 |          59.1083 |           5.7026 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0039 |          60.0695 |           5.6807 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0097 |          57.3115 |           5.6667 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0146 |          57.1734 |           5.7258 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0109 |          57.0049 |           5.6788 |
[32m[20221213 23:38:54 @agent_ppo2.py:185][0m |          -0.0119 |          56.9616 |           5.7511 |
[32m[20221213 23:38:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:38:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.37
[32m[20221213 23:38:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.85
[32m[20221213 23:38:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.31
[32m[20221213 23:38:54 @agent_ppo2.py:143][0m Total time:      26.37 min
[32m[20221213 23:38:54 @agent_ppo2.py:145][0m 2553856 total steps have happened
[32m[20221213 23:38:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3247 --------------------------#
[32m[20221213 23:38:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0026 |          59.6628 |           6.3910 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0089 |          56.1325 |           6.3884 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0086 |          55.1030 |           6.4399 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0131 |          54.2435 |           6.4399 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0096 |          53.7844 |           6.4931 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0086 |          53.3906 |           6.5011 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0105 |          53.3419 |           6.4958 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0120 |          52.8744 |           6.5322 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0146 |          52.4773 |           6.5702 |
[32m[20221213 23:38:55 @agent_ppo2.py:185][0m |          -0.0102 |          53.4411 |           6.5780 |
[32m[20221213 23:38:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.55
[32m[20221213 23:38:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.80
[32m[20221213 23:38:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.44
[32m[20221213 23:38:56 @agent_ppo2.py:143][0m Total time:      26.39 min
[32m[20221213 23:38:56 @agent_ppo2.py:145][0m 2555904 total steps have happened
[32m[20221213 23:38:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3248 --------------------------#
[32m[20221213 23:38:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:38:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:56 @agent_ppo2.py:185][0m |           0.0015 |          54.4963 |           5.9047 |
[32m[20221213 23:38:56 @agent_ppo2.py:185][0m |          -0.0039 |          50.9495 |           5.8905 |
[32m[20221213 23:38:56 @agent_ppo2.py:185][0m |          -0.0082 |          49.1672 |           5.9980 |
[32m[20221213 23:38:56 @agent_ppo2.py:185][0m |          -0.0074 |          47.9728 |           5.9375 |
[32m[20221213 23:38:56 @agent_ppo2.py:185][0m |          -0.0095 |          47.3094 |           5.9666 |
[32m[20221213 23:38:56 @agent_ppo2.py:185][0m |          -0.0101 |          46.9189 |           5.9241 |
[32m[20221213 23:38:56 @agent_ppo2.py:185][0m |          -0.0116 |          46.3804 |           5.8771 |
[32m[20221213 23:38:57 @agent_ppo2.py:185][0m |          -0.0105 |          46.1047 |           5.9474 |
[32m[20221213 23:38:57 @agent_ppo2.py:185][0m |          -0.0122 |          45.6294 |           5.8777 |
[32m[20221213 23:38:57 @agent_ppo2.py:185][0m |          -0.0127 |          45.4152 |           5.9394 |
[32m[20221213 23:38:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:38:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.09
[32m[20221213 23:38:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.09
[32m[20221213 23:38:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.31
[32m[20221213 23:38:57 @agent_ppo2.py:143][0m Total time:      26.41 min
[32m[20221213 23:38:57 @agent_ppo2.py:145][0m 2557952 total steps have happened
[32m[20221213 23:38:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3249 --------------------------#
[32m[20221213 23:38:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:38:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:57 @agent_ppo2.py:185][0m |           0.0006 |          52.9980 |           5.8491 |
[32m[20221213 23:38:57 @agent_ppo2.py:185][0m |          -0.0037 |          47.4001 |           5.7853 |
[32m[20221213 23:38:57 @agent_ppo2.py:185][0m |          -0.0092 |          46.2164 |           5.7888 |
[32m[20221213 23:38:57 @agent_ppo2.py:185][0m |          -0.0140 |          45.2587 |           5.7233 |
[32m[20221213 23:38:58 @agent_ppo2.py:185][0m |          -0.0118 |          44.7715 |           5.7444 |
[32m[20221213 23:38:58 @agent_ppo2.py:185][0m |          -0.0163 |          44.1341 |           5.7357 |
[32m[20221213 23:38:58 @agent_ppo2.py:185][0m |          -0.0165 |          43.8521 |           5.7265 |
[32m[20221213 23:38:58 @agent_ppo2.py:185][0m |          -0.0118 |          43.4303 |           5.6767 |
[32m[20221213 23:38:58 @agent_ppo2.py:185][0m |          -0.0144 |          43.2310 |           5.6727 |
[32m[20221213 23:38:58 @agent_ppo2.py:185][0m |          -0.0142 |          43.0056 |           5.7211 |
[32m[20221213 23:38:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.65
[32m[20221213 23:38:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.78
[32m[20221213 23:38:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.04
[32m[20221213 23:38:58 @agent_ppo2.py:143][0m Total time:      26.43 min
[32m[20221213 23:38:58 @agent_ppo2.py:145][0m 2560000 total steps have happened
[32m[20221213 23:38:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3250 --------------------------#
[32m[20221213 23:38:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:38:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:38:58 @agent_ppo2.py:185][0m |           0.0021 |          70.1578 |           5.4295 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |           0.0012 |          68.8402 |           5.5080 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0019 |          68.5687 |           5.4714 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0037 |          68.3564 |           5.4482 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0076 |          67.9953 |           5.4830 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0058 |          68.0255 |           5.4918 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0019 |          68.3252 |           5.5141 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0082 |          67.7564 |           5.4839 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0057 |          67.6809 |           5.5576 |
[32m[20221213 23:38:59 @agent_ppo2.py:185][0m |          -0.0070 |          67.5801 |           5.4803 |
[32m[20221213 23:38:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:38:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.64
[32m[20221213 23:38:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.27
[32m[20221213 23:38:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.81
[32m[20221213 23:38:59 @agent_ppo2.py:143][0m Total time:      26.46 min
[32m[20221213 23:38:59 @agent_ppo2.py:145][0m 2562048 total steps have happened
[32m[20221213 23:38:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3251 --------------------------#
[32m[20221213 23:38:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |           0.0014 |          60.1446 |           5.9659 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0071 |          58.2952 |           5.9616 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |           0.0001 |          59.8071 |           5.9234 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0144 |          57.1941 |           6.0028 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0115 |          56.7456 |           6.0020 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0111 |          56.4074 |           5.9633 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0120 |          56.4950 |           6.0326 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0166 |          56.0905 |           5.9867 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0081 |          58.5723 |           6.0214 |
[32m[20221213 23:39:00 @agent_ppo2.py:185][0m |          -0.0130 |          56.0318 |           5.9538 |
[32m[20221213 23:39:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.62
[32m[20221213 23:39:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.35
[32m[20221213 23:39:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 528.63
[32m[20221213 23:39:01 @agent_ppo2.py:143][0m Total time:      26.48 min
[32m[20221213 23:39:01 @agent_ppo2.py:145][0m 2564096 total steps have happened
[32m[20221213 23:39:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3252 --------------------------#
[32m[20221213 23:39:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:39:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:01 @agent_ppo2.py:185][0m |           0.0055 |          63.1478 |           6.0665 |
[32m[20221213 23:39:01 @agent_ppo2.py:185][0m |          -0.0011 |          62.1633 |           6.0562 |
[32m[20221213 23:39:01 @agent_ppo2.py:185][0m |          -0.0026 |          61.9465 |           6.1444 |
[32m[20221213 23:39:01 @agent_ppo2.py:185][0m |           0.0005 |          62.4756 |           6.2013 |
[32m[20221213 23:39:01 @agent_ppo2.py:185][0m |          -0.0041 |          61.4438 |           6.1459 |
[32m[20221213 23:39:01 @agent_ppo2.py:185][0m |           0.0035 |          64.1777 |           6.1360 |
[32m[20221213 23:39:01 @agent_ppo2.py:185][0m |          -0.0064 |          61.3101 |           6.2248 |
[32m[20221213 23:39:02 @agent_ppo2.py:185][0m |          -0.0068 |          61.0367 |           6.2118 |
[32m[20221213 23:39:02 @agent_ppo2.py:185][0m |           0.0048 |          62.8081 |           6.2237 |
[32m[20221213 23:39:02 @agent_ppo2.py:185][0m |          -0.0068 |          60.9308 |           6.1926 |
[32m[20221213 23:39:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:39:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.45
[32m[20221213 23:39:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.67
[32m[20221213 23:39:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.61
[32m[20221213 23:39:02 @agent_ppo2.py:143][0m Total time:      26.50 min
[32m[20221213 23:39:02 @agent_ppo2.py:145][0m 2566144 total steps have happened
[32m[20221213 23:39:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3253 --------------------------#
[32m[20221213 23:39:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:02 @agent_ppo2.py:185][0m |           0.0018 |          67.8885 |           6.1702 |
[32m[20221213 23:39:02 @agent_ppo2.py:185][0m |           0.0080 |          76.4704 |           6.2801 |
[32m[20221213 23:39:02 @agent_ppo2.py:185][0m |          -0.0048 |          67.7431 |           6.3331 |
[32m[20221213 23:39:02 @agent_ppo2.py:185][0m |          -0.0049 |          67.1008 |           6.3818 |
[32m[20221213 23:39:03 @agent_ppo2.py:185][0m |          -0.0003 |          70.8786 |           6.3904 |
[32m[20221213 23:39:03 @agent_ppo2.py:185][0m |          -0.0100 |          66.2402 |           6.3309 |
[32m[20221213 23:39:03 @agent_ppo2.py:185][0m |          -0.0107 |          66.1609 |           6.5310 |
[32m[20221213 23:39:03 @agent_ppo2.py:185][0m |          -0.0099 |          65.9545 |           6.4961 |
[32m[20221213 23:39:03 @agent_ppo2.py:185][0m |          -0.0007 |          70.4359 |           6.4812 |
[32m[20221213 23:39:03 @agent_ppo2.py:185][0m |          -0.0013 |          68.2111 |           6.5712 |
[32m[20221213 23:39:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:39:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.62
[32m[20221213 23:39:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.15
[32m[20221213 23:39:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.69
[32m[20221213 23:39:03 @agent_ppo2.py:143][0m Total time:      26.52 min
[32m[20221213 23:39:03 @agent_ppo2.py:145][0m 2568192 total steps have happened
[32m[20221213 23:39:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3254 --------------------------#
[32m[20221213 23:39:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:03 @agent_ppo2.py:185][0m |           0.0019 |          69.7521 |           6.7907 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |           0.0018 |          67.5005 |           6.8057 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0097 |          65.4540 |           6.7177 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0111 |          64.9519 |           6.7683 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0167 |          64.1467 |           6.7241 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0129 |          63.3627 |           6.7414 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0159 |          63.0082 |           6.7423 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0049 |          67.6001 |           6.6777 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0164 |          63.0903 |           6.7557 |
[32m[20221213 23:39:04 @agent_ppo2.py:185][0m |          -0.0146 |          61.9528 |           6.7584 |
[32m[20221213 23:39:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.23
[32m[20221213 23:39:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.30
[32m[20221213 23:39:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.81
[32m[20221213 23:39:04 @agent_ppo2.py:143][0m Total time:      26.54 min
[32m[20221213 23:39:04 @agent_ppo2.py:145][0m 2570240 total steps have happened
[32m[20221213 23:39:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3255 --------------------------#
[32m[20221213 23:39:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |           0.0058 |          64.0759 |           6.9778 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |          -0.0033 |          62.2390 |           7.0161 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |           0.0098 |          68.6276 |           6.9328 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |           0.0003 |          64.9794 |           6.9740 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |          -0.0056 |          61.4525 |           7.0240 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |          -0.0056 |          61.2901 |           6.9688 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |          -0.0093 |          61.2151 |           6.9546 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |          -0.0037 |          62.6926 |           6.9805 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |          -0.0077 |          60.9179 |           7.0107 |
[32m[20221213 23:39:05 @agent_ppo2.py:185][0m |          -0.0087 |          60.8848 |           6.9627 |
[32m[20221213 23:39:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.06
[32m[20221213 23:39:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.08
[32m[20221213 23:39:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.75
[32m[20221213 23:39:06 @agent_ppo2.py:143][0m Total time:      26.56 min
[32m[20221213 23:39:06 @agent_ppo2.py:145][0m 2572288 total steps have happened
[32m[20221213 23:39:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3256 --------------------------#
[32m[20221213 23:39:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:06 @agent_ppo2.py:185][0m |           0.0083 |          67.7457 |           6.2534 |
[32m[20221213 23:39:06 @agent_ppo2.py:185][0m |          -0.0060 |          59.9687 |           6.2477 |
[32m[20221213 23:39:06 @agent_ppo2.py:185][0m |          -0.0109 |          58.9817 |           6.3360 |
[32m[20221213 23:39:06 @agent_ppo2.py:185][0m |          -0.0102 |          59.2078 |           6.2086 |
[32m[20221213 23:39:06 @agent_ppo2.py:185][0m |          -0.0166 |          58.1860 |           6.2718 |
[32m[20221213 23:39:06 @agent_ppo2.py:185][0m |          -0.0093 |          59.3347 |           6.2139 |
[32m[20221213 23:39:06 @agent_ppo2.py:185][0m |          -0.0157 |          57.4839 |           6.2104 |
[32m[20221213 23:39:07 @agent_ppo2.py:185][0m |          -0.0080 |          60.1734 |           6.2199 |
[32m[20221213 23:39:07 @agent_ppo2.py:185][0m |          -0.0173 |          56.8894 |           6.2189 |
[32m[20221213 23:39:07 @agent_ppo2.py:185][0m |          -0.0188 |          56.5115 |           6.2465 |
[32m[20221213 23:39:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.19
[32m[20221213 23:39:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.99
[32m[20221213 23:39:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.32
[32m[20221213 23:39:07 @agent_ppo2.py:143][0m Total time:      26.58 min
[32m[20221213 23:39:07 @agent_ppo2.py:145][0m 2574336 total steps have happened
[32m[20221213 23:39:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3257 --------------------------#
[32m[20221213 23:39:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:07 @agent_ppo2.py:185][0m |          -0.0004 |          61.7720 |           6.3293 |
[32m[20221213 23:39:07 @agent_ppo2.py:185][0m |          -0.0042 |          58.3242 |           6.3676 |
[32m[20221213 23:39:07 @agent_ppo2.py:185][0m |          -0.0064 |          56.8540 |           6.3369 |
[32m[20221213 23:39:07 @agent_ppo2.py:185][0m |          -0.0098 |          55.8416 |           6.4862 |
[32m[20221213 23:39:08 @agent_ppo2.py:185][0m |          -0.0029 |          58.0724 |           6.4203 |
[32m[20221213 23:39:08 @agent_ppo2.py:185][0m |          -0.0132 |          54.6691 |           6.4137 |
[32m[20221213 23:39:08 @agent_ppo2.py:185][0m |          -0.0141 |          54.2997 |           6.4493 |
[32m[20221213 23:39:08 @agent_ppo2.py:185][0m |          -0.0112 |          54.3274 |           6.4270 |
[32m[20221213 23:39:08 @agent_ppo2.py:185][0m |          -0.0180 |          53.6059 |           6.4445 |
[32m[20221213 23:39:08 @agent_ppo2.py:185][0m |          -0.0004 |          61.4068 |           6.4458 |
[32m[20221213 23:39:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.82
[32m[20221213 23:39:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.98
[32m[20221213 23:39:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.05
[32m[20221213 23:39:08 @agent_ppo2.py:143][0m Total time:      26.60 min
[32m[20221213 23:39:08 @agent_ppo2.py:145][0m 2576384 total steps have happened
[32m[20221213 23:39:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3258 --------------------------#
[32m[20221213 23:39:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:08 @agent_ppo2.py:185][0m |           0.0032 |          67.6093 |           6.2603 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0071 |          62.8240 |           6.2726 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0064 |          61.0020 |           6.3088 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0101 |          59.8462 |           6.2701 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0098 |          59.3794 |           6.2782 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0140 |          58.7834 |           6.2760 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0052 |          61.9562 |           6.2712 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0094 |          58.3647 |           6.3054 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0152 |          57.7819 |           6.3002 |
[32m[20221213 23:39:09 @agent_ppo2.py:185][0m |          -0.0148 |          57.4764 |           6.2807 |
[32m[20221213 23:39:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:39:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.28
[32m[20221213 23:39:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.92
[32m[20221213 23:39:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.13
[32m[20221213 23:39:09 @agent_ppo2.py:143][0m Total time:      26.62 min
[32m[20221213 23:39:09 @agent_ppo2.py:145][0m 2578432 total steps have happened
[32m[20221213 23:39:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3259 --------------------------#
[32m[20221213 23:39:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |           0.0034 |          62.3146 |           6.5340 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0055 |          58.8373 |           6.4429 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0038 |          57.6972 |           6.5319 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0082 |          56.4139 |           6.5412 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0109 |          55.6471 |           6.4880 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0123 |          55.1090 |           6.4595 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0042 |          57.1384 |           6.5052 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0135 |          54.8619 |           6.4231 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0162 |          53.8864 |           6.5072 |
[32m[20221213 23:39:10 @agent_ppo2.py:185][0m |          -0.0177 |          53.5066 |           6.4440 |
[32m[20221213 23:39:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.11
[32m[20221213 23:39:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.17
[32m[20221213 23:39:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.92
[32m[20221213 23:39:11 @agent_ppo2.py:143][0m Total time:      26.64 min
[32m[20221213 23:39:11 @agent_ppo2.py:145][0m 2580480 total steps have happened
[32m[20221213 23:39:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3260 --------------------------#
[32m[20221213 23:39:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:39:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:11 @agent_ppo2.py:185][0m |           0.0032 |          63.9138 |           6.0241 |
[32m[20221213 23:39:11 @agent_ppo2.py:185][0m |          -0.0069 |          60.1428 |           6.0496 |
[32m[20221213 23:39:11 @agent_ppo2.py:185][0m |          -0.0056 |          59.4370 |           5.9933 |
[32m[20221213 23:39:11 @agent_ppo2.py:185][0m |           0.0078 |          67.6780 |           5.9731 |
[32m[20221213 23:39:11 @agent_ppo2.py:185][0m |          -0.0056 |          58.1161 |           6.0032 |
[32m[20221213 23:39:11 @agent_ppo2.py:185][0m |          -0.0140 |          57.5574 |           5.9737 |
[32m[20221213 23:39:11 @agent_ppo2.py:185][0m |          -0.0097 |          58.3828 |           5.9609 |
[32m[20221213 23:39:12 @agent_ppo2.py:185][0m |          -0.0147 |          57.1102 |           5.9386 |
[32m[20221213 23:39:12 @agent_ppo2.py:185][0m |          -0.0061 |          59.7374 |           5.8792 |
[32m[20221213 23:39:12 @agent_ppo2.py:185][0m |          -0.0109 |          56.7517 |           5.9239 |
[32m[20221213 23:39:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.54
[32m[20221213 23:39:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.72
[32m[20221213 23:39:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.81
[32m[20221213 23:39:12 @agent_ppo2.py:143][0m Total time:      26.66 min
[32m[20221213 23:39:12 @agent_ppo2.py:145][0m 2582528 total steps have happened
[32m[20221213 23:39:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3261 --------------------------#
[32m[20221213 23:39:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:12 @agent_ppo2.py:185][0m |           0.0061 |          51.3415 |           5.7101 |
[32m[20221213 23:39:12 @agent_ppo2.py:185][0m |          -0.0001 |          46.2248 |           5.8056 |
[32m[20221213 23:39:12 @agent_ppo2.py:185][0m |          -0.0083 |          44.7749 |           5.8094 |
[32m[20221213 23:39:12 @agent_ppo2.py:185][0m |          -0.0090 |          44.3593 |           5.7401 |
[32m[20221213 23:39:13 @agent_ppo2.py:185][0m |          -0.0121 |          43.1468 |           5.7521 |
[32m[20221213 23:39:13 @agent_ppo2.py:185][0m |          -0.0124 |          42.5302 |           5.8169 |
[32m[20221213 23:39:13 @agent_ppo2.py:185][0m |          -0.0082 |          42.1430 |           5.7924 |
[32m[20221213 23:39:13 @agent_ppo2.py:185][0m |          -0.0166 |          41.6533 |           5.8847 |
[32m[20221213 23:39:13 @agent_ppo2.py:185][0m |          -0.0172 |          41.3449 |           5.8362 |
[32m[20221213 23:39:13 @agent_ppo2.py:185][0m |          -0.0148 |          41.2466 |           5.8611 |
[32m[20221213 23:39:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.14
[32m[20221213 23:39:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.74
[32m[20221213 23:39:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.79
[32m[20221213 23:39:13 @agent_ppo2.py:143][0m Total time:      26.68 min
[32m[20221213 23:39:13 @agent_ppo2.py:145][0m 2584576 total steps have happened
[32m[20221213 23:39:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3262 --------------------------#
[32m[20221213 23:39:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:13 @agent_ppo2.py:185][0m |          -0.0025 |          51.4644 |           6.4099 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0080 |          48.3571 |           6.3635 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0094 |          47.3613 |           6.4288 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0093 |          46.9688 |           6.5097 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0102 |          46.5206 |           6.4571 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0119 |          46.3721 |           6.4544 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0143 |          46.1396 |           6.4205 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0134 |          46.1538 |           6.4916 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0138 |          45.9352 |           6.5099 |
[32m[20221213 23:39:14 @agent_ppo2.py:185][0m |          -0.0136 |          45.7340 |           6.5504 |
[32m[20221213 23:39:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.24
[32m[20221213 23:39:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.66
[32m[20221213 23:39:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.06
[32m[20221213 23:39:14 @agent_ppo2.py:143][0m Total time:      26.71 min
[32m[20221213 23:39:14 @agent_ppo2.py:145][0m 2586624 total steps have happened
[32m[20221213 23:39:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3263 --------------------------#
[32m[20221213 23:39:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |           0.0034 |          63.3748 |           5.9749 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |           0.0011 |          61.2303 |           5.9187 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0069 |          60.3568 |           5.9678 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0020 |          59.8742 |           6.0063 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0079 |          59.3964 |           5.9495 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0088 |          59.5238 |           5.9673 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0102 |          59.0218 |           6.0002 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0112 |          58.8686 |           5.9714 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0070 |          58.8357 |           6.0084 |
[32m[20221213 23:39:15 @agent_ppo2.py:185][0m |          -0.0097 |          58.8884 |           5.9951 |
[32m[20221213 23:39:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:39:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.08
[32m[20221213 23:39:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.98
[32m[20221213 23:39:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.68
[32m[20221213 23:39:16 @agent_ppo2.py:143][0m Total time:      26.73 min
[32m[20221213 23:39:16 @agent_ppo2.py:145][0m 2588672 total steps have happened
[32m[20221213 23:39:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3264 --------------------------#
[32m[20221213 23:39:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:39:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:16 @agent_ppo2.py:185][0m |          -0.0006 |          68.8733 |           5.8591 |
[32m[20221213 23:39:16 @agent_ppo2.py:185][0m |          -0.0069 |          66.2712 |           5.8198 |
[32m[20221213 23:39:16 @agent_ppo2.py:185][0m |          -0.0093 |          65.3918 |           5.9534 |
[32m[20221213 23:39:16 @agent_ppo2.py:185][0m |          -0.0100 |          65.1691 |           5.9908 |
[32m[20221213 23:39:16 @agent_ppo2.py:185][0m |           0.0022 |          71.0334 |           6.0529 |
[32m[20221213 23:39:16 @agent_ppo2.py:185][0m |          -0.0110 |          64.6645 |           6.0517 |
[32m[20221213 23:39:16 @agent_ppo2.py:185][0m |          -0.0134 |          64.2009 |           6.1209 |
[32m[20221213 23:39:17 @agent_ppo2.py:185][0m |          -0.0105 |          64.1366 |           6.1267 |
[32m[20221213 23:39:17 @agent_ppo2.py:185][0m |          -0.0139 |          63.9819 |           6.1956 |
[32m[20221213 23:39:17 @agent_ppo2.py:185][0m |          -0.0129 |          63.7575 |           6.2606 |
[32m[20221213 23:39:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.07
[32m[20221213 23:39:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.37
[32m[20221213 23:39:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.90
[32m[20221213 23:39:17 @agent_ppo2.py:143][0m Total time:      26.75 min
[32m[20221213 23:39:17 @agent_ppo2.py:145][0m 2590720 total steps have happened
[32m[20221213 23:39:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3265 --------------------------#
[32m[20221213 23:39:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:17 @agent_ppo2.py:185][0m |           0.0129 |          67.3653 |           6.7353 |
[32m[20221213 23:39:17 @agent_ppo2.py:185][0m |          -0.0061 |          58.6107 |           6.8565 |
[32m[20221213 23:39:17 @agent_ppo2.py:185][0m |           0.0046 |          64.7145 |           6.7540 |
[32m[20221213 23:39:17 @agent_ppo2.py:185][0m |          -0.0100 |          57.8154 |           6.7474 |
[32m[20221213 23:39:18 @agent_ppo2.py:185][0m |          -0.0089 |          57.4859 |           6.7634 |
[32m[20221213 23:39:18 @agent_ppo2.py:185][0m |           0.0080 |          68.0585 |           6.7523 |
[32m[20221213 23:39:18 @agent_ppo2.py:185][0m |          -0.0114 |          57.3244 |           6.7847 |
[32m[20221213 23:39:18 @agent_ppo2.py:185][0m |          -0.0134 |          56.9758 |           6.7446 |
[32m[20221213 23:39:18 @agent_ppo2.py:185][0m |          -0.0190 |          56.8105 |           6.7071 |
[32m[20221213 23:39:18 @agent_ppo2.py:185][0m |          -0.0188 |          56.6511 |           6.7346 |
[32m[20221213 23:39:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.85
[32m[20221213 23:39:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.84
[32m[20221213 23:39:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.78
[32m[20221213 23:39:18 @agent_ppo2.py:143][0m Total time:      26.77 min
[32m[20221213 23:39:18 @agent_ppo2.py:145][0m 2592768 total steps have happened
[32m[20221213 23:39:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3266 --------------------------#
[32m[20221213 23:39:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:18 @agent_ppo2.py:185][0m |          -0.0003 |          47.5598 |           6.3862 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0037 |          38.7398 |           6.3098 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0118 |          35.3931 |           6.4620 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0055 |          33.5031 |           6.3786 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0130 |          32.2248 |           6.3680 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0073 |          31.8022 |           6.3548 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0155 |          30.2689 |           6.4007 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0197 |          29.7411 |           6.4894 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0050 |          29.8786 |           6.4247 |
[32m[20221213 23:39:19 @agent_ppo2.py:185][0m |          -0.0228 |          28.8376 |           6.4869 |
[32m[20221213 23:39:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.61
[32m[20221213 23:39:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.57
[32m[20221213 23:39:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.92
[32m[20221213 23:39:19 @agent_ppo2.py:143][0m Total time:      26.79 min
[32m[20221213 23:39:19 @agent_ppo2.py:145][0m 2594816 total steps have happened
[32m[20221213 23:39:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3267 --------------------------#
[32m[20221213 23:39:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |           0.0004 |          52.2223 |           6.3261 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0040 |          48.9449 |           6.3843 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |           0.0083 |          52.8695 |           6.3564 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0008 |          47.7058 |           6.3770 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0052 |          46.6503 |           6.4572 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0096 |          46.1191 |           6.5297 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0045 |          45.9460 |           6.4614 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0093 |          45.7402 |           6.4660 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0129 |          45.4596 |           6.4542 |
[32m[20221213 23:39:20 @agent_ppo2.py:185][0m |          -0.0095 |          45.3883 |           6.4235 |
[32m[20221213 23:39:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:39:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.70
[32m[20221213 23:39:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.40
[32m[20221213 23:39:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.86
[32m[20221213 23:39:21 @agent_ppo2.py:143][0m Total time:      26.81 min
[32m[20221213 23:39:21 @agent_ppo2.py:145][0m 2596864 total steps have happened
[32m[20221213 23:39:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3268 --------------------------#
[32m[20221213 23:39:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:21 @agent_ppo2.py:185][0m |          -0.0030 |          58.3121 |           6.8167 |
[32m[20221213 23:39:21 @agent_ppo2.py:185][0m |          -0.0032 |          52.1949 |           6.7246 |
[32m[20221213 23:39:21 @agent_ppo2.py:185][0m |          -0.0072 |          49.6659 |           6.7185 |
[32m[20221213 23:39:21 @agent_ppo2.py:185][0m |          -0.0073 |          48.3028 |           6.6852 |
[32m[20221213 23:39:21 @agent_ppo2.py:185][0m |          -0.0131 |          47.2220 |           6.6858 |
[32m[20221213 23:39:21 @agent_ppo2.py:185][0m |          -0.0107 |          46.2413 |           6.6410 |
[32m[20221213 23:39:21 @agent_ppo2.py:185][0m |          -0.0135 |          45.5904 |           6.6602 |
[32m[20221213 23:39:22 @agent_ppo2.py:185][0m |          -0.0186 |          45.1946 |           6.6092 |
[32m[20221213 23:39:22 @agent_ppo2.py:185][0m |          -0.0184 |          44.7988 |           6.5010 |
[32m[20221213 23:39:22 @agent_ppo2.py:185][0m |          -0.0170 |          44.6074 |           6.5465 |
[32m[20221213 23:39:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:39:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.16
[32m[20221213 23:39:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.48
[32m[20221213 23:39:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.81
[32m[20221213 23:39:22 @agent_ppo2.py:143][0m Total time:      26.83 min
[32m[20221213 23:39:22 @agent_ppo2.py:145][0m 2598912 total steps have happened
[32m[20221213 23:39:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3269 --------------------------#
[32m[20221213 23:39:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:22 @agent_ppo2.py:185][0m |          -0.0004 |          62.8291 |           6.3365 |
[32m[20221213 23:39:22 @agent_ppo2.py:185][0m |          -0.0039 |          56.8104 |           6.2911 |
[32m[20221213 23:39:22 @agent_ppo2.py:185][0m |          -0.0104 |          54.6749 |           6.2323 |
[32m[20221213 23:39:22 @agent_ppo2.py:185][0m |          -0.0109 |          53.6507 |           6.1877 |
[32m[20221213 23:39:23 @agent_ppo2.py:185][0m |          -0.0082 |          52.4838 |           6.1718 |
[32m[20221213 23:39:23 @agent_ppo2.py:185][0m |          -0.0089 |          52.0275 |           6.1858 |
[32m[20221213 23:39:23 @agent_ppo2.py:185][0m |          -0.0081 |          51.0259 |           6.1297 |
[32m[20221213 23:39:23 @agent_ppo2.py:185][0m |          -0.0083 |          50.8447 |           6.1325 |
[32m[20221213 23:39:23 @agent_ppo2.py:185][0m |          -0.0120 |          49.6480 |           6.1076 |
[32m[20221213 23:39:23 @agent_ppo2.py:185][0m |          -0.0123 |          49.0929 |           6.1029 |
[32m[20221213 23:39:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.95
[32m[20221213 23:39:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.32
[32m[20221213 23:39:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.37
[32m[20221213 23:39:23 @agent_ppo2.py:143][0m Total time:      26.85 min
[32m[20221213 23:39:23 @agent_ppo2.py:145][0m 2600960 total steps have happened
[32m[20221213 23:39:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3270 --------------------------#
[32m[20221213 23:39:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:39:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:23 @agent_ppo2.py:185][0m |          -0.0015 |          65.8085 |           5.6221 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0043 |          63.2248 |           5.6255 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0062 |          61.9885 |           5.6262 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0114 |          60.7241 |           5.6880 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0095 |          60.0314 |           5.6619 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0042 |          61.7921 |           5.7213 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |           0.0097 |          69.4217 |           5.7145 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0083 |          58.9400 |           5.6633 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0101 |          57.8708 |           5.7601 |
[32m[20221213 23:39:24 @agent_ppo2.py:185][0m |          -0.0139 |          57.4374 |           5.7585 |
[32m[20221213 23:39:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.66
[32m[20221213 23:39:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.49
[32m[20221213 23:39:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.88
[32m[20221213 23:39:24 @agent_ppo2.py:143][0m Total time:      26.87 min
[32m[20221213 23:39:24 @agent_ppo2.py:145][0m 2603008 total steps have happened
[32m[20221213 23:39:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3271 --------------------------#
[32m[20221213 23:39:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:39:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0002 |          65.9006 |           6.4911 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0051 |          60.6825 |           6.5161 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0097 |          59.1046 |           6.5016 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |           0.0001 |          60.8909 |           6.4728 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0096 |          57.1964 |           6.5202 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0097 |          56.6629 |           6.5335 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0127 |          56.0059 |           6.5037 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0145 |          55.5709 |           6.5251 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0099 |          55.0086 |           6.5555 |
[32m[20221213 23:39:25 @agent_ppo2.py:185][0m |          -0.0162 |          54.7585 |           6.5302 |
[32m[20221213 23:39:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:39:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.96
[32m[20221213 23:39:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.78
[32m[20221213 23:39:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.72
[32m[20221213 23:39:26 @agent_ppo2.py:143][0m Total time:      26.89 min
[32m[20221213 23:39:26 @agent_ppo2.py:145][0m 2605056 total steps have happened
[32m[20221213 23:39:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3272 --------------------------#
[32m[20221213 23:39:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:26 @agent_ppo2.py:185][0m |           0.0026 |          78.1709 |           5.8214 |
[32m[20221213 23:39:26 @agent_ppo2.py:185][0m |          -0.0015 |          74.5978 |           5.8487 |
[32m[20221213 23:39:26 @agent_ppo2.py:185][0m |          -0.0025 |          73.0862 |           5.8682 |
[32m[20221213 23:39:26 @agent_ppo2.py:185][0m |          -0.0063 |          72.0157 |           5.8748 |
[32m[20221213 23:39:26 @agent_ppo2.py:185][0m |          -0.0076 |          71.2301 |           5.8583 |
[32m[20221213 23:39:26 @agent_ppo2.py:185][0m |          -0.0063 |          70.4619 |           5.9253 |
[32m[20221213 23:39:26 @agent_ppo2.py:185][0m |          -0.0097 |          69.9623 |           5.8895 |
[32m[20221213 23:39:27 @agent_ppo2.py:185][0m |          -0.0065 |          69.5141 |           5.9624 |
[32m[20221213 23:39:27 @agent_ppo2.py:185][0m |          -0.0090 |          69.3543 |           6.0333 |
[32m[20221213 23:39:27 @agent_ppo2.py:185][0m |          -0.0078 |          68.8220 |           5.9336 |
[32m[20221213 23:39:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:39:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.75
[32m[20221213 23:39:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.13
[32m[20221213 23:39:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.71
[32m[20221213 23:39:27 @agent_ppo2.py:143][0m Total time:      26.91 min
[32m[20221213 23:39:27 @agent_ppo2.py:145][0m 2607104 total steps have happened
[32m[20221213 23:39:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3273 --------------------------#
[32m[20221213 23:39:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:27 @agent_ppo2.py:185][0m |          -0.0021 |          57.0075 |           6.4233 |
[32m[20221213 23:39:27 @agent_ppo2.py:185][0m |          -0.0046 |          50.6647 |           6.4279 |
[32m[20221213 23:39:27 @agent_ppo2.py:185][0m |          -0.0059 |          48.5297 |           6.4267 |
[32m[20221213 23:39:27 @agent_ppo2.py:185][0m |          -0.0154 |          47.1463 |           6.4622 |
[32m[20221213 23:39:28 @agent_ppo2.py:185][0m |          -0.0139 |          46.2756 |           6.4842 |
[32m[20221213 23:39:28 @agent_ppo2.py:185][0m |          -0.0046 |          48.0840 |           6.4377 |
[32m[20221213 23:39:28 @agent_ppo2.py:185][0m |          -0.0147 |          45.4783 |           6.4752 |
[32m[20221213 23:39:28 @agent_ppo2.py:185][0m |          -0.0136 |          44.7519 |           6.4733 |
[32m[20221213 23:39:28 @agent_ppo2.py:185][0m |          -0.0158 |          44.0860 |           6.3879 |
[32m[20221213 23:39:28 @agent_ppo2.py:185][0m |          -0.0132 |          44.0413 |           6.4185 |
[32m[20221213 23:39:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.22
[32m[20221213 23:39:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.70
[32m[20221213 23:39:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.96
[32m[20221213 23:39:28 @agent_ppo2.py:143][0m Total time:      26.94 min
[32m[20221213 23:39:28 @agent_ppo2.py:145][0m 2609152 total steps have happened
[32m[20221213 23:39:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3274 --------------------------#
[32m[20221213 23:39:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:28 @agent_ppo2.py:185][0m |           0.0007 |          69.8864 |           6.0417 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0087 |          66.9390 |           6.0956 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0068 |          66.7834 |           6.0385 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0106 |          65.1280 |           6.1454 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0105 |          64.7689 |           6.1253 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0093 |          64.4068 |           6.1613 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0149 |          64.0477 |           6.2136 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0132 |          63.7180 |           6.1773 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0132 |          63.4463 |           6.2329 |
[32m[20221213 23:39:29 @agent_ppo2.py:185][0m |          -0.0136 |          63.4508 |           6.2136 |
[32m[20221213 23:39:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.51
[32m[20221213 23:39:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.66
[32m[20221213 23:39:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.31
[32m[20221213 23:39:29 @agent_ppo2.py:143][0m Total time:      26.96 min
[32m[20221213 23:39:29 @agent_ppo2.py:145][0m 2611200 total steps have happened
[32m[20221213 23:39:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3275 --------------------------#
[32m[20221213 23:39:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |           0.0020 |          61.6359 |           6.8842 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0018 |          58.9605 |           6.9231 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0008 |          58.1673 |           6.8849 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0055 |          57.7807 |           6.9063 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0043 |          57.4462 |           6.8698 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0070 |          57.2017 |           6.9429 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0059 |          57.2043 |           6.9218 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0075 |          57.0237 |           6.9390 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0101 |          56.7800 |           6.9687 |
[32m[20221213 23:39:30 @agent_ppo2.py:185][0m |          -0.0085 |          56.6574 |           7.0073 |
[32m[20221213 23:39:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.04
[32m[20221213 23:39:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.30
[32m[20221213 23:39:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.61
[32m[20221213 23:39:31 @agent_ppo2.py:143][0m Total time:      26.98 min
[32m[20221213 23:39:31 @agent_ppo2.py:145][0m 2613248 total steps have happened
[32m[20221213 23:39:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3276 --------------------------#
[32m[20221213 23:39:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:31 @agent_ppo2.py:185][0m |           0.0033 |          73.7874 |           6.1906 |
[32m[20221213 23:39:31 @agent_ppo2.py:185][0m |           0.0001 |          69.3851 |           6.2284 |
[32m[20221213 23:39:31 @agent_ppo2.py:185][0m |          -0.0057 |          67.2200 |           6.2645 |
[32m[20221213 23:39:31 @agent_ppo2.py:185][0m |          -0.0083 |          65.7652 |           6.1867 |
[32m[20221213 23:39:31 @agent_ppo2.py:185][0m |          -0.0033 |          65.1648 |           6.2408 |
[32m[20221213 23:39:31 @agent_ppo2.py:185][0m |          -0.0128 |          63.7997 |           6.2867 |
[32m[20221213 23:39:31 @agent_ppo2.py:185][0m |          -0.0092 |          64.2639 |           6.3197 |
[32m[20221213 23:39:32 @agent_ppo2.py:185][0m |          -0.0125 |          62.8103 |           6.3285 |
[32m[20221213 23:39:32 @agent_ppo2.py:185][0m |          -0.0037 |          69.5195 |           6.3176 |
[32m[20221213 23:39:32 @agent_ppo2.py:185][0m |          -0.0131 |          61.9830 |           6.3615 |
[32m[20221213 23:39:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.77
[32m[20221213 23:39:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.84
[32m[20221213 23:39:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.90
[32m[20221213 23:39:32 @agent_ppo2.py:143][0m Total time:      27.00 min
[32m[20221213 23:39:32 @agent_ppo2.py:145][0m 2615296 total steps have happened
[32m[20221213 23:39:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3277 --------------------------#
[32m[20221213 23:39:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:32 @agent_ppo2.py:185][0m |           0.0221 |          73.4928 |           6.9538 |
[32m[20221213 23:39:32 @agent_ppo2.py:185][0m |          -0.0018 |          64.5955 |           6.8722 |
[32m[20221213 23:39:32 @agent_ppo2.py:185][0m |          -0.0050 |          63.5209 |           6.9040 |
[32m[20221213 23:39:33 @agent_ppo2.py:185][0m |           0.0218 |          75.3300 |           6.9329 |
[32m[20221213 23:39:33 @agent_ppo2.py:185][0m |          -0.0039 |          62.3979 |           6.9344 |
[32m[20221213 23:39:33 @agent_ppo2.py:185][0m |          -0.0046 |          61.5399 |           7.0795 |
[32m[20221213 23:39:33 @agent_ppo2.py:185][0m |          -0.0060 |          62.4447 |           6.9565 |
[32m[20221213 23:39:33 @agent_ppo2.py:185][0m |          -0.0047 |          61.3263 |           6.9879 |
[32m[20221213 23:39:33 @agent_ppo2.py:185][0m |          -0.0085 |          61.0033 |           7.0544 |
[32m[20221213 23:39:33 @agent_ppo2.py:185][0m |          -0.0055 |          60.8103 |           6.9760 |
[32m[20221213 23:39:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:39:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.20
[32m[20221213 23:39:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.44
[32m[20221213 23:39:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.69
[32m[20221213 23:39:33 @agent_ppo2.py:143][0m Total time:      27.02 min
[32m[20221213 23:39:33 @agent_ppo2.py:145][0m 2617344 total steps have happened
[32m[20221213 23:39:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3278 --------------------------#
[32m[20221213 23:39:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:39:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0008 |          38.6454 |           7.3737 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0005 |          32.8441 |           7.3988 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0045 |          29.8795 |           7.4284 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0128 |          28.7759 |           7.3882 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0104 |          28.1259 |           7.4309 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0184 |          27.1210 |           7.3758 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0114 |          28.4038 |           7.3890 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0176 |          26.4371 |           7.3730 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0167 |          25.9408 |           7.4335 |
[32m[20221213 23:39:34 @agent_ppo2.py:185][0m |          -0.0173 |          25.5315 |           7.4120 |
[32m[20221213 23:39:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.46
[32m[20221213 23:39:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.21
[32m[20221213 23:39:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.08
[32m[20221213 23:39:34 @agent_ppo2.py:143][0m Total time:      27.04 min
[32m[20221213 23:39:34 @agent_ppo2.py:145][0m 2619392 total steps have happened
[32m[20221213 23:39:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3279 --------------------------#
[32m[20221213 23:39:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |          -0.0003 |          67.4206 |           7.4208 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |           0.0018 |          68.5210 |           7.3928 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |          -0.0059 |          65.5010 |           7.3777 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |          -0.0022 |          66.4981 |           7.3629 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |           0.0022 |          72.1310 |           7.3682 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |          -0.0075 |          64.9208 |           7.3638 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |          -0.0078 |          64.4675 |           7.2870 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |           0.0057 |          74.7549 |           7.3984 |
[32m[20221213 23:39:35 @agent_ppo2.py:185][0m |          -0.0098 |          64.6038 |           7.4350 |
[32m[20221213 23:39:36 @agent_ppo2.py:185][0m |          -0.0111 |          64.2329 |           7.3757 |
[32m[20221213 23:39:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:39:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.33
[32m[20221213 23:39:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.11
[32m[20221213 23:39:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.23
[32m[20221213 23:39:36 @agent_ppo2.py:143][0m Total time:      27.06 min
[32m[20221213 23:39:36 @agent_ppo2.py:145][0m 2621440 total steps have happened
[32m[20221213 23:39:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3280 --------------------------#
[32m[20221213 23:39:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:39:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:36 @agent_ppo2.py:185][0m |           0.0018 |          64.4550 |           7.1891 |
[32m[20221213 23:39:36 @agent_ppo2.py:185][0m |          -0.0034 |          62.3467 |           7.1083 |
[32m[20221213 23:39:36 @agent_ppo2.py:185][0m |          -0.0059 |          61.4665 |           7.1508 |
[32m[20221213 23:39:36 @agent_ppo2.py:185][0m |           0.0054 |          67.1500 |           7.0641 |
[32m[20221213 23:39:36 @agent_ppo2.py:185][0m |          -0.0021 |          60.5826 |           7.0596 |
[32m[20221213 23:39:36 @agent_ppo2.py:185][0m |          -0.0075 |          59.6504 |           7.0246 |
[32m[20221213 23:39:37 @agent_ppo2.py:185][0m |          -0.0087 |          59.2401 |           6.9936 |
[32m[20221213 23:39:37 @agent_ppo2.py:185][0m |           0.0029 |          61.8800 |           6.9628 |
[32m[20221213 23:39:37 @agent_ppo2.py:185][0m |          -0.0079 |          58.7376 |           6.9991 |
[32m[20221213 23:39:37 @agent_ppo2.py:185][0m |          -0.0095 |          58.1382 |           6.9835 |
[32m[20221213 23:39:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.00
[32m[20221213 23:39:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.80
[32m[20221213 23:39:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.85
[32m[20221213 23:39:37 @agent_ppo2.py:143][0m Total time:      27.08 min
[32m[20221213 23:39:37 @agent_ppo2.py:145][0m 2623488 total steps have happened
[32m[20221213 23:39:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3281 --------------------------#
[32m[20221213 23:39:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:37 @agent_ppo2.py:185][0m |           0.0020 |          70.5003 |           6.6276 |
[32m[20221213 23:39:37 @agent_ppo2.py:185][0m |          -0.0053 |          64.9064 |           6.6700 |
[32m[20221213 23:39:37 @agent_ppo2.py:185][0m |          -0.0054 |          63.3826 |           6.6280 |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |          -0.0096 |          62.9598 |           6.6767 |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |          -0.0082 |          61.8972 |           6.6457 |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |          -0.0044 |          62.6528 |           6.6466 |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |          -0.0138 |          61.0153 |           6.6316 |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |          -0.0147 |          60.5875 |           6.6294 |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |          -0.0123 |          60.3863 |           6.6567 |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |          -0.0142 |          60.2034 |           6.6573 |
[32m[20221213 23:39:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.41
[32m[20221213 23:39:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.26
[32m[20221213 23:39:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.76
[32m[20221213 23:39:38 @agent_ppo2.py:143][0m Total time:      27.10 min
[32m[20221213 23:39:38 @agent_ppo2.py:145][0m 2625536 total steps have happened
[32m[20221213 23:39:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3282 --------------------------#
[32m[20221213 23:39:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:38 @agent_ppo2.py:185][0m |           0.0070 |          46.6882 |           6.8588 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0047 |          39.3314 |           6.9487 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0096 |          36.4738 |           6.8725 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0103 |          35.1124 |           6.8729 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0177 |          34.1042 |           6.8433 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0156 |          33.3625 |           6.8387 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0156 |          32.5347 |           6.7758 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0196 |          32.1751 |           6.7733 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0173 |          31.8186 |           6.8157 |
[32m[20221213 23:39:39 @agent_ppo2.py:185][0m |          -0.0190 |          31.1951 |           6.7841 |
[32m[20221213 23:39:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:39:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.67
[32m[20221213 23:39:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.39
[32m[20221213 23:39:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.24
[32m[20221213 23:39:39 @agent_ppo2.py:143][0m Total time:      27.12 min
[32m[20221213 23:39:39 @agent_ppo2.py:145][0m 2627584 total steps have happened
[32m[20221213 23:39:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3283 --------------------------#
[32m[20221213 23:39:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:39:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |           0.0061 |          58.5759 |           6.6734 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0036 |          51.2847 |           6.6822 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0080 |          49.1255 |           6.6676 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0006 |          48.3915 |           6.6529 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0096 |          47.7445 |           6.7146 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0135 |          46.7328 |           6.7006 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0125 |          47.0405 |           6.6843 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0150 |          45.9489 |           6.7182 |
[32m[20221213 23:39:40 @agent_ppo2.py:185][0m |          -0.0138 |          45.3185 |           6.7553 |
[32m[20221213 23:39:41 @agent_ppo2.py:185][0m |          -0.0130 |          45.0464 |           6.7622 |
[32m[20221213 23:39:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.64
[32m[20221213 23:39:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.81
[32m[20221213 23:39:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.92
[32m[20221213 23:39:41 @agent_ppo2.py:143][0m Total time:      27.14 min
[32m[20221213 23:39:41 @agent_ppo2.py:145][0m 2629632 total steps have happened
[32m[20221213 23:39:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3284 --------------------------#
[32m[20221213 23:39:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:41 @agent_ppo2.py:185][0m |          -0.0041 |          55.2440 |           6.3365 |
[32m[20221213 23:39:41 @agent_ppo2.py:185][0m |          -0.0046 |          47.5843 |           6.4161 |
[32m[20221213 23:39:41 @agent_ppo2.py:185][0m |          -0.0077 |          44.7880 |           6.3949 |
[32m[20221213 23:39:41 @agent_ppo2.py:185][0m |          -0.0050 |          43.4986 |           6.4032 |
[32m[20221213 23:39:41 @agent_ppo2.py:185][0m |          -0.0153 |          42.3834 |           6.4131 |
[32m[20221213 23:39:41 @agent_ppo2.py:185][0m |          -0.0186 |          41.7151 |           6.3252 |
[32m[20221213 23:39:42 @agent_ppo2.py:185][0m |          -0.0094 |          41.1194 |           6.4080 |
[32m[20221213 23:39:42 @agent_ppo2.py:185][0m |          -0.0220 |          40.8262 |           6.3047 |
[32m[20221213 23:39:42 @agent_ppo2.py:185][0m |          -0.0178 |          40.5397 |           6.3715 |
[32m[20221213 23:39:42 @agent_ppo2.py:185][0m |          -0.0170 |          40.1003 |           6.3494 |
[32m[20221213 23:39:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.31
[32m[20221213 23:39:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.58
[32m[20221213 23:39:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.56
[32m[20221213 23:39:42 @agent_ppo2.py:143][0m Total time:      27.17 min
[32m[20221213 23:39:42 @agent_ppo2.py:145][0m 2631680 total steps have happened
[32m[20221213 23:39:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3285 --------------------------#
[32m[20221213 23:39:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:42 @agent_ppo2.py:185][0m |           0.0075 |          55.7856 |           6.8657 |
[32m[20221213 23:39:42 @agent_ppo2.py:185][0m |          -0.0046 |          50.8728 |           6.8137 |
[32m[20221213 23:39:42 @agent_ppo2.py:185][0m |          -0.0060 |          49.4023 |           6.8798 |
[32m[20221213 23:39:43 @agent_ppo2.py:185][0m |           0.0025 |          61.3387 |           6.9198 |
[32m[20221213 23:39:43 @agent_ppo2.py:185][0m |          -0.0096 |          49.0219 |           6.8782 |
[32m[20221213 23:39:43 @agent_ppo2.py:185][0m |          -0.0080 |          47.9618 |           6.8301 |
[32m[20221213 23:39:43 @agent_ppo2.py:185][0m |          -0.0150 |          46.9074 |           6.8941 |
[32m[20221213 23:39:43 @agent_ppo2.py:185][0m |          -0.0129 |          46.7117 |           6.8713 |
[32m[20221213 23:39:43 @agent_ppo2.py:185][0m |          -0.0145 |          46.2537 |           6.9006 |
[32m[20221213 23:39:43 @agent_ppo2.py:185][0m |          -0.0136 |          46.1109 |           6.8542 |
[32m[20221213 23:39:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.18
[32m[20221213 23:39:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.53
[32m[20221213 23:39:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.58
[32m[20221213 23:39:43 @agent_ppo2.py:143][0m Total time:      27.19 min
[32m[20221213 23:39:43 @agent_ppo2.py:145][0m 2633728 total steps have happened
[32m[20221213 23:39:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3286 --------------------------#
[32m[20221213 23:39:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |           0.0028 |          47.4962 |           6.9272 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0081 |          42.7375 |           7.0191 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0079 |          41.1226 |           6.9360 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0134 |          40.0327 |           7.0410 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0124 |          39.2719 |           7.0115 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0114 |          38.4698 |           7.0677 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0146 |          38.0095 |           7.0782 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0189 |          37.6887 |           7.0697 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0086 |          37.0671 |           7.0332 |
[32m[20221213 23:39:44 @agent_ppo2.py:185][0m |          -0.0187 |          37.0425 |           7.0001 |
[32m[20221213 23:39:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:39:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.29
[32m[20221213 23:39:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.00
[32m[20221213 23:39:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.95
[32m[20221213 23:39:44 @agent_ppo2.py:143][0m Total time:      27.21 min
[32m[20221213 23:39:44 @agent_ppo2.py:145][0m 2635776 total steps have happened
[32m[20221213 23:39:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3287 --------------------------#
[32m[20221213 23:39:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |           0.0020 |          64.0035 |           6.3594 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |          -0.0020 |          61.9523 |           6.4096 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |          -0.0047 |          61.1894 |           6.4409 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |          -0.0090 |          61.0262 |           6.3451 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |          -0.0078 |          60.9414 |           6.3642 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |           0.0073 |          68.6043 |           6.3645 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |          -0.0096 |          61.0698 |           6.2753 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |          -0.0064 |          61.1227 |           6.3296 |
[32m[20221213 23:39:45 @agent_ppo2.py:185][0m |           0.0098 |          70.9547 |           6.2694 |
[32m[20221213 23:39:46 @agent_ppo2.py:185][0m |          -0.0112 |          60.6931 |           6.2989 |
[32m[20221213 23:39:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:39:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.59
[32m[20221213 23:39:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.72
[32m[20221213 23:39:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.87
[32m[20221213 23:39:46 @agent_ppo2.py:143][0m Total time:      27.23 min
[32m[20221213 23:39:46 @agent_ppo2.py:145][0m 2637824 total steps have happened
[32m[20221213 23:39:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3288 --------------------------#
[32m[20221213 23:39:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:46 @agent_ppo2.py:185][0m |          -0.0002 |          55.1130 |           6.3244 |
[32m[20221213 23:39:46 @agent_ppo2.py:185][0m |          -0.0060 |          51.9032 |           6.3880 |
[32m[20221213 23:39:46 @agent_ppo2.py:185][0m |          -0.0055 |          50.3348 |           6.3255 |
[32m[20221213 23:39:46 @agent_ppo2.py:185][0m |          -0.0111 |          49.4981 |           6.4116 |
[32m[20221213 23:39:46 @agent_ppo2.py:185][0m |          -0.0027 |          51.3081 |           6.5311 |
[32m[20221213 23:39:46 @agent_ppo2.py:185][0m |          -0.0101 |          50.4180 |           6.5075 |
[32m[20221213 23:39:47 @agent_ppo2.py:185][0m |          -0.0116 |          47.9053 |           6.5128 |
[32m[20221213 23:39:47 @agent_ppo2.py:185][0m |          -0.0129 |          47.5750 |           6.6018 |
[32m[20221213 23:39:47 @agent_ppo2.py:185][0m |          -0.0166 |          46.8972 |           6.5807 |
[32m[20221213 23:39:47 @agent_ppo2.py:185][0m |          -0.0145 |          46.6332 |           6.6495 |
[32m[20221213 23:39:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.95
[32m[20221213 23:39:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.40
[32m[20221213 23:39:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.99
[32m[20221213 23:39:47 @agent_ppo2.py:143][0m Total time:      27.25 min
[32m[20221213 23:39:47 @agent_ppo2.py:145][0m 2639872 total steps have happened
[32m[20221213 23:39:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3289 --------------------------#
[32m[20221213 23:39:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:47 @agent_ppo2.py:185][0m |           0.0010 |          69.6136 |           6.5129 |
[32m[20221213 23:39:47 @agent_ppo2.py:185][0m |          -0.0031 |          65.7326 |           6.4160 |
[32m[20221213 23:39:47 @agent_ppo2.py:185][0m |          -0.0001 |          72.9482 |           6.4040 |
[32m[20221213 23:39:48 @agent_ppo2.py:185][0m |          -0.0136 |          63.4164 |           6.4943 |
[32m[20221213 23:39:48 @agent_ppo2.py:185][0m |          -0.0154 |          62.7845 |           6.4667 |
[32m[20221213 23:39:48 @agent_ppo2.py:185][0m |          -0.0182 |          62.3171 |           6.4663 |
[32m[20221213 23:39:48 @agent_ppo2.py:185][0m |          -0.0173 |          61.9191 |           6.4518 |
[32m[20221213 23:39:48 @agent_ppo2.py:185][0m |          -0.0181 |          61.4997 |           6.5198 |
[32m[20221213 23:39:48 @agent_ppo2.py:185][0m |          -0.0174 |          61.5006 |           6.4777 |
[32m[20221213 23:39:48 @agent_ppo2.py:185][0m |          -0.0193 |          61.6971 |           6.4744 |
[32m[20221213 23:39:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.26
[32m[20221213 23:39:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.26
[32m[20221213 23:39:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.52
[32m[20221213 23:39:48 @agent_ppo2.py:143][0m Total time:      27.27 min
[32m[20221213 23:39:48 @agent_ppo2.py:145][0m 2641920 total steps have happened
[32m[20221213 23:39:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3290 --------------------------#
[32m[20221213 23:39:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:39:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0020 |          64.3672 |           7.1635 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0035 |          61.8167 |           7.2777 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0046 |          59.9778 |           7.2252 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0009 |          59.2512 |           7.2501 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0074 |          58.4888 |           7.2539 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0016 |          60.3718 |           7.2524 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0055 |          57.6530 |           7.2117 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0091 |          57.3453 |           7.2972 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |          -0.0095 |          56.8873 |           7.2543 |
[32m[20221213 23:39:49 @agent_ppo2.py:185][0m |           0.0082 |          62.6606 |           7.3497 |
[32m[20221213 23:39:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.14
[32m[20221213 23:39:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.82
[32m[20221213 23:39:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.12
[32m[20221213 23:39:49 @agent_ppo2.py:143][0m Total time:      27.29 min
[32m[20221213 23:39:49 @agent_ppo2.py:145][0m 2643968 total steps have happened
[32m[20221213 23:39:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3291 --------------------------#
[32m[20221213 23:39:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |          -0.0011 |          52.2225 |           6.8209 |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |           0.0050 |          48.1778 |           6.8918 |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |          -0.0056 |          45.7448 |           6.8827 |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |          -0.0126 |          43.8744 |           6.8807 |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |          -0.0002 |          50.0133 |           6.9029 |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |          -0.0164 |          42.4224 |           6.9074 |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |          -0.0158 |          41.5375 |           6.9151 |
[32m[20221213 23:39:50 @agent_ppo2.py:185][0m |          -0.0151 |          41.1367 |           6.9047 |
[32m[20221213 23:39:51 @agent_ppo2.py:185][0m |          -0.0022 |          46.3659 |           6.9610 |
[32m[20221213 23:39:51 @agent_ppo2.py:185][0m |          -0.0162 |          40.6892 |           7.0051 |
[32m[20221213 23:39:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:39:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.11
[32m[20221213 23:39:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.92
[32m[20221213 23:39:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.76
[32m[20221213 23:39:51 @agent_ppo2.py:143][0m Total time:      27.31 min
[32m[20221213 23:39:51 @agent_ppo2.py:145][0m 2646016 total steps have happened
[32m[20221213 23:39:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3292 --------------------------#
[32m[20221213 23:39:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:51 @agent_ppo2.py:185][0m |           0.0047 |          43.0279 |           7.8036 |
[32m[20221213 23:39:51 @agent_ppo2.py:185][0m |          -0.0075 |          38.2885 |           7.6500 |
[32m[20221213 23:39:51 @agent_ppo2.py:185][0m |          -0.0107 |          37.2874 |           7.7177 |
[32m[20221213 23:39:51 @agent_ppo2.py:185][0m |          -0.0148 |          36.4174 |           7.7286 |
[32m[20221213 23:39:51 @agent_ppo2.py:185][0m |          -0.0069 |          36.3095 |           7.7357 |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0128 |          35.7419 |           7.7535 |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0190 |          35.3487 |           7.7660 |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0208 |          35.1830 |           7.8225 |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0194 |          34.8300 |           7.8009 |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0263 |          34.6902 |           7.8409 |
[32m[20221213 23:39:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.73
[32m[20221213 23:39:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.92
[32m[20221213 23:39:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.48
[32m[20221213 23:39:52 @agent_ppo2.py:143][0m Total time:      27.33 min
[32m[20221213 23:39:52 @agent_ppo2.py:145][0m 2648064 total steps have happened
[32m[20221213 23:39:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3293 --------------------------#
[32m[20221213 23:39:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0023 |          49.6781 |           6.7097 |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0057 |          44.6793 |           6.7251 |
[32m[20221213 23:39:52 @agent_ppo2.py:185][0m |          -0.0136 |          42.1857 |           6.6755 |
[32m[20221213 23:39:53 @agent_ppo2.py:185][0m |          -0.0039 |          46.4653 |           6.6733 |
[32m[20221213 23:39:53 @agent_ppo2.py:185][0m |          -0.0044 |          41.1731 |           6.6382 |
[32m[20221213 23:39:53 @agent_ppo2.py:185][0m |          -0.0166 |          39.2058 |           6.6465 |
[32m[20221213 23:39:53 @agent_ppo2.py:185][0m |          -0.0194 |          38.7527 |           6.6841 |
[32m[20221213 23:39:53 @agent_ppo2.py:185][0m |          -0.0173 |          38.1337 |           6.6341 |
[32m[20221213 23:39:53 @agent_ppo2.py:185][0m |          -0.0232 |          37.7898 |           6.6469 |
[32m[20221213 23:39:53 @agent_ppo2.py:185][0m |          -0.0225 |          37.6588 |           6.6461 |
[32m[20221213 23:39:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.25
[32m[20221213 23:39:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.43
[32m[20221213 23:39:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.22
[32m[20221213 23:39:53 @agent_ppo2.py:143][0m Total time:      27.35 min
[32m[20221213 23:39:53 @agent_ppo2.py:145][0m 2650112 total steps have happened
[32m[20221213 23:39:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3294 --------------------------#
[32m[20221213 23:39:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0036 |          61.7861 |           6.4670 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0060 |          57.4113 |           6.5045 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0053 |          56.3255 |           6.4462 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0120 |          55.0464 |           6.4483 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0078 |          54.7687 |           6.4481 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0124 |          54.0936 |           6.4176 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0137 |          53.4957 |           6.4631 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0166 |          53.4325 |           6.3788 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0175 |          52.8408 |           6.4165 |
[32m[20221213 23:39:54 @agent_ppo2.py:185][0m |          -0.0140 |          53.3735 |           6.4468 |
[32m[20221213 23:39:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.85
[32m[20221213 23:39:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.07
[32m[20221213 23:39:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.51
[32m[20221213 23:39:54 @agent_ppo2.py:143][0m Total time:      27.37 min
[32m[20221213 23:39:54 @agent_ppo2.py:145][0m 2652160 total steps have happened
[32m[20221213 23:39:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3295 --------------------------#
[32m[20221213 23:39:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |           0.0017 |          64.0689 |           7.1454 |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |          -0.0057 |          57.8153 |           7.1531 |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |          -0.0096 |          56.1471 |           7.1146 |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |           0.0008 |          62.1996 |           7.1684 |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |          -0.0065 |          55.2800 |           7.1605 |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |          -0.0152 |          53.2975 |           7.1495 |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |          -0.0127 |          52.9585 |           7.1900 |
[32m[20221213 23:39:55 @agent_ppo2.py:185][0m |          -0.0132 |          52.1760 |           7.1275 |
[32m[20221213 23:39:56 @agent_ppo2.py:185][0m |          -0.0126 |          51.5450 |           7.1148 |
[32m[20221213 23:39:56 @agent_ppo2.py:185][0m |          -0.0152 |          51.1110 |           7.1036 |
[32m[20221213 23:39:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:39:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.04
[32m[20221213 23:39:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.27
[32m[20221213 23:39:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.73
[32m[20221213 23:39:56 @agent_ppo2.py:143][0m Total time:      27.40 min
[32m[20221213 23:39:56 @agent_ppo2.py:145][0m 2654208 total steps have happened
[32m[20221213 23:39:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3296 --------------------------#
[32m[20221213 23:39:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:56 @agent_ppo2.py:185][0m |          -0.0013 |          79.6934 |           6.5765 |
[32m[20221213 23:39:56 @agent_ppo2.py:185][0m |          -0.0023 |          76.4750 |           6.6122 |
[32m[20221213 23:39:56 @agent_ppo2.py:185][0m |          -0.0050 |          75.1099 |           6.6182 |
[32m[20221213 23:39:56 @agent_ppo2.py:185][0m |          -0.0081 |          74.0139 |           6.6316 |
[32m[20221213 23:39:56 @agent_ppo2.py:185][0m |          -0.0094 |          73.2292 |           6.6874 |
[32m[20221213 23:39:57 @agent_ppo2.py:185][0m |          -0.0105 |          72.6202 |           6.6472 |
[32m[20221213 23:39:57 @agent_ppo2.py:185][0m |          -0.0115 |          71.9870 |           6.6576 |
[32m[20221213 23:39:57 @agent_ppo2.py:185][0m |          -0.0035 |          75.6683 |           6.6630 |
[32m[20221213 23:39:57 @agent_ppo2.py:185][0m |          -0.0101 |          71.5530 |           6.7272 |
[32m[20221213 23:39:57 @agent_ppo2.py:185][0m |          -0.0110 |          70.8214 |           6.6941 |
[32m[20221213 23:39:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:39:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.71
[32m[20221213 23:39:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.77
[32m[20221213 23:39:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.25
[32m[20221213 23:39:57 @agent_ppo2.py:143][0m Total time:      27.42 min
[32m[20221213 23:39:57 @agent_ppo2.py:145][0m 2656256 total steps have happened
[32m[20221213 23:39:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3297 --------------------------#
[32m[20221213 23:39:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:39:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:57 @agent_ppo2.py:185][0m |          -0.0009 |          53.1551 |           7.1262 |
[32m[20221213 23:39:57 @agent_ppo2.py:185][0m |          -0.0101 |          47.8570 |           7.1861 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0092 |          45.7409 |           7.2638 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0136 |          44.6845 |           7.2882 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0146 |          43.5773 |           7.2765 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0152 |          42.6883 |           7.3034 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0154 |          42.3295 |           7.2738 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0168 |          41.8032 |           7.3371 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0128 |          41.9087 |           7.3125 |
[32m[20221213 23:39:58 @agent_ppo2.py:185][0m |          -0.0181 |          41.2687 |           7.3169 |
[32m[20221213 23:39:58 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:39:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.43
[32m[20221213 23:39:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.81
[32m[20221213 23:39:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.43
[32m[20221213 23:39:58 @agent_ppo2.py:143][0m Total time:      27.44 min
[32m[20221213 23:39:58 @agent_ppo2.py:145][0m 2658304 total steps have happened
[32m[20221213 23:39:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3298 --------------------------#
[32m[20221213 23:39:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:39:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |           0.0022 |          37.7017 |           7.3395 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0084 |          30.8277 |           7.1941 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0128 |          29.1813 |           7.2848 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0115 |          28.2546 |           7.2543 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0174 |          27.6039 |           7.1211 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0125 |          27.1940 |           7.1783 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0134 |          26.6154 |           7.0958 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0202 |          26.3866 |           7.0807 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0112 |          27.5726 |           7.0584 |
[32m[20221213 23:39:59 @agent_ppo2.py:185][0m |          -0.0180 |          26.0120 |           6.9918 |
[32m[20221213 23:39:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:40:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.10
[32m[20221213 23:40:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.69
[32m[20221213 23:40:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.71
[32m[20221213 23:40:00 @agent_ppo2.py:143][0m Total time:      27.46 min
[32m[20221213 23:40:00 @agent_ppo2.py:145][0m 2660352 total steps have happened
[32m[20221213 23:40:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3299 --------------------------#
[32m[20221213 23:40:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:00 @agent_ppo2.py:185][0m |           0.0012 |          56.1955 |           7.3848 |
[32m[20221213 23:40:00 @agent_ppo2.py:185][0m |          -0.0042 |          50.9822 |           7.3448 |
[32m[20221213 23:40:00 @agent_ppo2.py:185][0m |          -0.0044 |          48.6693 |           7.4229 |
[32m[20221213 23:40:00 @agent_ppo2.py:185][0m |          -0.0101 |          47.0023 |           7.3427 |
[32m[20221213 23:40:00 @agent_ppo2.py:185][0m |          -0.0103 |          45.9301 |           7.3075 |
[32m[20221213 23:40:00 @agent_ppo2.py:185][0m |          -0.0120 |          45.1398 |           7.2794 |
[32m[20221213 23:40:01 @agent_ppo2.py:185][0m |          -0.0155 |          44.7811 |           7.3399 |
[32m[20221213 23:40:01 @agent_ppo2.py:185][0m |          -0.0133 |          43.9356 |           7.3301 |
[32m[20221213 23:40:01 @agent_ppo2.py:185][0m |          -0.0188 |          43.5808 |           7.3143 |
[32m[20221213 23:40:01 @agent_ppo2.py:185][0m |          -0.0183 |          43.2885 |           7.3209 |
[32m[20221213 23:40:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:40:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.17
[32m[20221213 23:40:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.13
[32m[20221213 23:40:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.64
[32m[20221213 23:40:01 @agent_ppo2.py:143][0m Total time:      27.48 min
[32m[20221213 23:40:01 @agent_ppo2.py:145][0m 2662400 total steps have happened
[32m[20221213 23:40:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3300 --------------------------#
[32m[20221213 23:40:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:40:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:01 @agent_ppo2.py:185][0m |           0.0037 |          64.7026 |           7.2532 |
[32m[20221213 23:40:01 @agent_ppo2.py:185][0m |          -0.0053 |          57.3049 |           7.2507 |
[32m[20221213 23:40:01 @agent_ppo2.py:185][0m |          -0.0078 |          55.7487 |           7.2725 |
[32m[20221213 23:40:02 @agent_ppo2.py:185][0m |          -0.0078 |          55.0282 |           7.3009 |
[32m[20221213 23:40:02 @agent_ppo2.py:185][0m |           0.0031 |          56.8451 |           7.3641 |
[32m[20221213 23:40:02 @agent_ppo2.py:185][0m |          -0.0105 |          53.1165 |           7.4048 |
[32m[20221213 23:40:02 @agent_ppo2.py:185][0m |          -0.0096 |          52.7089 |           7.3624 |
[32m[20221213 23:40:02 @agent_ppo2.py:185][0m |          -0.0212 |          52.9286 |           7.3753 |
[32m[20221213 23:40:02 @agent_ppo2.py:185][0m |          -0.0164 |          51.8515 |           7.3875 |
[32m[20221213 23:40:02 @agent_ppo2.py:185][0m |          -0.0161 |          51.9865 |           7.3611 |
[32m[20221213 23:40:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:40:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.37
[32m[20221213 23:40:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.00
[32m[20221213 23:40:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.14
[32m[20221213 23:40:02 @agent_ppo2.py:143][0m Total time:      27.50 min
[32m[20221213 23:40:02 @agent_ppo2.py:145][0m 2664448 total steps have happened
[32m[20221213 23:40:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3301 --------------------------#
[32m[20221213 23:40:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0060 |          77.2511 |           6.3244 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0072 |          71.7959 |           6.4002 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0072 |          70.7973 |           6.3898 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0077 |          69.6445 |           6.4199 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |           0.0006 |          78.6541 |           6.4411 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0093 |          68.8588 |           6.4332 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0139 |          68.0373 |           6.4669 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0163 |          67.6003 |           6.4677 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0144 |          67.2456 |           6.5444 |
[32m[20221213 23:40:03 @agent_ppo2.py:185][0m |          -0.0145 |          67.0164 |           6.4449 |
[32m[20221213 23:40:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:40:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.98
[32m[20221213 23:40:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.11
[32m[20221213 23:40:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.75
[32m[20221213 23:40:04 @agent_ppo2.py:143][0m Total time:      27.53 min
[32m[20221213 23:40:04 @agent_ppo2.py:145][0m 2666496 total steps have happened
[32m[20221213 23:40:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3302 --------------------------#
[32m[20221213 23:40:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |           0.0021 |          72.9623 |           6.9383 |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |          -0.0015 |          68.3654 |           6.9775 |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |          -0.0102 |          66.3045 |           6.9589 |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |          -0.0133 |          64.9279 |           6.9371 |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |          -0.0104 |          64.0301 |           6.9859 |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |          -0.0038 |          67.4551 |           7.0223 |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |          -0.0091 |          62.6136 |           7.0137 |
[32m[20221213 23:40:04 @agent_ppo2.py:185][0m |          -0.0157 |          61.2979 |           7.0397 |
[32m[20221213 23:40:05 @agent_ppo2.py:185][0m |          -0.0161 |          60.5017 |           7.0159 |
[32m[20221213 23:40:05 @agent_ppo2.py:185][0m |          -0.0160 |          59.5819 |           6.9827 |
[32m[20221213 23:40:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.66
[32m[20221213 23:40:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.91
[32m[20221213 23:40:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.63
[32m[20221213 23:40:05 @agent_ppo2.py:143][0m Total time:      27.55 min
[32m[20221213 23:40:05 @agent_ppo2.py:145][0m 2668544 total steps have happened
[32m[20221213 23:40:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3303 --------------------------#
[32m[20221213 23:40:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:05 @agent_ppo2.py:185][0m |          -0.0018 |          74.8573 |           6.3299 |
[32m[20221213 23:40:05 @agent_ppo2.py:185][0m |          -0.0058 |          70.1588 |           6.4471 |
[32m[20221213 23:40:05 @agent_ppo2.py:185][0m |          -0.0105 |          68.0845 |           6.4656 |
[32m[20221213 23:40:05 @agent_ppo2.py:185][0m |          -0.0047 |          67.1308 |           6.4865 |
[32m[20221213 23:40:05 @agent_ppo2.py:185][0m |          -0.0108 |          65.9618 |           6.4683 |
[32m[20221213 23:40:06 @agent_ppo2.py:185][0m |          -0.0098 |          64.8669 |           6.5469 |
[32m[20221213 23:40:06 @agent_ppo2.py:185][0m |          -0.0134 |          64.0282 |           6.5371 |
[32m[20221213 23:40:06 @agent_ppo2.py:185][0m |          -0.0070 |          64.0229 |           6.5912 |
[32m[20221213 23:40:06 @agent_ppo2.py:185][0m |          -0.0115 |          62.9032 |           6.6552 |
[32m[20221213 23:40:06 @agent_ppo2.py:185][0m |          -0.0132 |          62.6059 |           6.5956 |
[32m[20221213 23:40:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.80
[32m[20221213 23:40:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.77
[32m[20221213 23:40:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.94
[32m[20221213 23:40:06 @agent_ppo2.py:143][0m Total time:      27.57 min
[32m[20221213 23:40:06 @agent_ppo2.py:145][0m 2670592 total steps have happened
[32m[20221213 23:40:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3304 --------------------------#
[32m[20221213 23:40:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:06 @agent_ppo2.py:185][0m |           0.0045 |          58.9183 |           6.6625 |
[32m[20221213 23:40:06 @agent_ppo2.py:185][0m |          -0.0031 |          51.6336 |           6.5338 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |          -0.0068 |          49.6649 |           6.5132 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |           0.0015 |          50.3862 |           6.4416 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |          -0.0053 |          47.9223 |           6.4739 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |          -0.0037 |          48.3618 |           6.4800 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |          -0.0106 |          47.1727 |           6.4602 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |          -0.0061 |          46.5030 |           6.4346 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |          -0.0022 |          51.1127 |           6.3798 |
[32m[20221213 23:40:07 @agent_ppo2.py:185][0m |          -0.0111 |          46.0497 |           6.3870 |
[32m[20221213 23:40:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.13
[32m[20221213 23:40:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.24
[32m[20221213 23:40:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.85
[32m[20221213 23:40:07 @agent_ppo2.py:143][0m Total time:      27.59 min
[32m[20221213 23:40:07 @agent_ppo2.py:145][0m 2672640 total steps have happened
[32m[20221213 23:40:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3305 --------------------------#
[32m[20221213 23:40:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |           0.0034 |          52.1874 |           6.8919 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0089 |          44.4747 |           6.9493 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0022 |          43.5360 |           6.9429 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0148 |          40.3517 |           6.9996 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0142 |          38.8827 |           6.9684 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0109 |          37.6650 |           6.9995 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0139 |          37.0515 |           6.9995 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0150 |          36.3954 |           6.9739 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0108 |          35.7124 |           6.9443 |
[32m[20221213 23:40:08 @agent_ppo2.py:185][0m |          -0.0179 |          35.0623 |           7.0086 |
[32m[20221213 23:40:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:40:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.25
[32m[20221213 23:40:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.22
[32m[20221213 23:40:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.09
[32m[20221213 23:40:09 @agent_ppo2.py:143][0m Total time:      27.61 min
[32m[20221213 23:40:09 @agent_ppo2.py:145][0m 2674688 total steps have happened
[32m[20221213 23:40:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3306 --------------------------#
[32m[20221213 23:40:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |           0.0037 |          51.1907 |           6.9905 |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |          -0.0052 |          47.0306 |           6.9013 |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |          -0.0075 |          45.0863 |           6.9631 |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |          -0.0102 |          43.8194 |           6.8973 |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |          -0.0114 |          42.7987 |           6.9505 |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |          -0.0122 |          42.4851 |           6.9249 |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |          -0.0130 |          41.4835 |           6.9385 |
[32m[20221213 23:40:09 @agent_ppo2.py:185][0m |          -0.0091 |          42.6472 |           6.9846 |
[32m[20221213 23:40:10 @agent_ppo2.py:185][0m |          -0.0084 |          40.8089 |           6.9787 |
[32m[20221213 23:40:10 @agent_ppo2.py:185][0m |          -0.0174 |          40.5162 |           7.0353 |
[32m[20221213 23:40:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:40:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.71
[32m[20221213 23:40:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.93
[32m[20221213 23:40:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.20
[32m[20221213 23:40:10 @agent_ppo2.py:143][0m Total time:      27.63 min
[32m[20221213 23:40:10 @agent_ppo2.py:145][0m 2676736 total steps have happened
[32m[20221213 23:40:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3307 --------------------------#
[32m[20221213 23:40:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:10 @agent_ppo2.py:185][0m |           0.0017 |          48.2983 |           6.8754 |
[32m[20221213 23:40:10 @agent_ppo2.py:185][0m |          -0.0019 |          46.3106 |           6.9471 |
[32m[20221213 23:40:10 @agent_ppo2.py:185][0m |          -0.0064 |          45.4521 |           7.0052 |
[32m[20221213 23:40:10 @agent_ppo2.py:185][0m |          -0.0049 |          45.7645 |           7.0030 |
[32m[20221213 23:40:10 @agent_ppo2.py:185][0m |          -0.0086 |          45.2475 |           7.0322 |
[32m[20221213 23:40:11 @agent_ppo2.py:185][0m |          -0.0114 |          45.3008 |           7.0595 |
[32m[20221213 23:40:11 @agent_ppo2.py:185][0m |          -0.0011 |          47.0410 |           7.0845 |
[32m[20221213 23:40:11 @agent_ppo2.py:185][0m |          -0.0009 |          48.0124 |           7.1277 |
[32m[20221213 23:40:11 @agent_ppo2.py:185][0m |          -0.0110 |          44.6744 |           7.1682 |
[32m[20221213 23:40:11 @agent_ppo2.py:185][0m |          -0.0126 |          44.6150 |           7.1047 |
[32m[20221213 23:40:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.77
[32m[20221213 23:40:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.26
[32m[20221213 23:40:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.15
[32m[20221213 23:40:11 @agent_ppo2.py:143][0m Total time:      27.65 min
[32m[20221213 23:40:11 @agent_ppo2.py:145][0m 2678784 total steps have happened
[32m[20221213 23:40:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3308 --------------------------#
[32m[20221213 23:40:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:11 @agent_ppo2.py:185][0m |           0.0017 |          53.1889 |           6.4913 |
[32m[20221213 23:40:11 @agent_ppo2.py:185][0m |          -0.0018 |          50.6089 |           6.4661 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0029 |          49.5388 |           6.4798 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0128 |          48.4919 |           6.4497 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0101 |          47.8737 |           6.4001 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0124 |          47.5200 |           6.4329 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0111 |          47.1916 |           6.3932 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0037 |          48.7111 |           6.4353 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0161 |          46.7223 |           6.4000 |
[32m[20221213 23:40:12 @agent_ppo2.py:185][0m |          -0.0062 |          49.5138 |           6.3287 |
[32m[20221213 23:40:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.17
[32m[20221213 23:40:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.79
[32m[20221213 23:40:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.82
[32m[20221213 23:40:12 @agent_ppo2.py:143][0m Total time:      27.67 min
[32m[20221213 23:40:12 @agent_ppo2.py:145][0m 2680832 total steps have happened
[32m[20221213 23:40:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3309 --------------------------#
[32m[20221213 23:40:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |           0.0022 |          42.0743 |           6.9383 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |           0.0018 |          23.1213 |           6.8939 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |          -0.0005 |          21.4798 |           6.9269 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |           0.0001 |          20.2376 |           6.9530 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |          -0.0107 |          19.5472 |           6.8054 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |          -0.0141 |          19.3319 |           6.8153 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |          -0.0120 |          18.8139 |           6.8334 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |          -0.0049 |          21.0564 |           6.8098 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |          -0.0168 |          18.5803 |           6.8042 |
[32m[20221213 23:40:13 @agent_ppo2.py:185][0m |          -0.0079 |          18.2533 |           6.7809 |
[32m[20221213 23:40:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.89
[32m[20221213 23:40:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.86
[32m[20221213 23:40:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.58
[32m[20221213 23:40:14 @agent_ppo2.py:143][0m Total time:      27.69 min
[32m[20221213 23:40:14 @agent_ppo2.py:145][0m 2682880 total steps have happened
[32m[20221213 23:40:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3310 --------------------------#
[32m[20221213 23:40:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:40:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:14 @agent_ppo2.py:185][0m |           0.0031 |          45.1471 |           6.4511 |
[32m[20221213 23:40:14 @agent_ppo2.py:185][0m |          -0.0037 |          35.7621 |           6.4972 |
[32m[20221213 23:40:14 @agent_ppo2.py:185][0m |          -0.0007 |          36.0406 |           6.4982 |
[32m[20221213 23:40:14 @agent_ppo2.py:185][0m |          -0.0074 |          33.4052 |           6.4867 |
[32m[20221213 23:40:14 @agent_ppo2.py:185][0m |          -0.0100 |          32.5753 |           6.4724 |
[32m[20221213 23:40:14 @agent_ppo2.py:185][0m |          -0.0056 |          34.3520 |           6.4586 |
[32m[20221213 23:40:14 @agent_ppo2.py:185][0m |          -0.0096 |          31.4467 |           6.4510 |
[32m[20221213 23:40:15 @agent_ppo2.py:185][0m |          -0.0165 |          31.0939 |           6.3844 |
[32m[20221213 23:40:15 @agent_ppo2.py:185][0m |          -0.0089 |          30.9116 |           6.3688 |
[32m[20221213 23:40:15 @agent_ppo2.py:185][0m |          -0.0149 |          30.6210 |           6.4023 |
[32m[20221213 23:40:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:40:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.62
[32m[20221213 23:40:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.31
[32m[20221213 23:40:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.38
[32m[20221213 23:40:15 @agent_ppo2.py:143][0m Total time:      27.71 min
[32m[20221213 23:40:15 @agent_ppo2.py:145][0m 2684928 total steps have happened
[32m[20221213 23:40:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3311 --------------------------#
[32m[20221213 23:40:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:15 @agent_ppo2.py:185][0m |           0.0005 |          36.5109 |           6.1216 |
[32m[20221213 23:40:15 @agent_ppo2.py:185][0m |          -0.0068 |          29.4749 |           6.1867 |
[32m[20221213 23:40:15 @agent_ppo2.py:185][0m |           0.0028 |          28.1113 |           6.1453 |
[32m[20221213 23:40:15 @agent_ppo2.py:185][0m |          -0.0127 |          26.8200 |           6.0721 |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |          -0.0067 |          26.1628 |           6.0782 |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |          -0.0196 |          26.1369 |           5.9696 |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |          -0.0018 |          30.2267 |           6.0171 |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |          -0.0201 |          25.0843 |           5.9979 |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |          -0.0104 |          24.7073 |           5.9828 |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |          -0.0132 |          24.3094 |           6.0172 |
[32m[20221213 23:40:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:40:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.12
[32m[20221213 23:40:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.14
[32m[20221213 23:40:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.82
[32m[20221213 23:40:16 @agent_ppo2.py:143][0m Total time:      27.73 min
[32m[20221213 23:40:16 @agent_ppo2.py:145][0m 2686976 total steps have happened
[32m[20221213 23:40:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3312 --------------------------#
[32m[20221213 23:40:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |           0.0035 |          62.0787 |           5.9835 |
[32m[20221213 23:40:16 @agent_ppo2.py:185][0m |           0.0004 |          55.8753 |           5.9574 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0123 |          53.3964 |           6.0166 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0135 |          51.3357 |           6.0646 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0135 |          51.1579 |           6.0620 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0061 |          51.0534 |           6.1081 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0156 |          48.3610 |           6.0519 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0156 |          47.3923 |           6.0780 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0136 |          47.4054 |           6.1089 |
[32m[20221213 23:40:17 @agent_ppo2.py:185][0m |          -0.0126 |          46.0453 |           6.1163 |
[32m[20221213 23:40:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.14
[32m[20221213 23:40:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.18
[32m[20221213 23:40:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.83
[32m[20221213 23:40:17 @agent_ppo2.py:143][0m Total time:      27.76 min
[32m[20221213 23:40:17 @agent_ppo2.py:145][0m 2689024 total steps have happened
[32m[20221213 23:40:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3313 --------------------------#
[32m[20221213 23:40:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |           0.0172 |          74.5485 |           6.1748 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |           0.0008 |          65.9895 |           6.1170 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0030 |          65.0315 |           6.1429 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0034 |          64.3679 |           6.0849 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0042 |          63.1117 |           6.1059 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0086 |          62.8587 |           6.1379 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0099 |          62.1394 |           6.0123 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0112 |          61.8890 |           6.0462 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0159 |          61.4520 |           6.0420 |
[32m[20221213 23:40:18 @agent_ppo2.py:185][0m |          -0.0109 |          61.3450 |           6.0739 |
[32m[20221213 23:40:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:40:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.94
[32m[20221213 23:40:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.51
[32m[20221213 23:40:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.12
[32m[20221213 23:40:19 @agent_ppo2.py:143][0m Total time:      27.78 min
[32m[20221213 23:40:19 @agent_ppo2.py:145][0m 2691072 total steps have happened
[32m[20221213 23:40:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3314 --------------------------#
[32m[20221213 23:40:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:19 @agent_ppo2.py:185][0m |           0.0119 |          65.8749 |           6.0278 |
[32m[20221213 23:40:19 @agent_ppo2.py:185][0m |          -0.0026 |          56.7957 |           6.1207 |
[32m[20221213 23:40:19 @agent_ppo2.py:185][0m |          -0.0078 |          55.1411 |           6.0764 |
[32m[20221213 23:40:19 @agent_ppo2.py:185][0m |          -0.0039 |          54.1298 |           6.0785 |
[32m[20221213 23:40:19 @agent_ppo2.py:185][0m |          -0.0134 |          54.1813 |           6.0333 |
[32m[20221213 23:40:19 @agent_ppo2.py:185][0m |          -0.0122 |          52.7792 |           6.0366 |
[32m[20221213 23:40:19 @agent_ppo2.py:185][0m |          -0.0136 |          52.8797 |           6.0454 |
[32m[20221213 23:40:20 @agent_ppo2.py:185][0m |          -0.0160 |          52.1720 |           6.0384 |
[32m[20221213 23:40:20 @agent_ppo2.py:185][0m |          -0.0184 |          51.6410 |           5.9665 |
[32m[20221213 23:40:20 @agent_ppo2.py:185][0m |          -0.0161 |          51.1732 |           6.0193 |
[32m[20221213 23:40:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:40:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.50
[32m[20221213 23:40:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.54
[32m[20221213 23:40:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.33
[32m[20221213 23:40:20 @agent_ppo2.py:143][0m Total time:      27.80 min
[32m[20221213 23:40:20 @agent_ppo2.py:145][0m 2693120 total steps have happened
[32m[20221213 23:40:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3315 --------------------------#
[32m[20221213 23:40:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:20 @agent_ppo2.py:185][0m |          -0.0003 |          57.5995 |           5.4697 |
[32m[20221213 23:40:20 @agent_ppo2.py:185][0m |          -0.0063 |          56.2954 |           5.5043 |
[32m[20221213 23:40:20 @agent_ppo2.py:185][0m |          -0.0085 |          55.8011 |           5.4993 |
[32m[20221213 23:40:20 @agent_ppo2.py:185][0m |          -0.0090 |          55.9356 |           5.4412 |
[32m[20221213 23:40:21 @agent_ppo2.py:185][0m |          -0.0075 |          55.3390 |           5.5580 |
[32m[20221213 23:40:21 @agent_ppo2.py:185][0m |          -0.0131 |          55.1307 |           5.5307 |
[32m[20221213 23:40:21 @agent_ppo2.py:185][0m |          -0.0048 |          59.2885 |           5.5756 |
[32m[20221213 23:40:21 @agent_ppo2.py:185][0m |          -0.0094 |          55.1643 |           5.5874 |
[32m[20221213 23:40:21 @agent_ppo2.py:185][0m |          -0.0098 |          54.8005 |           5.5661 |
[32m[20221213 23:40:21 @agent_ppo2.py:185][0m |          -0.0120 |          54.6532 |           5.5836 |
[32m[20221213 23:40:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:40:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.36
[32m[20221213 23:40:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.88
[32m[20221213 23:40:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.93
[32m[20221213 23:40:21 @agent_ppo2.py:143][0m Total time:      27.82 min
[32m[20221213 23:40:21 @agent_ppo2.py:145][0m 2695168 total steps have happened
[32m[20221213 23:40:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3316 --------------------------#
[32m[20221213 23:40:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:21 @agent_ppo2.py:185][0m |           0.0038 |          52.4541 |           6.0533 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0102 |          48.1533 |           6.0684 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0026 |          46.1154 |           6.0752 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0057 |          44.1584 |           6.0803 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0072 |          43.5589 |           6.0781 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0159 |          43.3057 |           6.0102 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0100 |          42.1631 |           6.0707 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0113 |          41.8581 |           6.0352 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0081 |          41.5426 |           6.0244 |
[32m[20221213 23:40:22 @agent_ppo2.py:185][0m |          -0.0105 |          41.4331 |           5.9759 |
[32m[20221213 23:40:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.35
[32m[20221213 23:40:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.57
[32m[20221213 23:40:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.32
[32m[20221213 23:40:22 @agent_ppo2.py:143][0m Total time:      27.84 min
[32m[20221213 23:40:22 @agent_ppo2.py:145][0m 2697216 total steps have happened
[32m[20221213 23:40:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3317 --------------------------#
[32m[20221213 23:40:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |           0.0053 |          61.1502 |           5.6745 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |           0.0009 |          59.6771 |           5.6378 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0047 |          59.2424 |           5.5963 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0023 |          59.1509 |           5.5142 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0067 |          58.8963 |           5.5609 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0085 |          58.8350 |           5.5450 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0082 |          58.7208 |           5.4594 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0054 |          58.5972 |           5.4265 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0069 |          59.0200 |           5.4906 |
[32m[20221213 23:40:23 @agent_ppo2.py:185][0m |          -0.0075 |          58.3439 |           5.4020 |
[32m[20221213 23:40:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:40:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.75
[32m[20221213 23:40:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.80
[32m[20221213 23:40:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.58
[32m[20221213 23:40:24 @agent_ppo2.py:143][0m Total time:      27.86 min
[32m[20221213 23:40:24 @agent_ppo2.py:145][0m 2699264 total steps have happened
[32m[20221213 23:40:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3318 --------------------------#
[32m[20221213 23:40:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:24 @agent_ppo2.py:185][0m |           0.0000 |          33.8057 |           6.0829 |
[32m[20221213 23:40:24 @agent_ppo2.py:185][0m |          -0.0021 |          27.7641 |           6.1955 |
[32m[20221213 23:40:24 @agent_ppo2.py:185][0m |          -0.0085 |          26.1986 |           6.0781 |
[32m[20221213 23:40:24 @agent_ppo2.py:185][0m |          -0.0121 |          25.3611 |           6.1267 |
[32m[20221213 23:40:24 @agent_ppo2.py:185][0m |          -0.0163 |          24.7700 |           6.1031 |
[32m[20221213 23:40:24 @agent_ppo2.py:185][0m |          -0.0108 |          24.3617 |           6.1240 |
[32m[20221213 23:40:24 @agent_ppo2.py:185][0m |          -0.0178 |          23.8580 |           6.0851 |
[32m[20221213 23:40:25 @agent_ppo2.py:185][0m |          -0.0177 |          23.7114 |           6.1079 |
[32m[20221213 23:40:25 @agent_ppo2.py:185][0m |          -0.0191 |          23.3872 |           6.0644 |
[32m[20221213 23:40:25 @agent_ppo2.py:185][0m |          -0.0207 |          23.2181 |           6.1249 |
[32m[20221213 23:40:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.21
[32m[20221213 23:40:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.59
[32m[20221213 23:40:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.92
[32m[20221213 23:40:25 @agent_ppo2.py:143][0m Total time:      27.88 min
[32m[20221213 23:40:25 @agent_ppo2.py:145][0m 2701312 total steps have happened
[32m[20221213 23:40:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3319 --------------------------#
[32m[20221213 23:40:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:25 @agent_ppo2.py:185][0m |           0.0012 |          71.8524 |           5.5471 |
[32m[20221213 23:40:25 @agent_ppo2.py:185][0m |          -0.0064 |          68.1872 |           5.4927 |
[32m[20221213 23:40:25 @agent_ppo2.py:185][0m |          -0.0050 |          67.2455 |           5.5742 |
[32m[20221213 23:40:25 @agent_ppo2.py:185][0m |          -0.0067 |          66.6833 |           5.4251 |
[32m[20221213 23:40:26 @agent_ppo2.py:185][0m |          -0.0061 |          67.4485 |           5.4227 |
[32m[20221213 23:40:26 @agent_ppo2.py:185][0m |          -0.0073 |          65.9609 |           5.4663 |
[32m[20221213 23:40:26 @agent_ppo2.py:185][0m |          -0.0055 |          66.8163 |           5.4414 |
[32m[20221213 23:40:26 @agent_ppo2.py:185][0m |          -0.0078 |          65.6935 |           5.3805 |
[32m[20221213 23:40:26 @agent_ppo2.py:185][0m |          -0.0100 |          65.7100 |           5.4455 |
[32m[20221213 23:40:26 @agent_ppo2.py:185][0m |          -0.0122 |          65.0887 |           5.4340 |
[32m[20221213 23:40:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.46
[32m[20221213 23:40:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.93
[32m[20221213 23:40:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.56
[32m[20221213 23:40:26 @agent_ppo2.py:143][0m Total time:      27.90 min
[32m[20221213 23:40:26 @agent_ppo2.py:145][0m 2703360 total steps have happened
[32m[20221213 23:40:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3320 --------------------------#
[32m[20221213 23:40:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:40:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:26 @agent_ppo2.py:185][0m |           0.0071 |          52.7635 |           5.1091 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0060 |          47.5980 |           5.1499 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0041 |          46.4847 |           5.1797 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0076 |          45.5345 |           5.2504 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |           0.0024 |          46.4636 |           5.2697 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0105 |          44.5042 |           5.2498 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0153 |          43.9261 |           5.2931 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0149 |          43.7472 |           5.3069 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0159 |          43.1928 |           5.3299 |
[32m[20221213 23:40:27 @agent_ppo2.py:185][0m |          -0.0144 |          42.8375 |           5.3380 |
[32m[20221213 23:40:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:40:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.59
[32m[20221213 23:40:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.66
[32m[20221213 23:40:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.89
[32m[20221213 23:40:27 @agent_ppo2.py:143][0m Total time:      27.92 min
[32m[20221213 23:40:27 @agent_ppo2.py:145][0m 2705408 total steps have happened
[32m[20221213 23:40:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3321 --------------------------#
[32m[20221213 23:40:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0004 |          62.4595 |           5.4160 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |           0.0005 |          60.1001 |           5.5112 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0061 |          58.4393 |           5.4639 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0114 |          56.7411 |           5.4539 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0065 |          55.7478 |           5.5728 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0125 |          54.9533 |           5.5857 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0111 |          54.5067 |           5.5746 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0120 |          54.2934 |           5.6321 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0151 |          54.0605 |           5.6394 |
[32m[20221213 23:40:28 @agent_ppo2.py:185][0m |          -0.0132 |          53.8333 |           5.7426 |
[32m[20221213 23:40:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.18
[32m[20221213 23:40:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.49
[32m[20221213 23:40:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.36
[32m[20221213 23:40:29 @agent_ppo2.py:143][0m Total time:      27.94 min
[32m[20221213 23:40:29 @agent_ppo2.py:145][0m 2707456 total steps have happened
[32m[20221213 23:40:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3322 --------------------------#
[32m[20221213 23:40:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:29 @agent_ppo2.py:185][0m |           0.0005 |          50.8491 |           5.5489 |
[32m[20221213 23:40:29 @agent_ppo2.py:185][0m |          -0.0107 |          41.1987 |           5.4487 |
[32m[20221213 23:40:29 @agent_ppo2.py:185][0m |          -0.0069 |          38.8702 |           5.4505 |
[32m[20221213 23:40:29 @agent_ppo2.py:185][0m |          -0.0117 |          37.5168 |           5.5052 |
[32m[20221213 23:40:29 @agent_ppo2.py:185][0m |          -0.0088 |          36.3191 |           5.4863 |
[32m[20221213 23:40:29 @agent_ppo2.py:185][0m |          -0.0206 |          35.4742 |           5.4903 |
[32m[20221213 23:40:29 @agent_ppo2.py:185][0m |          -0.0114 |          34.6395 |           5.5266 |
[32m[20221213 23:40:30 @agent_ppo2.py:185][0m |          -0.0158 |          34.4186 |           5.5298 |
[32m[20221213 23:40:30 @agent_ppo2.py:185][0m |          -0.0183 |          33.8592 |           5.4894 |
[32m[20221213 23:40:30 @agent_ppo2.py:185][0m |          -0.0139 |          33.6790 |           5.4824 |
[32m[20221213 23:40:30 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 23:40:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.69
[32m[20221213 23:40:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.58
[32m[20221213 23:40:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.69
[32m[20221213 23:40:30 @agent_ppo2.py:143][0m Total time:      27.97 min
[32m[20221213 23:40:30 @agent_ppo2.py:145][0m 2709504 total steps have happened
[32m[20221213 23:40:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3323 --------------------------#
[32m[20221213 23:40:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:30 @agent_ppo2.py:185][0m |           0.0033 |          41.6578 |           4.3654 |
[32m[20221213 23:40:30 @agent_ppo2.py:185][0m |          -0.0044 |          35.9522 |           4.4887 |
[32m[20221213 23:40:30 @agent_ppo2.py:185][0m |          -0.0086 |          33.7346 |           4.5621 |
[32m[20221213 23:40:31 @agent_ppo2.py:185][0m |          -0.0092 |          32.7656 |           4.4642 |
[32m[20221213 23:40:31 @agent_ppo2.py:185][0m |          -0.0107 |          32.8790 |           4.5197 |
[32m[20221213 23:40:31 @agent_ppo2.py:185][0m |          -0.0091 |          31.2982 |           4.5222 |
[32m[20221213 23:40:31 @agent_ppo2.py:185][0m |          -0.0050 |          35.0607 |           4.5096 |
[32m[20221213 23:40:31 @agent_ppo2.py:185][0m |          -0.0085 |          30.8129 |           4.6134 |
[32m[20221213 23:40:31 @agent_ppo2.py:185][0m |          -0.0161 |          30.0601 |           4.6479 |
[32m[20221213 23:40:31 @agent_ppo2.py:185][0m |          -0.0206 |          29.7125 |           4.5955 |
[32m[20221213 23:40:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.30
[32m[20221213 23:40:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.88
[32m[20221213 23:40:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.49
[32m[20221213 23:40:31 @agent_ppo2.py:143][0m Total time:      27.99 min
[32m[20221213 23:40:31 @agent_ppo2.py:145][0m 2711552 total steps have happened
[32m[20221213 23:40:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3324 --------------------------#
[32m[20221213 23:40:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |           0.0015 |          57.0786 |           5.7077 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0037 |          55.4550 |           5.6848 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0061 |          55.1222 |           5.6615 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0088 |          54.9061 |           5.6168 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0058 |          54.6447 |           5.6496 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0107 |          54.5817 |           5.6265 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0093 |          54.4663 |           5.5366 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0060 |          54.8018 |           5.4873 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0070 |          54.2333 |           5.5136 |
[32m[20221213 23:40:32 @agent_ppo2.py:185][0m |          -0.0104 |          54.1362 |           5.4870 |
[32m[20221213 23:40:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:40:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.45
[32m[20221213 23:40:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.21
[32m[20221213 23:40:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.61
[32m[20221213 23:40:32 @agent_ppo2.py:143][0m Total time:      28.01 min
[32m[20221213 23:40:32 @agent_ppo2.py:145][0m 2713600 total steps have happened
[32m[20221213 23:40:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3325 --------------------------#
[32m[20221213 23:40:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |           0.0002 |          53.4277 |           5.2404 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |          -0.0005 |          46.5569 |           5.2106 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |          -0.0078 |          44.1428 |           5.0907 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |          -0.0092 |          42.6081 |           5.1380 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |          -0.0040 |          41.7287 |           5.1434 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |           0.0037 |          44.4700 |           5.0402 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |          -0.0082 |          40.7076 |           5.1098 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |          -0.0144 |          40.1269 |           5.0962 |
[32m[20221213 23:40:33 @agent_ppo2.py:185][0m |          -0.0120 |          39.7022 |           5.1353 |
[32m[20221213 23:40:34 @agent_ppo2.py:185][0m |          -0.0110 |          39.2807 |           5.1056 |
[32m[20221213 23:40:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:40:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.49
[32m[20221213 23:40:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.26
[32m[20221213 23:40:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.67
[32m[20221213 23:40:34 @agent_ppo2.py:143][0m Total time:      28.03 min
[32m[20221213 23:40:34 @agent_ppo2.py:145][0m 2715648 total steps have happened
[32m[20221213 23:40:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3326 --------------------------#
[32m[20221213 23:40:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:34 @agent_ppo2.py:185][0m |           0.0018 |          60.1909 |           5.3952 |
[32m[20221213 23:40:34 @agent_ppo2.py:185][0m |          -0.0065 |          56.1692 |           5.3520 |
[32m[20221213 23:40:34 @agent_ppo2.py:185][0m |          -0.0107 |          54.6030 |           5.2731 |
[32m[20221213 23:40:34 @agent_ppo2.py:185][0m |          -0.0118 |          53.4283 |           5.3587 |
[32m[20221213 23:40:34 @agent_ppo2.py:185][0m |          -0.0139 |          53.0618 |           5.2287 |
[32m[20221213 23:40:34 @agent_ppo2.py:185][0m |          -0.0106 |          52.1596 |           5.2634 |
[32m[20221213 23:40:35 @agent_ppo2.py:185][0m |          -0.0158 |          51.5330 |           5.2435 |
[32m[20221213 23:40:35 @agent_ppo2.py:185][0m |          -0.0123 |          51.0276 |           5.2178 |
[32m[20221213 23:40:35 @agent_ppo2.py:185][0m |          -0.0045 |          53.5020 |           5.2556 |
[32m[20221213 23:40:35 @agent_ppo2.py:185][0m |          -0.0161 |          50.2609 |           5.2587 |
[32m[20221213 23:40:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.18
[32m[20221213 23:40:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.15
[32m[20221213 23:40:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.57
[32m[20221213 23:40:35 @agent_ppo2.py:143][0m Total time:      28.05 min
[32m[20221213 23:40:35 @agent_ppo2.py:145][0m 2717696 total steps have happened
[32m[20221213 23:40:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3327 --------------------------#
[32m[20221213 23:40:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:35 @agent_ppo2.py:185][0m |           0.0054 |          51.5790 |           5.3977 |
[32m[20221213 23:40:35 @agent_ppo2.py:185][0m |          -0.0049 |          38.0173 |           5.5104 |
[32m[20221213 23:40:35 @agent_ppo2.py:185][0m |           0.0012 |          36.3562 |           5.5107 |
[32m[20221213 23:40:36 @agent_ppo2.py:185][0m |          -0.0075 |          37.3941 |           5.4451 |
[32m[20221213 23:40:36 @agent_ppo2.py:185][0m |          -0.0094 |          35.2510 |           5.4551 |
[32m[20221213 23:40:36 @agent_ppo2.py:185][0m |          -0.0110 |          34.8733 |           5.3911 |
[32m[20221213 23:40:36 @agent_ppo2.py:185][0m |          -0.0069 |          34.4000 |           5.4302 |
[32m[20221213 23:40:36 @agent_ppo2.py:185][0m |          -0.0091 |          34.1893 |           5.3430 |
[32m[20221213 23:40:36 @agent_ppo2.py:185][0m |          -0.0124 |          34.2677 |           5.3086 |
[32m[20221213 23:40:36 @agent_ppo2.py:185][0m |          -0.0098 |          33.8944 |           5.3452 |
[32m[20221213 23:40:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.12
[32m[20221213 23:40:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.50
[32m[20221213 23:40:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.29
[32m[20221213 23:40:36 @agent_ppo2.py:143][0m Total time:      28.07 min
[32m[20221213 23:40:36 @agent_ppo2.py:145][0m 2719744 total steps have happened
[32m[20221213 23:40:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3328 --------------------------#
[32m[20221213 23:40:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |           0.0010 |          60.6563 |           4.7028 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0020 |          54.6430 |           4.6740 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0048 |          52.4940 |           4.6847 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0083 |          51.0285 |           4.6685 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0014 |          50.6434 |           4.6252 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0078 |          48.9737 |           4.5864 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0124 |          48.3167 |           4.6051 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0011 |          51.4243 |           4.6414 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0083 |          47.7408 |           4.5676 |
[32m[20221213 23:40:37 @agent_ppo2.py:185][0m |          -0.0095 |          47.8254 |           4.5665 |
[32m[20221213 23:40:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.67
[32m[20221213 23:40:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.29
[32m[20221213 23:40:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.47
[32m[20221213 23:40:37 @agent_ppo2.py:143][0m Total time:      28.09 min
[32m[20221213 23:40:37 @agent_ppo2.py:145][0m 2721792 total steps have happened
[32m[20221213 23:40:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3329 --------------------------#
[32m[20221213 23:40:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0008 |          66.1745 |           4.9361 |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0047 |          63.7402 |           5.0234 |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0076 |          62.8725 |           4.9712 |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0089 |          62.3568 |           4.9891 |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0059 |          62.6492 |           5.0099 |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0111 |          61.8474 |           4.9988 |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0124 |          61.3869 |           5.0114 |
[32m[20221213 23:40:38 @agent_ppo2.py:185][0m |          -0.0121 |          61.1319 |           4.9769 |
[32m[20221213 23:40:39 @agent_ppo2.py:185][0m |          -0.0015 |          68.4667 |           5.0051 |
[32m[20221213 23:40:39 @agent_ppo2.py:185][0m |          -0.0121 |          60.8263 |           4.9441 |
[32m[20221213 23:40:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:40:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.44
[32m[20221213 23:40:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.53
[32m[20221213 23:40:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.83
[32m[20221213 23:40:39 @agent_ppo2.py:143][0m Total time:      28.11 min
[32m[20221213 23:40:39 @agent_ppo2.py:145][0m 2723840 total steps have happened
[32m[20221213 23:40:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3330 --------------------------#
[32m[20221213 23:40:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:40:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:39 @agent_ppo2.py:185][0m |          -0.0009 |          48.8945 |           5.4606 |
[32m[20221213 23:40:39 @agent_ppo2.py:185][0m |          -0.0059 |          42.6021 |           5.4331 |
[32m[20221213 23:40:39 @agent_ppo2.py:185][0m |           0.0004 |          41.8910 |           5.3580 |
[32m[20221213 23:40:39 @agent_ppo2.py:185][0m |          -0.0003 |          39.9015 |           5.4007 |
[32m[20221213 23:40:39 @agent_ppo2.py:185][0m |          -0.0114 |          39.2178 |           5.3278 |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0136 |          38.4163 |           5.2859 |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0147 |          38.0712 |           5.2759 |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0190 |          37.6172 |           5.2687 |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0135 |          37.8915 |           5.2371 |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0159 |          37.5387 |           5.2105 |
[32m[20221213 23:40:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.49
[32m[20221213 23:40:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.01
[32m[20221213 23:40:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.89
[32m[20221213 23:40:40 @agent_ppo2.py:143][0m Total time:      28.13 min
[32m[20221213 23:40:40 @agent_ppo2.py:145][0m 2725888 total steps have happened
[32m[20221213 23:40:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3331 --------------------------#
[32m[20221213 23:40:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0012 |          63.9266 |           4.5628 |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0017 |          65.9982 |           4.6145 |
[32m[20221213 23:40:40 @agent_ppo2.py:185][0m |          -0.0041 |          62.7765 |           4.5594 |
[32m[20221213 23:40:41 @agent_ppo2.py:185][0m |          -0.0080 |          61.6576 |           4.6660 |
[32m[20221213 23:40:41 @agent_ppo2.py:185][0m |          -0.0118 |          61.7394 |           4.6316 |
[32m[20221213 23:40:41 @agent_ppo2.py:185][0m |          -0.0109 |          61.3133 |           4.6344 |
[32m[20221213 23:40:41 @agent_ppo2.py:185][0m |          -0.0047 |          62.2867 |           4.6247 |
[32m[20221213 23:40:41 @agent_ppo2.py:185][0m |          -0.0136 |          61.1501 |           4.6462 |
[32m[20221213 23:40:41 @agent_ppo2.py:185][0m |          -0.0103 |          61.1095 |           4.6896 |
[32m[20221213 23:40:41 @agent_ppo2.py:185][0m |          -0.0137 |          61.0425 |           4.6988 |
[32m[20221213 23:40:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.64
[32m[20221213 23:40:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.16
[32m[20221213 23:40:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.43
[32m[20221213 23:40:41 @agent_ppo2.py:143][0m Total time:      28.15 min
[32m[20221213 23:40:41 @agent_ppo2.py:145][0m 2727936 total steps have happened
[32m[20221213 23:40:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3332 --------------------------#
[32m[20221213 23:40:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |           0.0021 |          68.7040 |           4.5784 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0043 |          67.6978 |           4.6411 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0059 |          67.2045 |           4.5704 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |           0.0043 |          70.7478 |           4.6011 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0053 |          66.9345 |           4.5280 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0088 |          66.6715 |           4.5362 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0078 |          66.5227 |           4.4854 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0087 |          66.5264 |           4.5591 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0109 |          66.4663 |           4.5553 |
[32m[20221213 23:40:42 @agent_ppo2.py:185][0m |          -0.0108 |          66.1649 |           4.4937 |
[32m[20221213 23:40:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.36
[32m[20221213 23:40:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.70
[32m[20221213 23:40:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.54
[32m[20221213 23:40:42 @agent_ppo2.py:143][0m Total time:      28.17 min
[32m[20221213 23:40:42 @agent_ppo2.py:145][0m 2729984 total steps have happened
[32m[20221213 23:40:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3333 --------------------------#
[32m[20221213 23:40:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |           0.0004 |          53.7285 |           4.5722 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |          -0.0046 |          52.0139 |           4.6329 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |          -0.0109 |          51.3564 |           4.5735 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |           0.0019 |          52.7169 |           4.6115 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |          -0.0089 |          50.6973 |           4.6647 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |          -0.0027 |          54.4285 |           4.7008 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |          -0.0019 |          53.7827 |           4.6464 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |          -0.0108 |          50.2076 |           4.6911 |
[32m[20221213 23:40:43 @agent_ppo2.py:185][0m |          -0.0117 |          50.0055 |           4.6782 |
[32m[20221213 23:40:44 @agent_ppo2.py:185][0m |          -0.0121 |          50.1368 |           4.7545 |
[32m[20221213 23:40:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.95
[32m[20221213 23:40:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.74
[32m[20221213 23:40:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.59
[32m[20221213 23:40:44 @agent_ppo2.py:143][0m Total time:      28.20 min
[32m[20221213 23:40:44 @agent_ppo2.py:145][0m 2732032 total steps have happened
[32m[20221213 23:40:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3334 --------------------------#
[32m[20221213 23:40:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:44 @agent_ppo2.py:185][0m |          -0.0004 |          60.4704 |           4.7530 |
[32m[20221213 23:40:44 @agent_ppo2.py:185][0m |          -0.0027 |          59.3935 |           4.6898 |
[32m[20221213 23:40:44 @agent_ppo2.py:185][0m |          -0.0078 |          58.9159 |           4.7119 |
[32m[20221213 23:40:44 @agent_ppo2.py:185][0m |          -0.0077 |          58.7062 |           4.8051 |
[32m[20221213 23:40:44 @agent_ppo2.py:185][0m |          -0.0084 |          58.5908 |           4.6833 |
[32m[20221213 23:40:45 @agent_ppo2.py:185][0m |          -0.0029 |          59.1327 |           4.6840 |
[32m[20221213 23:40:45 @agent_ppo2.py:185][0m |          -0.0025 |          61.9712 |           4.6455 |
[32m[20221213 23:40:45 @agent_ppo2.py:185][0m |          -0.0086 |          58.0905 |           4.8232 |
[32m[20221213 23:40:45 @agent_ppo2.py:185][0m |          -0.0094 |          58.0645 |           4.7166 |
[32m[20221213 23:40:45 @agent_ppo2.py:185][0m |          -0.0068 |          58.0070 |           4.7438 |
[32m[20221213 23:40:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:40:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.21
[32m[20221213 23:40:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.05
[32m[20221213 23:40:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.71
[32m[20221213 23:40:45 @agent_ppo2.py:143][0m Total time:      28.22 min
[32m[20221213 23:40:45 @agent_ppo2.py:145][0m 2734080 total steps have happened
[32m[20221213 23:40:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3335 --------------------------#
[32m[20221213 23:40:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:45 @agent_ppo2.py:185][0m |           0.0048 |          56.8965 |           5.0233 |
[32m[20221213 23:40:45 @agent_ppo2.py:185][0m |          -0.0041 |          54.5367 |           5.1475 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0000 |          54.3245 |           5.1393 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0062 |          52.9737 |           5.1752 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0008 |          53.3462 |           5.1605 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0062 |          52.2727 |           5.0954 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0061 |          51.8766 |           5.1286 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0070 |          51.5636 |           5.0492 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0068 |          51.5161 |           5.0831 |
[32m[20221213 23:40:46 @agent_ppo2.py:185][0m |          -0.0084 |          51.1955 |           5.1049 |
[32m[20221213 23:40:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.91
[32m[20221213 23:40:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.27
[32m[20221213 23:40:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.26
[32m[20221213 23:40:46 @agent_ppo2.py:143][0m Total time:      28.24 min
[32m[20221213 23:40:46 @agent_ppo2.py:145][0m 2736128 total steps have happened
[32m[20221213 23:40:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3336 --------------------------#
[32m[20221213 23:40:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |           0.0135 |          66.5535 |           3.8692 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |           0.0001 |          62.1715 |           3.7856 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |          -0.0023 |          60.4082 |           3.8706 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |          -0.0051 |          59.9789 |           3.8602 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |           0.0031 |          63.4048 |           3.9243 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |          -0.0057 |          59.4631 |           3.9181 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |          -0.0099 |          58.9711 |           3.9793 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |          -0.0026 |          61.7646 |           3.9735 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |          -0.0072 |          58.5553 |           3.9926 |
[32m[20221213 23:40:47 @agent_ppo2.py:185][0m |          -0.0099 |          58.2604 |           3.9834 |
[32m[20221213 23:40:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.32
[32m[20221213 23:40:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.66
[32m[20221213 23:40:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.29
[32m[20221213 23:40:48 @agent_ppo2.py:143][0m Total time:      28.26 min
[32m[20221213 23:40:48 @agent_ppo2.py:145][0m 2738176 total steps have happened
[32m[20221213 23:40:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3337 --------------------------#
[32m[20221213 23:40:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0016 |          58.4814 |           4.5270 |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0033 |          53.6246 |           4.5265 |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0061 |          53.0736 |           4.5071 |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0078 |          51.3595 |           4.4821 |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0128 |          50.7378 |           4.4164 |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0113 |          50.0804 |           4.4479 |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0044 |          50.3460 |           4.4658 |
[32m[20221213 23:40:48 @agent_ppo2.py:185][0m |          -0.0124 |          49.5610 |           4.5012 |
[32m[20221213 23:40:49 @agent_ppo2.py:185][0m |          -0.0149 |          49.2914 |           4.5041 |
[32m[20221213 23:40:49 @agent_ppo2.py:185][0m |          -0.0140 |          48.7055 |           4.4718 |
[32m[20221213 23:40:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.59
[32m[20221213 23:40:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.43
[32m[20221213 23:40:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.78
[32m[20221213 23:40:49 @agent_ppo2.py:143][0m Total time:      28.28 min
[32m[20221213 23:40:49 @agent_ppo2.py:145][0m 2740224 total steps have happened
[32m[20221213 23:40:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3338 --------------------------#
[32m[20221213 23:40:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:49 @agent_ppo2.py:185][0m |          -0.0004 |          64.6678 |           4.7255 |
[32m[20221213 23:40:49 @agent_ppo2.py:185][0m |          -0.0047 |          62.2379 |           4.7160 |
[32m[20221213 23:40:49 @agent_ppo2.py:185][0m |          -0.0055 |          61.7843 |           4.6689 |
[32m[20221213 23:40:49 @agent_ppo2.py:185][0m |          -0.0047 |          61.4943 |           4.7450 |
[32m[20221213 23:40:49 @agent_ppo2.py:185][0m |          -0.0081 |          61.3106 |           4.7678 |
[32m[20221213 23:40:50 @agent_ppo2.py:185][0m |          -0.0082 |          61.0815 |           4.7555 |
[32m[20221213 23:40:50 @agent_ppo2.py:185][0m |          -0.0099 |          60.9518 |           4.7780 |
[32m[20221213 23:40:50 @agent_ppo2.py:185][0m |          -0.0096 |          60.8056 |           4.8407 |
[32m[20221213 23:40:50 @agent_ppo2.py:185][0m |          -0.0082 |          60.6947 |           4.8366 |
[32m[20221213 23:40:50 @agent_ppo2.py:185][0m |          -0.0106 |          60.5707 |           4.7769 |
[32m[20221213 23:40:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.29
[32m[20221213 23:40:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.98
[32m[20221213 23:40:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.42
[32m[20221213 23:40:50 @agent_ppo2.py:143][0m Total time:      28.30 min
[32m[20221213 23:40:50 @agent_ppo2.py:145][0m 2742272 total steps have happened
[32m[20221213 23:40:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3339 --------------------------#
[32m[20221213 23:40:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:50 @agent_ppo2.py:185][0m |          -0.0022 |          50.8221 |           5.1478 |
[32m[20221213 23:40:50 @agent_ppo2.py:185][0m |          -0.0050 |          44.7160 |           5.2180 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0025 |          42.7644 |           5.1758 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0135 |          41.7040 |           5.1839 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0155 |          41.0332 |           5.0866 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0147 |          40.5192 |           5.1803 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0135 |          40.3264 |           5.1088 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0091 |          40.2496 |           5.0659 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0105 |          39.4122 |           5.1059 |
[32m[20221213 23:40:51 @agent_ppo2.py:185][0m |          -0.0147 |          38.8736 |           5.0577 |
[32m[20221213 23:40:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:40:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.64
[32m[20221213 23:40:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.17
[32m[20221213 23:40:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.12
[32m[20221213 23:40:51 @agent_ppo2.py:143][0m Total time:      28.32 min
[32m[20221213 23:40:51 @agent_ppo2.py:145][0m 2744320 total steps have happened
[32m[20221213 23:40:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3340 --------------------------#
[32m[20221213 23:40:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:40:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0013 |          54.0692 |           3.9706 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0032 |          51.3918 |           3.9515 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0035 |          50.8055 |           3.9209 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0046 |          50.1649 |           3.9571 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0036 |          49.9597 |           3.9134 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0042 |          50.3127 |           3.8605 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0100 |          49.2556 |           3.8648 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0109 |          49.2864 |           3.8732 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0130 |          48.9348 |           3.8669 |
[32m[20221213 23:40:52 @agent_ppo2.py:185][0m |          -0.0091 |          48.8303 |           3.8496 |
[32m[20221213 23:40:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:40:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.27
[32m[20221213 23:40:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.75
[32m[20221213 23:40:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.21
[32m[20221213 23:40:53 @agent_ppo2.py:143][0m Total time:      28.34 min
[32m[20221213 23:40:53 @agent_ppo2.py:145][0m 2746368 total steps have happened
[32m[20221213 23:40:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3341 --------------------------#
[32m[20221213 23:40:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0000 |          57.9832 |           3.8425 |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0041 |          56.9205 |           3.7924 |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0073 |          56.4727 |           3.7590 |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0074 |          56.2019 |           3.7488 |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0052 |          56.0346 |           3.8816 |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0099 |          55.6136 |           3.7263 |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0122 |          55.6600 |           3.8014 |
[32m[20221213 23:40:53 @agent_ppo2.py:185][0m |          -0.0114 |          55.3500 |           3.7212 |
[32m[20221213 23:40:54 @agent_ppo2.py:185][0m |          -0.0039 |          57.9359 |           3.8119 |
[32m[20221213 23:40:54 @agent_ppo2.py:185][0m |          -0.0078 |          56.8147 |           3.7404 |
[32m[20221213 23:40:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:40:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.77
[32m[20221213 23:40:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.94
[32m[20221213 23:40:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.88
[32m[20221213 23:40:54 @agent_ppo2.py:143][0m Total time:      28.36 min
[32m[20221213 23:40:54 @agent_ppo2.py:145][0m 2748416 total steps have happened
[32m[20221213 23:40:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3342 --------------------------#
[32m[20221213 23:40:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:54 @agent_ppo2.py:185][0m |           0.0005 |          60.6309 |           3.7617 |
[32m[20221213 23:40:54 @agent_ppo2.py:185][0m |          -0.0034 |          56.0424 |           3.8160 |
[32m[20221213 23:40:54 @agent_ppo2.py:185][0m |          -0.0072 |          55.2313 |           3.7931 |
[32m[20221213 23:40:54 @agent_ppo2.py:185][0m |          -0.0037 |          54.9820 |           3.9454 |
[32m[20221213 23:40:54 @agent_ppo2.py:185][0m |          -0.0090 |          54.3301 |           3.8279 |
[32m[20221213 23:40:55 @agent_ppo2.py:185][0m |          -0.0112 |          53.9494 |           3.9001 |
[32m[20221213 23:40:55 @agent_ppo2.py:185][0m |          -0.0033 |          55.7419 |           3.8514 |
[32m[20221213 23:40:55 @agent_ppo2.py:185][0m |          -0.0096 |          53.5400 |           3.9312 |
[32m[20221213 23:40:55 @agent_ppo2.py:185][0m |          -0.0124 |          53.3318 |           3.9255 |
[32m[20221213 23:40:55 @agent_ppo2.py:185][0m |          -0.0117 |          53.5357 |           3.9817 |
[32m[20221213 23:40:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.99
[32m[20221213 23:40:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.01
[32m[20221213 23:40:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.09
[32m[20221213 23:40:55 @agent_ppo2.py:143][0m Total time:      28.38 min
[32m[20221213 23:40:55 @agent_ppo2.py:145][0m 2750464 total steps have happened
[32m[20221213 23:40:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3343 --------------------------#
[32m[20221213 23:40:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:55 @agent_ppo2.py:185][0m |           0.0028 |          59.9325 |           4.5003 |
[32m[20221213 23:40:55 @agent_ppo2.py:185][0m |          -0.0034 |          58.6728 |           4.5119 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |          -0.0049 |          58.3227 |           4.5240 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |          -0.0035 |          58.2236 |           4.6023 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |          -0.0045 |          57.9213 |           4.5524 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |          -0.0046 |          57.9229 |           4.5753 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |           0.0010 |          59.9582 |           4.7171 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |          -0.0093 |          57.6024 |           4.6731 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |          -0.0069 |          57.3422 |           4.6727 |
[32m[20221213 23:40:56 @agent_ppo2.py:185][0m |          -0.0114 |          57.4144 |           4.7488 |
[32m[20221213 23:40:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.70
[32m[20221213 23:40:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.72
[32m[20221213 23:40:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.74
[32m[20221213 23:40:56 @agent_ppo2.py:143][0m Total time:      28.40 min
[32m[20221213 23:40:56 @agent_ppo2.py:145][0m 2752512 total steps have happened
[32m[20221213 23:40:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3344 --------------------------#
[32m[20221213 23:40:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0005 |          59.4001 |           4.1036 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0061 |          54.1856 |           4.0873 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0123 |          51.7000 |           4.0696 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0117 |          50.2602 |           4.1671 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0058 |          49.4275 |           4.0772 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0111 |          49.0303 |           4.1420 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0125 |          48.5632 |           4.0838 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0128 |          48.1658 |           4.0652 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0133 |          48.0197 |           4.0536 |
[32m[20221213 23:40:57 @agent_ppo2.py:185][0m |          -0.0146 |          47.5473 |           4.0537 |
[32m[20221213 23:40:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:40:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.66
[32m[20221213 23:40:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.55
[32m[20221213 23:40:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.09
[32m[20221213 23:40:58 @agent_ppo2.py:143][0m Total time:      28.43 min
[32m[20221213 23:40:58 @agent_ppo2.py:145][0m 2754560 total steps have happened
[32m[20221213 23:40:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3345 --------------------------#
[32m[20221213 23:40:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:40:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |           0.0012 |          60.5406 |           4.7093 |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |          -0.0054 |          58.1904 |           4.8229 |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |          -0.0034 |          57.2169 |           4.8549 |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |          -0.0095 |          56.2969 |           4.9762 |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |          -0.0109 |          55.7537 |           4.9949 |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |          -0.0114 |          55.2340 |           5.0868 |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |          -0.0112 |          54.9884 |           5.0722 |
[32m[20221213 23:40:58 @agent_ppo2.py:185][0m |          -0.0111 |          54.5985 |           5.1093 |
[32m[20221213 23:40:59 @agent_ppo2.py:185][0m |          -0.0100 |          54.3834 |           5.1661 |
[32m[20221213 23:40:59 @agent_ppo2.py:185][0m |          -0.0109 |          54.0678 |           5.2382 |
[32m[20221213 23:40:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:40:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.28
[32m[20221213 23:40:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.89
[32m[20221213 23:40:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.69
[32m[20221213 23:40:59 @agent_ppo2.py:143][0m Total time:      28.45 min
[32m[20221213 23:40:59 @agent_ppo2.py:145][0m 2756608 total steps have happened
[32m[20221213 23:40:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3346 --------------------------#
[32m[20221213 23:40:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:40:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:40:59 @agent_ppo2.py:185][0m |           0.0046 |          60.8756 |           4.9192 |
[32m[20221213 23:40:59 @agent_ppo2.py:185][0m |          -0.0053 |          57.9438 |           4.8354 |
[32m[20221213 23:40:59 @agent_ppo2.py:185][0m |          -0.0074 |          56.7366 |           4.8181 |
[32m[20221213 23:40:59 @agent_ppo2.py:185][0m |          -0.0085 |          55.9232 |           4.7548 |
[32m[20221213 23:40:59 @agent_ppo2.py:185][0m |          -0.0101 |          55.4840 |           4.7667 |
[32m[20221213 23:41:00 @agent_ppo2.py:185][0m |          -0.0087 |          54.9146 |           4.7118 |
[32m[20221213 23:41:00 @agent_ppo2.py:185][0m |          -0.0078 |          54.6367 |           4.8126 |
[32m[20221213 23:41:00 @agent_ppo2.py:185][0m |          -0.0130 |          54.3209 |           4.8380 |
[32m[20221213 23:41:00 @agent_ppo2.py:185][0m |          -0.0113 |          53.8955 |           4.7786 |
[32m[20221213 23:41:00 @agent_ppo2.py:185][0m |          -0.0134 |          53.4275 |           4.7309 |
[32m[20221213 23:41:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.23
[32m[20221213 23:41:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.52
[32m[20221213 23:41:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.39
[32m[20221213 23:41:00 @agent_ppo2.py:143][0m Total time:      28.47 min
[32m[20221213 23:41:00 @agent_ppo2.py:145][0m 2758656 total steps have happened
[32m[20221213 23:41:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3347 --------------------------#
[32m[20221213 23:41:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:00 @agent_ppo2.py:185][0m |          -0.0024 |          61.5732 |           5.4744 |
[32m[20221213 23:41:00 @agent_ppo2.py:185][0m |          -0.0043 |          57.8362 |           5.3650 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0023 |          56.5927 |           5.4413 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0025 |          55.3736 |           5.4092 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0093 |          54.9202 |           5.3591 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0090 |          54.5015 |           5.3313 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0118 |          53.5868 |           5.3111 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0087 |          54.0756 |           5.3433 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0077 |          52.7371 |           5.3877 |
[32m[20221213 23:41:01 @agent_ppo2.py:185][0m |          -0.0111 |          52.4602 |           5.4022 |
[32m[20221213 23:41:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.22
[32m[20221213 23:41:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.33
[32m[20221213 23:41:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.38
[32m[20221213 23:41:01 @agent_ppo2.py:143][0m Total time:      28.49 min
[32m[20221213 23:41:01 @agent_ppo2.py:145][0m 2760704 total steps have happened
[32m[20221213 23:41:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3348 --------------------------#
[32m[20221213 23:41:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |           0.0023 |          58.5950 |           4.6332 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0048 |          55.9735 |           4.6908 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0042 |          57.1111 |           4.7530 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0070 |          54.8068 |           4.8558 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0018 |          55.4279 |           4.8559 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0098 |          53.9945 |           4.9244 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0123 |          53.5897 |           4.9665 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0120 |          53.2855 |           4.9588 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0076 |          53.2941 |           5.1088 |
[32m[20221213 23:41:02 @agent_ppo2.py:185][0m |          -0.0080 |          53.3923 |           5.1317 |
[32m[20221213 23:41:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:41:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.38
[32m[20221213 23:41:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.30
[32m[20221213 23:41:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.34
[32m[20221213 23:41:03 @agent_ppo2.py:143][0m Total time:      28.51 min
[32m[20221213 23:41:03 @agent_ppo2.py:145][0m 2762752 total steps have happened
[32m[20221213 23:41:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3349 --------------------------#
[32m[20221213 23:41:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:03 @agent_ppo2.py:185][0m |           0.0055 |          60.3871 |           5.8875 |
[32m[20221213 23:41:03 @agent_ppo2.py:185][0m |          -0.0032 |          57.5466 |           5.9392 |
[32m[20221213 23:41:03 @agent_ppo2.py:185][0m |          -0.0031 |          56.4350 |           6.0083 |
[32m[20221213 23:41:03 @agent_ppo2.py:185][0m |          -0.0088 |          55.6025 |           5.8794 |
[32m[20221213 23:41:03 @agent_ppo2.py:185][0m |           0.0017 |          58.3604 |           5.9103 |
[32m[20221213 23:41:03 @agent_ppo2.py:185][0m |          -0.0067 |          54.8875 |           5.9106 |
[32m[20221213 23:41:03 @agent_ppo2.py:185][0m |          -0.0089 |          54.4131 |           5.8961 |
[32m[20221213 23:41:04 @agent_ppo2.py:185][0m |          -0.0041 |          54.6309 |           5.9465 |
[32m[20221213 23:41:04 @agent_ppo2.py:185][0m |          -0.0098 |          54.1750 |           5.8723 |
[32m[20221213 23:41:04 @agent_ppo2.py:185][0m |          -0.0105 |          53.6896 |           5.8519 |
[32m[20221213 23:41:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.40
[32m[20221213 23:41:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.13
[32m[20221213 23:41:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.41
[32m[20221213 23:41:04 @agent_ppo2.py:143][0m Total time:      28.53 min
[32m[20221213 23:41:04 @agent_ppo2.py:145][0m 2764800 total steps have happened
[32m[20221213 23:41:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3350 --------------------------#
[32m[20221213 23:41:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:41:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:04 @agent_ppo2.py:185][0m |          -0.0009 |          63.5874 |           4.9440 |
[32m[20221213 23:41:04 @agent_ppo2.py:185][0m |          -0.0055 |          61.6020 |           5.0224 |
[32m[20221213 23:41:04 @agent_ppo2.py:185][0m |          -0.0075 |          60.7564 |           5.0846 |
[32m[20221213 23:41:04 @agent_ppo2.py:185][0m |          -0.0104 |          60.4097 |           5.1081 |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |          -0.0074 |          60.2127 |           5.0856 |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |          -0.0121 |          60.0044 |           5.1060 |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |          -0.0114 |          59.6902 |           5.1185 |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |          -0.0121 |          59.5996 |           5.1100 |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |          -0.0121 |          59.5220 |           5.1148 |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |          -0.0105 |          59.4593 |           5.0990 |
[32m[20221213 23:41:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:41:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.53
[32m[20221213 23:41:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.19
[32m[20221213 23:41:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.50
[32m[20221213 23:41:05 @agent_ppo2.py:143][0m Total time:      28.55 min
[32m[20221213 23:41:05 @agent_ppo2.py:145][0m 2766848 total steps have happened
[32m[20221213 23:41:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3351 --------------------------#
[32m[20221213 23:41:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |           0.0011 |          64.1378 |           5.1376 |
[32m[20221213 23:41:05 @agent_ppo2.py:185][0m |          -0.0029 |          61.4656 |           5.1921 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |          -0.0071 |          61.0638 |           5.0798 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |          -0.0069 |          60.7814 |           5.1311 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |           0.0016 |          66.8798 |           5.1023 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |          -0.0024 |          63.6809 |           5.0788 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |          -0.0097 |          60.3962 |           5.1230 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |          -0.0085 |          60.1876 |           5.0905 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |          -0.0108 |          60.0482 |           5.1618 |
[32m[20221213 23:41:06 @agent_ppo2.py:185][0m |          -0.0140 |          59.9150 |           5.0489 |
[32m[20221213 23:41:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.75
[32m[20221213 23:41:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.54
[32m[20221213 23:41:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.28
[32m[20221213 23:41:06 @agent_ppo2.py:143][0m Total time:      28.57 min
[32m[20221213 23:41:06 @agent_ppo2.py:145][0m 2768896 total steps have happened
[32m[20221213 23:41:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3352 --------------------------#
[32m[20221213 23:41:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0011 |          53.2199 |           4.9550 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0038 |          52.7111 |           4.9018 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0067 |          52.2168 |           4.8345 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0076 |          52.0416 |           4.7410 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0091 |          51.9967 |           4.8059 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0070 |          51.8400 |           4.7236 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0069 |          51.8593 |           4.6236 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |           0.0045 |          54.7288 |           4.6096 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0087 |          51.9390 |           4.6369 |
[32m[20221213 23:41:07 @agent_ppo2.py:185][0m |          -0.0084 |          51.6420 |           4.5583 |
[32m[20221213 23:41:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:41:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.58
[32m[20221213 23:41:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.26
[32m[20221213 23:41:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.69
[32m[20221213 23:41:08 @agent_ppo2.py:143][0m Total time:      28.59 min
[32m[20221213 23:41:08 @agent_ppo2.py:145][0m 2770944 total steps have happened
[32m[20221213 23:41:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3353 --------------------------#
[32m[20221213 23:41:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:08 @agent_ppo2.py:185][0m |           0.0007 |          65.4477 |           4.5043 |
[32m[20221213 23:41:08 @agent_ppo2.py:185][0m |          -0.0019 |          63.3246 |           4.5296 |
[32m[20221213 23:41:08 @agent_ppo2.py:185][0m |          -0.0034 |          62.8557 |           4.5241 |
[32m[20221213 23:41:08 @agent_ppo2.py:185][0m |          -0.0078 |          61.5706 |           4.6078 |
[32m[20221213 23:41:08 @agent_ppo2.py:185][0m |          -0.0066 |          61.4266 |           4.5524 |
[32m[20221213 23:41:08 @agent_ppo2.py:185][0m |          -0.0103 |          60.7729 |           4.5403 |
[32m[20221213 23:41:08 @agent_ppo2.py:185][0m |          -0.0098 |          60.1838 |           4.5768 |
[32m[20221213 23:41:09 @agent_ppo2.py:185][0m |          -0.0084 |          61.3124 |           4.6562 |
[32m[20221213 23:41:09 @agent_ppo2.py:185][0m |          -0.0126 |          60.0637 |           4.6143 |
[32m[20221213 23:41:09 @agent_ppo2.py:185][0m |          -0.0151 |          59.8444 |           4.5803 |
[32m[20221213 23:41:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:41:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.50
[32m[20221213 23:41:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.48
[32m[20221213 23:41:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.01
[32m[20221213 23:41:09 @agent_ppo2.py:143][0m Total time:      28.61 min
[32m[20221213 23:41:09 @agent_ppo2.py:145][0m 2772992 total steps have happened
[32m[20221213 23:41:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3354 --------------------------#
[32m[20221213 23:41:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:41:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:09 @agent_ppo2.py:185][0m |           0.0003 |          61.8965 |           4.7669 |
[32m[20221213 23:41:09 @agent_ppo2.py:185][0m |           0.0009 |          60.3904 |           4.8172 |
[32m[20221213 23:41:09 @agent_ppo2.py:185][0m |          -0.0006 |          59.7758 |           4.7867 |
[32m[20221213 23:41:09 @agent_ppo2.py:185][0m |          -0.0032 |          59.5984 |           4.8319 |
[32m[20221213 23:41:10 @agent_ppo2.py:185][0m |          -0.0044 |          58.7870 |           4.7246 |
[32m[20221213 23:41:10 @agent_ppo2.py:185][0m |          -0.0039 |          58.7738 |           4.9114 |
[32m[20221213 23:41:10 @agent_ppo2.py:185][0m |          -0.0034 |          59.0072 |           4.7795 |
[32m[20221213 23:41:10 @agent_ppo2.py:185][0m |          -0.0077 |          58.1141 |           4.8783 |
[32m[20221213 23:41:10 @agent_ppo2.py:185][0m |           0.0001 |          59.0217 |           4.8498 |
[32m[20221213 23:41:10 @agent_ppo2.py:185][0m |          -0.0127 |          57.9203 |           4.8945 |
[32m[20221213 23:41:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.29
[32m[20221213 23:41:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.91
[32m[20221213 23:41:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.35
[32m[20221213 23:41:10 @agent_ppo2.py:143][0m Total time:      28.63 min
[32m[20221213 23:41:10 @agent_ppo2.py:145][0m 2775040 total steps have happened
[32m[20221213 23:41:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3355 --------------------------#
[32m[20221213 23:41:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:10 @agent_ppo2.py:185][0m |           0.0019 |          61.1841 |           4.4762 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0038 |          59.4061 |           4.4279 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0080 |          58.9140 |           4.3927 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0038 |          58.6083 |           4.5382 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0070 |          58.4157 |           4.4335 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0092 |          58.3054 |           4.4097 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0104 |          58.4491 |           4.4651 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0092 |          58.0187 |           4.4867 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0088 |          59.0861 |           4.4525 |
[32m[20221213 23:41:11 @agent_ppo2.py:185][0m |          -0.0075 |          57.6782 |           4.4956 |
[32m[20221213 23:41:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.23
[32m[20221213 23:41:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.35
[32m[20221213 23:41:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.52
[32m[20221213 23:41:11 @agent_ppo2.py:143][0m Total time:      28.66 min
[32m[20221213 23:41:11 @agent_ppo2.py:145][0m 2777088 total steps have happened
[32m[20221213 23:41:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3356 --------------------------#
[32m[20221213 23:41:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |           0.0120 |          75.5245 |           4.0911 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0058 |          66.3455 |           4.1819 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0023 |          65.5163 |           4.3277 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0094 |          65.1324 |           4.3533 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0096 |          64.7256 |           4.2662 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0103 |          64.7774 |           4.3063 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0107 |          64.3699 |           4.3319 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0035 |          65.8315 |           4.3849 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0044 |          65.0317 |           4.3979 |
[32m[20221213 23:41:12 @agent_ppo2.py:185][0m |          -0.0122 |          63.8980 |           4.4521 |
[32m[20221213 23:41:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.82
[32m[20221213 23:41:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.16
[32m[20221213 23:41:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.38
[32m[20221213 23:41:13 @agent_ppo2.py:143][0m Total time:      28.68 min
[32m[20221213 23:41:13 @agent_ppo2.py:145][0m 2779136 total steps have happened
[32m[20221213 23:41:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3357 --------------------------#
[32m[20221213 23:41:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:13 @agent_ppo2.py:185][0m |          -0.0015 |          58.7684 |           5.0860 |
[32m[20221213 23:41:13 @agent_ppo2.py:185][0m |          -0.0085 |          57.2089 |           5.1802 |
[32m[20221213 23:41:13 @agent_ppo2.py:185][0m |          -0.0111 |          56.6490 |           5.1809 |
[32m[20221213 23:41:13 @agent_ppo2.py:185][0m |          -0.0094 |          56.1380 |           5.2432 |
[32m[20221213 23:41:13 @agent_ppo2.py:185][0m |          -0.0122 |          56.0018 |           5.2376 |
[32m[20221213 23:41:13 @agent_ppo2.py:185][0m |          -0.0108 |          55.5850 |           5.3132 |
[32m[20221213 23:41:13 @agent_ppo2.py:185][0m |          -0.0133 |          55.4201 |           5.3593 |
[32m[20221213 23:41:14 @agent_ppo2.py:185][0m |          -0.0141 |          55.2541 |           5.3193 |
[32m[20221213 23:41:14 @agent_ppo2.py:185][0m |          -0.0118 |          55.0461 |           5.3380 |
[32m[20221213 23:41:14 @agent_ppo2.py:185][0m |          -0.0133 |          54.7383 |           5.4198 |
[32m[20221213 23:41:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.23
[32m[20221213 23:41:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.63
[32m[20221213 23:41:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.64
[32m[20221213 23:41:14 @agent_ppo2.py:143][0m Total time:      28.70 min
[32m[20221213 23:41:14 @agent_ppo2.py:145][0m 2781184 total steps have happened
[32m[20221213 23:41:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3358 --------------------------#
[32m[20221213 23:41:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:14 @agent_ppo2.py:185][0m |           0.0026 |          66.3094 |           6.0695 |
[32m[20221213 23:41:14 @agent_ppo2.py:185][0m |          -0.0055 |          64.1565 |           6.0512 |
[32m[20221213 23:41:14 @agent_ppo2.py:185][0m |          -0.0066 |          63.6917 |           6.0838 |
[32m[20221213 23:41:14 @agent_ppo2.py:185][0m |          -0.0096 |          62.9606 |           6.0996 |
[32m[20221213 23:41:15 @agent_ppo2.py:185][0m |          -0.0058 |          63.3385 |           6.1242 |
[32m[20221213 23:41:15 @agent_ppo2.py:185][0m |          -0.0089 |          62.5284 |           6.0611 |
[32m[20221213 23:41:15 @agent_ppo2.py:185][0m |          -0.0094 |          62.3792 |           6.0048 |
[32m[20221213 23:41:15 @agent_ppo2.py:185][0m |          -0.0116 |          62.1701 |           6.0976 |
[32m[20221213 23:41:15 @agent_ppo2.py:185][0m |          -0.0049 |          62.7492 |           6.0968 |
[32m[20221213 23:41:15 @agent_ppo2.py:185][0m |          -0.0118 |          61.8629 |           6.1407 |
[32m[20221213 23:41:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:41:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.20
[32m[20221213 23:41:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.09
[32m[20221213 23:41:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.11
[32m[20221213 23:41:15 @agent_ppo2.py:143][0m Total time:      28.72 min
[32m[20221213 23:41:15 @agent_ppo2.py:145][0m 2783232 total steps have happened
[32m[20221213 23:41:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3359 --------------------------#
[32m[20221213 23:41:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:15 @agent_ppo2.py:185][0m |          -0.0019 |          65.7093 |           5.6393 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0050 |          64.3531 |           5.7391 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0087 |          61.6551 |           5.7005 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0060 |          61.9435 |           5.7136 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0062 |          62.4715 |           5.8116 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0077 |          61.2862 |           5.6851 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0107 |          60.1483 |           5.7058 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0156 |          59.5656 |           5.7885 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0107 |          61.1980 |           5.7418 |
[32m[20221213 23:41:16 @agent_ppo2.py:185][0m |          -0.0073 |          64.3105 |           5.8122 |
[32m[20221213 23:41:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.25
[32m[20221213 23:41:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.34
[32m[20221213 23:41:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.40
[32m[20221213 23:41:16 @agent_ppo2.py:143][0m Total time:      28.74 min
[32m[20221213 23:41:16 @agent_ppo2.py:145][0m 2785280 total steps have happened
[32m[20221213 23:41:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3360 --------------------------#
[32m[20221213 23:41:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:41:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |           0.0009 |          66.3157 |           5.3438 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |          -0.0040 |          62.8625 |           5.4018 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |           0.0022 |          62.6548 |           5.3890 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |           0.0100 |          65.2193 |           5.4050 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |          -0.0058 |          60.5809 |           5.4569 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |          -0.0070 |          60.5010 |           5.5009 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |          -0.0048 |          60.5013 |           5.4992 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |          -0.0098 |          59.5086 |           5.4319 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |          -0.0080 |          59.1036 |           5.4897 |
[32m[20221213 23:41:17 @agent_ppo2.py:185][0m |          -0.0092 |          59.2198 |           5.4657 |
[32m[20221213 23:41:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.46
[32m[20221213 23:41:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.63
[32m[20221213 23:41:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.69
[32m[20221213 23:41:18 @agent_ppo2.py:143][0m Total time:      28.76 min
[32m[20221213 23:41:18 @agent_ppo2.py:145][0m 2787328 total steps have happened
[32m[20221213 23:41:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3361 --------------------------#
[32m[20221213 23:41:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:18 @agent_ppo2.py:185][0m |          -0.0012 |          66.8265 |           4.9149 |
[32m[20221213 23:41:18 @agent_ppo2.py:185][0m |          -0.0023 |          63.7246 |           4.9482 |
[32m[20221213 23:41:18 @agent_ppo2.py:185][0m |          -0.0067 |          63.0748 |           4.9141 |
[32m[20221213 23:41:18 @agent_ppo2.py:185][0m |          -0.0049 |          62.5512 |           4.8653 |
[32m[20221213 23:41:18 @agent_ppo2.py:185][0m |          -0.0134 |          61.9554 |           4.8758 |
[32m[20221213 23:41:18 @agent_ppo2.py:185][0m |          -0.0136 |          61.7058 |           4.7833 |
[32m[20221213 23:41:18 @agent_ppo2.py:185][0m |          -0.0032 |          67.0162 |           4.8117 |
[32m[20221213 23:41:19 @agent_ppo2.py:185][0m |          -0.0077 |          62.6657 |           4.8474 |
[32m[20221213 23:41:19 @agent_ppo2.py:185][0m |          -0.0162 |          61.0208 |           4.9080 |
[32m[20221213 23:41:19 @agent_ppo2.py:185][0m |          -0.0179 |          60.7420 |           4.8695 |
[32m[20221213 23:41:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.15
[32m[20221213 23:41:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.72
[32m[20221213 23:41:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.72
[32m[20221213 23:41:19 @agent_ppo2.py:143][0m Total time:      28.78 min
[32m[20221213 23:41:19 @agent_ppo2.py:145][0m 2789376 total steps have happened
[32m[20221213 23:41:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3362 --------------------------#
[32m[20221213 23:41:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:19 @agent_ppo2.py:185][0m |           0.0008 |          67.2448 |           5.4269 |
[32m[20221213 23:41:19 @agent_ppo2.py:185][0m |          -0.0056 |          65.5140 |           5.4444 |
[32m[20221213 23:41:19 @agent_ppo2.py:185][0m |          -0.0059 |          64.9411 |           5.4909 |
[32m[20221213 23:41:19 @agent_ppo2.py:185][0m |          -0.0042 |          64.3959 |           5.3633 |
[32m[20221213 23:41:20 @agent_ppo2.py:185][0m |          -0.0036 |          66.0187 |           5.3698 |
[32m[20221213 23:41:20 @agent_ppo2.py:185][0m |          -0.0048 |          63.9933 |           5.3439 |
[32m[20221213 23:41:20 @agent_ppo2.py:185][0m |          -0.0016 |          67.3913 |           5.2934 |
[32m[20221213 23:41:20 @agent_ppo2.py:185][0m |          -0.0092 |          63.9394 |           5.2692 |
[32m[20221213 23:41:20 @agent_ppo2.py:185][0m |          -0.0126 |          63.4122 |           5.2387 |
[32m[20221213 23:41:20 @agent_ppo2.py:185][0m |          -0.0051 |          63.5927 |           5.1552 |
[32m[20221213 23:41:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.83
[32m[20221213 23:41:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.65
[32m[20221213 23:41:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.29
[32m[20221213 23:41:20 @agent_ppo2.py:143][0m Total time:      28.80 min
[32m[20221213 23:41:20 @agent_ppo2.py:145][0m 2791424 total steps have happened
[32m[20221213 23:41:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3363 --------------------------#
[32m[20221213 23:41:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:41:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:20 @agent_ppo2.py:185][0m |           0.0030 |          61.5232 |           4.9144 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |           0.0033 |          58.3748 |           4.8367 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0064 |          54.8315 |           4.7307 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0101 |          53.7865 |           4.7588 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0091 |          53.4166 |           4.7618 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0131 |          52.5798 |           4.7242 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0160 |          52.0881 |           4.7800 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0163 |          51.2963 |           4.7662 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0163 |          50.8276 |           4.7436 |
[32m[20221213 23:41:21 @agent_ppo2.py:185][0m |          -0.0093 |          52.6237 |           4.7844 |
[32m[20221213 23:41:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:41:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.29
[32m[20221213 23:41:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.87
[32m[20221213 23:41:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.99
[32m[20221213 23:41:21 @agent_ppo2.py:143][0m Total time:      28.82 min
[32m[20221213 23:41:21 @agent_ppo2.py:145][0m 2793472 total steps have happened
[32m[20221213 23:41:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3364 --------------------------#
[32m[20221213 23:41:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |           0.0001 |          59.4669 |           4.6063 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |          -0.0032 |          56.7901 |           4.7305 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |          -0.0102 |          55.9104 |           4.7282 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |          -0.0058 |          54.6426 |           4.8350 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |          -0.0110 |          54.2204 |           4.8092 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |           0.0035 |          60.0378 |           4.8232 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |          -0.0110 |          53.5923 |           4.8415 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |          -0.0123 |          53.0743 |           4.7886 |
[32m[20221213 23:41:22 @agent_ppo2.py:185][0m |          -0.0153 |          52.9316 |           4.7899 |
[32m[20221213 23:41:23 @agent_ppo2.py:185][0m |          -0.0094 |          53.8597 |           4.8788 |
[32m[20221213 23:41:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.54
[32m[20221213 23:41:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.07
[32m[20221213 23:41:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.81
[32m[20221213 23:41:23 @agent_ppo2.py:143][0m Total time:      28.84 min
[32m[20221213 23:41:23 @agent_ppo2.py:145][0m 2795520 total steps have happened
[32m[20221213 23:41:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3365 --------------------------#
[32m[20221213 23:41:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:23 @agent_ppo2.py:185][0m |           0.0031 |          68.1351 |           5.3393 |
[32m[20221213 23:41:23 @agent_ppo2.py:185][0m |           0.0048 |          68.4657 |           5.2186 |
[32m[20221213 23:41:23 @agent_ppo2.py:185][0m |          -0.0038 |          64.1365 |           5.3050 |
[32m[20221213 23:41:23 @agent_ppo2.py:185][0m |          -0.0080 |          63.5214 |           5.2541 |
[32m[20221213 23:41:23 @agent_ppo2.py:185][0m |          -0.0073 |          63.0965 |           5.2156 |
[32m[20221213 23:41:23 @agent_ppo2.py:185][0m |          -0.0050 |          64.0985 |           5.2396 |
[32m[20221213 23:41:24 @agent_ppo2.py:185][0m |          -0.0105 |          62.4404 |           5.2361 |
[32m[20221213 23:41:24 @agent_ppo2.py:185][0m |          -0.0081 |          62.3848 |           5.1847 |
[32m[20221213 23:41:24 @agent_ppo2.py:185][0m |          -0.0113 |          62.0192 |           5.0674 |
[32m[20221213 23:41:24 @agent_ppo2.py:185][0m |          -0.0119 |          61.8761 |           5.1531 |
[32m[20221213 23:41:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.31
[32m[20221213 23:41:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.07
[32m[20221213 23:41:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.38
[32m[20221213 23:41:24 @agent_ppo2.py:143][0m Total time:      28.87 min
[32m[20221213 23:41:24 @agent_ppo2.py:145][0m 2797568 total steps have happened
[32m[20221213 23:41:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3366 --------------------------#
[32m[20221213 23:41:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:41:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:24 @agent_ppo2.py:185][0m |           0.0025 |          18.0517 |           4.5573 |
[32m[20221213 23:41:24 @agent_ppo2.py:185][0m |           0.0013 |          16.1221 |           4.4634 |
[32m[20221213 23:41:24 @agent_ppo2.py:185][0m |           0.0007 |          15.9241 |           4.4944 |
[32m[20221213 23:41:25 @agent_ppo2.py:185][0m |           0.0014 |          15.8859 |           4.5294 |
[32m[20221213 23:41:25 @agent_ppo2.py:185][0m |           0.0085 |          16.1708 |           4.3399 |
[32m[20221213 23:41:25 @agent_ppo2.py:185][0m |          -0.0021 |          15.5368 |           4.3962 |
[32m[20221213 23:41:25 @agent_ppo2.py:185][0m |          -0.0062 |          15.4999 |           4.4044 |
[32m[20221213 23:41:25 @agent_ppo2.py:185][0m |          -0.0044 |          15.4658 |           4.3545 |
[32m[20221213 23:41:25 @agent_ppo2.py:185][0m |          -0.0040 |          15.4618 |           4.3766 |
[32m[20221213 23:41:25 @agent_ppo2.py:185][0m |           0.0024 |          15.8638 |           4.3491 |
[32m[20221213 23:41:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:41:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:41:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:41:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.15
[32m[20221213 23:41:25 @agent_ppo2.py:143][0m Total time:      28.89 min
[32m[20221213 23:41:25 @agent_ppo2.py:145][0m 2799616 total steps have happened
[32m[20221213 23:41:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3367 --------------------------#
[32m[20221213 23:41:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |           0.0008 |          55.8859 |           4.1351 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |           0.0001 |          55.0354 |           4.1541 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0109 |          53.9085 |           4.1263 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0062 |          53.5875 |           4.2192 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0098 |          53.4606 |           4.2204 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0064 |          53.4655 |           4.2126 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0114 |          53.2562 |           4.2201 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0085 |          53.0649 |           4.3203 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0115 |          52.9222 |           4.3205 |
[32m[20221213 23:41:26 @agent_ppo2.py:185][0m |          -0.0048 |          53.6922 |           4.2696 |
[32m[20221213 23:41:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.06
[32m[20221213 23:41:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.58
[32m[20221213 23:41:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.37
[32m[20221213 23:41:26 @agent_ppo2.py:143][0m Total time:      28.91 min
[32m[20221213 23:41:26 @agent_ppo2.py:145][0m 2801664 total steps have happened
[32m[20221213 23:41:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3368 --------------------------#
[32m[20221213 23:41:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |           0.0028 |          68.1360 |           4.5612 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |          -0.0026 |          64.4735 |           4.4429 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |          -0.0043 |          63.7099 |           4.5837 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |          -0.0008 |          64.7413 |           4.5643 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |          -0.0065 |          62.8200 |           4.5099 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |          -0.0128 |          62.4537 |           4.6178 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |           0.0034 |          71.7279 |           4.5433 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |          -0.0107 |          62.4931 |           4.6585 |
[32m[20221213 23:41:27 @agent_ppo2.py:185][0m |          -0.0151 |          61.7737 |           4.5885 |
[32m[20221213 23:41:28 @agent_ppo2.py:185][0m |          -0.0046 |          64.7260 |           4.5730 |
[32m[20221213 23:41:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.88
[32m[20221213 23:41:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.14
[32m[20221213 23:41:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.42
[32m[20221213 23:41:28 @agent_ppo2.py:143][0m Total time:      28.93 min
[32m[20221213 23:41:28 @agent_ppo2.py:145][0m 2803712 total steps have happened
[32m[20221213 23:41:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3369 --------------------------#
[32m[20221213 23:41:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:41:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:28 @agent_ppo2.py:185][0m |           0.0093 |          59.2670 |           5.6815 |
[32m[20221213 23:41:28 @agent_ppo2.py:185][0m |          -0.0023 |          56.1163 |           5.6250 |
[32m[20221213 23:41:28 @agent_ppo2.py:185][0m |          -0.0048 |          55.1886 |           5.6051 |
[32m[20221213 23:41:28 @agent_ppo2.py:185][0m |          -0.0030 |          54.6336 |           5.6912 |
[32m[20221213 23:41:28 @agent_ppo2.py:185][0m |          -0.0088 |          53.9097 |           5.6816 |
[32m[20221213 23:41:28 @agent_ppo2.py:185][0m |           0.0013 |          59.4777 |           5.6730 |
[32m[20221213 23:41:29 @agent_ppo2.py:185][0m |          -0.0084 |          53.5284 |           5.7400 |
[32m[20221213 23:41:29 @agent_ppo2.py:185][0m |          -0.0115 |          53.3451 |           5.6998 |
[32m[20221213 23:41:29 @agent_ppo2.py:185][0m |          -0.0100 |          53.3383 |           5.7137 |
[32m[20221213 23:41:29 @agent_ppo2.py:185][0m |          -0.0109 |          52.8261 |           5.6724 |
[32m[20221213 23:41:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.01
[32m[20221213 23:41:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.54
[32m[20221213 23:41:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.07
[32m[20221213 23:41:29 @agent_ppo2.py:143][0m Total time:      28.95 min
[32m[20221213 23:41:29 @agent_ppo2.py:145][0m 2805760 total steps have happened
[32m[20221213 23:41:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3370 --------------------------#
[32m[20221213 23:41:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:41:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:29 @agent_ppo2.py:185][0m |           0.0002 |          63.1417 |           4.5809 |
[32m[20221213 23:41:29 @agent_ppo2.py:185][0m |          -0.0055 |          61.3758 |           4.5993 |
[32m[20221213 23:41:29 @agent_ppo2.py:185][0m |          -0.0064 |          60.8381 |           4.6151 |
[32m[20221213 23:41:30 @agent_ppo2.py:185][0m |          -0.0097 |          60.6403 |           4.5934 |
[32m[20221213 23:41:30 @agent_ppo2.py:185][0m |          -0.0074 |          60.6297 |           4.6434 |
[32m[20221213 23:41:30 @agent_ppo2.py:185][0m |          -0.0077 |          60.3002 |           4.5967 |
[32m[20221213 23:41:30 @agent_ppo2.py:185][0m |          -0.0093 |          60.2449 |           4.6186 |
[32m[20221213 23:41:30 @agent_ppo2.py:185][0m |          -0.0134 |          60.1559 |           4.6498 |
[32m[20221213 23:41:30 @agent_ppo2.py:185][0m |          -0.0090 |          59.9912 |           4.6459 |
[32m[20221213 23:41:30 @agent_ppo2.py:185][0m |          -0.0116 |          59.9141 |           4.6841 |
[32m[20221213 23:41:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:41:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.62
[32m[20221213 23:41:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.42
[32m[20221213 23:41:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.83
[32m[20221213 23:41:30 @agent_ppo2.py:143][0m Total time:      28.97 min
[32m[20221213 23:41:30 @agent_ppo2.py:145][0m 2807808 total steps have happened
[32m[20221213 23:41:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3371 --------------------------#
[32m[20221213 23:41:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0012 |          63.1154 |           5.0933 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0061 |          56.1976 |           5.1480 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0069 |          54.8497 |           5.2722 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0090 |          54.5294 |           5.1989 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0095 |          54.2748 |           5.2054 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0106 |          54.0698 |           5.1859 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0122 |          53.5255 |           5.1278 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0172 |          53.4570 |           5.1738 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0143 |          55.0814 |           5.1748 |
[32m[20221213 23:41:31 @agent_ppo2.py:185][0m |          -0.0189 |          53.3350 |           5.1195 |
[32m[20221213 23:41:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.76
[32m[20221213 23:41:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.64
[32m[20221213 23:41:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.83
[32m[20221213 23:41:31 @agent_ppo2.py:143][0m Total time:      28.99 min
[32m[20221213 23:41:31 @agent_ppo2.py:145][0m 2809856 total steps have happened
[32m[20221213 23:41:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3372 --------------------------#
[32m[20221213 23:41:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:41:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |           0.0113 |          61.7732 |           4.2549 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |          -0.0001 |          56.1115 |           4.1271 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |           0.0004 |          55.5130 |           4.0832 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |          -0.0053 |          54.9681 |           4.0274 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |          -0.0021 |          55.1374 |           4.0709 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |          -0.0038 |          54.5157 |           3.9581 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |          -0.0046 |          54.4429 |           3.9714 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |          -0.0067 |          54.2666 |           3.9526 |
[32m[20221213 23:41:32 @agent_ppo2.py:185][0m |          -0.0092 |          54.1198 |           3.8314 |
[32m[20221213 23:41:33 @agent_ppo2.py:185][0m |          -0.0078 |          54.2686 |           3.9940 |
[32m[20221213 23:41:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.87
[32m[20221213 23:41:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.33
[32m[20221213 23:41:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.00
[32m[20221213 23:41:33 @agent_ppo2.py:143][0m Total time:      29.01 min
[32m[20221213 23:41:33 @agent_ppo2.py:145][0m 2811904 total steps have happened
[32m[20221213 23:41:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3373 --------------------------#
[32m[20221213 23:41:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:33 @agent_ppo2.py:185][0m |           0.0022 |          68.3936 |           3.7694 |
[32m[20221213 23:41:33 @agent_ppo2.py:185][0m |          -0.0025 |          60.9853 |           3.8131 |
[32m[20221213 23:41:33 @agent_ppo2.py:185][0m |          -0.0109 |          58.0133 |           3.7781 |
[32m[20221213 23:41:33 @agent_ppo2.py:185][0m |          -0.0135 |          56.5989 |           3.8693 |
[32m[20221213 23:41:33 @agent_ppo2.py:185][0m |          -0.0102 |          55.2414 |           3.8092 |
[32m[20221213 23:41:33 @agent_ppo2.py:185][0m |          -0.0163 |          54.2507 |           3.7998 |
[32m[20221213 23:41:34 @agent_ppo2.py:185][0m |          -0.0165 |          53.7250 |           3.7841 |
[32m[20221213 23:41:34 @agent_ppo2.py:185][0m |          -0.0139 |          52.8507 |           3.8670 |
[32m[20221213 23:41:34 @agent_ppo2.py:185][0m |          -0.0163 |          52.4593 |           3.7941 |
[32m[20221213 23:41:34 @agent_ppo2.py:185][0m |          -0.0157 |          52.1715 |           3.7844 |
[32m[20221213 23:41:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:41:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.64
[32m[20221213 23:41:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.67
[32m[20221213 23:41:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.40
[32m[20221213 23:41:34 @agent_ppo2.py:143][0m Total time:      29.03 min
[32m[20221213 23:41:34 @agent_ppo2.py:145][0m 2813952 total steps have happened
[32m[20221213 23:41:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3374 --------------------------#
[32m[20221213 23:41:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:34 @agent_ppo2.py:185][0m |           0.0030 |          62.7264 |           3.8539 |
[32m[20221213 23:41:34 @agent_ppo2.py:185][0m |          -0.0011 |          60.3231 |           3.7837 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |          -0.0014 |          59.7207 |           3.7606 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |           0.0008 |          60.5063 |           3.7182 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |          -0.0057 |          59.1272 |           3.7035 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |          -0.0065 |          58.7740 |           3.7228 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |          -0.0044 |          58.9624 |           3.8006 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |           0.0072 |          66.8580 |           3.7748 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |          -0.0066 |          58.2821 |           3.8032 |
[32m[20221213 23:41:35 @agent_ppo2.py:185][0m |          -0.0079 |          58.1847 |           3.8012 |
[32m[20221213 23:41:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:41:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.31
[32m[20221213 23:41:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.23
[32m[20221213 23:41:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.48
[32m[20221213 23:41:35 @agent_ppo2.py:143][0m Total time:      29.05 min
[32m[20221213 23:41:35 @agent_ppo2.py:145][0m 2816000 total steps have happened
[32m[20221213 23:41:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3375 --------------------------#
[32m[20221213 23:41:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |           0.0030 |          68.3215 |           3.4270 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0034 |          66.5282 |           3.4725 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0017 |          65.3979 |           3.4151 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0047 |          64.6313 |           3.4312 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0083 |          64.3049 |           3.3793 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |           0.0021 |          66.5423 |           3.4096 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0079 |          63.7699 |           3.3665 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0067 |          63.5314 |           3.3711 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0062 |          63.6567 |           3.4406 |
[32m[20221213 23:41:36 @agent_ppo2.py:185][0m |          -0.0087 |          63.0246 |           3.4246 |
[32m[20221213 23:41:36 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:41:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.93
[32m[20221213 23:41:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.42
[32m[20221213 23:41:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.23
[32m[20221213 23:41:37 @agent_ppo2.py:143][0m Total time:      29.08 min
[32m[20221213 23:41:37 @agent_ppo2.py:145][0m 2818048 total steps have happened
[32m[20221213 23:41:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3376 --------------------------#
[32m[20221213 23:41:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:37 @agent_ppo2.py:185][0m |           0.0019 |          60.3363 |           3.5881 |
[32m[20221213 23:41:37 @agent_ppo2.py:185][0m |           0.0233 |          67.8298 |           3.6111 |
[32m[20221213 23:41:37 @agent_ppo2.py:185][0m |          -0.0022 |          54.2358 |           3.6214 |
[32m[20221213 23:41:37 @agent_ppo2.py:185][0m |          -0.0097 |          51.4617 |           3.6048 |
[32m[20221213 23:41:37 @agent_ppo2.py:185][0m |          -0.0095 |          50.5454 |           3.6262 |
[32m[20221213 23:41:37 @agent_ppo2.py:185][0m |          -0.0048 |          49.8540 |           3.6399 |
[32m[20221213 23:41:37 @agent_ppo2.py:185][0m |          -0.0116 |          49.1772 |           3.6863 |
[32m[20221213 23:41:38 @agent_ppo2.py:185][0m |          -0.0158 |          48.6937 |           3.7870 |
[32m[20221213 23:41:38 @agent_ppo2.py:185][0m |          -0.0112 |          48.0795 |           3.6517 |
[32m[20221213 23:41:38 @agent_ppo2.py:185][0m |          -0.0103 |          47.7281 |           3.7968 |
[32m[20221213 23:41:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.09
[32m[20221213 23:41:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.33
[32m[20221213 23:41:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.14
[32m[20221213 23:41:38 @agent_ppo2.py:143][0m Total time:      29.10 min
[32m[20221213 23:41:38 @agent_ppo2.py:145][0m 2820096 total steps have happened
[32m[20221213 23:41:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3377 --------------------------#
[32m[20221213 23:41:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:38 @agent_ppo2.py:185][0m |           0.0059 |          66.7502 |           3.8210 |
[32m[20221213 23:41:38 @agent_ppo2.py:185][0m |          -0.0094 |          63.4324 |           3.6712 |
[32m[20221213 23:41:38 @agent_ppo2.py:185][0m |          -0.0062 |          62.8325 |           3.7153 |
[32m[20221213 23:41:38 @agent_ppo2.py:185][0m |          -0.0103 |          62.2110 |           3.7334 |
[32m[20221213 23:41:39 @agent_ppo2.py:185][0m |          -0.0091 |          61.8581 |           3.7825 |
[32m[20221213 23:41:39 @agent_ppo2.py:185][0m |          -0.0119 |          61.7287 |           3.8427 |
[32m[20221213 23:41:39 @agent_ppo2.py:185][0m |          -0.0050 |          63.4686 |           3.8567 |
[32m[20221213 23:41:39 @agent_ppo2.py:185][0m |          -0.0127 |          61.0985 |           3.9967 |
[32m[20221213 23:41:39 @agent_ppo2.py:185][0m |           0.0238 |          78.2985 |           3.9656 |
[32m[20221213 23:41:39 @agent_ppo2.py:185][0m |          -0.0019 |          68.4305 |           4.0251 |
[32m[20221213 23:41:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:41:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.91
[32m[20221213 23:41:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.83
[32m[20221213 23:41:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.83
[32m[20221213 23:41:39 @agent_ppo2.py:143][0m Total time:      29.12 min
[32m[20221213 23:41:39 @agent_ppo2.py:145][0m 2822144 total steps have happened
[32m[20221213 23:41:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3378 --------------------------#
[32m[20221213 23:41:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:39 @agent_ppo2.py:185][0m |           0.0016 |          61.6678 |           3.9316 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0041 |          56.9142 |           3.9429 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0028 |          55.1609 |           3.7972 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0081 |          54.1445 |           3.7656 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0149 |          53.4534 |           3.7585 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0148 |          52.7560 |           3.7809 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0138 |          52.2584 |           3.6482 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0128 |          52.0126 |           3.6082 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0168 |          51.7086 |           3.7341 |
[32m[20221213 23:41:40 @agent_ppo2.py:185][0m |          -0.0150 |          51.3518 |           3.6374 |
[32m[20221213 23:41:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.89
[32m[20221213 23:41:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.24
[32m[20221213 23:41:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.64
[32m[20221213 23:41:40 @agent_ppo2.py:143][0m Total time:      29.14 min
[32m[20221213 23:41:40 @agent_ppo2.py:145][0m 2824192 total steps have happened
[32m[20221213 23:41:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3379 --------------------------#
[32m[20221213 23:41:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0027 |          52.4325 |           4.2449 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0046 |          48.9507 |           4.2963 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0080 |          47.4367 |           4.3056 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0039 |          47.5141 |           4.3380 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0071 |          45.8891 |           4.2871 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0130 |          45.3242 |           4.2122 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0114 |          45.3356 |           4.2521 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0131 |          44.6363 |           4.2179 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0012 |          53.4738 |           4.1780 |
[32m[20221213 23:41:41 @agent_ppo2.py:185][0m |          -0.0177 |          44.0643 |           4.2647 |
[32m[20221213 23:41:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.13
[32m[20221213 23:41:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.29
[32m[20221213 23:41:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.86
[32m[20221213 23:41:42 @agent_ppo2.py:143][0m Total time:      29.16 min
[32m[20221213 23:41:42 @agent_ppo2.py:145][0m 2826240 total steps have happened
[32m[20221213 23:41:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3380 --------------------------#
[32m[20221213 23:41:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:41:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:42 @agent_ppo2.py:185][0m |           0.0078 |          62.4930 |           3.4445 |
[32m[20221213 23:41:42 @agent_ppo2.py:185][0m |          -0.0043 |          60.0506 |           3.2161 |
[32m[20221213 23:41:42 @agent_ppo2.py:185][0m |          -0.0037 |          59.6736 |           3.2957 |
[32m[20221213 23:41:42 @agent_ppo2.py:185][0m |          -0.0053 |          59.1856 |           3.2403 |
[32m[20221213 23:41:42 @agent_ppo2.py:185][0m |          -0.0064 |          59.1326 |           3.2403 |
[32m[20221213 23:41:42 @agent_ppo2.py:185][0m |          -0.0091 |          58.8052 |           3.2852 |
[32m[20221213 23:41:42 @agent_ppo2.py:185][0m |           0.0078 |          64.4290 |           3.2384 |
[32m[20221213 23:41:43 @agent_ppo2.py:185][0m |           0.0105 |          64.2129 |           3.2520 |
[32m[20221213 23:41:43 @agent_ppo2.py:185][0m |          -0.0091 |          58.4675 |           3.3945 |
[32m[20221213 23:41:43 @agent_ppo2.py:185][0m |          -0.0002 |          59.4208 |           3.2216 |
[32m[20221213 23:41:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:41:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.70
[32m[20221213 23:41:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.51
[32m[20221213 23:41:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.07
[32m[20221213 23:41:43 @agent_ppo2.py:143][0m Total time:      29.18 min
[32m[20221213 23:41:43 @agent_ppo2.py:145][0m 2828288 total steps have happened
[32m[20221213 23:41:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3381 --------------------------#
[32m[20221213 23:41:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:43 @agent_ppo2.py:185][0m |           0.0036 |          64.2049 |           3.3161 |
[32m[20221213 23:41:43 @agent_ppo2.py:185][0m |          -0.0076 |          58.7427 |           3.2862 |
[32m[20221213 23:41:43 @agent_ppo2.py:185][0m |          -0.0131 |          57.0217 |           3.2403 |
[32m[20221213 23:41:43 @agent_ppo2.py:185][0m |          -0.0048 |          60.2550 |           3.3465 |
[32m[20221213 23:41:44 @agent_ppo2.py:185][0m |          -0.0145 |          55.4808 |           3.3734 |
[32m[20221213 23:41:44 @agent_ppo2.py:185][0m |          -0.0141 |          54.5009 |           3.3824 |
[32m[20221213 23:41:44 @agent_ppo2.py:185][0m |          -0.0121 |          54.0522 |           3.3526 |
[32m[20221213 23:41:44 @agent_ppo2.py:185][0m |          -0.0170 |          53.4583 |           3.3699 |
[32m[20221213 23:41:44 @agent_ppo2.py:185][0m |          -0.0191 |          52.8799 |           3.4357 |
[32m[20221213 23:41:44 @agent_ppo2.py:185][0m |          -0.0173 |          52.5315 |           3.3886 |
[32m[20221213 23:41:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.16
[32m[20221213 23:41:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.06
[32m[20221213 23:41:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.83
[32m[20221213 23:41:44 @agent_ppo2.py:143][0m Total time:      29.20 min
[32m[20221213 23:41:44 @agent_ppo2.py:145][0m 2830336 total steps have happened
[32m[20221213 23:41:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3382 --------------------------#
[32m[20221213 23:41:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:41:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:44 @agent_ppo2.py:185][0m |          -0.0003 |          62.6800 |           3.7840 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0011 |          60.6446 |           3.8994 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |           0.0044 |          62.2504 |           3.8988 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0025 |          59.6903 |           3.8747 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0095 |          59.4769 |           3.8165 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0063 |          58.8840 |           3.9559 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0012 |          60.1157 |           3.9637 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0044 |          58.3521 |           3.9803 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0116 |          58.0660 |           3.9804 |
[32m[20221213 23:41:45 @agent_ppo2.py:185][0m |          -0.0105 |          57.9294 |           3.9859 |
[32m[20221213 23:41:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:41:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.15
[32m[20221213 23:41:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.00
[32m[20221213 23:41:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 174.63
[32m[20221213 23:41:45 @agent_ppo2.py:143][0m Total time:      29.22 min
[32m[20221213 23:41:45 @agent_ppo2.py:145][0m 2832384 total steps have happened
[32m[20221213 23:41:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3383 --------------------------#
[32m[20221213 23:41:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:41:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |           0.0049 |          69.4517 |           2.8817 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0033 |          67.4397 |           2.9669 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0064 |          66.9882 |           2.9342 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0042 |          66.6854 |           2.9995 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0074 |          66.3159 |           2.9947 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0050 |          65.8190 |           3.1279 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0110 |          65.5504 |           3.1778 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0016 |          67.8795 |           3.1807 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0081 |          65.3147 |           3.1801 |
[32m[20221213 23:41:46 @agent_ppo2.py:185][0m |          -0.0081 |          64.6707 |           3.1793 |
[32m[20221213 23:41:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.14
[32m[20221213 23:41:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.44
[32m[20221213 23:41:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.62
[32m[20221213 23:41:47 @agent_ppo2.py:143][0m Total time:      29.24 min
[32m[20221213 23:41:47 @agent_ppo2.py:145][0m 2834432 total steps have happened
[32m[20221213 23:41:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3384 --------------------------#
[32m[20221213 23:41:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:47 @agent_ppo2.py:185][0m |          -0.0009 |          79.9699 |           4.0016 |
[32m[20221213 23:41:47 @agent_ppo2.py:185][0m |          -0.0021 |          75.1680 |           4.0047 |
[32m[20221213 23:41:47 @agent_ppo2.py:185][0m |          -0.0059 |          73.3913 |           3.9849 |
[32m[20221213 23:41:47 @agent_ppo2.py:185][0m |          -0.0077 |          72.1696 |           3.9617 |
[32m[20221213 23:41:47 @agent_ppo2.py:185][0m |          -0.0047 |          71.9828 |           3.9472 |
[32m[20221213 23:41:47 @agent_ppo2.py:185][0m |          -0.0074 |          70.9865 |           4.0833 |
[32m[20221213 23:41:47 @agent_ppo2.py:185][0m |          -0.0099 |          70.8218 |           3.9715 |
[32m[20221213 23:41:48 @agent_ppo2.py:185][0m |          -0.0102 |          70.2607 |           3.9263 |
[32m[20221213 23:41:48 @agent_ppo2.py:185][0m |          -0.0005 |          72.2176 |           3.9394 |
[32m[20221213 23:41:48 @agent_ppo2.py:185][0m |          -0.0027 |          72.0826 |           4.0103 |
[32m[20221213 23:41:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.81
[32m[20221213 23:41:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.53
[32m[20221213 23:41:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.91
[32m[20221213 23:41:48 @agent_ppo2.py:143][0m Total time:      29.26 min
[32m[20221213 23:41:48 @agent_ppo2.py:145][0m 2836480 total steps have happened
[32m[20221213 23:41:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3385 --------------------------#
[32m[20221213 23:41:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:48 @agent_ppo2.py:185][0m |           0.0111 |          71.5784 |           4.3717 |
[32m[20221213 23:41:48 @agent_ppo2.py:185][0m |          -0.0057 |          65.2669 |           4.3635 |
[32m[20221213 23:41:48 @agent_ppo2.py:185][0m |          -0.0087 |          64.2728 |           4.4267 |
[32m[20221213 23:41:48 @agent_ppo2.py:185][0m |          -0.0134 |          63.4971 |           4.4057 |
[32m[20221213 23:41:49 @agent_ppo2.py:185][0m |          -0.0110 |          63.0214 |           4.4020 |
[32m[20221213 23:41:49 @agent_ppo2.py:185][0m |          -0.0156 |          62.8964 |           4.3426 |
[32m[20221213 23:41:49 @agent_ppo2.py:185][0m |          -0.0176 |          62.4058 |           4.4080 |
[32m[20221213 23:41:49 @agent_ppo2.py:185][0m |          -0.0080 |          65.8347 |           4.4230 |
[32m[20221213 23:41:49 @agent_ppo2.py:185][0m |          -0.0230 |          62.0409 |           4.4142 |
[32m[20221213 23:41:49 @agent_ppo2.py:185][0m |          -0.0187 |          61.6769 |           4.4005 |
[32m[20221213 23:41:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.61
[32m[20221213 23:41:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.09
[32m[20221213 23:41:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.08
[32m[20221213 23:41:49 @agent_ppo2.py:143][0m Total time:      29.29 min
[32m[20221213 23:41:49 @agent_ppo2.py:145][0m 2838528 total steps have happened
[32m[20221213 23:41:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3386 --------------------------#
[32m[20221213 23:41:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:49 @agent_ppo2.py:185][0m |           0.0061 |          62.5057 |           4.3233 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0030 |          60.5761 |           4.2291 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0040 |          60.0294 |           4.2147 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0064 |          59.7919 |           4.2475 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0043 |          59.4789 |           4.2524 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0066 |          59.3252 |           4.2205 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0095 |          59.1998 |           4.3155 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0065 |          59.1534 |           4.2354 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0015 |          59.8294 |           4.2489 |
[32m[20221213 23:41:50 @agent_ppo2.py:185][0m |          -0.0081 |          58.7918 |           4.3236 |
[32m[20221213 23:41:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.14
[32m[20221213 23:41:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.74
[32m[20221213 23:41:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.80
[32m[20221213 23:41:50 @agent_ppo2.py:143][0m Total time:      29.31 min
[32m[20221213 23:41:50 @agent_ppo2.py:145][0m 2840576 total steps have happened
[32m[20221213 23:41:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3387 --------------------------#
[32m[20221213 23:41:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0001 |          63.2135 |           3.3841 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |           0.0035 |          58.4482 |           3.4088 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0074 |          56.9061 |           3.4693 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0070 |          56.2797 |           3.4483 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0097 |          55.5381 |           3.3675 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0066 |          55.2146 |           3.4637 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0119 |          54.9620 |           3.4001 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0073 |          56.7146 |           3.4316 |
[32m[20221213 23:41:51 @agent_ppo2.py:185][0m |          -0.0129 |          54.0742 |           3.3847 |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0049 |          54.8294 |           3.4688 |
[32m[20221213 23:41:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.52
[32m[20221213 23:41:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.64
[32m[20221213 23:41:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.12
[32m[20221213 23:41:52 @agent_ppo2.py:143][0m Total time:      29.33 min
[32m[20221213 23:41:52 @agent_ppo2.py:145][0m 2842624 total steps have happened
[32m[20221213 23:41:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3388 --------------------------#
[32m[20221213 23:41:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0001 |          64.6205 |           4.2318 |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0050 |          61.7143 |           4.2618 |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0075 |          60.6075 |           4.2828 |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0070 |          59.9020 |           4.3555 |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0095 |          59.3923 |           4.3634 |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0073 |          59.0016 |           4.3941 |
[32m[20221213 23:41:52 @agent_ppo2.py:185][0m |          -0.0157 |          58.7406 |           4.4261 |
[32m[20221213 23:41:53 @agent_ppo2.py:185][0m |          -0.0070 |          58.2580 |           4.4347 |
[32m[20221213 23:41:53 @agent_ppo2.py:185][0m |          -0.0123 |          58.1513 |           4.4673 |
[32m[20221213 23:41:53 @agent_ppo2.py:185][0m |           0.0014 |          64.2928 |           4.4038 |
[32m[20221213 23:41:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:41:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.72
[32m[20221213 23:41:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.20
[32m[20221213 23:41:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.30
[32m[20221213 23:41:53 @agent_ppo2.py:143][0m Total time:      29.35 min
[32m[20221213 23:41:53 @agent_ppo2.py:145][0m 2844672 total steps have happened
[32m[20221213 23:41:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3389 --------------------------#
[32m[20221213 23:41:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:53 @agent_ppo2.py:185][0m |           0.0041 |          68.9997 |           4.6644 |
[32m[20221213 23:41:53 @agent_ppo2.py:185][0m |          -0.0059 |          67.6547 |           4.7250 |
[32m[20221213 23:41:53 @agent_ppo2.py:185][0m |           0.0032 |          71.1790 |           4.7390 |
[32m[20221213 23:41:53 @agent_ppo2.py:185][0m |          -0.0066 |          67.0604 |           4.7239 |
[32m[20221213 23:41:54 @agent_ppo2.py:185][0m |          -0.0061 |          66.3802 |           4.8394 |
[32m[20221213 23:41:54 @agent_ppo2.py:185][0m |          -0.0060 |          66.2866 |           4.8390 |
[32m[20221213 23:41:54 @agent_ppo2.py:185][0m |          -0.0111 |          66.1784 |           4.8396 |
[32m[20221213 23:41:54 @agent_ppo2.py:185][0m |           0.0019 |          72.4550 |           4.8800 |
[32m[20221213 23:41:54 @agent_ppo2.py:185][0m |          -0.0112 |          66.0589 |           4.8967 |
[32m[20221213 23:41:54 @agent_ppo2.py:185][0m |           0.0063 |          75.4042 |           5.0312 |
[32m[20221213 23:41:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.67
[32m[20221213 23:41:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.98
[32m[20221213 23:41:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.20
[32m[20221213 23:41:54 @agent_ppo2.py:143][0m Total time:      29.37 min
[32m[20221213 23:41:54 @agent_ppo2.py:145][0m 2846720 total steps have happened
[32m[20221213 23:41:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3390 --------------------------#
[32m[20221213 23:41:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:41:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:54 @agent_ppo2.py:185][0m |           0.0037 |          69.6485 |           5.1398 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0033 |          65.0822 |           5.0404 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0007 |          64.1276 |           5.1605 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0092 |          61.4105 |           5.1827 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0131 |          60.6660 |           5.2198 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0110 |          60.2960 |           5.1767 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0157 |          60.0580 |           5.1273 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0142 |          59.3891 |           5.2104 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0116 |          59.0092 |           5.2432 |
[32m[20221213 23:41:55 @agent_ppo2.py:185][0m |          -0.0164 |          58.8961 |           5.2427 |
[32m[20221213 23:41:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.99
[32m[20221213 23:41:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.42
[32m[20221213 23:41:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.00
[32m[20221213 23:41:55 @agent_ppo2.py:143][0m Total time:      29.39 min
[32m[20221213 23:41:55 @agent_ppo2.py:145][0m 2848768 total steps have happened
[32m[20221213 23:41:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3391 --------------------------#
[32m[20221213 23:41:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |           0.0068 |          62.6945 |           4.2994 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0061 |          59.9513 |           4.3630 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0061 |          59.2564 |           4.4701 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0082 |          58.8495 |           4.3959 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0064 |          59.0760 |           4.4236 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0118 |          58.4240 |           4.4229 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0102 |          58.3194 |           4.4735 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0003 |          62.8252 |           4.5042 |
[32m[20221213 23:41:56 @agent_ppo2.py:185][0m |          -0.0064 |          59.8268 |           4.5818 |
[32m[20221213 23:41:57 @agent_ppo2.py:185][0m |          -0.0137 |          58.1185 |           4.6201 |
[32m[20221213 23:41:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:41:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.75
[32m[20221213 23:41:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.79
[32m[20221213 23:41:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.84
[32m[20221213 23:41:57 @agent_ppo2.py:143][0m Total time:      29.41 min
[32m[20221213 23:41:57 @agent_ppo2.py:145][0m 2850816 total steps have happened
[32m[20221213 23:41:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3392 --------------------------#
[32m[20221213 23:41:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:57 @agent_ppo2.py:185][0m |          -0.0011 |          62.1356 |           5.8771 |
[32m[20221213 23:41:57 @agent_ppo2.py:185][0m |          -0.0077 |          58.8520 |           5.9217 |
[32m[20221213 23:41:57 @agent_ppo2.py:185][0m |          -0.0076 |          57.5629 |           5.9593 |
[32m[20221213 23:41:57 @agent_ppo2.py:185][0m |          -0.0124 |          56.6095 |           5.8971 |
[32m[20221213 23:41:57 @agent_ppo2.py:185][0m |          -0.0098 |          55.7374 |           5.8982 |
[32m[20221213 23:41:57 @agent_ppo2.py:185][0m |          -0.0112 |          55.1148 |           5.9148 |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |          -0.0145 |          54.8758 |           5.9080 |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |          -0.0185 |          54.3616 |           5.9160 |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |          -0.0159 |          53.9800 |           5.9422 |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |          -0.0149 |          53.6809 |           5.9393 |
[32m[20221213 23:41:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:41:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.15
[32m[20221213 23:41:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.47
[32m[20221213 23:41:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.64
[32m[20221213 23:41:58 @agent_ppo2.py:143][0m Total time:      29.43 min
[32m[20221213 23:41:58 @agent_ppo2.py:145][0m 2852864 total steps have happened
[32m[20221213 23:41:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3393 --------------------------#
[32m[20221213 23:41:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |           0.0030 |          68.1027 |           5.7449 |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |          -0.0066 |          61.6679 |           5.6919 |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |          -0.0086 |          60.0795 |           5.7048 |
[32m[20221213 23:41:58 @agent_ppo2.py:185][0m |          -0.0115 |          58.2351 |           5.6356 |
[32m[20221213 23:41:59 @agent_ppo2.py:185][0m |          -0.0101 |          57.7532 |           5.6449 |
[32m[20221213 23:41:59 @agent_ppo2.py:185][0m |          -0.0150 |          56.7146 |           5.6840 |
[32m[20221213 23:41:59 @agent_ppo2.py:185][0m |          -0.0133 |          56.2461 |           5.7123 |
[32m[20221213 23:41:59 @agent_ppo2.py:185][0m |          -0.0196 |          55.7335 |           5.6566 |
[32m[20221213 23:41:59 @agent_ppo2.py:185][0m |          -0.0175 |          55.4883 |           5.6757 |
[32m[20221213 23:41:59 @agent_ppo2.py:185][0m |          -0.0094 |          56.2952 |           5.7535 |
[32m[20221213 23:41:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:41:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.19
[32m[20221213 23:41:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.18
[32m[20221213 23:41:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.67
[32m[20221213 23:41:59 @agent_ppo2.py:143][0m Total time:      29.45 min
[32m[20221213 23:41:59 @agent_ppo2.py:145][0m 2854912 total steps have happened
[32m[20221213 23:41:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3394 --------------------------#
[32m[20221213 23:41:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:41:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0018 |          49.8477 |           6.1597 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0056 |          42.4450 |           6.0833 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0070 |          40.3650 |           6.0515 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0069 |          41.2256 |           6.1325 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0127 |          38.9194 |           6.0691 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0099 |          38.1183 |           6.1371 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0114 |          37.6290 |           6.0944 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0153 |          37.1717 |           6.0708 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0118 |          37.0690 |           6.0361 |
[32m[20221213 23:42:00 @agent_ppo2.py:185][0m |          -0.0177 |          36.7225 |           6.0338 |
[32m[20221213 23:42:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:42:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.51
[32m[20221213 23:42:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.52
[32m[20221213 23:42:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.29
[32m[20221213 23:42:00 @agent_ppo2.py:143][0m Total time:      29.47 min
[32m[20221213 23:42:00 @agent_ppo2.py:145][0m 2856960 total steps have happened
[32m[20221213 23:42:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3395 --------------------------#
[32m[20221213 23:42:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:42:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |           0.0006 |          68.8961 |           5.4221 |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |          -0.0041 |          66.7298 |           5.4200 |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |          -0.0055 |          66.8251 |           5.4539 |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |          -0.0118 |          65.2277 |           5.5242 |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |          -0.0129 |          64.8845 |           5.5048 |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |          -0.0063 |          64.7637 |           5.5224 |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |          -0.0125 |          64.3155 |           5.4803 |
[32m[20221213 23:42:01 @agent_ppo2.py:185][0m |          -0.0133 |          64.1235 |           5.4873 |
[32m[20221213 23:42:02 @agent_ppo2.py:185][0m |          -0.0035 |          71.6723 |           5.5472 |
[32m[20221213 23:42:02 @agent_ppo2.py:185][0m |          -0.0083 |          65.8977 |           5.4805 |
[32m[20221213 23:42:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:42:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.10
[32m[20221213 23:42:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.84
[32m[20221213 23:42:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.38
[32m[20221213 23:42:02 @agent_ppo2.py:143][0m Total time:      29.50 min
[32m[20221213 23:42:02 @agent_ppo2.py:145][0m 2859008 total steps have happened
[32m[20221213 23:42:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3396 --------------------------#
[32m[20221213 23:42:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:02 @agent_ppo2.py:185][0m |           0.0022 |          53.0955 |           5.2385 |
[32m[20221213 23:42:02 @agent_ppo2.py:185][0m |          -0.0056 |          45.4150 |           5.2103 |
[32m[20221213 23:42:02 @agent_ppo2.py:185][0m |          -0.0053 |          43.3208 |           5.3036 |
[32m[20221213 23:42:02 @agent_ppo2.py:185][0m |          -0.0116 |          41.6826 |           5.2128 |
[32m[20221213 23:42:03 @agent_ppo2.py:185][0m |          -0.0066 |          40.1388 |           5.3250 |
[32m[20221213 23:42:03 @agent_ppo2.py:185][0m |          -0.0109 |          39.5870 |           5.3367 |
[32m[20221213 23:42:03 @agent_ppo2.py:185][0m |          -0.0079 |          39.0522 |           5.3099 |
[32m[20221213 23:42:03 @agent_ppo2.py:185][0m |          -0.0116 |          38.8507 |           5.3230 |
[32m[20221213 23:42:03 @agent_ppo2.py:185][0m |          -0.0157 |          38.1649 |           5.3154 |
[32m[20221213 23:42:03 @agent_ppo2.py:185][0m |          -0.0112 |          37.8469 |           5.2932 |
[32m[20221213 23:42:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:42:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.01
[32m[20221213 23:42:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.75
[32m[20221213 23:42:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.08
[32m[20221213 23:42:03 @agent_ppo2.py:143][0m Total time:      29.52 min
[32m[20221213 23:42:03 @agent_ppo2.py:145][0m 2861056 total steps have happened
[32m[20221213 23:42:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3397 --------------------------#
[32m[20221213 23:42:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:03 @agent_ppo2.py:185][0m |           0.0029 |          68.2462 |           5.2643 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0065 |          63.0556 |           5.3999 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0136 |          61.6431 |           5.5143 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0150 |          60.4125 |           5.4911 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0065 |          65.1239 |           5.5573 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |           0.0001 |          60.8227 |           5.6206 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0086 |          58.5020 |           5.6742 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0142 |          57.9110 |           5.7442 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0149 |          57.9562 |           5.8145 |
[32m[20221213 23:42:04 @agent_ppo2.py:185][0m |          -0.0221 |          57.8265 |           5.8135 |
[32m[20221213 23:42:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:42:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.64
[32m[20221213 23:42:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.28
[32m[20221213 23:42:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.88
[32m[20221213 23:42:04 @agent_ppo2.py:143][0m Total time:      29.54 min
[32m[20221213 23:42:04 @agent_ppo2.py:145][0m 2863104 total steps have happened
[32m[20221213 23:42:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3398 --------------------------#
[32m[20221213 23:42:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |           0.0016 |          75.5953 |           5.2406 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0023 |          71.4878 |           5.2214 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0029 |          72.2667 |           5.2655 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0101 |          68.6178 |           5.2462 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0088 |          67.6138 |           5.3177 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0073 |          68.0660 |           5.2961 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0133 |          66.3432 |           5.2758 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0118 |          65.9778 |           5.3800 |
[32m[20221213 23:42:05 @agent_ppo2.py:185][0m |          -0.0146 |          65.6532 |           5.4020 |
[32m[20221213 23:42:06 @agent_ppo2.py:185][0m |          -0.0122 |          65.3940 |           5.3691 |
[32m[20221213 23:42:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:42:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.02
[32m[20221213 23:42:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.96
[32m[20221213 23:42:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.79
[32m[20221213 23:42:06 @agent_ppo2.py:143][0m Total time:      29.56 min
[32m[20221213 23:42:06 @agent_ppo2.py:145][0m 2865152 total steps have happened
[32m[20221213 23:42:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3399 --------------------------#
[32m[20221213 23:42:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:06 @agent_ppo2.py:185][0m |           0.0024 |          66.7389 |           6.0137 |
[32m[20221213 23:42:06 @agent_ppo2.py:185][0m |          -0.0032 |          61.4554 |           5.9084 |
[32m[20221213 23:42:06 @agent_ppo2.py:185][0m |          -0.0032 |          59.7674 |           5.9627 |
[32m[20221213 23:42:06 @agent_ppo2.py:185][0m |          -0.0036 |          58.7462 |           5.8594 |
[32m[20221213 23:42:06 @agent_ppo2.py:185][0m |          -0.0018 |          57.9956 |           5.9050 |
[32m[20221213 23:42:06 @agent_ppo2.py:185][0m |          -0.0071 |          57.4697 |           5.8858 |
[32m[20221213 23:42:07 @agent_ppo2.py:185][0m |          -0.0088 |          56.9697 |           5.8527 |
[32m[20221213 23:42:07 @agent_ppo2.py:185][0m |          -0.0146 |          56.6105 |           5.8758 |
[32m[20221213 23:42:07 @agent_ppo2.py:185][0m |          -0.0114 |          56.3301 |           5.8668 |
[32m[20221213 23:42:07 @agent_ppo2.py:185][0m |          -0.0110 |          56.2601 |           5.8418 |
[32m[20221213 23:42:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.33
[32m[20221213 23:42:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.63
[32m[20221213 23:42:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.10
[32m[20221213 23:42:07 @agent_ppo2.py:143][0m Total time:      29.58 min
[32m[20221213 23:42:07 @agent_ppo2.py:145][0m 2867200 total steps have happened
[32m[20221213 23:42:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3400 --------------------------#
[32m[20221213 23:42:07 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:42:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:07 @agent_ppo2.py:185][0m |           0.0018 |          64.9172 |           5.9155 |
[32m[20221213 23:42:07 @agent_ppo2.py:185][0m |          -0.0051 |          59.5597 |           5.9825 |
[32m[20221213 23:42:07 @agent_ppo2.py:185][0m |          -0.0052 |          57.6475 |           5.9534 |
[32m[20221213 23:42:08 @agent_ppo2.py:185][0m |          -0.0030 |          56.8539 |           5.9658 |
[32m[20221213 23:42:08 @agent_ppo2.py:185][0m |          -0.0083 |          56.1702 |           5.9405 |
[32m[20221213 23:42:08 @agent_ppo2.py:185][0m |          -0.0098 |          55.2197 |           5.9609 |
[32m[20221213 23:42:08 @agent_ppo2.py:185][0m |           0.0013 |          59.9921 |           5.9238 |
[32m[20221213 23:42:08 @agent_ppo2.py:185][0m |           0.0004 |          58.0494 |           5.9609 |
[32m[20221213 23:42:08 @agent_ppo2.py:185][0m |          -0.0111 |          54.2546 |           5.8792 |
[32m[20221213 23:42:08 @agent_ppo2.py:185][0m |          -0.0149 |          53.9006 |           5.9225 |
[32m[20221213 23:42:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.05
[32m[20221213 23:42:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.56
[32m[20221213 23:42:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.85
[32m[20221213 23:42:08 @agent_ppo2.py:143][0m Total time:      29.60 min
[32m[20221213 23:42:08 @agent_ppo2.py:145][0m 2869248 total steps have happened
[32m[20221213 23:42:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3401 --------------------------#
[32m[20221213 23:42:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |           0.0048 |          68.4297 |           5.3469 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0054 |          62.6808 |           5.3771 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0112 |          60.4927 |           5.3825 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0143 |          59.4526 |           5.3442 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0122 |          58.8954 |           5.3707 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0121 |          58.4168 |           5.3143 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0021 |          64.0008 |           5.3414 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0115 |          57.5967 |           5.3300 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0152 |          56.9024 |           5.3364 |
[32m[20221213 23:42:09 @agent_ppo2.py:185][0m |          -0.0164 |          56.4128 |           5.3015 |
[32m[20221213 23:42:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:42:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.78
[32m[20221213 23:42:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.16
[32m[20221213 23:42:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.79
[32m[20221213 23:42:09 @agent_ppo2.py:143][0m Total time:      29.62 min
[32m[20221213 23:42:09 @agent_ppo2.py:145][0m 2871296 total steps have happened
[32m[20221213 23:42:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3402 --------------------------#
[32m[20221213 23:42:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |           0.0013 |          64.7830 |           5.9710 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0010 |          62.7063 |           5.9961 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0055 |          61.7473 |           5.8660 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0041 |          61.5629 |           5.8932 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0032 |          63.3518 |           5.9476 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0056 |          61.0835 |           5.9782 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0088 |          60.5657 |           6.0133 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0100 |          60.4800 |           6.0854 |
[32m[20221213 23:42:10 @agent_ppo2.py:185][0m |          -0.0123 |          60.1811 |           6.0339 |
[32m[20221213 23:42:11 @agent_ppo2.py:185][0m |          -0.0024 |          62.1659 |           6.0475 |
[32m[20221213 23:42:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:42:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.78
[32m[20221213 23:42:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.09
[32m[20221213 23:42:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.37
[32m[20221213 23:42:11 @agent_ppo2.py:143][0m Total time:      29.65 min
[32m[20221213 23:42:11 @agent_ppo2.py:145][0m 2873344 total steps have happened
[32m[20221213 23:42:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3403 --------------------------#
[32m[20221213 23:42:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:11 @agent_ppo2.py:185][0m |          -0.0049 |          72.4841 |           6.6107 |
[32m[20221213 23:42:11 @agent_ppo2.py:185][0m |          -0.0087 |          68.0583 |           6.4997 |
[32m[20221213 23:42:11 @agent_ppo2.py:185][0m |          -0.0098 |          67.1354 |           6.5767 |
[32m[20221213 23:42:11 @agent_ppo2.py:185][0m |          -0.0129 |          66.1485 |           6.5205 |
[32m[20221213 23:42:11 @agent_ppo2.py:185][0m |          -0.0140 |          65.6195 |           6.5095 |
[32m[20221213 23:42:11 @agent_ppo2.py:185][0m |          -0.0108 |          65.4964 |           6.5108 |
[32m[20221213 23:42:12 @agent_ppo2.py:185][0m |          -0.0062 |          70.8844 |           6.4741 |
[32m[20221213 23:42:12 @agent_ppo2.py:185][0m |          -0.0170 |          64.1718 |           6.4940 |
[32m[20221213 23:42:12 @agent_ppo2.py:185][0m |          -0.0155 |          63.9341 |           6.5073 |
[32m[20221213 23:42:12 @agent_ppo2.py:185][0m |          -0.0174 |          63.6726 |           6.4846 |
[32m[20221213 23:42:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.06
[32m[20221213 23:42:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.19
[32m[20221213 23:42:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.12
[32m[20221213 23:42:12 @agent_ppo2.py:143][0m Total time:      29.67 min
[32m[20221213 23:42:12 @agent_ppo2.py:145][0m 2875392 total steps have happened
[32m[20221213 23:42:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3404 --------------------------#
[32m[20221213 23:42:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:12 @agent_ppo2.py:185][0m |           0.0025 |          63.5829 |           5.5707 |
[32m[20221213 23:42:12 @agent_ppo2.py:185][0m |          -0.0084 |          60.1622 |           5.5250 |
[32m[20221213 23:42:12 @agent_ppo2.py:185][0m |           0.0012 |          62.6905 |           5.5280 |
[32m[20221213 23:42:13 @agent_ppo2.py:185][0m |          -0.0104 |          57.8670 |           5.5812 |
[32m[20221213 23:42:13 @agent_ppo2.py:185][0m |          -0.0144 |          57.0307 |           5.5657 |
[32m[20221213 23:42:13 @agent_ppo2.py:185][0m |          -0.0131 |          56.5361 |           5.5562 |
[32m[20221213 23:42:13 @agent_ppo2.py:185][0m |          -0.0127 |          55.8124 |           5.5624 |
[32m[20221213 23:42:13 @agent_ppo2.py:185][0m |          -0.0131 |          55.3294 |           5.6194 |
[32m[20221213 23:42:13 @agent_ppo2.py:185][0m |          -0.0156 |          54.8994 |           5.5846 |
[32m[20221213 23:42:13 @agent_ppo2.py:185][0m |          -0.0170 |          54.9219 |           5.6462 |
[32m[20221213 23:42:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.43
[32m[20221213 23:42:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.49
[32m[20221213 23:42:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.39
[32m[20221213 23:42:13 @agent_ppo2.py:143][0m Total time:      29.69 min
[32m[20221213 23:42:13 @agent_ppo2.py:145][0m 2877440 total steps have happened
[32m[20221213 23:42:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3405 --------------------------#
[32m[20221213 23:42:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0014 |          78.3650 |           6.3455 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0037 |          75.1100 |           6.2727 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0080 |          74.2244 |           6.3597 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |           0.0020 |          76.6236 |           6.3595 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0066 |          73.3074 |           6.3224 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |           0.0061 |          77.9945 |           6.3251 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0090 |          72.5945 |           6.3697 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0080 |          72.2863 |           6.3410 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0126 |          72.0025 |           6.2840 |
[32m[20221213 23:42:14 @agent_ppo2.py:185][0m |          -0.0100 |          71.8389 |           6.3113 |
[32m[20221213 23:42:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:42:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.32
[32m[20221213 23:42:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.15
[32m[20221213 23:42:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.28
[32m[20221213 23:42:15 @agent_ppo2.py:143][0m Total time:      29.71 min
[32m[20221213 23:42:15 @agent_ppo2.py:145][0m 2879488 total steps have happened
[32m[20221213 23:42:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3406 --------------------------#
[32m[20221213 23:42:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:42:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0001 |          60.2876 |           5.3278 |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0051 |          58.4895 |           5.3129 |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0063 |          58.0399 |           5.3536 |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0086 |          57.7786 |           5.2712 |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0069 |          57.7181 |           5.1640 |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0066 |          57.4823 |           5.1223 |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0039 |          57.5536 |           5.1460 |
[32m[20221213 23:42:15 @agent_ppo2.py:185][0m |          -0.0000 |          59.0085 |           5.1532 |
[32m[20221213 23:42:16 @agent_ppo2.py:185][0m |          -0.0085 |          57.3046 |           5.0554 |
[32m[20221213 23:42:16 @agent_ppo2.py:185][0m |           0.0010 |          63.1827 |           5.1980 |
[32m[20221213 23:42:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.70
[32m[20221213 23:42:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.57
[32m[20221213 23:42:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 220.70
[32m[20221213 23:42:16 @agent_ppo2.py:143][0m Total time:      29.73 min
[32m[20221213 23:42:16 @agent_ppo2.py:145][0m 2881536 total steps have happened
[32m[20221213 23:42:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3407 --------------------------#
[32m[20221213 23:42:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:16 @agent_ppo2.py:185][0m |           0.0010 |          70.0184 |           5.4357 |
[32m[20221213 23:42:16 @agent_ppo2.py:185][0m |          -0.0036 |          66.1911 |           5.4569 |
[32m[20221213 23:42:16 @agent_ppo2.py:185][0m |          -0.0110 |          65.0959 |           5.4492 |
[32m[20221213 23:42:16 @agent_ppo2.py:185][0m |          -0.0094 |          64.0426 |           5.5399 |
[32m[20221213 23:42:16 @agent_ppo2.py:185][0m |          -0.0123 |          63.3367 |           5.5482 |
[32m[20221213 23:42:17 @agent_ppo2.py:185][0m |          -0.0125 |          62.6707 |           5.5443 |
[32m[20221213 23:42:17 @agent_ppo2.py:185][0m |          -0.0120 |          62.3125 |           5.5868 |
[32m[20221213 23:42:17 @agent_ppo2.py:185][0m |          -0.0120 |          61.9093 |           5.6062 |
[32m[20221213 23:42:17 @agent_ppo2.py:185][0m |          -0.0087 |          62.6652 |           5.5698 |
[32m[20221213 23:42:17 @agent_ppo2.py:185][0m |          -0.0071 |          62.2533 |           5.6332 |
[32m[20221213 23:42:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:42:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.64
[32m[20221213 23:42:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.00
[32m[20221213 23:42:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.04
[32m[20221213 23:42:17 @agent_ppo2.py:143][0m Total time:      29.75 min
[32m[20221213 23:42:17 @agent_ppo2.py:145][0m 2883584 total steps have happened
[32m[20221213 23:42:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3408 --------------------------#
[32m[20221213 23:42:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:17 @agent_ppo2.py:185][0m |          -0.0004 |          67.9714 |           4.6963 |
[32m[20221213 23:42:17 @agent_ppo2.py:185][0m |          -0.0030 |          64.7713 |           4.7984 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0081 |          63.6817 |           4.7578 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0095 |          62.6456 |           4.7623 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0022 |          64.9494 |           4.7275 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0092 |          63.5088 |           4.6834 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0126 |          61.7059 |           4.6986 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0140 |          61.1889 |           4.6754 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0157 |          60.7609 |           4.6595 |
[32m[20221213 23:42:18 @agent_ppo2.py:185][0m |          -0.0130 |          60.6396 |           4.5758 |
[32m[20221213 23:42:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.05
[32m[20221213 23:42:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.15
[32m[20221213 23:42:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.04
[32m[20221213 23:42:18 @agent_ppo2.py:143][0m Total time:      29.77 min
[32m[20221213 23:42:18 @agent_ppo2.py:145][0m 2885632 total steps have happened
[32m[20221213 23:42:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3409 --------------------------#
[32m[20221213 23:42:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0038 |          50.9634 |           5.4504 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0084 |          44.3542 |           5.3445 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0062 |          42.6355 |           5.3018 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0132 |          41.3667 |           5.3107 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0124 |          40.8622 |           5.2322 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0163 |          40.1210 |           5.2186 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0134 |          39.5814 |           5.1728 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0169 |          39.2550 |           5.1834 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0130 |          38.6382 |           5.1477 |
[32m[20221213 23:42:19 @agent_ppo2.py:185][0m |          -0.0155 |          38.5805 |           5.1691 |
[32m[20221213 23:42:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.62
[32m[20221213 23:42:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.73
[32m[20221213 23:42:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.07
[32m[20221213 23:42:19 @agent_ppo2.py:143][0m Total time:      29.79 min
[32m[20221213 23:42:19 @agent_ppo2.py:145][0m 2887680 total steps have happened
[32m[20221213 23:42:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3410 --------------------------#
[32m[20221213 23:42:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:42:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |           0.0029 |          66.6589 |           4.9223 |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |          -0.0059 |          64.1575 |           4.9370 |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |          -0.0100 |          63.6024 |           4.8539 |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |          -0.0073 |          63.0426 |           4.8522 |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |          -0.0121 |          62.7824 |           4.7973 |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |          -0.0163 |          62.4866 |           4.8355 |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |          -0.0129 |          62.2496 |           4.8410 |
[32m[20221213 23:42:20 @agent_ppo2.py:185][0m |          -0.0120 |          61.9439 |           4.7691 |
[32m[20221213 23:42:21 @agent_ppo2.py:185][0m |          -0.0118 |          61.6950 |           4.7868 |
[32m[20221213 23:42:21 @agent_ppo2.py:185][0m |          -0.0153 |          61.5972 |           4.7576 |
[32m[20221213 23:42:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:42:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.13
[32m[20221213 23:42:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.43
[32m[20221213 23:42:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.85
[32m[20221213 23:42:21 @agent_ppo2.py:143][0m Total time:      29.81 min
[32m[20221213 23:42:21 @agent_ppo2.py:145][0m 2889728 total steps have happened
[32m[20221213 23:42:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3411 --------------------------#
[32m[20221213 23:42:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:21 @agent_ppo2.py:185][0m |           0.0020 |          66.4480 |           4.0202 |
[32m[20221213 23:42:21 @agent_ppo2.py:185][0m |           0.0024 |          63.9440 |           4.0731 |
[32m[20221213 23:42:21 @agent_ppo2.py:185][0m |           0.0132 |          72.2625 |           4.0294 |
[32m[20221213 23:42:21 @agent_ppo2.py:185][0m |          -0.0104 |          62.9611 |           4.2342 |
[32m[20221213 23:42:21 @agent_ppo2.py:185][0m |          -0.0085 |          62.2444 |           4.1709 |
[32m[20221213 23:42:22 @agent_ppo2.py:185][0m |          -0.0126 |          61.9877 |           4.1828 |
[32m[20221213 23:42:22 @agent_ppo2.py:185][0m |          -0.0133 |          61.7165 |           4.2594 |
[32m[20221213 23:42:22 @agent_ppo2.py:185][0m |          -0.0094 |          61.5631 |           4.3031 |
[32m[20221213 23:42:22 @agent_ppo2.py:185][0m |          -0.0049 |          64.6653 |           4.2809 |
[32m[20221213 23:42:22 @agent_ppo2.py:185][0m |          -0.0137 |          61.4098 |           4.3766 |
[32m[20221213 23:42:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.57
[32m[20221213 23:42:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.31
[32m[20221213 23:42:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.14
[32m[20221213 23:42:22 @agent_ppo2.py:143][0m Total time:      29.83 min
[32m[20221213 23:42:22 @agent_ppo2.py:145][0m 2891776 total steps have happened
[32m[20221213 23:42:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3412 --------------------------#
[32m[20221213 23:42:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:42:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:22 @agent_ppo2.py:185][0m |           0.0019 |          55.6357 |           4.3646 |
[32m[20221213 23:42:22 @agent_ppo2.py:185][0m |           0.0115 |          63.4648 |           4.4504 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0031 |          53.5922 |           4.4024 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0064 |          52.9488 |           4.4862 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0077 |          52.9787 |           4.4952 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0093 |          52.7230 |           4.4574 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0094 |          52.5939 |           4.4632 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0105 |          52.5185 |           4.5248 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0125 |          52.3890 |           4.4846 |
[32m[20221213 23:42:23 @agent_ppo2.py:185][0m |          -0.0139 |          52.3754 |           4.5243 |
[32m[20221213 23:42:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:42:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.15
[32m[20221213 23:42:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.56
[32m[20221213 23:42:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.06
[32m[20221213 23:42:23 @agent_ppo2.py:143][0m Total time:      29.85 min
[32m[20221213 23:42:23 @agent_ppo2.py:145][0m 2893824 total steps have happened
[32m[20221213 23:42:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3413 --------------------------#
[32m[20221213 23:42:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |           0.0009 |          53.8384 |           4.3167 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0024 |          48.2389 |           4.3725 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0087 |          46.6630 |           4.3276 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0099 |          45.6174 |           4.3051 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0013 |          44.8881 |           4.2930 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0112 |          44.5288 |           4.2738 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0112 |          43.7369 |           4.2747 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0142 |          43.5726 |           4.3062 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0034 |          51.5641 |           4.3515 |
[32m[20221213 23:42:24 @agent_ppo2.py:185][0m |          -0.0144 |          43.1949 |           4.2676 |
[32m[20221213 23:42:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.28
[32m[20221213 23:42:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.66
[32m[20221213 23:42:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.53
[32m[20221213 23:42:25 @agent_ppo2.py:143][0m Total time:      29.88 min
[32m[20221213 23:42:25 @agent_ppo2.py:145][0m 2895872 total steps have happened
[32m[20221213 23:42:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3414 --------------------------#
[32m[20221213 23:42:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |           0.0053 |          53.0522 |           4.7737 |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |          -0.0003 |          49.9189 |           4.8367 |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |          -0.0021 |          49.7386 |           4.8091 |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |          -0.0015 |          49.7395 |           4.8406 |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |          -0.0040 |          49.3701 |           4.8415 |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |          -0.0031 |          49.3114 |           4.8710 |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |           0.0073 |          53.0924 |           4.8657 |
[32m[20221213 23:42:25 @agent_ppo2.py:185][0m |          -0.0041 |          49.1332 |           4.8596 |
[32m[20221213 23:42:26 @agent_ppo2.py:185][0m |          -0.0082 |          48.9680 |           4.8623 |
[32m[20221213 23:42:26 @agent_ppo2.py:185][0m |          -0.0074 |          48.9949 |           4.9095 |
[32m[20221213 23:42:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:42:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.32
[32m[20221213 23:42:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.34
[32m[20221213 23:42:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.81
[32m[20221213 23:42:26 @agent_ppo2.py:143][0m Total time:      29.90 min
[32m[20221213 23:42:26 @agent_ppo2.py:145][0m 2897920 total steps have happened
[32m[20221213 23:42:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3415 --------------------------#
[32m[20221213 23:42:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:26 @agent_ppo2.py:185][0m |          -0.0015 |          42.2758 |           5.0443 |
[32m[20221213 23:42:26 @agent_ppo2.py:185][0m |          -0.0077 |          37.4905 |           5.1403 |
[32m[20221213 23:42:26 @agent_ppo2.py:185][0m |          -0.0083 |          35.7006 |           5.0243 |
[32m[20221213 23:42:26 @agent_ppo2.py:185][0m |          -0.0169 |          35.0248 |           5.0279 |
[32m[20221213 23:42:26 @agent_ppo2.py:185][0m |          -0.0182 |          34.2012 |           5.0127 |
[32m[20221213 23:42:27 @agent_ppo2.py:185][0m |          -0.0147 |          34.0464 |           4.8648 |
[32m[20221213 23:42:27 @agent_ppo2.py:185][0m |          -0.0182 |          33.5998 |           4.9446 |
[32m[20221213 23:42:27 @agent_ppo2.py:185][0m |          -0.0137 |          33.0189 |           4.9430 |
[32m[20221213 23:42:27 @agent_ppo2.py:185][0m |          -0.0154 |          32.9164 |           4.9252 |
[32m[20221213 23:42:27 @agent_ppo2.py:185][0m |          -0.0154 |          32.5376 |           4.8775 |
[32m[20221213 23:42:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:42:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.67
[32m[20221213 23:42:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.85
[32m[20221213 23:42:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.37
[32m[20221213 23:42:27 @agent_ppo2.py:143][0m Total time:      29.92 min
[32m[20221213 23:42:27 @agent_ppo2.py:145][0m 2899968 total steps have happened
[32m[20221213 23:42:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3416 --------------------------#
[32m[20221213 23:42:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:27 @agent_ppo2.py:185][0m |           0.0023 |          67.9271 |           4.7270 |
[32m[20221213 23:42:27 @agent_ppo2.py:185][0m |           0.0016 |          65.4596 |           4.7642 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |          -0.0085 |          63.2081 |           4.6779 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |           0.0018 |          66.4340 |           4.7000 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |          -0.0036 |          66.2036 |           4.6768 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |          -0.0116 |          61.2949 |           4.7227 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |          -0.0028 |          64.0086 |           4.5944 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |          -0.0088 |          61.0800 |           4.7152 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |          -0.0091 |          60.5726 |           4.6427 |
[32m[20221213 23:42:28 @agent_ppo2.py:185][0m |          -0.0118 |          60.3438 |           4.6367 |
[32m[20221213 23:42:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.33
[32m[20221213 23:42:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.23
[32m[20221213 23:42:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.98
[32m[20221213 23:42:28 @agent_ppo2.py:143][0m Total time:      29.94 min
[32m[20221213 23:42:28 @agent_ppo2.py:145][0m 2902016 total steps have happened
[32m[20221213 23:42:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3417 --------------------------#
[32m[20221213 23:42:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |           0.0080 |          82.6741 |           3.7844 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0002 |          75.5758 |           3.8658 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |           0.0029 |          74.8180 |           3.9567 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0055 |          71.4058 |           3.9425 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0096 |          70.2241 |           4.0103 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0112 |          69.4491 |           4.0052 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0082 |          68.8593 |           4.0490 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0122 |          68.2305 |           4.0671 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0114 |          67.7303 |           4.1687 |
[32m[20221213 23:42:29 @agent_ppo2.py:185][0m |          -0.0121 |          67.1443 |           4.1450 |
[32m[20221213 23:42:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:42:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.22
[32m[20221213 23:42:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.84
[32m[20221213 23:42:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.41
[32m[20221213 23:42:30 @agent_ppo2.py:143][0m Total time:      29.96 min
[32m[20221213 23:42:30 @agent_ppo2.py:145][0m 2904064 total steps have happened
[32m[20221213 23:42:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3418 --------------------------#
[32m[20221213 23:42:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:30 @agent_ppo2.py:185][0m |           0.0029 |          50.7797 |           4.4247 |
[32m[20221213 23:42:30 @agent_ppo2.py:185][0m |          -0.0043 |          46.0419 |           4.5359 |
[32m[20221213 23:42:30 @agent_ppo2.py:185][0m |          -0.0045 |          44.1288 |           4.4993 |
[32m[20221213 23:42:30 @agent_ppo2.py:185][0m |           0.0021 |          44.1318 |           4.5918 |
[32m[20221213 23:42:30 @agent_ppo2.py:185][0m |          -0.0120 |          41.8851 |           4.6092 |
[32m[20221213 23:42:30 @agent_ppo2.py:185][0m |          -0.0073 |          41.8082 |           4.5646 |
[32m[20221213 23:42:30 @agent_ppo2.py:185][0m |          -0.0023 |          46.1334 |           4.6391 |
[32m[20221213 23:42:31 @agent_ppo2.py:185][0m |          -0.0192 |          41.0598 |           4.6484 |
[32m[20221213 23:42:31 @agent_ppo2.py:185][0m |          -0.0136 |          39.7087 |           4.6314 |
[32m[20221213 23:42:31 @agent_ppo2.py:185][0m |          -0.0191 |          39.1717 |           4.6629 |
[32m[20221213 23:42:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:42:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.07
[32m[20221213 23:42:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.96
[32m[20221213 23:42:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.52
[32m[20221213 23:42:31 @agent_ppo2.py:143][0m Total time:      29.98 min
[32m[20221213 23:42:31 @agent_ppo2.py:145][0m 2906112 total steps have happened
[32m[20221213 23:42:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3419 --------------------------#
[32m[20221213 23:42:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:31 @agent_ppo2.py:185][0m |           0.0022 |          59.0055 |           4.8864 |
[32m[20221213 23:42:31 @agent_ppo2.py:185][0m |           0.0016 |          58.4513 |           5.0099 |
[32m[20221213 23:42:31 @agent_ppo2.py:185][0m |          -0.0006 |          57.1608 |           4.8598 |
[32m[20221213 23:42:31 @agent_ppo2.py:185][0m |          -0.0097 |          56.4234 |           4.9674 |
[32m[20221213 23:42:32 @agent_ppo2.py:185][0m |          -0.0090 |          56.2678 |           4.9710 |
[32m[20221213 23:42:32 @agent_ppo2.py:185][0m |          -0.0053 |          56.1602 |           5.0182 |
[32m[20221213 23:42:32 @agent_ppo2.py:185][0m |          -0.0084 |          55.8886 |           5.0284 |
[32m[20221213 23:42:32 @agent_ppo2.py:185][0m |          -0.0122 |          55.4729 |           4.9808 |
[32m[20221213 23:42:32 @agent_ppo2.py:185][0m |          -0.0097 |          55.4115 |           5.0349 |
[32m[20221213 23:42:32 @agent_ppo2.py:185][0m |          -0.0117 |          55.3079 |           5.0841 |
[32m[20221213 23:42:32 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:42:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.18
[32m[20221213 23:42:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.29
[32m[20221213 23:42:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.22
[32m[20221213 23:42:32 @agent_ppo2.py:143][0m Total time:      30.00 min
[32m[20221213 23:42:32 @agent_ppo2.py:145][0m 2908160 total steps have happened
[32m[20221213 23:42:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3420 --------------------------#
[32m[20221213 23:42:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:42:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:32 @agent_ppo2.py:185][0m |           0.0002 |          63.7869 |           4.5132 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0078 |          61.8848 |           4.5367 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0064 |          61.1626 |           4.5867 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0107 |          60.7325 |           4.6855 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0123 |          60.4286 |           4.7015 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0109 |          60.0275 |           4.5695 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0135 |          59.7992 |           4.5909 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0048 |          63.8829 |           4.6495 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0109 |          59.7769 |           4.6699 |
[32m[20221213 23:42:33 @agent_ppo2.py:185][0m |          -0.0123 |          59.4648 |           4.7161 |
[32m[20221213 23:42:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:42:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.65
[32m[20221213 23:42:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.85
[32m[20221213 23:42:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.73
[32m[20221213 23:42:33 @agent_ppo2.py:143][0m Total time:      30.02 min
[32m[20221213 23:42:33 @agent_ppo2.py:145][0m 2910208 total steps have happened
[32m[20221213 23:42:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3421 --------------------------#
[32m[20221213 23:42:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |           0.0004 |          79.5988 |           4.6831 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |          -0.0054 |          75.7363 |           4.6608 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |           0.0012 |          75.0599 |           4.6773 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |           0.0025 |          72.9390 |           4.7171 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |          -0.0085 |          70.6816 |           4.6370 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |          -0.0078 |          69.2350 |           4.5559 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |          -0.0086 |          69.0589 |           4.5791 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |          -0.0134 |          68.1422 |           4.6136 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |          -0.0137 |          66.8799 |           4.5799 |
[32m[20221213 23:42:34 @agent_ppo2.py:185][0m |          -0.0159 |          66.5069 |           4.5467 |
[32m[20221213 23:42:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.64
[32m[20221213 23:42:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.86
[32m[20221213 23:42:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.37
[32m[20221213 23:42:35 @agent_ppo2.py:143][0m Total time:      30.04 min
[32m[20221213 23:42:35 @agent_ppo2.py:145][0m 2912256 total steps have happened
[32m[20221213 23:42:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3422 --------------------------#
[32m[20221213 23:42:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:35 @agent_ppo2.py:185][0m |           0.0006 |          58.2616 |           4.2607 |
[32m[20221213 23:42:35 @agent_ppo2.py:185][0m |          -0.0055 |          55.8178 |           4.0923 |
[32m[20221213 23:42:35 @agent_ppo2.py:185][0m |          -0.0054 |          55.0045 |           4.2714 |
[32m[20221213 23:42:35 @agent_ppo2.py:185][0m |          -0.0067 |          54.3142 |           4.2270 |
[32m[20221213 23:42:35 @agent_ppo2.py:185][0m |          -0.0071 |          53.9814 |           4.2510 |
[32m[20221213 23:42:35 @agent_ppo2.py:185][0m |          -0.0074 |          53.7359 |           4.2706 |
[32m[20221213 23:42:35 @agent_ppo2.py:185][0m |          -0.0125 |          53.4432 |           4.2539 |
[32m[20221213 23:42:36 @agent_ppo2.py:185][0m |          -0.0107 |          53.2365 |           4.2999 |
[32m[20221213 23:42:36 @agent_ppo2.py:185][0m |          -0.0122 |          53.1015 |           4.2966 |
[32m[20221213 23:42:36 @agent_ppo2.py:185][0m |          -0.0102 |          53.0147 |           4.3587 |
[32m[20221213 23:42:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.46
[32m[20221213 23:42:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.05
[32m[20221213 23:42:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.91
[32m[20221213 23:42:36 @agent_ppo2.py:143][0m Total time:      30.06 min
[32m[20221213 23:42:36 @agent_ppo2.py:145][0m 2914304 total steps have happened
[32m[20221213 23:42:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3423 --------------------------#
[32m[20221213 23:42:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:36 @agent_ppo2.py:185][0m |           0.0039 |          62.8489 |           3.9558 |
[32m[20221213 23:42:36 @agent_ppo2.py:185][0m |          -0.0070 |          55.8581 |           3.9962 |
[32m[20221213 23:42:36 @agent_ppo2.py:185][0m |          -0.0003 |          54.3155 |           4.0051 |
[32m[20221213 23:42:36 @agent_ppo2.py:185][0m |          -0.0105 |          52.8303 |           4.0354 |
[32m[20221213 23:42:37 @agent_ppo2.py:185][0m |          -0.0094 |          52.4330 |           4.0667 |
[32m[20221213 23:42:37 @agent_ppo2.py:185][0m |          -0.0131 |          51.8175 |           3.9842 |
[32m[20221213 23:42:37 @agent_ppo2.py:185][0m |          -0.0158 |          51.5726 |           4.0139 |
[32m[20221213 23:42:37 @agent_ppo2.py:185][0m |          -0.0072 |          53.1137 |           4.0494 |
[32m[20221213 23:42:37 @agent_ppo2.py:185][0m |          -0.0099 |          50.7344 |           4.0759 |
[32m[20221213 23:42:37 @agent_ppo2.py:185][0m |          -0.0144 |          50.6763 |           4.0820 |
[32m[20221213 23:42:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.86
[32m[20221213 23:42:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.25
[32m[20221213 23:42:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.78
[32m[20221213 23:42:37 @agent_ppo2.py:143][0m Total time:      30.09 min
[32m[20221213 23:42:37 @agent_ppo2.py:145][0m 2916352 total steps have happened
[32m[20221213 23:42:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3424 --------------------------#
[32m[20221213 23:42:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:37 @agent_ppo2.py:185][0m |          -0.0014 |          67.2389 |           4.5408 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0026 |          64.1678 |           4.6635 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0044 |          63.1390 |           4.6624 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0112 |          62.5567 |           4.6821 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0025 |          66.3189 |           4.7059 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0072 |          62.0509 |           4.7639 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0125 |          61.6362 |           4.8153 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0105 |          61.5109 |           4.8623 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0124 |          61.0499 |           4.9399 |
[32m[20221213 23:42:38 @agent_ppo2.py:185][0m |          -0.0144 |          60.9382 |           4.9259 |
[32m[20221213 23:42:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:42:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.81
[32m[20221213 23:42:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.90
[32m[20221213 23:42:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.89
[32m[20221213 23:42:38 @agent_ppo2.py:143][0m Total time:      30.11 min
[32m[20221213 23:42:38 @agent_ppo2.py:145][0m 2918400 total steps have happened
[32m[20221213 23:42:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3425 --------------------------#
[32m[20221213 23:42:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:42:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |           0.0043 |          60.5785 |           5.6036 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0082 |          54.6454 |           5.5848 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0055 |          52.4309 |           5.6766 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0075 |          50.6635 |           5.7499 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0049 |          49.6340 |           5.7497 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0131 |          48.7266 |           5.7577 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0129 |          47.8980 |           5.8324 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0146 |          47.3471 |           5.8796 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0226 |          46.9776 |           5.8698 |
[32m[20221213 23:42:39 @agent_ppo2.py:185][0m |          -0.0154 |          46.6016 |           5.8543 |
[32m[20221213 23:42:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:42:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.35
[32m[20221213 23:42:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.86
[32m[20221213 23:42:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.75
[32m[20221213 23:42:40 @agent_ppo2.py:143][0m Total time:      30.13 min
[32m[20221213 23:42:40 @agent_ppo2.py:145][0m 2920448 total steps have happened
[32m[20221213 23:42:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3426 --------------------------#
[32m[20221213 23:42:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:40 @agent_ppo2.py:185][0m |           0.0052 |          74.1926 |           5.1226 |
[32m[20221213 23:42:40 @agent_ppo2.py:185][0m |           0.0062 |          73.7855 |           5.1722 |
[32m[20221213 23:42:40 @agent_ppo2.py:185][0m |          -0.0071 |          67.0991 |           5.1267 |
[32m[20221213 23:42:40 @agent_ppo2.py:185][0m |          -0.0117 |          65.5933 |           5.1230 |
[32m[20221213 23:42:40 @agent_ppo2.py:185][0m |          -0.0128 |          64.7977 |           5.1832 |
[32m[20221213 23:42:40 @agent_ppo2.py:185][0m |          -0.0153 |          64.1975 |           5.1460 |
[32m[20221213 23:42:40 @agent_ppo2.py:185][0m |          -0.0167 |          63.5219 |           5.2423 |
[32m[20221213 23:42:41 @agent_ppo2.py:185][0m |          -0.0159 |          63.2295 |           5.2783 |
[32m[20221213 23:42:41 @agent_ppo2.py:185][0m |          -0.0150 |          62.6929 |           5.2864 |
[32m[20221213 23:42:41 @agent_ppo2.py:185][0m |          -0.0152 |          62.3297 |           5.2816 |
[32m[20221213 23:42:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.88
[32m[20221213 23:42:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.84
[32m[20221213 23:42:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.59
[32m[20221213 23:42:41 @agent_ppo2.py:143][0m Total time:      30.15 min
[32m[20221213 23:42:41 @agent_ppo2.py:145][0m 2922496 total steps have happened
[32m[20221213 23:42:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3427 --------------------------#
[32m[20221213 23:42:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:41 @agent_ppo2.py:185][0m |          -0.0024 |          63.0016 |           5.4277 |
[32m[20221213 23:42:41 @agent_ppo2.py:185][0m |          -0.0094 |          58.0865 |           5.3819 |
[32m[20221213 23:42:41 @agent_ppo2.py:185][0m |          -0.0121 |          56.4052 |           5.3760 |
[32m[20221213 23:42:41 @agent_ppo2.py:185][0m |          -0.0106 |          54.7852 |           5.4291 |
[32m[20221213 23:42:42 @agent_ppo2.py:185][0m |          -0.0143 |          53.8494 |           5.3745 |
[32m[20221213 23:42:42 @agent_ppo2.py:185][0m |          -0.0120 |          52.9510 |           5.3762 |
[32m[20221213 23:42:42 @agent_ppo2.py:185][0m |          -0.0139 |          52.3441 |           5.3536 |
[32m[20221213 23:42:42 @agent_ppo2.py:185][0m |          -0.0184 |          51.6945 |           5.3192 |
[32m[20221213 23:42:42 @agent_ppo2.py:185][0m |          -0.0107 |          52.1642 |           5.3076 |
[32m[20221213 23:42:42 @agent_ppo2.py:185][0m |          -0.0148 |          50.8076 |           5.2952 |
[32m[20221213 23:42:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.15
[32m[20221213 23:42:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.05
[32m[20221213 23:42:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.01
[32m[20221213 23:42:42 @agent_ppo2.py:143][0m Total time:      30.17 min
[32m[20221213 23:42:42 @agent_ppo2.py:145][0m 2924544 total steps have happened
[32m[20221213 23:42:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3428 --------------------------#
[32m[20221213 23:42:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:42 @agent_ppo2.py:185][0m |           0.0056 |          38.0304 |           6.0784 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0076 |          30.9159 |           6.0705 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0109 |          28.9182 |           6.1028 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0151 |          27.2033 |           6.1230 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0100 |          26.3664 |           6.1222 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0117 |          25.5684 |           6.1693 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0105 |          25.0865 |           6.0791 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0117 |          24.6736 |           6.1376 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0171 |          24.2560 |           6.0866 |
[32m[20221213 23:42:43 @agent_ppo2.py:185][0m |          -0.0171 |          23.8666 |           6.0537 |
[32m[20221213 23:42:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.10
[32m[20221213 23:42:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.02
[32m[20221213 23:42:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.92
[32m[20221213 23:42:43 @agent_ppo2.py:143][0m Total time:      30.19 min
[32m[20221213 23:42:43 @agent_ppo2.py:145][0m 2926592 total steps have happened
[32m[20221213 23:42:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3429 --------------------------#
[32m[20221213 23:42:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0033 |          52.5482 |           5.3484 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0011 |          48.9361 |           5.3821 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0044 |          48.3278 |           5.3901 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0044 |          47.8498 |           5.4232 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0084 |          47.6898 |           5.4520 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0029 |          47.9693 |           5.4441 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0085 |          47.1563 |           5.4916 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0068 |          48.1395 |           5.5260 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0073 |          47.0336 |           5.4459 |
[32m[20221213 23:42:44 @agent_ppo2.py:185][0m |          -0.0170 |          46.8206 |           5.4940 |
[32m[20221213 23:42:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:42:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.91
[32m[20221213 23:42:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.66
[32m[20221213 23:42:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.04
[32m[20221213 23:42:45 @agent_ppo2.py:143][0m Total time:      30.21 min
[32m[20221213 23:42:45 @agent_ppo2.py:145][0m 2928640 total steps have happened
[32m[20221213 23:42:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3430 --------------------------#
[32m[20221213 23:42:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:42:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:45 @agent_ppo2.py:185][0m |          -0.0035 |          59.2917 |           5.5708 |
[32m[20221213 23:42:45 @agent_ppo2.py:185][0m |          -0.0065 |          53.5731 |           5.5525 |
[32m[20221213 23:42:45 @agent_ppo2.py:185][0m |          -0.0111 |          51.5039 |           5.6277 |
[32m[20221213 23:42:45 @agent_ppo2.py:185][0m |           0.0010 |          52.4417 |           5.6098 |
[32m[20221213 23:42:45 @agent_ppo2.py:185][0m |          -0.0093 |          49.1795 |           5.5671 |
[32m[20221213 23:42:45 @agent_ppo2.py:185][0m |          -0.0134 |          48.0908 |           5.6040 |
[32m[20221213 23:42:45 @agent_ppo2.py:185][0m |          -0.0165 |          47.1648 |           5.6159 |
[32m[20221213 23:42:46 @agent_ppo2.py:185][0m |          -0.0177 |          46.4934 |           5.6131 |
[32m[20221213 23:42:46 @agent_ppo2.py:185][0m |          -0.0191 |          45.7027 |           5.5980 |
[32m[20221213 23:42:46 @agent_ppo2.py:185][0m |          -0.0206 |          45.2428 |           5.5482 |
[32m[20221213 23:42:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.40
[32m[20221213 23:42:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.15
[32m[20221213 23:42:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.08
[32m[20221213 23:42:46 @agent_ppo2.py:143][0m Total time:      30.23 min
[32m[20221213 23:42:46 @agent_ppo2.py:145][0m 2930688 total steps have happened
[32m[20221213 23:42:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3431 --------------------------#
[32m[20221213 23:42:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:46 @agent_ppo2.py:185][0m |          -0.0015 |          60.4693 |           4.9498 |
[32m[20221213 23:42:46 @agent_ppo2.py:185][0m |           0.0020 |          55.6617 |           4.9782 |
[32m[20221213 23:42:46 @agent_ppo2.py:185][0m |          -0.0035 |          53.4190 |           4.9753 |
[32m[20221213 23:42:46 @agent_ppo2.py:185][0m |          -0.0060 |          52.1993 |           5.0146 |
[32m[20221213 23:42:47 @agent_ppo2.py:185][0m |          -0.0007 |          53.8442 |           5.0524 |
[32m[20221213 23:42:47 @agent_ppo2.py:185][0m |          -0.0079 |          50.5474 |           5.0705 |
[32m[20221213 23:42:47 @agent_ppo2.py:185][0m |          -0.0088 |          50.1812 |           5.0359 |
[32m[20221213 23:42:47 @agent_ppo2.py:185][0m |          -0.0099 |          49.7322 |           5.1253 |
[32m[20221213 23:42:47 @agent_ppo2.py:185][0m |          -0.0046 |          50.4277 |           5.0549 |
[32m[20221213 23:42:47 @agent_ppo2.py:185][0m |          -0.0078 |          49.2113 |           5.0929 |
[32m[20221213 23:42:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.09
[32m[20221213 23:42:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.55
[32m[20221213 23:42:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.54
[32m[20221213 23:42:47 @agent_ppo2.py:143][0m Total time:      30.25 min
[32m[20221213 23:42:47 @agent_ppo2.py:145][0m 2932736 total steps have happened
[32m[20221213 23:42:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3432 --------------------------#
[32m[20221213 23:42:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:47 @agent_ppo2.py:185][0m |          -0.0000 |          46.1337 |           5.7554 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |           0.0025 |          39.9288 |           5.5619 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0028 |          37.3277 |           5.6148 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0037 |          36.4571 |           5.5043 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0092 |          36.0306 |           5.5105 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0089 |          35.3938 |           5.4797 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0179 |          34.9957 |           5.5020 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0135 |          34.6646 |           5.3812 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0141 |          34.2485 |           5.3625 |
[32m[20221213 23:42:48 @agent_ppo2.py:185][0m |          -0.0146 |          33.9537 |           5.3542 |
[32m[20221213 23:42:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.49
[32m[20221213 23:42:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.87
[32m[20221213 23:42:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.12
[32m[20221213 23:42:48 @agent_ppo2.py:143][0m Total time:      30.27 min
[32m[20221213 23:42:48 @agent_ppo2.py:145][0m 2934784 total steps have happened
[32m[20221213 23:42:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3433 --------------------------#
[32m[20221213 23:42:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0012 |          42.9443 |           5.0189 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0093 |          36.8504 |           5.0583 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0096 |          35.0598 |           5.0149 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0087 |          33.7807 |           5.0322 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0160 |          33.1951 |           5.0469 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0193 |          32.4997 |           5.0340 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0160 |          32.1106 |           5.0688 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0203 |          31.5935 |           5.1142 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0219 |          31.3379 |           5.1159 |
[32m[20221213 23:42:49 @agent_ppo2.py:185][0m |          -0.0184 |          31.1079 |           5.1574 |
[32m[20221213 23:42:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.84
[32m[20221213 23:42:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.87
[32m[20221213 23:42:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.15
[32m[20221213 23:42:50 @agent_ppo2.py:143][0m Total time:      30.29 min
[32m[20221213 23:42:50 @agent_ppo2.py:145][0m 2936832 total steps have happened
[32m[20221213 23:42:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3434 --------------------------#
[32m[20221213 23:42:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:50 @agent_ppo2.py:185][0m |           0.0036 |          57.9309 |           5.2523 |
[32m[20221213 23:42:50 @agent_ppo2.py:185][0m |           0.0007 |          52.1620 |           5.3279 |
[32m[20221213 23:42:50 @agent_ppo2.py:185][0m |          -0.0053 |          50.4890 |           5.3654 |
[32m[20221213 23:42:50 @agent_ppo2.py:185][0m |          -0.0033 |          50.5801 |           5.3480 |
[32m[20221213 23:42:50 @agent_ppo2.py:185][0m |          -0.0056 |          48.5705 |           5.4091 |
[32m[20221213 23:42:50 @agent_ppo2.py:185][0m |          -0.0066 |          47.9746 |           5.4515 |
[32m[20221213 23:42:50 @agent_ppo2.py:185][0m |          -0.0130 |          47.5494 |           5.3803 |
[32m[20221213 23:42:51 @agent_ppo2.py:185][0m |          -0.0081 |          46.8603 |           5.4661 |
[32m[20221213 23:42:51 @agent_ppo2.py:185][0m |           0.0022 |          50.4008 |           5.4756 |
[32m[20221213 23:42:51 @agent_ppo2.py:185][0m |          -0.0141 |          46.5973 |           5.5146 |
[32m[20221213 23:42:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:42:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.48
[32m[20221213 23:42:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.54
[32m[20221213 23:42:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.83
[32m[20221213 23:42:51 @agent_ppo2.py:143][0m Total time:      30.31 min
[32m[20221213 23:42:51 @agent_ppo2.py:145][0m 2938880 total steps have happened
[32m[20221213 23:42:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3435 --------------------------#
[32m[20221213 23:42:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:51 @agent_ppo2.py:185][0m |          -0.0035 |          61.7406 |           5.7158 |
[32m[20221213 23:42:51 @agent_ppo2.py:185][0m |          -0.0042 |          55.8962 |           5.6849 |
[32m[20221213 23:42:51 @agent_ppo2.py:185][0m |          -0.0110 |          53.5795 |           5.7708 |
[32m[20221213 23:42:51 @agent_ppo2.py:185][0m |          -0.0154 |          52.5027 |           5.7359 |
[32m[20221213 23:42:52 @agent_ppo2.py:185][0m |          -0.0176 |          51.2922 |           5.7440 |
[32m[20221213 23:42:52 @agent_ppo2.py:185][0m |          -0.0128 |          50.4333 |           5.7063 |
[32m[20221213 23:42:52 @agent_ppo2.py:185][0m |          -0.0143 |          49.6269 |           5.6467 |
[32m[20221213 23:42:52 @agent_ppo2.py:185][0m |          -0.0165 |          49.1568 |           5.6922 |
[32m[20221213 23:42:52 @agent_ppo2.py:185][0m |          -0.0182 |          49.1198 |           5.6247 |
[32m[20221213 23:42:52 @agent_ppo2.py:185][0m |          -0.0172 |          48.6325 |           5.5664 |
[32m[20221213 23:42:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.23
[32m[20221213 23:42:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.47
[32m[20221213 23:42:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.03
[32m[20221213 23:42:52 @agent_ppo2.py:143][0m Total time:      30.34 min
[32m[20221213 23:42:52 @agent_ppo2.py:145][0m 2940928 total steps have happened
[32m[20221213 23:42:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3436 --------------------------#
[32m[20221213 23:42:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:42:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:52 @agent_ppo2.py:185][0m |          -0.0019 |          47.5549 |           4.9299 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |          -0.0028 |          42.7831 |           4.9037 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |          -0.0068 |          41.6808 |           4.9485 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |           0.0022 |          42.3123 |           4.8653 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |           0.0062 |          46.0821 |           4.8504 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |          -0.0002 |          46.5092 |           4.8887 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |          -0.0132 |          39.6495 |           4.8842 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |          -0.0126 |          39.3013 |           4.8584 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |          -0.0147 |          38.9891 |           4.8714 |
[32m[20221213 23:42:53 @agent_ppo2.py:185][0m |          -0.0157 |          38.5818 |           4.7950 |
[32m[20221213 23:42:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:42:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.93
[32m[20221213 23:42:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.14
[32m[20221213 23:42:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.61
[32m[20221213 23:42:53 @agent_ppo2.py:143][0m Total time:      30.36 min
[32m[20221213 23:42:53 @agent_ppo2.py:145][0m 2942976 total steps have happened
[32m[20221213 23:42:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3437 --------------------------#
[32m[20221213 23:42:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0018 |          66.1136 |           4.8325 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0097 |          61.8560 |           4.8694 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0120 |          60.0697 |           4.8858 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0098 |          58.7469 |           4.8013 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0113 |          58.0286 |           4.8143 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0102 |          57.2129 |           4.7519 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0113 |          56.7263 |           4.7117 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0128 |          56.2472 |           4.7065 |
[32m[20221213 23:42:54 @agent_ppo2.py:185][0m |          -0.0181 |          55.9433 |           4.7135 |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |          -0.0099 |          56.3455 |           4.6878 |
[32m[20221213 23:42:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:42:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.30
[32m[20221213 23:42:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.84
[32m[20221213 23:42:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.98
[32m[20221213 23:42:55 @agent_ppo2.py:143][0m Total time:      30.38 min
[32m[20221213 23:42:55 @agent_ppo2.py:145][0m 2945024 total steps have happened
[32m[20221213 23:42:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3438 --------------------------#
[32m[20221213 23:42:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |           0.0030 |          57.2044 |           5.1315 |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |           0.0052 |          55.4586 |           5.1341 |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |          -0.0050 |          53.8616 |           5.1593 |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |          -0.0041 |          53.2117 |           5.1455 |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |          -0.0084 |          52.7287 |           5.0185 |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |          -0.0069 |          52.4052 |           5.0487 |
[32m[20221213 23:42:55 @agent_ppo2.py:185][0m |          -0.0070 |          52.1381 |           5.0856 |
[32m[20221213 23:42:56 @agent_ppo2.py:185][0m |          -0.0065 |          51.9538 |           5.0325 |
[32m[20221213 23:42:56 @agent_ppo2.py:185][0m |          -0.0083 |          51.8817 |           5.0428 |
[32m[20221213 23:42:56 @agent_ppo2.py:185][0m |          -0.0084 |          51.6382 |           5.0024 |
[32m[20221213 23:42:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.55
[32m[20221213 23:42:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.31
[32m[20221213 23:42:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.24
[32m[20221213 23:42:56 @agent_ppo2.py:143][0m Total time:      30.40 min
[32m[20221213 23:42:56 @agent_ppo2.py:145][0m 2947072 total steps have happened
[32m[20221213 23:42:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3439 --------------------------#
[32m[20221213 23:42:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:42:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:56 @agent_ppo2.py:185][0m |           0.0026 |          30.1404 |           4.3342 |
[32m[20221213 23:42:56 @agent_ppo2.py:185][0m |           0.0024 |          23.5016 |           4.4732 |
[32m[20221213 23:42:56 @agent_ppo2.py:185][0m |           0.0026 |          21.8757 |           4.3242 |
[32m[20221213 23:42:57 @agent_ppo2.py:185][0m |          -0.0162 |          20.0079 |           4.3339 |
[32m[20221213 23:42:57 @agent_ppo2.py:185][0m |          -0.0131 |          19.0897 |           4.3141 |
[32m[20221213 23:42:57 @agent_ppo2.py:185][0m |          -0.0173 |          18.1943 |           4.2782 |
[32m[20221213 23:42:57 @agent_ppo2.py:185][0m |          -0.0171 |          17.9028 |           4.2003 |
[32m[20221213 23:42:57 @agent_ppo2.py:185][0m |          -0.0145 |          17.2239 |           4.2275 |
[32m[20221213 23:42:57 @agent_ppo2.py:185][0m |          -0.0210 |          16.5878 |           4.1564 |
[32m[20221213 23:42:57 @agent_ppo2.py:185][0m |          -0.0152 |          16.5050 |           4.1363 |
[32m[20221213 23:42:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:42:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.55
[32m[20221213 23:42:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.95
[32m[20221213 23:42:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.04
[32m[20221213 23:42:57 @agent_ppo2.py:143][0m Total time:      30.42 min
[32m[20221213 23:42:57 @agent_ppo2.py:145][0m 2949120 total steps have happened
[32m[20221213 23:42:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3440 --------------------------#
[32m[20221213 23:42:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:42:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0003 |          36.1819 |           4.3973 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0015 |          31.3354 |           4.3819 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0053 |          29.8843 |           4.3766 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0060 |          28.8304 |           4.3436 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0064 |          27.9927 |           4.3678 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0097 |          27.7173 |           4.4340 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0126 |          27.0349 |           4.3897 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0149 |          26.8938 |           4.3429 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0133 |          26.4963 |           4.3055 |
[32m[20221213 23:42:58 @agent_ppo2.py:185][0m |          -0.0097 |          26.3351 |           4.3638 |
[32m[20221213 23:42:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:42:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.17
[32m[20221213 23:42:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.28
[32m[20221213 23:42:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.54
[32m[20221213 23:42:58 @agent_ppo2.py:143][0m Total time:      30.44 min
[32m[20221213 23:42:58 @agent_ppo2.py:145][0m 2951168 total steps have happened
[32m[20221213 23:42:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3441 --------------------------#
[32m[20221213 23:42:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:42:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |           0.0018 |          56.1665 |           3.9102 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0011 |          54.5876 |           3.8887 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0069 |          53.5993 |           3.9149 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0074 |          53.2398 |           3.9306 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0022 |          53.9604 |           3.8174 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0062 |          52.9764 |           3.8228 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0098 |          52.8367 |           3.7735 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0078 |          52.6658 |           3.7636 |
[32m[20221213 23:42:59 @agent_ppo2.py:185][0m |          -0.0070 |          52.5731 |           3.6761 |
[32m[20221213 23:43:00 @agent_ppo2.py:185][0m |          -0.0087 |          52.4566 |           3.6471 |
[32m[20221213 23:43:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:43:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.21
[32m[20221213 23:43:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.00
[32m[20221213 23:43:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.96
[32m[20221213 23:43:00 @agent_ppo2.py:143][0m Total time:      30.46 min
[32m[20221213 23:43:00 @agent_ppo2.py:145][0m 2953216 total steps have happened
[32m[20221213 23:43:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3442 --------------------------#
[32m[20221213 23:43:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:00 @agent_ppo2.py:185][0m |          -0.0018 |          56.4429 |           2.3354 |
[32m[20221213 23:43:00 @agent_ppo2.py:185][0m |           0.0003 |          54.2995 |           2.2867 |
[32m[20221213 23:43:00 @agent_ppo2.py:185][0m |          -0.0034 |          53.7651 |           2.4185 |
[32m[20221213 23:43:00 @agent_ppo2.py:185][0m |          -0.0049 |          53.6401 |           2.3148 |
[32m[20221213 23:43:00 @agent_ppo2.py:185][0m |          -0.0072 |          53.4119 |           2.3828 |
[32m[20221213 23:43:00 @agent_ppo2.py:185][0m |          -0.0066 |          53.1486 |           2.3711 |
[32m[20221213 23:43:01 @agent_ppo2.py:185][0m |          -0.0069 |          53.1796 |           2.3998 |
[32m[20221213 23:43:01 @agent_ppo2.py:185][0m |          -0.0037 |          53.1297 |           2.4274 |
[32m[20221213 23:43:01 @agent_ppo2.py:185][0m |           0.0023 |          56.8664 |           2.4653 |
[32m[20221213 23:43:01 @agent_ppo2.py:185][0m |          -0.0054 |          52.8522 |           2.4508 |
[32m[20221213 23:43:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.39
[32m[20221213 23:43:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.02
[32m[20221213 23:43:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.65
[32m[20221213 23:43:01 @agent_ppo2.py:143][0m Total time:      30.48 min
[32m[20221213 23:43:01 @agent_ppo2.py:145][0m 2955264 total steps have happened
[32m[20221213 23:43:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3443 --------------------------#
[32m[20221213 23:43:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:01 @agent_ppo2.py:185][0m |          -0.0011 |          40.4824 |           3.3738 |
[32m[20221213 23:43:01 @agent_ppo2.py:185][0m |          -0.0062 |          34.1033 |           3.4636 |
[32m[20221213 23:43:01 @agent_ppo2.py:185][0m |          -0.0047 |          31.9799 |           3.3949 |
[32m[20221213 23:43:02 @agent_ppo2.py:185][0m |          -0.0021 |          31.2140 |           3.4285 |
[32m[20221213 23:43:02 @agent_ppo2.py:185][0m |          -0.0136 |          29.6653 |           3.4681 |
[32m[20221213 23:43:02 @agent_ppo2.py:185][0m |          -0.0111 |          29.5073 |           3.4109 |
[32m[20221213 23:43:02 @agent_ppo2.py:185][0m |          -0.0096 |          28.8790 |           3.4624 |
[32m[20221213 23:43:02 @agent_ppo2.py:185][0m |          -0.0135 |          27.8631 |           3.5281 |
[32m[20221213 23:43:02 @agent_ppo2.py:185][0m |          -0.0100 |          27.6059 |           3.4751 |
[32m[20221213 23:43:02 @agent_ppo2.py:185][0m |          -0.0137 |          27.2965 |           3.5024 |
[32m[20221213 23:43:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:43:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.59
[32m[20221213 23:43:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.66
[32m[20221213 23:43:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.87
[32m[20221213 23:43:02 @agent_ppo2.py:143][0m Total time:      30.50 min
[32m[20221213 23:43:02 @agent_ppo2.py:145][0m 2957312 total steps have happened
[32m[20221213 23:43:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3444 --------------------------#
[32m[20221213 23:43:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |           0.0004 |          64.5695 |           4.1196 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |           0.0024 |          60.5957 |           4.2453 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0083 |          58.7003 |           4.2853 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0042 |          57.3162 |           4.2497 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0034 |          56.8363 |           4.2008 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0087 |          55.6709 |           4.2835 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0103 |          55.3207 |           4.4091 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0113 |          54.9682 |           4.3335 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0096 |          54.3353 |           4.3785 |
[32m[20221213 23:43:03 @agent_ppo2.py:185][0m |          -0.0131 |          54.4455 |           4.4362 |
[32m[20221213 23:43:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:43:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.04
[32m[20221213 23:43:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.05
[32m[20221213 23:43:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.68
[32m[20221213 23:43:03 @agent_ppo2.py:143][0m Total time:      30.52 min
[32m[20221213 23:43:03 @agent_ppo2.py:145][0m 2959360 total steps have happened
[32m[20221213 23:43:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3445 --------------------------#
[32m[20221213 23:43:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0029 |          39.6583 |           4.3741 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0041 |          32.2879 |           4.2946 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0064 |          29.5737 |           4.2046 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0124 |          28.2644 |           4.1432 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0072 |          27.2594 |           4.1059 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0153 |          26.5558 |           4.1498 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0142 |          26.1896 |           4.1103 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0227 |          25.6308 |           3.9875 |
[32m[20221213 23:43:04 @agent_ppo2.py:185][0m |          -0.0201 |          25.1825 |           4.0008 |
[32m[20221213 23:43:05 @agent_ppo2.py:185][0m |          -0.0183 |          24.9697 |           3.9339 |
[32m[20221213 23:43:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.50
[32m[20221213 23:43:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.92
[32m[20221213 23:43:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.25
[32m[20221213 23:43:05 @agent_ppo2.py:143][0m Total time:      30.54 min
[32m[20221213 23:43:05 @agent_ppo2.py:145][0m 2961408 total steps have happened
[32m[20221213 23:43:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3446 --------------------------#
[32m[20221213 23:43:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:05 @agent_ppo2.py:185][0m |           0.0019 |          41.6038 |           3.1181 |
[32m[20221213 23:43:05 @agent_ppo2.py:185][0m |          -0.0062 |          37.3473 |           3.1720 |
[32m[20221213 23:43:05 @agent_ppo2.py:185][0m |          -0.0089 |          35.6012 |           3.2065 |
[32m[20221213 23:43:05 @agent_ppo2.py:185][0m |          -0.0042 |          34.7650 |           3.3334 |
[32m[20221213 23:43:05 @agent_ppo2.py:185][0m |          -0.0044 |          33.8312 |           3.3784 |
[32m[20221213 23:43:05 @agent_ppo2.py:185][0m |          -0.0112 |          33.2114 |           3.4496 |
[32m[20221213 23:43:06 @agent_ppo2.py:185][0m |          -0.0168 |          32.9428 |           3.4514 |
[32m[20221213 23:43:06 @agent_ppo2.py:185][0m |          -0.0169 |          32.4399 |           3.5022 |
[32m[20221213 23:43:06 @agent_ppo2.py:185][0m |          -0.0143 |          32.2010 |           3.4756 |
[32m[20221213 23:43:06 @agent_ppo2.py:185][0m |          -0.0210 |          31.7451 |           3.5613 |
[32m[20221213 23:43:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.51
[32m[20221213 23:43:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.54
[32m[20221213 23:43:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.61
[32m[20221213 23:43:06 @agent_ppo2.py:143][0m Total time:      30.57 min
[32m[20221213 23:43:06 @agent_ppo2.py:145][0m 2963456 total steps have happened
[32m[20221213 23:43:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3447 --------------------------#
[32m[20221213 23:43:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:06 @agent_ppo2.py:185][0m |           0.0023 |          56.6185 |           3.4512 |
[32m[20221213 23:43:06 @agent_ppo2.py:185][0m |          -0.0036 |          50.4373 |           3.5269 |
[32m[20221213 23:43:06 @agent_ppo2.py:185][0m |          -0.0070 |          47.9198 |           3.5888 |
[32m[20221213 23:43:07 @agent_ppo2.py:185][0m |          -0.0027 |          46.6983 |           3.5707 |
[32m[20221213 23:43:07 @agent_ppo2.py:185][0m |          -0.0095 |          45.8069 |           3.5601 |
[32m[20221213 23:43:07 @agent_ppo2.py:185][0m |          -0.0047 |          47.4770 |           3.5530 |
[32m[20221213 23:43:07 @agent_ppo2.py:185][0m |          -0.0051 |          45.0668 |           3.6362 |
[32m[20221213 23:43:07 @agent_ppo2.py:185][0m |          -0.0118 |          43.7751 |           3.6897 |
[32m[20221213 23:43:07 @agent_ppo2.py:185][0m |          -0.0122 |          43.2450 |           3.7035 |
[32m[20221213 23:43:07 @agent_ppo2.py:185][0m |          -0.0074 |          44.5715 |           3.6486 |
[32m[20221213 23:43:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.32
[32m[20221213 23:43:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.95
[32m[20221213 23:43:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.70
[32m[20221213 23:43:07 @agent_ppo2.py:143][0m Total time:      30.59 min
[32m[20221213 23:43:07 @agent_ppo2.py:145][0m 2965504 total steps have happened
[32m[20221213 23:43:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3448 --------------------------#
[32m[20221213 23:43:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |           0.0083 |          29.3207 |           4.4336 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0076 |          22.8733 |           4.4836 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0045 |          21.3266 |           4.4789 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0014 |          20.4366 |           4.5489 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0118 |          19.8538 |           4.6340 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0123 |          19.2737 |           4.5214 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0141 |          19.1258 |           4.4583 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0159 |          18.6045 |           4.5447 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0246 |          18.4651 |           4.5401 |
[32m[20221213 23:43:08 @agent_ppo2.py:185][0m |          -0.0191 |          18.1485 |           4.5133 |
[32m[20221213 23:43:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:43:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.25
[32m[20221213 23:43:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.60
[32m[20221213 23:43:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.20
[32m[20221213 23:43:08 @agent_ppo2.py:143][0m Total time:      30.61 min
[32m[20221213 23:43:08 @agent_ppo2.py:145][0m 2967552 total steps have happened
[32m[20221213 23:43:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3449 --------------------------#
[32m[20221213 23:43:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0023 |          60.8975 |           4.3736 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0067 |          59.9056 |           4.3810 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0077 |          59.2200 |           4.3307 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0067 |          58.8296 |           4.4036 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0067 |          58.7827 |           4.3749 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0086 |          58.3637 |           4.3991 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0096 |          58.0749 |           4.3690 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0074 |          57.7611 |           4.4740 |
[32m[20221213 23:43:09 @agent_ppo2.py:185][0m |          -0.0109 |          57.6843 |           4.4556 |
[32m[20221213 23:43:10 @agent_ppo2.py:185][0m |          -0.0100 |          57.5750 |           4.4189 |
[32m[20221213 23:43:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.98
[32m[20221213 23:43:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.56
[32m[20221213 23:43:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.93
[32m[20221213 23:43:10 @agent_ppo2.py:143][0m Total time:      30.63 min
[32m[20221213 23:43:10 @agent_ppo2.py:145][0m 2969600 total steps have happened
[32m[20221213 23:43:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3450 --------------------------#
[32m[20221213 23:43:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:43:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:10 @agent_ppo2.py:185][0m |          -0.0035 |          52.2321 |           4.3009 |
[32m[20221213 23:43:10 @agent_ppo2.py:185][0m |           0.0031 |          47.6047 |           4.2535 |
[32m[20221213 23:43:10 @agent_ppo2.py:185][0m |          -0.0006 |          45.0755 |           4.3120 |
[32m[20221213 23:43:10 @agent_ppo2.py:185][0m |          -0.0115 |          43.8269 |           4.3451 |
[32m[20221213 23:43:10 @agent_ppo2.py:185][0m |          -0.0104 |          43.2951 |           4.3899 |
[32m[20221213 23:43:10 @agent_ppo2.py:185][0m |          -0.0128 |          42.5685 |           4.3410 |
[32m[20221213 23:43:11 @agent_ppo2.py:185][0m |          -0.0130 |          42.1106 |           4.4211 |
[32m[20221213 23:43:11 @agent_ppo2.py:185][0m |          -0.0135 |          41.8211 |           4.4180 |
[32m[20221213 23:43:11 @agent_ppo2.py:185][0m |          -0.0100 |          41.4279 |           4.4139 |
[32m[20221213 23:43:11 @agent_ppo2.py:185][0m |          -0.0098 |          41.6145 |           4.4348 |
[32m[20221213 23:43:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.77
[32m[20221213 23:43:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.20
[32m[20221213 23:43:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.70
[32m[20221213 23:43:11 @agent_ppo2.py:143][0m Total time:      30.65 min
[32m[20221213 23:43:11 @agent_ppo2.py:145][0m 2971648 total steps have happened
[32m[20221213 23:43:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3451 --------------------------#
[32m[20221213 23:43:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:11 @agent_ppo2.py:185][0m |          -0.0049 |          34.1145 |           4.6142 |
[32m[20221213 23:43:11 @agent_ppo2.py:185][0m |          -0.0044 |          31.2440 |           4.6196 |
[32m[20221213 23:43:11 @agent_ppo2.py:185][0m |          -0.0080 |          30.0711 |           4.6706 |
[32m[20221213 23:43:12 @agent_ppo2.py:185][0m |          -0.0112 |          29.3685 |           4.6257 |
[32m[20221213 23:43:12 @agent_ppo2.py:185][0m |          -0.0132 |          28.6926 |           4.5766 |
[32m[20221213 23:43:12 @agent_ppo2.py:185][0m |          -0.0151 |          28.2007 |           4.5096 |
[32m[20221213 23:43:12 @agent_ppo2.py:185][0m |          -0.0238 |          27.8249 |           4.4890 |
[32m[20221213 23:43:12 @agent_ppo2.py:185][0m |          -0.0161 |          27.5763 |           4.4096 |
[32m[20221213 23:43:12 @agent_ppo2.py:185][0m |          -0.0116 |          27.2936 |           4.4143 |
[32m[20221213 23:43:12 @agent_ppo2.py:185][0m |          -0.0097 |          27.9896 |           4.4334 |
[32m[20221213 23:43:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.39
[32m[20221213 23:43:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.30
[32m[20221213 23:43:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.39
[32m[20221213 23:43:12 @agent_ppo2.py:143][0m Total time:      30.67 min
[32m[20221213 23:43:12 @agent_ppo2.py:145][0m 2973696 total steps have happened
[32m[20221213 23:43:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3452 --------------------------#
[32m[20221213 23:43:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |           0.0039 |          63.3928 |           3.6451 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |           0.0085 |          64.6200 |           3.6373 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0069 |          59.0822 |           3.5479 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0075 |          58.2365 |           3.6307 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0080 |          57.8460 |           3.7361 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0095 |          57.6759 |           3.7175 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0045 |          57.9100 |           3.7834 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0045 |          57.1388 |           3.6974 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0108 |          56.9721 |           3.7528 |
[32m[20221213 23:43:13 @agent_ppo2.py:185][0m |          -0.0124 |          56.6265 |           3.8682 |
[32m[20221213 23:43:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.60
[32m[20221213 23:43:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.18
[32m[20221213 23:43:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.86
[32m[20221213 23:43:13 @agent_ppo2.py:143][0m Total time:      30.69 min
[32m[20221213 23:43:13 @agent_ppo2.py:145][0m 2975744 total steps have happened
[32m[20221213 23:43:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3453 --------------------------#
[32m[20221213 23:43:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0007 |          60.4362 |           4.1053 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0031 |          56.3029 |           4.1805 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0004 |          56.8192 |           4.2324 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0048 |          54.3492 |           4.3124 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0111 |          54.0385 |           4.2614 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0071 |          53.6038 |           4.3231 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0094 |          53.1796 |           4.2946 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0115 |          53.1679 |           4.2927 |
[32m[20221213 23:43:14 @agent_ppo2.py:185][0m |          -0.0126 |          52.7713 |           4.4025 |
[32m[20221213 23:43:15 @agent_ppo2.py:185][0m |          -0.0110 |          52.7859 |           4.3550 |
[32m[20221213 23:43:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:43:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.88
[32m[20221213 23:43:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.89
[32m[20221213 23:43:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.71
[32m[20221213 23:43:15 @agent_ppo2.py:143][0m Total time:      30.71 min
[32m[20221213 23:43:15 @agent_ppo2.py:145][0m 2977792 total steps have happened
[32m[20221213 23:43:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3454 --------------------------#
[32m[20221213 23:43:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:15 @agent_ppo2.py:185][0m |           0.0023 |          69.4353 |           3.6638 |
[32m[20221213 23:43:15 @agent_ppo2.py:185][0m |          -0.0041 |          68.7163 |           3.7062 |
[32m[20221213 23:43:15 @agent_ppo2.py:185][0m |          -0.0061 |          68.1102 |           3.7423 |
[32m[20221213 23:43:15 @agent_ppo2.py:185][0m |          -0.0080 |          67.6141 |           3.8293 |
[32m[20221213 23:43:15 @agent_ppo2.py:185][0m |          -0.0068 |          67.4661 |           3.8170 |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0064 |          67.5610 |           3.7063 |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0115 |          66.9203 |           3.7158 |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0101 |          66.8741 |           3.7798 |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0088 |          66.7345 |           3.7730 |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0058 |          67.3167 |           3.7913 |
[32m[20221213 23:43:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.16
[32m[20221213 23:43:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.68
[32m[20221213 23:43:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.17
[32m[20221213 23:43:16 @agent_ppo2.py:143][0m Total time:      30.73 min
[32m[20221213 23:43:16 @agent_ppo2.py:145][0m 2979840 total steps have happened
[32m[20221213 23:43:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3455 --------------------------#
[32m[20221213 23:43:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0015 |          43.5958 |           4.1306 |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0073 |          38.7240 |           4.2448 |
[32m[20221213 23:43:16 @agent_ppo2.py:185][0m |          -0.0098 |          37.8323 |           4.2545 |
[32m[20221213 23:43:17 @agent_ppo2.py:185][0m |          -0.0031 |          38.0998 |           4.3812 |
[32m[20221213 23:43:17 @agent_ppo2.py:185][0m |          -0.0125 |          36.6433 |           4.3986 |
[32m[20221213 23:43:17 @agent_ppo2.py:185][0m |          -0.0122 |          36.1919 |           4.4487 |
[32m[20221213 23:43:17 @agent_ppo2.py:185][0m |          -0.0160 |          36.0295 |           4.4956 |
[32m[20221213 23:43:17 @agent_ppo2.py:185][0m |          -0.0131 |          35.7909 |           4.5578 |
[32m[20221213 23:43:17 @agent_ppo2.py:185][0m |          -0.0150 |          35.5142 |           4.5996 |
[32m[20221213 23:43:17 @agent_ppo2.py:185][0m |          -0.0089 |          35.2492 |           4.6155 |
[32m[20221213 23:43:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.92
[32m[20221213 23:43:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.87
[32m[20221213 23:43:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.98
[32m[20221213 23:43:17 @agent_ppo2.py:143][0m Total time:      30.75 min
[32m[20221213 23:43:17 @agent_ppo2.py:145][0m 2981888 total steps have happened
[32m[20221213 23:43:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3456 --------------------------#
[32m[20221213 23:43:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0019 |          68.1658 |           4.9969 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |           0.0040 |          71.7897 |           4.9853 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |           0.0056 |          72.8467 |           5.0014 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0076 |          64.7449 |           4.9352 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0140 |          64.2557 |           4.8603 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0040 |          65.4335 |           4.8818 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0108 |          63.8397 |           4.9032 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0123 |          63.4083 |           4.9177 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0145 |          63.5303 |           4.8111 |
[32m[20221213 23:43:18 @agent_ppo2.py:185][0m |          -0.0165 |          63.2342 |           4.8414 |
[32m[20221213 23:43:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.24
[32m[20221213 23:43:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.98
[32m[20221213 23:43:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.38
[32m[20221213 23:43:18 @agent_ppo2.py:143][0m Total time:      30.77 min
[32m[20221213 23:43:18 @agent_ppo2.py:145][0m 2983936 total steps have happened
[32m[20221213 23:43:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3457 --------------------------#
[32m[20221213 23:43:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |           0.0001 |          60.6352 |           5.3217 |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |          -0.0050 |          57.2145 |           5.2838 |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |          -0.0094 |          55.5199 |           5.4297 |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |          -0.0078 |          54.0089 |           5.4683 |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |          -0.0078 |          53.7183 |           5.4485 |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |          -0.0092 |          52.7327 |           5.5452 |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |          -0.0094 |          52.2560 |           5.5448 |
[32m[20221213 23:43:19 @agent_ppo2.py:185][0m |          -0.0041 |          52.4963 |           5.5753 |
[32m[20221213 23:43:20 @agent_ppo2.py:185][0m |          -0.0121 |          51.4732 |           5.6575 |
[32m[20221213 23:43:20 @agent_ppo2.py:185][0m |          -0.0098 |          51.2892 |           5.6463 |
[32m[20221213 23:43:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.77
[32m[20221213 23:43:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.52
[32m[20221213 23:43:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.04
[32m[20221213 23:43:20 @agent_ppo2.py:143][0m Total time:      30.80 min
[32m[20221213 23:43:20 @agent_ppo2.py:145][0m 2985984 total steps have happened
[32m[20221213 23:43:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3458 --------------------------#
[32m[20221213 23:43:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:20 @agent_ppo2.py:185][0m |          -0.0004 |          38.6843 |           4.9973 |
[32m[20221213 23:43:20 @agent_ppo2.py:185][0m |          -0.0010 |          31.9707 |           5.0204 |
[32m[20221213 23:43:20 @agent_ppo2.py:185][0m |          -0.0125 |          30.3594 |           5.0651 |
[32m[20221213 23:43:20 @agent_ppo2.py:185][0m |          -0.0123 |          29.3921 |           5.1055 |
[32m[20221213 23:43:20 @agent_ppo2.py:185][0m |          -0.0098 |          28.8120 |           5.1570 |
[32m[20221213 23:43:21 @agent_ppo2.py:185][0m |          -0.0139 |          28.4439 |           5.0475 |
[32m[20221213 23:43:21 @agent_ppo2.py:185][0m |          -0.0188 |          28.0944 |           5.1028 |
[32m[20221213 23:43:21 @agent_ppo2.py:185][0m |          -0.0137 |          27.6686 |           5.1981 |
[32m[20221213 23:43:21 @agent_ppo2.py:185][0m |          -0.0146 |          27.4562 |           5.1395 |
[32m[20221213 23:43:21 @agent_ppo2.py:185][0m |          -0.0167 |          27.1242 |           5.0809 |
[32m[20221213 23:43:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:43:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.96
[32m[20221213 23:43:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.19
[32m[20221213 23:43:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.74
[32m[20221213 23:43:21 @agent_ppo2.py:143][0m Total time:      30.82 min
[32m[20221213 23:43:21 @agent_ppo2.py:145][0m 2988032 total steps have happened
[32m[20221213 23:43:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3459 --------------------------#
[32m[20221213 23:43:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:21 @agent_ppo2.py:185][0m |          -0.0048 |          48.6073 |           5.1760 |
[32m[20221213 23:43:21 @agent_ppo2.py:185][0m |          -0.0052 |          43.5739 |           5.2585 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0090 |          41.5243 |           5.2727 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0081 |          40.8890 |           5.2749 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0080 |          40.0764 |           5.2610 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0129 |          39.3608 |           5.3023 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0149 |          38.9221 |           5.3251 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0054 |          40.4422 |           5.3019 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0140 |          38.6213 |           5.2748 |
[32m[20221213 23:43:22 @agent_ppo2.py:185][0m |          -0.0112 |          38.9893 |           5.3607 |
[32m[20221213 23:43:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.00
[32m[20221213 23:43:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.57
[32m[20221213 23:43:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.76
[32m[20221213 23:43:22 @agent_ppo2.py:143][0m Total time:      30.84 min
[32m[20221213 23:43:22 @agent_ppo2.py:145][0m 2990080 total steps have happened
[32m[20221213 23:43:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3460 --------------------------#
[32m[20221213 23:43:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:43:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0013 |          55.3157 |           5.5454 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0070 |          50.1238 |           5.5318 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0092 |          48.0562 |           5.4751 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0107 |          47.2675 |           5.5136 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0100 |          46.1328 |           5.5135 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0076 |          46.9177 |           5.5140 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0098 |          44.7473 |           5.4803 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0120 |          44.2933 |           5.4951 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0130 |          43.7535 |           5.4667 |
[32m[20221213 23:43:23 @agent_ppo2.py:185][0m |          -0.0158 |          43.3133 |           5.4848 |
[32m[20221213 23:43:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.34
[32m[20221213 23:43:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.06
[32m[20221213 23:43:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.24
[32m[20221213 23:43:24 @agent_ppo2.py:143][0m Total time:      30.86 min
[32m[20221213 23:43:24 @agent_ppo2.py:145][0m 2992128 total steps have happened
[32m[20221213 23:43:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3461 --------------------------#
[32m[20221213 23:43:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |           0.0198 |          63.7744 |           5.2363 |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |           0.0016 |          58.3492 |           5.2687 |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |           0.0025 |          56.6508 |           5.1851 |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |          -0.0077 |          54.0074 |           5.2118 |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |          -0.0140 |          53.2958 |           5.1815 |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |          -0.0159 |          52.8276 |           5.1547 |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |          -0.0155 |          52.5804 |           5.0501 |
[32m[20221213 23:43:24 @agent_ppo2.py:185][0m |          -0.0149 |          52.2713 |           5.0735 |
[32m[20221213 23:43:25 @agent_ppo2.py:185][0m |          -0.0049 |          56.0793 |           5.0871 |
[32m[20221213 23:43:25 @agent_ppo2.py:185][0m |          -0.0127 |          52.0913 |           5.0266 |
[32m[20221213 23:43:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.36
[32m[20221213 23:43:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.88
[32m[20221213 23:43:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.06
[32m[20221213 23:43:25 @agent_ppo2.py:143][0m Total time:      30.88 min
[32m[20221213 23:43:25 @agent_ppo2.py:145][0m 2994176 total steps have happened
[32m[20221213 23:43:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3462 --------------------------#
[32m[20221213 23:43:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:25 @agent_ppo2.py:185][0m |           0.0001 |          56.7408 |           4.4240 |
[32m[20221213 23:43:25 @agent_ppo2.py:185][0m |          -0.0075 |          53.1365 |           4.4134 |
[32m[20221213 23:43:25 @agent_ppo2.py:185][0m |          -0.0076 |          51.2259 |           4.4931 |
[32m[20221213 23:43:25 @agent_ppo2.py:185][0m |          -0.0064 |          50.2738 |           4.3454 |
[32m[20221213 23:43:25 @agent_ppo2.py:185][0m |          -0.0119 |          49.1777 |           4.4090 |
[32m[20221213 23:43:26 @agent_ppo2.py:185][0m |          -0.0136 |          48.6796 |           4.4041 |
[32m[20221213 23:43:26 @agent_ppo2.py:185][0m |          -0.0025 |          58.9845 |           4.3846 |
[32m[20221213 23:43:26 @agent_ppo2.py:185][0m |          -0.0119 |          48.1051 |           4.4722 |
[32m[20221213 23:43:26 @agent_ppo2.py:185][0m |          -0.0134 |          47.6311 |           4.3442 |
[32m[20221213 23:43:26 @agent_ppo2.py:185][0m |          -0.0177 |          46.6684 |           4.4045 |
[32m[20221213 23:43:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.34
[32m[20221213 23:43:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.76
[32m[20221213 23:43:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.70
[32m[20221213 23:43:26 @agent_ppo2.py:143][0m Total time:      30.90 min
[32m[20221213 23:43:26 @agent_ppo2.py:145][0m 2996224 total steps have happened
[32m[20221213 23:43:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3463 --------------------------#
[32m[20221213 23:43:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:26 @agent_ppo2.py:185][0m |          -0.0030 |          62.1086 |           4.3805 |
[32m[20221213 23:43:26 @agent_ppo2.py:185][0m |          -0.0042 |          57.7836 |           4.3490 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0095 |          55.5798 |           4.4640 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0074 |          55.3696 |           4.4607 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0142 |          54.4900 |           4.5593 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0079 |          53.0439 |           4.6620 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0086 |          52.8144 |           4.6125 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0105 |          52.2137 |           4.6213 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0164 |          51.6589 |           4.6468 |
[32m[20221213 23:43:27 @agent_ppo2.py:185][0m |          -0.0173 |          51.4212 |           4.7141 |
[32m[20221213 23:43:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:43:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.26
[32m[20221213 23:43:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.12
[32m[20221213 23:43:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.29
[32m[20221213 23:43:27 @agent_ppo2.py:143][0m Total time:      30.92 min
[32m[20221213 23:43:27 @agent_ppo2.py:145][0m 2998272 total steps have happened
[32m[20221213 23:43:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3464 --------------------------#
[32m[20221213 23:43:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0005 |          43.6819 |           5.9446 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0020 |          37.8889 |           6.0376 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0100 |          35.9703 |           6.1181 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0066 |          34.6759 |           6.0333 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0107 |          33.4288 |           6.1355 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0132 |          32.7206 |           6.0923 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0172 |          32.6260 |           6.1273 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0186 |          31.6285 |           6.0742 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0117 |          31.3856 |           6.1144 |
[32m[20221213 23:43:28 @agent_ppo2.py:185][0m |          -0.0145 |          30.7968 |           6.0966 |
[32m[20221213 23:43:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.37
[32m[20221213 23:43:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.90
[32m[20221213 23:43:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.08
[32m[20221213 23:43:29 @agent_ppo2.py:143][0m Total time:      30.94 min
[32m[20221213 23:43:29 @agent_ppo2.py:145][0m 3000320 total steps have happened
[32m[20221213 23:43:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3465 --------------------------#
[32m[20221213 23:43:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |          -0.0032 |          68.5352 |           5.1102 |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |           0.0006 |          68.1100 |           5.0384 |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |          -0.0073 |          65.9374 |           5.0616 |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |          -0.0092 |          65.2871 |           5.1588 |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |          -0.0101 |          65.1445 |           4.9742 |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |          -0.0129 |          64.7708 |           4.9389 |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |          -0.0127 |          64.9898 |           4.9720 |
[32m[20221213 23:43:29 @agent_ppo2.py:185][0m |          -0.0170 |          64.4347 |           4.8964 |
[32m[20221213 23:43:30 @agent_ppo2.py:185][0m |           0.0013 |          74.8802 |           4.9995 |
[32m[20221213 23:43:30 @agent_ppo2.py:185][0m |          -0.0142 |          64.8266 |           5.0117 |
[32m[20221213 23:43:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.82
[32m[20221213 23:43:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.16
[32m[20221213 23:43:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.09
[32m[20221213 23:43:30 @agent_ppo2.py:143][0m Total time:      30.96 min
[32m[20221213 23:43:30 @agent_ppo2.py:145][0m 3002368 total steps have happened
[32m[20221213 23:43:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3466 --------------------------#
[32m[20221213 23:43:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:30 @agent_ppo2.py:185][0m |           0.0078 |          38.2409 |           4.8296 |
[32m[20221213 23:43:30 @agent_ppo2.py:185][0m |           0.0053 |          36.4367 |           5.0052 |
[32m[20221213 23:43:30 @agent_ppo2.py:185][0m |          -0.0030 |          32.1196 |           5.0613 |
[32m[20221213 23:43:30 @agent_ppo2.py:185][0m |          -0.0030 |          31.1192 |           5.0884 |
[32m[20221213 23:43:30 @agent_ppo2.py:185][0m |          -0.0067 |          30.1917 |           5.0709 |
[32m[20221213 23:43:31 @agent_ppo2.py:185][0m |          -0.0124 |          29.9059 |           5.1718 |
[32m[20221213 23:43:31 @agent_ppo2.py:185][0m |          -0.0156 |          29.1985 |           5.1804 |
[32m[20221213 23:43:31 @agent_ppo2.py:185][0m |          -0.0159 |          28.6441 |           5.2148 |
[32m[20221213 23:43:31 @agent_ppo2.py:185][0m |          -0.0163 |          28.4534 |           5.2040 |
[32m[20221213 23:43:31 @agent_ppo2.py:185][0m |          -0.0205 |          28.2680 |           5.2667 |
[32m[20221213 23:43:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.69
[32m[20221213 23:43:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.24
[32m[20221213 23:43:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.92
[32m[20221213 23:43:31 @agent_ppo2.py:143][0m Total time:      30.98 min
[32m[20221213 23:43:31 @agent_ppo2.py:145][0m 3004416 total steps have happened
[32m[20221213 23:43:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3467 --------------------------#
[32m[20221213 23:43:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:31 @agent_ppo2.py:185][0m |           0.0000 |          61.2077 |           5.5993 |
[32m[20221213 23:43:31 @agent_ppo2.py:185][0m |           0.0045 |          60.5870 |           5.5573 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0084 |          56.3481 |           5.5355 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0074 |          55.1654 |           5.6265 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0067 |          54.1265 |           5.6503 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0079 |          53.5079 |           5.7119 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0134 |          53.0630 |           5.8258 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0083 |          52.4518 |           5.7110 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0140 |          51.8778 |           5.7960 |
[32m[20221213 23:43:32 @agent_ppo2.py:185][0m |          -0.0105 |          51.3839 |           5.8468 |
[32m[20221213 23:43:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:43:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.48
[32m[20221213 23:43:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.48
[32m[20221213 23:43:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.76
[32m[20221213 23:43:32 @agent_ppo2.py:143][0m Total time:      31.00 min
[32m[20221213 23:43:32 @agent_ppo2.py:145][0m 3006464 total steps have happened
[32m[20221213 23:43:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3468 --------------------------#
[32m[20221213 23:43:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0004 |          64.6763 |           5.2710 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0053 |          62.6130 |           5.1856 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0060 |          61.3563 |           5.2642 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0087 |          60.7127 |           5.3069 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |           0.0027 |          62.6988 |           5.3088 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0093 |          59.9075 |           5.3610 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0094 |          59.8195 |           5.3022 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0078 |          59.3879 |           5.3868 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0144 |          59.1204 |           5.3952 |
[32m[20221213 23:43:33 @agent_ppo2.py:185][0m |          -0.0135 |          59.0357 |           5.3318 |
[32m[20221213 23:43:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:43:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.34
[32m[20221213 23:43:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.95
[32m[20221213 23:43:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.26
[32m[20221213 23:43:34 @agent_ppo2.py:143][0m Total time:      31.03 min
[32m[20221213 23:43:34 @agent_ppo2.py:145][0m 3008512 total steps have happened
[32m[20221213 23:43:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3469 --------------------------#
[32m[20221213 23:43:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |          -0.0018 |          57.6041 |           5.5838 |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |          -0.0047 |          55.3628 |           5.6338 |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |          -0.0075 |          54.6343 |           5.5990 |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |          -0.0096 |          54.2465 |           5.7215 |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |           0.0048 |          59.3109 |           5.6671 |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |          -0.0076 |          53.6191 |           5.7053 |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |          -0.0078 |          53.3427 |           5.7491 |
[32m[20221213 23:43:34 @agent_ppo2.py:185][0m |          -0.0123 |          53.3348 |           5.7675 |
[32m[20221213 23:43:35 @agent_ppo2.py:185][0m |          -0.0083 |          53.0274 |           5.7891 |
[32m[20221213 23:43:35 @agent_ppo2.py:185][0m |          -0.0150 |          52.7296 |           5.7710 |
[32m[20221213 23:43:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.80
[32m[20221213 23:43:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.34
[32m[20221213 23:43:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.46
[32m[20221213 23:43:35 @agent_ppo2.py:143][0m Total time:      31.05 min
[32m[20221213 23:43:35 @agent_ppo2.py:145][0m 3010560 total steps have happened
[32m[20221213 23:43:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3470 --------------------------#
[32m[20221213 23:43:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:43:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:35 @agent_ppo2.py:185][0m |           0.0020 |          61.5161 |           6.5370 |
[32m[20221213 23:43:35 @agent_ppo2.py:185][0m |          -0.0085 |          55.1467 |           6.4835 |
[32m[20221213 23:43:35 @agent_ppo2.py:185][0m |          -0.0100 |          53.5186 |           6.5128 |
[32m[20221213 23:43:35 @agent_ppo2.py:185][0m |          -0.0113 |          52.0697 |           6.5214 |
[32m[20221213 23:43:35 @agent_ppo2.py:185][0m |          -0.0044 |          51.2863 |           6.5239 |
[32m[20221213 23:43:36 @agent_ppo2.py:185][0m |          -0.0097 |          50.7843 |           6.4686 |
[32m[20221213 23:43:36 @agent_ppo2.py:185][0m |          -0.0097 |          50.3228 |           6.5148 |
[32m[20221213 23:43:36 @agent_ppo2.py:185][0m |          -0.0004 |          57.8355 |           6.4908 |
[32m[20221213 23:43:36 @agent_ppo2.py:185][0m |          -0.0170 |          49.9036 |           6.5274 |
[32m[20221213 23:43:36 @agent_ppo2.py:185][0m |          -0.0130 |          49.1340 |           6.4824 |
[32m[20221213 23:43:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.90
[32m[20221213 23:43:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.55
[32m[20221213 23:43:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.50
[32m[20221213 23:43:36 @agent_ppo2.py:143][0m Total time:      31.07 min
[32m[20221213 23:43:36 @agent_ppo2.py:145][0m 3012608 total steps have happened
[32m[20221213 23:43:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3471 --------------------------#
[32m[20221213 23:43:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:36 @agent_ppo2.py:185][0m |           0.0019 |          57.0643 |           5.6546 |
[32m[20221213 23:43:36 @agent_ppo2.py:185][0m |          -0.0078 |          51.9787 |           5.5832 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0022 |          51.7402 |           5.6174 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0090 |          49.0554 |           5.6194 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0002 |          52.1024 |           5.6255 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0133 |          47.4301 |           5.6966 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0172 |          46.7677 |           5.6234 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0140 |          46.3492 |           5.7107 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0176 |          46.2741 |           5.6516 |
[32m[20221213 23:43:37 @agent_ppo2.py:185][0m |          -0.0196 |          45.3875 |           5.6733 |
[32m[20221213 23:43:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.06
[32m[20221213 23:43:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.72
[32m[20221213 23:43:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.13
[32m[20221213 23:43:37 @agent_ppo2.py:143][0m Total time:      31.09 min
[32m[20221213 23:43:37 @agent_ppo2.py:145][0m 3014656 total steps have happened
[32m[20221213 23:43:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3472 --------------------------#
[32m[20221213 23:43:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0035 |          51.7773 |           5.6144 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0063 |          46.6789 |           5.7577 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0058 |          44.4297 |           5.7421 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0093 |          43.0623 |           5.7788 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0084 |          41.7639 |           5.8433 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0113 |          40.9435 |           5.8165 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0128 |          40.2951 |           5.9256 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0128 |          39.8651 |           5.9287 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0064 |          42.8477 |           5.9399 |
[32m[20221213 23:43:38 @agent_ppo2.py:185][0m |          -0.0166 |          39.4492 |           5.9518 |
[32m[20221213 23:43:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:43:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.28
[32m[20221213 23:43:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.62
[32m[20221213 23:43:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.80
[32m[20221213 23:43:39 @agent_ppo2.py:143][0m Total time:      31.11 min
[32m[20221213 23:43:39 @agent_ppo2.py:145][0m 3016704 total steps have happened
[32m[20221213 23:43:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3473 --------------------------#
[32m[20221213 23:43:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:39 @agent_ppo2.py:185][0m |           0.0055 |          62.6126 |           5.7816 |
[32m[20221213 23:43:39 @agent_ppo2.py:185][0m |           0.0021 |          58.8412 |           5.8273 |
[32m[20221213 23:43:39 @agent_ppo2.py:185][0m |          -0.0031 |          56.2147 |           5.8175 |
[32m[20221213 23:43:39 @agent_ppo2.py:185][0m |          -0.0080 |          55.4308 |           5.8671 |
[32m[20221213 23:43:39 @agent_ppo2.py:185][0m |          -0.0059 |          54.8296 |           5.8884 |
[32m[20221213 23:43:39 @agent_ppo2.py:185][0m |          -0.0076 |          54.3140 |           5.8855 |
[32m[20221213 23:43:39 @agent_ppo2.py:185][0m |          -0.0080 |          53.8094 |           5.9360 |
[32m[20221213 23:43:40 @agent_ppo2.py:185][0m |          -0.0086 |          53.5671 |           5.9422 |
[32m[20221213 23:43:40 @agent_ppo2.py:185][0m |          -0.0078 |          53.5491 |           5.9978 |
[32m[20221213 23:43:40 @agent_ppo2.py:185][0m |          -0.0101 |          53.1090 |           5.9129 |
[32m[20221213 23:43:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.84
[32m[20221213 23:43:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.14
[32m[20221213 23:43:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.60
[32m[20221213 23:43:40 @agent_ppo2.py:143][0m Total time:      31.13 min
[32m[20221213 23:43:40 @agent_ppo2.py:145][0m 3018752 total steps have happened
[32m[20221213 23:43:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3474 --------------------------#
[32m[20221213 23:43:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:40 @agent_ppo2.py:185][0m |           0.0068 |          58.9167 |           6.2905 |
[32m[20221213 23:43:40 @agent_ppo2.py:185][0m |           0.0032 |          55.5471 |           6.3845 |
[32m[20221213 23:43:40 @agent_ppo2.py:185][0m |          -0.0034 |          52.1948 |           6.3914 |
[32m[20221213 23:43:40 @agent_ppo2.py:185][0m |          -0.0117 |          50.7286 |           6.3337 |
[32m[20221213 23:43:41 @agent_ppo2.py:185][0m |          -0.0151 |          49.9461 |           6.3964 |
[32m[20221213 23:43:41 @agent_ppo2.py:185][0m |          -0.0134 |          49.3472 |           6.3803 |
[32m[20221213 23:43:41 @agent_ppo2.py:185][0m |          -0.0104 |          49.0326 |           6.3702 |
[32m[20221213 23:43:41 @agent_ppo2.py:185][0m |          -0.0216 |          48.3593 |           6.3263 |
[32m[20221213 23:43:41 @agent_ppo2.py:185][0m |          -0.0136 |          49.0437 |           6.3250 |
[32m[20221213 23:43:41 @agent_ppo2.py:185][0m |          -0.0229 |          47.9642 |           6.3226 |
[32m[20221213 23:43:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.77
[32m[20221213 23:43:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.33
[32m[20221213 23:43:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.88
[32m[20221213 23:43:41 @agent_ppo2.py:143][0m Total time:      31.15 min
[32m[20221213 23:43:41 @agent_ppo2.py:145][0m 3020800 total steps have happened
[32m[20221213 23:43:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3475 --------------------------#
[32m[20221213 23:43:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:41 @agent_ppo2.py:185][0m |          -0.0020 |          67.8753 |           5.7415 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0038 |          65.9345 |           5.6642 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |           0.0007 |          63.5810 |           5.6615 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0126 |          60.4115 |           5.6392 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0118 |          59.2543 |           5.6389 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0145 |          58.4156 |           5.6231 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0133 |          57.9497 |           5.5842 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0171 |          57.5408 |           5.5600 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0190 |          57.1627 |           5.5533 |
[32m[20221213 23:43:42 @agent_ppo2.py:185][0m |          -0.0204 |          56.8317 |           5.5293 |
[32m[20221213 23:43:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.23
[32m[20221213 23:43:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.20
[32m[20221213 23:43:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.71
[32m[20221213 23:43:42 @agent_ppo2.py:143][0m Total time:      31.17 min
[32m[20221213 23:43:42 @agent_ppo2.py:145][0m 3022848 total steps have happened
[32m[20221213 23:43:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3476 --------------------------#
[32m[20221213 23:43:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0006 |          60.7264 |           6.0824 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |           0.0003 |          57.0506 |           6.1778 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0085 |          55.2436 |           6.0629 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0069 |          54.5117 |           6.0579 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0159 |          53.9003 |           6.1372 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0124 |          53.2116 |           6.0084 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0107 |          52.7771 |           5.9965 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0190 |          52.5506 |           6.0974 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0129 |          52.1163 |           6.0455 |
[32m[20221213 23:43:43 @agent_ppo2.py:185][0m |          -0.0144 |          51.6886 |           6.0379 |
[32m[20221213 23:43:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.04
[32m[20221213 23:43:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.41
[32m[20221213 23:43:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.10
[32m[20221213 23:43:44 @agent_ppo2.py:143][0m Total time:      31.19 min
[32m[20221213 23:43:44 @agent_ppo2.py:145][0m 3024896 total steps have happened
[32m[20221213 23:43:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3477 --------------------------#
[32m[20221213 23:43:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:44 @agent_ppo2.py:185][0m |          -0.0006 |          56.0892 |           6.1885 |
[32m[20221213 23:43:44 @agent_ppo2.py:185][0m |          -0.0027 |          52.9464 |           6.1807 |
[32m[20221213 23:43:44 @agent_ppo2.py:185][0m |          -0.0091 |          52.4029 |           6.0573 |
[32m[20221213 23:43:44 @agent_ppo2.py:185][0m |          -0.0101 |          51.4519 |           6.0508 |
[32m[20221213 23:43:44 @agent_ppo2.py:185][0m |          -0.0102 |          50.7102 |           6.0488 |
[32m[20221213 23:43:44 @agent_ppo2.py:185][0m |          -0.0084 |          50.5275 |           6.0367 |
[32m[20221213 23:43:44 @agent_ppo2.py:185][0m |          -0.0119 |          50.3882 |           5.9325 |
[32m[20221213 23:43:45 @agent_ppo2.py:185][0m |          -0.0049 |          50.0068 |           5.8907 |
[32m[20221213 23:43:45 @agent_ppo2.py:185][0m |          -0.0122 |          49.7078 |           5.9028 |
[32m[20221213 23:43:45 @agent_ppo2.py:185][0m |          -0.0045 |          54.4802 |           5.8716 |
[32m[20221213 23:43:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:43:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.91
[32m[20221213 23:43:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.99
[32m[20221213 23:43:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.22
[32m[20221213 23:43:45 @agent_ppo2.py:143][0m Total time:      31.21 min
[32m[20221213 23:43:45 @agent_ppo2.py:145][0m 3026944 total steps have happened
[32m[20221213 23:43:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3478 --------------------------#
[32m[20221213 23:43:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:45 @agent_ppo2.py:185][0m |           0.0021 |          48.9672 |           5.4256 |
[32m[20221213 23:43:45 @agent_ppo2.py:185][0m |          -0.0043 |          42.6040 |           5.3985 |
[32m[20221213 23:43:45 @agent_ppo2.py:185][0m |          -0.0115 |          39.7857 |           5.3887 |
[32m[20221213 23:43:45 @agent_ppo2.py:185][0m |          -0.0130 |          38.1599 |           5.3667 |
[32m[20221213 23:43:46 @agent_ppo2.py:185][0m |          -0.0144 |          36.9126 |           5.3030 |
[32m[20221213 23:43:46 @agent_ppo2.py:185][0m |          -0.0130 |          36.1229 |           5.3852 |
[32m[20221213 23:43:46 @agent_ppo2.py:185][0m |          -0.0037 |          39.9675 |           5.3461 |
[32m[20221213 23:43:46 @agent_ppo2.py:185][0m |          -0.0187 |          34.6640 |           5.3854 |
[32m[20221213 23:43:46 @agent_ppo2.py:185][0m |          -0.0190 |          34.1531 |           5.4819 |
[32m[20221213 23:43:46 @agent_ppo2.py:185][0m |          -0.0189 |          33.9746 |           5.3946 |
[32m[20221213 23:43:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.18
[32m[20221213 23:43:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.26
[32m[20221213 23:43:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.98
[32m[20221213 23:43:46 @agent_ppo2.py:143][0m Total time:      31.23 min
[32m[20221213 23:43:46 @agent_ppo2.py:145][0m 3028992 total steps have happened
[32m[20221213 23:43:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3479 --------------------------#
[32m[20221213 23:43:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:46 @agent_ppo2.py:185][0m |           0.0010 |          59.9274 |           5.9266 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0003 |          56.3650 |           5.9305 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |           0.0022 |          54.7568 |           5.8463 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0054 |          53.6776 |           5.8413 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0048 |          53.0692 |           5.8198 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0014 |          52.6095 |           5.9335 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0103 |          52.1631 |           5.7843 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0064 |          51.7938 |           5.8016 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0083 |          51.6813 |           5.8311 |
[32m[20221213 23:43:47 @agent_ppo2.py:185][0m |          -0.0011 |          52.9807 |           5.8279 |
[32m[20221213 23:43:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.15
[32m[20221213 23:43:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.15
[32m[20221213 23:43:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.99
[32m[20221213 23:43:47 @agent_ppo2.py:143][0m Total time:      31.26 min
[32m[20221213 23:43:47 @agent_ppo2.py:145][0m 3031040 total steps have happened
[32m[20221213 23:43:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3480 --------------------------#
[32m[20221213 23:43:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:43:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |           0.0102 |          65.0505 |           5.4592 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |           0.0001 |          58.8135 |           5.3545 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0058 |          56.7132 |           5.3045 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0110 |          55.5321 |           5.3233 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0028 |          55.1944 |           5.2530 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0142 |          53.9787 |           5.3044 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0001 |          59.9127 |           5.1885 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0126 |          52.9261 |           5.2247 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0097 |          52.6810 |           5.1777 |
[32m[20221213 23:43:48 @agent_ppo2.py:185][0m |          -0.0142 |          52.3536 |           5.1193 |
[32m[20221213 23:43:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.97
[32m[20221213 23:43:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.51
[32m[20221213 23:43:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.78
[32m[20221213 23:43:49 @agent_ppo2.py:143][0m Total time:      31.28 min
[32m[20221213 23:43:49 @agent_ppo2.py:145][0m 3033088 total steps have happened
[32m[20221213 23:43:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3481 --------------------------#
[32m[20221213 23:43:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:49 @agent_ppo2.py:185][0m |           0.0012 |          60.6626 |           4.8011 |
[32m[20221213 23:43:49 @agent_ppo2.py:185][0m |          -0.0027 |          58.6390 |           4.9007 |
[32m[20221213 23:43:49 @agent_ppo2.py:185][0m |          -0.0044 |          57.9363 |           4.8796 |
[32m[20221213 23:43:49 @agent_ppo2.py:185][0m |          -0.0015 |          57.8990 |           4.9046 |
[32m[20221213 23:43:49 @agent_ppo2.py:185][0m |          -0.0045 |          57.6420 |           4.9587 |
[32m[20221213 23:43:49 @agent_ppo2.py:185][0m |          -0.0063 |          57.2903 |           4.9659 |
[32m[20221213 23:43:49 @agent_ppo2.py:185][0m |          -0.0082 |          57.0450 |           5.0285 |
[32m[20221213 23:43:50 @agent_ppo2.py:185][0m |          -0.0059 |          57.0565 |           5.0086 |
[32m[20221213 23:43:50 @agent_ppo2.py:185][0m |          -0.0067 |          57.1036 |           5.0784 |
[32m[20221213 23:43:50 @agent_ppo2.py:185][0m |          -0.0066 |          57.0673 |           5.0011 |
[32m[20221213 23:43:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:43:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.64
[32m[20221213 23:43:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.79
[32m[20221213 23:43:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.13
[32m[20221213 23:43:50 @agent_ppo2.py:143][0m Total time:      31.30 min
[32m[20221213 23:43:50 @agent_ppo2.py:145][0m 3035136 total steps have happened
[32m[20221213 23:43:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3482 --------------------------#
[32m[20221213 23:43:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:50 @agent_ppo2.py:185][0m |           0.0027 |          61.2781 |           5.0902 |
[32m[20221213 23:43:50 @agent_ppo2.py:185][0m |          -0.0044 |          59.6423 |           5.1788 |
[32m[20221213 23:43:50 @agent_ppo2.py:185][0m |          -0.0059 |          59.8484 |           5.0948 |
[32m[20221213 23:43:50 @agent_ppo2.py:185][0m |          -0.0092 |          58.8005 |           5.1312 |
[32m[20221213 23:43:51 @agent_ppo2.py:185][0m |          -0.0100 |          58.6941 |           5.1326 |
[32m[20221213 23:43:51 @agent_ppo2.py:185][0m |          -0.0028 |          64.3058 |           5.0973 |
[32m[20221213 23:43:51 @agent_ppo2.py:185][0m |          -0.0144 |          58.4057 |           5.1615 |
[32m[20221213 23:43:51 @agent_ppo2.py:185][0m |          -0.0143 |          58.2993 |           5.1694 |
[32m[20221213 23:43:51 @agent_ppo2.py:185][0m |          -0.0027 |          62.7543 |           5.1811 |
[32m[20221213 23:43:51 @agent_ppo2.py:185][0m |          -0.0119 |          58.2766 |           5.2370 |
[32m[20221213 23:43:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:43:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.26
[32m[20221213 23:43:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.13
[32m[20221213 23:43:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.38
[32m[20221213 23:43:51 @agent_ppo2.py:143][0m Total time:      31.32 min
[32m[20221213 23:43:51 @agent_ppo2.py:145][0m 3037184 total steps have happened
[32m[20221213 23:43:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3483 --------------------------#
[32m[20221213 23:43:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:51 @agent_ppo2.py:185][0m |           0.0001 |          35.3647 |           5.3603 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0035 |          27.5635 |           5.3451 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0079 |          25.9591 |           5.3472 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0108 |          24.5705 |           5.3193 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0137 |          23.9556 |           5.3257 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0124 |          23.1384 |           5.3568 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0147 |          22.8727 |           5.3073 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0119 |          22.3625 |           5.3310 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0195 |          21.8662 |           5.3197 |
[32m[20221213 23:43:52 @agent_ppo2.py:185][0m |          -0.0164 |          21.6287 |           5.3194 |
[32m[20221213 23:43:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.34
[32m[20221213 23:43:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.91
[32m[20221213 23:43:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.28
[32m[20221213 23:43:52 @agent_ppo2.py:143][0m Total time:      31.34 min
[32m[20221213 23:43:52 @agent_ppo2.py:145][0m 3039232 total steps have happened
[32m[20221213 23:43:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3484 --------------------------#
[32m[20221213 23:43:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0013 |          58.2381 |           4.7393 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0031 |          51.0966 |           4.7681 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0098 |          49.0510 |           4.7642 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0056 |          47.8439 |           4.8690 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |           0.0055 |          55.5477 |           4.8092 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0120 |          45.9277 |           4.8339 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0185 |          45.2164 |           4.8185 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0115 |          44.6104 |           4.8619 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0127 |          44.1922 |           4.8602 |
[32m[20221213 23:43:53 @agent_ppo2.py:185][0m |          -0.0086 |          44.1377 |           4.8543 |
[32m[20221213 23:43:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.52
[32m[20221213 23:43:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.64
[32m[20221213 23:43:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.41
[32m[20221213 23:43:54 @agent_ppo2.py:143][0m Total time:      31.36 min
[32m[20221213 23:43:54 @agent_ppo2.py:145][0m 3041280 total steps have happened
[32m[20221213 23:43:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3485 --------------------------#
[32m[20221213 23:43:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:54 @agent_ppo2.py:185][0m |          -0.0003 |          49.6161 |           4.8701 |
[32m[20221213 23:43:54 @agent_ppo2.py:185][0m |          -0.0048 |          45.5425 |           4.9502 |
[32m[20221213 23:43:54 @agent_ppo2.py:185][0m |           0.0010 |          46.2787 |           4.8799 |
[32m[20221213 23:43:54 @agent_ppo2.py:185][0m |          -0.0126 |          43.0992 |           4.9101 |
[32m[20221213 23:43:54 @agent_ppo2.py:185][0m |          -0.0121 |          42.1733 |           4.8754 |
[32m[20221213 23:43:54 @agent_ppo2.py:185][0m |          -0.0074 |          41.5833 |           4.9071 |
[32m[20221213 23:43:54 @agent_ppo2.py:185][0m |          -0.0121 |          40.9022 |           4.9116 |
[32m[20221213 23:43:55 @agent_ppo2.py:185][0m |          -0.0080 |          40.6449 |           4.9212 |
[32m[20221213 23:43:55 @agent_ppo2.py:185][0m |          -0.0049 |          42.8347 |           4.9742 |
[32m[20221213 23:43:55 @agent_ppo2.py:185][0m |          -0.0131 |          39.7075 |           4.9332 |
[32m[20221213 23:43:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:43:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.69
[32m[20221213 23:43:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.85
[32m[20221213 23:43:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.56
[32m[20221213 23:43:55 @agent_ppo2.py:143][0m Total time:      31.38 min
[32m[20221213 23:43:55 @agent_ppo2.py:145][0m 3043328 total steps have happened
[32m[20221213 23:43:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3486 --------------------------#
[32m[20221213 23:43:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:55 @agent_ppo2.py:185][0m |          -0.0011 |          67.3310 |           4.5828 |
[32m[20221213 23:43:55 @agent_ppo2.py:185][0m |          -0.0106 |          64.7090 |           4.6016 |
[32m[20221213 23:43:55 @agent_ppo2.py:185][0m |          -0.0065 |          63.6975 |           4.5316 |
[32m[20221213 23:43:55 @agent_ppo2.py:185][0m |          -0.0099 |          63.0602 |           4.5189 |
[32m[20221213 23:43:56 @agent_ppo2.py:185][0m |          -0.0029 |          64.4203 |           4.5312 |
[32m[20221213 23:43:56 @agent_ppo2.py:185][0m |          -0.0111 |          62.3056 |           4.5552 |
[32m[20221213 23:43:56 @agent_ppo2.py:185][0m |          -0.0136 |          61.8835 |           4.4984 |
[32m[20221213 23:43:56 @agent_ppo2.py:185][0m |          -0.0124 |          61.4846 |           4.5459 |
[32m[20221213 23:43:56 @agent_ppo2.py:185][0m |          -0.0144 |          61.1946 |           4.5652 |
[32m[20221213 23:43:56 @agent_ppo2.py:185][0m |          -0.0095 |          61.7635 |           4.5402 |
[32m[20221213 23:43:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:43:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.18
[32m[20221213 23:43:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.07
[32m[20221213 23:43:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.86
[32m[20221213 23:43:56 @agent_ppo2.py:143][0m Total time:      31.40 min
[32m[20221213 23:43:56 @agent_ppo2.py:145][0m 3045376 total steps have happened
[32m[20221213 23:43:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3487 --------------------------#
[32m[20221213 23:43:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:56 @agent_ppo2.py:185][0m |          -0.0008 |          59.4490 |           5.1301 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0067 |          56.7421 |           5.2036 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0097 |          55.4300 |           4.9833 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0082 |          55.1700 |           4.9877 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0064 |          55.0343 |           4.9923 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0115 |          53.6399 |           4.8598 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0090 |          53.3032 |           4.8994 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0122 |          53.1675 |           4.8962 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0132 |          52.8095 |           4.8754 |
[32m[20221213 23:43:57 @agent_ppo2.py:185][0m |          -0.0110 |          52.3879 |           4.8395 |
[32m[20221213 23:43:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:43:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.82
[32m[20221213 23:43:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.72
[32m[20221213 23:43:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.68
[32m[20221213 23:43:57 @agent_ppo2.py:143][0m Total time:      31.42 min
[32m[20221213 23:43:57 @agent_ppo2.py:145][0m 3047424 total steps have happened
[32m[20221213 23:43:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3488 --------------------------#
[32m[20221213 23:43:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:43:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0009 |          71.0451 |           4.3120 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0037 |          67.0500 |           4.3421 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0061 |          65.2309 |           4.2860 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0055 |          63.8219 |           4.3145 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0103 |          62.2503 |           4.2546 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0119 |          61.5724 |           4.3529 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0020 |          65.3739 |           4.3415 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |          -0.0115 |          60.5918 |           4.3479 |
[32m[20221213 23:43:58 @agent_ppo2.py:185][0m |           0.0050 |          65.4062 |           4.3205 |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |          -0.0151 |          59.9037 |           4.3215 |
[32m[20221213 23:43:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:43:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.89
[32m[20221213 23:43:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.34
[32m[20221213 23:43:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.77
[32m[20221213 23:43:59 @agent_ppo2.py:143][0m Total time:      31.44 min
[32m[20221213 23:43:59 @agent_ppo2.py:145][0m 3049472 total steps have happened
[32m[20221213 23:43:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3489 --------------------------#
[32m[20221213 23:43:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:43:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |           0.0048 |          68.2476 |           3.9386 |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |          -0.0052 |          64.8619 |           3.9426 |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |          -0.0092 |          64.3266 |           3.9042 |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |          -0.0097 |          63.8989 |           3.9289 |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |          -0.0096 |          63.6202 |           3.8733 |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |          -0.0124 |          63.5101 |           3.9102 |
[32m[20221213 23:43:59 @agent_ppo2.py:185][0m |          -0.0104 |          63.3238 |           3.9375 |
[32m[20221213 23:44:00 @agent_ppo2.py:185][0m |          -0.0101 |          62.9824 |           3.8722 |
[32m[20221213 23:44:00 @agent_ppo2.py:185][0m |          -0.0136 |          62.9369 |           3.9950 |
[32m[20221213 23:44:00 @agent_ppo2.py:185][0m |          -0.0025 |          64.9685 |           4.0015 |
[32m[20221213 23:44:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:44:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.96
[32m[20221213 23:44:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.89
[32m[20221213 23:44:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.52
[32m[20221213 23:44:00 @agent_ppo2.py:143][0m Total time:      31.46 min
[32m[20221213 23:44:00 @agent_ppo2.py:145][0m 3051520 total steps have happened
[32m[20221213 23:44:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3490 --------------------------#
[32m[20221213 23:44:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:44:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:00 @agent_ppo2.py:185][0m |           0.0021 |          55.9894 |           4.2726 |
[32m[20221213 23:44:00 @agent_ppo2.py:185][0m |          -0.0050 |          52.0307 |           4.3545 |
[32m[20221213 23:44:00 @agent_ppo2.py:185][0m |          -0.0111 |          49.9944 |           4.4655 |
[32m[20221213 23:44:00 @agent_ppo2.py:185][0m |          -0.0111 |          48.8500 |           4.4791 |
[32m[20221213 23:44:01 @agent_ppo2.py:185][0m |          -0.0055 |          49.4662 |           4.5619 |
[32m[20221213 23:44:01 @agent_ppo2.py:185][0m |          -0.0151 |          47.3895 |           4.6205 |
[32m[20221213 23:44:01 @agent_ppo2.py:185][0m |          -0.0098 |          48.2013 |           4.6191 |
[32m[20221213 23:44:01 @agent_ppo2.py:185][0m |          -0.0136 |          46.6195 |           4.6528 |
[32m[20221213 23:44:01 @agent_ppo2.py:185][0m |          -0.0149 |          46.0566 |           4.6829 |
[32m[20221213 23:44:01 @agent_ppo2.py:185][0m |          -0.0166 |          45.7724 |           4.7060 |
[32m[20221213 23:44:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:44:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.12
[32m[20221213 23:44:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.74
[32m[20221213 23:44:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.42
[32m[20221213 23:44:01 @agent_ppo2.py:143][0m Total time:      31.49 min
[32m[20221213 23:44:01 @agent_ppo2.py:145][0m 3053568 total steps have happened
[32m[20221213 23:44:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3491 --------------------------#
[32m[20221213 23:44:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:01 @agent_ppo2.py:185][0m |           0.0036 |          55.5801 |           4.7057 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0014 |          49.7751 |           4.6298 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0062 |          47.4803 |           4.7564 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0054 |          46.0868 |           4.6987 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0104 |          45.3129 |           4.7065 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0130 |          44.6040 |           4.6559 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0160 |          44.2650 |           4.6188 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0098 |          44.1084 |           4.6715 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0086 |          43.9478 |           4.6070 |
[32m[20221213 23:44:02 @agent_ppo2.py:185][0m |          -0.0099 |          43.5473 |           4.5667 |
[32m[20221213 23:44:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.76
[32m[20221213 23:44:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.57
[32m[20221213 23:44:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.92
[32m[20221213 23:44:02 @agent_ppo2.py:143][0m Total time:      31.51 min
[32m[20221213 23:44:02 @agent_ppo2.py:145][0m 3055616 total steps have happened
[32m[20221213 23:44:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3492 --------------------------#
[32m[20221213 23:44:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |           0.0148 |          47.7949 |           4.6036 |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |          -0.0061 |          41.3193 |           4.6873 |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |          -0.0071 |          39.8454 |           4.4981 |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |          -0.0100 |          39.4147 |           4.6275 |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |          -0.0139 |          39.0623 |           4.5765 |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |          -0.0088 |          38.8068 |           4.5643 |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |          -0.0178 |          38.5918 |           4.6231 |
[32m[20221213 23:44:03 @agent_ppo2.py:185][0m |          -0.0187 |          37.9016 |           4.6680 |
[32m[20221213 23:44:04 @agent_ppo2.py:185][0m |          -0.0118 |          38.9007 |           4.5914 |
[32m[20221213 23:44:04 @agent_ppo2.py:185][0m |          -0.0154 |          37.4921 |           4.5473 |
[32m[20221213 23:44:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.65
[32m[20221213 23:44:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.82
[32m[20221213 23:44:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.93
[32m[20221213 23:44:04 @agent_ppo2.py:143][0m Total time:      31.53 min
[32m[20221213 23:44:04 @agent_ppo2.py:145][0m 3057664 total steps have happened
[32m[20221213 23:44:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3493 --------------------------#
[32m[20221213 23:44:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:04 @agent_ppo2.py:185][0m |          -0.0014 |          69.8656 |           4.3734 |
[32m[20221213 23:44:04 @agent_ppo2.py:185][0m |           0.0127 |          73.4379 |           4.3500 |
[32m[20221213 23:44:04 @agent_ppo2.py:185][0m |          -0.0040 |          62.6858 |           4.3882 |
[32m[20221213 23:44:04 @agent_ppo2.py:185][0m |          -0.0032 |          62.4342 |           4.3893 |
[32m[20221213 23:44:04 @agent_ppo2.py:185][0m |          -0.0104 |          60.0780 |           4.3531 |
[32m[20221213 23:44:05 @agent_ppo2.py:185][0m |          -0.0055 |          60.4986 |           4.3846 |
[32m[20221213 23:44:05 @agent_ppo2.py:185][0m |          -0.0131 |          58.3988 |           4.3621 |
[32m[20221213 23:44:05 @agent_ppo2.py:185][0m |          -0.0120 |          58.3854 |           4.3789 |
[32m[20221213 23:44:05 @agent_ppo2.py:185][0m |          -0.0099 |          57.7792 |           4.4431 |
[32m[20221213 23:44:05 @agent_ppo2.py:185][0m |          -0.0115 |          57.3209 |           4.4099 |
[32m[20221213 23:44:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:44:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.84
[32m[20221213 23:44:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.75
[32m[20221213 23:44:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.34
[32m[20221213 23:44:05 @agent_ppo2.py:143][0m Total time:      31.55 min
[32m[20221213 23:44:05 @agent_ppo2.py:145][0m 3059712 total steps have happened
[32m[20221213 23:44:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3494 --------------------------#
[32m[20221213 23:44:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:05 @agent_ppo2.py:185][0m |          -0.0001 |          69.0430 |           4.5076 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0062 |          65.5854 |           4.3925 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0077 |          64.4439 |           4.4058 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0055 |          64.1344 |           4.3765 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0106 |          63.5182 |           4.2626 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0117 |          63.0904 |           4.3175 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0140 |          62.8436 |           4.2901 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0148 |          62.6813 |           4.3182 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0085 |          63.4144 |           4.2169 |
[32m[20221213 23:44:06 @agent_ppo2.py:185][0m |          -0.0001 |          66.6275 |           4.2534 |
[32m[20221213 23:44:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:44:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.26
[32m[20221213 23:44:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.97
[32m[20221213 23:44:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.80
[32m[20221213 23:44:06 @agent_ppo2.py:143][0m Total time:      31.57 min
[32m[20221213 23:44:06 @agent_ppo2.py:145][0m 3061760 total steps have happened
[32m[20221213 23:44:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3495 --------------------------#
[32m[20221213 23:44:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0017 |          62.0349 |           4.5754 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |           0.0154 |          68.6444 |           4.6402 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0024 |          59.7680 |           4.6040 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0078 |          58.5841 |           4.6349 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0078 |          58.4251 |           4.6788 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0091 |          58.1355 |           4.6264 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0017 |          59.4768 |           4.6872 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0005 |          63.6488 |           4.6730 |
[32m[20221213 23:44:07 @agent_ppo2.py:185][0m |          -0.0100 |          57.7166 |           4.7762 |
[32m[20221213 23:44:08 @agent_ppo2.py:185][0m |          -0.0016 |          58.9546 |           4.7643 |
[32m[20221213 23:44:08 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.71
[32m[20221213 23:44:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.53
[32m[20221213 23:44:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.67
[32m[20221213 23:44:08 @agent_ppo2.py:143][0m Total time:      31.59 min
[32m[20221213 23:44:08 @agent_ppo2.py:145][0m 3063808 total steps have happened
[32m[20221213 23:44:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3496 --------------------------#
[32m[20221213 23:44:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:08 @agent_ppo2.py:185][0m |           0.0012 |          59.5851 |           4.4114 |
[32m[20221213 23:44:08 @agent_ppo2.py:185][0m |          -0.0004 |          56.9003 |           4.4717 |
[32m[20221213 23:44:08 @agent_ppo2.py:185][0m |           0.0070 |          61.2058 |           4.3799 |
[32m[20221213 23:44:08 @agent_ppo2.py:185][0m |          -0.0018 |          55.8936 |           4.4903 |
[32m[20221213 23:44:08 @agent_ppo2.py:185][0m |          -0.0051 |          54.7231 |           4.4570 |
[32m[20221213 23:44:08 @agent_ppo2.py:185][0m |          -0.0077 |          54.2258 |           4.3983 |
[32m[20221213 23:44:09 @agent_ppo2.py:185][0m |          -0.0095 |          53.7716 |           4.3676 |
[32m[20221213 23:44:09 @agent_ppo2.py:185][0m |          -0.0101 |          53.6304 |           4.3088 |
[32m[20221213 23:44:09 @agent_ppo2.py:185][0m |          -0.0135 |          53.2352 |           4.4475 |
[32m[20221213 23:44:09 @agent_ppo2.py:185][0m |          -0.0120 |          53.2306 |           4.2538 |
[32m[20221213 23:44:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.60
[32m[20221213 23:44:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.98
[32m[20221213 23:44:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.95
[32m[20221213 23:44:09 @agent_ppo2.py:143][0m Total time:      31.62 min
[32m[20221213 23:44:09 @agent_ppo2.py:145][0m 3065856 total steps have happened
[32m[20221213 23:44:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3497 --------------------------#
[32m[20221213 23:44:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:09 @agent_ppo2.py:185][0m |           0.0004 |          72.2276 |           4.2004 |
[32m[20221213 23:44:09 @agent_ppo2.py:185][0m |          -0.0054 |          69.1713 |           4.3030 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0091 |          68.1220 |           4.4492 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0099 |          67.5734 |           4.3487 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0019 |          71.3570 |           4.3785 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0056 |          68.6450 |           4.4589 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0069 |          67.2518 |           4.4107 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0127 |          66.6356 |           4.4033 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0133 |          66.2878 |           4.4755 |
[32m[20221213 23:44:10 @agent_ppo2.py:185][0m |          -0.0093 |          67.3328 |           4.4383 |
[32m[20221213 23:44:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:44:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.36
[32m[20221213 23:44:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.06
[32m[20221213 23:44:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.84
[32m[20221213 23:44:10 @agent_ppo2.py:143][0m Total time:      31.64 min
[32m[20221213 23:44:10 @agent_ppo2.py:145][0m 3067904 total steps have happened
[32m[20221213 23:44:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3498 --------------------------#
[32m[20221213 23:44:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |           0.0016 |          68.4948 |           3.9950 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0060 |          65.6612 |           4.1221 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0082 |          64.7495 |           4.1397 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0113 |          64.2023 |           4.1214 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0070 |          63.9397 |           4.1780 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0102 |          63.4864 |           4.1704 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |           0.0193 |          79.3024 |           4.2596 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0110 |          63.6279 |           4.2967 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0104 |          62.6461 |           4.3388 |
[32m[20221213 23:44:11 @agent_ppo2.py:185][0m |          -0.0083 |          62.4401 |           4.2818 |
[32m[20221213 23:44:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.36
[32m[20221213 23:44:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.04
[32m[20221213 23:44:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.61
[32m[20221213 23:44:12 @agent_ppo2.py:143][0m Total time:      31.66 min
[32m[20221213 23:44:12 @agent_ppo2.py:145][0m 3069952 total steps have happened
[32m[20221213 23:44:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3499 --------------------------#
[32m[20221213 23:44:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:12 @agent_ppo2.py:185][0m |           0.0009 |          53.0549 |           5.2276 |
[32m[20221213 23:44:12 @agent_ppo2.py:185][0m |          -0.0025 |          47.5231 |           5.3019 |
[32m[20221213 23:44:12 @agent_ppo2.py:185][0m |           0.0039 |          47.9731 |           5.3123 |
[32m[20221213 23:44:12 @agent_ppo2.py:185][0m |          -0.0099 |          44.6582 |           5.4262 |
[32m[20221213 23:44:12 @agent_ppo2.py:185][0m |          -0.0107 |          44.0539 |           5.3472 |
[32m[20221213 23:44:12 @agent_ppo2.py:185][0m |          -0.0049 |          48.9961 |           5.4302 |
[32m[20221213 23:44:12 @agent_ppo2.py:185][0m |          -0.0099 |          43.0512 |           5.4617 |
[32m[20221213 23:44:13 @agent_ppo2.py:185][0m |          -0.0115 |          42.4290 |           5.5678 |
[32m[20221213 23:44:13 @agent_ppo2.py:185][0m |          -0.0182 |          41.9963 |           5.5687 |
[32m[20221213 23:44:13 @agent_ppo2.py:185][0m |          -0.0137 |          41.8716 |           5.5928 |
[32m[20221213 23:44:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.95
[32m[20221213 23:44:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.64
[32m[20221213 23:44:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.23
[32m[20221213 23:44:13 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 573.08
[32m[20221213 23:44:13 @agent_ppo2.py:143][0m Total time:      31.68 min
[32m[20221213 23:44:13 @agent_ppo2.py:145][0m 3072000 total steps have happened
[32m[20221213 23:44:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3500 --------------------------#
[32m[20221213 23:44:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:44:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:13 @agent_ppo2.py:185][0m |          -0.0035 |          83.8960 |           4.6860 |
[32m[20221213 23:44:13 @agent_ppo2.py:185][0m |          -0.0096 |          80.1790 |           4.7712 |
[32m[20221213 23:44:13 @agent_ppo2.py:185][0m |          -0.0083 |          78.6522 |           4.7845 |
[32m[20221213 23:44:14 @agent_ppo2.py:185][0m |          -0.0109 |          78.0327 |           4.7289 |
[32m[20221213 23:44:14 @agent_ppo2.py:185][0m |          -0.0109 |          78.0252 |           4.8322 |
[32m[20221213 23:44:14 @agent_ppo2.py:185][0m |          -0.0109 |          76.8712 |           4.7113 |
[32m[20221213 23:44:14 @agent_ppo2.py:185][0m |          -0.0126 |          76.5732 |           4.7083 |
[32m[20221213 23:44:14 @agent_ppo2.py:185][0m |          -0.0128 |          76.3319 |           4.7358 |
[32m[20221213 23:44:14 @agent_ppo2.py:185][0m |          -0.0150 |          76.0974 |           4.7331 |
[32m[20221213 23:44:14 @agent_ppo2.py:185][0m |          -0.0189 |          75.8276 |           4.7259 |
[32m[20221213 23:44:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.11
[32m[20221213 23:44:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.38
[32m[20221213 23:44:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.14
[32m[20221213 23:44:14 @agent_ppo2.py:143][0m Total time:      31.70 min
[32m[20221213 23:44:14 @agent_ppo2.py:145][0m 3074048 total steps have happened
[32m[20221213 23:44:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3501 --------------------------#
[32m[20221213 23:44:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |           0.0007 |          52.2251 |           5.3416 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0050 |          45.7651 |           5.4348 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0061 |          43.3421 |           5.4974 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |           0.0070 |          47.9386 |           5.5419 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0112 |          40.4799 |           5.4508 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0103 |          39.9635 |           5.5897 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0118 |          39.2402 |           5.5914 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0146 |          38.6512 |           5.5358 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0173 |          38.2388 |           5.5305 |
[32m[20221213 23:44:15 @agent_ppo2.py:185][0m |          -0.0175 |          38.0578 |           5.5724 |
[32m[20221213 23:44:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.43
[32m[20221213 23:44:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.75
[32m[20221213 23:44:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.88
[32m[20221213 23:44:16 @agent_ppo2.py:143][0m Total time:      31.72 min
[32m[20221213 23:44:16 @agent_ppo2.py:145][0m 3076096 total steps have happened
[32m[20221213 23:44:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3502 --------------------------#
[32m[20221213 23:44:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0056 |          58.1452 |           5.4952 |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0052 |          52.9525 |           5.6430 |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0083 |          51.1138 |           5.4932 |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0060 |          49.9487 |           5.5613 |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0141 |          49.2165 |           5.4673 |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0106 |          48.5478 |           5.4907 |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0123 |          48.1101 |           5.5117 |
[32m[20221213 23:44:16 @agent_ppo2.py:185][0m |          -0.0132 |          47.6351 |           5.5085 |
[32m[20221213 23:44:17 @agent_ppo2.py:185][0m |          -0.0196 |          47.3616 |           5.4797 |
[32m[20221213 23:44:17 @agent_ppo2.py:185][0m |          -0.0179 |          47.0037 |           5.4741 |
[32m[20221213 23:44:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:44:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.32
[32m[20221213 23:44:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.68
[32m[20221213 23:44:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.00
[32m[20221213 23:44:17 @agent_ppo2.py:143][0m Total time:      31.75 min
[32m[20221213 23:44:17 @agent_ppo2.py:145][0m 3078144 total steps have happened
[32m[20221213 23:44:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3503 --------------------------#
[32m[20221213 23:44:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:17 @agent_ppo2.py:185][0m |           0.0014 |          56.9155 |           5.2356 |
[32m[20221213 23:44:17 @agent_ppo2.py:185][0m |          -0.0028 |          54.5458 |           5.3342 |
[32m[20221213 23:44:17 @agent_ppo2.py:185][0m |          -0.0037 |          52.5234 |           5.3234 |
[32m[20221213 23:44:17 @agent_ppo2.py:185][0m |          -0.0067 |          51.6722 |           5.3265 |
[32m[20221213 23:44:17 @agent_ppo2.py:185][0m |          -0.0104 |          51.2788 |           5.3638 |
[32m[20221213 23:44:18 @agent_ppo2.py:185][0m |           0.0112 |          60.7455 |           5.3381 |
[32m[20221213 23:44:18 @agent_ppo2.py:185][0m |          -0.0138 |          50.6513 |           5.4159 |
[32m[20221213 23:44:18 @agent_ppo2.py:185][0m |          -0.0155 |          49.9775 |           5.3665 |
[32m[20221213 23:44:18 @agent_ppo2.py:185][0m |          -0.0122 |          49.7800 |           5.3846 |
[32m[20221213 23:44:18 @agent_ppo2.py:185][0m |          -0.0163 |          49.4234 |           5.3620 |
[32m[20221213 23:44:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.57
[32m[20221213 23:44:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.47
[32m[20221213 23:44:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.97
[32m[20221213 23:44:18 @agent_ppo2.py:143][0m Total time:      31.77 min
[32m[20221213 23:44:18 @agent_ppo2.py:145][0m 3080192 total steps have happened
[32m[20221213 23:44:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3504 --------------------------#
[32m[20221213 23:44:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:18 @agent_ppo2.py:185][0m |          -0.0013 |          83.0293 |           5.8934 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0023 |          77.3610 |           5.9094 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |           0.0020 |          79.2882 |           5.9012 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0057 |          74.2616 |           5.8112 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0091 |          72.9949 |           5.8258 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0113 |          72.1948 |           5.7726 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0099 |          71.5539 |           5.8129 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0169 |          71.5976 |           5.8238 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0120 |          70.7176 |           5.8308 |
[32m[20221213 23:44:19 @agent_ppo2.py:185][0m |          -0.0102 |          70.3716 |           5.7786 |
[32m[20221213 23:44:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.11
[32m[20221213 23:44:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.52
[32m[20221213 23:44:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.41
[32m[20221213 23:44:19 @agent_ppo2.py:143][0m Total time:      31.79 min
[32m[20221213 23:44:19 @agent_ppo2.py:145][0m 3082240 total steps have happened
[32m[20221213 23:44:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3505 --------------------------#
[32m[20221213 23:44:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |           0.0003 |          57.6352 |           5.7766 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |          -0.0005 |          55.5220 |           5.6671 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |          -0.0053 |          54.8184 |           5.5947 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |          -0.0075 |          54.5264 |           5.6325 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |          -0.0084 |          54.1441 |           5.6636 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |           0.0089 |          60.3874 |           5.5736 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |          -0.0055 |          53.9527 |           5.6665 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |          -0.0007 |          55.0569 |           5.5540 |
[32m[20221213 23:44:20 @agent_ppo2.py:185][0m |          -0.0082 |          53.6660 |           5.5752 |
[32m[20221213 23:44:21 @agent_ppo2.py:185][0m |          -0.0051 |          54.4070 |           5.5625 |
[32m[20221213 23:44:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:44:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.16
[32m[20221213 23:44:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.96
[32m[20221213 23:44:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.49
[32m[20221213 23:44:21 @agent_ppo2.py:143][0m Total time:      31.81 min
[32m[20221213 23:44:21 @agent_ppo2.py:145][0m 3084288 total steps have happened
[32m[20221213 23:44:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3506 --------------------------#
[32m[20221213 23:44:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:21 @agent_ppo2.py:185][0m |          -0.0003 |          65.7836 |           5.0617 |
[32m[20221213 23:44:21 @agent_ppo2.py:185][0m |           0.0008 |          54.6165 |           5.0982 |
[32m[20221213 23:44:21 @agent_ppo2.py:185][0m |          -0.0047 |          50.8621 |           5.1371 |
[32m[20221213 23:44:21 @agent_ppo2.py:185][0m |          -0.0095 |          48.9569 |           5.1287 |
[32m[20221213 23:44:21 @agent_ppo2.py:185][0m |          -0.0079 |          46.8071 |           5.0410 |
[32m[20221213 23:44:21 @agent_ppo2.py:185][0m |          -0.0016 |          46.3020 |           5.1304 |
[32m[20221213 23:44:22 @agent_ppo2.py:185][0m |          -0.0113 |          44.7409 |           5.2505 |
[32m[20221213 23:44:22 @agent_ppo2.py:185][0m |          -0.0085 |          44.0414 |           5.2543 |
[32m[20221213 23:44:22 @agent_ppo2.py:185][0m |          -0.0128 |          42.7926 |           5.2554 |
[32m[20221213 23:44:22 @agent_ppo2.py:185][0m |          -0.0044 |          44.3747 |           5.2550 |
[32m[20221213 23:44:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.24
[32m[20221213 23:44:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.86
[32m[20221213 23:44:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.67
[32m[20221213 23:44:22 @agent_ppo2.py:143][0m Total time:      31.83 min
[32m[20221213 23:44:22 @agent_ppo2.py:145][0m 3086336 total steps have happened
[32m[20221213 23:44:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3507 --------------------------#
[32m[20221213 23:44:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:22 @agent_ppo2.py:185][0m |           0.0007 |          81.4219 |           4.9905 |
[32m[20221213 23:44:22 @agent_ppo2.py:185][0m |          -0.0031 |          70.5262 |           4.9251 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0087 |          67.0897 |           4.9106 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0018 |          65.1543 |           4.9175 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0108 |          64.1227 |           4.8970 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0093 |          63.3561 |           4.8573 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0092 |          62.3208 |           4.8553 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0020 |          62.1943 |           4.8268 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0117 |          61.1976 |           4.7865 |
[32m[20221213 23:44:23 @agent_ppo2.py:185][0m |          -0.0104 |          60.9626 |           4.7871 |
[32m[20221213 23:44:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:44:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.27
[32m[20221213 23:44:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.25
[32m[20221213 23:44:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.38
[32m[20221213 23:44:23 @agent_ppo2.py:143][0m Total time:      31.85 min
[32m[20221213 23:44:23 @agent_ppo2.py:145][0m 3088384 total steps have happened
[32m[20221213 23:44:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3508 --------------------------#
[32m[20221213 23:44:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0024 |          56.6611 |           4.3398 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0046 |          55.5308 |           4.3216 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0052 |          55.2707 |           4.3467 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0085 |          55.1023 |           4.3899 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0082 |          54.8440 |           4.3463 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0096 |          54.6660 |           4.3501 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0090 |          54.5424 |           4.3883 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0090 |          54.7628 |           4.3949 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0118 |          54.4340 |           4.4556 |
[32m[20221213 23:44:24 @agent_ppo2.py:185][0m |          -0.0124 |          54.3589 |           4.4333 |
[32m[20221213 23:44:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.91
[32m[20221213 23:44:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.68
[32m[20221213 23:44:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.66
[32m[20221213 23:44:25 @agent_ppo2.py:143][0m Total time:      31.88 min
[32m[20221213 23:44:25 @agent_ppo2.py:145][0m 3090432 total steps have happened
[32m[20221213 23:44:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3509 --------------------------#
[32m[20221213 23:44:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:25 @agent_ppo2.py:185][0m |           0.0022 |          56.7382 |           5.1341 |
[32m[20221213 23:44:25 @agent_ppo2.py:185][0m |          -0.0035 |          55.8268 |           5.2633 |
[32m[20221213 23:44:25 @agent_ppo2.py:185][0m |           0.0009 |          56.5732 |           5.2039 |
[32m[20221213 23:44:25 @agent_ppo2.py:185][0m |          -0.0053 |          55.5099 |           5.1967 |
[32m[20221213 23:44:25 @agent_ppo2.py:185][0m |          -0.0060 |          55.2724 |           5.3126 |
[32m[20221213 23:44:25 @agent_ppo2.py:185][0m |           0.0024 |          59.0730 |           5.1730 |
[32m[20221213 23:44:25 @agent_ppo2.py:185][0m |           0.0015 |          55.6993 |           5.2916 |
[32m[20221213 23:44:26 @agent_ppo2.py:185][0m |          -0.0020 |          55.5794 |           5.1457 |
[32m[20221213 23:44:26 @agent_ppo2.py:185][0m |          -0.0100 |          54.8541 |           5.1575 |
[32m[20221213 23:44:26 @agent_ppo2.py:185][0m |          -0.0049 |          54.8039 |           5.1446 |
[32m[20221213 23:44:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.72
[32m[20221213 23:44:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.48
[32m[20221213 23:44:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.60
[32m[20221213 23:44:26 @agent_ppo2.py:143][0m Total time:      31.90 min
[32m[20221213 23:44:26 @agent_ppo2.py:145][0m 3092480 total steps have happened
[32m[20221213 23:44:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3510 --------------------------#
[32m[20221213 23:44:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:44:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:26 @agent_ppo2.py:185][0m |           0.0008 |          66.1471 |           4.7254 |
[32m[20221213 23:44:26 @agent_ppo2.py:185][0m |          -0.0008 |          65.5619 |           4.7726 |
[32m[20221213 23:44:26 @agent_ppo2.py:185][0m |          -0.0043 |          63.7821 |           4.7015 |
[32m[20221213 23:44:27 @agent_ppo2.py:185][0m |          -0.0075 |          63.3979 |           4.7808 |
[32m[20221213 23:44:27 @agent_ppo2.py:185][0m |          -0.0086 |          63.1711 |           4.7378 |
[32m[20221213 23:44:27 @agent_ppo2.py:185][0m |          -0.0001 |          68.6669 |           4.7593 |
[32m[20221213 23:44:27 @agent_ppo2.py:185][0m |          -0.0098 |          62.6328 |           4.7870 |
[32m[20221213 23:44:27 @agent_ppo2.py:185][0m |          -0.0114 |          62.4568 |           4.7382 |
[32m[20221213 23:44:27 @agent_ppo2.py:185][0m |          -0.0103 |          62.1765 |           4.8021 |
[32m[20221213 23:44:27 @agent_ppo2.py:185][0m |          -0.0043 |          62.8970 |           4.7807 |
[32m[20221213 23:44:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:44:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.61
[32m[20221213 23:44:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.32
[32m[20221213 23:44:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.14
[32m[20221213 23:44:27 @agent_ppo2.py:143][0m Total time:      31.92 min
[32m[20221213 23:44:27 @agent_ppo2.py:145][0m 3094528 total steps have happened
[32m[20221213 23:44:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3511 --------------------------#
[32m[20221213 23:44:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |           0.0074 |          64.5907 |           5.6482 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0065 |          59.4734 |           5.7270 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0057 |          57.8693 |           5.7326 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0132 |          56.9143 |           5.7598 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0139 |          56.3056 |           5.7806 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0148 |          55.6261 |           5.7841 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0100 |          55.3340 |           5.7874 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0134 |          54.8662 |           5.8356 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0145 |          54.9428 |           5.8121 |
[32m[20221213 23:44:28 @agent_ppo2.py:185][0m |          -0.0110 |          54.6833 |           5.8442 |
[32m[20221213 23:44:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:44:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.74
[32m[20221213 23:44:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.57
[32m[20221213 23:44:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.97
[32m[20221213 23:44:29 @agent_ppo2.py:143][0m Total time:      31.94 min
[32m[20221213 23:44:29 @agent_ppo2.py:145][0m 3096576 total steps have happened
[32m[20221213 23:44:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3512 --------------------------#
[32m[20221213 23:44:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0030 |          87.6828 |           5.3848 |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0098 |          83.7316 |           5.3962 |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0076 |          82.4853 |           5.5041 |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0107 |          81.8592 |           5.5050 |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0092 |          80.9016 |           5.5825 |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0106 |          80.6629 |           5.5853 |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0143 |          80.2169 |           5.6683 |
[32m[20221213 23:44:29 @agent_ppo2.py:185][0m |          -0.0105 |          79.5743 |           5.6726 |
[32m[20221213 23:44:30 @agent_ppo2.py:185][0m |          -0.0115 |          79.2613 |           5.7158 |
[32m[20221213 23:44:30 @agent_ppo2.py:185][0m |          -0.0156 |          79.1121 |           5.7683 |
[32m[20221213 23:44:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.55
[32m[20221213 23:44:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.57
[32m[20221213 23:44:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.85
[32m[20221213 23:44:30 @agent_ppo2.py:143][0m Total time:      31.96 min
[32m[20221213 23:44:30 @agent_ppo2.py:145][0m 3098624 total steps have happened
[32m[20221213 23:44:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3513 --------------------------#
[32m[20221213 23:44:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:30 @agent_ppo2.py:185][0m |           0.0008 |          57.7683 |           5.5623 |
[32m[20221213 23:44:30 @agent_ppo2.py:185][0m |           0.0030 |          57.7326 |           5.5462 |
[32m[20221213 23:44:30 @agent_ppo2.py:185][0m |          -0.0065 |          56.5928 |           5.6412 |
[32m[20221213 23:44:30 @agent_ppo2.py:185][0m |          -0.0067 |          56.2912 |           5.6687 |
[32m[20221213 23:44:30 @agent_ppo2.py:185][0m |          -0.0024 |          57.8485 |           5.7210 |
[32m[20221213 23:44:31 @agent_ppo2.py:185][0m |          -0.0080 |          55.8897 |           5.7179 |
[32m[20221213 23:44:31 @agent_ppo2.py:185][0m |          -0.0112 |          55.9406 |           5.6754 |
[32m[20221213 23:44:31 @agent_ppo2.py:185][0m |          -0.0041 |          56.3165 |           5.7169 |
[32m[20221213 23:44:31 @agent_ppo2.py:185][0m |          -0.0106 |          55.5628 |           5.7103 |
[32m[20221213 23:44:31 @agent_ppo2.py:185][0m |          -0.0028 |          59.4204 |           5.7177 |
[32m[20221213 23:44:31 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:44:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.36
[32m[20221213 23:44:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.08
[32m[20221213 23:44:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.87
[32m[20221213 23:44:31 @agent_ppo2.py:143][0m Total time:      31.98 min
[32m[20221213 23:44:31 @agent_ppo2.py:145][0m 3100672 total steps have happened
[32m[20221213 23:44:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3514 --------------------------#
[32m[20221213 23:44:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:31 @agent_ppo2.py:185][0m |          -0.0060 |          50.5124 |           5.1231 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0050 |          45.3484 |           5.1319 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0091 |          43.9812 |           5.2410 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0103 |          43.0592 |           5.2702 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0107 |          42.4877 |           5.3406 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0045 |          42.1866 |           5.3911 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0146 |          41.8309 |           5.3359 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0120 |          41.4258 |           5.4533 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0162 |          41.1512 |           5.5292 |
[32m[20221213 23:44:32 @agent_ppo2.py:185][0m |          -0.0114 |          43.1330 |           5.5528 |
[32m[20221213 23:44:32 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.82
[32m[20221213 23:44:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.62
[32m[20221213 23:44:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.05
[32m[20221213 23:44:32 @agent_ppo2.py:143][0m Total time:      32.01 min
[32m[20221213 23:44:32 @agent_ppo2.py:145][0m 3102720 total steps have happened
[32m[20221213 23:44:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3515 --------------------------#
[32m[20221213 23:44:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0022 |          62.5516 |           5.6466 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0061 |          56.5616 |           5.7450 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0067 |          54.8637 |           5.6975 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0085 |          53.9040 |           5.7701 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0071 |          53.1592 |           5.8526 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0072 |          52.8718 |           5.8069 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0134 |          52.3704 |           5.8138 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0172 |          51.9344 |           5.7920 |
[32m[20221213 23:44:33 @agent_ppo2.py:185][0m |          -0.0140 |          51.6366 |           5.8611 |
[32m[20221213 23:44:34 @agent_ppo2.py:185][0m |          -0.0135 |          51.2598 |           5.8483 |
[32m[20221213 23:44:34 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.86
[32m[20221213 23:44:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.35
[32m[20221213 23:44:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.01
[32m[20221213 23:44:34 @agent_ppo2.py:143][0m Total time:      32.03 min
[32m[20221213 23:44:34 @agent_ppo2.py:145][0m 3104768 total steps have happened
[32m[20221213 23:44:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3516 --------------------------#
[32m[20221213 23:44:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:34 @agent_ppo2.py:185][0m |          -0.0027 |          71.4626 |           6.7995 |
[32m[20221213 23:44:34 @agent_ppo2.py:185][0m |          -0.0083 |          64.9709 |           6.8751 |
[32m[20221213 23:44:34 @agent_ppo2.py:185][0m |          -0.0025 |          64.1517 |           6.9177 |
[32m[20221213 23:44:34 @agent_ppo2.py:185][0m |          -0.0087 |          61.0410 |           6.9106 |
[32m[20221213 23:44:34 @agent_ppo2.py:185][0m |          -0.0110 |          59.9932 |           6.8056 |
[32m[20221213 23:44:34 @agent_ppo2.py:185][0m |          -0.0107 |          59.0858 |           6.8770 |
[32m[20221213 23:44:35 @agent_ppo2.py:185][0m |          -0.0142 |          58.5929 |           6.7594 |
[32m[20221213 23:44:35 @agent_ppo2.py:185][0m |          -0.0112 |          58.0187 |           6.8175 |
[32m[20221213 23:44:35 @agent_ppo2.py:185][0m |          -0.0086 |          58.9656 |           6.8156 |
[32m[20221213 23:44:35 @agent_ppo2.py:185][0m |           0.0017 |          66.4940 |           6.8255 |
[32m[20221213 23:44:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:44:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.29
[32m[20221213 23:44:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.05
[32m[20221213 23:44:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.87
[32m[20221213 23:44:35 @agent_ppo2.py:143][0m Total time:      32.05 min
[32m[20221213 23:44:35 @agent_ppo2.py:145][0m 3106816 total steps have happened
[32m[20221213 23:44:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3517 --------------------------#
[32m[20221213 23:44:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:35 @agent_ppo2.py:185][0m |           0.0050 |          57.7315 |           6.3810 |
[32m[20221213 23:44:35 @agent_ppo2.py:185][0m |          -0.0031 |          51.8135 |           6.4637 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |           0.0088 |          55.8788 |           6.4285 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |          -0.0027 |          47.9055 |           6.3796 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |          -0.0062 |          47.1869 |           6.4034 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |          -0.0074 |          46.5423 |           6.4011 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |          -0.0081 |          45.8651 |           6.4539 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |          -0.0077 |          45.3745 |           6.4427 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |          -0.0063 |          45.0283 |           6.4204 |
[32m[20221213 23:44:36 @agent_ppo2.py:185][0m |          -0.0082 |          44.7618 |           6.3541 |
[32m[20221213 23:44:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:44:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.85
[32m[20221213 23:44:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.07
[32m[20221213 23:44:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.96
[32m[20221213 23:44:36 @agent_ppo2.py:143][0m Total time:      32.07 min
[32m[20221213 23:44:36 @agent_ppo2.py:145][0m 3108864 total steps have happened
[32m[20221213 23:44:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3518 --------------------------#
[32m[20221213 23:44:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |           0.0035 |          66.5054 |           6.0949 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0027 |          61.6831 |           6.0304 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |           0.0026 |          65.6463 |           6.0624 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0049 |          59.3619 |           6.0552 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0056 |          59.0370 |           6.0701 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0016 |          60.7338 |           6.0585 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0085 |          58.0661 |           6.0731 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0095 |          57.8723 |           6.0798 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0103 |          57.6842 |           6.0914 |
[32m[20221213 23:44:37 @agent_ppo2.py:185][0m |          -0.0087 |          57.6469 |           6.0631 |
[32m[20221213 23:44:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.22
[32m[20221213 23:44:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.28
[32m[20221213 23:44:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.31
[32m[20221213 23:44:38 @agent_ppo2.py:143][0m Total time:      32.09 min
[32m[20221213 23:44:38 @agent_ppo2.py:145][0m 3110912 total steps have happened
[32m[20221213 23:44:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3519 --------------------------#
[32m[20221213 23:44:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:38 @agent_ppo2.py:185][0m |          -0.0015 |          65.2123 |           5.9644 |
[32m[20221213 23:44:38 @agent_ppo2.py:185][0m |          -0.0065 |          63.2268 |           5.9175 |
[32m[20221213 23:44:38 @agent_ppo2.py:185][0m |          -0.0103 |          62.4325 |           5.9296 |
[32m[20221213 23:44:38 @agent_ppo2.py:185][0m |          -0.0092 |          62.0125 |           5.9712 |
[32m[20221213 23:44:38 @agent_ppo2.py:185][0m |          -0.0122 |          61.7718 |           5.9937 |
[32m[20221213 23:44:38 @agent_ppo2.py:185][0m |          -0.0134 |          61.6318 |           6.0893 |
[32m[20221213 23:44:38 @agent_ppo2.py:185][0m |          -0.0124 |          61.2753 |           6.0864 |
[32m[20221213 23:44:39 @agent_ppo2.py:185][0m |          -0.0028 |          66.6861 |           6.0921 |
[32m[20221213 23:44:39 @agent_ppo2.py:185][0m |          -0.0148 |          61.1433 |           6.2507 |
[32m[20221213 23:44:39 @agent_ppo2.py:185][0m |          -0.0148 |          60.7930 |           6.1705 |
[32m[20221213 23:44:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.19
[32m[20221213 23:44:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.67
[32m[20221213 23:44:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.02
[32m[20221213 23:44:39 @agent_ppo2.py:143][0m Total time:      32.11 min
[32m[20221213 23:44:39 @agent_ppo2.py:145][0m 3112960 total steps have happened
[32m[20221213 23:44:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3520 --------------------------#
[32m[20221213 23:44:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:44:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:39 @agent_ppo2.py:185][0m |           0.0026 |          50.8680 |           5.9265 |
[32m[20221213 23:44:39 @agent_ppo2.py:185][0m |          -0.0048 |          46.2456 |           5.8638 |
[32m[20221213 23:44:39 @agent_ppo2.py:185][0m |          -0.0042 |          44.7092 |           5.8718 |
[32m[20221213 23:44:40 @agent_ppo2.py:185][0m |          -0.0050 |          43.9899 |           5.8933 |
[32m[20221213 23:44:40 @agent_ppo2.py:185][0m |          -0.0068 |          43.5950 |           5.8996 |
[32m[20221213 23:44:40 @agent_ppo2.py:185][0m |          -0.0033 |          43.2163 |           5.9016 |
[32m[20221213 23:44:40 @agent_ppo2.py:185][0m |          -0.0060 |          43.6843 |           5.8654 |
[32m[20221213 23:44:40 @agent_ppo2.py:185][0m |          -0.0091 |          42.7271 |           5.8402 |
[32m[20221213 23:44:40 @agent_ppo2.py:185][0m |          -0.0119 |          42.3307 |           5.9177 |
[32m[20221213 23:44:40 @agent_ppo2.py:185][0m |          -0.0095 |          42.2322 |           5.8613 |
[32m[20221213 23:44:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:44:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.08
[32m[20221213 23:44:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.09
[32m[20221213 23:44:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.26
[32m[20221213 23:44:40 @agent_ppo2.py:143][0m Total time:      32.14 min
[32m[20221213 23:44:40 @agent_ppo2.py:145][0m 3115008 total steps have happened
[32m[20221213 23:44:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3521 --------------------------#
[32m[20221213 23:44:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0019 |          72.3121 |           5.8727 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0045 |          67.3958 |           5.8943 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0066 |          66.2241 |           5.9208 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0094 |          65.3960 |           5.9915 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0033 |          68.5255 |           5.8883 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0081 |          64.3847 |           5.9517 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0027 |          68.5688 |           5.9614 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |           0.0001 |          70.5671 |           5.9350 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0097 |          63.4107 |           5.9671 |
[32m[20221213 23:44:41 @agent_ppo2.py:185][0m |          -0.0124 |          62.9841 |           5.9557 |
[32m[20221213 23:44:41 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.20
[32m[20221213 23:44:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.87
[32m[20221213 23:44:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.26
[32m[20221213 23:44:41 @agent_ppo2.py:143][0m Total time:      32.16 min
[32m[20221213 23:44:41 @agent_ppo2.py:145][0m 3117056 total steps have happened
[32m[20221213 23:44:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3522 --------------------------#
[32m[20221213 23:44:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |           0.0176 |          75.6388 |           6.0043 |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |           0.0012 |          68.1260 |           6.0211 |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |          -0.0059 |          65.4454 |           6.0082 |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |          -0.0057 |          64.9247 |           5.9435 |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |          -0.0063 |          64.5041 |           5.9261 |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |           0.0029 |          66.5437 |           5.9096 |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |          -0.0063 |          64.1957 |           5.8724 |
[32m[20221213 23:44:42 @agent_ppo2.py:185][0m |          -0.0080 |          63.6145 |           5.8713 |
[32m[20221213 23:44:43 @agent_ppo2.py:185][0m |          -0.0086 |          63.3854 |           5.8661 |
[32m[20221213 23:44:43 @agent_ppo2.py:185][0m |          -0.0105 |          63.2949 |           5.8452 |
[32m[20221213 23:44:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:44:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.10
[32m[20221213 23:44:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.40
[32m[20221213 23:44:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.48
[32m[20221213 23:44:43 @agent_ppo2.py:143][0m Total time:      32.18 min
[32m[20221213 23:44:43 @agent_ppo2.py:145][0m 3119104 total steps have happened
[32m[20221213 23:44:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3523 --------------------------#
[32m[20221213 23:44:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:43 @agent_ppo2.py:185][0m |           0.0014 |          71.3461 |           5.7681 |
[32m[20221213 23:44:43 @agent_ppo2.py:185][0m |          -0.0060 |          67.4042 |           5.8137 |
[32m[20221213 23:44:43 @agent_ppo2.py:185][0m |          -0.0057 |          65.8861 |           5.8284 |
[32m[20221213 23:44:43 @agent_ppo2.py:185][0m |          -0.0031 |          64.9289 |           5.7830 |
[32m[20221213 23:44:43 @agent_ppo2.py:185][0m |          -0.0091 |          64.1844 |           5.7850 |
[32m[20221213 23:44:44 @agent_ppo2.py:185][0m |          -0.0111 |          63.8008 |           5.7429 |
[32m[20221213 23:44:44 @agent_ppo2.py:185][0m |          -0.0064 |          63.4615 |           5.7768 |
[32m[20221213 23:44:44 @agent_ppo2.py:185][0m |          -0.0131 |          63.0150 |           5.7448 |
[32m[20221213 23:44:44 @agent_ppo2.py:185][0m |          -0.0127 |          62.6857 |           5.7648 |
[32m[20221213 23:44:44 @agent_ppo2.py:185][0m |          -0.0062 |          62.6406 |           5.7387 |
[32m[20221213 23:44:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.93
[32m[20221213 23:44:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.73
[32m[20221213 23:44:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.17
[32m[20221213 23:44:44 @agent_ppo2.py:143][0m Total time:      32.20 min
[32m[20221213 23:44:44 @agent_ppo2.py:145][0m 3121152 total steps have happened
[32m[20221213 23:44:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3524 --------------------------#
[32m[20221213 23:44:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:44 @agent_ppo2.py:185][0m |          -0.0019 |          56.9072 |           5.9538 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0045 |          54.0895 |           5.9866 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0098 |          53.2537 |           5.9635 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0076 |          52.5048 |           6.0407 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0074 |          52.0529 |           6.0056 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0107 |          52.3377 |           5.9791 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0108 |          51.5920 |           6.0167 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0116 |          52.2023 |           6.0215 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0173 |          50.9773 |           5.9777 |
[32m[20221213 23:44:45 @agent_ppo2.py:185][0m |          -0.0143 |          50.8020 |           6.0307 |
[32m[20221213 23:44:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:44:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.50
[32m[20221213 23:44:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.05
[32m[20221213 23:44:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.58
[32m[20221213 23:44:45 @agent_ppo2.py:143][0m Total time:      32.22 min
[32m[20221213 23:44:45 @agent_ppo2.py:145][0m 3123200 total steps have happened
[32m[20221213 23:44:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3525 --------------------------#
[32m[20221213 23:44:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:44:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |           0.0022 |          61.6311 |           6.5642 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |           0.0098 |          63.4384 |           6.5814 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |          -0.0058 |          56.5171 |           6.6008 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |           0.0004 |          58.0139 |           6.5458 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |          -0.0088 |          54.9479 |           6.5964 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |          -0.0088 |          54.7398 |           6.5644 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |          -0.0079 |          54.5878 |           6.5910 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |          -0.0030 |          58.4244 |           6.5439 |
[32m[20221213 23:44:46 @agent_ppo2.py:185][0m |          -0.0096 |          54.6751 |           6.5803 |
[32m[20221213 23:44:47 @agent_ppo2.py:185][0m |          -0.0123 |          53.5756 |           6.5892 |
[32m[20221213 23:44:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:44:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.88
[32m[20221213 23:44:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.08
[32m[20221213 23:44:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.53
[32m[20221213 23:44:47 @agent_ppo2.py:143][0m Total time:      32.24 min
[32m[20221213 23:44:47 @agent_ppo2.py:145][0m 3125248 total steps have happened
[32m[20221213 23:44:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3526 --------------------------#
[32m[20221213 23:44:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:47 @agent_ppo2.py:185][0m |           0.0000 |          65.0431 |           5.8430 |
[32m[20221213 23:44:47 @agent_ppo2.py:185][0m |           0.0110 |          69.0297 |           5.7959 |
[32m[20221213 23:44:47 @agent_ppo2.py:185][0m |          -0.0051 |          62.2685 |           5.8760 |
[32m[20221213 23:44:47 @agent_ppo2.py:185][0m |          -0.0060 |          61.6276 |           5.8511 |
[32m[20221213 23:44:47 @agent_ppo2.py:185][0m |          -0.0067 |          61.3255 |           5.8631 |
[32m[20221213 23:44:47 @agent_ppo2.py:185][0m |          -0.0030 |          61.5610 |           5.7671 |
[32m[20221213 23:44:48 @agent_ppo2.py:185][0m |          -0.0080 |          61.0085 |           5.7239 |
[32m[20221213 23:44:48 @agent_ppo2.py:185][0m |          -0.0089 |          60.8984 |           5.7674 |
[32m[20221213 23:44:48 @agent_ppo2.py:185][0m |           0.0036 |          69.4574 |           5.6655 |
[32m[20221213 23:44:48 @agent_ppo2.py:185][0m |          -0.0096 |          60.8624 |           5.6843 |
[32m[20221213 23:44:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.02
[32m[20221213 23:44:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.28
[32m[20221213 23:44:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.50
[32m[20221213 23:44:48 @agent_ppo2.py:143][0m Total time:      32.27 min
[32m[20221213 23:44:48 @agent_ppo2.py:145][0m 3127296 total steps have happened
[32m[20221213 23:44:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3527 --------------------------#
[32m[20221213 23:44:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:48 @agent_ppo2.py:185][0m |          -0.0011 |          49.4209 |           5.1518 |
[32m[20221213 23:44:48 @agent_ppo2.py:185][0m |          -0.0026 |          43.7887 |           5.2564 |
[32m[20221213 23:44:48 @agent_ppo2.py:185][0m |          -0.0038 |          42.3550 |           5.2707 |
[32m[20221213 23:44:49 @agent_ppo2.py:185][0m |           0.0017 |          42.8148 |           5.3810 |
[32m[20221213 23:44:49 @agent_ppo2.py:185][0m |          -0.0088 |          40.6374 |           5.4415 |
[32m[20221213 23:44:49 @agent_ppo2.py:185][0m |          -0.0082 |          40.4392 |           5.4783 |
[32m[20221213 23:44:49 @agent_ppo2.py:185][0m |          -0.0157 |          40.0150 |           5.5426 |
[32m[20221213 23:44:49 @agent_ppo2.py:185][0m |          -0.0089 |          39.7319 |           5.5701 |
[32m[20221213 23:44:49 @agent_ppo2.py:185][0m |          -0.0147 |          39.1782 |           5.6483 |
[32m[20221213 23:44:49 @agent_ppo2.py:185][0m |          -0.0088 |          39.6624 |           5.6927 |
[32m[20221213 23:44:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:44:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.68
[32m[20221213 23:44:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.34
[32m[20221213 23:44:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.55
[32m[20221213 23:44:49 @agent_ppo2.py:143][0m Total time:      32.29 min
[32m[20221213 23:44:49 @agent_ppo2.py:145][0m 3129344 total steps have happened
[32m[20221213 23:44:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3528 --------------------------#
[32m[20221213 23:44:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0002 |          66.7739 |           6.2608 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0080 |          62.6910 |           6.2802 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0053 |          61.9923 |           6.2367 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0042 |          61.8777 |           6.2813 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0028 |          62.5077 |           6.2491 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0114 |          59.9736 |           6.3412 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0109 |          59.7948 |           6.2744 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0115 |          59.3992 |           6.2931 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0066 |          63.1419 |           6.2916 |
[32m[20221213 23:44:50 @agent_ppo2.py:185][0m |          -0.0139 |          59.0820 |           6.3336 |
[32m[20221213 23:44:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:44:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.12
[32m[20221213 23:44:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.58
[32m[20221213 23:44:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.30
[32m[20221213 23:44:51 @agent_ppo2.py:143][0m Total time:      32.31 min
[32m[20221213 23:44:51 @agent_ppo2.py:145][0m 3131392 total steps have happened
[32m[20221213 23:44:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3529 --------------------------#
[32m[20221213 23:44:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:51 @agent_ppo2.py:185][0m |          -0.0031 |          58.7522 |           6.1136 |
[32m[20221213 23:44:51 @agent_ppo2.py:185][0m |          -0.0066 |          57.1542 |           6.1276 |
[32m[20221213 23:44:51 @agent_ppo2.py:185][0m |          -0.0032 |          56.7458 |           6.0717 |
[32m[20221213 23:44:51 @agent_ppo2.py:185][0m |          -0.0003 |          57.0810 |           6.0923 |
[32m[20221213 23:44:51 @agent_ppo2.py:185][0m |          -0.0088 |          56.3378 |           6.1126 |
[32m[20221213 23:44:51 @agent_ppo2.py:185][0m |          -0.0053 |          56.3471 |           6.0892 |
[32m[20221213 23:44:51 @agent_ppo2.py:185][0m |          -0.0075 |          56.1356 |           6.1135 |
[32m[20221213 23:44:52 @agent_ppo2.py:185][0m |          -0.0102 |          55.9089 |           6.1302 |
[32m[20221213 23:44:52 @agent_ppo2.py:185][0m |           0.0079 |          59.8528 |           6.1442 |
[32m[20221213 23:44:52 @agent_ppo2.py:185][0m |          -0.0083 |          55.8121 |           6.1543 |
[32m[20221213 23:44:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.89
[32m[20221213 23:44:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.26
[32m[20221213 23:44:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.51
[32m[20221213 23:44:52 @agent_ppo2.py:143][0m Total time:      32.33 min
[32m[20221213 23:44:52 @agent_ppo2.py:145][0m 3133440 total steps have happened
[32m[20221213 23:44:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3530 --------------------------#
[32m[20221213 23:44:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:44:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:52 @agent_ppo2.py:185][0m |          -0.0006 |          63.3048 |           6.3261 |
[32m[20221213 23:44:52 @agent_ppo2.py:185][0m |           0.0099 |          65.8285 |           6.4173 |
[32m[20221213 23:44:52 @agent_ppo2.py:185][0m |          -0.0040 |          60.6376 |           6.4515 |
[32m[20221213 23:44:53 @agent_ppo2.py:185][0m |          -0.0014 |          60.9434 |           6.5072 |
[32m[20221213 23:44:53 @agent_ppo2.py:185][0m |           0.0022 |          62.5414 |           6.4607 |
[32m[20221213 23:44:53 @agent_ppo2.py:185][0m |          -0.0049 |          61.4864 |           6.5256 |
[32m[20221213 23:44:53 @agent_ppo2.py:185][0m |          -0.0074 |          59.4265 |           6.5393 |
[32m[20221213 23:44:53 @agent_ppo2.py:185][0m |          -0.0096 |          59.1306 |           6.5166 |
[32m[20221213 23:44:53 @agent_ppo2.py:185][0m |          -0.0139 |          59.0068 |           6.5607 |
[32m[20221213 23:44:53 @agent_ppo2.py:185][0m |          -0.0012 |          63.3242 |           6.6148 |
[32m[20221213 23:44:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.73
[32m[20221213 23:44:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.15
[32m[20221213 23:44:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 15.27
[32m[20221213 23:44:53 @agent_ppo2.py:143][0m Total time:      32.35 min
[32m[20221213 23:44:53 @agent_ppo2.py:145][0m 3135488 total steps have happened
[32m[20221213 23:44:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3531 --------------------------#
[32m[20221213 23:44:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0019 |          57.6419 |           6.6555 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0071 |          52.5667 |           6.6744 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0020 |          51.2540 |           6.7238 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0070 |          50.2425 |           6.7132 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0153 |          49.9443 |           6.6572 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0092 |          49.3388 |           6.6827 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0197 |          49.0385 |           6.6676 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0111 |          48.7912 |           6.6923 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0144 |          48.3933 |           6.6781 |
[32m[20221213 23:44:54 @agent_ppo2.py:185][0m |          -0.0115 |          48.2453 |           6.7130 |
[32m[20221213 23:44:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:44:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.53
[32m[20221213 23:44:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.16
[32m[20221213 23:44:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.69
[32m[20221213 23:44:54 @agent_ppo2.py:143][0m Total time:      32.37 min
[32m[20221213 23:44:54 @agent_ppo2.py:145][0m 3137536 total steps have happened
[32m[20221213 23:44:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3532 --------------------------#
[32m[20221213 23:44:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |           0.0025 |          75.9499 |           6.4991 |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |          -0.0030 |          70.6576 |           6.5317 |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |          -0.0058 |          69.2603 |           6.5448 |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |          -0.0042 |          68.0028 |           6.5320 |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |           0.0017 |          72.8992 |           6.5408 |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |          -0.0021 |          67.0905 |           6.6689 |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |          -0.0084 |          66.1147 |           6.5347 |
[32m[20221213 23:44:55 @agent_ppo2.py:185][0m |           0.0001 |          70.7939 |           6.6630 |
[32m[20221213 23:44:56 @agent_ppo2.py:185][0m |          -0.0101 |          65.8873 |           6.6749 |
[32m[20221213 23:44:56 @agent_ppo2.py:185][0m |          -0.0046 |          66.1678 |           6.7235 |
[32m[20221213 23:44:56 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.52
[32m[20221213 23:44:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.02
[32m[20221213 23:44:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.34
[32m[20221213 23:44:56 @agent_ppo2.py:143][0m Total time:      32.40 min
[32m[20221213 23:44:56 @agent_ppo2.py:145][0m 3139584 total steps have happened
[32m[20221213 23:44:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3533 --------------------------#
[32m[20221213 23:44:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:56 @agent_ppo2.py:185][0m |          -0.0011 |          36.1462 |           6.3204 |
[32m[20221213 23:44:56 @agent_ppo2.py:185][0m |          -0.0046 |          31.5236 |           6.4407 |
[32m[20221213 23:44:56 @agent_ppo2.py:185][0m |          -0.0090 |          30.1097 |           6.3863 |
[32m[20221213 23:44:56 @agent_ppo2.py:185][0m |          -0.0066 |          28.9552 |           6.3546 |
[32m[20221213 23:44:56 @agent_ppo2.py:185][0m |          -0.0134 |          28.3187 |           6.3882 |
[32m[20221213 23:44:57 @agent_ppo2.py:185][0m |          -0.0141 |          27.7798 |           6.4026 |
[32m[20221213 23:44:57 @agent_ppo2.py:185][0m |          -0.0095 |          27.1164 |           6.4123 |
[32m[20221213 23:44:57 @agent_ppo2.py:185][0m |          -0.0195 |          26.6016 |           6.4290 |
[32m[20221213 23:44:57 @agent_ppo2.py:185][0m |          -0.0165 |          26.1394 |           6.4164 |
[32m[20221213 23:44:57 @agent_ppo2.py:185][0m |          -0.0207 |          25.8939 |           6.4128 |
[32m[20221213 23:44:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:44:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.25
[32m[20221213 23:44:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.03
[32m[20221213 23:44:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.24
[32m[20221213 23:44:57 @agent_ppo2.py:143][0m Total time:      32.42 min
[32m[20221213 23:44:57 @agent_ppo2.py:145][0m 3141632 total steps have happened
[32m[20221213 23:44:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3534 --------------------------#
[32m[20221213 23:44:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:57 @agent_ppo2.py:185][0m |          -0.0007 |          43.2322 |           6.4997 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0056 |          40.4316 |           6.5112 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0044 |          39.2205 |           6.5383 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0094 |          38.5221 |           6.5412 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0119 |          37.8137 |           6.5533 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0131 |          37.5390 |           6.5831 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0105 |          37.2696 |           6.6310 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0170 |          36.8857 |           6.6051 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0158 |          36.6141 |           6.6106 |
[32m[20221213 23:44:58 @agent_ppo2.py:185][0m |          -0.0135 |          36.7143 |           6.6672 |
[32m[20221213 23:44:58 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:44:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.35
[32m[20221213 23:44:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.00
[32m[20221213 23:44:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 226.14
[32m[20221213 23:44:58 @agent_ppo2.py:143][0m Total time:      32.44 min
[32m[20221213 23:44:58 @agent_ppo2.py:145][0m 3143680 total steps have happened
[32m[20221213 23:44:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3535 --------------------------#
[32m[20221213 23:44:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:44:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |           0.0015 |          63.5126 |           6.8845 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |          -0.0057 |          62.0408 |           6.9979 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |           0.0049 |          67.9741 |           6.9878 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |          -0.0038 |          61.3883 |           7.0309 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |          -0.0087 |          60.9300 |           6.9539 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |          -0.0088 |          60.5921 |           6.9971 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |          -0.0102 |          60.5516 |           6.9493 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |          -0.0121 |          60.4780 |           6.9801 |
[32m[20221213 23:44:59 @agent_ppo2.py:185][0m |          -0.0114 |          60.2024 |           6.9094 |
[32m[20221213 23:45:00 @agent_ppo2.py:185][0m |          -0.0090 |          60.7096 |           6.9415 |
[32m[20221213 23:45:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.46
[32m[20221213 23:45:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.15
[32m[20221213 23:45:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.80
[32m[20221213 23:45:00 @agent_ppo2.py:143][0m Total time:      32.46 min
[32m[20221213 23:45:00 @agent_ppo2.py:145][0m 3145728 total steps have happened
[32m[20221213 23:45:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3536 --------------------------#
[32m[20221213 23:45:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:00 @agent_ppo2.py:185][0m |           0.0014 |          40.0009 |           7.2158 |
[32m[20221213 23:45:00 @agent_ppo2.py:185][0m |           0.0015 |          36.1709 |           7.2320 |
[32m[20221213 23:45:00 @agent_ppo2.py:185][0m |          -0.0055 |          34.4845 |           7.2996 |
[32m[20221213 23:45:00 @agent_ppo2.py:185][0m |          -0.0074 |          33.5377 |           7.2581 |
[32m[20221213 23:45:00 @agent_ppo2.py:185][0m |          -0.0058 |          32.7206 |           7.2682 |
[32m[20221213 23:45:00 @agent_ppo2.py:185][0m |          -0.0076 |          32.0713 |           7.3018 |
[32m[20221213 23:45:01 @agent_ppo2.py:185][0m |          -0.0116 |          31.6375 |           7.2521 |
[32m[20221213 23:45:01 @agent_ppo2.py:185][0m |          -0.0108 |          31.0943 |           7.3889 |
[32m[20221213 23:45:01 @agent_ppo2.py:185][0m |          -0.0152 |          30.9023 |           7.2260 |
[32m[20221213 23:45:01 @agent_ppo2.py:185][0m |          -0.0133 |          30.4541 |           7.2773 |
[32m[20221213 23:45:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:45:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.11
[32m[20221213 23:45:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.44
[32m[20221213 23:45:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.10
[32m[20221213 23:45:01 @agent_ppo2.py:143][0m Total time:      32.48 min
[32m[20221213 23:45:01 @agent_ppo2.py:145][0m 3147776 total steps have happened
[32m[20221213 23:45:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3537 --------------------------#
[32m[20221213 23:45:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:01 @agent_ppo2.py:185][0m |           0.0037 |          58.8446 |           6.8974 |
[32m[20221213 23:45:01 @agent_ppo2.py:185][0m |          -0.0019 |          54.7808 |           6.9553 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0083 |          53.4821 |           6.9203 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0078 |          52.7346 |           6.9284 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0048 |          52.6007 |           6.9548 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0116 |          51.9286 |           6.9834 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0108 |          51.1360 |           6.8930 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0160 |          50.8368 |           6.9237 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0166 |          50.9907 |           6.9193 |
[32m[20221213 23:45:02 @agent_ppo2.py:185][0m |          -0.0172 |          50.5583 |           6.9484 |
[32m[20221213 23:45:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:45:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.15
[32m[20221213 23:45:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.69
[32m[20221213 23:45:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 197.90
[32m[20221213 23:45:02 @agent_ppo2.py:143][0m Total time:      32.51 min
[32m[20221213 23:45:02 @agent_ppo2.py:145][0m 3149824 total steps have happened
[32m[20221213 23:45:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3538 --------------------------#
[32m[20221213 23:45:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:45:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |           0.0044 |          65.7884 |           6.7677 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0076 |          60.1745 |           6.7390 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0075 |          58.5313 |           6.7659 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0042 |          59.0369 |           6.7389 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0115 |          56.4059 |           6.7585 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0122 |          55.4980 |           6.7525 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0111 |          57.1175 |           6.7623 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0134 |          54.1739 |           6.7663 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0169 |          53.7374 |           6.7480 |
[32m[20221213 23:45:03 @agent_ppo2.py:185][0m |          -0.0164 |          53.5056 |           6.7938 |
[32m[20221213 23:45:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:45:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.87
[32m[20221213 23:45:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.18
[32m[20221213 23:45:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.33
[32m[20221213 23:45:04 @agent_ppo2.py:143][0m Total time:      32.53 min
[32m[20221213 23:45:04 @agent_ppo2.py:145][0m 3151872 total steps have happened
[32m[20221213 23:45:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3539 --------------------------#
[32m[20221213 23:45:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:04 @agent_ppo2.py:185][0m |           0.0020 |          43.5088 |           6.5710 |
[32m[20221213 23:45:04 @agent_ppo2.py:185][0m |           0.0020 |          38.6276 |           6.5468 |
[32m[20221213 23:45:04 @agent_ppo2.py:185][0m |          -0.0061 |          36.2323 |           6.5164 |
[32m[20221213 23:45:04 @agent_ppo2.py:185][0m |          -0.0123 |          35.0630 |           6.5640 |
[32m[20221213 23:45:04 @agent_ppo2.py:185][0m |          -0.0152 |          33.9480 |           6.5125 |
[32m[20221213 23:45:04 @agent_ppo2.py:185][0m |          -0.0218 |          33.1232 |           6.4951 |
[32m[20221213 23:45:04 @agent_ppo2.py:185][0m |          -0.0081 |          32.4928 |           6.4730 |
[32m[20221213 23:45:05 @agent_ppo2.py:185][0m |          -0.0067 |          32.2036 |           6.4190 |
[32m[20221213 23:45:05 @agent_ppo2.py:185][0m |          -0.0120 |          31.2872 |           6.5373 |
[32m[20221213 23:45:05 @agent_ppo2.py:185][0m |          -0.0173 |          30.9105 |           6.4404 |
[32m[20221213 23:45:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.51
[32m[20221213 23:45:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.26
[32m[20221213 23:45:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 196.42
[32m[20221213 23:45:05 @agent_ppo2.py:143][0m Total time:      32.55 min
[32m[20221213 23:45:05 @agent_ppo2.py:145][0m 3153920 total steps have happened
[32m[20221213 23:45:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3540 --------------------------#
[32m[20221213 23:45:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:45:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:05 @agent_ppo2.py:185][0m |           0.0034 |          43.9695 |           6.7318 |
[32m[20221213 23:45:05 @agent_ppo2.py:185][0m |          -0.0079 |          39.0609 |           6.7124 |
[32m[20221213 23:45:05 @agent_ppo2.py:185][0m |          -0.0083 |          36.8665 |           6.7428 |
[32m[20221213 23:45:06 @agent_ppo2.py:185][0m |          -0.0119 |          35.8877 |           6.7314 |
[32m[20221213 23:45:06 @agent_ppo2.py:185][0m |          -0.0161 |          35.3608 |           6.7408 |
[32m[20221213 23:45:06 @agent_ppo2.py:185][0m |          -0.0144 |          34.4401 |           6.7418 |
[32m[20221213 23:45:06 @agent_ppo2.py:185][0m |          -0.0113 |          33.8093 |           6.7185 |
[32m[20221213 23:45:06 @agent_ppo2.py:185][0m |          -0.0210 |          33.4201 |           6.6850 |
[32m[20221213 23:45:06 @agent_ppo2.py:185][0m |          -0.0148 |          32.8742 |           6.6763 |
[32m[20221213 23:45:06 @agent_ppo2.py:185][0m |          -0.0179 |          32.7779 |           6.7108 |
[32m[20221213 23:45:06 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.90
[32m[20221213 23:45:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.39
[32m[20221213 23:45:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.76
[32m[20221213 23:45:06 @agent_ppo2.py:143][0m Total time:      32.57 min
[32m[20221213 23:45:06 @agent_ppo2.py:145][0m 3155968 total steps have happened
[32m[20221213 23:45:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3541 --------------------------#
[32m[20221213 23:45:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |           0.0115 |          64.3202 |           6.5564 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |           0.0066 |          61.6437 |           6.5873 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0091 |          54.7460 |           6.7086 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0142 |          52.9614 |           6.6536 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0115 |          52.4133 |           6.6351 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0144 |          51.9043 |           6.7360 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0150 |          51.7625 |           6.7520 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0129 |          51.4296 |           6.7491 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0162 |          51.2549 |           6.7660 |
[32m[20221213 23:45:07 @agent_ppo2.py:185][0m |          -0.0158 |          51.0212 |           6.7512 |
[32m[20221213 23:45:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:45:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.43
[32m[20221213 23:45:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.89
[32m[20221213 23:45:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.18
[32m[20221213 23:45:07 @agent_ppo2.py:143][0m Total time:      32.59 min
[32m[20221213 23:45:07 @agent_ppo2.py:145][0m 3158016 total steps have happened
[32m[20221213 23:45:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3542 --------------------------#
[32m[20221213 23:45:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |           0.0007 |          45.2015 |           6.3702 |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |          -0.0025 |          41.5706 |           6.3477 |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |          -0.0009 |          40.5974 |           6.3246 |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |          -0.0010 |          40.0349 |           6.3687 |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |          -0.0082 |          39.3443 |           6.3922 |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |          -0.0110 |          38.7704 |           6.4333 |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |          -0.0078 |          38.2978 |           6.3590 |
[32m[20221213 23:45:08 @agent_ppo2.py:185][0m |          -0.0097 |          37.9018 |           6.3527 |
[32m[20221213 23:45:09 @agent_ppo2.py:185][0m |          -0.0142 |          37.6937 |           6.3354 |
[32m[20221213 23:45:09 @agent_ppo2.py:185][0m |          -0.0110 |          37.1569 |           6.3377 |
[32m[20221213 23:45:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:45:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.95
[32m[20221213 23:45:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.83
[32m[20221213 23:45:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.76
[32m[20221213 23:45:09 @agent_ppo2.py:143][0m Total time:      32.61 min
[32m[20221213 23:45:09 @agent_ppo2.py:145][0m 3160064 total steps have happened
[32m[20221213 23:45:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3543 --------------------------#
[32m[20221213 23:45:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:09 @agent_ppo2.py:185][0m |           0.0029 |          59.7973 |           6.3448 |
[32m[20221213 23:45:09 @agent_ppo2.py:185][0m |           0.0016 |          57.3815 |           6.2811 |
[32m[20221213 23:45:09 @agent_ppo2.py:185][0m |          -0.0080 |          52.1861 |           6.2346 |
[32m[20221213 23:45:09 @agent_ppo2.py:185][0m |          -0.0070 |          51.8120 |           6.2129 |
[32m[20221213 23:45:09 @agent_ppo2.py:185][0m |          -0.0079 |          50.1480 |           6.2526 |
[32m[20221213 23:45:10 @agent_ppo2.py:185][0m |          -0.0109 |          49.1380 |           6.2221 |
[32m[20221213 23:45:10 @agent_ppo2.py:185][0m |          -0.0127 |          48.6630 |           6.2203 |
[32m[20221213 23:45:10 @agent_ppo2.py:185][0m |          -0.0129 |          48.2346 |           6.1950 |
[32m[20221213 23:45:10 @agent_ppo2.py:185][0m |          -0.0247 |          49.5201 |           6.1633 |
[32m[20221213 23:45:10 @agent_ppo2.py:185][0m |          -0.0125 |          47.6666 |           6.1747 |
[32m[20221213 23:45:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.39
[32m[20221213 23:45:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.44
[32m[20221213 23:45:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.54
[32m[20221213 23:45:10 @agent_ppo2.py:143][0m Total time:      32.63 min
[32m[20221213 23:45:10 @agent_ppo2.py:145][0m 3162112 total steps have happened
[32m[20221213 23:45:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3544 --------------------------#
[32m[20221213 23:45:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:10 @agent_ppo2.py:185][0m |           0.0112 |          63.0310 |           6.0168 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0000 |          55.3575 |           6.1185 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0012 |          54.8746 |           6.0881 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |           0.0006 |          54.7786 |           6.1817 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0047 |          54.3859 |           6.1177 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0059 |          54.2984 |           6.1770 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0058 |          54.2695 |           6.1769 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0057 |          54.1193 |           6.1654 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0052 |          53.9980 |           6.2628 |
[32m[20221213 23:45:11 @agent_ppo2.py:185][0m |          -0.0047 |          53.9521 |           6.2625 |
[32m[20221213 23:45:11 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.18
[32m[20221213 23:45:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.28
[32m[20221213 23:45:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.89
[32m[20221213 23:45:11 @agent_ppo2.py:143][0m Total time:      32.66 min
[32m[20221213 23:45:11 @agent_ppo2.py:145][0m 3164160 total steps have happened
[32m[20221213 23:45:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3545 --------------------------#
[32m[20221213 23:45:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0013 |          77.4065 |           6.4262 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0045 |          72.8255 |           6.5337 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0011 |          75.9289 |           6.5118 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0084 |          69.7144 |           6.4874 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0097 |          68.7145 |           6.3941 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0122 |          68.0740 |           6.4566 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0138 |          67.7032 |           6.3804 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |           0.0087 |          79.6902 |           6.4243 |
[32m[20221213 23:45:12 @agent_ppo2.py:185][0m |          -0.0144 |          67.5884 |           6.4540 |
[32m[20221213 23:45:13 @agent_ppo2.py:185][0m |          -0.0153 |          67.2287 |           6.4079 |
[32m[20221213 23:45:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.93
[32m[20221213 23:45:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.41
[32m[20221213 23:45:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.72
[32m[20221213 23:45:13 @agent_ppo2.py:143][0m Total time:      32.68 min
[32m[20221213 23:45:13 @agent_ppo2.py:145][0m 3166208 total steps have happened
[32m[20221213 23:45:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3546 --------------------------#
[32m[20221213 23:45:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:13 @agent_ppo2.py:185][0m |           0.0026 |          68.0871 |           6.8762 |
[32m[20221213 23:45:13 @agent_ppo2.py:185][0m |          -0.0031 |          65.7957 |           6.9598 |
[32m[20221213 23:45:13 @agent_ppo2.py:185][0m |          -0.0043 |          65.1042 |           6.9615 |
[32m[20221213 23:45:13 @agent_ppo2.py:185][0m |          -0.0073 |          64.6464 |           6.9481 |
[32m[20221213 23:45:13 @agent_ppo2.py:185][0m |          -0.0083 |          64.4746 |           6.8896 |
[32m[20221213 23:45:13 @agent_ppo2.py:185][0m |          -0.0115 |          64.4760 |           6.9111 |
[32m[20221213 23:45:14 @agent_ppo2.py:185][0m |          -0.0094 |          64.2078 |           6.9221 |
[32m[20221213 23:45:14 @agent_ppo2.py:185][0m |          -0.0115 |          64.0230 |           6.9399 |
[32m[20221213 23:45:14 @agent_ppo2.py:185][0m |          -0.0079 |          64.2223 |           6.9262 |
[32m[20221213 23:45:14 @agent_ppo2.py:185][0m |          -0.0102 |          63.8924 |           6.8962 |
[32m[20221213 23:45:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:45:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.25
[32m[20221213 23:45:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.77
[32m[20221213 23:45:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.08
[32m[20221213 23:45:14 @agent_ppo2.py:143][0m Total time:      32.70 min
[32m[20221213 23:45:14 @agent_ppo2.py:145][0m 3168256 total steps have happened
[32m[20221213 23:45:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3547 --------------------------#
[32m[20221213 23:45:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:14 @agent_ppo2.py:185][0m |           0.0041 |          36.5979 |           6.4663 |
[32m[20221213 23:45:14 @agent_ppo2.py:185][0m |          -0.0078 |          31.9706 |           6.3848 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |          -0.0017 |          30.2932 |           6.3832 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |          -0.0145 |          29.1826 |           6.3627 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |           0.0019 |          30.6617 |           6.3655 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |          -0.0104 |          28.1610 |           6.3186 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |          -0.0133 |          27.4405 |           6.2431 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |          -0.0139 |          27.1663 |           6.2751 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |          -0.0171 |          26.7519 |           6.1967 |
[32m[20221213 23:45:15 @agent_ppo2.py:185][0m |          -0.0180 |          26.6200 |           6.1857 |
[32m[20221213 23:45:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.82
[32m[20221213 23:45:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.23
[32m[20221213 23:45:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.31
[32m[20221213 23:45:15 @agent_ppo2.py:143][0m Total time:      32.72 min
[32m[20221213 23:45:15 @agent_ppo2.py:145][0m 3170304 total steps have happened
[32m[20221213 23:45:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3548 --------------------------#
[32m[20221213 23:45:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:45:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0013 |          62.9306 |           6.0030 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0072 |          61.6092 |           6.0258 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0056 |          61.3583 |           6.0477 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0080 |          61.2431 |           6.0857 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0101 |          60.9134 |           6.0931 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0041 |          62.7518 |           6.0944 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0032 |          61.3324 |           6.2038 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0093 |          60.7425 |           6.1578 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0110 |          60.6574 |           6.1124 |
[32m[20221213 23:45:16 @agent_ppo2.py:185][0m |          -0.0123 |          60.5399 |           6.1278 |
[32m[20221213 23:45:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.48
[32m[20221213 23:45:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.02
[32m[20221213 23:45:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.85
[32m[20221213 23:45:17 @agent_ppo2.py:143][0m Total time:      32.74 min
[32m[20221213 23:45:17 @agent_ppo2.py:145][0m 3172352 total steps have happened
[32m[20221213 23:45:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3549 --------------------------#
[32m[20221213 23:45:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:17 @agent_ppo2.py:185][0m |           0.0024 |          68.4275 |           6.2135 |
[32m[20221213 23:45:17 @agent_ppo2.py:185][0m |          -0.0023 |          65.2665 |           6.2298 |
[32m[20221213 23:45:17 @agent_ppo2.py:185][0m |          -0.0005 |          64.6891 |           6.1654 |
[32m[20221213 23:45:17 @agent_ppo2.py:185][0m |          -0.0090 |          63.9994 |           6.2473 |
[32m[20221213 23:45:17 @agent_ppo2.py:185][0m |          -0.0093 |          63.4358 |           6.2083 |
[32m[20221213 23:45:17 @agent_ppo2.py:185][0m |          -0.0089 |          63.0486 |           6.2498 |
[32m[20221213 23:45:17 @agent_ppo2.py:185][0m |          -0.0082 |          62.7775 |           6.2397 |
[32m[20221213 23:45:18 @agent_ppo2.py:185][0m |          -0.0119 |          62.4986 |           6.1798 |
[32m[20221213 23:45:18 @agent_ppo2.py:185][0m |          -0.0123 |          62.3363 |           6.2270 |
[32m[20221213 23:45:18 @agent_ppo2.py:185][0m |           0.0004 |          66.2734 |           6.2246 |
[32m[20221213 23:45:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:45:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.92
[32m[20221213 23:45:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.30
[32m[20221213 23:45:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.77
[32m[20221213 23:45:18 @agent_ppo2.py:143][0m Total time:      32.76 min
[32m[20221213 23:45:18 @agent_ppo2.py:145][0m 3174400 total steps have happened
[32m[20221213 23:45:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3550 --------------------------#
[32m[20221213 23:45:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:45:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:18 @agent_ppo2.py:185][0m |           0.0048 |          80.4096 |           6.2431 |
[32m[20221213 23:45:18 @agent_ppo2.py:185][0m |          -0.0080 |          73.3401 |           6.2450 |
[32m[20221213 23:45:18 @agent_ppo2.py:185][0m |          -0.0088 |          70.4827 |           6.2524 |
[32m[20221213 23:45:18 @agent_ppo2.py:185][0m |          -0.0097 |          69.9735 |           6.3266 |
[32m[20221213 23:45:19 @agent_ppo2.py:185][0m |          -0.0077 |          69.8370 |           6.3122 |
[32m[20221213 23:45:19 @agent_ppo2.py:185][0m |          -0.0137 |          68.4927 |           6.2834 |
[32m[20221213 23:45:19 @agent_ppo2.py:185][0m |          -0.0067 |          72.1166 |           6.3646 |
[32m[20221213 23:45:19 @agent_ppo2.py:185][0m |          -0.0135 |          67.5142 |           6.3315 |
[32m[20221213 23:45:19 @agent_ppo2.py:185][0m |          -0.0128 |          67.9386 |           6.3744 |
[32m[20221213 23:45:19 @agent_ppo2.py:185][0m |          -0.0069 |          67.4855 |           6.3769 |
[32m[20221213 23:45:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.27
[32m[20221213 23:45:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.17
[32m[20221213 23:45:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.62
[32m[20221213 23:45:19 @agent_ppo2.py:143][0m Total time:      32.79 min
[32m[20221213 23:45:19 @agent_ppo2.py:145][0m 3176448 total steps have happened
[32m[20221213 23:45:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3551 --------------------------#
[32m[20221213 23:45:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0005 |          74.1635 |           6.3528 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0055 |          67.9147 |           6.4085 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0077 |          65.3866 |           6.3903 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0072 |          63.5818 |           6.4844 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0100 |          62.4662 |           6.5834 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0046 |          65.8998 |           6.5126 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0083 |          61.9869 |           6.5407 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0127 |          59.8731 |           6.5827 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0127 |          59.1870 |           6.5608 |
[32m[20221213 23:45:20 @agent_ppo2.py:185][0m |          -0.0160 |          58.5877 |           6.5524 |
[32m[20221213 23:45:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:45:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.24
[32m[20221213 23:45:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.23
[32m[20221213 23:45:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.70
[32m[20221213 23:45:20 @agent_ppo2.py:143][0m Total time:      32.81 min
[32m[20221213 23:45:20 @agent_ppo2.py:145][0m 3178496 total steps have happened
[32m[20221213 23:45:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3552 --------------------------#
[32m[20221213 23:45:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |           0.0074 |          59.4018 |           6.1278 |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |           0.0009 |          56.8891 |           6.1297 |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |          -0.0026 |          56.1697 |           6.1609 |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |           0.0087 |          63.6359 |           6.1369 |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |          -0.0126 |          54.9885 |           6.2314 |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |          -0.0154 |          54.4538 |           6.2892 |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |          -0.0150 |          54.2695 |           6.3246 |
[32m[20221213 23:45:21 @agent_ppo2.py:185][0m |          -0.0158 |          54.0651 |           6.3166 |
[32m[20221213 23:45:22 @agent_ppo2.py:185][0m |          -0.0169 |          54.1200 |           6.3354 |
[32m[20221213 23:45:22 @agent_ppo2.py:185][0m |          -0.0130 |          53.9066 |           6.3563 |
[32m[20221213 23:45:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.86
[32m[20221213 23:45:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.13
[32m[20221213 23:45:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.50
[32m[20221213 23:45:22 @agent_ppo2.py:143][0m Total time:      32.83 min
[32m[20221213 23:45:22 @agent_ppo2.py:145][0m 3180544 total steps have happened
[32m[20221213 23:45:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3553 --------------------------#
[32m[20221213 23:45:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:22 @agent_ppo2.py:185][0m |           0.0024 |          71.1075 |           6.4314 |
[32m[20221213 23:45:22 @agent_ppo2.py:185][0m |          -0.0020 |          68.6556 |           6.3557 |
[32m[20221213 23:45:22 @agent_ppo2.py:185][0m |          -0.0024 |          65.4324 |           6.3734 |
[32m[20221213 23:45:22 @agent_ppo2.py:185][0m |          -0.0088 |          64.3184 |           6.3852 |
[32m[20221213 23:45:22 @agent_ppo2.py:185][0m |          -0.0125 |          62.9656 |           6.4413 |
[32m[20221213 23:45:23 @agent_ppo2.py:185][0m |          -0.0163 |          62.4815 |           6.4518 |
[32m[20221213 23:45:23 @agent_ppo2.py:185][0m |          -0.0192 |          62.3064 |           6.5102 |
[32m[20221213 23:45:23 @agent_ppo2.py:185][0m |          -0.0030 |          64.4756 |           6.4645 |
[32m[20221213 23:45:23 @agent_ppo2.py:185][0m |          -0.0152 |          60.5552 |           6.5107 |
[32m[20221213 23:45:23 @agent_ppo2.py:185][0m |          -0.0077 |          62.2108 |           6.5143 |
[32m[20221213 23:45:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.18
[32m[20221213 23:45:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.41
[32m[20221213 23:45:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.57
[32m[20221213 23:45:23 @agent_ppo2.py:143][0m Total time:      32.85 min
[32m[20221213 23:45:23 @agent_ppo2.py:145][0m 3182592 total steps have happened
[32m[20221213 23:45:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3554 --------------------------#
[32m[20221213 23:45:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:23 @agent_ppo2.py:185][0m |          -0.0025 |          60.1058 |           6.3016 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0064 |          58.4489 |           6.1896 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0077 |          57.9679 |           6.2555 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0070 |          57.6656 |           6.2575 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0009 |          60.1371 |           6.2914 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0024 |          57.5970 |           6.2856 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0099 |          57.4743 |           6.2577 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0102 |          57.2074 |           6.2804 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0085 |          57.1981 |           6.2657 |
[32m[20221213 23:45:24 @agent_ppo2.py:185][0m |          -0.0094 |          57.8635 |           6.2271 |
[32m[20221213 23:45:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:45:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.26
[32m[20221213 23:45:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.20
[32m[20221213 23:45:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.27
[32m[20221213 23:45:24 @agent_ppo2.py:143][0m Total time:      32.87 min
[32m[20221213 23:45:24 @agent_ppo2.py:145][0m 3184640 total steps have happened
[32m[20221213 23:45:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3555 --------------------------#
[32m[20221213 23:45:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0029 |          78.4790 |           6.8402 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0044 |          74.0978 |           6.7837 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0023 |          76.1159 |           6.9153 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0053 |          72.7986 |           6.9004 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0134 |          71.7628 |           6.8593 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0127 |          71.7695 |           6.9423 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0104 |          71.2822 |           6.9324 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0120 |          71.4835 |           6.9561 |
[32m[20221213 23:45:25 @agent_ppo2.py:185][0m |          -0.0116 |          70.9368 |           6.9173 |
[32m[20221213 23:45:26 @agent_ppo2.py:185][0m |          -0.0166 |          70.7164 |           6.9994 |
[32m[20221213 23:45:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:45:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.81
[32m[20221213 23:45:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.30
[32m[20221213 23:45:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.95
[32m[20221213 23:45:26 @agent_ppo2.py:143][0m Total time:      32.89 min
[32m[20221213 23:45:26 @agent_ppo2.py:145][0m 3186688 total steps have happened
[32m[20221213 23:45:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3556 --------------------------#
[32m[20221213 23:45:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:45:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:26 @agent_ppo2.py:185][0m |           0.0009 |          61.7630 |           7.2529 |
[32m[20221213 23:45:26 @agent_ppo2.py:185][0m |           0.0019 |          61.2350 |           7.2606 |
[32m[20221213 23:45:26 @agent_ppo2.py:185][0m |          -0.0044 |          60.0489 |           7.2433 |
[32m[20221213 23:45:26 @agent_ppo2.py:185][0m |          -0.0051 |          59.7282 |           7.1482 |
[32m[20221213 23:45:26 @agent_ppo2.py:185][0m |          -0.0101 |          59.6489 |           7.1272 |
[32m[20221213 23:45:26 @agent_ppo2.py:185][0m |          -0.0092 |          59.3970 |           7.1354 |
[32m[20221213 23:45:27 @agent_ppo2.py:185][0m |          -0.0074 |          59.0487 |           7.0761 |
[32m[20221213 23:45:27 @agent_ppo2.py:185][0m |          -0.0098 |          59.0394 |           7.0497 |
[32m[20221213 23:45:27 @agent_ppo2.py:185][0m |           0.0067 |          67.8341 |           7.0495 |
[32m[20221213 23:45:27 @agent_ppo2.py:185][0m |          -0.0097 |          58.5905 |           7.0358 |
[32m[20221213 23:45:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.69
[32m[20221213 23:45:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.16
[32m[20221213 23:45:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.07
[32m[20221213 23:45:27 @agent_ppo2.py:143][0m Total time:      32.92 min
[32m[20221213 23:45:27 @agent_ppo2.py:145][0m 3188736 total steps have happened
[32m[20221213 23:45:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3557 --------------------------#
[32m[20221213 23:45:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:27 @agent_ppo2.py:185][0m |           0.0025 |          64.0099 |           6.5176 |
[32m[20221213 23:45:27 @agent_ppo2.py:185][0m |          -0.0014 |          62.9686 |           6.5696 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |          -0.0017 |          62.9968 |           6.5544 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |           0.0036 |          64.8238 |           6.6175 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |           0.0153 |          73.6600 |           6.6018 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |          -0.0045 |          62.0162 |           6.6089 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |          -0.0051 |          61.6709 |           6.6039 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |           0.0044 |          68.1725 |           6.6114 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |           0.0023 |          70.1149 |           6.6559 |
[32m[20221213 23:45:28 @agent_ppo2.py:185][0m |          -0.0039 |          63.5430 |           6.6456 |
[32m[20221213 23:45:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.60
[32m[20221213 23:45:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.85
[32m[20221213 23:45:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.18
[32m[20221213 23:45:28 @agent_ppo2.py:143][0m Total time:      32.94 min
[32m[20221213 23:45:28 @agent_ppo2.py:145][0m 3190784 total steps have happened
[32m[20221213 23:45:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3558 --------------------------#
[32m[20221213 23:45:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |           0.0210 |          78.6581 |           6.8908 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0050 |          66.0140 |           6.8820 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0081 |          64.3367 |           6.9086 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0071 |          63.4255 |           6.9690 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0082 |          62.8816 |           7.0230 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0120 |          62.2331 |           6.9847 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0133 |          61.7221 |           7.0791 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0167 |          61.4122 |           7.0652 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0134 |          60.9016 |           7.1288 |
[32m[20221213 23:45:29 @agent_ppo2.py:185][0m |          -0.0160 |          60.5742 |           7.1413 |
[32m[20221213 23:45:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.89
[32m[20221213 23:45:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.27
[32m[20221213 23:45:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.32
[32m[20221213 23:45:30 @agent_ppo2.py:143][0m Total time:      32.96 min
[32m[20221213 23:45:30 @agent_ppo2.py:145][0m 3192832 total steps have happened
[32m[20221213 23:45:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3559 --------------------------#
[32m[20221213 23:45:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:30 @agent_ppo2.py:185][0m |           0.0158 |          71.3430 |           6.8971 |
[32m[20221213 23:45:30 @agent_ppo2.py:185][0m |          -0.0053 |          64.6861 |           6.8775 |
[32m[20221213 23:45:30 @agent_ppo2.py:185][0m |          -0.0090 |          63.8428 |           6.9007 |
[32m[20221213 23:45:30 @agent_ppo2.py:185][0m |          -0.0099 |          63.6555 |           6.8402 |
[32m[20221213 23:45:30 @agent_ppo2.py:185][0m |          -0.0090 |          63.6258 |           6.8593 |
[32m[20221213 23:45:30 @agent_ppo2.py:185][0m |          -0.0110 |          63.3414 |           6.8953 |
[32m[20221213 23:45:30 @agent_ppo2.py:185][0m |          -0.0107 |          63.0893 |           6.9392 |
[32m[20221213 23:45:31 @agent_ppo2.py:185][0m |          -0.0141 |          62.9547 |           6.8584 |
[32m[20221213 23:45:31 @agent_ppo2.py:185][0m |          -0.0115 |          63.1648 |           6.7927 |
[32m[20221213 23:45:31 @agent_ppo2.py:185][0m |          -0.0127 |          62.8090 |           6.8518 |
[32m[20221213 23:45:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:45:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.59
[32m[20221213 23:45:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.93
[32m[20221213 23:45:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.51
[32m[20221213 23:45:31 @agent_ppo2.py:143][0m Total time:      32.98 min
[32m[20221213 23:45:31 @agent_ppo2.py:145][0m 3194880 total steps have happened
[32m[20221213 23:45:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3560 --------------------------#
[32m[20221213 23:45:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:45:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:31 @agent_ppo2.py:185][0m |           0.0014 |          79.8068 |           7.0234 |
[32m[20221213 23:45:31 @agent_ppo2.py:185][0m |          -0.0084 |          74.9036 |           7.0975 |
[32m[20221213 23:45:31 @agent_ppo2.py:185][0m |          -0.0070 |          72.7231 |           7.1108 |
[32m[20221213 23:45:32 @agent_ppo2.py:185][0m |          -0.0121 |          71.6422 |           7.1416 |
[32m[20221213 23:45:32 @agent_ppo2.py:185][0m |          -0.0113 |          70.7911 |           7.0979 |
[32m[20221213 23:45:32 @agent_ppo2.py:185][0m |          -0.0106 |          70.5186 |           7.1112 |
[32m[20221213 23:45:32 @agent_ppo2.py:185][0m |          -0.0085 |          70.4145 |           7.0332 |
[32m[20221213 23:45:32 @agent_ppo2.py:185][0m |          -0.0113 |          69.2554 |           6.9975 |
[32m[20221213 23:45:32 @agent_ppo2.py:185][0m |          -0.0159 |          68.6774 |           7.0757 |
[32m[20221213 23:45:32 @agent_ppo2.py:185][0m |          -0.0168 |          68.7374 |           7.1022 |
[32m[20221213 23:45:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:45:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.84
[32m[20221213 23:45:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.28
[32m[20221213 23:45:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.27
[32m[20221213 23:45:32 @agent_ppo2.py:143][0m Total time:      33.00 min
[32m[20221213 23:45:32 @agent_ppo2.py:145][0m 3196928 total steps have happened
[32m[20221213 23:45:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3561 --------------------------#
[32m[20221213 23:45:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |           0.0036 |          36.7879 |           6.5774 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |           0.0016 |          32.0606 |           6.5117 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0058 |          30.3682 |           6.5346 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0094 |          29.1087 |           6.4492 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0035 |          28.6958 |           6.4633 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0120 |          28.4152 |           6.4472 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0157 |          27.4750 |           6.5370 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0122 |          26.0478 |           6.5011 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0129 |          25.4440 |           6.5043 |
[32m[20221213 23:45:33 @agent_ppo2.py:185][0m |          -0.0189 |          25.2378 |           6.4668 |
[32m[20221213 23:45:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:45:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.14
[32m[20221213 23:45:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.50
[32m[20221213 23:45:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.35
[32m[20221213 23:45:33 @agent_ppo2.py:143][0m Total time:      33.02 min
[32m[20221213 23:45:33 @agent_ppo2.py:145][0m 3198976 total steps have happened
[32m[20221213 23:45:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3562 --------------------------#
[32m[20221213 23:45:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0019 |          76.0010 |           7.0296 |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0130 |          72.4269 |           7.0710 |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0139 |          70.8131 |           7.0632 |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0119 |          69.7513 |           7.0971 |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0150 |          68.9491 |           7.0469 |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0152 |          68.5408 |           7.1147 |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0215 |          68.2818 |           7.0455 |
[32m[20221213 23:45:34 @agent_ppo2.py:185][0m |          -0.0205 |          67.8504 |           7.0906 |
[32m[20221213 23:45:35 @agent_ppo2.py:185][0m |          -0.0169 |          67.3415 |           7.1004 |
[32m[20221213 23:45:35 @agent_ppo2.py:185][0m |          -0.0204 |          67.1660 |           7.0548 |
[32m[20221213 23:45:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:45:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.78
[32m[20221213 23:45:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.78
[32m[20221213 23:45:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.62
[32m[20221213 23:45:35 @agent_ppo2.py:143][0m Total time:      33.05 min
[32m[20221213 23:45:35 @agent_ppo2.py:145][0m 3201024 total steps have happened
[32m[20221213 23:45:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3563 --------------------------#
[32m[20221213 23:45:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:45:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:35 @agent_ppo2.py:185][0m |           0.0009 |          83.2805 |           7.2062 |
[32m[20221213 23:45:35 @agent_ppo2.py:185][0m |          -0.0064 |          76.1384 |           7.3066 |
[32m[20221213 23:45:35 @agent_ppo2.py:185][0m |          -0.0064 |          73.0055 |           7.2953 |
[32m[20221213 23:45:35 @agent_ppo2.py:185][0m |          -0.0103 |          71.1203 |           7.3067 |
[32m[20221213 23:45:35 @agent_ppo2.py:185][0m |          -0.0119 |          69.4068 |           7.2756 |
[32m[20221213 23:45:36 @agent_ppo2.py:185][0m |          -0.0144 |          67.8269 |           7.2529 |
[32m[20221213 23:45:36 @agent_ppo2.py:185][0m |          -0.0130 |          66.8506 |           7.2200 |
[32m[20221213 23:45:36 @agent_ppo2.py:185][0m |          -0.0088 |          66.0580 |           7.2654 |
[32m[20221213 23:45:36 @agent_ppo2.py:185][0m |          -0.0138 |          65.7955 |           7.2812 |
[32m[20221213 23:45:36 @agent_ppo2.py:185][0m |          -0.0173 |          64.9312 |           7.2256 |
[32m[20221213 23:45:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.89
[32m[20221213 23:45:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.14
[32m[20221213 23:45:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.28
[32m[20221213 23:45:36 @agent_ppo2.py:143][0m Total time:      33.07 min
[32m[20221213 23:45:36 @agent_ppo2.py:145][0m 3203072 total steps have happened
[32m[20221213 23:45:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3564 --------------------------#
[32m[20221213 23:45:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:36 @agent_ppo2.py:185][0m |           0.0002 |          56.6801 |           7.0610 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |           0.0040 |          56.1628 |           7.1285 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0060 |          54.6349 |           7.2065 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0072 |          54.4336 |           7.2036 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0073 |          54.7551 |           7.2526 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0089 |          54.2931 |           7.2781 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0094 |          54.1011 |           7.2725 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0110 |          54.2903 |           7.2222 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0044 |          55.9429 |           7.3031 |
[32m[20221213 23:45:37 @agent_ppo2.py:185][0m |          -0.0072 |          53.9028 |           7.4198 |
[32m[20221213 23:45:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:45:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.06
[32m[20221213 23:45:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.30
[32m[20221213 23:45:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 226.59
[32m[20221213 23:45:37 @agent_ppo2.py:143][0m Total time:      33.09 min
[32m[20221213 23:45:37 @agent_ppo2.py:145][0m 3205120 total steps have happened
[32m[20221213 23:45:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3565 --------------------------#
[32m[20221213 23:45:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0011 |          43.4578 |           7.1239 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0014 |          37.7932 |           7.1121 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0070 |          36.7985 |           7.1090 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0089 |          35.3982 |           7.0207 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0153 |          34.5426 |           7.0740 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0158 |          33.7231 |           7.0724 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0107 |          33.4110 |           7.0784 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0139 |          32.9096 |           7.0627 |
[32m[20221213 23:45:38 @agent_ppo2.py:185][0m |          -0.0160 |          32.4985 |           7.0288 |
[32m[20221213 23:45:39 @agent_ppo2.py:185][0m |          -0.0120 |          32.4989 |           7.1242 |
[32m[20221213 23:45:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.26
[32m[20221213 23:45:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.37
[32m[20221213 23:45:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.72
[32m[20221213 23:45:39 @agent_ppo2.py:143][0m Total time:      33.11 min
[32m[20221213 23:45:39 @agent_ppo2.py:145][0m 3207168 total steps have happened
[32m[20221213 23:45:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3566 --------------------------#
[32m[20221213 23:45:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:39 @agent_ppo2.py:185][0m |           0.0064 |          40.5335 |           8.0893 |
[32m[20221213 23:45:39 @agent_ppo2.py:185][0m |          -0.0052 |          35.7666 |           8.1204 |
[32m[20221213 23:45:39 @agent_ppo2.py:185][0m |          -0.0072 |          33.6562 |           8.0827 |
[32m[20221213 23:45:39 @agent_ppo2.py:185][0m |          -0.0114 |          31.9330 |           8.0805 |
[32m[20221213 23:45:39 @agent_ppo2.py:185][0m |          -0.0083 |          31.1143 |           8.1186 |
[32m[20221213 23:45:39 @agent_ppo2.py:185][0m |          -0.0120 |          30.1716 |           8.0481 |
[32m[20221213 23:45:40 @agent_ppo2.py:185][0m |          -0.0096 |          29.6579 |           8.1075 |
[32m[20221213 23:45:40 @agent_ppo2.py:185][0m |          -0.0108 |          29.1208 |           8.0766 |
[32m[20221213 23:45:40 @agent_ppo2.py:185][0m |          -0.0134 |          28.6531 |           8.0220 |
[32m[20221213 23:45:40 @agent_ppo2.py:185][0m |          -0.0019 |          34.3445 |           8.0595 |
[32m[20221213 23:45:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.61
[32m[20221213 23:45:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.63
[32m[20221213 23:45:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.37
[32m[20221213 23:45:40 @agent_ppo2.py:143][0m Total time:      33.13 min
[32m[20221213 23:45:40 @agent_ppo2.py:145][0m 3209216 total steps have happened
[32m[20221213 23:45:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3567 --------------------------#
[32m[20221213 23:45:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:40 @agent_ppo2.py:185][0m |          -0.0036 |          54.8465 |           7.1273 |
[32m[20221213 23:45:40 @agent_ppo2.py:185][0m |          -0.0064 |          50.6430 |           7.1793 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |          -0.0069 |          49.5804 |           7.2297 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |           0.0082 |          51.7598 |           7.1974 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |          -0.0116 |          48.7741 |           7.2358 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |          -0.0113 |          48.4236 |           7.2877 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |          -0.0111 |          48.1669 |           7.2898 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |          -0.0127 |          47.8114 |           7.3284 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |          -0.0119 |          47.7639 |           7.3359 |
[32m[20221213 23:45:41 @agent_ppo2.py:185][0m |          -0.0103 |          47.5548 |           7.3461 |
[32m[20221213 23:45:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:45:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.36
[32m[20221213 23:45:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.39
[32m[20221213 23:45:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.80
[32m[20221213 23:45:41 @agent_ppo2.py:143][0m Total time:      33.15 min
[32m[20221213 23:45:41 @agent_ppo2.py:145][0m 3211264 total steps have happened
[32m[20221213 23:45:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3568 --------------------------#
[32m[20221213 23:45:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0039 |          72.3148 |           7.6640 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0056 |          65.1188 |           7.6724 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0092 |          63.1866 |           7.6605 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0131 |          61.8328 |           7.6743 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0144 |          60.9572 |           7.7283 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0160 |          60.1322 |           7.7083 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0166 |          59.3578 |           7.7260 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0170 |          58.8326 |           7.7564 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0180 |          58.2184 |           7.6866 |
[32m[20221213 23:45:42 @agent_ppo2.py:185][0m |          -0.0129 |          59.1072 |           7.7483 |
[32m[20221213 23:45:42 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.85
[32m[20221213 23:45:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.42
[32m[20221213 23:45:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.78
[32m[20221213 23:45:43 @agent_ppo2.py:143][0m Total time:      33.18 min
[32m[20221213 23:45:43 @agent_ppo2.py:145][0m 3213312 total steps have happened
[32m[20221213 23:45:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3569 --------------------------#
[32m[20221213 23:45:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:43 @agent_ppo2.py:185][0m |          -0.0004 |          52.0786 |           7.0084 |
[32m[20221213 23:45:43 @agent_ppo2.py:185][0m |          -0.0070 |          50.2867 |           7.0387 |
[32m[20221213 23:45:43 @agent_ppo2.py:185][0m |          -0.0081 |          49.3728 |           6.9710 |
[32m[20221213 23:45:43 @agent_ppo2.py:185][0m |          -0.0081 |          48.9077 |           6.9957 |
[32m[20221213 23:45:43 @agent_ppo2.py:185][0m |          -0.0114 |          48.4183 |           7.0052 |
[32m[20221213 23:45:43 @agent_ppo2.py:185][0m |          -0.0127 |          48.0999 |           6.9074 |
[32m[20221213 23:45:43 @agent_ppo2.py:185][0m |          -0.0121 |          47.8076 |           6.9148 |
[32m[20221213 23:45:44 @agent_ppo2.py:185][0m |          -0.0070 |          48.6712 |           6.9339 |
[32m[20221213 23:45:44 @agent_ppo2.py:185][0m |          -0.0142 |          47.5535 |           6.8911 |
[32m[20221213 23:45:44 @agent_ppo2.py:185][0m |          -0.0171 |          47.2269 |           6.9179 |
[32m[20221213 23:45:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.73
[32m[20221213 23:45:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.21
[32m[20221213 23:45:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.54
[32m[20221213 23:45:44 @agent_ppo2.py:143][0m Total time:      33.20 min
[32m[20221213 23:45:44 @agent_ppo2.py:145][0m 3215360 total steps have happened
[32m[20221213 23:45:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3570 --------------------------#
[32m[20221213 23:45:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:45:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:44 @agent_ppo2.py:185][0m |          -0.0009 |          43.6287 |           6.9165 |
[32m[20221213 23:45:44 @agent_ppo2.py:185][0m |          -0.0077 |          37.5124 |           6.9457 |
[32m[20221213 23:45:44 @agent_ppo2.py:185][0m |          -0.0051 |          35.1281 |           7.0092 |
[32m[20221213 23:45:45 @agent_ppo2.py:185][0m |          -0.0125 |          33.3658 |           6.9281 |
[32m[20221213 23:45:45 @agent_ppo2.py:185][0m |          -0.0029 |          32.7429 |           6.9453 |
[32m[20221213 23:45:45 @agent_ppo2.py:185][0m |          -0.0145 |          31.7534 |           6.8547 |
[32m[20221213 23:45:45 @agent_ppo2.py:185][0m |          -0.0169 |          31.5132 |           6.8917 |
[32m[20221213 23:45:45 @agent_ppo2.py:185][0m |          -0.0178 |          30.6677 |           6.8916 |
[32m[20221213 23:45:45 @agent_ppo2.py:185][0m |          -0.0163 |          30.2942 |           6.8938 |
[32m[20221213 23:45:45 @agent_ppo2.py:185][0m |          -0.0190 |          30.2217 |           6.8239 |
[32m[20221213 23:45:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:45:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.96
[32m[20221213 23:45:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.12
[32m[20221213 23:45:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.59
[32m[20221213 23:45:45 @agent_ppo2.py:143][0m Total time:      33.22 min
[32m[20221213 23:45:45 @agent_ppo2.py:145][0m 3217408 total steps have happened
[32m[20221213 23:45:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3571 --------------------------#
[32m[20221213 23:45:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0001 |          61.1677 |           6.8864 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0083 |          59.2586 |           6.8994 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0066 |          58.6334 |           6.9419 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0066 |          58.0806 |           7.0292 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0080 |          57.9205 |           7.0794 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0015 |          59.3904 |           7.0839 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0042 |          58.1340 |           7.1060 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0091 |          57.3825 |           7.2398 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0105 |          57.1375 |           7.1733 |
[32m[20221213 23:45:46 @agent_ppo2.py:185][0m |          -0.0006 |          60.5044 |           7.2009 |
[32m[20221213 23:45:46 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.65
[32m[20221213 23:45:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.94
[32m[20221213 23:45:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.26
[32m[20221213 23:45:47 @agent_ppo2.py:143][0m Total time:      33.24 min
[32m[20221213 23:45:47 @agent_ppo2.py:145][0m 3219456 total steps have happened
[32m[20221213 23:45:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3572 --------------------------#
[32m[20221213 23:45:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |           0.0034 |          44.4725 |           7.3448 |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |          -0.0057 |          38.3032 |           7.3737 |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |          -0.0137 |          35.7218 |           7.2863 |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |          -0.0041 |          34.7548 |           7.3258 |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |          -0.0129 |          33.8055 |           7.3577 |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |          -0.0119 |          32.9411 |           7.3108 |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |          -0.0116 |          35.7141 |           7.3077 |
[32m[20221213 23:45:47 @agent_ppo2.py:185][0m |          -0.0177 |          32.0486 |           7.2796 |
[32m[20221213 23:45:48 @agent_ppo2.py:185][0m |          -0.0235 |          31.7633 |           7.3255 |
[32m[20221213 23:45:48 @agent_ppo2.py:185][0m |          -0.0154 |          31.6168 |           7.2991 |
[32m[20221213 23:45:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:45:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.64
[32m[20221213 23:45:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.77
[32m[20221213 23:45:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.52
[32m[20221213 23:45:48 @agent_ppo2.py:143][0m Total time:      33.26 min
[32m[20221213 23:45:48 @agent_ppo2.py:145][0m 3221504 total steps have happened
[32m[20221213 23:45:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3573 --------------------------#
[32m[20221213 23:45:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:48 @agent_ppo2.py:185][0m |          -0.0012 |          70.5543 |           7.0736 |
[32m[20221213 23:45:48 @agent_ppo2.py:185][0m |          -0.0046 |          65.0876 |           7.1272 |
[32m[20221213 23:45:48 @agent_ppo2.py:185][0m |          -0.0067 |          63.5929 |           7.2039 |
[32m[20221213 23:45:48 @agent_ppo2.py:185][0m |          -0.0114 |          61.7769 |           7.1783 |
[32m[20221213 23:45:48 @agent_ppo2.py:185][0m |          -0.0001 |          64.5695 |           7.2067 |
[32m[20221213 23:45:49 @agent_ppo2.py:185][0m |          -0.0152 |          60.2851 |           7.2057 |
[32m[20221213 23:45:49 @agent_ppo2.py:185][0m |          -0.0146 |          59.7597 |           7.1807 |
[32m[20221213 23:45:49 @agent_ppo2.py:185][0m |          -0.0160 |          59.2200 |           7.1542 |
[32m[20221213 23:45:49 @agent_ppo2.py:185][0m |          -0.0159 |          58.9218 |           7.1777 |
[32m[20221213 23:45:49 @agent_ppo2.py:185][0m |          -0.0120 |          58.8956 |           7.1610 |
[32m[20221213 23:45:49 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:45:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.80
[32m[20221213 23:45:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.34
[32m[20221213 23:45:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.37
[32m[20221213 23:45:49 @agent_ppo2.py:143][0m Total time:      33.28 min
[32m[20221213 23:45:49 @agent_ppo2.py:145][0m 3223552 total steps have happened
[32m[20221213 23:45:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3574 --------------------------#
[32m[20221213 23:45:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:45:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:49 @agent_ppo2.py:185][0m |           0.0100 |          59.8167 |           7.8476 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |           0.0037 |          53.2347 |           7.9019 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |           0.0029 |          53.6538 |           7.8581 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |          -0.0073 |          50.7841 |           7.8486 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |          -0.0136 |          49.7511 |           7.7967 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |          -0.0142 |          49.2543 |           7.8330 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |          -0.0005 |          56.2187 |           7.8240 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |          -0.0116 |          50.9658 |           7.8618 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |          -0.0212 |          48.0006 |           7.8130 |
[32m[20221213 23:45:50 @agent_ppo2.py:185][0m |          -0.0178 |          47.6687 |           7.8599 |
[32m[20221213 23:45:50 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:45:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.51
[32m[20221213 23:45:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.53
[32m[20221213 23:45:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.31
[32m[20221213 23:45:50 @agent_ppo2.py:143][0m Total time:      33.31 min
[32m[20221213 23:45:50 @agent_ppo2.py:145][0m 3225600 total steps have happened
[32m[20221213 23:45:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3575 --------------------------#
[32m[20221213 23:45:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0013 |          62.6497 |           6.5014 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0071 |          59.2153 |           6.5861 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0120 |          57.7659 |           6.6197 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0136 |          57.1160 |           6.6041 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0151 |          56.5677 |           6.6219 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0161 |          56.1976 |           6.6266 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0157 |          55.7343 |           6.6722 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0145 |          55.5341 |           6.6238 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0135 |          56.1449 |           6.6462 |
[32m[20221213 23:45:51 @agent_ppo2.py:185][0m |          -0.0176 |          55.0161 |           6.6951 |
[32m[20221213 23:45:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:45:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.28
[32m[20221213 23:45:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.75
[32m[20221213 23:45:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.15
[32m[20221213 23:45:52 @agent_ppo2.py:143][0m Total time:      33.33 min
[32m[20221213 23:45:52 @agent_ppo2.py:145][0m 3227648 total steps have happened
[32m[20221213 23:45:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3576 --------------------------#
[32m[20221213 23:45:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:52 @agent_ppo2.py:185][0m |          -0.0009 |          57.7952 |           7.8575 |
[32m[20221213 23:45:52 @agent_ppo2.py:185][0m |          -0.0031 |          52.7360 |           7.8220 |
[32m[20221213 23:45:52 @agent_ppo2.py:185][0m |          -0.0114 |          50.5262 |           7.8179 |
[32m[20221213 23:45:52 @agent_ppo2.py:185][0m |          -0.0107 |          49.5279 |           7.8298 |
[32m[20221213 23:45:52 @agent_ppo2.py:185][0m |          -0.0153 |          48.8465 |           7.7995 |
[32m[20221213 23:45:52 @agent_ppo2.py:185][0m |          -0.0058 |          50.0139 |           7.7651 |
[32m[20221213 23:45:52 @agent_ppo2.py:185][0m |          -0.0139 |          47.7116 |           7.7591 |
[32m[20221213 23:45:53 @agent_ppo2.py:185][0m |          -0.0088 |          48.4562 |           7.7630 |
[32m[20221213 23:45:53 @agent_ppo2.py:185][0m |          -0.0098 |          47.8873 |           7.7519 |
[32m[20221213 23:45:53 @agent_ppo2.py:185][0m |          -0.0210 |          46.7622 |           7.7736 |
[32m[20221213 23:45:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:45:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.74
[32m[20221213 23:45:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.98
[32m[20221213 23:45:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.55
[32m[20221213 23:45:53 @agent_ppo2.py:143][0m Total time:      33.35 min
[32m[20221213 23:45:53 @agent_ppo2.py:145][0m 3229696 total steps have happened
[32m[20221213 23:45:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3577 --------------------------#
[32m[20221213 23:45:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:53 @agent_ppo2.py:185][0m |          -0.0026 |          44.2065 |           7.4860 |
[32m[20221213 23:45:53 @agent_ppo2.py:185][0m |          -0.0076 |          40.7952 |           7.5162 |
[32m[20221213 23:45:53 @agent_ppo2.py:185][0m |          -0.0058 |          39.7984 |           7.5738 |
[32m[20221213 23:45:53 @agent_ppo2.py:185][0m |          -0.0033 |          42.7964 |           7.6180 |
[32m[20221213 23:45:54 @agent_ppo2.py:185][0m |          -0.0071 |          39.0867 |           7.6383 |
[32m[20221213 23:45:54 @agent_ppo2.py:185][0m |          -0.0159 |          37.7173 |           7.7251 |
[32m[20221213 23:45:54 @agent_ppo2.py:185][0m |          -0.0154 |          37.5581 |           7.7284 |
[32m[20221213 23:45:54 @agent_ppo2.py:185][0m |          -0.0123 |          37.2802 |           7.7271 |
[32m[20221213 23:45:54 @agent_ppo2.py:185][0m |          -0.0159 |          36.8382 |           7.8222 |
[32m[20221213 23:45:54 @agent_ppo2.py:185][0m |          -0.0153 |          36.5856 |           7.8195 |
[32m[20221213 23:45:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:45:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.17
[32m[20221213 23:45:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.87
[32m[20221213 23:45:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 183.77
[32m[20221213 23:45:54 @agent_ppo2.py:143][0m Total time:      33.37 min
[32m[20221213 23:45:54 @agent_ppo2.py:145][0m 3231744 total steps have happened
[32m[20221213 23:45:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3578 --------------------------#
[32m[20221213 23:45:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:54 @agent_ppo2.py:185][0m |           0.0009 |          62.0980 |           7.4975 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0042 |          57.6303 |           7.4708 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |           0.0062 |          64.4911 |           7.4729 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0103 |          55.2904 |           7.4863 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0138 |          54.1320 |           7.4556 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0119 |          53.5296 |           7.4565 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0058 |          53.7236 |           7.4267 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0113 |          52.6974 |           7.3445 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0207 |          52.0788 |           7.4048 |
[32m[20221213 23:45:55 @agent_ppo2.py:185][0m |          -0.0179 |          51.5456 |           7.3474 |
[32m[20221213 23:45:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:45:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.37
[32m[20221213 23:45:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.36
[32m[20221213 23:45:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.58
[32m[20221213 23:45:55 @agent_ppo2.py:143][0m Total time:      33.39 min
[32m[20221213 23:45:55 @agent_ppo2.py:145][0m 3233792 total steps have happened
[32m[20221213 23:45:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3579 --------------------------#
[32m[20221213 23:45:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:45:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |           0.0057 |          62.4479 |           7.7594 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0007 |          55.9826 |           7.8212 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0089 |          53.8497 |           7.7704 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0082 |          52.7880 |           7.7854 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |           0.0009 |          56.6222 |           7.7959 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0075 |          51.6145 |           7.7507 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0142 |          51.1535 |           7.7568 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0202 |          50.8235 |           7.7955 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0174 |          50.8170 |           7.8272 |
[32m[20221213 23:45:56 @agent_ppo2.py:185][0m |          -0.0141 |          50.5150 |           7.7601 |
[32m[20221213 23:45:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:45:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.15
[32m[20221213 23:45:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.73
[32m[20221213 23:45:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.93
[32m[20221213 23:45:57 @agent_ppo2.py:143][0m Total time:      33.41 min
[32m[20221213 23:45:57 @agent_ppo2.py:145][0m 3235840 total steps have happened
[32m[20221213 23:45:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3580 --------------------------#
[32m[20221213 23:45:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:45:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:57 @agent_ppo2.py:185][0m |           0.0025 |          68.3302 |           7.4761 |
[32m[20221213 23:45:57 @agent_ppo2.py:185][0m |          -0.0028 |          66.0429 |           7.4289 |
[32m[20221213 23:45:57 @agent_ppo2.py:185][0m |          -0.0065 |          65.2816 |           7.5114 |
[32m[20221213 23:45:57 @agent_ppo2.py:185][0m |          -0.0054 |          64.6849 |           7.5159 |
[32m[20221213 23:45:57 @agent_ppo2.py:185][0m |          -0.0046 |          64.3009 |           7.5734 |
[32m[20221213 23:45:57 @agent_ppo2.py:185][0m |          -0.0035 |          64.0432 |           7.5955 |
[32m[20221213 23:45:57 @agent_ppo2.py:185][0m |           0.0011 |          66.3424 |           7.4856 |
[32m[20221213 23:45:58 @agent_ppo2.py:185][0m |          -0.0072 |          63.7590 |           7.5751 |
[32m[20221213 23:45:58 @agent_ppo2.py:185][0m |          -0.0120 |          63.6407 |           7.6319 |
[32m[20221213 23:45:58 @agent_ppo2.py:185][0m |           0.0030 |          70.6270 |           7.6153 |
[32m[20221213 23:45:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:45:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.91
[32m[20221213 23:45:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.49
[32m[20221213 23:45:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.85
[32m[20221213 23:45:58 @agent_ppo2.py:143][0m Total time:      33.43 min
[32m[20221213 23:45:58 @agent_ppo2.py:145][0m 3237888 total steps have happened
[32m[20221213 23:45:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3581 --------------------------#
[32m[20221213 23:45:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:58 @agent_ppo2.py:185][0m |           0.0026 |          75.8759 |           7.4308 |
[32m[20221213 23:45:58 @agent_ppo2.py:185][0m |          -0.0045 |          73.3797 |           7.3138 |
[32m[20221213 23:45:58 @agent_ppo2.py:185][0m |          -0.0078 |          72.3320 |           7.4288 |
[32m[20221213 23:45:58 @agent_ppo2.py:185][0m |          -0.0052 |          71.9824 |           7.3860 |
[32m[20221213 23:45:59 @agent_ppo2.py:185][0m |           0.0026 |          74.5328 |           7.4253 |
[32m[20221213 23:45:59 @agent_ppo2.py:185][0m |          -0.0094 |          71.0827 |           7.4072 |
[32m[20221213 23:45:59 @agent_ppo2.py:185][0m |           0.0015 |          79.1756 |           7.4231 |
[32m[20221213 23:45:59 @agent_ppo2.py:185][0m |          -0.0039 |          73.2928 |           7.4508 |
[32m[20221213 23:45:59 @agent_ppo2.py:185][0m |          -0.0074 |          71.1863 |           7.4104 |
[32m[20221213 23:45:59 @agent_ppo2.py:185][0m |          -0.0046 |          73.9174 |           7.4347 |
[32m[20221213 23:45:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:45:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.00
[32m[20221213 23:45:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.40
[32m[20221213 23:45:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.01
[32m[20221213 23:45:59 @agent_ppo2.py:143][0m Total time:      33.45 min
[32m[20221213 23:45:59 @agent_ppo2.py:145][0m 3239936 total steps have happened
[32m[20221213 23:45:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3582 --------------------------#
[32m[20221213 23:45:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:45:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:45:59 @agent_ppo2.py:185][0m |           0.0127 |          58.7635 |           7.4210 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0040 |          46.9785 |           7.4944 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0083 |          44.8243 |           7.5540 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |           0.0039 |          49.7142 |           7.5508 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0113 |          42.1002 |           7.5206 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0100 |          41.1209 |           7.5313 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0174 |          40.4076 |           7.5964 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0118 |          39.8566 |           7.4998 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0134 |          39.8236 |           7.5721 |
[32m[20221213 23:46:00 @agent_ppo2.py:185][0m |          -0.0137 |          38.9984 |           7.5746 |
[32m[20221213 23:46:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.71
[32m[20221213 23:46:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.72
[32m[20221213 23:46:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.15
[32m[20221213 23:46:00 @agent_ppo2.py:143][0m Total time:      33.47 min
[32m[20221213 23:46:00 @agent_ppo2.py:145][0m 3241984 total steps have happened
[32m[20221213 23:46:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3583 --------------------------#
[32m[20221213 23:46:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |           0.0055 |          27.3570 |           8.0595 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0036 |          20.1764 |           8.0135 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0116 |          19.1365 |           8.0505 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0051 |          18.3634 |           8.0086 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0121 |          17.6533 |           7.9983 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0138 |          17.2003 |           7.9995 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0168 |          16.9456 |           7.9389 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0116 |          16.7825 |           7.9397 |
[32m[20221213 23:46:01 @agent_ppo2.py:185][0m |          -0.0160 |          16.9609 |           7.9297 |
[32m[20221213 23:46:02 @agent_ppo2.py:185][0m |          -0.0199 |          16.5074 |           7.8661 |
[32m[20221213 23:46:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:46:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.20
[32m[20221213 23:46:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.38
[32m[20221213 23:46:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.59
[32m[20221213 23:46:02 @agent_ppo2.py:143][0m Total time:      33.49 min
[32m[20221213 23:46:02 @agent_ppo2.py:145][0m 3244032 total steps have happened
[32m[20221213 23:46:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3584 --------------------------#
[32m[20221213 23:46:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:02 @agent_ppo2.py:185][0m |          -0.0021 |          59.5325 |           7.8223 |
[32m[20221213 23:46:02 @agent_ppo2.py:185][0m |           0.0000 |          57.8288 |           7.7987 |
[32m[20221213 23:46:02 @agent_ppo2.py:185][0m |          -0.0048 |          56.8595 |           7.8634 |
[32m[20221213 23:46:02 @agent_ppo2.py:185][0m |           0.0051 |          58.2640 |           7.8142 |
[32m[20221213 23:46:02 @agent_ppo2.py:185][0m |          -0.0033 |          57.6710 |           7.8514 |
[32m[20221213 23:46:02 @agent_ppo2.py:185][0m |          -0.0114 |          55.9789 |           7.9060 |
[32m[20221213 23:46:03 @agent_ppo2.py:185][0m |          -0.0091 |          55.6552 |           7.8817 |
[32m[20221213 23:46:03 @agent_ppo2.py:185][0m |          -0.0091 |          55.4891 |           7.8856 |
[32m[20221213 23:46:03 @agent_ppo2.py:185][0m |          -0.0131 |          55.4934 |           7.8722 |
[32m[20221213 23:46:03 @agent_ppo2.py:185][0m |          -0.0119 |          55.1983 |           7.9280 |
[32m[20221213 23:46:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.04
[32m[20221213 23:46:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.93
[32m[20221213 23:46:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.73
[32m[20221213 23:46:03 @agent_ppo2.py:143][0m Total time:      33.52 min
[32m[20221213 23:46:03 @agent_ppo2.py:145][0m 3246080 total steps have happened
[32m[20221213 23:46:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3585 --------------------------#
[32m[20221213 23:46:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:03 @agent_ppo2.py:185][0m |           0.0047 |          32.5613 |           7.7004 |
[32m[20221213 23:46:03 @agent_ppo2.py:185][0m |          -0.0053 |          27.3067 |           7.7763 |
[32m[20221213 23:46:03 @agent_ppo2.py:185][0m |           0.0004 |          26.9114 |           7.8566 |
[32m[20221213 23:46:04 @agent_ppo2.py:185][0m |          -0.0182 |          25.4428 |           7.8620 |
[32m[20221213 23:46:04 @agent_ppo2.py:185][0m |          -0.0133 |          24.9797 |           7.9068 |
[32m[20221213 23:46:04 @agent_ppo2.py:185][0m |          -0.0159 |          24.2958 |           7.8913 |
[32m[20221213 23:46:04 @agent_ppo2.py:185][0m |          -0.0111 |          24.0251 |           8.0310 |
[32m[20221213 23:46:04 @agent_ppo2.py:185][0m |          -0.0154 |          24.1066 |           8.0011 |
[32m[20221213 23:46:04 @agent_ppo2.py:185][0m |          -0.0228 |          23.5293 |           8.0702 |
[32m[20221213 23:46:04 @agent_ppo2.py:185][0m |          -0.0248 |          23.2421 |           8.1007 |
[32m[20221213 23:46:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.72
[32m[20221213 23:46:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.57
[32m[20221213 23:46:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.42
[32m[20221213 23:46:04 @agent_ppo2.py:143][0m Total time:      33.54 min
[32m[20221213 23:46:04 @agent_ppo2.py:145][0m 3248128 total steps have happened
[32m[20221213 23:46:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3586 --------------------------#
[32m[20221213 23:46:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |           0.0001 |          60.0620 |           8.0504 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |           0.0116 |          64.8639 |           8.0154 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0074 |          58.5338 |           8.0934 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0112 |          57.5839 |           8.0742 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0095 |          57.3777 |           8.0947 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0105 |          57.1951 |           8.0974 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0046 |          60.7394 |           8.1261 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0090 |          56.9751 |           8.0766 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0133 |          56.5639 |           8.0807 |
[32m[20221213 23:46:05 @agent_ppo2.py:185][0m |          -0.0012 |          59.6511 |           8.0884 |
[32m[20221213 23:46:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.53
[32m[20221213 23:46:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.87
[32m[20221213 23:46:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.99
[32m[20221213 23:46:05 @agent_ppo2.py:143][0m Total time:      33.56 min
[32m[20221213 23:46:05 @agent_ppo2.py:145][0m 3250176 total steps have happened
[32m[20221213 23:46:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3587 --------------------------#
[32m[20221213 23:46:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |           0.0096 |          50.8992 |           7.9679 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0045 |          44.5054 |           8.0660 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0066 |          42.4257 |           8.0693 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0068 |          40.7754 |           8.0955 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0075 |          39.7838 |           8.0935 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0153 |          38.9882 |           8.1373 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0112 |          38.2567 |           8.1335 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0110 |          37.8658 |           8.1376 |
[32m[20221213 23:46:06 @agent_ppo2.py:185][0m |          -0.0101 |          37.4474 |           8.1831 |
[32m[20221213 23:46:07 @agent_ppo2.py:185][0m |          -0.0146 |          37.0410 |           8.2138 |
[32m[20221213 23:46:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.52
[32m[20221213 23:46:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.21
[32m[20221213 23:46:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.40
[32m[20221213 23:46:07 @agent_ppo2.py:143][0m Total time:      33.58 min
[32m[20221213 23:46:07 @agent_ppo2.py:145][0m 3252224 total steps have happened
[32m[20221213 23:46:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3588 --------------------------#
[32m[20221213 23:46:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:07 @agent_ppo2.py:185][0m |           0.0074 |          30.1631 |           8.3228 |
[32m[20221213 23:46:07 @agent_ppo2.py:185][0m |          -0.0065 |          25.0350 |           8.3184 |
[32m[20221213 23:46:07 @agent_ppo2.py:185][0m |          -0.0094 |          22.8099 |           8.2345 |
[32m[20221213 23:46:07 @agent_ppo2.py:185][0m |          -0.0089 |          21.8898 |           8.1937 |
[32m[20221213 23:46:07 @agent_ppo2.py:185][0m |          -0.0073 |          21.0350 |           8.1916 |
[32m[20221213 23:46:07 @agent_ppo2.py:185][0m |          -0.0132 |          20.4134 |           8.2694 |
[32m[20221213 23:46:08 @agent_ppo2.py:185][0m |          -0.0171 |          20.0747 |           8.2064 |
[32m[20221213 23:46:08 @agent_ppo2.py:185][0m |          -0.0126 |          19.6064 |           8.2134 |
[32m[20221213 23:46:08 @agent_ppo2.py:185][0m |          -0.0179 |          19.1868 |           8.1479 |
[32m[20221213 23:46:08 @agent_ppo2.py:185][0m |          -0.0228 |          18.9418 |           8.1453 |
[32m[20221213 23:46:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:46:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.14
[32m[20221213 23:46:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.34
[32m[20221213 23:46:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.78
[32m[20221213 23:46:08 @agent_ppo2.py:143][0m Total time:      33.60 min
[32m[20221213 23:46:08 @agent_ppo2.py:145][0m 3254272 total steps have happened
[32m[20221213 23:46:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3589 --------------------------#
[32m[20221213 23:46:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:08 @agent_ppo2.py:185][0m |           0.0003 |          82.3434 |           8.3435 |
[32m[20221213 23:46:08 @agent_ppo2.py:185][0m |          -0.0058 |          76.1381 |           8.3710 |
[32m[20221213 23:46:08 @agent_ppo2.py:185][0m |           0.0020 |          80.6999 |           8.3729 |
[32m[20221213 23:46:09 @agent_ppo2.py:185][0m |          -0.0085 |          74.2570 |           8.3715 |
[32m[20221213 23:46:09 @agent_ppo2.py:185][0m |          -0.0139 |          73.1069 |           8.4221 |
[32m[20221213 23:46:09 @agent_ppo2.py:185][0m |          -0.0134 |          72.0481 |           8.4144 |
[32m[20221213 23:46:09 @agent_ppo2.py:185][0m |          -0.0138 |          71.4734 |           8.4448 |
[32m[20221213 23:46:09 @agent_ppo2.py:185][0m |          -0.0161 |          71.4135 |           8.4249 |
[32m[20221213 23:46:09 @agent_ppo2.py:185][0m |          -0.0169 |          70.7298 |           8.4716 |
[32m[20221213 23:46:09 @agent_ppo2.py:185][0m |          -0.0201 |          70.4931 |           8.5149 |
[32m[20221213 23:46:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.29
[32m[20221213 23:46:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.01
[32m[20221213 23:46:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.73
[32m[20221213 23:46:09 @agent_ppo2.py:143][0m Total time:      33.62 min
[32m[20221213 23:46:09 @agent_ppo2.py:145][0m 3256320 total steps have happened
[32m[20221213 23:46:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3590 --------------------------#
[32m[20221213 23:46:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:46:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |           0.0139 |          64.3065 |           8.3006 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0049 |          53.0082 |           8.3579 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0064 |          49.9394 |           8.3996 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0112 |          48.9725 |           8.4315 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0142 |          48.0774 |           8.4674 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0185 |          47.6524 |           8.4574 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0132 |          46.8542 |           8.4218 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0181 |          46.1193 |           8.4470 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0172 |          45.8044 |           8.4685 |
[32m[20221213 23:46:10 @agent_ppo2.py:185][0m |          -0.0140 |          45.1688 |           8.4979 |
[32m[20221213 23:46:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.77
[32m[20221213 23:46:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.40
[32m[20221213 23:46:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.00
[32m[20221213 23:46:10 @agent_ppo2.py:143][0m Total time:      33.64 min
[32m[20221213 23:46:10 @agent_ppo2.py:145][0m 3258368 total steps have happened
[32m[20221213 23:46:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3591 --------------------------#
[32m[20221213 23:46:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0011 |          67.6533 |           8.7475 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0084 |          63.0008 |           8.6677 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0084 |          60.9793 |           8.6902 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |           0.0002 |          69.8443 |           8.5967 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0108 |          59.6011 |           8.6250 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0113 |          58.1022 |           8.6565 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0173 |          57.5680 |           8.6689 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0161 |          56.7653 |           8.5903 |
[32m[20221213 23:46:11 @agent_ppo2.py:185][0m |          -0.0139 |          56.1886 |           8.5464 |
[32m[20221213 23:46:12 @agent_ppo2.py:185][0m |          -0.0071 |          57.8577 |           8.5955 |
[32m[20221213 23:46:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.58
[32m[20221213 23:46:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.37
[32m[20221213 23:46:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.15
[32m[20221213 23:46:12 @agent_ppo2.py:143][0m Total time:      33.66 min
[32m[20221213 23:46:12 @agent_ppo2.py:145][0m 3260416 total steps have happened
[32m[20221213 23:46:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3592 --------------------------#
[32m[20221213 23:46:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:12 @agent_ppo2.py:185][0m |           0.0057 |          57.9275 |           8.2580 |
[32m[20221213 23:46:12 @agent_ppo2.py:185][0m |          -0.0045 |          54.4106 |           8.3481 |
[32m[20221213 23:46:12 @agent_ppo2.py:185][0m |          -0.0084 |          53.3985 |           8.4026 |
[32m[20221213 23:46:12 @agent_ppo2.py:185][0m |          -0.0041 |          53.3307 |           8.3667 |
[32m[20221213 23:46:12 @agent_ppo2.py:185][0m |          -0.0093 |          52.7086 |           8.3952 |
[32m[20221213 23:46:12 @agent_ppo2.py:185][0m |          -0.0086 |          52.3122 |           8.5099 |
[32m[20221213 23:46:13 @agent_ppo2.py:185][0m |          -0.0084 |          52.6590 |           8.4540 |
[32m[20221213 23:46:13 @agent_ppo2.py:185][0m |          -0.0109 |          51.9808 |           8.4735 |
[32m[20221213 23:46:13 @agent_ppo2.py:185][0m |          -0.0125 |          51.8832 |           8.4404 |
[32m[20221213 23:46:13 @agent_ppo2.py:185][0m |           0.0006 |          54.2740 |           8.4848 |
[32m[20221213 23:46:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.37
[32m[20221213 23:46:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.86
[32m[20221213 23:46:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.62
[32m[20221213 23:46:13 @agent_ppo2.py:143][0m Total time:      33.68 min
[32m[20221213 23:46:13 @agent_ppo2.py:145][0m 3262464 total steps have happened
[32m[20221213 23:46:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3593 --------------------------#
[32m[20221213 23:46:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:13 @agent_ppo2.py:185][0m |           0.0018 |          49.5486 |           8.5733 |
[32m[20221213 23:46:13 @agent_ppo2.py:185][0m |          -0.0071 |          44.4573 |           8.5775 |
[32m[20221213 23:46:13 @agent_ppo2.py:185][0m |          -0.0073 |          41.9293 |           8.5864 |
[32m[20221213 23:46:14 @agent_ppo2.py:185][0m |          -0.0144 |          40.3204 |           8.5235 |
[32m[20221213 23:46:14 @agent_ppo2.py:185][0m |          -0.0156 |          39.2213 |           8.5384 |
[32m[20221213 23:46:14 @agent_ppo2.py:185][0m |          -0.0119 |          37.9123 |           8.4994 |
[32m[20221213 23:46:14 @agent_ppo2.py:185][0m |          -0.0163 |          36.8477 |           8.5128 |
[32m[20221213 23:46:14 @agent_ppo2.py:185][0m |          -0.0198 |          36.2947 |           8.4949 |
[32m[20221213 23:46:14 @agent_ppo2.py:185][0m |          -0.0185 |          35.9619 |           8.4904 |
[32m[20221213 23:46:14 @agent_ppo2.py:185][0m |          -0.0192 |          35.2714 |           8.4773 |
[32m[20221213 23:46:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:46:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.92
[32m[20221213 23:46:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.20
[32m[20221213 23:46:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.07
[32m[20221213 23:46:14 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 602.07
[32m[20221213 23:46:14 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 602.07
[32m[20221213 23:46:14 @agent_ppo2.py:143][0m Total time:      33.70 min
[32m[20221213 23:46:14 @agent_ppo2.py:145][0m 3264512 total steps have happened
[32m[20221213 23:46:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3594 --------------------------#
[32m[20221213 23:46:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0018 |          57.7350 |           8.4573 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0074 |          52.1666 |           8.4378 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0100 |          50.5290 |           8.6047 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0048 |          54.1208 |           8.5752 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0138 |          49.0191 |           8.5869 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0116 |          48.1980 |           8.6066 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0007 |          52.6849 |           8.6418 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0126 |          47.4827 |           8.6318 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0129 |          47.0782 |           8.6648 |
[32m[20221213 23:46:15 @agent_ppo2.py:185][0m |          -0.0158 |          47.0136 |           8.7503 |
[32m[20221213 23:46:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.96
[32m[20221213 23:46:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.60
[32m[20221213 23:46:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.02
[32m[20221213 23:46:15 @agent_ppo2.py:143][0m Total time:      33.72 min
[32m[20221213 23:46:15 @agent_ppo2.py:145][0m 3266560 total steps have happened
[32m[20221213 23:46:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3595 --------------------------#
[32m[20221213 23:46:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |           0.0029 |          44.6419 |           8.7531 |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |          -0.0060 |          40.3121 |           8.7177 |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |          -0.0034 |          38.7372 |           8.7096 |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |          -0.0102 |          37.7024 |           8.7149 |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |          -0.0041 |          37.0639 |           8.7198 |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |          -0.0147 |          36.6400 |           8.7111 |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |          -0.0175 |          36.1005 |           8.7180 |
[32m[20221213 23:46:16 @agent_ppo2.py:185][0m |          -0.0196 |          36.0526 |           8.7524 |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |          -0.0069 |          38.1939 |           8.7615 |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |          -0.0215 |          35.5549 |           8.7464 |
[32m[20221213 23:46:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.36
[32m[20221213 23:46:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.08
[32m[20221213 23:46:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.55
[32m[20221213 23:46:17 @agent_ppo2.py:143][0m Total time:      33.75 min
[32m[20221213 23:46:17 @agent_ppo2.py:145][0m 3268608 total steps have happened
[32m[20221213 23:46:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3596 --------------------------#
[32m[20221213 23:46:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |           0.0003 |          59.5131 |           8.7612 |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |           0.0007 |          55.4959 |           8.8509 |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |          -0.0073 |          53.5978 |           8.7760 |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |          -0.0135 |          52.5571 |           8.8609 |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |          -0.0062 |          52.3408 |           8.8293 |
[32m[20221213 23:46:17 @agent_ppo2.py:185][0m |           0.0008 |          55.0713 |           8.9082 |
[32m[20221213 23:46:18 @agent_ppo2.py:185][0m |          -0.0108 |          51.0045 |           8.9051 |
[32m[20221213 23:46:18 @agent_ppo2.py:185][0m |          -0.0153 |          50.3518 |           8.9537 |
[32m[20221213 23:46:18 @agent_ppo2.py:185][0m |          -0.0139 |          50.0315 |           8.9471 |
[32m[20221213 23:46:18 @agent_ppo2.py:185][0m |          -0.0125 |          49.7092 |           8.9499 |
[32m[20221213 23:46:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.27
[32m[20221213 23:46:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.71
[32m[20221213 23:46:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.39
[32m[20221213 23:46:18 @agent_ppo2.py:143][0m Total time:      33.77 min
[32m[20221213 23:46:18 @agent_ppo2.py:145][0m 3270656 total steps have happened
[32m[20221213 23:46:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3597 --------------------------#
[32m[20221213 23:46:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:18 @agent_ppo2.py:185][0m |           0.0010 |          77.7128 |           8.8948 |
[32m[20221213 23:46:18 @agent_ppo2.py:185][0m |          -0.0080 |          74.0441 |           8.8764 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0035 |          73.0322 |           8.8328 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0108 |          72.1216 |           8.7981 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0102 |          71.5918 |           8.8925 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0099 |          71.1239 |           8.8300 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0141 |          70.5181 |           8.7950 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0119 |          70.7396 |           8.7889 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0139 |          70.0806 |           8.7852 |
[32m[20221213 23:46:19 @agent_ppo2.py:185][0m |          -0.0168 |          69.8222 |           8.8064 |
[32m[20221213 23:46:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.81
[32m[20221213 23:46:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.75
[32m[20221213 23:46:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.01
[32m[20221213 23:46:19 @agent_ppo2.py:143][0m Total time:      33.79 min
[32m[20221213 23:46:19 @agent_ppo2.py:145][0m 3272704 total steps have happened
[32m[20221213 23:46:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3598 --------------------------#
[32m[20221213 23:46:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0004 |          49.6587 |           8.8521 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0024 |          45.0550 |           8.8252 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0038 |          43.7119 |           8.8119 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0037 |          43.5366 |           8.8227 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0122 |          42.3133 |           8.8167 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0145 |          41.4811 |           8.7984 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0124 |          41.5128 |           8.7591 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0133 |          40.3756 |           8.7734 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0124 |          39.9618 |           8.7995 |
[32m[20221213 23:46:20 @agent_ppo2.py:185][0m |          -0.0167 |          39.6548 |           8.7453 |
[32m[20221213 23:46:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:46:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.21
[32m[20221213 23:46:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.94
[32m[20221213 23:46:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.52
[32m[20221213 23:46:21 @agent_ppo2.py:143][0m Total time:      33.81 min
[32m[20221213 23:46:21 @agent_ppo2.py:145][0m 3274752 total steps have happened
[32m[20221213 23:46:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3599 --------------------------#
[32m[20221213 23:46:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |           0.0022 |          56.3788 |           8.4338 |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |          -0.0034 |          53.7111 |           8.4446 |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |          -0.0110 |          52.9293 |           8.3578 |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |          -0.0117 |          52.7044 |           8.3514 |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |          -0.0118 |          52.4522 |           8.3444 |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |          -0.0151 |          52.3530 |           8.3497 |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |          -0.0122 |          51.9985 |           8.3150 |
[32m[20221213 23:46:21 @agent_ppo2.py:185][0m |          -0.0146 |          51.9095 |           8.2674 |
[32m[20221213 23:46:22 @agent_ppo2.py:185][0m |          -0.0116 |          51.7246 |           8.2485 |
[32m[20221213 23:46:22 @agent_ppo2.py:185][0m |           0.0022 |          60.8826 |           8.3004 |
[32m[20221213 23:46:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.79
[32m[20221213 23:46:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.59
[32m[20221213 23:46:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.66
[32m[20221213 23:46:22 @agent_ppo2.py:143][0m Total time:      33.83 min
[32m[20221213 23:46:22 @agent_ppo2.py:145][0m 3276800 total steps have happened
[32m[20221213 23:46:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3600 --------------------------#
[32m[20221213 23:46:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:46:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:22 @agent_ppo2.py:185][0m |          -0.0017 |          55.6550 |           8.2362 |
[32m[20221213 23:46:22 @agent_ppo2.py:185][0m |          -0.0020 |          50.2172 |           8.3034 |
[32m[20221213 23:46:22 @agent_ppo2.py:185][0m |          -0.0032 |          49.9850 |           8.3509 |
[32m[20221213 23:46:22 @agent_ppo2.py:185][0m |          -0.0097 |          46.3114 |           8.3317 |
[32m[20221213 23:46:22 @agent_ppo2.py:185][0m |          -0.0030 |          49.6870 |           8.3240 |
[32m[20221213 23:46:23 @agent_ppo2.py:185][0m |          -0.0075 |          46.5267 |           8.2967 |
[32m[20221213 23:46:23 @agent_ppo2.py:185][0m |          -0.0170 |          44.2190 |           8.2880 |
[32m[20221213 23:46:23 @agent_ppo2.py:185][0m |          -0.0122 |          43.5293 |           8.2603 |
[32m[20221213 23:46:23 @agent_ppo2.py:185][0m |          -0.0170 |          43.2450 |           8.2678 |
[32m[20221213 23:46:23 @agent_ppo2.py:185][0m |          -0.0196 |          42.9825 |           8.2257 |
[32m[20221213 23:46:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.71
[32m[20221213 23:46:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.35
[32m[20221213 23:46:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.78
[32m[20221213 23:46:23 @agent_ppo2.py:143][0m Total time:      33.85 min
[32m[20221213 23:46:23 @agent_ppo2.py:145][0m 3278848 total steps have happened
[32m[20221213 23:46:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3601 --------------------------#
[32m[20221213 23:46:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:23 @agent_ppo2.py:185][0m |           0.0023 |          58.7539 |           8.6157 |
[32m[20221213 23:46:23 @agent_ppo2.py:185][0m |          -0.0052 |          55.4021 |           8.6198 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0080 |          53.7298 |           8.5820 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0032 |          57.6707 |           8.6192 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0139 |          51.9688 |           8.5518 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0122 |          51.0344 |           8.5648 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0137 |          50.7150 |           8.5980 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0144 |          50.1937 |           8.6409 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0176 |          49.9789 |           8.5852 |
[32m[20221213 23:46:24 @agent_ppo2.py:185][0m |          -0.0177 |          49.4869 |           8.5714 |
[32m[20221213 23:46:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.14
[32m[20221213 23:46:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.73
[32m[20221213 23:46:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.77
[32m[20221213 23:46:24 @agent_ppo2.py:143][0m Total time:      33.87 min
[32m[20221213 23:46:24 @agent_ppo2.py:145][0m 3280896 total steps have happened
[32m[20221213 23:46:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3602 --------------------------#
[32m[20221213 23:46:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |           0.0060 |          61.2793 |           8.7506 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |          -0.0020 |          56.8604 |           8.7732 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |          -0.0089 |          54.9303 |           8.7921 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |          -0.0001 |          55.9073 |           8.7606 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |           0.0043 |          55.9781 |           8.7369 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |           0.0086 |          60.0897 |           8.7830 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |          -0.0094 |          52.6178 |           8.8387 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |          -0.0110 |          52.8039 |           8.8714 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |          -0.0099 |          51.8077 |           8.8400 |
[32m[20221213 23:46:25 @agent_ppo2.py:185][0m |          -0.0133 |          51.8298 |           8.8276 |
[32m[20221213 23:46:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:46:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.79
[32m[20221213 23:46:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.60
[32m[20221213 23:46:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.85
[32m[20221213 23:46:26 @agent_ppo2.py:143][0m Total time:      33.89 min
[32m[20221213 23:46:26 @agent_ppo2.py:145][0m 3282944 total steps have happened
[32m[20221213 23:46:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3603 --------------------------#
[32m[20221213 23:46:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |           0.0085 |          69.6332 |           8.2163 |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |          -0.0019 |          64.8371 |           8.2075 |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |          -0.0070 |          63.8921 |           8.2148 |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |          -0.0078 |          63.2823 |           8.2345 |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |          -0.0074 |          63.1451 |           8.2151 |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |          -0.0051 |          63.5284 |           8.2529 |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |          -0.0047 |          62.9179 |           8.2088 |
[32m[20221213 23:46:26 @agent_ppo2.py:185][0m |          -0.0112 |          62.0732 |           8.2207 |
[32m[20221213 23:46:27 @agent_ppo2.py:185][0m |          -0.0114 |          61.8163 |           8.2255 |
[32m[20221213 23:46:27 @agent_ppo2.py:185][0m |          -0.0039 |          65.7500 |           8.2194 |
[32m[20221213 23:46:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.40
[32m[20221213 23:46:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.41
[32m[20221213 23:46:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.16
[32m[20221213 23:46:27 @agent_ppo2.py:143][0m Total time:      33.91 min
[32m[20221213 23:46:27 @agent_ppo2.py:145][0m 3284992 total steps have happened
[32m[20221213 23:46:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3604 --------------------------#
[32m[20221213 23:46:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:27 @agent_ppo2.py:185][0m |           0.0007 |          65.5566 |           7.8738 |
[32m[20221213 23:46:27 @agent_ppo2.py:185][0m |          -0.0048 |          60.4755 |           7.8776 |
[32m[20221213 23:46:27 @agent_ppo2.py:185][0m |          -0.0087 |          58.7468 |           7.8834 |
[32m[20221213 23:46:27 @agent_ppo2.py:185][0m |          -0.0112 |          58.1125 |           7.8633 |
[32m[20221213 23:46:27 @agent_ppo2.py:185][0m |          -0.0158 |          57.3157 |           7.8954 |
[32m[20221213 23:46:28 @agent_ppo2.py:185][0m |          -0.0093 |          56.8447 |           7.8851 |
[32m[20221213 23:46:28 @agent_ppo2.py:185][0m |          -0.0096 |          57.6299 |           7.9505 |
[32m[20221213 23:46:28 @agent_ppo2.py:185][0m |          -0.0165 |          55.9958 |           7.8986 |
[32m[20221213 23:46:28 @agent_ppo2.py:185][0m |          -0.0178 |          55.7352 |           7.9321 |
[32m[20221213 23:46:28 @agent_ppo2.py:185][0m |          -0.0166 |          55.6412 |           7.8886 |
[32m[20221213 23:46:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.69
[32m[20221213 23:46:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.71
[32m[20221213 23:46:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.09
[32m[20221213 23:46:28 @agent_ppo2.py:143][0m Total time:      33.93 min
[32m[20221213 23:46:28 @agent_ppo2.py:145][0m 3287040 total steps have happened
[32m[20221213 23:46:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3605 --------------------------#
[32m[20221213 23:46:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:28 @agent_ppo2.py:185][0m |           0.0025 |          47.2852 |           8.5287 |
[32m[20221213 23:46:28 @agent_ppo2.py:185][0m |          -0.0068 |          42.5136 |           8.4947 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0107 |          40.9370 |           8.5038 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0147 |          40.1012 |           8.5097 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0121 |          40.1241 |           8.5357 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0139 |          38.9650 |           8.5286 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0094 |          40.0067 |           8.5170 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0169 |          38.3524 |           8.4938 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0211 |          38.1649 |           8.4954 |
[32m[20221213 23:46:29 @agent_ppo2.py:185][0m |          -0.0215 |          37.7715 |           8.5126 |
[32m[20221213 23:46:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.05
[32m[20221213 23:46:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.11
[32m[20221213 23:46:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.26
[32m[20221213 23:46:29 @agent_ppo2.py:143][0m Total time:      33.95 min
[32m[20221213 23:46:29 @agent_ppo2.py:145][0m 3289088 total steps have happened
[32m[20221213 23:46:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3606 --------------------------#
[32m[20221213 23:46:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0007 |          59.7862 |           8.3232 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0061 |          53.6432 |           8.4357 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0095 |          51.7891 |           8.4516 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0121 |          50.8676 |           8.4244 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0134 |          50.1925 |           8.5149 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0091 |          49.4631 |           8.5196 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0139 |          48.7523 |           8.5540 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0139 |          48.3483 |           8.5795 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0147 |          47.8748 |           8.6104 |
[32m[20221213 23:46:30 @agent_ppo2.py:185][0m |          -0.0048 |          49.4560 |           8.6728 |
[32m[20221213 23:46:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.61
[32m[20221213 23:46:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.01
[32m[20221213 23:46:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.03
[32m[20221213 23:46:31 @agent_ppo2.py:143][0m Total time:      33.98 min
[32m[20221213 23:46:31 @agent_ppo2.py:145][0m 3291136 total steps have happened
[32m[20221213 23:46:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3607 --------------------------#
[32m[20221213 23:46:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |           0.0028 |          38.9642 |           8.5210 |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |          -0.0007 |          32.8072 |           8.5454 |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |          -0.0009 |          30.8612 |           8.5566 |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |          -0.0095 |          29.4223 |           8.5606 |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |          -0.0098 |          28.6098 |           8.5392 |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |          -0.0189 |          27.8403 |           8.4988 |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |          -0.0171 |          27.2400 |           8.5305 |
[32m[20221213 23:46:31 @agent_ppo2.py:185][0m |          -0.0200 |          26.8161 |           8.5504 |
[32m[20221213 23:46:32 @agent_ppo2.py:185][0m |          -0.0108 |          26.5406 |           8.5026 |
[32m[20221213 23:46:32 @agent_ppo2.py:185][0m |          -0.0170 |          25.9340 |           8.4594 |
[32m[20221213 23:46:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:46:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.43
[32m[20221213 23:46:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.44
[32m[20221213 23:46:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.21
[32m[20221213 23:46:32 @agent_ppo2.py:143][0m Total time:      34.00 min
[32m[20221213 23:46:32 @agent_ppo2.py:145][0m 3293184 total steps have happened
[32m[20221213 23:46:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3608 --------------------------#
[32m[20221213 23:46:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:32 @agent_ppo2.py:185][0m |          -0.0022 |          53.6451 |           8.6085 |
[32m[20221213 23:46:32 @agent_ppo2.py:185][0m |          -0.0039 |          50.1651 |           8.5157 |
[32m[20221213 23:46:32 @agent_ppo2.py:185][0m |          -0.0112 |          49.3356 |           8.4311 |
[32m[20221213 23:46:32 @agent_ppo2.py:185][0m |          -0.0044 |          51.2123 |           8.4659 |
[32m[20221213 23:46:32 @agent_ppo2.py:185][0m |          -0.0116 |          48.4740 |           8.3979 |
[32m[20221213 23:46:33 @agent_ppo2.py:185][0m |          -0.0082 |          48.0723 |           8.4232 |
[32m[20221213 23:46:33 @agent_ppo2.py:185][0m |          -0.0136 |          47.7155 |           8.3949 |
[32m[20221213 23:46:33 @agent_ppo2.py:185][0m |          -0.0125 |          47.8297 |           8.3881 |
[32m[20221213 23:46:33 @agent_ppo2.py:185][0m |          -0.0161 |          47.1221 |           8.3444 |
[32m[20221213 23:46:33 @agent_ppo2.py:185][0m |          -0.0118 |          47.1955 |           8.3576 |
[32m[20221213 23:46:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:46:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.80
[32m[20221213 23:46:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.69
[32m[20221213 23:46:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.53
[32m[20221213 23:46:33 @agent_ppo2.py:143][0m Total time:      34.02 min
[32m[20221213 23:46:33 @agent_ppo2.py:145][0m 3295232 total steps have happened
[32m[20221213 23:46:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3609 --------------------------#
[32m[20221213 23:46:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:33 @agent_ppo2.py:185][0m |          -0.0042 |          19.2210 |           8.1536 |
[32m[20221213 23:46:33 @agent_ppo2.py:185][0m |          -0.0101 |          14.8054 |           8.1482 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0077 |          13.4787 |           8.2255 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0140 |          12.7340 |           8.1787 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0158 |          12.1410 |           8.1344 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0161 |          11.7503 |           8.1841 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0198 |          11.5031 |           8.1558 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0224 |          11.1323 |           8.1649 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0222 |          10.8980 |           8.1870 |
[32m[20221213 23:46:34 @agent_ppo2.py:185][0m |          -0.0201 |          10.6351 |           8.1633 |
[32m[20221213 23:46:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.66
[32m[20221213 23:46:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.39
[32m[20221213 23:46:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.70
[32m[20221213 23:46:34 @agent_ppo2.py:143][0m Total time:      34.04 min
[32m[20221213 23:46:34 @agent_ppo2.py:145][0m 3297280 total steps have happened
[32m[20221213 23:46:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3610 --------------------------#
[32m[20221213 23:46:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:46:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |           0.0095 |          33.4014 |           8.2991 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0052 |          28.5090 |           8.2770 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0109 |          26.9347 |           8.3000 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0129 |          25.9834 |           8.2493 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0113 |          25.4774 |           8.3361 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0094 |          24.8088 |           8.2590 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0166 |          24.3418 |           8.2239 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0082 |          24.2212 |           8.2673 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0162 |          23.9717 |           8.1774 |
[32m[20221213 23:46:35 @agent_ppo2.py:185][0m |          -0.0154 |          23.4338 |           8.1798 |
[32m[20221213 23:46:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.66
[32m[20221213 23:46:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.42
[32m[20221213 23:46:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.74
[32m[20221213 23:46:36 @agent_ppo2.py:143][0m Total time:      34.06 min
[32m[20221213 23:46:36 @agent_ppo2.py:145][0m 3299328 total steps have happened
[32m[20221213 23:46:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3611 --------------------------#
[32m[20221213 23:46:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |           0.0069 |          61.2629 |           8.1751 |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |          -0.0080 |          57.1746 |           8.2432 |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |          -0.0018 |          57.5953 |           8.1931 |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |          -0.0074 |          56.9909 |           8.1989 |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |          -0.0112 |          55.1549 |           8.2463 |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |          -0.0111 |          55.0986 |           8.1395 |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |          -0.0027 |          56.0952 |           8.1540 |
[32m[20221213 23:46:36 @agent_ppo2.py:185][0m |          -0.0145 |          54.5456 |           8.1970 |
[32m[20221213 23:46:37 @agent_ppo2.py:185][0m |          -0.0120 |          54.2987 |           8.1921 |
[32m[20221213 23:46:37 @agent_ppo2.py:185][0m |          -0.0143 |          54.3312 |           8.1585 |
[32m[20221213 23:46:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:46:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.16
[32m[20221213 23:46:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.71
[32m[20221213 23:46:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 111.94
[32m[20221213 23:46:37 @agent_ppo2.py:143][0m Total time:      34.08 min
[32m[20221213 23:46:37 @agent_ppo2.py:145][0m 3301376 total steps have happened
[32m[20221213 23:46:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3612 --------------------------#
[32m[20221213 23:46:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:37 @agent_ppo2.py:185][0m |          -0.0024 |          65.1798 |           8.1496 |
[32m[20221213 23:46:37 @agent_ppo2.py:185][0m |          -0.0043 |          55.1042 |           8.1910 |
[32m[20221213 23:46:37 @agent_ppo2.py:185][0m |           0.0049 |          57.1864 |           8.0871 |
[32m[20221213 23:46:37 @agent_ppo2.py:185][0m |          -0.0014 |          56.6256 |           8.1435 |
[32m[20221213 23:46:37 @agent_ppo2.py:185][0m |          -0.0133 |          49.2743 |           8.0940 |
[32m[20221213 23:46:38 @agent_ppo2.py:185][0m |          -0.0148 |          48.2613 |           8.1395 |
[32m[20221213 23:46:38 @agent_ppo2.py:185][0m |          -0.0113 |          47.7314 |           8.1410 |
[32m[20221213 23:46:38 @agent_ppo2.py:185][0m |          -0.0147 |          46.9940 |           8.1815 |
[32m[20221213 23:46:38 @agent_ppo2.py:185][0m |          -0.0069 |          51.5327 |           8.1884 |
[32m[20221213 23:46:38 @agent_ppo2.py:185][0m |          -0.0147 |          46.3347 |           8.2275 |
[32m[20221213 23:46:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:46:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.12
[32m[20221213 23:46:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.15
[32m[20221213 23:46:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.39
[32m[20221213 23:46:38 @agent_ppo2.py:143][0m Total time:      34.10 min
[32m[20221213 23:46:38 @agent_ppo2.py:145][0m 3303424 total steps have happened
[32m[20221213 23:46:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3613 --------------------------#
[32m[20221213 23:46:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:38 @agent_ppo2.py:185][0m |          -0.0013 |          36.4335 |           7.4381 |
[32m[20221213 23:46:38 @agent_ppo2.py:185][0m |          -0.0056 |          28.1232 |           7.4300 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0107 |          25.7676 |           7.3984 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0015 |          24.6380 |           7.3188 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0089 |          23.4202 |           7.2961 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0016 |          24.3957 |           7.2672 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0076 |          22.6421 |           7.3258 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0130 |          22.0750 |           7.2572 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0172 |          21.7880 |           7.1954 |
[32m[20221213 23:46:39 @agent_ppo2.py:185][0m |          -0.0113 |          21.5264 |           7.1793 |
[32m[20221213 23:46:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.06
[32m[20221213 23:46:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.88
[32m[20221213 23:46:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.04
[32m[20221213 23:46:39 @agent_ppo2.py:143][0m Total time:      34.12 min
[32m[20221213 23:46:39 @agent_ppo2.py:145][0m 3305472 total steps have happened
[32m[20221213 23:46:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3614 --------------------------#
[32m[20221213 23:46:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0020 |          33.0606 |           7.8699 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0078 |          27.4335 |           7.9348 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0048 |          25.4828 |           7.9631 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0065 |          24.5887 |           8.0480 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0107 |          23.9874 |           8.0784 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0081 |          23.4761 |           8.0839 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0142 |          22.9213 |           8.1565 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0129 |          22.6959 |           8.1454 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0129 |          22.6357 |           8.2263 |
[32m[20221213 23:46:40 @agent_ppo2.py:185][0m |          -0.0134 |          22.1651 |           8.2710 |
[32m[20221213 23:46:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.19
[32m[20221213 23:46:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.96
[32m[20221213 23:46:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.63
[32m[20221213 23:46:41 @agent_ppo2.py:143][0m Total time:      34.14 min
[32m[20221213 23:46:41 @agent_ppo2.py:145][0m 3307520 total steps have happened
[32m[20221213 23:46:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3615 --------------------------#
[32m[20221213 23:46:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |           0.0033 |          32.6997 |           7.6675 |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |          -0.0083 |          26.8668 |           7.6642 |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |          -0.0107 |          25.8053 |           7.6776 |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |          -0.0124 |          25.0662 |           7.6390 |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |          -0.0131 |          24.8637 |           7.6319 |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |          -0.0147 |          24.2221 |           7.6111 |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |          -0.0064 |          26.4809 |           7.5901 |
[32m[20221213 23:46:41 @agent_ppo2.py:185][0m |          -0.0224 |          23.5535 |           7.5931 |
[32m[20221213 23:46:42 @agent_ppo2.py:185][0m |          -0.0150 |          23.0476 |           7.5767 |
[32m[20221213 23:46:42 @agent_ppo2.py:185][0m |          -0.0165 |          22.8116 |           7.5505 |
[32m[20221213 23:46:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.89
[32m[20221213 23:46:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.87
[32m[20221213 23:46:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.96
[32m[20221213 23:46:42 @agent_ppo2.py:143][0m Total time:      34.16 min
[32m[20221213 23:46:42 @agent_ppo2.py:145][0m 3309568 total steps have happened
[32m[20221213 23:46:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3616 --------------------------#
[32m[20221213 23:46:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:42 @agent_ppo2.py:185][0m |          -0.0007 |          40.9404 |           8.2067 |
[32m[20221213 23:46:42 @agent_ppo2.py:185][0m |          -0.0085 |          36.6937 |           8.1938 |
[32m[20221213 23:46:42 @agent_ppo2.py:185][0m |          -0.0127 |          35.3139 |           8.1839 |
[32m[20221213 23:46:42 @agent_ppo2.py:185][0m |          -0.0123 |          34.4615 |           8.1341 |
[32m[20221213 23:46:42 @agent_ppo2.py:185][0m |          -0.0149 |          33.6936 |           8.1506 |
[32m[20221213 23:46:43 @agent_ppo2.py:185][0m |          -0.0115 |          33.2609 |           8.1454 |
[32m[20221213 23:46:43 @agent_ppo2.py:185][0m |          -0.0183 |          32.7970 |           8.1502 |
[32m[20221213 23:46:43 @agent_ppo2.py:185][0m |          -0.0196 |          32.4537 |           8.1146 |
[32m[20221213 23:46:43 @agent_ppo2.py:185][0m |          -0.0201 |          32.1329 |           8.1192 |
[32m[20221213 23:46:43 @agent_ppo2.py:185][0m |          -0.0155 |          31.9486 |           8.1263 |
[32m[20221213 23:46:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.72
[32m[20221213 23:46:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.43
[32m[20221213 23:46:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.98
[32m[20221213 23:46:43 @agent_ppo2.py:143][0m Total time:      34.18 min
[32m[20221213 23:46:43 @agent_ppo2.py:145][0m 3311616 total steps have happened
[32m[20221213 23:46:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3617 --------------------------#
[32m[20221213 23:46:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:46:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:43 @agent_ppo2.py:185][0m |           0.0003 |          46.8582 |           7.3858 |
[32m[20221213 23:46:43 @agent_ppo2.py:185][0m |          -0.0102 |          41.9622 |           7.4115 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0059 |          40.0081 |           7.3507 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0008 |          39.5179 |           7.4058 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0091 |          37.8597 |           7.3441 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0038 |          39.3110 |           7.3406 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0131 |          36.7274 |           7.2977 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0118 |          36.4694 |           7.3641 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0159 |          36.0964 |           7.3486 |
[32m[20221213 23:46:44 @agent_ppo2.py:185][0m |          -0.0159 |          35.4805 |           7.2556 |
[32m[20221213 23:46:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:46:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.92
[32m[20221213 23:46:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.37
[32m[20221213 23:46:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.68
[32m[20221213 23:46:44 @agent_ppo2.py:143][0m Total time:      34.21 min
[32m[20221213 23:46:44 @agent_ppo2.py:145][0m 3313664 total steps have happened
[32m[20221213 23:46:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3618 --------------------------#
[32m[20221213 23:46:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |           0.0005 |          53.2098 |           7.4594 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0090 |          46.3837 |           7.5549 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0078 |          44.8934 |           7.5014 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0068 |          43.0201 |           7.5365 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0141 |          41.7164 |           7.5277 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0167 |          40.8679 |           7.5795 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0137 |          40.3334 |           7.5679 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0160 |          39.7394 |           7.5876 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0166 |          39.3713 |           7.5891 |
[32m[20221213 23:46:45 @agent_ppo2.py:185][0m |          -0.0209 |          38.8553 |           7.5425 |
[32m[20221213 23:46:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.82
[32m[20221213 23:46:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.28
[32m[20221213 23:46:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.19
[32m[20221213 23:46:46 @agent_ppo2.py:143][0m Total time:      34.23 min
[32m[20221213 23:46:46 @agent_ppo2.py:145][0m 3315712 total steps have happened
[32m[20221213 23:46:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3619 --------------------------#
[32m[20221213 23:46:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |           0.0210 |          70.9749 |           6.8051 |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |          -0.0083 |          58.3567 |           6.8886 |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |          -0.0101 |          56.7242 |           6.8924 |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |          -0.0122 |          56.0529 |           6.9411 |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |          -0.0147 |          55.5301 |           6.9263 |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |          -0.0112 |          54.9394 |           6.9146 |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |          -0.0093 |          55.7730 |           6.9286 |
[32m[20221213 23:46:46 @agent_ppo2.py:185][0m |          -0.0136 |          54.1745 |           6.8447 |
[32m[20221213 23:46:47 @agent_ppo2.py:185][0m |          -0.0079 |          57.5833 |           6.9480 |
[32m[20221213 23:46:47 @agent_ppo2.py:185][0m |          -0.0163 |          53.6027 |           6.9299 |
[32m[20221213 23:46:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:46:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.15
[32m[20221213 23:46:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.62
[32m[20221213 23:46:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.00
[32m[20221213 23:46:47 @agent_ppo2.py:143][0m Total time:      34.25 min
[32m[20221213 23:46:47 @agent_ppo2.py:145][0m 3317760 total steps have happened
[32m[20221213 23:46:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3620 --------------------------#
[32m[20221213 23:46:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:46:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:47 @agent_ppo2.py:185][0m |          -0.0002 |          69.0196 |           7.2962 |
[32m[20221213 23:46:47 @agent_ppo2.py:185][0m |          -0.0056 |          64.8025 |           7.4075 |
[32m[20221213 23:46:47 @agent_ppo2.py:185][0m |          -0.0090 |          62.6798 |           7.3592 |
[32m[20221213 23:46:47 @agent_ppo2.py:185][0m |          -0.0126 |          61.8030 |           7.4561 |
[32m[20221213 23:46:47 @agent_ppo2.py:185][0m |          -0.0108 |          60.8471 |           7.4723 |
[32m[20221213 23:46:48 @agent_ppo2.py:185][0m |           0.0020 |          65.3399 |           7.4998 |
[32m[20221213 23:46:48 @agent_ppo2.py:185][0m |          -0.0142 |          59.5781 |           7.4887 |
[32m[20221213 23:46:48 @agent_ppo2.py:185][0m |          -0.0125 |          58.9688 |           7.5201 |
[32m[20221213 23:46:48 @agent_ppo2.py:185][0m |          -0.0142 |          58.2686 |           7.5131 |
[32m[20221213 23:46:48 @agent_ppo2.py:185][0m |          -0.0168 |          57.9923 |           7.5606 |
[32m[20221213 23:46:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.35
[32m[20221213 23:46:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.58
[32m[20221213 23:46:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.54
[32m[20221213 23:46:48 @agent_ppo2.py:143][0m Total time:      34.27 min
[32m[20221213 23:46:48 @agent_ppo2.py:145][0m 3319808 total steps have happened
[32m[20221213 23:46:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3621 --------------------------#
[32m[20221213 23:46:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:48 @agent_ppo2.py:185][0m |          -0.0040 |          36.7596 |           7.9582 |
[32m[20221213 23:46:48 @agent_ppo2.py:185][0m |          -0.0033 |          29.0879 |           7.9554 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0085 |          27.0421 |           8.0417 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0165 |          26.0965 |           7.9883 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0179 |          24.8220 |           7.9970 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0166 |          24.1674 |           8.0371 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0121 |          23.5588 |           8.0044 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0221 |          23.0657 |           7.9607 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0120 |          22.7259 |           7.9706 |
[32m[20221213 23:46:49 @agent_ppo2.py:185][0m |          -0.0267 |          22.2773 |           7.9776 |
[32m[20221213 23:46:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:46:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.96
[32m[20221213 23:46:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.72
[32m[20221213 23:46:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.97
[32m[20221213 23:46:49 @agent_ppo2.py:143][0m Total time:      34.29 min
[32m[20221213 23:46:49 @agent_ppo2.py:145][0m 3321856 total steps have happened
[32m[20221213 23:46:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3622 --------------------------#
[32m[20221213 23:46:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |           0.0011 |          64.4084 |           7.2335 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |           0.0006 |          58.4591 |           7.3070 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |           0.0010 |          61.1481 |           7.2786 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |          -0.0053 |          56.1839 |           7.3314 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |          -0.0071 |          55.8245 |           7.2917 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |          -0.0125 |          53.9711 |           7.3469 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |          -0.0134 |          53.5143 |           7.3502 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |          -0.0157 |          53.1285 |           7.3504 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |          -0.0102 |          53.3432 |           7.2879 |
[32m[20221213 23:46:50 @agent_ppo2.py:185][0m |          -0.0144 |          52.7282 |           7.2889 |
[32m[20221213 23:46:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:46:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.80
[32m[20221213 23:46:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.99
[32m[20221213 23:46:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.56
[32m[20221213 23:46:51 @agent_ppo2.py:143][0m Total time:      34.31 min
[32m[20221213 23:46:51 @agent_ppo2.py:145][0m 3323904 total steps have happened
[32m[20221213 23:46:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3623 --------------------------#
[32m[20221213 23:46:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:51 @agent_ppo2.py:185][0m |           0.0002 |          56.9675 |           7.9610 |
[32m[20221213 23:46:51 @agent_ppo2.py:185][0m |          -0.0070 |          52.2262 |           7.9948 |
[32m[20221213 23:46:51 @agent_ppo2.py:185][0m |           0.0032 |          59.6495 |           7.9903 |
[32m[20221213 23:46:51 @agent_ppo2.py:185][0m |          -0.0065 |          49.4099 |           8.0270 |
[32m[20221213 23:46:51 @agent_ppo2.py:185][0m |          -0.0165 |          47.7516 |           8.0756 |
[32m[20221213 23:46:51 @agent_ppo2.py:185][0m |          -0.0104 |          47.4640 |           8.0473 |
[32m[20221213 23:46:51 @agent_ppo2.py:185][0m |          -0.0134 |          46.5697 |           8.0846 |
[32m[20221213 23:46:52 @agent_ppo2.py:185][0m |          -0.0157 |          46.2076 |           8.0525 |
[32m[20221213 23:46:52 @agent_ppo2.py:185][0m |          -0.0150 |          45.8899 |           8.0948 |
[32m[20221213 23:46:52 @agent_ppo2.py:185][0m |          -0.0167 |          45.2709 |           8.1349 |
[32m[20221213 23:46:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.17
[32m[20221213 23:46:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.84
[32m[20221213 23:46:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.11
[32m[20221213 23:46:52 @agent_ppo2.py:143][0m Total time:      34.33 min
[32m[20221213 23:46:52 @agent_ppo2.py:145][0m 3325952 total steps have happened
[32m[20221213 23:46:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3624 --------------------------#
[32m[20221213 23:46:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:52 @agent_ppo2.py:185][0m |           0.0025 |          42.9145 |           7.4279 |
[32m[20221213 23:46:52 @agent_ppo2.py:185][0m |          -0.0046 |          39.1078 |           7.4901 |
[32m[20221213 23:46:52 @agent_ppo2.py:185][0m |           0.0004 |          38.0245 |           7.4628 |
[32m[20221213 23:46:52 @agent_ppo2.py:185][0m |          -0.0054 |          37.2353 |           7.4712 |
[32m[20221213 23:46:53 @agent_ppo2.py:185][0m |          -0.0077 |          36.6882 |           7.4860 |
[32m[20221213 23:46:53 @agent_ppo2.py:185][0m |          -0.0124 |          36.2021 |           7.4294 |
[32m[20221213 23:46:53 @agent_ppo2.py:185][0m |          -0.0056 |          38.3556 |           7.4500 |
[32m[20221213 23:46:53 @agent_ppo2.py:185][0m |          -0.0129 |          35.5987 |           7.4240 |
[32m[20221213 23:46:53 @agent_ppo2.py:185][0m |          -0.0151 |          35.2867 |           7.4326 |
[32m[20221213 23:46:53 @agent_ppo2.py:185][0m |          -0.0159 |          35.1848 |           7.4050 |
[32m[20221213 23:46:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.71
[32m[20221213 23:46:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.37
[32m[20221213 23:46:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.24
[32m[20221213 23:46:53 @agent_ppo2.py:143][0m Total time:      34.35 min
[32m[20221213 23:46:53 @agent_ppo2.py:145][0m 3328000 total steps have happened
[32m[20221213 23:46:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3625 --------------------------#
[32m[20221213 23:46:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:53 @agent_ppo2.py:185][0m |           0.0011 |          61.7122 |           7.5530 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0045 |          59.2180 |           7.6737 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0028 |          59.7023 |           7.6177 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0078 |          56.6796 |           7.6492 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0053 |          58.4887 |           7.6442 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0080 |          55.6854 |           7.6175 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0116 |          55.4875 |           7.6355 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0139 |          54.8924 |           7.5726 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0141 |          54.6442 |           7.5990 |
[32m[20221213 23:46:54 @agent_ppo2.py:185][0m |          -0.0095 |          54.8859 |           7.5699 |
[32m[20221213 23:46:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.58
[32m[20221213 23:46:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.56
[32m[20221213 23:46:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.93
[32m[20221213 23:46:54 @agent_ppo2.py:143][0m Total time:      34.37 min
[32m[20221213 23:46:54 @agent_ppo2.py:145][0m 3330048 total steps have happened
[32m[20221213 23:46:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3626 --------------------------#
[32m[20221213 23:46:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |           0.0018 |          66.5612 |           7.3696 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |          -0.0048 |          62.5016 |           7.3890 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |           0.0018 |          61.4161 |           7.3799 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |          -0.0025 |          60.1902 |           7.3710 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |           0.0013 |          61.3448 |           7.3596 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |          -0.0122 |          57.9459 |           7.2893 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |          -0.0102 |          57.6023 |           7.3166 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |          -0.0134 |          56.9433 |           7.2803 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |          -0.0136 |          55.9573 |           7.2874 |
[32m[20221213 23:46:55 @agent_ppo2.py:185][0m |          -0.0139 |          55.3733 |           7.2317 |
[32m[20221213 23:46:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:46:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.85
[32m[20221213 23:46:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.13
[32m[20221213 23:46:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.93
[32m[20221213 23:46:56 @agent_ppo2.py:143][0m Total time:      34.39 min
[32m[20221213 23:46:56 @agent_ppo2.py:145][0m 3332096 total steps have happened
[32m[20221213 23:46:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3627 --------------------------#
[32m[20221213 23:46:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:56 @agent_ppo2.py:185][0m |           0.0011 |          70.0371 |           7.4122 |
[32m[20221213 23:46:56 @agent_ppo2.py:185][0m |          -0.0027 |          64.9512 |           7.4087 |
[32m[20221213 23:46:56 @agent_ppo2.py:185][0m |          -0.0054 |          63.1233 |           7.4512 |
[32m[20221213 23:46:56 @agent_ppo2.py:185][0m |           0.0041 |          67.2105 |           7.4575 |
[32m[20221213 23:46:56 @agent_ppo2.py:185][0m |          -0.0035 |          61.5264 |           7.4675 |
[32m[20221213 23:46:56 @agent_ppo2.py:185][0m |          -0.0030 |          60.1920 |           7.5315 |
[32m[20221213 23:46:56 @agent_ppo2.py:185][0m |          -0.0107 |          59.6190 |           7.4844 |
[32m[20221213 23:46:57 @agent_ppo2.py:185][0m |          -0.0129 |          59.1168 |           7.5141 |
[32m[20221213 23:46:57 @agent_ppo2.py:185][0m |          -0.0107 |          59.0947 |           7.5499 |
[32m[20221213 23:46:57 @agent_ppo2.py:185][0m |          -0.0122 |          58.9332 |           7.5490 |
[32m[20221213 23:46:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:46:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.30
[32m[20221213 23:46:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.53
[32m[20221213 23:46:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.25
[32m[20221213 23:46:57 @agent_ppo2.py:143][0m Total time:      34.41 min
[32m[20221213 23:46:57 @agent_ppo2.py:145][0m 3334144 total steps have happened
[32m[20221213 23:46:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3628 --------------------------#
[32m[20221213 23:46:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:57 @agent_ppo2.py:185][0m |           0.0017 |          53.5355 |           7.3010 |
[32m[20221213 23:46:57 @agent_ppo2.py:185][0m |          -0.0005 |          44.5948 |           7.3389 |
[32m[20221213 23:46:57 @agent_ppo2.py:185][0m |          -0.0034 |          40.6315 |           7.3092 |
[32m[20221213 23:46:57 @agent_ppo2.py:185][0m |          -0.0089 |          38.4743 |           7.2991 |
[32m[20221213 23:46:58 @agent_ppo2.py:185][0m |          -0.0072 |          36.6106 |           7.3041 |
[32m[20221213 23:46:58 @agent_ppo2.py:185][0m |          -0.0042 |          35.2345 |           7.2881 |
[32m[20221213 23:46:58 @agent_ppo2.py:185][0m |          -0.0135 |          34.2614 |           7.3064 |
[32m[20221213 23:46:58 @agent_ppo2.py:185][0m |          -0.0149 |          33.6988 |           7.3140 |
[32m[20221213 23:46:58 @agent_ppo2.py:185][0m |          -0.0114 |          33.2411 |           7.2185 |
[32m[20221213 23:46:58 @agent_ppo2.py:185][0m |          -0.0132 |          32.3185 |           7.2488 |
[32m[20221213 23:46:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.77
[32m[20221213 23:46:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.21
[32m[20221213 23:46:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.19
[32m[20221213 23:46:58 @agent_ppo2.py:143][0m Total time:      34.44 min
[32m[20221213 23:46:58 @agent_ppo2.py:145][0m 3336192 total steps have happened
[32m[20221213 23:46:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3629 --------------------------#
[32m[20221213 23:46:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:46:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:46:58 @agent_ppo2.py:185][0m |          -0.0016 |          47.8290 |           7.7914 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0049 |          41.4477 |           7.8362 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0042 |          38.6912 |           7.7817 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0122 |          38.0378 |           7.8057 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0108 |          36.8352 |           7.7169 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0102 |          36.5122 |           7.7371 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0099 |          35.6031 |           7.7894 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0167 |          35.1339 |           7.7499 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0134 |          34.8987 |           7.8103 |
[32m[20221213 23:46:59 @agent_ppo2.py:185][0m |          -0.0162 |          34.3042 |           7.7466 |
[32m[20221213 23:46:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:46:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.93
[32m[20221213 23:46:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.99
[32m[20221213 23:46:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.28
[32m[20221213 23:46:59 @agent_ppo2.py:143][0m Total time:      34.46 min
[32m[20221213 23:46:59 @agent_ppo2.py:145][0m 3338240 total steps have happened
[32m[20221213 23:46:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3630 --------------------------#
[32m[20221213 23:47:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:47:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |           0.0041 |          61.3971 |           7.5508 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |          -0.0010 |          54.0873 |           7.5348 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |           0.0059 |          58.9019 |           7.5538 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |          -0.0060 |          51.1510 |           7.5261 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |           0.0012 |          50.9501 |           7.5681 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |          -0.0104 |          49.0199 |           7.4531 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |          -0.0072 |          48.5203 |           7.4789 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |           0.0026 |          52.0222 |           7.4972 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |          -0.0100 |          47.9908 |           7.5100 |
[32m[20221213 23:47:00 @agent_ppo2.py:185][0m |          -0.0108 |          47.1734 |           7.5269 |
[32m[20221213 23:47:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:47:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.24
[32m[20221213 23:47:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.41
[32m[20221213 23:47:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.14
[32m[20221213 23:47:01 @agent_ppo2.py:143][0m Total time:      34.48 min
[32m[20221213 23:47:01 @agent_ppo2.py:145][0m 3340288 total steps have happened
[32m[20221213 23:47:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3631 --------------------------#
[32m[20221213 23:47:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:01 @agent_ppo2.py:185][0m |          -0.0026 |          54.7578 |           6.9370 |
[32m[20221213 23:47:01 @agent_ppo2.py:185][0m |          -0.0063 |          50.4242 |           6.9067 |
[32m[20221213 23:47:01 @agent_ppo2.py:185][0m |          -0.0104 |          48.7475 |           6.9890 |
[32m[20221213 23:47:01 @agent_ppo2.py:185][0m |          -0.0129 |          47.3707 |           6.9508 |
[32m[20221213 23:47:01 @agent_ppo2.py:185][0m |          -0.0029 |          46.7735 |           7.0249 |
[32m[20221213 23:47:01 @agent_ppo2.py:185][0m |          -0.0113 |          45.6984 |           6.9975 |
[32m[20221213 23:47:01 @agent_ppo2.py:185][0m |          -0.0093 |          45.6727 |           7.0076 |
[32m[20221213 23:47:02 @agent_ppo2.py:185][0m |          -0.0143 |          45.0249 |           7.0518 |
[32m[20221213 23:47:02 @agent_ppo2.py:185][0m |          -0.0096 |          45.5906 |           7.0674 |
[32m[20221213 23:47:02 @agent_ppo2.py:185][0m |          -0.0144 |          44.4055 |           7.0941 |
[32m[20221213 23:47:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:47:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.56
[32m[20221213 23:47:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.85
[32m[20221213 23:47:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.14
[32m[20221213 23:47:02 @agent_ppo2.py:143][0m Total time:      34.50 min
[32m[20221213 23:47:02 @agent_ppo2.py:145][0m 3342336 total steps have happened
[32m[20221213 23:47:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3632 --------------------------#
[32m[20221213 23:47:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:02 @agent_ppo2.py:185][0m |           0.0021 |          50.4811 |           6.9382 |
[32m[20221213 23:47:02 @agent_ppo2.py:185][0m |          -0.0042 |          45.3323 |           6.8521 |
[32m[20221213 23:47:02 @agent_ppo2.py:185][0m |          -0.0032 |          45.5957 |           6.8679 |
[32m[20221213 23:47:02 @agent_ppo2.py:185][0m |          -0.0114 |          42.3114 |           6.8305 |
[32m[20221213 23:47:03 @agent_ppo2.py:185][0m |          -0.0136 |          41.1958 |           6.8132 |
[32m[20221213 23:47:03 @agent_ppo2.py:185][0m |          -0.0217 |          40.4002 |           6.8439 |
[32m[20221213 23:47:03 @agent_ppo2.py:185][0m |          -0.0170 |          39.8779 |           6.7796 |
[32m[20221213 23:47:03 @agent_ppo2.py:185][0m |          -0.0189 |          39.3401 |           6.7904 |
[32m[20221213 23:47:03 @agent_ppo2.py:185][0m |          -0.0134 |          40.5375 |           6.7066 |
[32m[20221213 23:47:03 @agent_ppo2.py:185][0m |          -0.0138 |          38.8364 |           6.7585 |
[32m[20221213 23:47:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.43
[32m[20221213 23:47:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.86
[32m[20221213 23:47:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.25
[32m[20221213 23:47:03 @agent_ppo2.py:143][0m Total time:      34.52 min
[32m[20221213 23:47:03 @agent_ppo2.py:145][0m 3344384 total steps have happened
[32m[20221213 23:47:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3633 --------------------------#
[32m[20221213 23:47:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:03 @agent_ppo2.py:185][0m |           0.0021 |          47.6633 |           7.7904 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0089 |          44.0620 |           7.7819 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0113 |          42.8068 |           7.7815 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0095 |          42.0741 |           7.7485 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0149 |          41.6353 |           7.7684 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0108 |          41.1215 |           7.7491 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0138 |          40.6851 |           7.7235 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0106 |          40.5555 |           7.7354 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0105 |          40.1972 |           7.7161 |
[32m[20221213 23:47:04 @agent_ppo2.py:185][0m |          -0.0124 |          39.9265 |           7.6689 |
[32m[20221213 23:47:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.10
[32m[20221213 23:47:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.30
[32m[20221213 23:47:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.01
[32m[20221213 23:47:04 @agent_ppo2.py:143][0m Total time:      34.54 min
[32m[20221213 23:47:04 @agent_ppo2.py:145][0m 3346432 total steps have happened
[32m[20221213 23:47:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3634 --------------------------#
[32m[20221213 23:47:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:47:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |           0.0002 |          69.0387 |           7.0584 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |           0.0057 |          70.8415 |           7.0219 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |          -0.0039 |          65.6751 |           6.9870 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |          -0.0072 |          64.6891 |           6.9014 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |          -0.0057 |          64.4185 |           6.9762 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |          -0.0077 |          64.1117 |           6.9186 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |          -0.0084 |          63.9489 |           6.9455 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |          -0.0107 |          63.8713 |           6.9403 |
[32m[20221213 23:47:05 @agent_ppo2.py:185][0m |          -0.0111 |          63.6038 |           6.9115 |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |          -0.0097 |          63.6418 |           6.8919 |
[32m[20221213 23:47:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.77
[32m[20221213 23:47:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.99
[32m[20221213 23:47:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.73
[32m[20221213 23:47:06 @agent_ppo2.py:143][0m Total time:      34.56 min
[32m[20221213 23:47:06 @agent_ppo2.py:145][0m 3348480 total steps have happened
[32m[20221213 23:47:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3635 --------------------------#
[32m[20221213 23:47:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |           0.0002 |          69.2599 |           7.3240 |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |          -0.0045 |          66.0440 |           7.4001 |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |          -0.0069 |          64.6987 |           7.3815 |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |          -0.0103 |          63.6495 |           7.4025 |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |          -0.0075 |          63.0316 |           7.4248 |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |          -0.0063 |          62.7031 |           7.4444 |
[32m[20221213 23:47:06 @agent_ppo2.py:185][0m |          -0.0116 |          62.1088 |           7.4525 |
[32m[20221213 23:47:07 @agent_ppo2.py:185][0m |          -0.0116 |          61.6939 |           7.4600 |
[32m[20221213 23:47:07 @agent_ppo2.py:185][0m |          -0.0020 |          63.6449 |           7.5094 |
[32m[20221213 23:47:07 @agent_ppo2.py:185][0m |          -0.0143 |          60.9673 |           7.5335 |
[32m[20221213 23:47:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.11
[32m[20221213 23:47:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.98
[32m[20221213 23:47:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.96
[32m[20221213 23:47:07 @agent_ppo2.py:143][0m Total time:      34.58 min
[32m[20221213 23:47:07 @agent_ppo2.py:145][0m 3350528 total steps have happened
[32m[20221213 23:47:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3636 --------------------------#
[32m[20221213 23:47:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:07 @agent_ppo2.py:185][0m |           0.0128 |          60.2565 |           7.4215 |
[32m[20221213 23:47:07 @agent_ppo2.py:185][0m |           0.0030 |          52.2337 |           7.4460 |
[32m[20221213 23:47:07 @agent_ppo2.py:185][0m |          -0.0087 |          41.5798 |           7.4240 |
[32m[20221213 23:47:07 @agent_ppo2.py:185][0m |          -0.0114 |          40.1306 |           7.4472 |
[32m[20221213 23:47:08 @agent_ppo2.py:185][0m |          -0.0103 |          39.2498 |           7.3822 |
[32m[20221213 23:47:08 @agent_ppo2.py:185][0m |          -0.0096 |          38.5866 |           7.3923 |
[32m[20221213 23:47:08 @agent_ppo2.py:185][0m |          -0.0137 |          37.9271 |           7.4010 |
[32m[20221213 23:47:08 @agent_ppo2.py:185][0m |          -0.0228 |          37.6733 |           7.4392 |
[32m[20221213 23:47:08 @agent_ppo2.py:185][0m |          -0.0199 |          36.9417 |           7.3839 |
[32m[20221213 23:47:08 @agent_ppo2.py:185][0m |          -0.0180 |          36.4808 |           7.3758 |
[32m[20221213 23:47:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:47:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.79
[32m[20221213 23:47:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.27
[32m[20221213 23:47:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.49
[32m[20221213 23:47:08 @agent_ppo2.py:143][0m Total time:      34.60 min
[32m[20221213 23:47:08 @agent_ppo2.py:145][0m 3352576 total steps have happened
[32m[20221213 23:47:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3637 --------------------------#
[32m[20221213 23:47:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0011 |          60.1545 |           7.1514 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |           0.0055 |          59.0599 |           7.2910 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0017 |          55.9712 |           7.2788 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0048 |          55.8052 |           7.2814 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0084 |          54.7965 |           7.2955 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |           0.0018 |          58.0884 |           7.3088 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0079 |          54.4211 |           7.2950 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0025 |          54.5095 |           7.3663 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0131 |          53.6976 |           7.3238 |
[32m[20221213 23:47:09 @agent_ppo2.py:185][0m |          -0.0143 |          53.4850 |           7.3235 |
[32m[20221213 23:47:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.04
[32m[20221213 23:47:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.50
[32m[20221213 23:47:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.80
[32m[20221213 23:47:09 @agent_ppo2.py:143][0m Total time:      34.62 min
[32m[20221213 23:47:09 @agent_ppo2.py:145][0m 3354624 total steps have happened
[32m[20221213 23:47:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3638 --------------------------#
[32m[20221213 23:47:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0004 |          53.2767 |           7.3971 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0043 |          48.3679 |           7.3391 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0047 |          45.2742 |           7.3729 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0073 |          42.7181 |           7.3185 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0148 |          41.0373 |           7.3775 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0098 |          39.7596 |           7.3986 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0096 |          39.7370 |           7.4379 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0165 |          38.4121 |           7.3963 |
[32m[20221213 23:47:10 @agent_ppo2.py:185][0m |          -0.0133 |          37.6678 |           7.4300 |
[32m[20221213 23:47:11 @agent_ppo2.py:185][0m |          -0.0169 |          37.4082 |           7.4479 |
[32m[20221213 23:47:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.08
[32m[20221213 23:47:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.90
[32m[20221213 23:47:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.42
[32m[20221213 23:47:11 @agent_ppo2.py:143][0m Total time:      34.64 min
[32m[20221213 23:47:11 @agent_ppo2.py:145][0m 3356672 total steps have happened
[32m[20221213 23:47:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3639 --------------------------#
[32m[20221213 23:47:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:11 @agent_ppo2.py:185][0m |           0.0026 |          64.5047 |           7.5745 |
[32m[20221213 23:47:11 @agent_ppo2.py:185][0m |          -0.0087 |          59.7272 |           7.4769 |
[32m[20221213 23:47:11 @agent_ppo2.py:185][0m |          -0.0119 |          57.6566 |           7.5404 |
[32m[20221213 23:47:11 @agent_ppo2.py:185][0m |          -0.0121 |          56.6485 |           7.5522 |
[32m[20221213 23:47:11 @agent_ppo2.py:185][0m |          -0.0115 |          55.9632 |           7.6142 |
[32m[20221213 23:47:11 @agent_ppo2.py:185][0m |          -0.0069 |          57.3970 |           7.5731 |
[32m[20221213 23:47:12 @agent_ppo2.py:185][0m |          -0.0129 |          54.5272 |           7.6129 |
[32m[20221213 23:47:12 @agent_ppo2.py:185][0m |          -0.0066 |          55.5348 |           7.6018 |
[32m[20221213 23:47:12 @agent_ppo2.py:185][0m |          -0.0143 |          53.6852 |           7.6273 |
[32m[20221213 23:47:12 @agent_ppo2.py:185][0m |          -0.0187 |          53.1179 |           7.6555 |
[32m[20221213 23:47:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.41
[32m[20221213 23:47:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.08
[32m[20221213 23:47:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.67
[32m[20221213 23:47:12 @agent_ppo2.py:143][0m Total time:      34.67 min
[32m[20221213 23:47:12 @agent_ppo2.py:145][0m 3358720 total steps have happened
[32m[20221213 23:47:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3640 --------------------------#
[32m[20221213 23:47:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:47:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:12 @agent_ppo2.py:185][0m |          -0.0014 |          49.5942 |           8.0510 |
[32m[20221213 23:47:12 @agent_ppo2.py:185][0m |          -0.0004 |          45.5692 |           8.1062 |
[32m[20221213 23:47:12 @agent_ppo2.py:185][0m |          -0.0054 |          43.3030 |           8.1403 |
[32m[20221213 23:47:13 @agent_ppo2.py:185][0m |          -0.0088 |          42.4170 |           8.1580 |
[32m[20221213 23:47:13 @agent_ppo2.py:185][0m |          -0.0154 |          41.9757 |           8.1802 |
[32m[20221213 23:47:13 @agent_ppo2.py:185][0m |          -0.0160 |          41.2058 |           8.1650 |
[32m[20221213 23:47:13 @agent_ppo2.py:185][0m |          -0.0170 |          40.6251 |           8.2616 |
[32m[20221213 23:47:13 @agent_ppo2.py:185][0m |          -0.0183 |          40.1447 |           8.2640 |
[32m[20221213 23:47:13 @agent_ppo2.py:185][0m |          -0.0121 |          40.1534 |           8.2456 |
[32m[20221213 23:47:13 @agent_ppo2.py:185][0m |          -0.0275 |          39.8598 |           8.2999 |
[32m[20221213 23:47:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.31
[32m[20221213 23:47:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.26
[32m[20221213 23:47:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.26
[32m[20221213 23:47:13 @agent_ppo2.py:143][0m Total time:      34.69 min
[32m[20221213 23:47:13 @agent_ppo2.py:145][0m 3360768 total steps have happened
[32m[20221213 23:47:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3641 --------------------------#
[32m[20221213 23:47:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |           0.0033 |          75.0159 |           7.9392 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |           0.0009 |          72.2315 |           7.9811 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |           0.0130 |          76.3531 |           7.9452 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |          -0.0038 |          70.8306 |           8.0616 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |          -0.0076 |          70.2838 |           8.1019 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |          -0.0086 |          69.8901 |           8.1088 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |          -0.0076 |          69.8176 |           8.0999 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |          -0.0109 |          69.4141 |           8.1493 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |           0.0065 |          76.7715 |           8.1932 |
[32m[20221213 23:47:14 @agent_ppo2.py:185][0m |          -0.0052 |          69.1367 |           8.2457 |
[32m[20221213 23:47:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:47:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.20
[32m[20221213 23:47:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.15
[32m[20221213 23:47:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.40
[32m[20221213 23:47:14 @agent_ppo2.py:143][0m Total time:      34.71 min
[32m[20221213 23:47:14 @agent_ppo2.py:145][0m 3362816 total steps have happened
[32m[20221213 23:47:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3642 --------------------------#
[32m[20221213 23:47:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0013 |          65.4520 |           7.5347 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0081 |          62.2513 |           7.5124 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0093 |          61.7552 |           7.5542 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |           0.0053 |          67.2319 |           7.5755 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0094 |          61.1797 |           7.5056 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0126 |          60.4336 |           7.5911 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0103 |          60.4859 |           7.5501 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0167 |          60.2780 |           7.5831 |
[32m[20221213 23:47:15 @agent_ppo2.py:185][0m |          -0.0162 |          59.9154 |           7.6130 |
[32m[20221213 23:47:16 @agent_ppo2.py:185][0m |          -0.0132 |          59.9010 |           7.5483 |
[32m[20221213 23:47:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.14
[32m[20221213 23:47:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.21
[32m[20221213 23:47:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.22
[32m[20221213 23:47:16 @agent_ppo2.py:143][0m Total time:      34.73 min
[32m[20221213 23:47:16 @agent_ppo2.py:145][0m 3364864 total steps have happened
[32m[20221213 23:47:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3643 --------------------------#
[32m[20221213 23:47:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:16 @agent_ppo2.py:185][0m |           0.0019 |          48.6215 |           7.8106 |
[32m[20221213 23:47:16 @agent_ppo2.py:185][0m |          -0.0039 |          43.7780 |           7.7774 |
[32m[20221213 23:47:16 @agent_ppo2.py:185][0m |          -0.0032 |          42.8786 |           7.7879 |
[32m[20221213 23:47:16 @agent_ppo2.py:185][0m |          -0.0042 |          41.4792 |           7.8087 |
[32m[20221213 23:47:16 @agent_ppo2.py:185][0m |          -0.0086 |          40.8404 |           7.7997 |
[32m[20221213 23:47:16 @agent_ppo2.py:185][0m |          -0.0087 |          40.6153 |           7.7492 |
[32m[20221213 23:47:17 @agent_ppo2.py:185][0m |          -0.0130 |          40.0782 |           7.7861 |
[32m[20221213 23:47:17 @agent_ppo2.py:185][0m |          -0.0157 |          39.7703 |           7.7914 |
[32m[20221213 23:47:17 @agent_ppo2.py:185][0m |          -0.0098 |          39.6176 |           7.7713 |
[32m[20221213 23:47:17 @agent_ppo2.py:185][0m |          -0.0158 |          39.2837 |           7.8151 |
[32m[20221213 23:47:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.00
[32m[20221213 23:47:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.67
[32m[20221213 23:47:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.50
[32m[20221213 23:47:17 @agent_ppo2.py:143][0m Total time:      34.75 min
[32m[20221213 23:47:17 @agent_ppo2.py:145][0m 3366912 total steps have happened
[32m[20221213 23:47:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3644 --------------------------#
[32m[20221213 23:47:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:17 @agent_ppo2.py:185][0m |           0.0020 |          65.4765 |           8.0481 |
[32m[20221213 23:47:17 @agent_ppo2.py:185][0m |          -0.0044 |          61.1584 |           8.1526 |
[32m[20221213 23:47:17 @agent_ppo2.py:185][0m |           0.0005 |          61.4963 |           8.1149 |
[32m[20221213 23:47:18 @agent_ppo2.py:185][0m |          -0.0067 |          58.8397 |           8.1714 |
[32m[20221213 23:47:18 @agent_ppo2.py:185][0m |          -0.0110 |          58.1784 |           8.2200 |
[32m[20221213 23:47:18 @agent_ppo2.py:185][0m |          -0.0148 |          57.5898 |           8.2536 |
[32m[20221213 23:47:18 @agent_ppo2.py:185][0m |          -0.0156 |          57.1727 |           8.2959 |
[32m[20221213 23:47:18 @agent_ppo2.py:185][0m |          -0.0095 |          56.8064 |           8.2996 |
[32m[20221213 23:47:18 @agent_ppo2.py:185][0m |          -0.0157 |          56.5290 |           8.3015 |
[32m[20221213 23:47:18 @agent_ppo2.py:185][0m |          -0.0195 |          56.4483 |           8.2969 |
[32m[20221213 23:47:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.40
[32m[20221213 23:47:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.12
[32m[20221213 23:47:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.56
[32m[20221213 23:47:18 @agent_ppo2.py:143][0m Total time:      34.77 min
[32m[20221213 23:47:18 @agent_ppo2.py:145][0m 3368960 total steps have happened
[32m[20221213 23:47:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3645 --------------------------#
[32m[20221213 23:47:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |           0.0000 |          71.9840 |           8.5369 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0043 |          68.9765 |           8.5193 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0096 |          64.2155 |           8.5795 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0114 |          63.3028 |           8.5634 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0131 |          62.1476 |           8.5662 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0145 |          61.2423 |           8.6133 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0145 |          60.6344 |           8.5827 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0143 |          60.9286 |           8.5964 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0164 |          59.9277 |           8.5878 |
[32m[20221213 23:47:19 @agent_ppo2.py:185][0m |          -0.0137 |          59.2302 |           8.6189 |
[32m[20221213 23:47:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.79
[32m[20221213 23:47:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.00
[32m[20221213 23:47:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.34
[32m[20221213 23:47:19 @agent_ppo2.py:143][0m Total time:      34.79 min
[32m[20221213 23:47:19 @agent_ppo2.py:145][0m 3371008 total steps have happened
[32m[20221213 23:47:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3646 --------------------------#
[32m[20221213 23:47:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |           0.0181 |          83.7192 |           8.5089 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0023 |          69.1130 |           8.4040 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0067 |          67.2347 |           8.4445 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0079 |          65.8240 |           8.4445 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0093 |          65.1564 |           8.4465 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0117 |          64.6245 |           8.4087 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0154 |          64.1901 |           8.4081 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0163 |          63.7027 |           8.3702 |
[32m[20221213 23:47:20 @agent_ppo2.py:185][0m |          -0.0107 |          63.4630 |           8.3858 |
[32m[20221213 23:47:21 @agent_ppo2.py:185][0m |          -0.0157 |          62.9662 |           8.3520 |
[32m[20221213 23:47:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:47:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.54
[32m[20221213 23:47:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.10
[32m[20221213 23:47:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.05
[32m[20221213 23:47:21 @agent_ppo2.py:143][0m Total time:      34.81 min
[32m[20221213 23:47:21 @agent_ppo2.py:145][0m 3373056 total steps have happened
[32m[20221213 23:47:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3647 --------------------------#
[32m[20221213 23:47:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:21 @agent_ppo2.py:185][0m |           0.0084 |          77.0372 |           8.6912 |
[32m[20221213 23:47:21 @agent_ppo2.py:185][0m |          -0.0053 |          70.4028 |           8.6705 |
[32m[20221213 23:47:21 @agent_ppo2.py:185][0m |          -0.0076 |          68.0647 |           8.7005 |
[32m[20221213 23:47:21 @agent_ppo2.py:185][0m |          -0.0092 |          66.9912 |           8.7124 |
[32m[20221213 23:47:21 @agent_ppo2.py:185][0m |          -0.0068 |          66.7662 |           8.6780 |
[32m[20221213 23:47:21 @agent_ppo2.py:185][0m |          -0.0186 |          65.6985 |           8.6689 |
[32m[20221213 23:47:22 @agent_ppo2.py:185][0m |          -0.0128 |          65.3166 |           8.6756 |
[32m[20221213 23:47:22 @agent_ppo2.py:185][0m |          -0.0127 |          65.0932 |           8.6326 |
[32m[20221213 23:47:22 @agent_ppo2.py:185][0m |          -0.0148 |          64.6819 |           8.6505 |
[32m[20221213 23:47:22 @agent_ppo2.py:185][0m |          -0.0163 |          64.1547 |           8.6113 |
[32m[20221213 23:47:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.45
[32m[20221213 23:47:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.15
[32m[20221213 23:47:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.70
[32m[20221213 23:47:22 @agent_ppo2.py:143][0m Total time:      34.83 min
[32m[20221213 23:47:22 @agent_ppo2.py:145][0m 3375104 total steps have happened
[32m[20221213 23:47:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3648 --------------------------#
[32m[20221213 23:47:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:22 @agent_ppo2.py:185][0m |           0.0013 |          64.9314 |           8.4143 |
[32m[20221213 23:47:22 @agent_ppo2.py:185][0m |           0.0044 |          65.1405 |           8.4644 |
[32m[20221213 23:47:22 @agent_ppo2.py:185][0m |           0.0022 |          62.5907 |           8.4294 |
[32m[20221213 23:47:23 @agent_ppo2.py:185][0m |           0.0079 |          68.2279 |           8.4889 |
[32m[20221213 23:47:23 @agent_ppo2.py:185][0m |          -0.0072 |          61.4587 |           8.5854 |
[32m[20221213 23:47:23 @agent_ppo2.py:185][0m |          -0.0080 |          61.2019 |           8.5488 |
[32m[20221213 23:47:23 @agent_ppo2.py:185][0m |           0.0003 |          63.5146 |           8.6108 |
[32m[20221213 23:47:23 @agent_ppo2.py:185][0m |          -0.0074 |          61.0065 |           8.6079 |
[32m[20221213 23:47:23 @agent_ppo2.py:185][0m |          -0.0079 |          60.9498 |           8.6714 |
[32m[20221213 23:47:23 @agent_ppo2.py:185][0m |          -0.0032 |          61.3890 |           8.6783 |
[32m[20221213 23:47:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:47:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.53
[32m[20221213 23:47:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.88
[32m[20221213 23:47:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.15
[32m[20221213 23:47:23 @agent_ppo2.py:143][0m Total time:      34.85 min
[32m[20221213 23:47:23 @agent_ppo2.py:145][0m 3377152 total steps have happened
[32m[20221213 23:47:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3649 --------------------------#
[32m[20221213 23:47:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0013 |          74.6562 |           8.4226 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |           0.0020 |          71.6762 |           8.3880 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0047 |          71.4826 |           8.4167 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0044 |          70.6472 |           8.4059 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0113 |          68.7169 |           8.4427 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0115 |          68.3952 |           8.4935 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0026 |          72.1814 |           8.5209 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0080 |          67.8376 |           8.5022 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0138 |          67.7572 |           8.5618 |
[32m[20221213 23:47:24 @agent_ppo2.py:185][0m |          -0.0116 |          67.5087 |           8.5796 |
[32m[20221213 23:47:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.49
[32m[20221213 23:47:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.41
[32m[20221213 23:47:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.35
[32m[20221213 23:47:24 @agent_ppo2.py:143][0m Total time:      34.87 min
[32m[20221213 23:47:24 @agent_ppo2.py:145][0m 3379200 total steps have happened
[32m[20221213 23:47:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3650 --------------------------#
[32m[20221213 23:47:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:47:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |           0.0027 |          45.4875 |           8.1495 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0074 |          40.8348 |           8.1403 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0090 |          38.8836 |           8.1374 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0140 |          37.4139 |           8.0798 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0079 |          36.7157 |           8.0818 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0136 |          35.9725 |           8.0715 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0130 |          35.7245 |           8.0635 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0108 |          35.8363 |           8.0567 |
[32m[20221213 23:47:25 @agent_ppo2.py:185][0m |          -0.0191 |          34.6614 |           8.0205 |
[32m[20221213 23:47:26 @agent_ppo2.py:185][0m |          -0.0199 |          34.1130 |           8.0512 |
[32m[20221213 23:47:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:47:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.35
[32m[20221213 23:47:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.67
[32m[20221213 23:47:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.27
[32m[20221213 23:47:26 @agent_ppo2.py:143][0m Total time:      34.90 min
[32m[20221213 23:47:26 @agent_ppo2.py:145][0m 3381248 total steps have happened
[32m[20221213 23:47:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3651 --------------------------#
[32m[20221213 23:47:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:26 @agent_ppo2.py:185][0m |          -0.0007 |          65.7252 |           8.7762 |
[32m[20221213 23:47:26 @agent_ppo2.py:185][0m |          -0.0055 |          63.0232 |           8.8182 |
[32m[20221213 23:47:26 @agent_ppo2.py:185][0m |          -0.0058 |          62.2424 |           8.7487 |
[32m[20221213 23:47:26 @agent_ppo2.py:185][0m |          -0.0088 |          61.7526 |           8.7124 |
[32m[20221213 23:47:26 @agent_ppo2.py:185][0m |          -0.0100 |          61.8050 |           8.6749 |
[32m[20221213 23:47:26 @agent_ppo2.py:185][0m |          -0.0110 |          61.5930 |           8.6501 |
[32m[20221213 23:47:27 @agent_ppo2.py:185][0m |          -0.0090 |          61.3345 |           8.6085 |
[32m[20221213 23:47:27 @agent_ppo2.py:185][0m |           0.0006 |          67.6973 |           8.6204 |
[32m[20221213 23:47:27 @agent_ppo2.py:185][0m |          -0.0107 |          61.5590 |           8.6164 |
[32m[20221213 23:47:27 @agent_ppo2.py:185][0m |          -0.0137 |          60.8606 |           8.5585 |
[32m[20221213 23:47:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.05
[32m[20221213 23:47:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.84
[32m[20221213 23:47:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.45
[32m[20221213 23:47:27 @agent_ppo2.py:143][0m Total time:      34.92 min
[32m[20221213 23:47:27 @agent_ppo2.py:145][0m 3383296 total steps have happened
[32m[20221213 23:47:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3652 --------------------------#
[32m[20221213 23:47:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:27 @agent_ppo2.py:185][0m |           0.0062 |          56.6730 |           8.0107 |
[32m[20221213 23:47:27 @agent_ppo2.py:185][0m |          -0.0055 |          50.5705 |           7.9106 |
[32m[20221213 23:47:27 @agent_ppo2.py:185][0m |          -0.0156 |          48.8397 |           7.8657 |
[32m[20221213 23:47:28 @agent_ppo2.py:185][0m |          -0.0134 |          47.7310 |           7.9325 |
[32m[20221213 23:47:28 @agent_ppo2.py:185][0m |          -0.0159 |          46.8450 |           7.8657 |
[32m[20221213 23:47:28 @agent_ppo2.py:185][0m |          -0.0160 |          46.1016 |           7.8220 |
[32m[20221213 23:47:28 @agent_ppo2.py:185][0m |          -0.0173 |          45.7917 |           7.8342 |
[32m[20221213 23:47:28 @agent_ppo2.py:185][0m |          -0.0107 |          47.5803 |           7.8482 |
[32m[20221213 23:47:28 @agent_ppo2.py:185][0m |          -0.0211 |          44.7178 |           7.8558 |
[32m[20221213 23:47:28 @agent_ppo2.py:185][0m |          -0.0205 |          44.2315 |           7.8398 |
[32m[20221213 23:47:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.31
[32m[20221213 23:47:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.90
[32m[20221213 23:47:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.03
[32m[20221213 23:47:28 @agent_ppo2.py:143][0m Total time:      34.94 min
[32m[20221213 23:47:28 @agent_ppo2.py:145][0m 3385344 total steps have happened
[32m[20221213 23:47:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3653 --------------------------#
[32m[20221213 23:47:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |           0.0001 |          44.5473 |           7.9210 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0034 |          39.0862 |           7.8896 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0097 |          36.7294 |           7.9976 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0095 |          35.4289 |           7.9745 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0107 |          34.1810 |           7.9994 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0147 |          33.4390 |           8.0223 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0132 |          32.9087 |           8.0367 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0165 |          32.2001 |           8.0286 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0149 |          31.8593 |           8.0649 |
[32m[20221213 23:47:29 @agent_ppo2.py:185][0m |          -0.0104 |          33.3849 |           8.0673 |
[32m[20221213 23:47:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.78
[32m[20221213 23:47:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.08
[32m[20221213 23:47:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.06
[32m[20221213 23:47:29 @agent_ppo2.py:143][0m Total time:      34.96 min
[32m[20221213 23:47:29 @agent_ppo2.py:145][0m 3387392 total steps have happened
[32m[20221213 23:47:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3654 --------------------------#
[32m[20221213 23:47:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0002 |          51.5755 |           8.1340 |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0051 |          45.6236 |           8.0524 |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0033 |          43.5312 |           8.0057 |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0113 |          42.5613 |           8.0506 |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0101 |          40.8409 |           8.0019 |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0026 |          47.4398 |           8.0041 |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0172 |          39.6940 |           7.9588 |
[32m[20221213 23:47:30 @agent_ppo2.py:185][0m |          -0.0137 |          39.2213 |           8.0112 |
[32m[20221213 23:47:31 @agent_ppo2.py:185][0m |          -0.0171 |          38.9384 |           7.9214 |
[32m[20221213 23:47:31 @agent_ppo2.py:185][0m |          -0.0162 |          38.5907 |           7.9682 |
[32m[20221213 23:47:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.26
[32m[20221213 23:47:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.22
[32m[20221213 23:47:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.89
[32m[20221213 23:47:31 @agent_ppo2.py:143][0m Total time:      34.98 min
[32m[20221213 23:47:31 @agent_ppo2.py:145][0m 3389440 total steps have happened
[32m[20221213 23:47:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3655 --------------------------#
[32m[20221213 23:47:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:31 @agent_ppo2.py:185][0m |           0.0120 |          67.7868 |           7.4286 |
[32m[20221213 23:47:31 @agent_ppo2.py:185][0m |          -0.0069 |          51.5488 |           7.5512 |
[32m[20221213 23:47:31 @agent_ppo2.py:185][0m |          -0.0106 |          49.2502 |           7.6250 |
[32m[20221213 23:47:31 @agent_ppo2.py:185][0m |          -0.0109 |          47.6619 |           7.6209 |
[32m[20221213 23:47:31 @agent_ppo2.py:185][0m |          -0.0096 |          46.9356 |           7.7082 |
[32m[20221213 23:47:32 @agent_ppo2.py:185][0m |          -0.0136 |          46.2685 |           7.6440 |
[32m[20221213 23:47:32 @agent_ppo2.py:185][0m |          -0.0166 |          45.6713 |           7.6734 |
[32m[20221213 23:47:32 @agent_ppo2.py:185][0m |          -0.0162 |          45.1541 |           7.6863 |
[32m[20221213 23:47:32 @agent_ppo2.py:185][0m |          -0.0176 |          44.7264 |           7.7703 |
[32m[20221213 23:47:32 @agent_ppo2.py:185][0m |          -0.0186 |          44.4616 |           7.8259 |
[32m[20221213 23:47:32 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:47:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.96
[32m[20221213 23:47:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.51
[32m[20221213 23:47:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.30
[32m[20221213 23:47:32 @agent_ppo2.py:143][0m Total time:      35.00 min
[32m[20221213 23:47:32 @agent_ppo2.py:145][0m 3391488 total steps have happened
[32m[20221213 23:47:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3656 --------------------------#
[32m[20221213 23:47:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:32 @agent_ppo2.py:185][0m |           0.0009 |          49.9340 |           7.6905 |
[32m[20221213 23:47:32 @agent_ppo2.py:185][0m |          -0.0082 |          45.5350 |           7.6781 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |          -0.0081 |          43.9043 |           7.6724 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |          -0.0112 |          43.3570 |           7.6181 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |           0.0046 |          50.1660 |           7.6330 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |          -0.0140 |          42.5048 |           7.6354 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |          -0.0135 |          41.7072 |           7.6339 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |          -0.0131 |          41.4986 |           7.5682 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |          -0.0158 |          41.2776 |           7.5674 |
[32m[20221213 23:47:33 @agent_ppo2.py:185][0m |          -0.0128 |          40.9197 |           7.5964 |
[32m[20221213 23:47:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.98
[32m[20221213 23:47:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.56
[32m[20221213 23:47:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.00
[32m[20221213 23:47:33 @agent_ppo2.py:143][0m Total time:      35.02 min
[32m[20221213 23:47:33 @agent_ppo2.py:145][0m 3393536 total steps have happened
[32m[20221213 23:47:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3657 --------------------------#
[32m[20221213 23:47:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:47:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0006 |          58.7419 |           7.7600 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0045 |          56.6594 |           7.7637 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0074 |          56.2979 |           7.8210 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0116 |          55.6041 |           7.8719 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0128 |          55.3095 |           7.8502 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0117 |          55.2627 |           7.8502 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0105 |          55.6954 |           7.8327 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0110 |          55.0200 |           7.8999 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0117 |          55.1059 |           7.8968 |
[32m[20221213 23:47:34 @agent_ppo2.py:185][0m |          -0.0105 |          54.7267 |           7.9087 |
[32m[20221213 23:47:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.44
[32m[20221213 23:47:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.87
[32m[20221213 23:47:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.76
[32m[20221213 23:47:35 @agent_ppo2.py:143][0m Total time:      35.04 min
[32m[20221213 23:47:35 @agent_ppo2.py:145][0m 3395584 total steps have happened
[32m[20221213 23:47:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3658 --------------------------#
[32m[20221213 23:47:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:47:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |          -0.0000 |          19.3032 |           8.0652 |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |          -0.0011 |          15.8781 |           8.1307 |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |           0.0001 |          15.8526 |           8.1254 |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |          -0.0035 |          15.6312 |           8.1331 |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |          -0.0033 |          15.6227 |           8.1865 |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |          -0.0061 |          15.6465 |           8.2435 |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |          -0.0015 |          15.5982 |           8.1234 |
[32m[20221213 23:47:35 @agent_ppo2.py:185][0m |           0.0003 |          15.8106 |           8.1646 |
[32m[20221213 23:47:36 @agent_ppo2.py:185][0m |           0.0023 |          16.6634 |           8.2169 |
[32m[20221213 23:47:36 @agent_ppo2.py:185][0m |          -0.0044 |          15.5590 |           8.2044 |
[32m[20221213 23:47:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:47:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:47:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:47:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:47:36 @agent_ppo2.py:143][0m Total time:      35.06 min
[32m[20221213 23:47:36 @agent_ppo2.py:145][0m 3397632 total steps have happened
[32m[20221213 23:47:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3659 --------------------------#
[32m[20221213 23:47:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:36 @agent_ppo2.py:185][0m |           0.0006 |          66.5392 |           8.3565 |
[32m[20221213 23:47:36 @agent_ppo2.py:185][0m |          -0.0014 |          64.5083 |           8.2965 |
[32m[20221213 23:47:36 @agent_ppo2.py:185][0m |          -0.0076 |          63.2633 |           8.3403 |
[32m[20221213 23:47:36 @agent_ppo2.py:185][0m |          -0.0058 |          62.6586 |           8.3064 |
[32m[20221213 23:47:36 @agent_ppo2.py:185][0m |          -0.0107 |          62.1909 |           8.3411 |
[32m[20221213 23:47:37 @agent_ppo2.py:185][0m |          -0.0113 |          61.7072 |           8.3332 |
[32m[20221213 23:47:37 @agent_ppo2.py:185][0m |          -0.0138 |          61.5144 |           8.2778 |
[32m[20221213 23:47:37 @agent_ppo2.py:185][0m |          -0.0027 |          68.9422 |           8.2941 |
[32m[20221213 23:47:37 @agent_ppo2.py:185][0m |          -0.0118 |          61.4823 |           8.3555 |
[32m[20221213 23:47:37 @agent_ppo2.py:185][0m |          -0.0161 |          60.7272 |           8.2997 |
[32m[20221213 23:47:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.98
[32m[20221213 23:47:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.67
[32m[20221213 23:47:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.16
[32m[20221213 23:47:37 @agent_ppo2.py:143][0m Total time:      35.08 min
[32m[20221213 23:47:37 @agent_ppo2.py:145][0m 3399680 total steps have happened
[32m[20221213 23:47:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3660 --------------------------#
[32m[20221213 23:47:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:47:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:37 @agent_ppo2.py:185][0m |           0.0026 |          66.2337 |           7.8517 |
[32m[20221213 23:47:37 @agent_ppo2.py:185][0m |          -0.0058 |          61.8955 |           7.8461 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0101 |          60.7950 |           7.8764 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0080 |          60.1756 |           7.8608 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0071 |          59.9766 |           7.9419 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0116 |          59.2145 |           7.8699 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0180 |          58.8108 |           7.9085 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0169 |          58.5084 |           7.8589 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0167 |          58.1801 |           7.8675 |
[32m[20221213 23:47:38 @agent_ppo2.py:185][0m |          -0.0140 |          58.3719 |           7.9139 |
[32m[20221213 23:47:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:47:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.60
[32m[20221213 23:47:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.73
[32m[20221213 23:47:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.86
[32m[20221213 23:47:38 @agent_ppo2.py:143][0m Total time:      35.11 min
[32m[20221213 23:47:38 @agent_ppo2.py:145][0m 3401728 total steps have happened
[32m[20221213 23:47:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3661 --------------------------#
[32m[20221213 23:47:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0011 |          68.3633 |           8.4270 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0018 |          66.8142 |           8.5070 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0041 |          66.5038 |           8.5080 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0103 |          66.0282 |           8.5308 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0093 |          65.5628 |           8.4731 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0115 |          65.5113 |           8.5202 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0112 |          65.3137 |           8.4773 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0129 |          64.9843 |           8.5190 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0081 |          66.9420 |           8.5038 |
[32m[20221213 23:47:39 @agent_ppo2.py:185][0m |          -0.0106 |          64.9556 |           8.5766 |
[32m[20221213 23:47:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.97
[32m[20221213 23:47:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.15
[32m[20221213 23:47:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.99
[32m[20221213 23:47:40 @agent_ppo2.py:143][0m Total time:      35.13 min
[32m[20221213 23:47:40 @agent_ppo2.py:145][0m 3403776 total steps have happened
[32m[20221213 23:47:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3662 --------------------------#
[32m[20221213 23:47:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |           0.0135 |          53.6806 |           7.8710 |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |          -0.0044 |          44.2449 |           7.8153 |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |          -0.0013 |          42.3914 |           7.8424 |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |          -0.0049 |          41.5678 |           7.7649 |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |          -0.0089 |          41.4775 |           7.8128 |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |          -0.0093 |          40.3870 |           7.7366 |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |          -0.0123 |          39.9383 |           7.7166 |
[32m[20221213 23:47:40 @agent_ppo2.py:185][0m |          -0.0155 |          39.3300 |           7.6728 |
[32m[20221213 23:47:41 @agent_ppo2.py:185][0m |          -0.0104 |          39.0824 |           7.6673 |
[32m[20221213 23:47:41 @agent_ppo2.py:185][0m |          -0.0157 |          38.8747 |           7.7033 |
[32m[20221213 23:47:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.71
[32m[20221213 23:47:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.17
[32m[20221213 23:47:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.87
[32m[20221213 23:47:41 @agent_ppo2.py:143][0m Total time:      35.15 min
[32m[20221213 23:47:41 @agent_ppo2.py:145][0m 3405824 total steps have happened
[32m[20221213 23:47:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3663 --------------------------#
[32m[20221213 23:47:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:41 @agent_ppo2.py:185][0m |          -0.0004 |          66.8122 |           7.3268 |
[32m[20221213 23:47:41 @agent_ppo2.py:185][0m |          -0.0018 |          61.7238 |           7.2578 |
[32m[20221213 23:47:41 @agent_ppo2.py:185][0m |          -0.0110 |          59.9437 |           7.2913 |
[32m[20221213 23:47:41 @agent_ppo2.py:185][0m |          -0.0091 |          58.6128 |           7.2640 |
[32m[20221213 23:47:41 @agent_ppo2.py:185][0m |          -0.0094 |          59.1309 |           7.2539 |
[32m[20221213 23:47:42 @agent_ppo2.py:185][0m |          -0.0167 |          57.3570 |           7.2044 |
[32m[20221213 23:47:42 @agent_ppo2.py:185][0m |          -0.0161 |          57.6855 |           7.1940 |
[32m[20221213 23:47:42 @agent_ppo2.py:185][0m |          -0.0134 |          56.6980 |           7.1804 |
[32m[20221213 23:47:42 @agent_ppo2.py:185][0m |          -0.0193 |          56.5303 |           7.1659 |
[32m[20221213 23:47:42 @agent_ppo2.py:185][0m |          -0.0142 |          57.7743 |           7.1385 |
[32m[20221213 23:47:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.94
[32m[20221213 23:47:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.81
[32m[20221213 23:47:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.25
[32m[20221213 23:47:42 @agent_ppo2.py:143][0m Total time:      35.17 min
[32m[20221213 23:47:42 @agent_ppo2.py:145][0m 3407872 total steps have happened
[32m[20221213 23:47:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3664 --------------------------#
[32m[20221213 23:47:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:42 @agent_ppo2.py:185][0m |           0.0004 |          63.5530 |           7.2152 |
[32m[20221213 23:47:42 @agent_ppo2.py:185][0m |          -0.0020 |          60.9141 |           7.2464 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0062 |          59.9077 |           7.1598 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0056 |          59.3682 |           7.1718 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0025 |          60.9816 |           7.0898 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0159 |          58.7219 |           7.1081 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0157 |          58.3180 |           7.1047 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0075 |          59.1603 |           7.0649 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0178 |          57.8340 |           7.0123 |
[32m[20221213 23:47:43 @agent_ppo2.py:185][0m |          -0.0187 |          57.5325 |           7.0170 |
[32m[20221213 23:47:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.18
[32m[20221213 23:47:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.92
[32m[20221213 23:47:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.48
[32m[20221213 23:47:43 @agent_ppo2.py:143][0m Total time:      35.19 min
[32m[20221213 23:47:43 @agent_ppo2.py:145][0m 3409920 total steps have happened
[32m[20221213 23:47:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3665 --------------------------#
[32m[20221213 23:47:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:47:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |          -0.0010 |          51.5057 |           7.0908 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |          -0.0061 |          50.2468 |           7.0336 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |           0.0041 |          53.8210 |           6.9510 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |          -0.0015 |          50.5194 |           7.0726 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |           0.0020 |          52.2075 |           7.0390 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |          -0.0092 |          49.2874 |           7.0841 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |          -0.0103 |          49.0351 |           7.0382 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |          -0.0032 |          49.5472 |           6.9786 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |          -0.0085 |          48.7319 |           7.0568 |
[32m[20221213 23:47:44 @agent_ppo2.py:185][0m |           0.0078 |          54.4140 |           7.0802 |
[32m[20221213 23:47:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:47:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.62
[32m[20221213 23:47:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.68
[32m[20221213 23:47:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.35
[32m[20221213 23:47:45 @agent_ppo2.py:143][0m Total time:      35.21 min
[32m[20221213 23:47:45 @agent_ppo2.py:145][0m 3411968 total steps have happened
[32m[20221213 23:47:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3666 --------------------------#
[32m[20221213 23:47:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:45 @agent_ppo2.py:185][0m |           0.0017 |          63.7317 |           7.6242 |
[32m[20221213 23:47:45 @agent_ppo2.py:185][0m |          -0.0056 |          61.7753 |           7.5972 |
[32m[20221213 23:47:45 @agent_ppo2.py:185][0m |          -0.0069 |          61.0948 |           7.6475 |
[32m[20221213 23:47:45 @agent_ppo2.py:185][0m |          -0.0115 |          60.4715 |           7.6142 |
[32m[20221213 23:47:45 @agent_ppo2.py:185][0m |          -0.0024 |          64.9981 |           7.6811 |
[32m[20221213 23:47:45 @agent_ppo2.py:185][0m |          -0.0052 |          62.6710 |           7.7009 |
[32m[20221213 23:47:46 @agent_ppo2.py:185][0m |          -0.0118 |          59.6145 |           7.7194 |
[32m[20221213 23:47:46 @agent_ppo2.py:185][0m |          -0.0152 |          59.1907 |           7.7846 |
[32m[20221213 23:47:46 @agent_ppo2.py:185][0m |          -0.0126 |          59.4401 |           7.7059 |
[32m[20221213 23:47:46 @agent_ppo2.py:185][0m |          -0.0109 |          60.5466 |           7.7607 |
[32m[20221213 23:47:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:47:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.65
[32m[20221213 23:47:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.49
[32m[20221213 23:47:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.96
[32m[20221213 23:47:46 @agent_ppo2.py:143][0m Total time:      35.23 min
[32m[20221213 23:47:46 @agent_ppo2.py:145][0m 3414016 total steps have happened
[32m[20221213 23:47:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3667 --------------------------#
[32m[20221213 23:47:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:46 @agent_ppo2.py:185][0m |           0.0022 |          51.5976 |           7.2677 |
[32m[20221213 23:47:46 @agent_ppo2.py:185][0m |          -0.0095 |          45.8885 |           7.3136 |
[32m[20221213 23:47:46 @agent_ppo2.py:185][0m |          -0.0081 |          44.6280 |           7.4141 |
[32m[20221213 23:47:47 @agent_ppo2.py:185][0m |          -0.0023 |          48.4723 |           7.3984 |
[32m[20221213 23:47:47 @agent_ppo2.py:185][0m |          -0.0115 |          42.9509 |           7.4141 |
[32m[20221213 23:47:47 @agent_ppo2.py:185][0m |           0.0033 |          45.5274 |           7.3989 |
[32m[20221213 23:47:47 @agent_ppo2.py:185][0m |          -0.0176 |          41.9869 |           7.4335 |
[32m[20221213 23:47:47 @agent_ppo2.py:185][0m |          -0.0156 |          41.2836 |           7.4024 |
[32m[20221213 23:47:47 @agent_ppo2.py:185][0m |          -0.0194 |          41.2545 |           7.4620 |
[32m[20221213 23:47:47 @agent_ppo2.py:185][0m |          -0.0140 |          41.0122 |           7.5031 |
[32m[20221213 23:47:47 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:47:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.10
[32m[20221213 23:47:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.07
[32m[20221213 23:47:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.26
[32m[20221213 23:47:47 @agent_ppo2.py:143][0m Total time:      35.25 min
[32m[20221213 23:47:47 @agent_ppo2.py:145][0m 3416064 total steps have happened
[32m[20221213 23:47:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3668 --------------------------#
[32m[20221213 23:47:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |           0.0041 |          75.3220 |           7.5760 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0084 |          70.3147 |           7.6179 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0136 |          68.8487 |           7.5846 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0061 |          68.6682 |           7.6669 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0106 |          68.2293 |           7.6604 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0032 |          73.2939 |           7.6460 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0158 |          66.1389 |           7.6859 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0169 |          65.5148 |           7.6858 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0186 |          65.3288 |           7.7276 |
[32m[20221213 23:47:48 @agent_ppo2.py:185][0m |          -0.0065 |          71.3726 |           7.7737 |
[32m[20221213 23:47:48 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:47:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.12
[32m[20221213 23:47:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.63
[32m[20221213 23:47:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.05
[32m[20221213 23:47:49 @agent_ppo2.py:143][0m Total time:      35.28 min
[32m[20221213 23:47:49 @agent_ppo2.py:145][0m 3418112 total steps have happened
[32m[20221213 23:47:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3669 --------------------------#
[32m[20221213 23:47:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:49 @agent_ppo2.py:185][0m |           0.0003 |          80.8321 |           7.8445 |
[32m[20221213 23:47:49 @agent_ppo2.py:185][0m |          -0.0014 |          77.1429 |           7.8927 |
[32m[20221213 23:47:49 @agent_ppo2.py:185][0m |          -0.0031 |          75.2656 |           7.9080 |
[32m[20221213 23:47:49 @agent_ppo2.py:185][0m |          -0.0097 |          74.2290 |           7.9433 |
[32m[20221213 23:47:49 @agent_ppo2.py:185][0m |          -0.0101 |          73.8080 |           7.9424 |
[32m[20221213 23:47:49 @agent_ppo2.py:185][0m |          -0.0021 |          74.8618 |           7.9495 |
[32m[20221213 23:47:49 @agent_ppo2.py:185][0m |          -0.0127 |          72.3674 |           8.0139 |
[32m[20221213 23:47:50 @agent_ppo2.py:185][0m |          -0.0112 |          71.8490 |           8.0173 |
[32m[20221213 23:47:50 @agent_ppo2.py:185][0m |          -0.0165 |          71.3372 |           8.0307 |
[32m[20221213 23:47:50 @agent_ppo2.py:185][0m |          -0.0150 |          71.0831 |           8.0338 |
[32m[20221213 23:47:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:47:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.46
[32m[20221213 23:47:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.55
[32m[20221213 23:47:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.32
[32m[20221213 23:47:50 @agent_ppo2.py:143][0m Total time:      35.30 min
[32m[20221213 23:47:50 @agent_ppo2.py:145][0m 3420160 total steps have happened
[32m[20221213 23:47:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3670 --------------------------#
[32m[20221213 23:47:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:47:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:50 @agent_ppo2.py:185][0m |           0.0029 |          65.9223 |           7.6013 |
[32m[20221213 23:47:50 @agent_ppo2.py:185][0m |          -0.0045 |          64.0041 |           7.6705 |
[32m[20221213 23:47:50 @agent_ppo2.py:185][0m |          -0.0068 |          63.3354 |           7.5844 |
[32m[20221213 23:47:50 @agent_ppo2.py:185][0m |          -0.0095 |          62.8416 |           7.6587 |
[32m[20221213 23:47:51 @agent_ppo2.py:185][0m |          -0.0082 |          62.4135 |           7.7278 |
[32m[20221213 23:47:51 @agent_ppo2.py:185][0m |          -0.0098 |          62.4577 |           7.6634 |
[32m[20221213 23:47:51 @agent_ppo2.py:185][0m |          -0.0114 |          62.1376 |           7.6818 |
[32m[20221213 23:47:51 @agent_ppo2.py:185][0m |          -0.0052 |          63.5747 |           7.7223 |
[32m[20221213 23:47:51 @agent_ppo2.py:185][0m |          -0.0114 |          61.7946 |           7.7421 |
[32m[20221213 23:47:51 @agent_ppo2.py:185][0m |          -0.0061 |          64.5672 |           7.7505 |
[32m[20221213 23:47:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:47:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.95
[32m[20221213 23:47:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.34
[32m[20221213 23:47:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.78
[32m[20221213 23:47:51 @agent_ppo2.py:143][0m Total time:      35.32 min
[32m[20221213 23:47:51 @agent_ppo2.py:145][0m 3422208 total steps have happened
[32m[20221213 23:47:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3671 --------------------------#
[32m[20221213 23:47:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:51 @agent_ppo2.py:185][0m |          -0.0044 |          63.7890 |           7.9780 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0056 |          60.4745 |           7.9654 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0070 |          59.5327 |           8.0381 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0119 |          58.4395 |           7.9946 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0104 |          57.7790 |           8.0258 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0120 |          57.3288 |           7.9583 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0080 |          58.9013 |           7.9985 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0142 |          56.6646 |           7.9402 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0160 |          56.2682 |           7.9741 |
[32m[20221213 23:47:52 @agent_ppo2.py:185][0m |          -0.0154 |          56.0545 |           7.9670 |
[32m[20221213 23:47:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.47
[32m[20221213 23:47:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.30
[32m[20221213 23:47:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.61
[32m[20221213 23:47:52 @agent_ppo2.py:143][0m Total time:      35.34 min
[32m[20221213 23:47:52 @agent_ppo2.py:145][0m 3424256 total steps have happened
[32m[20221213 23:47:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3672 --------------------------#
[32m[20221213 23:47:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |           0.0016 |          66.6529 |           7.8843 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0003 |          57.5850 |           8.0080 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0112 |          56.2478 |           8.0332 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0057 |          55.5918 |           8.0503 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0064 |          55.0382 |           7.9898 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0139 |          54.6203 |           8.0583 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0159 |          54.1895 |           8.0618 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0169 |          54.0398 |           8.0435 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0148 |          53.9455 |           8.0515 |
[32m[20221213 23:47:53 @agent_ppo2.py:185][0m |          -0.0137 |          53.6374 |           8.1016 |
[32m[20221213 23:47:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.38
[32m[20221213 23:47:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.20
[32m[20221213 23:47:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.73
[32m[20221213 23:47:54 @agent_ppo2.py:143][0m Total time:      35.36 min
[32m[20221213 23:47:54 @agent_ppo2.py:145][0m 3426304 total steps have happened
[32m[20221213 23:47:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3673 --------------------------#
[32m[20221213 23:47:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:54 @agent_ppo2.py:185][0m |           0.0022 |          54.1026 |           7.9219 |
[32m[20221213 23:47:54 @agent_ppo2.py:185][0m |          -0.0026 |          48.5314 |           7.8983 |
[32m[20221213 23:47:54 @agent_ppo2.py:185][0m |          -0.0081 |          47.4712 |           7.8496 |
[32m[20221213 23:47:54 @agent_ppo2.py:185][0m |          -0.0135 |          46.6565 |           7.8424 |
[32m[20221213 23:47:54 @agent_ppo2.py:185][0m |          -0.0036 |          49.2108 |           7.9124 |
[32m[20221213 23:47:54 @agent_ppo2.py:185][0m |          -0.0160 |          46.3248 |           7.9068 |
[32m[20221213 23:47:54 @agent_ppo2.py:185][0m |          -0.0136 |          45.6517 |           7.9381 |
[32m[20221213 23:47:55 @agent_ppo2.py:185][0m |          -0.0151 |          45.2854 |           7.9070 |
[32m[20221213 23:47:55 @agent_ppo2.py:185][0m |          -0.0184 |          45.2217 |           7.8706 |
[32m[20221213 23:47:55 @agent_ppo2.py:185][0m |          -0.0150 |          44.8307 |           7.9105 |
[32m[20221213 23:47:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.42
[32m[20221213 23:47:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.31
[32m[20221213 23:47:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.40
[32m[20221213 23:47:55 @agent_ppo2.py:143][0m Total time:      35.38 min
[32m[20221213 23:47:55 @agent_ppo2.py:145][0m 3428352 total steps have happened
[32m[20221213 23:47:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3674 --------------------------#
[32m[20221213 23:47:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:47:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:55 @agent_ppo2.py:185][0m |           0.0031 |          72.9769 |           7.5326 |
[32m[20221213 23:47:55 @agent_ppo2.py:185][0m |          -0.0076 |          64.2176 |           7.5063 |
[32m[20221213 23:47:55 @agent_ppo2.py:185][0m |          -0.0099 |          62.4808 |           7.4410 |
[32m[20221213 23:47:55 @agent_ppo2.py:185][0m |          -0.0087 |          61.4069 |           7.4762 |
[32m[20221213 23:47:56 @agent_ppo2.py:185][0m |          -0.0110 |          60.4222 |           7.4409 |
[32m[20221213 23:47:56 @agent_ppo2.py:185][0m |          -0.0120 |          59.7905 |           7.4099 |
[32m[20221213 23:47:56 @agent_ppo2.py:185][0m |          -0.0129 |          59.1222 |           7.3951 |
[32m[20221213 23:47:56 @agent_ppo2.py:185][0m |          -0.0140 |          58.6019 |           7.3873 |
[32m[20221213 23:47:56 @agent_ppo2.py:185][0m |          -0.0135 |          58.3585 |           7.3154 |
[32m[20221213 23:47:56 @agent_ppo2.py:185][0m |          -0.0150 |          57.6718 |           7.2787 |
[32m[20221213 23:47:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:47:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.71
[32m[20221213 23:47:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.37
[32m[20221213 23:47:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.16
[32m[20221213 23:47:56 @agent_ppo2.py:143][0m Total time:      35.40 min
[32m[20221213 23:47:56 @agent_ppo2.py:145][0m 3430400 total steps have happened
[32m[20221213 23:47:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3675 --------------------------#
[32m[20221213 23:47:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |           0.0017 |          69.3498 |           7.8561 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0053 |          66.3965 |           7.9151 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0083 |          65.7795 |           7.8449 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0061 |          65.4240 |           7.9516 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0082 |          64.9348 |           7.9430 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0111 |          64.6083 |           7.8782 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0127 |          64.4641 |           7.8860 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0092 |          66.7816 |           7.9019 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0127 |          64.3095 |           7.9747 |
[32m[20221213 23:47:57 @agent_ppo2.py:185][0m |          -0.0113 |          64.1971 |           7.8971 |
[32m[20221213 23:47:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.57
[32m[20221213 23:47:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.86
[32m[20221213 23:47:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.98
[32m[20221213 23:47:57 @agent_ppo2.py:143][0m Total time:      35.42 min
[32m[20221213 23:47:57 @agent_ppo2.py:145][0m 3432448 total steps have happened
[32m[20221213 23:47:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3676 --------------------------#
[32m[20221213 23:47:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |           0.0031 |          78.3740 |           7.4537 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0004 |          73.3154 |           7.4970 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0101 |          71.4848 |           7.4866 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0108 |          70.8370 |           7.4848 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0124 |          69.9356 |           7.5128 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0147 |          69.7589 |           7.4798 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0135 |          69.3401 |           7.5405 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0119 |          69.2645 |           7.4820 |
[32m[20221213 23:47:58 @agent_ppo2.py:185][0m |          -0.0142 |          68.8487 |           7.5009 |
[32m[20221213 23:47:59 @agent_ppo2.py:185][0m |          -0.0157 |          68.6697 |           7.4969 |
[32m[20221213 23:47:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:47:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.82
[32m[20221213 23:47:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.24
[32m[20221213 23:47:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.53
[32m[20221213 23:47:59 @agent_ppo2.py:143][0m Total time:      35.44 min
[32m[20221213 23:47:59 @agent_ppo2.py:145][0m 3434496 total steps have happened
[32m[20221213 23:47:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3677 --------------------------#
[32m[20221213 23:47:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:47:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:47:59 @agent_ppo2.py:185][0m |          -0.0010 |          62.3892 |           7.5468 |
[32m[20221213 23:47:59 @agent_ppo2.py:185][0m |          -0.0050 |          61.0945 |           7.4969 |
[32m[20221213 23:47:59 @agent_ppo2.py:185][0m |          -0.0079 |          60.7464 |           7.4719 |
[32m[20221213 23:47:59 @agent_ppo2.py:185][0m |          -0.0075 |          60.2974 |           7.4738 |
[32m[20221213 23:47:59 @agent_ppo2.py:185][0m |          -0.0076 |          59.8808 |           7.4685 |
[32m[20221213 23:47:59 @agent_ppo2.py:185][0m |          -0.0125 |          59.9135 |           7.4486 |
[32m[20221213 23:48:00 @agent_ppo2.py:185][0m |          -0.0126 |          59.7608 |           7.4394 |
[32m[20221213 23:48:00 @agent_ppo2.py:185][0m |          -0.0119 |          59.4947 |           7.3817 |
[32m[20221213 23:48:00 @agent_ppo2.py:185][0m |          -0.0125 |          59.2516 |           7.4125 |
[32m[20221213 23:48:00 @agent_ppo2.py:185][0m |          -0.0061 |          62.9904 |           7.3666 |
[32m[20221213 23:48:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.08
[32m[20221213 23:48:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.44
[32m[20221213 23:48:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.96
[32m[20221213 23:48:00 @agent_ppo2.py:143][0m Total time:      35.47 min
[32m[20221213 23:48:00 @agent_ppo2.py:145][0m 3436544 total steps have happened
[32m[20221213 23:48:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3678 --------------------------#
[32m[20221213 23:48:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:00 @agent_ppo2.py:185][0m |          -0.0004 |          63.5403 |           7.2516 |
[32m[20221213 23:48:00 @agent_ppo2.py:185][0m |          -0.0032 |          61.8227 |           7.3861 |
[32m[20221213 23:48:00 @agent_ppo2.py:185][0m |           0.0078 |          69.4238 |           7.4036 |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |          -0.0017 |          61.1405 |           7.4977 |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |           0.0085 |          66.0166 |           7.5044 |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |          -0.0091 |          60.2604 |           7.5370 |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |          -0.0075 |          59.9887 |           7.5543 |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |          -0.0102 |          59.8561 |           7.5730 |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |          -0.0055 |          60.7556 |           7.6713 |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |          -0.0100 |          59.7030 |           7.6324 |
[32m[20221213 23:48:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:48:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.49
[32m[20221213 23:48:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.77
[32m[20221213 23:48:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.28
[32m[20221213 23:48:01 @agent_ppo2.py:143][0m Total time:      35.49 min
[32m[20221213 23:48:01 @agent_ppo2.py:145][0m 3438592 total steps have happened
[32m[20221213 23:48:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3679 --------------------------#
[32m[20221213 23:48:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:01 @agent_ppo2.py:185][0m |           0.0048 |          50.1328 |           7.7555 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |           0.0024 |          46.5265 |           7.7619 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0064 |          44.8979 |           7.8250 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0058 |          44.2310 |           7.7670 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0028 |          44.7823 |           7.7767 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0081 |          42.9720 |           7.8000 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0105 |          42.5969 |           7.7739 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0002 |          49.2795 |           7.7767 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0044 |          46.6702 |           7.8022 |
[32m[20221213 23:48:02 @agent_ppo2.py:185][0m |          -0.0134 |          41.4558 |           7.7841 |
[32m[20221213 23:48:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:48:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.26
[32m[20221213 23:48:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.70
[32m[20221213 23:48:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.77
[32m[20221213 23:48:02 @agent_ppo2.py:143][0m Total time:      35.51 min
[32m[20221213 23:48:02 @agent_ppo2.py:145][0m 3440640 total steps have happened
[32m[20221213 23:48:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3680 --------------------------#
[32m[20221213 23:48:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:48:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0036 |          68.5262 |           7.4219 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0052 |          66.2350 |           7.4176 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0109 |          64.1823 |           7.4292 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0093 |          63.4844 |           7.4341 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0111 |          63.6725 |           7.4176 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0130 |          63.5399 |           7.3821 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0173 |          62.2581 |           7.3982 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0148 |          61.7521 |           7.4356 |
[32m[20221213 23:48:03 @agent_ppo2.py:185][0m |          -0.0170 |          61.6288 |           7.4486 |
[32m[20221213 23:48:04 @agent_ppo2.py:185][0m |          -0.0149 |          61.2615 |           7.3929 |
[32m[20221213 23:48:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.76
[32m[20221213 23:48:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.63
[32m[20221213 23:48:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.04
[32m[20221213 23:48:04 @agent_ppo2.py:143][0m Total time:      35.53 min
[32m[20221213 23:48:04 @agent_ppo2.py:145][0m 3442688 total steps have happened
[32m[20221213 23:48:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3681 --------------------------#
[32m[20221213 23:48:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:04 @agent_ppo2.py:185][0m |          -0.0003 |          54.9817 |           7.4142 |
[32m[20221213 23:48:04 @agent_ppo2.py:185][0m |          -0.0104 |          48.6090 |           7.3100 |
[32m[20221213 23:48:04 @agent_ppo2.py:185][0m |           0.0090 |          50.8461 |           7.4214 |
[32m[20221213 23:48:04 @agent_ppo2.py:185][0m |          -0.0092 |          46.1939 |           7.4114 |
[32m[20221213 23:48:04 @agent_ppo2.py:185][0m |          -0.0141 |          45.4950 |           7.4284 |
[32m[20221213 23:48:04 @agent_ppo2.py:185][0m |          -0.0116 |          45.1277 |           7.4337 |
[32m[20221213 23:48:05 @agent_ppo2.py:185][0m |          -0.0125 |          44.6690 |           7.4217 |
[32m[20221213 23:48:05 @agent_ppo2.py:185][0m |          -0.0034 |          47.2927 |           7.4419 |
[32m[20221213 23:48:05 @agent_ppo2.py:185][0m |           0.0001 |          49.4041 |           7.4620 |
[32m[20221213 23:48:05 @agent_ppo2.py:185][0m |          -0.0165 |          44.0344 |           7.4961 |
[32m[20221213 23:48:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.88
[32m[20221213 23:48:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.49
[32m[20221213 23:48:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.29
[32m[20221213 23:48:05 @agent_ppo2.py:143][0m Total time:      35.55 min
[32m[20221213 23:48:05 @agent_ppo2.py:145][0m 3444736 total steps have happened
[32m[20221213 23:48:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3682 --------------------------#
[32m[20221213 23:48:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:05 @agent_ppo2.py:185][0m |          -0.0024 |          55.5331 |           6.8343 |
[32m[20221213 23:48:05 @agent_ppo2.py:185][0m |          -0.0065 |          51.5868 |           6.9331 |
[32m[20221213 23:48:05 @agent_ppo2.py:185][0m |          -0.0082 |          49.9823 |           6.8864 |
[32m[20221213 23:48:06 @agent_ppo2.py:185][0m |          -0.0060 |          49.0976 |           6.9294 |
[32m[20221213 23:48:06 @agent_ppo2.py:185][0m |          -0.0067 |          48.5939 |           6.9764 |
[32m[20221213 23:48:06 @agent_ppo2.py:185][0m |          -0.0019 |          52.1588 |           7.0009 |
[32m[20221213 23:48:06 @agent_ppo2.py:185][0m |          -0.0125 |          47.3796 |           7.0520 |
[32m[20221213 23:48:06 @agent_ppo2.py:185][0m |          -0.0147 |          47.0008 |           7.0013 |
[32m[20221213 23:48:06 @agent_ppo2.py:185][0m |          -0.0107 |          46.5551 |           7.0542 |
[32m[20221213 23:48:06 @agent_ppo2.py:185][0m |          -0.0132 |          46.2988 |           7.0611 |
[32m[20221213 23:48:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.76
[32m[20221213 23:48:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.29
[32m[20221213 23:48:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.20
[32m[20221213 23:48:06 @agent_ppo2.py:143][0m Total time:      35.57 min
[32m[20221213 23:48:06 @agent_ppo2.py:145][0m 3446784 total steps have happened
[32m[20221213 23:48:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3683 --------------------------#
[32m[20221213 23:48:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |           0.0104 |          62.3791 |           7.6432 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0056 |          58.3227 |           7.6575 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0008 |          60.5523 |           7.6344 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0077 |          57.2025 |           7.6240 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0118 |          56.9199 |           7.6433 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0115 |          56.7616 |           7.6224 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0072 |          59.9347 |           7.7057 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0108 |          56.4960 |           7.7300 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0112 |          56.1243 |           7.7524 |
[32m[20221213 23:48:07 @agent_ppo2.py:185][0m |          -0.0107 |          56.2971 |           7.7128 |
[32m[20221213 23:48:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:48:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.56
[32m[20221213 23:48:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.31
[32m[20221213 23:48:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.54
[32m[20221213 23:48:07 @agent_ppo2.py:143][0m Total time:      35.59 min
[32m[20221213 23:48:07 @agent_ppo2.py:145][0m 3448832 total steps have happened
[32m[20221213 23:48:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3684 --------------------------#
[32m[20221213 23:48:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |           0.0010 |          80.1868 |           8.4091 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0055 |          74.8949 |           8.4358 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0038 |          75.6487 |           8.4532 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0070 |          71.3620 |           8.4358 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0090 |          70.3553 |           8.4450 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0076 |          69.5547 |           8.4508 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0124 |          69.0823 |           8.4063 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0134 |          68.5817 |           8.4684 |
[32m[20221213 23:48:08 @agent_ppo2.py:185][0m |          -0.0084 |          68.5117 |           8.5213 |
[32m[20221213 23:48:09 @agent_ppo2.py:185][0m |          -0.0116 |          67.8527 |           8.5149 |
[32m[20221213 23:48:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:48:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.97
[32m[20221213 23:48:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.93
[32m[20221213 23:48:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.93
[32m[20221213 23:48:09 @agent_ppo2.py:143][0m Total time:      35.61 min
[32m[20221213 23:48:09 @agent_ppo2.py:145][0m 3450880 total steps have happened
[32m[20221213 23:48:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3685 --------------------------#
[32m[20221213 23:48:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:09 @agent_ppo2.py:185][0m |           0.0141 |          74.1156 |           8.0492 |
[32m[20221213 23:48:09 @agent_ppo2.py:185][0m |          -0.0049 |          63.6934 |           8.1058 |
[32m[20221213 23:48:09 @agent_ppo2.py:185][0m |          -0.0086 |          61.2513 |           8.2439 |
[32m[20221213 23:48:09 @agent_ppo2.py:185][0m |          -0.0107 |          60.5964 |           8.2503 |
[32m[20221213 23:48:09 @agent_ppo2.py:185][0m |          -0.0081 |          59.7362 |           8.2573 |
[32m[20221213 23:48:09 @agent_ppo2.py:185][0m |          -0.0108 |          59.2113 |           8.2795 |
[32m[20221213 23:48:10 @agent_ppo2.py:185][0m |          -0.0057 |          59.5499 |           8.3063 |
[32m[20221213 23:48:10 @agent_ppo2.py:185][0m |          -0.0153 |          59.3009 |           8.3012 |
[32m[20221213 23:48:10 @agent_ppo2.py:185][0m |          -0.0138 |          58.2788 |           8.3405 |
[32m[20221213 23:48:10 @agent_ppo2.py:185][0m |          -0.0134 |          57.9858 |           8.2745 |
[32m[20221213 23:48:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.37
[32m[20221213 23:48:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.54
[32m[20221213 23:48:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.10
[32m[20221213 23:48:10 @agent_ppo2.py:143][0m Total time:      35.63 min
[32m[20221213 23:48:10 @agent_ppo2.py:145][0m 3452928 total steps have happened
[32m[20221213 23:48:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3686 --------------------------#
[32m[20221213 23:48:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:10 @agent_ppo2.py:185][0m |          -0.0032 |          77.7165 |           8.0921 |
[32m[20221213 23:48:10 @agent_ppo2.py:185][0m |          -0.0057 |          74.0489 |           8.0985 |
[32m[20221213 23:48:10 @agent_ppo2.py:185][0m |          -0.0092 |          72.8941 |           8.1614 |
[32m[20221213 23:48:11 @agent_ppo2.py:185][0m |          -0.0033 |          73.3275 |           8.1951 |
[32m[20221213 23:48:11 @agent_ppo2.py:185][0m |          -0.0114 |          71.5701 |           8.3196 |
[32m[20221213 23:48:11 @agent_ppo2.py:185][0m |          -0.0072 |          73.0408 |           8.1786 |
[32m[20221213 23:48:11 @agent_ppo2.py:185][0m |          -0.0121 |          70.7756 |           8.2353 |
[32m[20221213 23:48:11 @agent_ppo2.py:185][0m |          -0.0055 |          71.4120 |           8.2977 |
[32m[20221213 23:48:11 @agent_ppo2.py:185][0m |          -0.0141 |          70.2075 |           8.2464 |
[32m[20221213 23:48:11 @agent_ppo2.py:185][0m |          -0.0129 |          69.8298 |           8.2954 |
[32m[20221213 23:48:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.15
[32m[20221213 23:48:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.41
[32m[20221213 23:48:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.62
[32m[20221213 23:48:11 @agent_ppo2.py:143][0m Total time:      35.65 min
[32m[20221213 23:48:11 @agent_ppo2.py:145][0m 3454976 total steps have happened
[32m[20221213 23:48:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3687 --------------------------#
[32m[20221213 23:48:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |           0.0048 |          64.3112 |           8.2963 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0037 |          62.4814 |           8.2910 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0071 |          61.6053 |           8.2174 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0080 |          61.3252 |           8.2156 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0107 |          61.0585 |           8.2149 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0094 |          60.6729 |           8.1696 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0060 |          61.6425 |           8.1759 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0042 |          63.7761 |           8.1252 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0112 |          60.1552 |           8.1678 |
[32m[20221213 23:48:12 @agent_ppo2.py:185][0m |          -0.0104 |          59.9252 |           8.1134 |
[32m[20221213 23:48:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:48:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.41
[32m[20221213 23:48:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.45
[32m[20221213 23:48:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.14
[32m[20221213 23:48:12 @agent_ppo2.py:143][0m Total time:      35.67 min
[32m[20221213 23:48:12 @agent_ppo2.py:145][0m 3457024 total steps have happened
[32m[20221213 23:48:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3688 --------------------------#
[32m[20221213 23:48:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |           0.0120 |          69.8516 |           8.4932 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |           0.0004 |          65.7335 |           8.5020 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |          -0.0035 |          65.8238 |           8.4483 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |          -0.0077 |          64.5176 |           8.4400 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |          -0.0091 |          64.2626 |           8.4362 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |          -0.0067 |          64.0198 |           8.4055 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |          -0.0104 |          63.6632 |           8.3920 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |          -0.0057 |          64.7773 |           8.4197 |
[32m[20221213 23:48:13 @agent_ppo2.py:185][0m |          -0.0094 |          63.7165 |           8.4161 |
[32m[20221213 23:48:14 @agent_ppo2.py:185][0m |          -0.0105 |          63.2225 |           8.3845 |
[32m[20221213 23:48:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:48:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.46
[32m[20221213 23:48:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.70
[32m[20221213 23:48:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.76
[32m[20221213 23:48:14 @agent_ppo2.py:143][0m Total time:      35.69 min
[32m[20221213 23:48:14 @agent_ppo2.py:145][0m 3459072 total steps have happened
[32m[20221213 23:48:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3689 --------------------------#
[32m[20221213 23:48:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:48:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:14 @agent_ppo2.py:185][0m |           0.0106 |          70.6724 |           7.5431 |
[32m[20221213 23:48:14 @agent_ppo2.py:185][0m |          -0.0064 |          58.4407 |           7.5968 |
[32m[20221213 23:48:14 @agent_ppo2.py:185][0m |          -0.0089 |          56.9744 |           7.6594 |
[32m[20221213 23:48:14 @agent_ppo2.py:185][0m |          -0.0036 |          62.0300 |           7.5989 |
[32m[20221213 23:48:14 @agent_ppo2.py:185][0m |          -0.0127 |          55.5486 |           7.6746 |
[32m[20221213 23:48:14 @agent_ppo2.py:185][0m |          -0.0150 |          55.0455 |           7.6422 |
[32m[20221213 23:48:15 @agent_ppo2.py:185][0m |          -0.0152 |          54.5302 |           7.6756 |
[32m[20221213 23:48:15 @agent_ppo2.py:185][0m |          -0.0048 |          57.6845 |           7.6881 |
[32m[20221213 23:48:15 @agent_ppo2.py:185][0m |          -0.0100 |          57.4596 |           7.7525 |
[32m[20221213 23:48:15 @agent_ppo2.py:185][0m |          -0.0172 |          53.0889 |           7.7667 |
[32m[20221213 23:48:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.19
[32m[20221213 23:48:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.26
[32m[20221213 23:48:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.72
[32m[20221213 23:48:15 @agent_ppo2.py:143][0m Total time:      35.72 min
[32m[20221213 23:48:15 @agent_ppo2.py:145][0m 3461120 total steps have happened
[32m[20221213 23:48:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3690 --------------------------#
[32m[20221213 23:48:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:48:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:15 @agent_ppo2.py:185][0m |           0.0011 |          66.5504 |           8.2535 |
[32m[20221213 23:48:15 @agent_ppo2.py:185][0m |          -0.0037 |          65.0389 |           8.2720 |
[32m[20221213 23:48:15 @agent_ppo2.py:185][0m |          -0.0066 |          64.4639 |           8.2507 |
[32m[20221213 23:48:16 @agent_ppo2.py:185][0m |           0.0083 |          71.3754 |           8.2321 |
[32m[20221213 23:48:16 @agent_ppo2.py:185][0m |          -0.0061 |          64.1429 |           8.1839 |
[32m[20221213 23:48:16 @agent_ppo2.py:185][0m |          -0.0101 |          63.9594 |           8.2327 |
[32m[20221213 23:48:16 @agent_ppo2.py:185][0m |          -0.0098 |          63.4868 |           8.1694 |
[32m[20221213 23:48:16 @agent_ppo2.py:185][0m |          -0.0086 |          63.4331 |           8.2410 |
[32m[20221213 23:48:16 @agent_ppo2.py:185][0m |          -0.0089 |          63.3505 |           8.1837 |
[32m[20221213 23:48:16 @agent_ppo2.py:185][0m |          -0.0093 |          63.2911 |           8.1387 |
[32m[20221213 23:48:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:48:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.62
[32m[20221213 23:48:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.72
[32m[20221213 23:48:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 219.30
[32m[20221213 23:48:16 @agent_ppo2.py:143][0m Total time:      35.74 min
[32m[20221213 23:48:16 @agent_ppo2.py:145][0m 3463168 total steps have happened
[32m[20221213 23:48:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3691 --------------------------#
[32m[20221213 23:48:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |           0.0010 |          69.1133 |           7.9415 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0037 |          67.8289 |           7.9456 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0048 |          67.1605 |           8.0107 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0042 |          67.2207 |           8.0845 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0082 |          66.4664 |           8.0745 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0070 |          66.3024 |           8.0745 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0088 |          65.9731 |           8.1150 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0082 |          65.7467 |           8.1353 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0073 |          65.9539 |           8.1841 |
[32m[20221213 23:48:17 @agent_ppo2.py:185][0m |          -0.0086 |          65.8695 |           8.1688 |
[32m[20221213 23:48:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.21
[32m[20221213 23:48:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.16
[32m[20221213 23:48:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.22
[32m[20221213 23:48:17 @agent_ppo2.py:143][0m Total time:      35.76 min
[32m[20221213 23:48:17 @agent_ppo2.py:145][0m 3465216 total steps have happened
[32m[20221213 23:48:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3692 --------------------------#
[32m[20221213 23:48:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |           0.0004 |          54.5221 |           8.0229 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |           0.0091 |          51.9769 |           8.0601 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |          -0.0055 |          46.8033 |           8.1323 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |          -0.0095 |          45.6369 |           8.1250 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |          -0.0125 |          44.9297 |           8.0529 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |          -0.0149 |          44.7066 |           8.1170 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |          -0.0136 |          44.1581 |           8.1154 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |          -0.0172 |          43.4174 |           8.0989 |
[32m[20221213 23:48:18 @agent_ppo2.py:185][0m |          -0.0169 |          43.2708 |           8.0948 |
[32m[20221213 23:48:19 @agent_ppo2.py:185][0m |          -0.0146 |          42.9442 |           8.0843 |
[32m[20221213 23:48:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.03
[32m[20221213 23:48:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.64
[32m[20221213 23:48:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.84
[32m[20221213 23:48:19 @agent_ppo2.py:143][0m Total time:      35.78 min
[32m[20221213 23:48:19 @agent_ppo2.py:145][0m 3467264 total steps have happened
[32m[20221213 23:48:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3693 --------------------------#
[32m[20221213 23:48:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:19 @agent_ppo2.py:185][0m |           0.0083 |          55.2688 |           7.8964 |
[32m[20221213 23:48:19 @agent_ppo2.py:185][0m |          -0.0074 |          44.2822 |           7.8363 |
[32m[20221213 23:48:19 @agent_ppo2.py:185][0m |          -0.0077 |          42.5129 |           7.8538 |
[32m[20221213 23:48:19 @agent_ppo2.py:185][0m |          -0.0087 |          41.4563 |           7.7787 |
[32m[20221213 23:48:19 @agent_ppo2.py:185][0m |          -0.0126 |          40.5110 |           7.8042 |
[32m[20221213 23:48:19 @agent_ppo2.py:185][0m |          -0.0171 |          40.1065 |           7.8368 |
[32m[20221213 23:48:20 @agent_ppo2.py:185][0m |          -0.0119 |          38.9826 |           7.7919 |
[32m[20221213 23:48:20 @agent_ppo2.py:185][0m |          -0.0099 |          38.9285 |           7.8518 |
[32m[20221213 23:48:20 @agent_ppo2.py:185][0m |          -0.0152 |          37.9626 |           7.8885 |
[32m[20221213 23:48:20 @agent_ppo2.py:185][0m |          -0.0164 |          37.6217 |           7.8608 |
[32m[20221213 23:48:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:48:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.79
[32m[20221213 23:48:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.90
[32m[20221213 23:48:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.86
[32m[20221213 23:48:20 @agent_ppo2.py:143][0m Total time:      35.80 min
[32m[20221213 23:48:20 @agent_ppo2.py:145][0m 3469312 total steps have happened
[32m[20221213 23:48:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3694 --------------------------#
[32m[20221213 23:48:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:20 @agent_ppo2.py:185][0m |           0.0062 |          61.0578 |           8.2389 |
[32m[20221213 23:48:20 @agent_ppo2.py:185][0m |          -0.0078 |          53.3948 |           8.2835 |
[32m[20221213 23:48:20 @agent_ppo2.py:185][0m |          -0.0106 |          52.6805 |           8.2709 |
[32m[20221213 23:48:21 @agent_ppo2.py:185][0m |          -0.0114 |          52.1616 |           8.3473 |
[32m[20221213 23:48:21 @agent_ppo2.py:185][0m |          -0.0118 |          51.7339 |           8.3331 |
[32m[20221213 23:48:21 @agent_ppo2.py:185][0m |          -0.0062 |          54.4786 |           8.3540 |
[32m[20221213 23:48:21 @agent_ppo2.py:185][0m |          -0.0124 |          51.2010 |           8.3768 |
[32m[20221213 23:48:21 @agent_ppo2.py:185][0m |          -0.0145 |          51.1545 |           8.4083 |
[32m[20221213 23:48:21 @agent_ppo2.py:185][0m |          -0.0083 |          51.6511 |           8.3575 |
[32m[20221213 23:48:21 @agent_ppo2.py:185][0m |          -0.0154 |          50.7273 |           8.4091 |
[32m[20221213 23:48:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:48:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.05
[32m[20221213 23:48:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.07
[32m[20221213 23:48:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.71
[32m[20221213 23:48:21 @agent_ppo2.py:143][0m Total time:      35.82 min
[32m[20221213 23:48:21 @agent_ppo2.py:145][0m 3471360 total steps have happened
[32m[20221213 23:48:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3695 --------------------------#
[32m[20221213 23:48:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |           0.0028 |          60.9656 |           8.5232 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0093 |          59.2469 |           8.5464 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0083 |          58.9130 |           8.5382 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0076 |          58.5680 |           8.5299 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0112 |          58.3977 |           8.5871 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0103 |          58.1368 |           8.6526 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0137 |          57.8305 |           8.5913 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0130 |          57.7529 |           8.5835 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0062 |          62.2950 |           8.5628 |
[32m[20221213 23:48:22 @agent_ppo2.py:185][0m |          -0.0122 |          58.0095 |           8.5683 |
[32m[20221213 23:48:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.60
[32m[20221213 23:48:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.57
[32m[20221213 23:48:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.38
[32m[20221213 23:48:22 @agent_ppo2.py:143][0m Total time:      35.84 min
[32m[20221213 23:48:22 @agent_ppo2.py:145][0m 3473408 total steps have happened
[32m[20221213 23:48:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3696 --------------------------#
[32m[20221213 23:48:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0007 |          62.1456 |           7.6196 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0123 |          59.0571 |           7.6416 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |           0.0046 |          63.4090 |           7.6914 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0110 |          56.1358 |           7.7058 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0152 |          55.3200 |           7.6744 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0150 |          54.6733 |           7.6888 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0133 |          55.6096 |           7.7665 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0181 |          54.0024 |           7.7394 |
[32m[20221213 23:48:23 @agent_ppo2.py:185][0m |          -0.0175 |          53.7350 |           7.7522 |
[32m[20221213 23:48:24 @agent_ppo2.py:185][0m |          -0.0190 |          53.6209 |           7.7566 |
[32m[20221213 23:48:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:48:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.30
[32m[20221213 23:48:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.17
[32m[20221213 23:48:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.87
[32m[20221213 23:48:24 @agent_ppo2.py:143][0m Total time:      35.86 min
[32m[20221213 23:48:24 @agent_ppo2.py:145][0m 3475456 total steps have happened
[32m[20221213 23:48:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3697 --------------------------#
[32m[20221213 23:48:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:24 @agent_ppo2.py:185][0m |           0.0043 |          60.1706 |           8.3827 |
[32m[20221213 23:48:24 @agent_ppo2.py:185][0m |          -0.0080 |          56.1403 |           8.3218 |
[32m[20221213 23:48:24 @agent_ppo2.py:185][0m |          -0.0069 |          56.0412 |           8.3664 |
[32m[20221213 23:48:24 @agent_ppo2.py:185][0m |          -0.0089 |          54.3831 |           8.2876 |
[32m[20221213 23:48:24 @agent_ppo2.py:185][0m |          -0.0085 |          53.6913 |           8.2624 |
[32m[20221213 23:48:25 @agent_ppo2.py:185][0m |          -0.0182 |          53.2579 |           8.2256 |
[32m[20221213 23:48:25 @agent_ppo2.py:185][0m |          -0.0147 |          53.1071 |           8.2323 |
[32m[20221213 23:48:25 @agent_ppo2.py:185][0m |          -0.0197 |          52.5733 |           8.1831 |
[32m[20221213 23:48:25 @agent_ppo2.py:185][0m |          -0.0143 |          52.3197 |           8.1439 |
[32m[20221213 23:48:25 @agent_ppo2.py:185][0m |          -0.0172 |          51.9885 |           8.1081 |
[32m[20221213 23:48:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:48:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.36
[32m[20221213 23:48:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.20
[32m[20221213 23:48:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.26
[32m[20221213 23:48:25 @agent_ppo2.py:143][0m Total time:      35.88 min
[32m[20221213 23:48:25 @agent_ppo2.py:145][0m 3477504 total steps have happened
[32m[20221213 23:48:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3698 --------------------------#
[32m[20221213 23:48:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:25 @agent_ppo2.py:185][0m |          -0.0012 |          62.1905 |           7.8828 |
[32m[20221213 23:48:25 @agent_ppo2.py:185][0m |          -0.0071 |          59.8814 |           7.9564 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |           0.0049 |          66.3625 |           7.8716 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |          -0.0082 |          58.8017 |           7.8666 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |          -0.0099 |          58.5908 |           7.9272 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |          -0.0099 |          58.8427 |           7.8837 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |          -0.0109 |          58.2276 |           7.8881 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |          -0.0140 |          58.0985 |           7.9413 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |          -0.0133 |          57.9419 |           7.8654 |
[32m[20221213 23:48:26 @agent_ppo2.py:185][0m |          -0.0079 |          58.2383 |           7.8513 |
[32m[20221213 23:48:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:48:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.95
[32m[20221213 23:48:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.87
[32m[20221213 23:48:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.04
[32m[20221213 23:48:26 @agent_ppo2.py:143][0m Total time:      35.90 min
[32m[20221213 23:48:26 @agent_ppo2.py:145][0m 3479552 total steps have happened
[32m[20221213 23:48:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3699 --------------------------#
[32m[20221213 23:48:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |           0.0001 |          65.5909 |           8.2670 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0068 |          61.8951 |           8.3081 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0094 |          60.3625 |           8.3683 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |           0.0001 |          66.0766 |           8.3575 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0122 |          58.8663 |           8.3880 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0144 |          58.1245 |           8.3412 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0121 |          57.7809 |           8.4283 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0124 |          57.3605 |           8.4436 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0079 |          58.5500 |           8.4874 |
[32m[20221213 23:48:27 @agent_ppo2.py:185][0m |          -0.0157 |          56.9945 |           8.4843 |
[32m[20221213 23:48:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:48:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.52
[32m[20221213 23:48:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.50
[32m[20221213 23:48:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.62
[32m[20221213 23:48:28 @agent_ppo2.py:143][0m Total time:      35.93 min
[32m[20221213 23:48:28 @agent_ppo2.py:145][0m 3481600 total steps have happened
[32m[20221213 23:48:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3700 --------------------------#
[32m[20221213 23:48:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:48:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:28 @agent_ppo2.py:185][0m |          -0.0030 |          59.9906 |           7.9708 |
[32m[20221213 23:48:28 @agent_ppo2.py:185][0m |          -0.0043 |          58.1455 |           7.9574 |
[32m[20221213 23:48:28 @agent_ppo2.py:185][0m |           0.0063 |          61.4259 |           8.0302 |
[32m[20221213 23:48:28 @agent_ppo2.py:185][0m |          -0.0106 |          57.0916 |           8.0484 |
[32m[20221213 23:48:28 @agent_ppo2.py:185][0m |          -0.0117 |          56.9400 |           8.0377 |
[32m[20221213 23:48:28 @agent_ppo2.py:185][0m |          -0.0092 |          56.5464 |           8.0458 |
[32m[20221213 23:48:28 @agent_ppo2.py:185][0m |          -0.0089 |          56.3758 |           8.0630 |
[32m[20221213 23:48:29 @agent_ppo2.py:185][0m |          -0.0119 |          56.3266 |           8.0430 |
[32m[20221213 23:48:29 @agent_ppo2.py:185][0m |          -0.0117 |          56.1531 |           8.0566 |
[32m[20221213 23:48:29 @agent_ppo2.py:185][0m |          -0.0106 |          56.1806 |           8.1230 |
[32m[20221213 23:48:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.58
[32m[20221213 23:48:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.41
[32m[20221213 23:48:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.52
[32m[20221213 23:48:29 @agent_ppo2.py:143][0m Total time:      35.95 min
[32m[20221213 23:48:29 @agent_ppo2.py:145][0m 3483648 total steps have happened
[32m[20221213 23:48:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3701 --------------------------#
[32m[20221213 23:48:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:29 @agent_ppo2.py:185][0m |           0.0015 |          59.0740 |           7.8716 |
[32m[20221213 23:48:29 @agent_ppo2.py:185][0m |          -0.0025 |          57.2740 |           7.9286 |
[32m[20221213 23:48:29 @agent_ppo2.py:185][0m |           0.0046 |          62.5215 |           7.8974 |
[32m[20221213 23:48:29 @agent_ppo2.py:185][0m |           0.0019 |          58.6328 |           7.9330 |
[32m[20221213 23:48:30 @agent_ppo2.py:185][0m |          -0.0058 |          55.9624 |           7.9671 |
[32m[20221213 23:48:30 @agent_ppo2.py:185][0m |          -0.0079 |          56.1023 |           7.9812 |
[32m[20221213 23:48:30 @agent_ppo2.py:185][0m |          -0.0116 |          55.7165 |           7.9747 |
[32m[20221213 23:48:30 @agent_ppo2.py:185][0m |          -0.0117 |          55.2343 |           7.9304 |
[32m[20221213 23:48:30 @agent_ppo2.py:185][0m |          -0.0058 |          55.4713 |           7.9524 |
[32m[20221213 23:48:30 @agent_ppo2.py:185][0m |          -0.0141 |          55.0646 |           7.9749 |
[32m[20221213 23:48:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:48:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.21
[32m[20221213 23:48:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.38
[32m[20221213 23:48:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.01
[32m[20221213 23:48:30 @agent_ppo2.py:143][0m Total time:      35.97 min
[32m[20221213 23:48:30 @agent_ppo2.py:145][0m 3485696 total steps have happened
[32m[20221213 23:48:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3702 --------------------------#
[32m[20221213 23:48:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:30 @agent_ppo2.py:185][0m |           0.0038 |          44.5341 |           8.2568 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0096 |          37.6083 |           8.3444 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0093 |          36.0558 |           8.3898 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0166 |          34.8932 |           8.3530 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0160 |          34.2056 |           8.3943 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0116 |          33.7314 |           8.4128 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0170 |          32.9109 |           8.4127 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0132 |          32.4055 |           8.3641 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0218 |          32.0860 |           8.3774 |
[32m[20221213 23:48:31 @agent_ppo2.py:185][0m |          -0.0235 |          31.7213 |           8.4434 |
[32m[20221213 23:48:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:48:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.92
[32m[20221213 23:48:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.80
[32m[20221213 23:48:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.34
[32m[20221213 23:48:31 @agent_ppo2.py:143][0m Total time:      35.99 min
[32m[20221213 23:48:31 @agent_ppo2.py:145][0m 3487744 total steps have happened
[32m[20221213 23:48:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3703 --------------------------#
[32m[20221213 23:48:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |           0.0081 |          54.6595 |           8.0171 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0031 |          47.9341 |           7.9837 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0052 |          46.5067 |           7.9856 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0089 |          44.9503 |           8.0177 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0119 |          44.2408 |           8.0257 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0137 |          44.1123 |           7.9806 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0190 |          42.9772 |           7.9917 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0141 |          42.5666 |           8.0206 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0212 |          41.9895 |           7.9711 |
[32m[20221213 23:48:32 @agent_ppo2.py:185][0m |          -0.0094 |          45.3873 |           7.9908 |
[32m[20221213 23:48:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:48:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.11
[32m[20221213 23:48:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 547.25
[32m[20221213 23:48:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.96
[32m[20221213 23:48:33 @agent_ppo2.py:143][0m Total time:      36.01 min
[32m[20221213 23:48:33 @agent_ppo2.py:145][0m 3489792 total steps have happened
[32m[20221213 23:48:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3704 --------------------------#
[32m[20221213 23:48:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:33 @agent_ppo2.py:185][0m |           0.0037 |          60.4307 |           8.4539 |
[32m[20221213 23:48:33 @agent_ppo2.py:185][0m |           0.0007 |          57.9713 |           8.5680 |
[32m[20221213 23:48:33 @agent_ppo2.py:185][0m |          -0.0073 |          56.9574 |           8.5945 |
[32m[20221213 23:48:33 @agent_ppo2.py:185][0m |          -0.0068 |          56.6030 |           8.5545 |
[32m[20221213 23:48:33 @agent_ppo2.py:185][0m |          -0.0110 |          56.0833 |           8.6300 |
[32m[20221213 23:48:33 @agent_ppo2.py:185][0m |          -0.0114 |          55.9648 |           8.6316 |
[32m[20221213 23:48:33 @agent_ppo2.py:185][0m |          -0.0079 |          55.5714 |           8.5725 |
[32m[20221213 23:48:34 @agent_ppo2.py:185][0m |          -0.0124 |          55.3081 |           8.6302 |
[32m[20221213 23:48:34 @agent_ppo2.py:185][0m |          -0.0106 |          55.2386 |           8.6547 |
[32m[20221213 23:48:34 @agent_ppo2.py:185][0m |          -0.0111 |          55.0146 |           8.6551 |
[32m[20221213 23:48:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.11
[32m[20221213 23:48:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.69
[32m[20221213 23:48:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.21
[32m[20221213 23:48:34 @agent_ppo2.py:143][0m Total time:      36.03 min
[32m[20221213 23:48:34 @agent_ppo2.py:145][0m 3491840 total steps have happened
[32m[20221213 23:48:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3705 --------------------------#
[32m[20221213 23:48:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:34 @agent_ppo2.py:185][0m |           0.0019 |          36.3519 |           8.6789 |
[32m[20221213 23:48:34 @agent_ppo2.py:185][0m |          -0.0015 |          31.3590 |           8.6040 |
[32m[20221213 23:48:34 @agent_ppo2.py:185][0m |          -0.0096 |          29.6007 |           8.6449 |
[32m[20221213 23:48:34 @agent_ppo2.py:185][0m |          -0.0078 |          28.8122 |           8.6387 |
[32m[20221213 23:48:35 @agent_ppo2.py:185][0m |          -0.0140 |          27.7799 |           8.6891 |
[32m[20221213 23:48:35 @agent_ppo2.py:185][0m |          -0.0111 |          27.2213 |           8.6589 |
[32m[20221213 23:48:35 @agent_ppo2.py:185][0m |          -0.0123 |          26.7827 |           8.6897 |
[32m[20221213 23:48:35 @agent_ppo2.py:185][0m |          -0.0147 |          26.5781 |           8.7341 |
[32m[20221213 23:48:35 @agent_ppo2.py:185][0m |          -0.0136 |          25.9851 |           8.7021 |
[32m[20221213 23:48:35 @agent_ppo2.py:185][0m |          -0.0153 |          26.0117 |           8.8204 |
[32m[20221213 23:48:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.20
[32m[20221213 23:48:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.35
[32m[20221213 23:48:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.96
[32m[20221213 23:48:35 @agent_ppo2.py:143][0m Total time:      36.05 min
[32m[20221213 23:48:35 @agent_ppo2.py:145][0m 3493888 total steps have happened
[32m[20221213 23:48:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3706 --------------------------#
[32m[20221213 23:48:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:48:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:35 @agent_ppo2.py:185][0m |          -0.0036 |          53.7762 |           8.4638 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0079 |          49.1358 |           8.4942 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0001 |          55.2657 |           8.5048 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0045 |          47.8787 |           8.5861 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0209 |          46.4095 |           8.5525 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0171 |          46.1261 |           8.6185 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0165 |          45.5605 |           8.6246 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0124 |          45.4389 |           8.6697 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0173 |          45.2126 |           8.7322 |
[32m[20221213 23:48:36 @agent_ppo2.py:185][0m |          -0.0158 |          45.1191 |           8.7591 |
[32m[20221213 23:48:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:48:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.45
[32m[20221213 23:48:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.40
[32m[20221213 23:48:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.32
[32m[20221213 23:48:36 @agent_ppo2.py:143][0m Total time:      36.07 min
[32m[20221213 23:48:36 @agent_ppo2.py:145][0m 3495936 total steps have happened
[32m[20221213 23:48:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3707 --------------------------#
[32m[20221213 23:48:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |           0.0031 |          62.2345 |           8.6617 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |           0.0003 |          57.7967 |           8.6347 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |          -0.0093 |          56.2167 |           8.6304 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |          -0.0085 |          55.3676 |           8.6292 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |          -0.0074 |          54.8726 |           8.7191 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |          -0.0109 |          54.3507 |           8.6582 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |          -0.0072 |          54.3452 |           8.6568 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |          -0.0109 |          53.6083 |           8.6864 |
[32m[20221213 23:48:37 @agent_ppo2.py:185][0m |          -0.0049 |          55.2933 |           8.6415 |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |          -0.0046 |          57.9769 |           8.6705 |
[32m[20221213 23:48:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:48:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.75
[32m[20221213 23:48:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.66
[32m[20221213 23:48:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.79
[32m[20221213 23:48:38 @agent_ppo2.py:143][0m Total time:      36.09 min
[32m[20221213 23:48:38 @agent_ppo2.py:145][0m 3497984 total steps have happened
[32m[20221213 23:48:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3708 --------------------------#
[32m[20221213 23:48:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |           0.0052 |          67.3143 |           9.0392 |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |           0.0000 |          65.0314 |           9.0683 |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |          -0.0069 |          61.9812 |           9.0512 |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |          -0.0097 |          61.1775 |           9.0945 |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |          -0.0078 |          61.2272 |           9.1504 |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |          -0.0099 |          60.5520 |           9.1541 |
[32m[20221213 23:48:38 @agent_ppo2.py:185][0m |          -0.0090 |          60.3543 |           9.1718 |
[32m[20221213 23:48:39 @agent_ppo2.py:185][0m |          -0.0130 |          60.2125 |           9.2060 |
[32m[20221213 23:48:39 @agent_ppo2.py:185][0m |          -0.0163 |          60.0765 |           9.2391 |
[32m[20221213 23:48:39 @agent_ppo2.py:185][0m |          -0.0126 |          59.6550 |           9.2668 |
[32m[20221213 23:48:39 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:48:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.37
[32m[20221213 23:48:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.46
[32m[20221213 23:48:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.11
[32m[20221213 23:48:39 @agent_ppo2.py:143][0m Total time:      36.11 min
[32m[20221213 23:48:39 @agent_ppo2.py:145][0m 3500032 total steps have happened
[32m[20221213 23:48:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3709 --------------------------#
[32m[20221213 23:48:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:39 @agent_ppo2.py:185][0m |          -0.0039 |          59.9316 |           9.2406 |
[32m[20221213 23:48:39 @agent_ppo2.py:185][0m |          -0.0066 |          53.0505 |           9.1951 |
[32m[20221213 23:48:39 @agent_ppo2.py:185][0m |          -0.0107 |          50.9664 |           9.1429 |
[32m[20221213 23:48:39 @agent_ppo2.py:185][0m |          -0.0105 |          50.5090 |           9.1383 |
[32m[20221213 23:48:40 @agent_ppo2.py:185][0m |          -0.0105 |          48.4264 |           9.1489 |
[32m[20221213 23:48:40 @agent_ppo2.py:185][0m |          -0.0133 |          47.8925 |           9.1388 |
[32m[20221213 23:48:40 @agent_ppo2.py:185][0m |          -0.0140 |          47.1625 |           9.0739 |
[32m[20221213 23:48:40 @agent_ppo2.py:185][0m |          -0.0168 |          46.5331 |           9.0447 |
[32m[20221213 23:48:40 @agent_ppo2.py:185][0m |          -0.0162 |          46.1949 |           9.0550 |
[32m[20221213 23:48:40 @agent_ppo2.py:185][0m |          -0.0199 |          46.0475 |           9.0287 |
[32m[20221213 23:48:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.26
[32m[20221213 23:48:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.02
[32m[20221213 23:48:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.64
[32m[20221213 23:48:40 @agent_ppo2.py:143][0m Total time:      36.14 min
[32m[20221213 23:48:40 @agent_ppo2.py:145][0m 3502080 total steps have happened
[32m[20221213 23:48:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3710 --------------------------#
[32m[20221213 23:48:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:48:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:40 @agent_ppo2.py:185][0m |          -0.0039 |          74.8826 |           9.0457 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0059 |          71.2124 |           9.0576 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0065 |          70.4859 |           9.1047 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0102 |          70.1728 |           9.0440 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0070 |          69.7196 |           9.0691 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0124 |          69.7662 |           9.0148 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |           0.0019 |          73.7368 |           9.0629 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0109 |          69.4247 |           9.0117 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0113 |          69.2482 |           9.0458 |
[32m[20221213 23:48:41 @agent_ppo2.py:185][0m |          -0.0133 |          68.9591 |           8.9870 |
[32m[20221213 23:48:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.17
[32m[20221213 23:48:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.87
[32m[20221213 23:48:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.62
[32m[20221213 23:48:41 @agent_ppo2.py:143][0m Total time:      36.16 min
[32m[20221213 23:48:41 @agent_ppo2.py:145][0m 3504128 total steps have happened
[32m[20221213 23:48:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3711 --------------------------#
[32m[20221213 23:48:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |           0.0005 |          78.2161 |           8.4164 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0045 |          75.7123 |           8.5026 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0072 |          74.1213 |           8.4998 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0025 |          74.2152 |           8.5135 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0095 |          72.4592 |           8.5466 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0096 |          71.8026 |           8.6059 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0082 |          71.4558 |           8.6527 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0099 |          71.0782 |           8.7211 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0107 |          70.8928 |           8.6923 |
[32m[20221213 23:48:42 @agent_ppo2.py:185][0m |          -0.0071 |          71.3206 |           8.7599 |
[32m[20221213 23:48:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.87
[32m[20221213 23:48:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.29
[32m[20221213 23:48:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.18
[32m[20221213 23:48:43 @agent_ppo2.py:143][0m Total time:      36.18 min
[32m[20221213 23:48:43 @agent_ppo2.py:145][0m 3506176 total steps have happened
[32m[20221213 23:48:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3712 --------------------------#
[32m[20221213 23:48:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:48:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:43 @agent_ppo2.py:185][0m |           0.0056 |          43.9904 |           9.3205 |
[32m[20221213 23:48:43 @agent_ppo2.py:185][0m |           0.0171 |          37.5014 |           9.3735 |
[32m[20221213 23:48:43 @agent_ppo2.py:185][0m |          -0.0056 |          35.4948 |           9.3784 |
[32m[20221213 23:48:43 @agent_ppo2.py:185][0m |          -0.0043 |          34.3942 |           9.3260 |
[32m[20221213 23:48:43 @agent_ppo2.py:185][0m |          -0.0022 |          34.0740 |           9.3410 |
[32m[20221213 23:48:43 @agent_ppo2.py:185][0m |          -0.0061 |          32.8475 |           9.3430 |
[32m[20221213 23:48:43 @agent_ppo2.py:185][0m |          -0.0057 |          32.5391 |           9.3675 |
[32m[20221213 23:48:44 @agent_ppo2.py:185][0m |          -0.0127 |          32.0592 |           9.3594 |
[32m[20221213 23:48:44 @agent_ppo2.py:185][0m |          -0.0128 |          31.7004 |           9.3725 |
[32m[20221213 23:48:44 @agent_ppo2.py:185][0m |          -0.0133 |          31.2218 |           9.3452 |
[32m[20221213 23:48:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:48:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.74
[32m[20221213 23:48:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.72
[32m[20221213 23:48:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.69
[32m[20221213 23:48:44 @agent_ppo2.py:143][0m Total time:      36.20 min
[32m[20221213 23:48:44 @agent_ppo2.py:145][0m 3508224 total steps have happened
[32m[20221213 23:48:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3713 --------------------------#
[32m[20221213 23:48:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:44 @agent_ppo2.py:185][0m |          -0.0011 |          46.6477 |           9.1779 |
[32m[20221213 23:48:44 @agent_ppo2.py:185][0m |          -0.0057 |          41.5596 |           9.0982 |
[32m[20221213 23:48:44 @agent_ppo2.py:185][0m |          -0.0002 |          40.6600 |           9.1032 |
[32m[20221213 23:48:44 @agent_ppo2.py:185][0m |          -0.0001 |          40.4623 |           9.1585 |
[32m[20221213 23:48:45 @agent_ppo2.py:185][0m |          -0.0097 |          37.3597 |           9.0802 |
[32m[20221213 23:48:45 @agent_ppo2.py:185][0m |          -0.0126 |          36.3985 |           9.0466 |
[32m[20221213 23:48:45 @agent_ppo2.py:185][0m |          -0.0124 |          35.5885 |           9.0283 |
[32m[20221213 23:48:45 @agent_ppo2.py:185][0m |          -0.0110 |          34.8219 |           9.0780 |
[32m[20221213 23:48:45 @agent_ppo2.py:185][0m |          -0.0134 |          34.3028 |           9.0619 |
[32m[20221213 23:48:45 @agent_ppo2.py:185][0m |          -0.0142 |          33.9183 |           9.0843 |
[32m[20221213 23:48:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.49
[32m[20221213 23:48:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.15
[32m[20221213 23:48:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.72
[32m[20221213 23:48:45 @agent_ppo2.py:143][0m Total time:      36.22 min
[32m[20221213 23:48:45 @agent_ppo2.py:145][0m 3510272 total steps have happened
[32m[20221213 23:48:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3714 --------------------------#
[32m[20221213 23:48:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0053 |          67.9196 |           9.3902 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |           0.0009 |          64.0355 |           9.4249 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0097 |          62.4628 |           9.4250 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0109 |          61.4492 |           9.4603 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0105 |          60.9557 |           9.4601 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0129 |          60.4489 |           9.4331 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0118 |          60.1609 |           9.4841 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0132 |          59.6321 |           9.5192 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0151 |          59.5765 |           9.5027 |
[32m[20221213 23:48:46 @agent_ppo2.py:185][0m |          -0.0072 |          64.4330 |           9.5777 |
[32m[20221213 23:48:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.24
[32m[20221213 23:48:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.22
[32m[20221213 23:48:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.54
[32m[20221213 23:48:46 @agent_ppo2.py:143][0m Total time:      36.24 min
[32m[20221213 23:48:46 @agent_ppo2.py:145][0m 3512320 total steps have happened
[32m[20221213 23:48:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3715 --------------------------#
[32m[20221213 23:48:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |           0.0053 |          69.6271 |           9.5305 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |          -0.0090 |          65.4800 |           9.5655 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |          -0.0079 |          63.9912 |           9.5611 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |           0.0099 |          69.7186 |           9.4819 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |          -0.0095 |          62.4201 |           9.5472 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |           0.0075 |          66.8358 |           9.5669 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |          -0.0126 |          61.6893 |           9.5630 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |          -0.0123 |          61.0560 |           9.5236 |
[32m[20221213 23:48:47 @agent_ppo2.py:185][0m |          -0.0105 |          60.7059 |           9.5851 |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |          -0.0114 |          60.5003 |           9.6044 |
[32m[20221213 23:48:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:48:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.10
[32m[20221213 23:48:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.82
[32m[20221213 23:48:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.00
[32m[20221213 23:48:48 @agent_ppo2.py:143][0m Total time:      36.26 min
[32m[20221213 23:48:48 @agent_ppo2.py:145][0m 3514368 total steps have happened
[32m[20221213 23:48:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3716 --------------------------#
[32m[20221213 23:48:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |           0.0086 |          88.6030 |           8.9663 |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |          -0.0053 |          80.0548 |           9.0444 |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |          -0.0031 |          78.6717 |           9.0461 |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |          -0.0084 |          77.9097 |           9.0812 |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |          -0.0116 |          78.4295 |           9.0679 |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |          -0.0020 |          81.2474 |           9.1085 |
[32m[20221213 23:48:48 @agent_ppo2.py:185][0m |          -0.0081 |          74.9395 |           9.0392 |
[32m[20221213 23:48:49 @agent_ppo2.py:185][0m |          -0.0104 |          74.3316 |           9.0536 |
[32m[20221213 23:48:49 @agent_ppo2.py:185][0m |          -0.0106 |          74.2126 |           9.0442 |
[32m[20221213 23:48:49 @agent_ppo2.py:185][0m |          -0.0087 |          76.2053 |           9.0850 |
[32m[20221213 23:48:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.86
[32m[20221213 23:48:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.24
[32m[20221213 23:48:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.81
[32m[20221213 23:48:49 @agent_ppo2.py:143][0m Total time:      36.28 min
[32m[20221213 23:48:49 @agent_ppo2.py:145][0m 3516416 total steps have happened
[32m[20221213 23:48:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3717 --------------------------#
[32m[20221213 23:48:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:49 @agent_ppo2.py:185][0m |           0.0010 |          49.3150 |           9.5555 |
[32m[20221213 23:48:49 @agent_ppo2.py:185][0m |          -0.0083 |          44.6969 |           9.6179 |
[32m[20221213 23:48:49 @agent_ppo2.py:185][0m |          -0.0077 |          42.9510 |           9.5499 |
[32m[20221213 23:48:50 @agent_ppo2.py:185][0m |          -0.0078 |          41.9160 |           9.5527 |
[32m[20221213 23:48:50 @agent_ppo2.py:185][0m |          -0.0132 |          41.1586 |           9.5496 |
[32m[20221213 23:48:50 @agent_ppo2.py:185][0m |          -0.0142 |          40.6076 |           9.5337 |
[32m[20221213 23:48:50 @agent_ppo2.py:185][0m |          -0.0114 |          40.0340 |           9.5134 |
[32m[20221213 23:48:50 @agent_ppo2.py:185][0m |          -0.0159 |          39.4884 |           9.6000 |
[32m[20221213 23:48:50 @agent_ppo2.py:185][0m |          -0.0164 |          38.9523 |           9.5107 |
[32m[20221213 23:48:50 @agent_ppo2.py:185][0m |          -0.0083 |          38.5672 |           9.4909 |
[32m[20221213 23:48:50 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:48:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.81
[32m[20221213 23:48:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.31
[32m[20221213 23:48:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.66
[32m[20221213 23:48:50 @agent_ppo2.py:143][0m Total time:      36.30 min
[32m[20221213 23:48:50 @agent_ppo2.py:145][0m 3518464 total steps have happened
[32m[20221213 23:48:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3718 --------------------------#
[32m[20221213 23:48:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |           0.0093 |          38.5065 |           9.3879 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0056 |          31.9007 |           9.3864 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |           0.0012 |          31.0531 |           9.3541 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0096 |          29.2704 |           9.4038 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0096 |          28.3405 |           9.3920 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0173 |          27.7814 |           9.3873 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0165 |          27.3387 |           9.3717 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0217 |          26.8958 |           9.3759 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0190 |          26.5945 |           9.3823 |
[32m[20221213 23:48:51 @agent_ppo2.py:185][0m |          -0.0165 |          26.3379 |           9.3942 |
[32m[20221213 23:48:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.10
[32m[20221213 23:48:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.75
[32m[20221213 23:48:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.49
[32m[20221213 23:48:51 @agent_ppo2.py:143][0m Total time:      36.32 min
[32m[20221213 23:48:51 @agent_ppo2.py:145][0m 3520512 total steps have happened
[32m[20221213 23:48:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3719 --------------------------#
[32m[20221213 23:48:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:48:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0006 |          64.3904 |           8.7081 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0034 |          60.8383 |           8.7331 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0122 |          59.4706 |           8.7138 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0103 |          58.9458 |           8.6947 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0134 |          58.3076 |           8.7689 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0146 |          57.7965 |           8.7255 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0170 |          57.4567 |           8.7225 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0166 |          57.4429 |           8.7704 |
[32m[20221213 23:48:52 @agent_ppo2.py:185][0m |          -0.0230 |          57.1096 |           8.7824 |
[32m[20221213 23:48:53 @agent_ppo2.py:185][0m |          -0.0153 |          56.5913 |           8.7983 |
[32m[20221213 23:48:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.41
[32m[20221213 23:48:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.03
[32m[20221213 23:48:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.49
[32m[20221213 23:48:53 @agent_ppo2.py:143][0m Total time:      36.34 min
[32m[20221213 23:48:53 @agent_ppo2.py:145][0m 3522560 total steps have happened
[32m[20221213 23:48:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3720 --------------------------#
[32m[20221213 23:48:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:48:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:53 @agent_ppo2.py:185][0m |          -0.0015 |          57.2106 |           9.2913 |
[32m[20221213 23:48:53 @agent_ppo2.py:185][0m |          -0.0073 |          51.4787 |           9.3335 |
[32m[20221213 23:48:53 @agent_ppo2.py:185][0m |          -0.0082 |          50.6053 |           9.3830 |
[32m[20221213 23:48:53 @agent_ppo2.py:185][0m |          -0.0074 |          50.0365 |           9.4133 |
[32m[20221213 23:48:53 @agent_ppo2.py:185][0m |          -0.0090 |          49.8880 |           9.4851 |
[32m[20221213 23:48:53 @agent_ppo2.py:185][0m |          -0.0090 |          49.4247 |           9.4784 |
[32m[20221213 23:48:54 @agent_ppo2.py:185][0m |          -0.0047 |          50.3993 |           9.4680 |
[32m[20221213 23:48:54 @agent_ppo2.py:185][0m |          -0.0121 |          48.9476 |           9.4876 |
[32m[20221213 23:48:54 @agent_ppo2.py:185][0m |          -0.0152 |          48.9966 |           9.4785 |
[32m[20221213 23:48:54 @agent_ppo2.py:185][0m |          -0.0143 |          48.7889 |           9.5144 |
[32m[20221213 23:48:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.79
[32m[20221213 23:48:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.18
[32m[20221213 23:48:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.55
[32m[20221213 23:48:54 @agent_ppo2.py:143][0m Total time:      36.37 min
[32m[20221213 23:48:54 @agent_ppo2.py:145][0m 3524608 total steps have happened
[32m[20221213 23:48:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3721 --------------------------#
[32m[20221213 23:48:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:54 @agent_ppo2.py:185][0m |           0.0017 |          67.3650 |           9.1964 |
[32m[20221213 23:48:54 @agent_ppo2.py:185][0m |          -0.0036 |          57.3434 |           9.1664 |
[32m[20221213 23:48:54 @agent_ppo2.py:185][0m |          -0.0061 |          55.4085 |           9.1471 |
[32m[20221213 23:48:55 @agent_ppo2.py:185][0m |          -0.0104 |          54.0437 |           9.1662 |
[32m[20221213 23:48:55 @agent_ppo2.py:185][0m |          -0.0077 |          53.6737 |           9.1394 |
[32m[20221213 23:48:55 @agent_ppo2.py:185][0m |          -0.0058 |          54.2071 |           9.1757 |
[32m[20221213 23:48:55 @agent_ppo2.py:185][0m |          -0.0072 |          52.3362 |           9.1660 |
[32m[20221213 23:48:55 @agent_ppo2.py:185][0m |          -0.0099 |          52.2860 |           9.1190 |
[32m[20221213 23:48:55 @agent_ppo2.py:185][0m |          -0.0145 |          51.6684 |           9.1259 |
[32m[20221213 23:48:55 @agent_ppo2.py:185][0m |          -0.0163 |          51.0642 |           9.1117 |
[32m[20221213 23:48:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.36
[32m[20221213 23:48:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.11
[32m[20221213 23:48:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.30
[32m[20221213 23:48:55 @agent_ppo2.py:143][0m Total time:      36.39 min
[32m[20221213 23:48:55 @agent_ppo2.py:145][0m 3526656 total steps have happened
[32m[20221213 23:48:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3722 --------------------------#
[32m[20221213 23:48:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |           0.0096 |          76.0799 |           9.8462 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0061 |          68.3345 |           9.9182 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0131 |          65.8263 |           9.8877 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0121 |          64.4052 |           9.9403 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0171 |          63.6629 |           9.9567 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0099 |          64.4765 |           9.9774 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0197 |          62.5610 |           9.9867 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0189 |          61.9535 |          10.0202 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0172 |          61.7248 |          10.0202 |
[32m[20221213 23:48:56 @agent_ppo2.py:185][0m |          -0.0222 |          61.3609 |          10.0474 |
[32m[20221213 23:48:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:48:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.12
[32m[20221213 23:48:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.36
[32m[20221213 23:48:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.66
[32m[20221213 23:48:56 @agent_ppo2.py:143][0m Total time:      36.41 min
[32m[20221213 23:48:56 @agent_ppo2.py:145][0m 3528704 total steps have happened
[32m[20221213 23:48:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3723 --------------------------#
[32m[20221213 23:48:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |           0.0097 |          69.2760 |           9.7975 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0061 |          65.5250 |           9.7461 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0068 |          64.8680 |           9.7654 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0126 |          64.3549 |           9.7408 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0054 |          64.5528 |           9.6708 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0129 |          63.9112 |           9.6926 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0127 |          63.6877 |           9.7178 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0133 |          63.4983 |           9.6609 |
[32m[20221213 23:48:57 @agent_ppo2.py:185][0m |          -0.0135 |          63.2103 |           9.5966 |
[32m[20221213 23:48:58 @agent_ppo2.py:185][0m |          -0.0149 |          62.8486 |           9.6664 |
[32m[20221213 23:48:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.79
[32m[20221213 23:48:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.35
[32m[20221213 23:48:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.49
[32m[20221213 23:48:58 @agent_ppo2.py:143][0m Total time:      36.43 min
[32m[20221213 23:48:58 @agent_ppo2.py:145][0m 3530752 total steps have happened
[32m[20221213 23:48:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3724 --------------------------#
[32m[20221213 23:48:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:48:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:58 @agent_ppo2.py:185][0m |          -0.0021 |          48.2727 |           9.7363 |
[32m[20221213 23:48:58 @agent_ppo2.py:185][0m |          -0.0069 |          43.5467 |           9.7326 |
[32m[20221213 23:48:58 @agent_ppo2.py:185][0m |          -0.0069 |          41.4602 |           9.7330 |
[32m[20221213 23:48:58 @agent_ppo2.py:185][0m |          -0.0115 |          40.4634 |           9.7585 |
[32m[20221213 23:48:58 @agent_ppo2.py:185][0m |          -0.0091 |          39.9915 |           9.6605 |
[32m[20221213 23:48:58 @agent_ppo2.py:185][0m |          -0.0112 |          39.6761 |           9.6545 |
[32m[20221213 23:48:59 @agent_ppo2.py:185][0m |          -0.0096 |          38.6500 |           9.7020 |
[32m[20221213 23:48:59 @agent_ppo2.py:185][0m |          -0.0070 |          41.0503 |           9.6692 |
[32m[20221213 23:48:59 @agent_ppo2.py:185][0m |          -0.0189 |          37.6072 |           9.6395 |
[32m[20221213 23:48:59 @agent_ppo2.py:185][0m |          -0.0227 |          37.5545 |           9.5994 |
[32m[20221213 23:48:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:48:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.33
[32m[20221213 23:48:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.42
[32m[20221213 23:48:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.75
[32m[20221213 23:48:59 @agent_ppo2.py:143][0m Total time:      36.45 min
[32m[20221213 23:48:59 @agent_ppo2.py:145][0m 3532800 total steps have happened
[32m[20221213 23:48:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3725 --------------------------#
[32m[20221213 23:48:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:48:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:48:59 @agent_ppo2.py:185][0m |          -0.0008 |          38.6139 |           8.3145 |
[32m[20221213 23:48:59 @agent_ppo2.py:185][0m |          -0.0066 |          34.1336 |           8.3329 |
[32m[20221213 23:48:59 @agent_ppo2.py:185][0m |          -0.0051 |          32.2534 |           8.5237 |
[32m[20221213 23:49:00 @agent_ppo2.py:185][0m |          -0.0167 |          31.3160 |           8.3738 |
[32m[20221213 23:49:00 @agent_ppo2.py:185][0m |          -0.0089 |          30.4534 |           8.4235 |
[32m[20221213 23:49:00 @agent_ppo2.py:185][0m |          -0.0199 |          30.2488 |           8.3170 |
[32m[20221213 23:49:00 @agent_ppo2.py:185][0m |          -0.0132 |          29.5400 |           8.3166 |
[32m[20221213 23:49:00 @agent_ppo2.py:185][0m |          -0.0090 |          28.9791 |           8.3176 |
[32m[20221213 23:49:00 @agent_ppo2.py:185][0m |          -0.0185 |          28.5696 |           8.3538 |
[32m[20221213 23:49:00 @agent_ppo2.py:185][0m |          -0.0167 |          28.3338 |           8.3303 |
[32m[20221213 23:49:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.42
[32m[20221213 23:49:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.29
[32m[20221213 23:49:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.47
[32m[20221213 23:49:00 @agent_ppo2.py:143][0m Total time:      36.47 min
[32m[20221213 23:49:00 @agent_ppo2.py:145][0m 3534848 total steps have happened
[32m[20221213 23:49:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3726 --------------------------#
[32m[20221213 23:49:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |           0.0030 |          62.3677 |           8.2421 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0022 |          57.4908 |           8.2243 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0133 |          55.7770 |           8.2694 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0104 |          54.3246 |           8.2345 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0102 |          53.7950 |           8.2980 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0148 |          52.9307 |           8.3141 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0091 |          52.7294 |           8.3480 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0090 |          52.4675 |           8.3388 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0155 |          51.8780 |           8.3309 |
[32m[20221213 23:49:01 @agent_ppo2.py:185][0m |          -0.0162 |          51.3209 |           8.3485 |
[32m[20221213 23:49:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:49:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.62
[32m[20221213 23:49:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.19
[32m[20221213 23:49:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.93
[32m[20221213 23:49:01 @agent_ppo2.py:143][0m Total time:      36.49 min
[32m[20221213 23:49:01 @agent_ppo2.py:145][0m 3536896 total steps have happened
[32m[20221213 23:49:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3727 --------------------------#
[32m[20221213 23:49:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |           0.0006 |          78.4503 |           8.7613 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0057 |          74.8400 |           8.7187 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0074 |          73.1821 |           8.7003 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0086 |          72.1400 |           8.7675 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0125 |          71.6098 |           8.7930 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0085 |          70.9054 |           8.7316 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0064 |          70.5840 |           8.7364 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0108 |          70.1918 |           8.8064 |
[32m[20221213 23:49:02 @agent_ppo2.py:185][0m |          -0.0105 |          69.3726 |           8.8078 |
[32m[20221213 23:49:03 @agent_ppo2.py:185][0m |          -0.0135 |          69.3859 |           8.8153 |
[32m[20221213 23:49:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:49:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.79
[32m[20221213 23:49:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.52
[32m[20221213 23:49:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.17
[32m[20221213 23:49:03 @agent_ppo2.py:143][0m Total time:      36.51 min
[32m[20221213 23:49:03 @agent_ppo2.py:145][0m 3538944 total steps have happened
[32m[20221213 23:49:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3728 --------------------------#
[32m[20221213 23:49:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:03 @agent_ppo2.py:185][0m |           0.0001 |          68.7317 |           9.2125 |
[32m[20221213 23:49:03 @agent_ppo2.py:185][0m |          -0.0074 |          64.8761 |           9.2589 |
[32m[20221213 23:49:03 @agent_ppo2.py:185][0m |          -0.0130 |          63.3055 |           9.2315 |
[32m[20221213 23:49:03 @agent_ppo2.py:185][0m |          -0.0128 |          62.0265 |           9.2403 |
[32m[20221213 23:49:03 @agent_ppo2.py:185][0m |          -0.0158 |          61.0913 |           9.2342 |
[32m[20221213 23:49:03 @agent_ppo2.py:185][0m |          -0.0148 |          60.0547 |           9.2561 |
[32m[20221213 23:49:04 @agent_ppo2.py:185][0m |          -0.0172 |          59.2817 |           9.2858 |
[32m[20221213 23:49:04 @agent_ppo2.py:185][0m |          -0.0154 |          58.8183 |           9.2900 |
[32m[20221213 23:49:04 @agent_ppo2.py:185][0m |          -0.0175 |          58.1391 |           9.2912 |
[32m[20221213 23:49:04 @agent_ppo2.py:185][0m |          -0.0173 |          57.5859 |           9.2566 |
[32m[20221213 23:49:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.50
[32m[20221213 23:49:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.08
[32m[20221213 23:49:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.57
[32m[20221213 23:49:04 @agent_ppo2.py:143][0m Total time:      36.53 min
[32m[20221213 23:49:04 @agent_ppo2.py:145][0m 3540992 total steps have happened
[32m[20221213 23:49:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3729 --------------------------#
[32m[20221213 23:49:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:04 @agent_ppo2.py:185][0m |          -0.0000 |          54.0577 |           8.7768 |
[32m[20221213 23:49:04 @agent_ppo2.py:185][0m |          -0.0099 |          48.8501 |           8.8701 |
[32m[20221213 23:49:04 @agent_ppo2.py:185][0m |          -0.0105 |          46.9946 |           8.8138 |
[32m[20221213 23:49:05 @agent_ppo2.py:185][0m |          -0.0092 |          46.2532 |           8.8156 |
[32m[20221213 23:49:05 @agent_ppo2.py:185][0m |          -0.0144 |          45.6015 |           8.8016 |
[32m[20221213 23:49:05 @agent_ppo2.py:185][0m |          -0.0160 |          44.4687 |           8.7600 |
[32m[20221213 23:49:05 @agent_ppo2.py:185][0m |          -0.0163 |          43.8253 |           8.7354 |
[32m[20221213 23:49:05 @agent_ppo2.py:185][0m |          -0.0184 |          43.4792 |           8.7414 |
[32m[20221213 23:49:05 @agent_ppo2.py:185][0m |          -0.0195 |          42.9093 |           8.7624 |
[32m[20221213 23:49:05 @agent_ppo2.py:185][0m |          -0.0160 |          42.6221 |           8.7234 |
[32m[20221213 23:49:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.80
[32m[20221213 23:49:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.09
[32m[20221213 23:49:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.13
[32m[20221213 23:49:05 @agent_ppo2.py:143][0m Total time:      36.55 min
[32m[20221213 23:49:05 @agent_ppo2.py:145][0m 3543040 total steps have happened
[32m[20221213 23:49:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3730 --------------------------#
[32m[20221213 23:49:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:49:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |           0.0014 |          75.3105 |           8.8095 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0006 |          75.7009 |           8.8102 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0107 |          65.6725 |           8.7904 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0137 |          64.9118 |           8.7438 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0140 |          64.0123 |           8.7459 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0149 |          63.7028 |           8.7624 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0159 |          63.2486 |           8.7616 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0155 |          62.8253 |           8.7593 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0138 |          62.6814 |           8.7667 |
[32m[20221213 23:49:06 @agent_ppo2.py:185][0m |          -0.0171 |          62.0817 |           8.7458 |
[32m[20221213 23:49:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:49:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.34
[32m[20221213 23:49:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.28
[32m[20221213 23:49:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.85
[32m[20221213 23:49:06 @agent_ppo2.py:143][0m Total time:      36.57 min
[32m[20221213 23:49:06 @agent_ppo2.py:145][0m 3545088 total steps have happened
[32m[20221213 23:49:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3731 --------------------------#
[32m[20221213 23:49:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |          -0.0002 |          62.5664 |           8.5760 |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |          -0.0001 |          54.0092 |           8.7098 |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |           0.0018 |          57.5993 |           8.7547 |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |          -0.0025 |          51.4099 |           8.6976 |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |          -0.0112 |          50.4835 |           8.7399 |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |          -0.0068 |          49.7783 |           8.7824 |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |          -0.0101 |          49.3706 |           8.7170 |
[32m[20221213 23:49:07 @agent_ppo2.py:185][0m |          -0.0165 |          49.1889 |           8.7610 |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0032 |          49.7349 |           8.7408 |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0115 |          48.4802 |           8.7627 |
[32m[20221213 23:49:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:49:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.20
[32m[20221213 23:49:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.40
[32m[20221213 23:49:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.97
[32m[20221213 23:49:08 @agent_ppo2.py:143][0m Total time:      36.60 min
[32m[20221213 23:49:08 @agent_ppo2.py:145][0m 3547136 total steps have happened
[32m[20221213 23:49:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3732 --------------------------#
[32m[20221213 23:49:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:49:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0046 |          39.4900 |           8.7384 |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0082 |          34.1710 |           8.7165 |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0080 |          32.5282 |           8.6802 |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0056 |          31.4998 |           8.7133 |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0109 |          30.8056 |           8.7462 |
[32m[20221213 23:49:08 @agent_ppo2.py:185][0m |          -0.0139 |          30.2304 |           8.7637 |
[32m[20221213 23:49:09 @agent_ppo2.py:185][0m |          -0.0151 |          29.6686 |           8.7466 |
[32m[20221213 23:49:09 @agent_ppo2.py:185][0m |          -0.0121 |          29.2651 |           8.7078 |
[32m[20221213 23:49:09 @agent_ppo2.py:185][0m |          -0.0126 |          28.9511 |           8.7368 |
[32m[20221213 23:49:09 @agent_ppo2.py:185][0m |          -0.0167 |          28.6005 |           8.7556 |
[32m[20221213 23:49:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.68
[32m[20221213 23:49:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.09
[32m[20221213 23:49:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.62
[32m[20221213 23:49:09 @agent_ppo2.py:143][0m Total time:      36.62 min
[32m[20221213 23:49:09 @agent_ppo2.py:145][0m 3549184 total steps have happened
[32m[20221213 23:49:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3733 --------------------------#
[32m[20221213 23:49:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:09 @agent_ppo2.py:185][0m |           0.0122 |          50.1909 |           7.9691 |
[32m[20221213 23:49:09 @agent_ppo2.py:185][0m |          -0.0077 |          40.1343 |           8.0166 |
[32m[20221213 23:49:09 @agent_ppo2.py:185][0m |          -0.0075 |          37.1805 |           8.0583 |
[32m[20221213 23:49:10 @agent_ppo2.py:185][0m |          -0.0117 |          35.7249 |           8.0720 |
[32m[20221213 23:49:10 @agent_ppo2.py:185][0m |          -0.0117 |          34.5779 |           7.9955 |
[32m[20221213 23:49:10 @agent_ppo2.py:185][0m |          -0.0164 |          33.7924 |           8.0440 |
[32m[20221213 23:49:10 @agent_ppo2.py:185][0m |          -0.0166 |          33.1878 |           7.9898 |
[32m[20221213 23:49:10 @agent_ppo2.py:185][0m |          -0.0189 |          32.6974 |           7.9973 |
[32m[20221213 23:49:10 @agent_ppo2.py:185][0m |          -0.0179 |          32.2248 |           7.9629 |
[32m[20221213 23:49:10 @agent_ppo2.py:185][0m |          -0.0174 |          31.8514 |           7.9715 |
[32m[20221213 23:49:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.64
[32m[20221213 23:49:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.37
[32m[20221213 23:49:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.16
[32m[20221213 23:49:10 @agent_ppo2.py:143][0m Total time:      36.64 min
[32m[20221213 23:49:10 @agent_ppo2.py:145][0m 3551232 total steps have happened
[32m[20221213 23:49:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3734 --------------------------#
[32m[20221213 23:49:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:49:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |           0.0035 |          40.5574 |           9.0716 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0034 |          33.3343 |           9.1032 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0055 |          31.0833 |           9.1467 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0047 |          29.8718 |           9.1271 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0055 |          29.3112 |           9.1326 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0130 |          28.7622 |           9.1483 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0115 |          28.3740 |           9.1367 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0098 |          28.1079 |           9.1204 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0143 |          27.5930 |           9.1282 |
[32m[20221213 23:49:11 @agent_ppo2.py:185][0m |          -0.0219 |          27.4305 |           9.0996 |
[32m[20221213 23:49:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.80
[32m[20221213 23:49:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.73
[32m[20221213 23:49:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.13
[32m[20221213 23:49:11 @agent_ppo2.py:143][0m Total time:      36.66 min
[32m[20221213 23:49:11 @agent_ppo2.py:145][0m 3553280 total steps have happened
[32m[20221213 23:49:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3735 --------------------------#
[32m[20221213 23:49:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0009 |          61.7361 |           8.6357 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0023 |          59.4742 |           8.6421 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0050 |          59.2858 |           8.6040 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |           0.0044 |          61.1115 |           8.6698 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0069 |          58.7844 |           8.5846 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0114 |          58.4054 |           8.6184 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0084 |          58.1875 |           8.6172 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0055 |          58.0018 |           8.6079 |
[32m[20221213 23:49:12 @agent_ppo2.py:185][0m |          -0.0115 |          57.8628 |           8.5837 |
[32m[20221213 23:49:13 @agent_ppo2.py:185][0m |          -0.0140 |          57.7375 |           8.6039 |
[32m[20221213 23:49:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.21
[32m[20221213 23:49:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.33
[32m[20221213 23:49:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.74
[32m[20221213 23:49:13 @agent_ppo2.py:143][0m Total time:      36.68 min
[32m[20221213 23:49:13 @agent_ppo2.py:145][0m 3555328 total steps have happened
[32m[20221213 23:49:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3736 --------------------------#
[32m[20221213 23:49:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:13 @agent_ppo2.py:185][0m |           0.0031 |          50.1950 |           8.6537 |
[32m[20221213 23:49:13 @agent_ppo2.py:185][0m |          -0.0046 |          46.8840 |           8.7504 |
[32m[20221213 23:49:13 @agent_ppo2.py:185][0m |          -0.0087 |          45.8006 |           8.6349 |
[32m[20221213 23:49:13 @agent_ppo2.py:185][0m |          -0.0042 |          45.5026 |           8.6680 |
[32m[20221213 23:49:13 @agent_ppo2.py:185][0m |          -0.0123 |          44.6752 |           8.6065 |
[32m[20221213 23:49:13 @agent_ppo2.py:185][0m |          -0.0129 |          44.3814 |           8.6474 |
[32m[20221213 23:49:14 @agent_ppo2.py:185][0m |          -0.0121 |          44.3543 |           8.6916 |
[32m[20221213 23:49:14 @agent_ppo2.py:185][0m |          -0.0160 |          43.7660 |           8.6530 |
[32m[20221213 23:49:14 @agent_ppo2.py:185][0m |          -0.0167 |          43.3747 |           8.6546 |
[32m[20221213 23:49:14 @agent_ppo2.py:185][0m |          -0.0212 |          43.0296 |           8.6727 |
[32m[20221213 23:49:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:49:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.47
[32m[20221213 23:49:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.82
[32m[20221213 23:49:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.51
[32m[20221213 23:49:14 @agent_ppo2.py:143][0m Total time:      36.70 min
[32m[20221213 23:49:14 @agent_ppo2.py:145][0m 3557376 total steps have happened
[32m[20221213 23:49:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3737 --------------------------#
[32m[20221213 23:49:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:14 @agent_ppo2.py:185][0m |          -0.0016 |          66.6101 |           7.9834 |
[32m[20221213 23:49:14 @agent_ppo2.py:185][0m |          -0.0044 |          62.8777 |           8.0335 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0067 |          62.3442 |           8.0794 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0087 |          61.7247 |           8.0586 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0102 |          61.1801 |           8.1176 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0132 |          60.8133 |           8.0982 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0102 |          60.6206 |           8.1705 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0135 |          60.2826 |           8.2130 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0152 |          59.9634 |           8.2220 |
[32m[20221213 23:49:15 @agent_ppo2.py:185][0m |          -0.0146 |          59.8128 |           8.2580 |
[32m[20221213 23:49:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.67
[32m[20221213 23:49:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.32
[32m[20221213 23:49:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.90
[32m[20221213 23:49:15 @agent_ppo2.py:143][0m Total time:      36.72 min
[32m[20221213 23:49:15 @agent_ppo2.py:145][0m 3559424 total steps have happened
[32m[20221213 23:49:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3738 --------------------------#
[32m[20221213 23:49:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0019 |          53.4522 |           8.6140 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0062 |          50.1505 |           8.6077 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0118 |          48.7164 |           8.5944 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0152 |          47.8792 |           8.5942 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0122 |          47.0642 |           8.6320 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0152 |          46.7644 |           8.6558 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0148 |          46.1291 |           8.6003 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0171 |          45.8318 |           8.6056 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0216 |          45.7224 |           8.5884 |
[32m[20221213 23:49:16 @agent_ppo2.py:185][0m |          -0.0172 |          45.3913 |           8.6148 |
[32m[20221213 23:49:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.98
[32m[20221213 23:49:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.23
[32m[20221213 23:49:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.39
[32m[20221213 23:49:17 @agent_ppo2.py:143][0m Total time:      36.74 min
[32m[20221213 23:49:17 @agent_ppo2.py:145][0m 3561472 total steps have happened
[32m[20221213 23:49:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3739 --------------------------#
[32m[20221213 23:49:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |           0.0018 |          58.9504 |           8.8718 |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |          -0.0051 |          56.4923 |           8.9271 |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |          -0.0068 |          56.2510 |           8.9531 |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |          -0.0155 |          54.3871 |           8.9272 |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |          -0.0145 |          53.7517 |           8.9844 |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |          -0.0115 |          53.4275 |           8.9737 |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |          -0.0193 |          52.9353 |           8.9865 |
[32m[20221213 23:49:17 @agent_ppo2.py:185][0m |          -0.0146 |          52.5957 |           8.9910 |
[32m[20221213 23:49:18 @agent_ppo2.py:185][0m |          -0.0203 |          52.4605 |           8.9918 |
[32m[20221213 23:49:18 @agent_ppo2.py:185][0m |          -0.0193 |          52.0420 |           9.0231 |
[32m[20221213 23:49:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.47
[32m[20221213 23:49:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.02
[32m[20221213 23:49:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.60
[32m[20221213 23:49:18 @agent_ppo2.py:143][0m Total time:      36.76 min
[32m[20221213 23:49:18 @agent_ppo2.py:145][0m 3563520 total steps have happened
[32m[20221213 23:49:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3740 --------------------------#
[32m[20221213 23:49:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:49:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:18 @agent_ppo2.py:185][0m |           0.0129 |          70.9174 |           9.1968 |
[32m[20221213 23:49:18 @agent_ppo2.py:185][0m |           0.0020 |          64.1464 |           9.0731 |
[32m[20221213 23:49:18 @agent_ppo2.py:185][0m |           0.0063 |          67.7552 |           9.0565 |
[32m[20221213 23:49:18 @agent_ppo2.py:185][0m |          -0.0033 |          62.1399 |           9.0957 |
[32m[20221213 23:49:18 @agent_ppo2.py:185][0m |          -0.0048 |          61.7103 |           9.0689 |
[32m[20221213 23:49:19 @agent_ppo2.py:185][0m |          -0.0003 |          62.4482 |           9.0903 |
[32m[20221213 23:49:19 @agent_ppo2.py:185][0m |          -0.0108 |          60.6675 |           9.0586 |
[32m[20221213 23:49:19 @agent_ppo2.py:185][0m |          -0.0000 |          62.9220 |           9.0145 |
[32m[20221213 23:49:19 @agent_ppo2.py:185][0m |          -0.0126 |          60.2267 |           9.0284 |
[32m[20221213 23:49:19 @agent_ppo2.py:185][0m |          -0.0116 |          60.1277 |           8.9873 |
[32m[20221213 23:49:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.36
[32m[20221213 23:49:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.01
[32m[20221213 23:49:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.26
[32m[20221213 23:49:19 @agent_ppo2.py:143][0m Total time:      36.78 min
[32m[20221213 23:49:19 @agent_ppo2.py:145][0m 3565568 total steps have happened
[32m[20221213 23:49:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3741 --------------------------#
[32m[20221213 23:49:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:49:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:19 @agent_ppo2.py:185][0m |           0.0038 |          48.8501 |           8.6427 |
[32m[20221213 23:49:19 @agent_ppo2.py:185][0m |          -0.0029 |          42.9420 |           8.6515 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0072 |          41.4406 |           8.6735 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0101 |          39.7444 |           8.6785 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0038 |          39.4175 |           8.6480 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0088 |          38.1212 |           8.6614 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0030 |          39.7549 |           8.6823 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0093 |          37.4122 |           8.6806 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0132 |          36.0870 |           8.6448 |
[32m[20221213 23:49:20 @agent_ppo2.py:185][0m |          -0.0193 |          35.9114 |           8.6649 |
[32m[20221213 23:49:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:49:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.13
[32m[20221213 23:49:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.73
[32m[20221213 23:49:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.11
[32m[20221213 23:49:20 @agent_ppo2.py:143][0m Total time:      36.80 min
[32m[20221213 23:49:20 @agent_ppo2.py:145][0m 3567616 total steps have happened
[32m[20221213 23:49:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3742 --------------------------#
[32m[20221213 23:49:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0014 |          59.3474 |           8.5799 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0054 |          56.1216 |           8.6705 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0058 |          55.2907 |           8.6872 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0084 |          54.8695 |           8.6807 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0086 |          54.6368 |           8.7550 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0095 |          54.0493 |           8.7944 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0017 |          54.4060 |           8.7438 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0107 |          53.4882 |           8.8012 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0072 |          53.4713 |           8.7746 |
[32m[20221213 23:49:21 @agent_ppo2.py:185][0m |          -0.0125 |          53.1186 |           8.8074 |
[32m[20221213 23:49:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.45
[32m[20221213 23:49:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.81
[32m[20221213 23:49:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.64
[32m[20221213 23:49:22 @agent_ppo2.py:143][0m Total time:      36.83 min
[32m[20221213 23:49:22 @agent_ppo2.py:145][0m 3569664 total steps have happened
[32m[20221213 23:49:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3743 --------------------------#
[32m[20221213 23:49:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |           0.0001 |          59.5889 |           9.6877 |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |          -0.0070 |          55.6957 |           9.7028 |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |          -0.0048 |          54.2995 |           9.6265 |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |          -0.0077 |          53.3175 |           9.7000 |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |          -0.0088 |          52.4330 |           9.7101 |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |          -0.0118 |          51.9609 |           9.7317 |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |          -0.0146 |          51.6508 |           9.7606 |
[32m[20221213 23:49:22 @agent_ppo2.py:185][0m |          -0.0160 |          51.3672 |           9.7546 |
[32m[20221213 23:49:23 @agent_ppo2.py:185][0m |          -0.0140 |          50.8681 |           9.7508 |
[32m[20221213 23:49:23 @agent_ppo2.py:185][0m |          -0.0010 |          58.2386 |           9.7883 |
[32m[20221213 23:49:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.63
[32m[20221213 23:49:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.44
[32m[20221213 23:49:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.16
[32m[20221213 23:49:23 @agent_ppo2.py:143][0m Total time:      36.85 min
[32m[20221213 23:49:23 @agent_ppo2.py:145][0m 3571712 total steps have happened
[32m[20221213 23:49:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3744 --------------------------#
[32m[20221213 23:49:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:49:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:23 @agent_ppo2.py:185][0m |           0.0006 |          44.3329 |           9.0599 |
[32m[20221213 23:49:23 @agent_ppo2.py:185][0m |          -0.0080 |          37.0496 |           9.0144 |
[32m[20221213 23:49:23 @agent_ppo2.py:185][0m |          -0.0137 |          35.3459 |           9.0188 |
[32m[20221213 23:49:23 @agent_ppo2.py:185][0m |          -0.0098 |          34.2156 |           9.0424 |
[32m[20221213 23:49:23 @agent_ppo2.py:185][0m |          -0.0105 |          33.5347 |           9.0193 |
[32m[20221213 23:49:24 @agent_ppo2.py:185][0m |          -0.0147 |          32.7783 |           9.0061 |
[32m[20221213 23:49:24 @agent_ppo2.py:185][0m |          -0.0122 |          32.4209 |           8.9892 |
[32m[20221213 23:49:24 @agent_ppo2.py:185][0m |          -0.0148 |          31.9754 |           8.9892 |
[32m[20221213 23:49:24 @agent_ppo2.py:185][0m |          -0.0130 |          31.4330 |           8.9544 |
[32m[20221213 23:49:24 @agent_ppo2.py:185][0m |          -0.0168 |          31.2757 |           8.9823 |
[32m[20221213 23:49:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.14
[32m[20221213 23:49:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.11
[32m[20221213 23:49:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.38
[32m[20221213 23:49:24 @agent_ppo2.py:143][0m Total time:      36.87 min
[32m[20221213 23:49:24 @agent_ppo2.py:145][0m 3573760 total steps have happened
[32m[20221213 23:49:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3745 --------------------------#
[32m[20221213 23:49:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:24 @agent_ppo2.py:185][0m |           0.0046 |          39.3560 |           8.3656 |
[32m[20221213 23:49:24 @agent_ppo2.py:185][0m |          -0.0060 |          33.8838 |           8.3951 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0103 |          32.1890 |           8.4059 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0134 |          30.8747 |           8.3973 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0093 |          30.5619 |           8.3737 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0143 |          30.0077 |           8.4534 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0131 |          29.1454 |           8.4035 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0159 |          28.6088 |           8.4182 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0251 |          28.4153 |           8.3955 |
[32m[20221213 23:49:25 @agent_ppo2.py:185][0m |          -0.0212 |          27.9057 |           8.4139 |
[32m[20221213 23:49:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:49:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.12
[32m[20221213 23:49:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.07
[32m[20221213 23:49:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.96
[32m[20221213 23:49:25 @agent_ppo2.py:143][0m Total time:      36.89 min
[32m[20221213 23:49:25 @agent_ppo2.py:145][0m 3575808 total steps have happened
[32m[20221213 23:49:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3746 --------------------------#
[32m[20221213 23:49:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |           0.0026 |          62.5412 |           8.8524 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |           0.0071 |          62.7323 |           8.8943 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |          -0.0059 |          56.6034 |           8.8365 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |          -0.0093 |          55.6077 |           8.8789 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |          -0.0128 |          55.1595 |           8.8533 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |          -0.0142 |          54.8328 |           8.8498 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |          -0.0101 |          54.5412 |           8.7927 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |           0.0005 |          57.2693 |           8.8031 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |          -0.0124 |          54.1394 |           8.8261 |
[32m[20221213 23:49:26 @agent_ppo2.py:185][0m |          -0.0146 |          53.7281 |           8.7791 |
[32m[20221213 23:49:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:49:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.52
[32m[20221213 23:49:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.71
[32m[20221213 23:49:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.49
[32m[20221213 23:49:27 @agent_ppo2.py:143][0m Total time:      36.91 min
[32m[20221213 23:49:27 @agent_ppo2.py:145][0m 3577856 total steps have happened
[32m[20221213 23:49:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3747 --------------------------#
[32m[20221213 23:49:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |           0.0012 |          56.9530 |           8.3439 |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |          -0.0036 |          53.0562 |           8.4458 |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |          -0.0071 |          51.7113 |           8.4480 |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |          -0.0086 |          50.6237 |           8.4555 |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |          -0.0064 |          49.8192 |           8.4539 |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |          -0.0055 |          51.0092 |           8.4660 |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |          -0.0069 |          49.0687 |           8.4433 |
[32m[20221213 23:49:27 @agent_ppo2.py:185][0m |          -0.0132 |          48.5155 |           8.4747 |
[32m[20221213 23:49:28 @agent_ppo2.py:185][0m |          -0.0150 |          48.3729 |           8.4502 |
[32m[20221213 23:49:28 @agent_ppo2.py:185][0m |          -0.0146 |          48.0279 |           8.4941 |
[32m[20221213 23:49:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.70
[32m[20221213 23:49:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.98
[32m[20221213 23:49:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.58
[32m[20221213 23:49:28 @agent_ppo2.py:143][0m Total time:      36.93 min
[32m[20221213 23:49:28 @agent_ppo2.py:145][0m 3579904 total steps have happened
[32m[20221213 23:49:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3748 --------------------------#
[32m[20221213 23:49:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:28 @agent_ppo2.py:185][0m |           0.0009 |          61.0041 |           8.8595 |
[32m[20221213 23:49:28 @agent_ppo2.py:185][0m |          -0.0059 |          56.8642 |           8.8340 |
[32m[20221213 23:49:28 @agent_ppo2.py:185][0m |          -0.0079 |          55.3282 |           8.9255 |
[32m[20221213 23:49:28 @agent_ppo2.py:185][0m |          -0.0084 |          54.7411 |           8.8920 |
[32m[20221213 23:49:28 @agent_ppo2.py:185][0m |           0.0004 |          56.6934 |           8.8763 |
[32m[20221213 23:49:29 @agent_ppo2.py:185][0m |          -0.0046 |          54.5526 |           8.9354 |
[32m[20221213 23:49:29 @agent_ppo2.py:185][0m |          -0.0112 |          53.7674 |           8.9827 |
[32m[20221213 23:49:29 @agent_ppo2.py:185][0m |          -0.0061 |          54.5990 |           8.9628 |
[32m[20221213 23:49:29 @agent_ppo2.py:185][0m |          -0.0129 |          53.2241 |           8.9692 |
[32m[20221213 23:49:29 @agent_ppo2.py:185][0m |          -0.0130 |          52.9985 |           8.9932 |
[32m[20221213 23:49:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.81
[32m[20221213 23:49:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.14
[32m[20221213 23:49:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.17
[32m[20221213 23:49:29 @agent_ppo2.py:143][0m Total time:      36.95 min
[32m[20221213 23:49:29 @agent_ppo2.py:145][0m 3581952 total steps have happened
[32m[20221213 23:49:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3749 --------------------------#
[32m[20221213 23:49:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:29 @agent_ppo2.py:185][0m |           0.0032 |          44.5453 |           8.7584 |
[32m[20221213 23:49:29 @agent_ppo2.py:185][0m |          -0.0009 |          39.6854 |           8.7828 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |          -0.0070 |          38.1296 |           8.7971 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |          -0.0126 |          36.7477 |           8.7962 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |           0.0080 |          45.0636 |           8.8283 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |          -0.0142 |          35.8149 |           8.8358 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |          -0.0177 |          34.7890 |           8.8771 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |          -0.0139 |          34.3810 |           8.8835 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |          -0.0140 |          34.0284 |           8.9022 |
[32m[20221213 23:49:30 @agent_ppo2.py:185][0m |          -0.0156 |          33.7829 |           8.8912 |
[32m[20221213 23:49:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:49:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.42
[32m[20221213 23:49:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.36
[32m[20221213 23:49:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.82
[32m[20221213 23:49:30 @agent_ppo2.py:143][0m Total time:      36.97 min
[32m[20221213 23:49:30 @agent_ppo2.py:145][0m 3584000 total steps have happened
[32m[20221213 23:49:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3750 --------------------------#
[32m[20221213 23:49:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:49:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0022 |          69.8827 |           9.2479 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0014 |          68.9663 |           9.3720 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0120 |          65.3362 |           9.2972 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0025 |          72.4977 |           9.3015 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0118 |          64.3384 |           9.3468 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0121 |          63.7463 |           9.3397 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0051 |          71.4480 |           9.3608 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0145 |          63.2397 |           9.3487 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0125 |          63.1838 |           9.3591 |
[32m[20221213 23:49:31 @agent_ppo2.py:185][0m |          -0.0121 |          63.2120 |           9.3696 |
[32m[20221213 23:49:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:49:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.47
[32m[20221213 23:49:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.94
[32m[20221213 23:49:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.81
[32m[20221213 23:49:32 @agent_ppo2.py:143][0m Total time:      36.99 min
[32m[20221213 23:49:32 @agent_ppo2.py:145][0m 3586048 total steps have happened
[32m[20221213 23:49:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3751 --------------------------#
[32m[20221213 23:49:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:32 @agent_ppo2.py:185][0m |           0.0013 |          66.9581 |           8.9807 |
[32m[20221213 23:49:32 @agent_ppo2.py:185][0m |          -0.0075 |          62.5337 |           8.9977 |
[32m[20221213 23:49:32 @agent_ppo2.py:185][0m |          -0.0111 |          60.7349 |           8.9925 |
[32m[20221213 23:49:32 @agent_ppo2.py:185][0m |          -0.0072 |          59.5514 |           9.0849 |
[32m[20221213 23:49:32 @agent_ppo2.py:185][0m |          -0.0089 |          58.8275 |           9.0878 |
[32m[20221213 23:49:32 @agent_ppo2.py:185][0m |          -0.0094 |          58.2090 |           9.1051 |
[32m[20221213 23:49:32 @agent_ppo2.py:185][0m |          -0.0146 |          57.8020 |           9.1426 |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |          -0.0110 |          57.1713 |           9.1423 |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |          -0.0134 |          57.0123 |           9.2051 |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |          -0.0130 |          56.4361 |           9.1657 |
[32m[20221213 23:49:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.12
[32m[20221213 23:49:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.41
[32m[20221213 23:49:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.91
[32m[20221213 23:49:33 @agent_ppo2.py:143][0m Total time:      37.01 min
[32m[20221213 23:49:33 @agent_ppo2.py:145][0m 3588096 total steps have happened
[32m[20221213 23:49:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3752 --------------------------#
[32m[20221213 23:49:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |           0.0026 |          58.2577 |           9.6104 |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |          -0.0080 |          52.8770 |           9.6505 |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |          -0.0065 |          50.6815 |           9.5860 |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |          -0.0085 |          49.2160 |           9.6069 |
[32m[20221213 23:49:33 @agent_ppo2.py:185][0m |          -0.0086 |          49.1030 |           9.6500 |
[32m[20221213 23:49:34 @agent_ppo2.py:185][0m |          -0.0064 |          49.6315 |           9.6133 |
[32m[20221213 23:49:34 @agent_ppo2.py:185][0m |          -0.0133 |          47.2246 |           9.6672 |
[32m[20221213 23:49:34 @agent_ppo2.py:185][0m |          -0.0070 |          50.1949 |           9.6387 |
[32m[20221213 23:49:34 @agent_ppo2.py:185][0m |          -0.0211 |          46.3587 |           9.6310 |
[32m[20221213 23:49:34 @agent_ppo2.py:185][0m |          -0.0173 |          45.6134 |           9.6306 |
[32m[20221213 23:49:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.07
[32m[20221213 23:49:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.73
[32m[20221213 23:49:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.18
[32m[20221213 23:49:34 @agent_ppo2.py:143][0m Total time:      37.03 min
[32m[20221213 23:49:34 @agent_ppo2.py:145][0m 3590144 total steps have happened
[32m[20221213 23:49:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3753 --------------------------#
[32m[20221213 23:49:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:49:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:34 @agent_ppo2.py:185][0m |           0.0124 |          74.2173 |           9.8887 |
[32m[20221213 23:49:34 @agent_ppo2.py:185][0m |          -0.0040 |          64.1775 |           9.8559 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0107 |          62.8150 |           9.8854 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0107 |          62.0769 |           9.9239 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0149 |          60.9429 |           9.8944 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0144 |          60.3809 |           9.9178 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0139 |          59.9538 |           9.8597 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0178 |          59.6856 |           9.9037 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0177 |          59.3805 |           9.9398 |
[32m[20221213 23:49:35 @agent_ppo2.py:185][0m |          -0.0184 |          59.2664 |           9.9343 |
[32m[20221213 23:49:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.45
[32m[20221213 23:49:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.55
[32m[20221213 23:49:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.02
[32m[20221213 23:49:35 @agent_ppo2.py:143][0m Total time:      37.06 min
[32m[20221213 23:49:35 @agent_ppo2.py:145][0m 3592192 total steps have happened
[32m[20221213 23:49:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3754 --------------------------#
[32m[20221213 23:49:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0006 |          67.2130 |           9.5001 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0080 |          61.7298 |           9.4141 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0131 |          59.5974 |           9.4319 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0106 |          58.2255 |           9.4608 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0105 |          57.2773 |           9.4737 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0137 |          56.5871 |           9.4650 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0149 |          56.3060 |           9.4721 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0141 |          55.5357 |           9.4531 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0147 |          54.8993 |           9.4699 |
[32m[20221213 23:49:36 @agent_ppo2.py:185][0m |          -0.0172 |          54.4399 |           9.3933 |
[32m[20221213 23:49:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.94
[32m[20221213 23:49:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.70
[32m[20221213 23:49:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.21
[32m[20221213 23:49:37 @agent_ppo2.py:143][0m Total time:      37.08 min
[32m[20221213 23:49:37 @agent_ppo2.py:145][0m 3594240 total steps have happened
[32m[20221213 23:49:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3755 --------------------------#
[32m[20221213 23:49:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:37 @agent_ppo2.py:185][0m |           0.0007 |          58.9505 |           9.6056 |
[32m[20221213 23:49:37 @agent_ppo2.py:185][0m |          -0.0044 |          56.8482 |           9.5667 |
[32m[20221213 23:49:37 @agent_ppo2.py:185][0m |          -0.0086 |          56.4128 |           9.5628 |
[32m[20221213 23:49:37 @agent_ppo2.py:185][0m |          -0.0102 |          56.1160 |           9.5936 |
[32m[20221213 23:49:37 @agent_ppo2.py:185][0m |          -0.0085 |          55.9050 |           9.5571 |
[32m[20221213 23:49:37 @agent_ppo2.py:185][0m |          -0.0101 |          55.6765 |           9.5297 |
[32m[20221213 23:49:37 @agent_ppo2.py:185][0m |          -0.0018 |          57.1490 |           9.5549 |
[32m[20221213 23:49:38 @agent_ppo2.py:185][0m |          -0.0127 |          55.6810 |           9.5335 |
[32m[20221213 23:49:38 @agent_ppo2.py:185][0m |          -0.0066 |          56.4644 |           9.5758 |
[32m[20221213 23:49:38 @agent_ppo2.py:185][0m |          -0.0049 |          56.1133 |           9.5820 |
[32m[20221213 23:49:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:49:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.50
[32m[20221213 23:49:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.72
[32m[20221213 23:49:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.77
[32m[20221213 23:49:38 @agent_ppo2.py:143][0m Total time:      37.10 min
[32m[20221213 23:49:38 @agent_ppo2.py:145][0m 3596288 total steps have happened
[32m[20221213 23:49:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3756 --------------------------#
[32m[20221213 23:49:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:49:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:38 @agent_ppo2.py:185][0m |          -0.0003 |          58.9001 |           9.3566 |
[32m[20221213 23:49:38 @agent_ppo2.py:185][0m |           0.0027 |          58.4675 |           9.2712 |
[32m[20221213 23:49:38 @agent_ppo2.py:185][0m |          -0.0078 |          53.4148 |           9.3442 |
[32m[20221213 23:49:38 @agent_ppo2.py:185][0m |          -0.0066 |          52.8297 |           9.3723 |
[32m[20221213 23:49:39 @agent_ppo2.py:185][0m |          -0.0066 |          52.4167 |           9.3303 |
[32m[20221213 23:49:39 @agent_ppo2.py:185][0m |          -0.0103 |          52.1679 |           9.4061 |
[32m[20221213 23:49:39 @agent_ppo2.py:185][0m |          -0.0102 |          51.9943 |           9.3815 |
[32m[20221213 23:49:39 @agent_ppo2.py:185][0m |          -0.0108 |          51.6497 |           9.3583 |
[32m[20221213 23:49:39 @agent_ppo2.py:185][0m |          -0.0116 |          51.4462 |           9.3860 |
[32m[20221213 23:49:39 @agent_ppo2.py:185][0m |          -0.0108 |          51.3759 |           9.3599 |
[32m[20221213 23:49:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.22
[32m[20221213 23:49:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.20
[32m[20221213 23:49:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.72
[32m[20221213 23:49:39 @agent_ppo2.py:143][0m Total time:      37.12 min
[32m[20221213 23:49:39 @agent_ppo2.py:145][0m 3598336 total steps have happened
[32m[20221213 23:49:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3757 --------------------------#
[32m[20221213 23:49:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:49:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:39 @agent_ppo2.py:185][0m |           0.0002 |          16.6500 |           9.9047 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |          -0.0043 |          13.6493 |           9.9124 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |           0.0019 |          14.5046 |           9.9002 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |          -0.0023 |          13.6499 |           9.9582 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |          -0.0021 |          13.5328 |           9.9170 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |          -0.0059 |          13.2968 |           9.9593 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |          -0.0068 |          13.2856 |           9.9634 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |           0.0035 |          13.7680 |           9.9700 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |          -0.0066 |          13.2741 |           9.9629 |
[32m[20221213 23:49:40 @agent_ppo2.py:185][0m |          -0.0077 |          13.1945 |           9.8899 |
[32m[20221213 23:49:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:49:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:49:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:49:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.07
[32m[20221213 23:49:40 @agent_ppo2.py:143][0m Total time:      37.14 min
[32m[20221213 23:49:40 @agent_ppo2.py:145][0m 3600384 total steps have happened
[32m[20221213 23:49:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3758 --------------------------#
[32m[20221213 23:49:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |           0.0052 |          69.6287 |           9.2283 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |           0.0060 |          71.2583 |           9.2908 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0052 |          65.7937 |           9.3007 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0120 |          65.2830 |           9.2746 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0077 |          64.7011 |           9.3314 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0020 |          68.7070 |           9.3024 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0113 |          64.1018 |           9.3212 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0113 |          63.8957 |           9.3470 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0149 |          63.5700 |           9.3973 |
[32m[20221213 23:49:41 @agent_ppo2.py:185][0m |          -0.0051 |          66.6445 |           9.3180 |
[32m[20221213 23:49:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.12
[32m[20221213 23:49:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.64
[32m[20221213 23:49:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 190.26
[32m[20221213 23:49:42 @agent_ppo2.py:143][0m Total time:      37.16 min
[32m[20221213 23:49:42 @agent_ppo2.py:145][0m 3602432 total steps have happened
[32m[20221213 23:49:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3759 --------------------------#
[32m[20221213 23:49:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:42 @agent_ppo2.py:185][0m |           0.0059 |          70.6470 |           9.7385 |
[32m[20221213 23:49:42 @agent_ppo2.py:185][0m |          -0.0050 |          61.8576 |           9.8156 |
[32m[20221213 23:49:42 @agent_ppo2.py:185][0m |          -0.0092 |          59.6353 |           9.7918 |
[32m[20221213 23:49:42 @agent_ppo2.py:185][0m |          -0.0114 |          59.0235 |           9.8792 |
[32m[20221213 23:49:42 @agent_ppo2.py:185][0m |          -0.0136 |          58.3345 |           9.9160 |
[32m[20221213 23:49:42 @agent_ppo2.py:185][0m |          -0.0144 |          57.9306 |           9.9545 |
[32m[20221213 23:49:42 @agent_ppo2.py:185][0m |          -0.0154 |          57.5143 |           9.9944 |
[32m[20221213 23:49:43 @agent_ppo2.py:185][0m |          -0.0161 |          57.2879 |           9.9817 |
[32m[20221213 23:49:43 @agent_ppo2.py:185][0m |          -0.0142 |          56.8596 |          10.1593 |
[32m[20221213 23:49:43 @agent_ppo2.py:185][0m |          -0.0151 |          56.7781 |          10.0661 |
[32m[20221213 23:49:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:49:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.47
[32m[20221213 23:49:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.21
[32m[20221213 23:49:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.59
[32m[20221213 23:49:43 @agent_ppo2.py:143][0m Total time:      37.18 min
[32m[20221213 23:49:43 @agent_ppo2.py:145][0m 3604480 total steps have happened
[32m[20221213 23:49:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3760 --------------------------#
[32m[20221213 23:49:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:49:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:43 @agent_ppo2.py:185][0m |           0.0044 |          66.5531 |           9.8640 |
[32m[20221213 23:49:43 @agent_ppo2.py:185][0m |          -0.0058 |          63.5557 |           9.8699 |
[32m[20221213 23:49:43 @agent_ppo2.py:185][0m |          -0.0077 |          62.1467 |           9.9447 |
[32m[20221213 23:49:43 @agent_ppo2.py:185][0m |          -0.0096 |          61.5324 |           9.9930 |
[32m[20221213 23:49:44 @agent_ppo2.py:185][0m |           0.0010 |          69.8017 |           9.9633 |
[32m[20221213 23:49:44 @agent_ppo2.py:185][0m |          -0.0127 |          60.8985 |          10.0202 |
[32m[20221213 23:49:44 @agent_ppo2.py:185][0m |          -0.0151 |          60.0021 |          10.0427 |
[32m[20221213 23:49:44 @agent_ppo2.py:185][0m |          -0.0096 |          59.8496 |          10.0732 |
[32m[20221213 23:49:44 @agent_ppo2.py:185][0m |          -0.0130 |          59.5368 |          10.0896 |
[32m[20221213 23:49:44 @agent_ppo2.py:185][0m |          -0.0160 |          59.2329 |          10.1040 |
[32m[20221213 23:49:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:49:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.38
[32m[20221213 23:49:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.91
[32m[20221213 23:49:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.75
[32m[20221213 23:49:44 @agent_ppo2.py:143][0m Total time:      37.20 min
[32m[20221213 23:49:44 @agent_ppo2.py:145][0m 3606528 total steps have happened
[32m[20221213 23:49:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3761 --------------------------#
[32m[20221213 23:49:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:44 @agent_ppo2.py:185][0m |           0.0028 |          47.9279 |          10.1147 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0019 |          42.0385 |          10.1059 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0070 |          40.2646 |          10.0351 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0089 |          38.9778 |          10.0715 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0069 |          38.2417 |          10.0703 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0131 |          37.8249 |          10.0700 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0045 |          37.3214 |          10.0751 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0136 |          37.0560 |          10.0473 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0137 |          36.8470 |          10.0572 |
[32m[20221213 23:49:45 @agent_ppo2.py:185][0m |          -0.0120 |          36.0097 |          10.0339 |
[32m[20221213 23:49:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.98
[32m[20221213 23:49:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.31
[32m[20221213 23:49:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.74
[32m[20221213 23:49:45 @agent_ppo2.py:143][0m Total time:      37.22 min
[32m[20221213 23:49:45 @agent_ppo2.py:145][0m 3608576 total steps have happened
[32m[20221213 23:49:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3762 --------------------------#
[32m[20221213 23:49:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0014 |          67.8898 |           9.8478 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0057 |          60.5397 |           9.8999 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0068 |          57.8887 |           9.9571 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0013 |          58.1083 |           9.9625 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0083 |          55.5949 |           9.9984 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0042 |          55.4291 |          10.0219 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0072 |          54.9542 |           9.9883 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0117 |          53.3026 |          10.0673 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0121 |          52.7345 |          10.0690 |
[32m[20221213 23:49:46 @agent_ppo2.py:185][0m |          -0.0116 |          52.3806 |          10.0595 |
[32m[20221213 23:49:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.38
[32m[20221213 23:49:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.80
[32m[20221213 23:49:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.45
[32m[20221213 23:49:47 @agent_ppo2.py:143][0m Total time:      37.24 min
[32m[20221213 23:49:47 @agent_ppo2.py:145][0m 3610624 total steps have happened
[32m[20221213 23:49:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3763 --------------------------#
[32m[20221213 23:49:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:47 @agent_ppo2.py:185][0m |           0.0046 |          69.0850 |          10.3916 |
[32m[20221213 23:49:47 @agent_ppo2.py:185][0m |          -0.0112 |          58.0553 |          10.3754 |
[32m[20221213 23:49:47 @agent_ppo2.py:185][0m |          -0.0057 |          55.9733 |          10.4080 |
[32m[20221213 23:49:47 @agent_ppo2.py:185][0m |          -0.0033 |          53.1671 |          10.4036 |
[32m[20221213 23:49:47 @agent_ppo2.py:185][0m |          -0.0126 |          52.5724 |          10.4242 |
[32m[20221213 23:49:47 @agent_ppo2.py:185][0m |          -0.0056 |          50.9995 |          10.4648 |
[32m[20221213 23:49:47 @agent_ppo2.py:185][0m |          -0.0200 |          50.0800 |          10.4333 |
[32m[20221213 23:49:48 @agent_ppo2.py:185][0m |          -0.0149 |          49.1407 |          10.4060 |
[32m[20221213 23:49:48 @agent_ppo2.py:185][0m |          -0.0095 |          49.4422 |          10.4565 |
[32m[20221213 23:49:48 @agent_ppo2.py:185][0m |          -0.0130 |          47.9212 |          10.3975 |
[32m[20221213 23:49:48 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:49:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.48
[32m[20221213 23:49:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.82
[32m[20221213 23:49:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.57
[32m[20221213 23:49:48 @agent_ppo2.py:143][0m Total time:      37.27 min
[32m[20221213 23:49:48 @agent_ppo2.py:145][0m 3612672 total steps have happened
[32m[20221213 23:49:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3764 --------------------------#
[32m[20221213 23:49:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:49:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:48 @agent_ppo2.py:185][0m |          -0.0026 |          58.9158 |          10.4243 |
[32m[20221213 23:49:48 @agent_ppo2.py:185][0m |          -0.0087 |          52.8370 |          10.4701 |
[32m[20221213 23:49:48 @agent_ppo2.py:185][0m |          -0.0051 |          51.0580 |          10.4277 |
[32m[20221213 23:49:49 @agent_ppo2.py:185][0m |          -0.0039 |          51.0784 |          10.4686 |
[32m[20221213 23:49:49 @agent_ppo2.py:185][0m |          -0.0093 |          47.5341 |          10.4887 |
[32m[20221213 23:49:49 @agent_ppo2.py:185][0m |          -0.0140 |          46.4823 |          10.4524 |
[32m[20221213 23:49:49 @agent_ppo2.py:185][0m |          -0.0010 |          53.5519 |          10.4590 |
[32m[20221213 23:49:49 @agent_ppo2.py:185][0m |           0.0002 |          48.4764 |          10.4494 |
[32m[20221213 23:49:49 @agent_ppo2.py:185][0m |          -0.0168 |          44.9206 |          10.4436 |
[32m[20221213 23:49:49 @agent_ppo2.py:185][0m |          -0.0129 |          44.2044 |          10.4370 |
[32m[20221213 23:49:49 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:49:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.12
[32m[20221213 23:49:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.40
[32m[20221213 23:49:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.16
[32m[20221213 23:49:49 @agent_ppo2.py:143][0m Total time:      37.29 min
[32m[20221213 23:49:49 @agent_ppo2.py:145][0m 3614720 total steps have happened
[32m[20221213 23:49:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3765 --------------------------#
[32m[20221213 23:49:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0053 |          67.2648 |           9.8019 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0087 |          60.2009 |           9.7724 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0097 |          58.2857 |           9.7531 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0076 |          59.5000 |           9.7926 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0132 |          56.5323 |           9.8074 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0144 |          55.7721 |           9.7564 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0164 |          55.4097 |           9.7589 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0150 |          54.9504 |           9.7792 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0070 |          56.6491 |           9.7911 |
[32m[20221213 23:49:50 @agent_ppo2.py:185][0m |          -0.0141 |          53.9921 |           9.8425 |
[32m[20221213 23:49:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 23:49:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.66
[32m[20221213 23:49:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.59
[32m[20221213 23:49:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.49
[32m[20221213 23:49:51 @agent_ppo2.py:143][0m Total time:      37.31 min
[32m[20221213 23:49:51 @agent_ppo2.py:145][0m 3616768 total steps have happened
[32m[20221213 23:49:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3766 --------------------------#
[32m[20221213 23:49:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |          -0.0006 |          60.0137 |          10.4222 |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |          -0.0061 |          52.6716 |          10.4444 |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |          -0.0106 |          50.5636 |          10.4371 |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |          -0.0101 |          49.5789 |          10.4314 |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |          -0.0100 |          48.3305 |          10.4775 |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |          -0.0089 |          47.5876 |          10.4729 |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |          -0.0098 |          47.5712 |          10.4829 |
[32m[20221213 23:49:51 @agent_ppo2.py:185][0m |           0.0012 |          51.9141 |          10.4841 |
[32m[20221213 23:49:52 @agent_ppo2.py:185][0m |          -0.0115 |          45.7929 |          10.4709 |
[32m[20221213 23:49:52 @agent_ppo2.py:185][0m |          -0.0120 |          45.0968 |          10.4920 |
[32m[20221213 23:49:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:49:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.79
[32m[20221213 23:49:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.29
[32m[20221213 23:49:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.77
[32m[20221213 23:49:52 @agent_ppo2.py:143][0m Total time:      37.33 min
[32m[20221213 23:49:52 @agent_ppo2.py:145][0m 3618816 total steps have happened
[32m[20221213 23:49:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3767 --------------------------#
[32m[20221213 23:49:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:52 @agent_ppo2.py:185][0m |          -0.0018 |          72.7901 |          10.7311 |
[32m[20221213 23:49:52 @agent_ppo2.py:185][0m |          -0.0049 |          68.5695 |          10.7526 |
[32m[20221213 23:49:52 @agent_ppo2.py:185][0m |          -0.0028 |          68.8770 |          10.7348 |
[32m[20221213 23:49:52 @agent_ppo2.py:185][0m |          -0.0113 |          64.9131 |          10.7583 |
[32m[20221213 23:49:53 @agent_ppo2.py:185][0m |          -0.0133 |          63.4766 |          10.7396 |
[32m[20221213 23:49:53 @agent_ppo2.py:185][0m |          -0.0137 |          62.4502 |          10.7532 |
[32m[20221213 23:49:53 @agent_ppo2.py:185][0m |          -0.0029 |          67.6894 |          10.7434 |
[32m[20221213 23:49:53 @agent_ppo2.py:185][0m |          -0.0104 |          61.7433 |          10.6940 |
[32m[20221213 23:49:53 @agent_ppo2.py:185][0m |          -0.0159 |          60.8153 |          10.7030 |
[32m[20221213 23:49:53 @agent_ppo2.py:185][0m |          -0.0140 |          60.5988 |          10.7273 |
[32m[20221213 23:49:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:49:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.23
[32m[20221213 23:49:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.83
[32m[20221213 23:49:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.92
[32m[20221213 23:49:53 @agent_ppo2.py:143][0m Total time:      37.35 min
[32m[20221213 23:49:53 @agent_ppo2.py:145][0m 3620864 total steps have happened
[32m[20221213 23:49:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3768 --------------------------#
[32m[20221213 23:49:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:53 @agent_ppo2.py:185][0m |          -0.0011 |          57.3550 |          10.1890 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0070 |          49.8594 |          10.2615 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0117 |          48.0316 |          10.2360 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0138 |          46.4459 |          10.2687 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0145 |          45.6846 |          10.3127 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0126 |          45.3036 |          10.2892 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0193 |          44.8812 |          10.2634 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0113 |          46.3016 |          10.3082 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0140 |          44.2248 |          10.3628 |
[32m[20221213 23:49:54 @agent_ppo2.py:185][0m |          -0.0189 |          43.8381 |          10.3537 |
[32m[20221213 23:49:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.13
[32m[20221213 23:49:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.18
[32m[20221213 23:49:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.37
[32m[20221213 23:49:54 @agent_ppo2.py:143][0m Total time:      37.37 min
[32m[20221213 23:49:54 @agent_ppo2.py:145][0m 3622912 total steps have happened
[32m[20221213 23:49:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3769 --------------------------#
[32m[20221213 23:49:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |           0.0060 |          69.1883 |           9.9281 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0013 |          65.5061 |           9.9340 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0026 |          64.3445 |           9.9345 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0067 |          64.3307 |           9.9566 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |           0.0047 |          71.1879 |           9.8955 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0113 |          63.3981 |           9.9108 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0124 |          62.7687 |           9.8863 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0068 |          65.2264 |           9.8458 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0156 |          62.2897 |           9.8540 |
[32m[20221213 23:49:55 @agent_ppo2.py:185][0m |          -0.0152 |          62.1173 |           9.8322 |
[32m[20221213 23:49:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:49:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.58
[32m[20221213 23:49:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.92
[32m[20221213 23:49:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.72
[32m[20221213 23:49:56 @agent_ppo2.py:143][0m Total time:      37.39 min
[32m[20221213 23:49:56 @agent_ppo2.py:145][0m 3624960 total steps have happened
[32m[20221213 23:49:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3770 --------------------------#
[32m[20221213 23:49:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:49:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:56 @agent_ppo2.py:185][0m |          -0.0015 |          63.7450 |          10.1504 |
[32m[20221213 23:49:56 @agent_ppo2.py:185][0m |          -0.0033 |          56.6097 |          10.1322 |
[32m[20221213 23:49:56 @agent_ppo2.py:185][0m |          -0.0054 |          54.4800 |          10.1793 |
[32m[20221213 23:49:56 @agent_ppo2.py:185][0m |           0.0010 |          57.1226 |          10.1791 |
[32m[20221213 23:49:56 @agent_ppo2.py:185][0m |          -0.0114 |          52.5170 |          10.2326 |
[32m[20221213 23:49:56 @agent_ppo2.py:185][0m |          -0.0115 |          51.9654 |          10.2366 |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |          -0.0140 |          51.6296 |          10.2858 |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |          -0.0111 |          51.2822 |          10.2965 |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |          -0.0117 |          50.8976 |          10.2954 |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |          -0.0031 |          56.3511 |          10.3322 |
[32m[20221213 23:49:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.64
[32m[20221213 23:49:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.70
[32m[20221213 23:49:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.89
[32m[20221213 23:49:57 @agent_ppo2.py:143][0m Total time:      37.41 min
[32m[20221213 23:49:57 @agent_ppo2.py:145][0m 3627008 total steps have happened
[32m[20221213 23:49:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3771 --------------------------#
[32m[20221213 23:49:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |           0.0026 |          52.9369 |          10.2332 |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |          -0.0076 |          48.9077 |          10.2765 |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |          -0.0029 |          47.5763 |          10.3312 |
[32m[20221213 23:49:57 @agent_ppo2.py:185][0m |          -0.0011 |          49.0964 |          10.3274 |
[32m[20221213 23:49:58 @agent_ppo2.py:185][0m |          -0.0121 |          46.1713 |          10.3600 |
[32m[20221213 23:49:58 @agent_ppo2.py:185][0m |          -0.0115 |          45.3537 |          10.3283 |
[32m[20221213 23:49:58 @agent_ppo2.py:185][0m |          -0.0163 |          45.0290 |          10.4080 |
[32m[20221213 23:49:58 @agent_ppo2.py:185][0m |          -0.0169 |          44.7277 |          10.3882 |
[32m[20221213 23:49:58 @agent_ppo2.py:185][0m |          -0.0136 |          44.1431 |          10.4075 |
[32m[20221213 23:49:58 @agent_ppo2.py:185][0m |          -0.0222 |          44.2031 |          10.3923 |
[32m[20221213 23:49:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:49:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.16
[32m[20221213 23:49:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.00
[32m[20221213 23:49:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.38
[32m[20221213 23:49:58 @agent_ppo2.py:143][0m Total time:      37.44 min
[32m[20221213 23:49:58 @agent_ppo2.py:145][0m 3629056 total steps have happened
[32m[20221213 23:49:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3772 --------------------------#
[32m[20221213 23:49:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:49:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:49:58 @agent_ppo2.py:185][0m |           0.0116 |          67.9842 |          10.6572 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0019 |          54.9686 |          10.6962 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0088 |          52.9557 |          10.6416 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0147 |          51.7097 |          10.6420 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0088 |          50.5913 |          10.6494 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0083 |          49.7272 |          10.6343 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0114 |          49.9793 |          10.5809 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0102 |          49.2482 |          10.6034 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0160 |          48.1517 |          10.5651 |
[32m[20221213 23:49:59 @agent_ppo2.py:185][0m |          -0.0168 |          47.7252 |          10.5097 |
[32m[20221213 23:49:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:49:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.33
[32m[20221213 23:49:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.19
[32m[20221213 23:49:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.25
[32m[20221213 23:49:59 @agent_ppo2.py:143][0m Total time:      37.46 min
[32m[20221213 23:49:59 @agent_ppo2.py:145][0m 3631104 total steps have happened
[32m[20221213 23:49:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3773 --------------------------#
[32m[20221213 23:50:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |           0.0062 |          61.6558 |          10.4182 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0005 |          58.9009 |          10.4392 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0078 |          57.4239 |          10.4115 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0077 |          56.4810 |          10.4383 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0036 |          56.5956 |          10.4370 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0091 |          55.0166 |          10.4676 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0060 |          54.5336 |          10.4281 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0108 |          54.0569 |          10.4714 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0103 |          53.5467 |          10.4977 |
[32m[20221213 23:50:00 @agent_ppo2.py:185][0m |          -0.0137 |          53.4838 |          10.5388 |
[32m[20221213 23:50:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.03
[32m[20221213 23:50:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.71
[32m[20221213 23:50:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.99
[32m[20221213 23:50:01 @agent_ppo2.py:143][0m Total time:      37.48 min
[32m[20221213 23:50:01 @agent_ppo2.py:145][0m 3633152 total steps have happened
[32m[20221213 23:50:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3774 --------------------------#
[32m[20221213 23:50:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:01 @agent_ppo2.py:185][0m |          -0.0008 |          55.9476 |          10.2606 |
[32m[20221213 23:50:01 @agent_ppo2.py:185][0m |          -0.0091 |          51.2675 |          10.2954 |
[32m[20221213 23:50:01 @agent_ppo2.py:185][0m |          -0.0099 |          49.6668 |          10.2569 |
[32m[20221213 23:50:01 @agent_ppo2.py:185][0m |          -0.0152 |          48.6237 |          10.3181 |
[32m[20221213 23:50:01 @agent_ppo2.py:185][0m |          -0.0114 |          47.4199 |          10.2204 |
[32m[20221213 23:50:01 @agent_ppo2.py:185][0m |          -0.0131 |          45.6515 |          10.2476 |
[32m[20221213 23:50:02 @agent_ppo2.py:185][0m |          -0.0069 |          47.5048 |          10.2408 |
[32m[20221213 23:50:02 @agent_ppo2.py:185][0m |          -0.0084 |          44.3587 |          10.2479 |
[32m[20221213 23:50:02 @agent_ppo2.py:185][0m |          -0.0156 |          44.1083 |          10.2109 |
[32m[20221213 23:50:02 @agent_ppo2.py:185][0m |          -0.0193 |          43.7464 |          10.1991 |
[32m[20221213 23:50:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:50:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.38
[32m[20221213 23:50:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.23
[32m[20221213 23:50:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.65
[32m[20221213 23:50:02 @agent_ppo2.py:143][0m Total time:      37.50 min
[32m[20221213 23:50:02 @agent_ppo2.py:145][0m 3635200 total steps have happened
[32m[20221213 23:50:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3775 --------------------------#
[32m[20221213 23:50:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:02 @agent_ppo2.py:185][0m |          -0.0015 |          83.9284 |          10.4856 |
[32m[20221213 23:50:02 @agent_ppo2.py:185][0m |          -0.0050 |          71.5395 |          10.5214 |
[32m[20221213 23:50:02 @agent_ppo2.py:185][0m |          -0.0110 |          69.3944 |          10.4862 |
[32m[20221213 23:50:03 @agent_ppo2.py:185][0m |          -0.0038 |          68.2992 |          10.5491 |
[32m[20221213 23:50:03 @agent_ppo2.py:185][0m |          -0.0075 |          67.6884 |          10.5496 |
[32m[20221213 23:50:03 @agent_ppo2.py:185][0m |          -0.0052 |          66.8530 |          10.5316 |
[32m[20221213 23:50:03 @agent_ppo2.py:185][0m |          -0.0096 |          66.0616 |          10.5484 |
[32m[20221213 23:50:03 @agent_ppo2.py:185][0m |          -0.0006 |          71.0687 |          10.5491 |
[32m[20221213 23:50:03 @agent_ppo2.py:185][0m |          -0.0163 |          65.3502 |          10.5728 |
[32m[20221213 23:50:03 @agent_ppo2.py:185][0m |          -0.0139 |          65.0332 |          10.5739 |
[32m[20221213 23:50:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.85
[32m[20221213 23:50:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.02
[32m[20221213 23:50:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.37
[32m[20221213 23:50:03 @agent_ppo2.py:143][0m Total time:      37.52 min
[32m[20221213 23:50:03 @agent_ppo2.py:145][0m 3637248 total steps have happened
[32m[20221213 23:50:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3776 --------------------------#
[32m[20221213 23:50:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |           0.0145 |          78.8171 |          10.7036 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |           0.0011 |          66.5710 |          10.6886 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0074 |          62.7233 |          10.7244 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0141 |          61.1820 |          10.7424 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0131 |          60.5569 |          10.7511 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0142 |          59.4302 |          10.7790 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0155 |          58.8588 |          10.8198 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0072 |          59.4284 |          10.8227 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0181 |          58.1542 |          10.8513 |
[32m[20221213 23:50:04 @agent_ppo2.py:185][0m |          -0.0140 |          57.6779 |          10.8288 |
[32m[20221213 23:50:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.16
[32m[20221213 23:50:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.81
[32m[20221213 23:50:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.79
[32m[20221213 23:50:04 @agent_ppo2.py:143][0m Total time:      37.54 min
[32m[20221213 23:50:04 @agent_ppo2.py:145][0m 3639296 total steps have happened
[32m[20221213 23:50:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3777 --------------------------#
[32m[20221213 23:50:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |           0.0017 |          73.1703 |          10.8464 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0025 |          68.0064 |          10.8665 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0077 |          66.0747 |          10.8427 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0020 |          72.0353 |          10.8815 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0104 |          64.9867 |          10.8467 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0048 |          64.4276 |          10.8188 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0106 |          64.6228 |          10.8244 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0151 |          63.7648 |          10.8470 |
[32m[20221213 23:50:05 @agent_ppo2.py:185][0m |          -0.0037 |          72.6227 |          10.8671 |
[32m[20221213 23:50:06 @agent_ppo2.py:185][0m |          -0.0153 |          63.6704 |          10.8213 |
[32m[20221213 23:50:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.00
[32m[20221213 23:50:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.52
[32m[20221213 23:50:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.77
[32m[20221213 23:50:06 @agent_ppo2.py:143][0m Total time:      37.56 min
[32m[20221213 23:50:06 @agent_ppo2.py:145][0m 3641344 total steps have happened
[32m[20221213 23:50:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3778 --------------------------#
[32m[20221213 23:50:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:06 @agent_ppo2.py:185][0m |           0.0119 |          69.3768 |          11.1826 |
[32m[20221213 23:50:06 @agent_ppo2.py:185][0m |          -0.0022 |          50.3345 |          11.2044 |
[32m[20221213 23:50:06 @agent_ppo2.py:185][0m |          -0.0057 |          44.5272 |          11.2054 |
[32m[20221213 23:50:06 @agent_ppo2.py:185][0m |          -0.0009 |          43.7852 |          11.2175 |
[32m[20221213 23:50:06 @agent_ppo2.py:185][0m |          -0.0076 |          40.8775 |          11.2067 |
[32m[20221213 23:50:06 @agent_ppo2.py:185][0m |          -0.0105 |          39.0965 |          11.2491 |
[32m[20221213 23:50:07 @agent_ppo2.py:185][0m |          -0.0107 |          38.1258 |          11.2083 |
[32m[20221213 23:50:07 @agent_ppo2.py:185][0m |          -0.0141 |          37.3053 |          11.2451 |
[32m[20221213 23:50:07 @agent_ppo2.py:185][0m |          -0.0112 |          36.6887 |          11.2183 |
[32m[20221213 23:50:07 @agent_ppo2.py:185][0m |          -0.0131 |          35.9149 |          11.1791 |
[32m[20221213 23:50:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.79
[32m[20221213 23:50:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.59
[32m[20221213 23:50:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.96
[32m[20221213 23:50:07 @agent_ppo2.py:143][0m Total time:      37.58 min
[32m[20221213 23:50:07 @agent_ppo2.py:145][0m 3643392 total steps have happened
[32m[20221213 23:50:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3779 --------------------------#
[32m[20221213 23:50:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:07 @agent_ppo2.py:185][0m |          -0.0053 |          85.3364 |          11.1247 |
[32m[20221213 23:50:07 @agent_ppo2.py:185][0m |          -0.0087 |          73.2447 |          11.1311 |
[32m[20221213 23:50:07 @agent_ppo2.py:185][0m |          -0.0076 |          71.4416 |          11.1321 |
[32m[20221213 23:50:08 @agent_ppo2.py:185][0m |          -0.0090 |          69.6852 |          11.1142 |
[32m[20221213 23:50:08 @agent_ppo2.py:185][0m |          -0.0105 |          68.7301 |          11.1169 |
[32m[20221213 23:50:08 @agent_ppo2.py:185][0m |          -0.0041 |          69.4541 |          11.0658 |
[32m[20221213 23:50:08 @agent_ppo2.py:185][0m |          -0.0109 |          67.6393 |          11.1091 |
[32m[20221213 23:50:08 @agent_ppo2.py:185][0m |          -0.0138 |          66.9384 |          11.0532 |
[32m[20221213 23:50:08 @agent_ppo2.py:185][0m |          -0.0148 |          66.5570 |          11.0734 |
[32m[20221213 23:50:08 @agent_ppo2.py:185][0m |          -0.0132 |          66.1363 |          11.0701 |
[32m[20221213 23:50:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:50:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.75
[32m[20221213 23:50:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.42
[32m[20221213 23:50:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.16
[32m[20221213 23:50:08 @agent_ppo2.py:143][0m Total time:      37.60 min
[32m[20221213 23:50:08 @agent_ppo2.py:145][0m 3645440 total steps have happened
[32m[20221213 23:50:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3780 --------------------------#
[32m[20221213 23:50:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:50:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0004 |          64.6453 |          10.6043 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0061 |          59.4155 |          10.5727 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0087 |          57.2549 |          10.5481 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0115 |          55.5365 |          10.5513 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0117 |          54.3514 |          10.5262 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0119 |          53.5402 |          10.5369 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0159 |          53.0865 |          10.4864 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0157 |          52.3603 |          10.5489 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0156 |          51.9730 |          10.5309 |
[32m[20221213 23:50:09 @agent_ppo2.py:185][0m |          -0.0147 |          51.3877 |          10.5508 |
[32m[20221213 23:50:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:50:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.45
[32m[20221213 23:50:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.38
[32m[20221213 23:50:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.80
[32m[20221213 23:50:09 @agent_ppo2.py:143][0m Total time:      37.62 min
[32m[20221213 23:50:09 @agent_ppo2.py:145][0m 3647488 total steps have happened
[32m[20221213 23:50:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3781 --------------------------#
[32m[20221213 23:50:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0017 |          55.4697 |          10.6467 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0051 |          49.3672 |          10.6935 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0061 |          47.8400 |          10.7016 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0134 |          47.0844 |          10.7029 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0094 |          46.3390 |          10.8009 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0152 |          46.0273 |          10.7826 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0116 |          45.1940 |          10.7886 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0126 |          44.5958 |          10.7748 |
[32m[20221213 23:50:10 @agent_ppo2.py:185][0m |          -0.0134 |          44.2021 |          10.7628 |
[32m[20221213 23:50:11 @agent_ppo2.py:185][0m |          -0.0181 |          43.9562 |          10.8235 |
[32m[20221213 23:50:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.73
[32m[20221213 23:50:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.04
[32m[20221213 23:50:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.17
[32m[20221213 23:50:11 @agent_ppo2.py:143][0m Total time:      37.65 min
[32m[20221213 23:50:11 @agent_ppo2.py:145][0m 3649536 total steps have happened
[32m[20221213 23:50:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3782 --------------------------#
[32m[20221213 23:50:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:11 @agent_ppo2.py:185][0m |          -0.0008 |          59.7676 |          10.7963 |
[32m[20221213 23:50:11 @agent_ppo2.py:185][0m |          -0.0051 |          55.6577 |          10.7498 |
[32m[20221213 23:50:11 @agent_ppo2.py:185][0m |          -0.0020 |          54.3796 |          10.7442 |
[32m[20221213 23:50:11 @agent_ppo2.py:185][0m |          -0.0095 |          53.2588 |          10.6838 |
[32m[20221213 23:50:11 @agent_ppo2.py:185][0m |          -0.0143 |          52.5802 |          10.6775 |
[32m[20221213 23:50:11 @agent_ppo2.py:185][0m |          -0.0098 |          52.7112 |          10.6645 |
[32m[20221213 23:50:12 @agent_ppo2.py:185][0m |          -0.0047 |          56.5965 |          10.6119 |
[32m[20221213 23:50:12 @agent_ppo2.py:185][0m |          -0.0014 |          54.2155 |          10.6189 |
[32m[20221213 23:50:12 @agent_ppo2.py:185][0m |          -0.0153 |          50.8615 |          10.5830 |
[32m[20221213 23:50:12 @agent_ppo2.py:185][0m |          -0.0188 |          50.3934 |          10.5781 |
[32m[20221213 23:50:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.82
[32m[20221213 23:50:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.86
[32m[20221213 23:50:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.16
[32m[20221213 23:50:12 @agent_ppo2.py:143][0m Total time:      37.67 min
[32m[20221213 23:50:12 @agent_ppo2.py:145][0m 3651584 total steps have happened
[32m[20221213 23:50:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3783 --------------------------#
[32m[20221213 23:50:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:12 @agent_ppo2.py:185][0m |           0.0005 |          53.0859 |          10.6107 |
[32m[20221213 23:50:12 @agent_ppo2.py:185][0m |          -0.0062 |          49.1871 |          10.5772 |
[32m[20221213 23:50:12 @agent_ppo2.py:185][0m |          -0.0108 |          48.1302 |          10.6162 |
[32m[20221213 23:50:13 @agent_ppo2.py:185][0m |          -0.0080 |          47.5177 |          10.6087 |
[32m[20221213 23:50:13 @agent_ppo2.py:185][0m |          -0.0059 |          47.4347 |          10.5709 |
[32m[20221213 23:50:13 @agent_ppo2.py:185][0m |          -0.0056 |          46.7010 |          10.6055 |
[32m[20221213 23:50:13 @agent_ppo2.py:185][0m |          -0.0101 |          46.4708 |          10.6156 |
[32m[20221213 23:50:13 @agent_ppo2.py:185][0m |          -0.0137 |          46.0228 |          10.6220 |
[32m[20221213 23:50:13 @agent_ppo2.py:185][0m |          -0.0182 |          45.9566 |          10.5939 |
[32m[20221213 23:50:13 @agent_ppo2.py:185][0m |          -0.0158 |          45.7414 |          10.6433 |
[32m[20221213 23:50:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:50:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.30
[32m[20221213 23:50:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.15
[32m[20221213 23:50:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.51
[32m[20221213 23:50:13 @agent_ppo2.py:143][0m Total time:      37.69 min
[32m[20221213 23:50:13 @agent_ppo2.py:145][0m 3653632 total steps have happened
[32m[20221213 23:50:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3784 --------------------------#
[32m[20221213 23:50:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |           0.0054 |          46.7409 |          10.3999 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0062 |          42.7171 |          10.3406 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0039 |          41.1110 |          10.3534 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0128 |          39.9740 |          10.4176 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0163 |          39.1116 |          10.3969 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0078 |          38.3715 |          10.3531 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0144 |          37.8453 |          10.3966 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0140 |          37.4730 |          10.4391 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0161 |          37.1351 |          10.4575 |
[32m[20221213 23:50:14 @agent_ppo2.py:185][0m |          -0.0141 |          38.0342 |          10.4535 |
[32m[20221213 23:50:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:50:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.20
[32m[20221213 23:50:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.34
[32m[20221213 23:50:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.00
[32m[20221213 23:50:15 @agent_ppo2.py:143][0m Total time:      37.71 min
[32m[20221213 23:50:15 @agent_ppo2.py:145][0m 3655680 total steps have happened
[32m[20221213 23:50:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3785 --------------------------#
[32m[20221213 23:50:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |           0.0046 |          38.6847 |          10.2321 |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |          -0.0060 |          33.4485 |          10.3067 |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |           0.0064 |          31.7497 |          10.2621 |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |          -0.0104 |          30.8487 |          10.2632 |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |          -0.0104 |          29.9847 |          10.3334 |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |          -0.0128 |          29.2463 |          10.3572 |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |          -0.0111 |          28.8571 |          10.3614 |
[32m[20221213 23:50:15 @agent_ppo2.py:185][0m |          -0.0157 |          28.4798 |          10.3674 |
[32m[20221213 23:50:16 @agent_ppo2.py:185][0m |          -0.0173 |          28.1867 |          10.3932 |
[32m[20221213 23:50:16 @agent_ppo2.py:185][0m |          -0.0279 |          28.0854 |          10.4282 |
[32m[20221213 23:50:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.67
[32m[20221213 23:50:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.62
[32m[20221213 23:50:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.07
[32m[20221213 23:50:16 @agent_ppo2.py:143][0m Total time:      37.73 min
[32m[20221213 23:50:16 @agent_ppo2.py:145][0m 3657728 total steps have happened
[32m[20221213 23:50:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3786 --------------------------#
[32m[20221213 23:50:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:16 @agent_ppo2.py:185][0m |           0.0011 |          60.3262 |          10.1532 |
[32m[20221213 23:50:16 @agent_ppo2.py:185][0m |           0.0006 |          58.9061 |          10.1414 |
[32m[20221213 23:50:16 @agent_ppo2.py:185][0m |          -0.0156 |          56.2804 |          10.1532 |
[32m[20221213 23:50:16 @agent_ppo2.py:185][0m |          -0.0107 |          55.4520 |          10.1473 |
[32m[20221213 23:50:16 @agent_ppo2.py:185][0m |          -0.0143 |          54.9387 |          10.2024 |
[32m[20221213 23:50:17 @agent_ppo2.py:185][0m |          -0.0126 |          54.4224 |          10.1763 |
[32m[20221213 23:50:17 @agent_ppo2.py:185][0m |          -0.0120 |          53.9696 |          10.1685 |
[32m[20221213 23:50:17 @agent_ppo2.py:185][0m |          -0.0164 |          53.7048 |          10.1807 |
[32m[20221213 23:50:17 @agent_ppo2.py:185][0m |          -0.0164 |          53.2870 |          10.1941 |
[32m[20221213 23:50:17 @agent_ppo2.py:185][0m |          -0.0107 |          53.8421 |          10.2148 |
[32m[20221213 23:50:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.46
[32m[20221213 23:50:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.72
[32m[20221213 23:50:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.22
[32m[20221213 23:50:17 @agent_ppo2.py:143][0m Total time:      37.75 min
[32m[20221213 23:50:17 @agent_ppo2.py:145][0m 3659776 total steps have happened
[32m[20221213 23:50:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3787 --------------------------#
[32m[20221213 23:50:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:17 @agent_ppo2.py:185][0m |          -0.0032 |          66.8847 |          10.8949 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0075 |          63.7983 |          10.9452 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0110 |          63.5610 |          10.9644 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0093 |          62.6295 |          10.9794 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0111 |          61.9296 |          10.9808 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0107 |          62.9488 |          10.9736 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0145 |          61.0682 |          10.9656 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0128 |          60.9675 |          10.9958 |
[32m[20221213 23:50:18 @agent_ppo2.py:185][0m |          -0.0117 |          61.9826 |          11.0280 |
[32m[20221213 23:50:19 @agent_ppo2.py:185][0m |          -0.0149 |          60.6022 |          11.0145 |
[32m[20221213 23:50:19 @agent_ppo2.py:130][0m Policy update time: 1.33 s
[32m[20221213 23:50:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.35
[32m[20221213 23:50:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.29
[32m[20221213 23:50:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.81
[32m[20221213 23:50:19 @agent_ppo2.py:143][0m Total time:      37.78 min
[32m[20221213 23:50:19 @agent_ppo2.py:145][0m 3661824 total steps have happened
[32m[20221213 23:50:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3788 --------------------------#
[32m[20221213 23:50:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:19 @agent_ppo2.py:185][0m |           0.0035 |          42.2355 |          10.9574 |
[32m[20221213 23:50:19 @agent_ppo2.py:185][0m |          -0.0024 |          37.0073 |          10.9414 |
[32m[20221213 23:50:19 @agent_ppo2.py:185][0m |          -0.0059 |          35.4869 |          10.9180 |
[32m[20221213 23:50:20 @agent_ppo2.py:185][0m |          -0.0058 |          34.8546 |          10.8882 |
[32m[20221213 23:50:20 @agent_ppo2.py:185][0m |          -0.0085 |          34.1337 |          10.8572 |
[32m[20221213 23:50:20 @agent_ppo2.py:185][0m |          -0.0066 |          36.1905 |          10.8174 |
[32m[20221213 23:50:20 @agent_ppo2.py:185][0m |          -0.0079 |          33.4122 |          10.8033 |
[32m[20221213 23:50:20 @agent_ppo2.py:185][0m |          -0.0138 |          33.0062 |          10.7996 |
[32m[20221213 23:50:20 @agent_ppo2.py:185][0m |          -0.0206 |          32.6468 |          10.7520 |
[32m[20221213 23:50:20 @agent_ppo2.py:185][0m |          -0.0135 |          32.5692 |          10.7345 |
[32m[20221213 23:50:20 @agent_ppo2.py:130][0m Policy update time: 1.38 s
[32m[20221213 23:50:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.83
[32m[20221213 23:50:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.97
[32m[20221213 23:50:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.91
[32m[20221213 23:50:20 @agent_ppo2.py:143][0m Total time:      37.81 min
[32m[20221213 23:50:20 @agent_ppo2.py:145][0m 3663872 total steps have happened
[32m[20221213 23:50:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3789 --------------------------#
[32m[20221213 23:50:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |           0.0014 |          52.4091 |          10.4652 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0035 |          48.3126 |          10.4528 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0045 |          46.4240 |          10.4500 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0050 |          46.2139 |          10.4691 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0099 |          44.8329 |          10.4819 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0069 |          44.7024 |          10.4580 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0076 |          43.9856 |          10.4943 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0063 |          45.5820 |          10.4809 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0009 |          46.6153 |          10.4989 |
[32m[20221213 23:50:21 @agent_ppo2.py:185][0m |          -0.0104 |          43.2124 |          10.5127 |
[32m[20221213 23:50:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:50:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.58
[32m[20221213 23:50:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.55
[32m[20221213 23:50:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.19
[32m[20221213 23:50:22 @agent_ppo2.py:143][0m Total time:      37.83 min
[32m[20221213 23:50:22 @agent_ppo2.py:145][0m 3665920 total steps have happened
[32m[20221213 23:50:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3790 --------------------------#
[32m[20221213 23:50:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:50:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:22 @agent_ppo2.py:185][0m |          -0.0021 |          60.6955 |          10.4692 |
[32m[20221213 23:50:22 @agent_ppo2.py:185][0m |          -0.0097 |          54.6079 |          10.4801 |
[32m[20221213 23:50:22 @agent_ppo2.py:185][0m |          -0.0076 |          52.1285 |          10.5011 |
[32m[20221213 23:50:22 @agent_ppo2.py:185][0m |          -0.0161 |          50.7425 |          10.5357 |
[32m[20221213 23:50:22 @agent_ppo2.py:185][0m |          -0.0074 |          51.2739 |          10.5907 |
[32m[20221213 23:50:22 @agent_ppo2.py:185][0m |          -0.0149 |          48.3187 |          10.5990 |
[32m[20221213 23:50:23 @agent_ppo2.py:185][0m |          -0.0152 |          47.6122 |          10.6143 |
[32m[20221213 23:50:23 @agent_ppo2.py:185][0m |          -0.0215 |          46.3085 |          10.6330 |
[32m[20221213 23:50:23 @agent_ppo2.py:185][0m |          -0.0150 |          45.7277 |          10.6657 |
[32m[20221213 23:50:23 @agent_ppo2.py:185][0m |          -0.0105 |          48.3285 |          10.7000 |
[32m[20221213 23:50:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:50:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.87
[32m[20221213 23:50:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.31
[32m[20221213 23:50:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.33
[32m[20221213 23:50:23 @agent_ppo2.py:143][0m Total time:      37.85 min
[32m[20221213 23:50:23 @agent_ppo2.py:145][0m 3667968 total steps have happened
[32m[20221213 23:50:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3791 --------------------------#
[32m[20221213 23:50:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:23 @agent_ppo2.py:185][0m |           0.0041 |          70.2321 |          11.1067 |
[32m[20221213 23:50:23 @agent_ppo2.py:185][0m |          -0.0039 |          64.0547 |          11.0166 |
[32m[20221213 23:50:23 @agent_ppo2.py:185][0m |          -0.0069 |          62.2139 |          10.9990 |
[32m[20221213 23:50:24 @agent_ppo2.py:185][0m |          -0.0137 |          60.7568 |          10.9653 |
[32m[20221213 23:50:24 @agent_ppo2.py:185][0m |          -0.0096 |          59.3466 |          10.9378 |
[32m[20221213 23:50:24 @agent_ppo2.py:185][0m |          -0.0052 |          59.6212 |          10.9297 |
[32m[20221213 23:50:24 @agent_ppo2.py:185][0m |          -0.0049 |          61.0107 |          10.8750 |
[32m[20221213 23:50:24 @agent_ppo2.py:185][0m |          -0.0168 |          56.9317 |          10.8852 |
[32m[20221213 23:50:24 @agent_ppo2.py:185][0m |          -0.0119 |          57.4279 |          10.8253 |
[32m[20221213 23:50:24 @agent_ppo2.py:185][0m |          -0.0149 |          55.5452 |          10.7964 |
[32m[20221213 23:50:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:50:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.14
[32m[20221213 23:50:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.49
[32m[20221213 23:50:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.87
[32m[20221213 23:50:24 @agent_ppo2.py:143][0m Total time:      37.87 min
[32m[20221213 23:50:24 @agent_ppo2.py:145][0m 3670016 total steps have happened
[32m[20221213 23:50:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3792 --------------------------#
[32m[20221213 23:50:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |           0.0022 |          41.3486 |          11.1779 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |           0.0013 |          34.3060 |          11.1418 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0039 |          32.1437 |          11.1456 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0159 |          31.1394 |          11.1156 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0053 |          30.3245 |          11.1661 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0087 |          30.1301 |          11.1491 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0089 |          29.3908 |          11.1488 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0115 |          30.6454 |          11.0842 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0094 |          28.9875 |          11.1657 |
[32m[20221213 23:50:25 @agent_ppo2.py:185][0m |          -0.0118 |          28.1396 |          11.0926 |
[32m[20221213 23:50:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:50:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.14
[32m[20221213 23:50:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.12
[32m[20221213 23:50:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.49
[32m[20221213 23:50:25 @agent_ppo2.py:143][0m Total time:      37.89 min
[32m[20221213 23:50:25 @agent_ppo2.py:145][0m 3672064 total steps have happened
[32m[20221213 23:50:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3793 --------------------------#
[32m[20221213 23:50:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |           0.0005 |          50.4105 |          10.7033 |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |          -0.0090 |          46.1832 |          10.6882 |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |          -0.0112 |          44.6290 |          10.6631 |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |          -0.0097 |          43.2257 |          10.6820 |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |          -0.0081 |          42.3664 |          10.6247 |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |          -0.0087 |          42.1410 |          10.6287 |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |          -0.0120 |          41.6762 |          10.6680 |
[32m[20221213 23:50:26 @agent_ppo2.py:185][0m |          -0.0054 |          42.3167 |          10.6290 |
[32m[20221213 23:50:27 @agent_ppo2.py:185][0m |          -0.0133 |          40.4050 |          10.6181 |
[32m[20221213 23:50:27 @agent_ppo2.py:185][0m |          -0.0178 |          40.2967 |          10.5825 |
[32m[20221213 23:50:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:50:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.15
[32m[20221213 23:50:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.71
[32m[20221213 23:50:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.41
[32m[20221213 23:50:27 @agent_ppo2.py:143][0m Total time:      37.91 min
[32m[20221213 23:50:27 @agent_ppo2.py:145][0m 3674112 total steps have happened
[32m[20221213 23:50:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3794 --------------------------#
[32m[20221213 23:50:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:27 @agent_ppo2.py:185][0m |           0.0006 |          63.4665 |          10.3772 |
[32m[20221213 23:50:27 @agent_ppo2.py:185][0m |          -0.0053 |          57.2344 |          10.3760 |
[32m[20221213 23:50:27 @agent_ppo2.py:185][0m |          -0.0052 |          55.3636 |          10.3429 |
[32m[20221213 23:50:27 @agent_ppo2.py:185][0m |          -0.0082 |          54.1405 |          10.3724 |
[32m[20221213 23:50:28 @agent_ppo2.py:185][0m |          -0.0139 |          52.9364 |          10.4162 |
[32m[20221213 23:50:28 @agent_ppo2.py:185][0m |          -0.0095 |          52.5335 |          10.4552 |
[32m[20221213 23:50:28 @agent_ppo2.py:185][0m |          -0.0161 |          51.6056 |          10.4554 |
[32m[20221213 23:50:28 @agent_ppo2.py:185][0m |          -0.0135 |          51.7965 |          10.4781 |
[32m[20221213 23:50:28 @agent_ppo2.py:185][0m |          -0.0149 |          50.7692 |          10.4985 |
[32m[20221213 23:50:28 @agent_ppo2.py:185][0m |          -0.0146 |          50.3153 |          10.5006 |
[32m[20221213 23:50:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:50:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.46
[32m[20221213 23:50:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.21
[32m[20221213 23:50:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.76
[32m[20221213 23:50:28 @agent_ppo2.py:143][0m Total time:      37.93 min
[32m[20221213 23:50:28 @agent_ppo2.py:145][0m 3676160 total steps have happened
[32m[20221213 23:50:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3795 --------------------------#
[32m[20221213 23:50:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:28 @agent_ppo2.py:185][0m |           0.0043 |          64.5924 |          10.4998 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0084 |          57.5211 |          10.4431 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0065 |          56.2083 |          10.4500 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0083 |          55.0526 |          10.4801 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0132 |          54.7786 |          10.4800 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0151 |          53.8857 |          10.4722 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0108 |          54.1930 |          10.4484 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0176 |          53.1837 |          10.4942 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0109 |          53.0325 |          10.4871 |
[32m[20221213 23:50:29 @agent_ppo2.py:185][0m |          -0.0222 |          52.6984 |          10.4857 |
[32m[20221213 23:50:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.07
[32m[20221213 23:50:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.63
[32m[20221213 23:50:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.50
[32m[20221213 23:50:29 @agent_ppo2.py:143][0m Total time:      37.96 min
[32m[20221213 23:50:29 @agent_ppo2.py:145][0m 3678208 total steps have happened
[32m[20221213 23:50:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3796 --------------------------#
[32m[20221213 23:50:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |           0.0012 |          41.1920 |          10.4179 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0111 |          36.5387 |          10.4297 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0115 |          35.4280 |          10.4441 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0083 |          34.5602 |          10.4832 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0127 |          34.0949 |          10.4456 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0132 |          33.4903 |          10.4593 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0132 |          33.3766 |          10.4798 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0026 |          36.9786 |          10.5152 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0146 |          32.8225 |          10.5277 |
[32m[20221213 23:50:30 @agent_ppo2.py:185][0m |          -0.0152 |          32.4618 |          10.4959 |
[32m[20221213 23:50:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.06
[32m[20221213 23:50:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.69
[32m[20221213 23:50:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:50:31 @agent_ppo2.py:143][0m Total time:      37.98 min
[32m[20221213 23:50:31 @agent_ppo2.py:145][0m 3680256 total steps have happened
[32m[20221213 23:50:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3797 --------------------------#
[32m[20221213 23:50:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:31 @agent_ppo2.py:185][0m |          -0.0004 |          59.1991 |          10.4206 |
[32m[20221213 23:50:31 @agent_ppo2.py:185][0m |          -0.0003 |          55.3569 |          10.4571 |
[32m[20221213 23:50:31 @agent_ppo2.py:185][0m |          -0.0122 |          50.6366 |          10.5479 |
[32m[20221213 23:50:31 @agent_ppo2.py:185][0m |          -0.0061 |          49.4594 |          10.5183 |
[32m[20221213 23:50:31 @agent_ppo2.py:185][0m |           0.0026 |          50.9922 |          10.5391 |
[32m[20221213 23:50:31 @agent_ppo2.py:185][0m |          -0.0039 |          49.1526 |          10.5600 |
[32m[20221213 23:50:31 @agent_ppo2.py:185][0m |          -0.0117 |          47.0841 |          10.5863 |
[32m[20221213 23:50:32 @agent_ppo2.py:185][0m |          -0.0126 |          46.3405 |          10.5934 |
[32m[20221213 23:50:32 @agent_ppo2.py:185][0m |          -0.0165 |          45.7489 |          10.5755 |
[32m[20221213 23:50:32 @agent_ppo2.py:185][0m |          -0.0171 |          45.0357 |          10.5997 |
[32m[20221213 23:50:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:50:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.08
[32m[20221213 23:50:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.66
[32m[20221213 23:50:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.38
[32m[20221213 23:50:32 @agent_ppo2.py:143][0m Total time:      38.00 min
[32m[20221213 23:50:32 @agent_ppo2.py:145][0m 3682304 total steps have happened
[32m[20221213 23:50:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3798 --------------------------#
[32m[20221213 23:50:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:32 @agent_ppo2.py:185][0m |           0.0005 |          69.6472 |          10.8838 |
[32m[20221213 23:50:32 @agent_ppo2.py:185][0m |          -0.0043 |          66.3472 |          10.8311 |
[32m[20221213 23:50:32 @agent_ppo2.py:185][0m |           0.0046 |          75.3083 |          10.7974 |
[32m[20221213 23:50:32 @agent_ppo2.py:185][0m |          -0.0059 |          64.4186 |          10.8309 |
[32m[20221213 23:50:33 @agent_ppo2.py:185][0m |          -0.0097 |          63.6151 |          10.8334 |
[32m[20221213 23:50:33 @agent_ppo2.py:185][0m |          -0.0094 |          63.3735 |          10.8226 |
[32m[20221213 23:50:33 @agent_ppo2.py:185][0m |          -0.0078 |          63.2049 |          10.8216 |
[32m[20221213 23:50:33 @agent_ppo2.py:185][0m |          -0.0115 |          62.7926 |          10.8203 |
[32m[20221213 23:50:33 @agent_ppo2.py:185][0m |          -0.0093 |          62.6679 |          10.8404 |
[32m[20221213 23:50:33 @agent_ppo2.py:185][0m |          -0.0115 |          62.4623 |          10.8091 |
[32m[20221213 23:50:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.81
[32m[20221213 23:50:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.87
[32m[20221213 23:50:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.65
[32m[20221213 23:50:33 @agent_ppo2.py:143][0m Total time:      38.02 min
[32m[20221213 23:50:33 @agent_ppo2.py:145][0m 3684352 total steps have happened
[32m[20221213 23:50:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3799 --------------------------#
[32m[20221213 23:50:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:33 @agent_ppo2.py:185][0m |           0.0094 |          58.4692 |          10.9638 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0031 |          46.0677 |          10.9927 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0017 |          44.7490 |          10.9623 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0041 |          41.3033 |          10.9760 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0095 |          37.9585 |          10.9924 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0115 |          37.2895 |          10.9789 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0154 |          36.0261 |          11.0006 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0199 |          35.5119 |          10.9696 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0155 |          34.8237 |          11.0104 |
[32m[20221213 23:50:34 @agent_ppo2.py:185][0m |          -0.0159 |          34.0185 |          11.0114 |
[32m[20221213 23:50:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.29
[32m[20221213 23:50:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.59
[32m[20221213 23:50:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.10
[32m[20221213 23:50:34 @agent_ppo2.py:143][0m Total time:      38.04 min
[32m[20221213 23:50:34 @agent_ppo2.py:145][0m 3686400 total steps have happened
[32m[20221213 23:50:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3800 --------------------------#
[32m[20221213 23:50:35 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:50:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |           0.0021 |          55.9557 |          10.4186 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0027 |          52.5063 |          10.3631 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0039 |          51.0568 |          10.3193 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0110 |          50.2958 |          10.2987 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |           0.0001 |          55.3060 |          10.3015 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0112 |          49.3728 |          10.2986 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0120 |          49.1427 |          10.2451 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0117 |          48.8370 |          10.2801 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0106 |          48.6639 |          10.2612 |
[32m[20221213 23:50:35 @agent_ppo2.py:185][0m |          -0.0124 |          48.5391 |          10.2468 |
[32m[20221213 23:50:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.25
[32m[20221213 23:50:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.05
[32m[20221213 23:50:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.83
[32m[20221213 23:50:36 @agent_ppo2.py:143][0m Total time:      38.06 min
[32m[20221213 23:50:36 @agent_ppo2.py:145][0m 3688448 total steps have happened
[32m[20221213 23:50:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3801 --------------------------#
[32m[20221213 23:50:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:36 @agent_ppo2.py:185][0m |           0.0010 |          52.1368 |          10.4822 |
[32m[20221213 23:50:36 @agent_ppo2.py:185][0m |          -0.0083 |          48.2828 |          10.5343 |
[32m[20221213 23:50:36 @agent_ppo2.py:185][0m |          -0.0078 |          47.9560 |          10.5818 |
[32m[20221213 23:50:36 @agent_ppo2.py:185][0m |          -0.0157 |          46.0429 |          10.6343 |
[32m[20221213 23:50:36 @agent_ppo2.py:185][0m |          -0.0139 |          45.4652 |          10.6577 |
[32m[20221213 23:50:36 @agent_ppo2.py:185][0m |          -0.0157 |          45.0111 |          10.7467 |
[32m[20221213 23:50:36 @agent_ppo2.py:185][0m |          -0.0152 |          45.1063 |          10.7278 |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |          -0.0179 |          44.4863 |          10.7429 |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |          -0.0154 |          44.7017 |          10.7736 |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |          -0.0196 |          44.0347 |          10.7828 |
[32m[20221213 23:50:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.28
[32m[20221213 23:50:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.43
[32m[20221213 23:50:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.71
[32m[20221213 23:50:37 @agent_ppo2.py:143][0m Total time:      38.08 min
[32m[20221213 23:50:37 @agent_ppo2.py:145][0m 3690496 total steps have happened
[32m[20221213 23:50:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3802 --------------------------#
[32m[20221213 23:50:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |          -0.0002 |          67.2035 |          10.7188 |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |          -0.0023 |          65.4074 |          10.7200 |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |          -0.0032 |          65.2990 |          10.7005 |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |          -0.0069 |          64.5066 |          10.7075 |
[32m[20221213 23:50:37 @agent_ppo2.py:185][0m |           0.0051 |          69.2601 |          10.7243 |
[32m[20221213 23:50:38 @agent_ppo2.py:185][0m |          -0.0069 |          64.4078 |          10.7596 |
[32m[20221213 23:50:38 @agent_ppo2.py:185][0m |          -0.0072 |          64.0418 |          10.7920 |
[32m[20221213 23:50:38 @agent_ppo2.py:185][0m |          -0.0100 |          63.8803 |          10.7470 |
[32m[20221213 23:50:38 @agent_ppo2.py:185][0m |          -0.0127 |          63.8971 |          10.7515 |
[32m[20221213 23:50:38 @agent_ppo2.py:185][0m |          -0.0090 |          63.7419 |          10.7745 |
[32m[20221213 23:50:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:50:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.33
[32m[20221213 23:50:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.82
[32m[20221213 23:50:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.15
[32m[20221213 23:50:38 @agent_ppo2.py:143][0m Total time:      38.10 min
[32m[20221213 23:50:38 @agent_ppo2.py:145][0m 3692544 total steps have happened
[32m[20221213 23:50:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3803 --------------------------#
[32m[20221213 23:50:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:38 @agent_ppo2.py:185][0m |           0.0008 |          69.4179 |          11.0427 |
[32m[20221213 23:50:38 @agent_ppo2.py:185][0m |           0.0093 |          71.2873 |          11.0249 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0071 |          61.7020 |          10.9998 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0080 |          60.2739 |          11.0032 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0055 |          59.4977 |          10.9699 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0050 |          60.8974 |          11.0000 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0077 |          58.5892 |          10.9774 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0125 |          58.2384 |          10.9679 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0138 |          57.7976 |          10.9858 |
[32m[20221213 23:50:39 @agent_ppo2.py:185][0m |          -0.0137 |          57.5341 |          10.9515 |
[32m[20221213 23:50:39 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.08
[32m[20221213 23:50:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.97
[32m[20221213 23:50:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.78
[32m[20221213 23:50:39 @agent_ppo2.py:143][0m Total time:      38.12 min
[32m[20221213 23:50:39 @agent_ppo2.py:145][0m 3694592 total steps have happened
[32m[20221213 23:50:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3804 --------------------------#
[32m[20221213 23:50:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |           0.0006 |          62.4410 |          11.2131 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0111 |          57.6270 |          11.2280 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0027 |          55.8017 |          11.2438 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0052 |          55.8506 |          11.2765 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0083 |          56.0394 |          11.2788 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |           0.0024 |          61.1898 |          11.2965 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0179 |          54.0306 |          11.3261 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0104 |          53.2151 |          11.3406 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0127 |          52.7921 |          11.3262 |
[32m[20221213 23:50:40 @agent_ppo2.py:185][0m |          -0.0141 |          52.5019 |          11.3582 |
[32m[20221213 23:50:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.82
[32m[20221213 23:50:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.06
[32m[20221213 23:50:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.04
[32m[20221213 23:50:41 @agent_ppo2.py:143][0m Total time:      38.14 min
[32m[20221213 23:50:41 @agent_ppo2.py:145][0m 3696640 total steps have happened
[32m[20221213 23:50:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3805 --------------------------#
[32m[20221213 23:50:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |          -0.0032 |          63.2923 |          11.0876 |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |          -0.0069 |          57.3749 |          11.1264 |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |          -0.0054 |          55.0853 |          11.0881 |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |           0.0035 |          57.5268 |          11.1479 |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |          -0.0087 |          52.8963 |          11.1078 |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |          -0.0082 |          52.3097 |          11.1578 |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |          -0.0128 |          51.6585 |          11.1671 |
[32m[20221213 23:50:41 @agent_ppo2.py:185][0m |          -0.0125 |          51.8073 |          11.1576 |
[32m[20221213 23:50:42 @agent_ppo2.py:185][0m |          -0.0089 |          53.5882 |          11.1527 |
[32m[20221213 23:50:42 @agent_ppo2.py:185][0m |          -0.0174 |          50.7105 |          11.2181 |
[32m[20221213 23:50:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.10
[32m[20221213 23:50:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.26
[32m[20221213 23:50:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.29
[32m[20221213 23:50:42 @agent_ppo2.py:143][0m Total time:      38.16 min
[32m[20221213 23:50:42 @agent_ppo2.py:145][0m 3698688 total steps have happened
[32m[20221213 23:50:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3806 --------------------------#
[32m[20221213 23:50:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:42 @agent_ppo2.py:185][0m |           0.0020 |          58.0243 |          10.9978 |
[32m[20221213 23:50:42 @agent_ppo2.py:185][0m |          -0.0047 |          53.6586 |          10.9699 |
[32m[20221213 23:50:42 @agent_ppo2.py:185][0m |          -0.0108 |          52.7628 |          11.0038 |
[32m[20221213 23:50:42 @agent_ppo2.py:185][0m |          -0.0081 |          52.3206 |          10.9824 |
[32m[20221213 23:50:42 @agent_ppo2.py:185][0m |          -0.0132 |          52.0496 |          11.0523 |
[32m[20221213 23:50:43 @agent_ppo2.py:185][0m |          -0.0132 |          51.7329 |          10.9856 |
[32m[20221213 23:50:43 @agent_ppo2.py:185][0m |          -0.0122 |          51.5516 |          10.9959 |
[32m[20221213 23:50:43 @agent_ppo2.py:185][0m |          -0.0122 |          51.3961 |          10.9966 |
[32m[20221213 23:50:43 @agent_ppo2.py:185][0m |          -0.0145 |          51.8084 |          10.9821 |
[32m[20221213 23:50:43 @agent_ppo2.py:185][0m |          -0.0135 |          51.1683 |          10.9592 |
[32m[20221213 23:50:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.71
[32m[20221213 23:50:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.44
[32m[20221213 23:50:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.95
[32m[20221213 23:50:43 @agent_ppo2.py:143][0m Total time:      38.18 min
[32m[20221213 23:50:43 @agent_ppo2.py:145][0m 3700736 total steps have happened
[32m[20221213 23:50:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3807 --------------------------#
[32m[20221213 23:50:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:43 @agent_ppo2.py:185][0m |           0.0017 |          65.1058 |          11.4254 |
[32m[20221213 23:50:43 @agent_ppo2.py:185][0m |          -0.0068 |          54.7217 |          11.3817 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |          -0.0077 |          53.3455 |          11.3823 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |           0.0009 |          53.1679 |          11.3697 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |          -0.0075 |          51.7049 |          11.4215 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |          -0.0130 |          51.4518 |          11.4460 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |          -0.0192 |          50.6117 |          11.4497 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |          -0.0115 |          50.0773 |          11.4809 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |          -0.0162 |          50.1395 |          11.5017 |
[32m[20221213 23:50:44 @agent_ppo2.py:185][0m |          -0.0148 |          50.1450 |          11.4992 |
[32m[20221213 23:50:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:50:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.91
[32m[20221213 23:50:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.95
[32m[20221213 23:50:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.49
[32m[20221213 23:50:44 @agent_ppo2.py:143][0m Total time:      38.21 min
[32m[20221213 23:50:44 @agent_ppo2.py:145][0m 3702784 total steps have happened
[32m[20221213 23:50:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3808 --------------------------#
[32m[20221213 23:50:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |           0.0001 |          40.2985 |          11.4880 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0049 |          34.7472 |          11.4415 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0112 |          32.6029 |          11.4320 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0046 |          31.5544 |          11.4827 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0203 |          30.7539 |          11.4894 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0070 |          30.2666 |          11.4816 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0212 |          29.7393 |          11.4973 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0142 |          29.8507 |          11.5096 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0197 |          28.7250 |          11.5160 |
[32m[20221213 23:50:45 @agent_ppo2.py:185][0m |          -0.0214 |          28.3993 |          11.5247 |
[32m[20221213 23:50:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.32
[32m[20221213 23:50:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.63
[32m[20221213 23:50:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.11
[32m[20221213 23:50:46 @agent_ppo2.py:143][0m Total time:      38.23 min
[32m[20221213 23:50:46 @agent_ppo2.py:145][0m 3704832 total steps have happened
[32m[20221213 23:50:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3809 --------------------------#
[32m[20221213 23:50:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |           0.0079 |          52.4065 |          11.2493 |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |           0.0027 |          53.9652 |          11.1962 |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |          -0.0102 |          43.8182 |          11.2424 |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |          -0.0142 |          42.4006 |          11.2552 |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |          -0.0181 |          41.6197 |          11.2426 |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |          -0.0169 |          41.2445 |          11.2620 |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |          -0.0171 |          40.9294 |          11.2691 |
[32m[20221213 23:50:46 @agent_ppo2.py:185][0m |          -0.0156 |          40.3001 |          11.2861 |
[32m[20221213 23:50:47 @agent_ppo2.py:185][0m |          -0.0180 |          40.1765 |          11.2662 |
[32m[20221213 23:50:47 @agent_ppo2.py:185][0m |          -0.0197 |          39.7877 |          11.2879 |
[32m[20221213 23:50:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.58
[32m[20221213 23:50:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.99
[32m[20221213 23:50:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.17
[32m[20221213 23:50:47 @agent_ppo2.py:143][0m Total time:      38.25 min
[32m[20221213 23:50:47 @agent_ppo2.py:145][0m 3706880 total steps have happened
[32m[20221213 23:50:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3810 --------------------------#
[32m[20221213 23:50:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:50:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:47 @agent_ppo2.py:185][0m |           0.0006 |          36.9656 |          11.7645 |
[32m[20221213 23:50:47 @agent_ppo2.py:185][0m |          -0.0122 |          32.1454 |          11.7253 |
[32m[20221213 23:50:47 @agent_ppo2.py:185][0m |          -0.0098 |          30.6207 |          11.7359 |
[32m[20221213 23:50:47 @agent_ppo2.py:185][0m |          -0.0138 |          29.6776 |          11.7102 |
[32m[20221213 23:50:47 @agent_ppo2.py:185][0m |          -0.0157 |          29.0241 |          11.7116 |
[32m[20221213 23:50:48 @agent_ppo2.py:185][0m |          -0.0095 |          28.5076 |          11.7107 |
[32m[20221213 23:50:48 @agent_ppo2.py:185][0m |          -0.0132 |          27.8775 |          11.7141 |
[32m[20221213 23:50:48 @agent_ppo2.py:185][0m |          -0.0210 |          27.4644 |          11.6535 |
[32m[20221213 23:50:48 @agent_ppo2.py:185][0m |          -0.0198 |          26.9958 |          11.6844 |
[32m[20221213 23:50:48 @agent_ppo2.py:185][0m |          -0.0244 |          26.8008 |          11.6643 |
[32m[20221213 23:50:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.61
[32m[20221213 23:50:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.78
[32m[20221213 23:50:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.74
[32m[20221213 23:50:48 @agent_ppo2.py:143][0m Total time:      38.27 min
[32m[20221213 23:50:48 @agent_ppo2.py:145][0m 3708928 total steps have happened
[32m[20221213 23:50:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3811 --------------------------#
[32m[20221213 23:50:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:48 @agent_ppo2.py:185][0m |           0.0033 |          32.1596 |          11.3713 |
[32m[20221213 23:50:48 @agent_ppo2.py:185][0m |          -0.0035 |          27.4036 |          11.3830 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0049 |          25.8822 |          11.3529 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0131 |          24.3662 |          11.3380 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0129 |          23.7390 |          11.3008 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0048 |          23.0601 |          11.3137 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0194 |          22.4901 |          11.3363 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0278 |          21.9475 |          11.3283 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0127 |          21.9721 |          11.3200 |
[32m[20221213 23:50:49 @agent_ppo2.py:185][0m |          -0.0184 |          21.0685 |          11.3442 |
[32m[20221213 23:50:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.80
[32m[20221213 23:50:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.69
[32m[20221213 23:50:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.19
[32m[20221213 23:50:49 @agent_ppo2.py:143][0m Total time:      38.29 min
[32m[20221213 23:50:49 @agent_ppo2.py:145][0m 3710976 total steps have happened
[32m[20221213 23:50:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3812 --------------------------#
[32m[20221213 23:50:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:50:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |           0.0057 |          52.8273 |          11.6890 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0068 |          49.2787 |          11.6598 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |           0.0027 |          51.2163 |          11.6407 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0113 |          47.5935 |          11.6507 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0139 |          46.6786 |          11.6562 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0115 |          46.2665 |          11.6695 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0153 |          45.7386 |          11.6995 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0142 |          45.3006 |          11.7048 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0144 |          44.8846 |          11.6984 |
[32m[20221213 23:50:50 @agent_ppo2.py:185][0m |          -0.0183 |          44.5401 |          11.7074 |
[32m[20221213 23:50:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:50:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.84
[32m[20221213 23:50:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.64
[32m[20221213 23:50:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.57
[32m[20221213 23:50:51 @agent_ppo2.py:143][0m Total time:      38.31 min
[32m[20221213 23:50:51 @agent_ppo2.py:145][0m 3713024 total steps have happened
[32m[20221213 23:50:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3813 --------------------------#
[32m[20221213 23:50:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0006 |          49.6933 |          11.1166 |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0077 |          43.7741 |          11.1186 |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0080 |          41.7871 |          11.1446 |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0139 |          40.7275 |          11.1687 |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0128 |          40.0109 |          11.1920 |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0181 |          39.5548 |          11.1751 |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0147 |          39.0070 |          11.1899 |
[32m[20221213 23:50:51 @agent_ppo2.py:185][0m |          -0.0171 |          38.6904 |          11.2359 |
[32m[20221213 23:50:52 @agent_ppo2.py:185][0m |          -0.0182 |          38.4139 |          11.2107 |
[32m[20221213 23:50:52 @agent_ppo2.py:185][0m |          -0.0168 |          38.3087 |          11.2237 |
[32m[20221213 23:50:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:50:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.22
[32m[20221213 23:50:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.75
[32m[20221213 23:50:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.71
[32m[20221213 23:50:52 @agent_ppo2.py:143][0m Total time:      38.33 min
[32m[20221213 23:50:52 @agent_ppo2.py:145][0m 3715072 total steps have happened
[32m[20221213 23:50:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3814 --------------------------#
[32m[20221213 23:50:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:52 @agent_ppo2.py:185][0m |          -0.0038 |          46.9821 |          11.7781 |
[32m[20221213 23:50:52 @agent_ppo2.py:185][0m |          -0.0068 |          41.8350 |          11.7638 |
[32m[20221213 23:50:52 @agent_ppo2.py:185][0m |          -0.0130 |          40.1322 |          11.7788 |
[32m[20221213 23:50:52 @agent_ppo2.py:185][0m |          -0.0078 |          39.1059 |          11.7963 |
[32m[20221213 23:50:52 @agent_ppo2.py:185][0m |          -0.0149 |          37.8982 |          11.7914 |
[32m[20221213 23:50:53 @agent_ppo2.py:185][0m |          -0.0165 |          37.2700 |          11.8232 |
[32m[20221213 23:50:53 @agent_ppo2.py:185][0m |          -0.0168 |          36.5909 |          11.8101 |
[32m[20221213 23:50:53 @agent_ppo2.py:185][0m |          -0.0205 |          36.1921 |          11.8476 |
[32m[20221213 23:50:53 @agent_ppo2.py:185][0m |          -0.0137 |          35.7771 |          11.8563 |
[32m[20221213 23:50:53 @agent_ppo2.py:185][0m |          -0.0188 |          35.3680 |          11.8625 |
[32m[20221213 23:50:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:50:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.58
[32m[20221213 23:50:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.23
[32m[20221213 23:50:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.20
[32m[20221213 23:50:53 @agent_ppo2.py:143][0m Total time:      38.35 min
[32m[20221213 23:50:53 @agent_ppo2.py:145][0m 3717120 total steps have happened
[32m[20221213 23:50:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3815 --------------------------#
[32m[20221213 23:50:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:53 @agent_ppo2.py:185][0m |          -0.0026 |          43.6524 |          11.7538 |
[32m[20221213 23:50:53 @agent_ppo2.py:185][0m |           0.0028 |          41.0238 |          11.7404 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0066 |          38.3541 |          11.7580 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0060 |          37.7631 |          11.7419 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0103 |          36.8750 |          11.7602 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0119 |          36.4245 |          11.7531 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0136 |          35.8462 |          11.7461 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0151 |          35.3186 |          11.7587 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0140 |          35.2979 |          11.7428 |
[32m[20221213 23:50:54 @agent_ppo2.py:185][0m |          -0.0182 |          34.7983 |          11.7665 |
[32m[20221213 23:50:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:50:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.44
[32m[20221213 23:50:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.34
[32m[20221213 23:50:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.64
[32m[20221213 23:50:54 @agent_ppo2.py:143][0m Total time:      38.37 min
[32m[20221213 23:50:54 @agent_ppo2.py:145][0m 3719168 total steps have happened
[32m[20221213 23:50:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3816 --------------------------#
[32m[20221213 23:50:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |           0.0019 |          58.9196 |          11.6991 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0083 |          56.2525 |          11.6747 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0125 |          55.4318 |          11.6728 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0123 |          54.9287 |          11.6443 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0118 |          54.3304 |          11.6849 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0152 |          54.1642 |          11.6624 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0110 |          53.7952 |          11.6596 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0010 |          61.7481 |          11.6679 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0147 |          53.9736 |          11.6571 |
[32m[20221213 23:50:55 @agent_ppo2.py:185][0m |          -0.0153 |          53.3209 |          11.6605 |
[32m[20221213 23:50:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.14
[32m[20221213 23:50:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.51
[32m[20221213 23:50:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.93
[32m[20221213 23:50:55 @agent_ppo2.py:143][0m Total time:      38.39 min
[32m[20221213 23:50:55 @agent_ppo2.py:145][0m 3721216 total steps have happened
[32m[20221213 23:50:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3817 --------------------------#
[32m[20221213 23:50:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |           0.0118 |          62.5511 |          11.9910 |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |          -0.0014 |          57.2556 |          11.9916 |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |          -0.0061 |          55.5882 |          11.9748 |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |          -0.0044 |          55.1636 |          11.9340 |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |          -0.0094 |          54.5448 |          11.9180 |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |           0.0095 |          62.0039 |          11.9245 |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |          -0.0106 |          54.2480 |          11.9209 |
[32m[20221213 23:50:56 @agent_ppo2.py:185][0m |          -0.0122 |          53.5011 |          11.9182 |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0160 |          53.0853 |          11.9151 |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0095 |          53.5810 |          11.8920 |
[32m[20221213 23:50:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:50:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.26
[32m[20221213 23:50:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.95
[32m[20221213 23:50:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.14
[32m[20221213 23:50:57 @agent_ppo2.py:143][0m Total time:      38.41 min
[32m[20221213 23:50:57 @agent_ppo2.py:145][0m 3723264 total steps have happened
[32m[20221213 23:50:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3818 --------------------------#
[32m[20221213 23:50:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0002 |          34.9028 |          11.7308 |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0037 |          29.2257 |          11.7616 |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0108 |          27.6143 |          11.7795 |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0059 |          26.9028 |          11.7708 |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0139 |          26.2158 |          11.7994 |
[32m[20221213 23:50:57 @agent_ppo2.py:185][0m |          -0.0125 |          25.6387 |          11.8634 |
[32m[20221213 23:50:58 @agent_ppo2.py:185][0m |          -0.0201 |          25.1514 |          11.8499 |
[32m[20221213 23:50:58 @agent_ppo2.py:185][0m |          -0.0181 |          24.8831 |          11.8724 |
[32m[20221213 23:50:58 @agent_ppo2.py:185][0m |          -0.0098 |          24.8050 |          11.8722 |
[32m[20221213 23:50:58 @agent_ppo2.py:185][0m |          -0.0193 |          24.3333 |          11.8938 |
[32m[20221213 23:50:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:50:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.26
[32m[20221213 23:50:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.30
[32m[20221213 23:50:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.15
[32m[20221213 23:50:58 @agent_ppo2.py:143][0m Total time:      38.43 min
[32m[20221213 23:50:58 @agent_ppo2.py:145][0m 3725312 total steps have happened
[32m[20221213 23:50:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3819 --------------------------#
[32m[20221213 23:50:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:50:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:50:58 @agent_ppo2.py:185][0m |           0.0066 |          47.7285 |          11.6385 |
[32m[20221213 23:50:58 @agent_ppo2.py:185][0m |          -0.0046 |          43.7320 |          11.7189 |
[32m[20221213 23:50:58 @agent_ppo2.py:185][0m |          -0.0001 |          42.4394 |          11.7298 |
[32m[20221213 23:50:59 @agent_ppo2.py:185][0m |          -0.0123 |          41.1844 |          11.7682 |
[32m[20221213 23:50:59 @agent_ppo2.py:185][0m |          -0.0109 |          40.0504 |          11.7876 |
[32m[20221213 23:50:59 @agent_ppo2.py:185][0m |          -0.0133 |          39.4220 |          11.8282 |
[32m[20221213 23:50:59 @agent_ppo2.py:185][0m |          -0.0171 |          38.9971 |          11.8755 |
[32m[20221213 23:50:59 @agent_ppo2.py:185][0m |          -0.0166 |          38.7018 |          11.8844 |
[32m[20221213 23:50:59 @agent_ppo2.py:185][0m |          -0.0180 |          38.2257 |          11.8738 |
[32m[20221213 23:50:59 @agent_ppo2.py:185][0m |          -0.0193 |          37.9674 |          11.8728 |
[32m[20221213 23:50:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:50:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.37
[32m[20221213 23:50:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.04
[32m[20221213 23:50:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.79
[32m[20221213 23:50:59 @agent_ppo2.py:143][0m Total time:      38.45 min
[32m[20221213 23:50:59 @agent_ppo2.py:145][0m 3727360 total steps have happened
[32m[20221213 23:50:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3820 --------------------------#
[32m[20221213 23:50:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:50:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0000 |          53.2984 |          12.0854 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |           0.0108 |          49.1368 |          12.1178 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0087 |          44.8386 |          12.1338 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0103 |          43.7693 |          12.1287 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0114 |          43.2361 |          12.1629 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0121 |          42.8433 |          12.1695 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0180 |          42.4099 |          12.1875 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0146 |          42.1528 |          12.1748 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0151 |          41.8716 |          12.1542 |
[32m[20221213 23:51:00 @agent_ppo2.py:185][0m |          -0.0131 |          43.3284 |          12.1842 |
[32m[20221213 23:51:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.64
[32m[20221213 23:51:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.50
[32m[20221213 23:51:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.11
[32m[20221213 23:51:00 @agent_ppo2.py:143][0m Total time:      38.47 min
[32m[20221213 23:51:00 @agent_ppo2.py:145][0m 3729408 total steps have happened
[32m[20221213 23:51:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3821 --------------------------#
[32m[20221213 23:51:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |           0.0052 |          36.0656 |          11.9169 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |           0.0004 |          29.2189 |          11.9050 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |          -0.0041 |          27.8124 |          11.8428 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |          -0.0137 |          26.8129 |          11.8827 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |          -0.0159 |          26.0355 |          11.8406 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |          -0.0141 |          25.4182 |          11.8374 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |          -0.0137 |          25.1325 |          11.7975 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |          -0.0158 |          24.6110 |          11.7507 |
[32m[20221213 23:51:01 @agent_ppo2.py:185][0m |          -0.0211 |          24.1201 |          11.7559 |
[32m[20221213 23:51:02 @agent_ppo2.py:185][0m |          -0.0190 |          23.8692 |          11.7399 |
[32m[20221213 23:51:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:51:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.54
[32m[20221213 23:51:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.94
[32m[20221213 23:51:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.51
[32m[20221213 23:51:02 @agent_ppo2.py:143][0m Total time:      38.49 min
[32m[20221213 23:51:02 @agent_ppo2.py:145][0m 3731456 total steps have happened
[32m[20221213 23:51:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3822 --------------------------#
[32m[20221213 23:51:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:02 @agent_ppo2.py:185][0m |          -0.0029 |          66.7265 |          11.6696 |
[32m[20221213 23:51:02 @agent_ppo2.py:185][0m |          -0.0076 |          62.4827 |          11.6207 |
[32m[20221213 23:51:02 @agent_ppo2.py:185][0m |          -0.0012 |          64.6643 |          11.6609 |
[32m[20221213 23:51:02 @agent_ppo2.py:185][0m |          -0.0094 |          60.3590 |          11.6942 |
[32m[20221213 23:51:02 @agent_ppo2.py:185][0m |          -0.0137 |          59.5682 |          11.7126 |
[32m[20221213 23:51:02 @agent_ppo2.py:185][0m |          -0.0134 |          59.0642 |          11.7270 |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |          -0.0114 |          58.4179 |          11.7666 |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |          -0.0129 |          57.9159 |          11.7590 |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |          -0.0161 |          57.7065 |          11.7752 |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |          -0.0124 |          57.6856 |          11.7560 |
[32m[20221213 23:51:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:51:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.00
[32m[20221213 23:51:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.34
[32m[20221213 23:51:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.66
[32m[20221213 23:51:03 @agent_ppo2.py:143][0m Total time:      38.51 min
[32m[20221213 23:51:03 @agent_ppo2.py:145][0m 3733504 total steps have happened
[32m[20221213 23:51:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3823 --------------------------#
[32m[20221213 23:51:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |           0.0021 |          64.4489 |          11.6099 |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |          -0.0042 |          59.7480 |          11.6967 |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |           0.0026 |          61.0354 |          11.6823 |
[32m[20221213 23:51:03 @agent_ppo2.py:185][0m |          -0.0073 |          55.5387 |          11.7658 |
[32m[20221213 23:51:04 @agent_ppo2.py:185][0m |          -0.0079 |          54.3057 |          11.7869 |
[32m[20221213 23:51:04 @agent_ppo2.py:185][0m |          -0.0073 |          53.2673 |          11.8139 |
[32m[20221213 23:51:04 @agent_ppo2.py:185][0m |          -0.0088 |          52.5117 |          11.8344 |
[32m[20221213 23:51:04 @agent_ppo2.py:185][0m |           0.0117 |          60.2437 |          11.8660 |
[32m[20221213 23:51:04 @agent_ppo2.py:185][0m |          -0.0078 |          51.2881 |          11.8835 |
[32m[20221213 23:51:04 @agent_ppo2.py:185][0m |          -0.0093 |          50.5730 |          11.9007 |
[32m[20221213 23:51:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.45
[32m[20221213 23:51:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.09
[32m[20221213 23:51:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.64
[32m[20221213 23:51:04 @agent_ppo2.py:143][0m Total time:      38.54 min
[32m[20221213 23:51:04 @agent_ppo2.py:145][0m 3735552 total steps have happened
[32m[20221213 23:51:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3824 --------------------------#
[32m[20221213 23:51:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:04 @agent_ppo2.py:185][0m |           0.0018 |          66.3524 |          12.0934 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0038 |          58.4636 |          12.1795 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0065 |          56.4298 |          12.0947 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0106 |          55.0881 |          12.1486 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0117 |          54.4252 |          12.1024 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0099 |          53.9740 |          12.1384 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0131 |          53.4371 |          12.1797 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0136 |          53.1443 |          12.1698 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0145 |          52.8693 |          12.1747 |
[32m[20221213 23:51:05 @agent_ppo2.py:185][0m |          -0.0126 |          52.4409 |          12.1663 |
[32m[20221213 23:51:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.75
[32m[20221213 23:51:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.22
[32m[20221213 23:51:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.13
[32m[20221213 23:51:05 @agent_ppo2.py:143][0m Total time:      38.56 min
[32m[20221213 23:51:05 @agent_ppo2.py:145][0m 3737600 total steps have happened
[32m[20221213 23:51:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3825 --------------------------#
[32m[20221213 23:51:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |           0.0088 |          53.3247 |          12.1536 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0028 |          48.6397 |          12.1352 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0073 |          46.9118 |          12.1457 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0087 |          45.8819 |          12.1041 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0098 |          45.0797 |          12.1094 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0120 |          44.4103 |          12.1093 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0085 |          44.4761 |          12.0979 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0138 |          43.7749 |          12.0932 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |           0.0011 |          48.5117 |          12.0921 |
[32m[20221213 23:51:06 @agent_ppo2.py:185][0m |          -0.0148 |          42.8505 |          12.0458 |
[32m[20221213 23:51:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 23:51:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.16
[32m[20221213 23:51:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.57
[32m[20221213 23:51:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.13
[32m[20221213 23:51:07 @agent_ppo2.py:143][0m Total time:      38.58 min
[32m[20221213 23:51:07 @agent_ppo2.py:145][0m 3739648 total steps have happened
[32m[20221213 23:51:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3826 --------------------------#
[32m[20221213 23:51:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:07 @agent_ppo2.py:185][0m |           0.0009 |          64.1543 |          11.8140 |
[32m[20221213 23:51:07 @agent_ppo2.py:185][0m |          -0.0039 |          57.5799 |          11.7963 |
[32m[20221213 23:51:07 @agent_ppo2.py:185][0m |          -0.0062 |          56.0500 |          11.7933 |
[32m[20221213 23:51:07 @agent_ppo2.py:185][0m |          -0.0114 |          55.0654 |          11.7942 |
[32m[20221213 23:51:07 @agent_ppo2.py:185][0m |          -0.0068 |          54.0974 |          11.7977 |
[32m[20221213 23:51:07 @agent_ppo2.py:185][0m |          -0.0070 |          53.6657 |          11.7585 |
[32m[20221213 23:51:07 @agent_ppo2.py:185][0m |          -0.0088 |          53.1653 |          11.7784 |
[32m[20221213 23:51:08 @agent_ppo2.py:185][0m |          -0.0110 |          52.6834 |          11.7342 |
[32m[20221213 23:51:08 @agent_ppo2.py:185][0m |          -0.0008 |          58.7492 |          11.7296 |
[32m[20221213 23:51:08 @agent_ppo2.py:185][0m |          -0.0124 |          52.1788 |          11.7426 |
[32m[20221213 23:51:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:51:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.61
[32m[20221213 23:51:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.57
[32m[20221213 23:51:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.91
[32m[20221213 23:51:08 @agent_ppo2.py:143][0m Total time:      38.60 min
[32m[20221213 23:51:08 @agent_ppo2.py:145][0m 3741696 total steps have happened
[32m[20221213 23:51:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3827 --------------------------#
[32m[20221213 23:51:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:08 @agent_ppo2.py:185][0m |          -0.0037 |          48.9452 |          11.7841 |
[32m[20221213 23:51:08 @agent_ppo2.py:185][0m |           0.0005 |          44.5296 |          11.7481 |
[32m[20221213 23:51:08 @agent_ppo2.py:185][0m |          -0.0086 |          40.3761 |          11.7912 |
[32m[20221213 23:51:08 @agent_ppo2.py:185][0m |          -0.0178 |          39.1940 |          11.8209 |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0152 |          38.5822 |          11.8222 |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0080 |          38.4245 |          11.8596 |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0197 |          37.6132 |          11.8525 |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0186 |          37.4130 |          11.8695 |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0214 |          36.9743 |          11.8430 |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0224 |          36.7487 |          11.8508 |
[32m[20221213 23:51:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.19
[32m[20221213 23:51:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.66
[32m[20221213 23:51:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.99
[32m[20221213 23:51:09 @agent_ppo2.py:143][0m Total time:      38.62 min
[32m[20221213 23:51:09 @agent_ppo2.py:145][0m 3743744 total steps have happened
[32m[20221213 23:51:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3828 --------------------------#
[32m[20221213 23:51:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0005 |          59.2533 |          12.1792 |
[32m[20221213 23:51:09 @agent_ppo2.py:185][0m |          -0.0062 |          54.2169 |          12.1447 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0139 |          53.1311 |          12.1521 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0143 |          52.5759 |          12.0394 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0151 |          52.1144 |          12.0967 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0187 |          51.7169 |          12.0638 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0138 |          51.3594 |          12.0158 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0094 |          53.5492 |          12.0243 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0153 |          51.0803 |          12.0541 |
[32m[20221213 23:51:10 @agent_ppo2.py:185][0m |          -0.0161 |          50.8175 |          11.9933 |
[32m[20221213 23:51:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:51:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.59
[32m[20221213 23:51:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.32
[32m[20221213 23:51:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.51
[32m[20221213 23:51:10 @agent_ppo2.py:143][0m Total time:      38.64 min
[32m[20221213 23:51:10 @agent_ppo2.py:145][0m 3745792 total steps have happened
[32m[20221213 23:51:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3829 --------------------------#
[32m[20221213 23:51:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |           0.0018 |          51.8923 |          11.8596 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0106 |          44.9805 |          11.8530 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0104 |          42.6284 |          11.8611 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0086 |          41.5286 |          11.8602 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0008 |          44.0066 |          11.8366 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |           0.0029 |          46.6952 |          11.8313 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0047 |          42.1287 |          11.8153 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0138 |          38.5518 |          11.8197 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0139 |          37.9685 |          11.8071 |
[32m[20221213 23:51:11 @agent_ppo2.py:185][0m |          -0.0156 |          37.8283 |          11.7990 |
[32m[20221213 23:51:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.09
[32m[20221213 23:51:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.66
[32m[20221213 23:51:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.25
[32m[20221213 23:51:12 @agent_ppo2.py:143][0m Total time:      38.66 min
[32m[20221213 23:51:12 @agent_ppo2.py:145][0m 3747840 total steps have happened
[32m[20221213 23:51:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3830 --------------------------#
[32m[20221213 23:51:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:51:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0021 |          63.3763 |          12.0685 |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0081 |          58.5969 |          12.1078 |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0099 |          55.9707 |          12.1250 |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0103 |          53.9733 |          12.1303 |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0117 |          53.2823 |          12.1294 |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0177 |          52.7206 |          12.1407 |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0138 |          51.5177 |          12.1316 |
[32m[20221213 23:51:12 @agent_ppo2.py:185][0m |          -0.0191 |          51.2453 |          12.1293 |
[32m[20221213 23:51:13 @agent_ppo2.py:185][0m |          -0.0155 |          50.7737 |          12.1702 |
[32m[20221213 23:51:13 @agent_ppo2.py:185][0m |          -0.0071 |          51.7440 |          12.1779 |
[32m[20221213 23:51:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.43
[32m[20221213 23:51:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.90
[32m[20221213 23:51:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.10
[32m[20221213 23:51:13 @agent_ppo2.py:143][0m Total time:      38.68 min
[32m[20221213 23:51:13 @agent_ppo2.py:145][0m 3749888 total steps have happened
[32m[20221213 23:51:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3831 --------------------------#
[32m[20221213 23:51:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:13 @agent_ppo2.py:185][0m |           0.0018 |          42.9434 |          11.7978 |
[32m[20221213 23:51:13 @agent_ppo2.py:185][0m |           0.0105 |          40.4275 |          11.7607 |
[32m[20221213 23:51:13 @agent_ppo2.py:185][0m |          -0.0061 |          33.4465 |          11.7162 |
[32m[20221213 23:51:13 @agent_ppo2.py:185][0m |          -0.0055 |          32.2076 |          11.7021 |
[32m[20221213 23:51:13 @agent_ppo2.py:185][0m |          -0.0069 |          31.4947 |          11.6871 |
[32m[20221213 23:51:14 @agent_ppo2.py:185][0m |          -0.0042 |          30.6383 |          11.6207 |
[32m[20221213 23:51:14 @agent_ppo2.py:185][0m |          -0.0064 |          30.2586 |          11.6680 |
[32m[20221213 23:51:14 @agent_ppo2.py:185][0m |          -0.0109 |          31.4154 |          11.6357 |
[32m[20221213 23:51:14 @agent_ppo2.py:185][0m |          -0.0148 |          31.0332 |          11.6486 |
[32m[20221213 23:51:14 @agent_ppo2.py:185][0m |          -0.0112 |          30.0116 |          11.6074 |
[32m[20221213 23:51:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:51:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.42
[32m[20221213 23:51:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.10
[32m[20221213 23:51:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.02
[32m[20221213 23:51:14 @agent_ppo2.py:143][0m Total time:      38.70 min
[32m[20221213 23:51:14 @agent_ppo2.py:145][0m 3751936 total steps have happened
[32m[20221213 23:51:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3832 --------------------------#
[32m[20221213 23:51:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:14 @agent_ppo2.py:185][0m |           0.0326 |          49.7082 |          11.6010 |
[32m[20221213 23:51:14 @agent_ppo2.py:185][0m |          -0.0079 |          38.4894 |          11.6068 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |          -0.0102 |          37.3105 |          11.6328 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |          -0.0087 |          36.2637 |          11.6303 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |           0.0044 |          37.5255 |          11.6523 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |          -0.0135 |          35.0313 |          11.6616 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |          -0.0054 |          35.4591 |          11.6843 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |          -0.0146 |          34.2799 |          11.6817 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |          -0.0145 |          33.9383 |          11.6806 |
[32m[20221213 23:51:15 @agent_ppo2.py:185][0m |          -0.0128 |          33.7341 |          11.6892 |
[32m[20221213 23:51:15 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.37
[32m[20221213 23:51:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.65
[32m[20221213 23:51:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.17
[32m[20221213 23:51:15 @agent_ppo2.py:143][0m Total time:      38.72 min
[32m[20221213 23:51:15 @agent_ppo2.py:145][0m 3753984 total steps have happened
[32m[20221213 23:51:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3833 --------------------------#
[32m[20221213 23:51:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0035 |          68.4997 |          11.8120 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |           0.0026 |          65.5066 |          11.8386 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |           0.0001 |          65.2591 |          11.8640 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0104 |          60.8142 |          11.8781 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0007 |          67.2518 |          11.9149 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0113 |          59.5013 |          11.9053 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0130 |          58.8709 |          11.9355 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0149 |          58.3270 |          11.9547 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0138 |          58.0669 |          11.9360 |
[32m[20221213 23:51:16 @agent_ppo2.py:185][0m |          -0.0092 |          58.5726 |          11.9682 |
[32m[20221213 23:51:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.67
[32m[20221213 23:51:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.17
[32m[20221213 23:51:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.08
[32m[20221213 23:51:17 @agent_ppo2.py:143][0m Total time:      38.74 min
[32m[20221213 23:51:17 @agent_ppo2.py:145][0m 3756032 total steps have happened
[32m[20221213 23:51:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3834 --------------------------#
[32m[20221213 23:51:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |           0.0015 |          52.9747 |          12.0057 |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |          -0.0033 |          43.6375 |          12.0077 |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |          -0.0038 |          42.6009 |          12.0126 |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |          -0.0017 |          41.1307 |          11.9634 |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |          -0.0073 |          40.9992 |          11.9440 |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |          -0.0088 |          39.9334 |          11.9036 |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |          -0.0081 |          42.0319 |          11.9268 |
[32m[20221213 23:51:17 @agent_ppo2.py:185][0m |          -0.0099 |          39.2006 |          11.8695 |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0132 |          39.0464 |          11.8612 |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0139 |          39.6368 |          11.8686 |
[32m[20221213 23:51:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.69
[32m[20221213 23:51:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.82
[32m[20221213 23:51:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.55
[32m[20221213 23:51:18 @agent_ppo2.py:143][0m Total time:      38.76 min
[32m[20221213 23:51:18 @agent_ppo2.py:145][0m 3758080 total steps have happened
[32m[20221213 23:51:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3835 --------------------------#
[32m[20221213 23:51:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0029 |          63.9088 |          11.5356 |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0091 |          58.0759 |          11.5493 |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0090 |          56.1962 |          11.5870 |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0071 |          55.8041 |          11.6147 |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0092 |          54.6364 |          11.6155 |
[32m[20221213 23:51:18 @agent_ppo2.py:185][0m |          -0.0124 |          53.9987 |          11.6487 |
[32m[20221213 23:51:19 @agent_ppo2.py:185][0m |          -0.0069 |          53.9186 |          11.6624 |
[32m[20221213 23:51:19 @agent_ppo2.py:185][0m |          -0.0144 |          53.0516 |          11.6687 |
[32m[20221213 23:51:19 @agent_ppo2.py:185][0m |          -0.0061 |          58.9779 |          11.7134 |
[32m[20221213 23:51:19 @agent_ppo2.py:185][0m |          -0.0126 |          53.2786 |          11.7261 |
[32m[20221213 23:51:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.26
[32m[20221213 23:51:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.48
[32m[20221213 23:51:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.50
[32m[20221213 23:51:19 @agent_ppo2.py:143][0m Total time:      38.78 min
[32m[20221213 23:51:19 @agent_ppo2.py:145][0m 3760128 total steps have happened
[32m[20221213 23:51:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3836 --------------------------#
[32m[20221213 23:51:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:19 @agent_ppo2.py:185][0m |           0.0024 |          66.5717 |          11.7141 |
[32m[20221213 23:51:19 @agent_ppo2.py:185][0m |          -0.0045 |          64.2229 |          11.7501 |
[32m[20221213 23:51:19 @agent_ppo2.py:185][0m |          -0.0077 |          63.2627 |          11.7230 |
[32m[20221213 23:51:20 @agent_ppo2.py:185][0m |          -0.0079 |          62.6163 |          11.7386 |
[32m[20221213 23:51:20 @agent_ppo2.py:185][0m |          -0.0057 |          62.8466 |          11.7303 |
[32m[20221213 23:51:20 @agent_ppo2.py:185][0m |          -0.0064 |          62.3479 |          11.7580 |
[32m[20221213 23:51:20 @agent_ppo2.py:185][0m |          -0.0084 |          61.7088 |          11.7485 |
[32m[20221213 23:51:20 @agent_ppo2.py:185][0m |          -0.0095 |          61.6252 |          11.8199 |
[32m[20221213 23:51:20 @agent_ppo2.py:185][0m |          -0.0131 |          61.4880 |          11.8184 |
[32m[20221213 23:51:20 @agent_ppo2.py:185][0m |          -0.0116 |          61.2147 |          11.8032 |
[32m[20221213 23:51:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:51:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.68
[32m[20221213 23:51:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.83
[32m[20221213 23:51:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.86
[32m[20221213 23:51:20 @agent_ppo2.py:143][0m Total time:      38.80 min
[32m[20221213 23:51:20 @agent_ppo2.py:145][0m 3762176 total steps have happened
[32m[20221213 23:51:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3837 --------------------------#
[32m[20221213 23:51:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |           0.0007 |          78.9976 |          11.8202 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0059 |          76.2968 |          11.8094 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0030 |          75.1160 |          11.8055 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0067 |          73.6773 |          11.8322 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0086 |          72.8793 |          11.8673 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0094 |          72.3034 |          11.8434 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0102 |          71.3387 |          11.8794 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0092 |          70.8079 |          11.8459 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0022 |          75.2350 |          11.8911 |
[32m[20221213 23:51:21 @agent_ppo2.py:185][0m |          -0.0040 |          76.4531 |          11.8484 |
[32m[20221213 23:51:21 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.73
[32m[20221213 23:51:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.22
[32m[20221213 23:51:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.47
[32m[20221213 23:51:21 @agent_ppo2.py:143][0m Total time:      38.82 min
[32m[20221213 23:51:21 @agent_ppo2.py:145][0m 3764224 total steps have happened
[32m[20221213 23:51:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3838 --------------------------#
[32m[20221213 23:51:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |           0.0086 |          51.2711 |          12.2266 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |          -0.0013 |          43.7686 |          12.2572 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |           0.0037 |          43.7062 |          12.2401 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |          -0.0118 |          40.7967 |          12.1792 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |          -0.0151 |          40.3278 |          12.1437 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |          -0.0133 |          39.7879 |          12.1407 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |          -0.0128 |          39.1309 |          12.1460 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |          -0.0097 |          38.8676 |          12.1088 |
[32m[20221213 23:51:22 @agent_ppo2.py:185][0m |          -0.0188 |          38.3776 |          12.1093 |
[32m[20221213 23:51:23 @agent_ppo2.py:185][0m |          -0.0169 |          38.2895 |          12.0923 |
[32m[20221213 23:51:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.94
[32m[20221213 23:51:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.39
[32m[20221213 23:51:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.12
[32m[20221213 23:51:23 @agent_ppo2.py:143][0m Total time:      38.84 min
[32m[20221213 23:51:23 @agent_ppo2.py:145][0m 3766272 total steps have happened
[32m[20221213 23:51:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3839 --------------------------#
[32m[20221213 23:51:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:23 @agent_ppo2.py:185][0m |           0.0034 |          33.7746 |          11.6640 |
[32m[20221213 23:51:23 @agent_ppo2.py:185][0m |          -0.0046 |          30.3962 |          11.6445 |
[32m[20221213 23:51:23 @agent_ppo2.py:185][0m |          -0.0113 |          29.6137 |          11.6201 |
[32m[20221213 23:51:23 @agent_ppo2.py:185][0m |          -0.0088 |          29.3422 |          11.6192 |
[32m[20221213 23:51:23 @agent_ppo2.py:185][0m |          -0.0070 |          28.7033 |          11.6092 |
[32m[20221213 23:51:23 @agent_ppo2.py:185][0m |          -0.0139 |          28.4963 |          11.6263 |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |          -0.0114 |          28.1091 |          11.5968 |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |           0.0006 |          29.6860 |          11.5984 |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |          -0.0130 |          27.6708 |          11.6163 |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |          -0.0147 |          27.4914 |          11.6360 |
[32m[20221213 23:51:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.96
[32m[20221213 23:51:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.36
[32m[20221213 23:51:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.20
[32m[20221213 23:51:24 @agent_ppo2.py:143][0m Total time:      38.86 min
[32m[20221213 23:51:24 @agent_ppo2.py:145][0m 3768320 total steps have happened
[32m[20221213 23:51:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3840 --------------------------#
[32m[20221213 23:51:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:51:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |          -0.0038 |          56.0916 |          11.6221 |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |          -0.0035 |          52.0750 |          11.5964 |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |           0.0104 |          54.0244 |          11.6158 |
[32m[20221213 23:51:24 @agent_ppo2.py:185][0m |          -0.0096 |          50.5963 |          11.5964 |
[32m[20221213 23:51:25 @agent_ppo2.py:185][0m |          -0.0142 |          49.8624 |          11.6184 |
[32m[20221213 23:51:25 @agent_ppo2.py:185][0m |          -0.0117 |          49.5625 |          11.6424 |
[32m[20221213 23:51:25 @agent_ppo2.py:185][0m |          -0.0132 |          49.2961 |          11.6422 |
[32m[20221213 23:51:25 @agent_ppo2.py:185][0m |           0.0002 |          55.5521 |          11.6830 |
[32m[20221213 23:51:25 @agent_ppo2.py:185][0m |          -0.0185 |          48.8527 |          11.6545 |
[32m[20221213 23:51:25 @agent_ppo2.py:185][0m |          -0.0070 |          51.9014 |          11.6647 |
[32m[20221213 23:51:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 23:51:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.56
[32m[20221213 23:51:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.41
[32m[20221213 23:51:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.85
[32m[20221213 23:51:25 @agent_ppo2.py:143][0m Total time:      38.89 min
[32m[20221213 23:51:25 @agent_ppo2.py:145][0m 3770368 total steps have happened
[32m[20221213 23:51:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3841 --------------------------#
[32m[20221213 23:51:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:25 @agent_ppo2.py:185][0m |           0.0020 |          52.7233 |          11.7329 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0095 |          50.2542 |          11.7589 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0098 |          49.3869 |          11.7747 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0109 |          48.7753 |          11.7535 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0111 |          48.3379 |          11.7547 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0123 |          48.1128 |          11.7508 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0098 |          47.7648 |          11.7071 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0113 |          47.7504 |          11.7092 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0122 |          47.3620 |          11.7454 |
[32m[20221213 23:51:26 @agent_ppo2.py:185][0m |          -0.0138 |          47.4124 |          11.7351 |
[32m[20221213 23:51:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:51:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.73
[32m[20221213 23:51:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.53
[32m[20221213 23:51:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.41
[32m[20221213 23:51:26 @agent_ppo2.py:143][0m Total time:      38.91 min
[32m[20221213 23:51:26 @agent_ppo2.py:145][0m 3772416 total steps have happened
[32m[20221213 23:51:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3842 --------------------------#
[32m[20221213 23:51:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0012 |          46.7625 |          12.0115 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0110 |          39.9438 |          12.0265 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0112 |          37.7911 |          12.0186 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0073 |          36.9422 |          12.0096 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0176 |          36.0301 |          11.9894 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0180 |          35.4399 |          11.9622 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0119 |          34.7889 |          11.9712 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0203 |          34.1733 |          11.9356 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0225 |          33.5875 |          11.9822 |
[32m[20221213 23:51:27 @agent_ppo2.py:185][0m |          -0.0195 |          33.2752 |          11.9502 |
[32m[20221213 23:51:27 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.70
[32m[20221213 23:51:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.12
[32m[20221213 23:51:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.03
[32m[20221213 23:51:28 @agent_ppo2.py:143][0m Total time:      38.93 min
[32m[20221213 23:51:28 @agent_ppo2.py:145][0m 3774464 total steps have happened
[32m[20221213 23:51:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3843 --------------------------#
[32m[20221213 23:51:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:28 @agent_ppo2.py:185][0m |           0.0031 |          53.4671 |          11.9225 |
[32m[20221213 23:51:28 @agent_ppo2.py:185][0m |          -0.0065 |          49.3200 |          11.9355 |
[32m[20221213 23:51:28 @agent_ppo2.py:185][0m |          -0.0061 |          48.0156 |          11.9928 |
[32m[20221213 23:51:28 @agent_ppo2.py:185][0m |          -0.0081 |          47.2936 |          11.9302 |
[32m[20221213 23:51:28 @agent_ppo2.py:185][0m |          -0.0129 |          46.4954 |          11.9536 |
[32m[20221213 23:51:28 @agent_ppo2.py:185][0m |          -0.0081 |          46.2426 |          11.9404 |
[32m[20221213 23:51:28 @agent_ppo2.py:185][0m |          -0.0106 |          47.0707 |          11.9588 |
[32m[20221213 23:51:29 @agent_ppo2.py:185][0m |          -0.0123 |          46.0634 |          11.9431 |
[32m[20221213 23:51:29 @agent_ppo2.py:185][0m |          -0.0073 |          46.6072 |          11.9515 |
[32m[20221213 23:51:29 @agent_ppo2.py:185][0m |          -0.0177 |          45.0456 |          11.9303 |
[32m[20221213 23:51:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.59
[32m[20221213 23:51:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.53
[32m[20221213 23:51:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.35
[32m[20221213 23:51:29 @agent_ppo2.py:143][0m Total time:      38.95 min
[32m[20221213 23:51:29 @agent_ppo2.py:145][0m 3776512 total steps have happened
[32m[20221213 23:51:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3844 --------------------------#
[32m[20221213 23:51:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:29 @agent_ppo2.py:185][0m |           0.0006 |          68.4008 |          11.4825 |
[32m[20221213 23:51:29 @agent_ppo2.py:185][0m |          -0.0056 |          66.0431 |          11.5209 |
[32m[20221213 23:51:29 @agent_ppo2.py:185][0m |          -0.0063 |          65.4262 |          11.5246 |
[32m[20221213 23:51:29 @agent_ppo2.py:185][0m |          -0.0066 |          64.9068 |          11.5541 |
[32m[20221213 23:51:30 @agent_ppo2.py:185][0m |          -0.0109 |          64.0752 |          11.4916 |
[32m[20221213 23:51:30 @agent_ppo2.py:185][0m |          -0.0100 |          63.8297 |          11.5596 |
[32m[20221213 23:51:30 @agent_ppo2.py:185][0m |          -0.0075 |          63.8779 |          11.5758 |
[32m[20221213 23:51:30 @agent_ppo2.py:185][0m |          -0.0124 |          63.2557 |          11.5502 |
[32m[20221213 23:51:30 @agent_ppo2.py:185][0m |          -0.0084 |          63.4467 |          11.5884 |
[32m[20221213 23:51:30 @agent_ppo2.py:185][0m |          -0.0113 |          63.0003 |          11.5623 |
[32m[20221213 23:51:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.40
[32m[20221213 23:51:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.33
[32m[20221213 23:51:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.46
[32m[20221213 23:51:30 @agent_ppo2.py:143][0m Total time:      38.97 min
[32m[20221213 23:51:30 @agent_ppo2.py:145][0m 3778560 total steps have happened
[32m[20221213 23:51:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3845 --------------------------#
[32m[20221213 23:51:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:30 @agent_ppo2.py:185][0m |          -0.0009 |          66.9530 |          11.7125 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0085 |          60.3553 |          11.6978 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0041 |          57.3280 |          11.6824 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0102 |          53.5918 |          11.7098 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0078 |          51.9443 |          11.6762 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0095 |          51.1286 |          11.6750 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0149 |          50.3885 |          11.6430 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0177 |          49.9623 |          11.6333 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0189 |          49.7087 |          11.5666 |
[32m[20221213 23:51:31 @agent_ppo2.py:185][0m |          -0.0166 |          49.6884 |          11.5862 |
[32m[20221213 23:51:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.02
[32m[20221213 23:51:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.46
[32m[20221213 23:51:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.03
[32m[20221213 23:51:31 @agent_ppo2.py:143][0m Total time:      38.99 min
[32m[20221213 23:51:31 @agent_ppo2.py:145][0m 3780608 total steps have happened
[32m[20221213 23:51:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3846 --------------------------#
[32m[20221213 23:51:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |           0.0016 |          74.0645 |          11.6565 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |           0.0037 |          73.5733 |          11.6277 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |          -0.0067 |          67.8645 |          11.6102 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |          -0.0075 |          66.7732 |          11.6283 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |          -0.0090 |          65.9690 |          11.6554 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |           0.0010 |          71.6969 |          11.6091 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |          -0.0126 |          65.3172 |          11.5855 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |          -0.0005 |          73.4197 |          11.6216 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |          -0.0092 |          65.4052 |          11.5859 |
[32m[20221213 23:51:32 @agent_ppo2.py:185][0m |          -0.0102 |          64.2945 |          11.6062 |
[32m[20221213 23:51:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:51:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.63
[32m[20221213 23:51:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.90
[32m[20221213 23:51:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.87
[32m[20221213 23:51:33 @agent_ppo2.py:143][0m Total time:      39.01 min
[32m[20221213 23:51:33 @agent_ppo2.py:145][0m 3782656 total steps have happened
[32m[20221213 23:51:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3847 --------------------------#
[32m[20221213 23:51:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:33 @agent_ppo2.py:185][0m |           0.0039 |          64.1441 |          11.3623 |
[32m[20221213 23:51:33 @agent_ppo2.py:185][0m |          -0.0024 |          58.3376 |          11.3990 |
[32m[20221213 23:51:33 @agent_ppo2.py:185][0m |          -0.0074 |          56.5645 |          11.3627 |
[32m[20221213 23:51:33 @agent_ppo2.py:185][0m |          -0.0074 |          55.4890 |          11.3514 |
[32m[20221213 23:51:33 @agent_ppo2.py:185][0m |          -0.0067 |          55.3770 |          11.3446 |
[32m[20221213 23:51:33 @agent_ppo2.py:185][0m |          -0.0086 |          53.9961 |          11.3488 |
[32m[20221213 23:51:33 @agent_ppo2.py:185][0m |          -0.0126 |          53.3218 |          11.3103 |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |          -0.0101 |          52.7883 |          11.3007 |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |          -0.0157 |          52.3553 |          11.3102 |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |          -0.0115 |          51.9401 |          11.2950 |
[32m[20221213 23:51:34 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.66
[32m[20221213 23:51:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.94
[32m[20221213 23:51:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.55
[32m[20221213 23:51:34 @agent_ppo2.py:143][0m Total time:      39.03 min
[32m[20221213 23:51:34 @agent_ppo2.py:145][0m 3784704 total steps have happened
[32m[20221213 23:51:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3848 --------------------------#
[32m[20221213 23:51:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |           0.0021 |          52.3036 |          11.9028 |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |          -0.0035 |          45.9091 |          11.8450 |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |          -0.0078 |          44.0738 |          11.8051 |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |          -0.0108 |          42.9864 |          11.7899 |
[32m[20221213 23:51:34 @agent_ppo2.py:185][0m |          -0.0125 |          42.4241 |          11.7861 |
[32m[20221213 23:51:35 @agent_ppo2.py:185][0m |          -0.0089 |          42.0191 |          11.7976 |
[32m[20221213 23:51:35 @agent_ppo2.py:185][0m |          -0.0154 |          41.5398 |          11.7820 |
[32m[20221213 23:51:35 @agent_ppo2.py:185][0m |          -0.0162 |          41.2663 |          11.7744 |
[32m[20221213 23:51:35 @agent_ppo2.py:185][0m |          -0.0082 |          41.9730 |          11.7748 |
[32m[20221213 23:51:35 @agent_ppo2.py:185][0m |          -0.0173 |          40.9594 |          11.7644 |
[32m[20221213 23:51:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.10
[32m[20221213 23:51:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.91
[32m[20221213 23:51:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.69
[32m[20221213 23:51:35 @agent_ppo2.py:143][0m Total time:      39.05 min
[32m[20221213 23:51:35 @agent_ppo2.py:145][0m 3786752 total steps have happened
[32m[20221213 23:51:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3849 --------------------------#
[32m[20221213 23:51:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:35 @agent_ppo2.py:185][0m |          -0.0012 |          50.2156 |          11.5094 |
[32m[20221213 23:51:35 @agent_ppo2.py:185][0m |          -0.0083 |          45.7042 |          11.5262 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0097 |          44.5501 |          11.5283 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0096 |          43.6569 |          11.5979 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0093 |          42.8358 |          11.6254 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0140 |          41.9797 |          11.6201 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0166 |          41.5985 |          11.6350 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0154 |          41.0338 |          11.6624 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0138 |          40.6360 |          11.6866 |
[32m[20221213 23:51:36 @agent_ppo2.py:185][0m |          -0.0176 |          40.2665 |          11.7531 |
[32m[20221213 23:51:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.26
[32m[20221213 23:51:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.15
[32m[20221213 23:51:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.32
[32m[20221213 23:51:36 @agent_ppo2.py:143][0m Total time:      39.07 min
[32m[20221213 23:51:36 @agent_ppo2.py:145][0m 3788800 total steps have happened
[32m[20221213 23:51:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3850 --------------------------#
[32m[20221213 23:51:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:51:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |           0.0010 |          56.3378 |          11.4821 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0086 |          52.8347 |          11.4617 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0072 |          51.4061 |          11.4338 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0114 |          50.3894 |          11.4737 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0011 |          54.6740 |          11.4723 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0008 |          51.2859 |          11.4679 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0114 |          48.3504 |          11.4644 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0181 |          47.6443 |          11.4489 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0097 |          49.5433 |          11.4101 |
[32m[20221213 23:51:37 @agent_ppo2.py:185][0m |          -0.0169 |          47.0727 |          11.4061 |
[32m[20221213 23:51:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.58
[32m[20221213 23:51:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.43
[32m[20221213 23:51:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.80
[32m[20221213 23:51:38 @agent_ppo2.py:143][0m Total time:      39.09 min
[32m[20221213 23:51:38 @agent_ppo2.py:145][0m 3790848 total steps have happened
[32m[20221213 23:51:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3851 --------------------------#
[32m[20221213 23:51:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |           0.0150 |          50.6873 |          11.7019 |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |          -0.0018 |          42.0072 |          11.7377 |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |          -0.0052 |          40.5334 |          11.7234 |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |          -0.0037 |          40.2143 |          11.7268 |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |          -0.0132 |          38.7204 |          11.7265 |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |          -0.0030 |          40.5488 |          11.7419 |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |          -0.0135 |          37.4618 |          11.7647 |
[32m[20221213 23:51:38 @agent_ppo2.py:185][0m |          -0.0118 |          36.8575 |          11.7544 |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |          -0.0172 |          36.2026 |          11.7526 |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |          -0.0164 |          35.6685 |          11.7175 |
[32m[20221213 23:51:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:51:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.87
[32m[20221213 23:51:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.03
[32m[20221213 23:51:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.64
[32m[20221213 23:51:39 @agent_ppo2.py:143][0m Total time:      39.11 min
[32m[20221213 23:51:39 @agent_ppo2.py:145][0m 3792896 total steps have happened
[32m[20221213 23:51:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3852 --------------------------#
[32m[20221213 23:51:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |           0.0033 |          54.5638 |          11.5122 |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |          -0.0082 |          49.9579 |          11.5017 |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |          -0.0030 |          49.0927 |          11.5041 |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |          -0.0112 |          47.8946 |          11.4726 |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |          -0.0141 |          47.1713 |          11.4691 |
[32m[20221213 23:51:39 @agent_ppo2.py:185][0m |          -0.0119 |          46.4809 |          11.4387 |
[32m[20221213 23:51:40 @agent_ppo2.py:185][0m |          -0.0135 |          45.9765 |          11.4640 |
[32m[20221213 23:51:40 @agent_ppo2.py:185][0m |          -0.0169 |          45.7723 |          11.4651 |
[32m[20221213 23:51:40 @agent_ppo2.py:185][0m |          -0.0165 |          45.0848 |          11.4286 |
[32m[20221213 23:51:40 @agent_ppo2.py:185][0m |          -0.0162 |          44.8075 |          11.4490 |
[32m[20221213 23:51:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:51:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.11
[32m[20221213 23:51:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.73
[32m[20221213 23:51:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.40
[32m[20221213 23:51:40 @agent_ppo2.py:143][0m Total time:      39.13 min
[32m[20221213 23:51:40 @agent_ppo2.py:145][0m 3794944 total steps have happened
[32m[20221213 23:51:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3853 --------------------------#
[32m[20221213 23:51:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:40 @agent_ppo2.py:185][0m |          -0.0020 |          57.0349 |          11.3989 |
[32m[20221213 23:51:40 @agent_ppo2.py:185][0m |          -0.0060 |          52.8776 |          11.4177 |
[32m[20221213 23:51:40 @agent_ppo2.py:185][0m |          -0.0101 |          51.4878 |          11.3961 |
[32m[20221213 23:51:41 @agent_ppo2.py:185][0m |          -0.0049 |          55.2462 |          11.4464 |
[32m[20221213 23:51:41 @agent_ppo2.py:185][0m |          -0.0057 |          51.7029 |          11.4166 |
[32m[20221213 23:51:41 @agent_ppo2.py:185][0m |          -0.0159 |          48.9808 |          11.4358 |
[32m[20221213 23:51:41 @agent_ppo2.py:185][0m |          -0.0035 |          54.1762 |          11.3735 |
[32m[20221213 23:51:41 @agent_ppo2.py:185][0m |          -0.0146 |          48.2084 |          11.4011 |
[32m[20221213 23:51:41 @agent_ppo2.py:185][0m |          -0.0162 |          47.5724 |          11.3932 |
[32m[20221213 23:51:41 @agent_ppo2.py:185][0m |          -0.0064 |          55.5643 |          11.3926 |
[32m[20221213 23:51:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.74
[32m[20221213 23:51:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.57
[32m[20221213 23:51:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.93
[32m[20221213 23:51:41 @agent_ppo2.py:143][0m Total time:      39.15 min
[32m[20221213 23:51:41 @agent_ppo2.py:145][0m 3796992 total steps have happened
[32m[20221213 23:51:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3854 --------------------------#
[32m[20221213 23:51:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |           0.0021 |          57.1975 |          11.6269 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0072 |          52.7644 |          11.6103 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0102 |          51.4263 |          11.5836 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0108 |          50.5270 |          11.5845 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0108 |          49.8385 |          11.5953 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0124 |          49.4909 |          11.6042 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0123 |          49.1234 |          11.5521 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0158 |          48.6826 |          11.5809 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0159 |          48.5624 |          11.5945 |
[32m[20221213 23:51:42 @agent_ppo2.py:185][0m |          -0.0161 |          48.2985 |          11.5881 |
[32m[20221213 23:51:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:51:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.80
[32m[20221213 23:51:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.43
[32m[20221213 23:51:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.86
[32m[20221213 23:51:42 @agent_ppo2.py:143][0m Total time:      39.17 min
[32m[20221213 23:51:42 @agent_ppo2.py:145][0m 3799040 total steps have happened
[32m[20221213 23:51:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3855 --------------------------#
[32m[20221213 23:51:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |           0.0021 |          54.3711 |          11.0935 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |           0.0041 |          53.0095 |          11.0335 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |          -0.0083 |          47.5264 |          11.0610 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |          -0.0082 |          46.6478 |          11.0003 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |          -0.0107 |          45.5278 |          10.9949 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |          -0.0132 |          44.6114 |          10.9538 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |          -0.0147 |          44.1583 |          10.9545 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |          -0.0160 |          43.8321 |          10.9384 |
[32m[20221213 23:51:43 @agent_ppo2.py:185][0m |          -0.0119 |          43.7015 |          10.9238 |
[32m[20221213 23:51:44 @agent_ppo2.py:185][0m |          -0.0161 |          43.5057 |          10.9459 |
[32m[20221213 23:51:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:51:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.69
[32m[20221213 23:51:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.82
[32m[20221213 23:51:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.54
[32m[20221213 23:51:44 @agent_ppo2.py:143][0m Total time:      39.19 min
[32m[20221213 23:51:44 @agent_ppo2.py:145][0m 3801088 total steps have happened
[32m[20221213 23:51:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3856 --------------------------#
[32m[20221213 23:51:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:44 @agent_ppo2.py:185][0m |          -0.0026 |          52.5080 |          11.0041 |
[32m[20221213 23:51:44 @agent_ppo2.py:185][0m |          -0.0015 |          48.2857 |          10.9473 |
[32m[20221213 23:51:44 @agent_ppo2.py:185][0m |          -0.0089 |          46.3188 |          10.9483 |
[32m[20221213 23:51:44 @agent_ppo2.py:185][0m |          -0.0120 |          45.0240 |          10.9063 |
[32m[20221213 23:51:44 @agent_ppo2.py:185][0m |          -0.0108 |          44.2936 |          10.8942 |
[32m[20221213 23:51:44 @agent_ppo2.py:185][0m |          -0.0075 |          43.7818 |          10.8913 |
[32m[20221213 23:51:45 @agent_ppo2.py:185][0m |          -0.0144 |          43.0303 |          10.8700 |
[32m[20221213 23:51:45 @agent_ppo2.py:185][0m |          -0.0138 |          42.0347 |          10.8591 |
[32m[20221213 23:51:45 @agent_ppo2.py:185][0m |          -0.0149 |          41.5015 |          10.8438 |
[32m[20221213 23:51:45 @agent_ppo2.py:185][0m |          -0.0188 |          41.5900 |          10.8203 |
[32m[20221213 23:51:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:51:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.61
[32m[20221213 23:51:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.43
[32m[20221213 23:51:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.50
[32m[20221213 23:51:45 @agent_ppo2.py:143][0m Total time:      39.22 min
[32m[20221213 23:51:45 @agent_ppo2.py:145][0m 3803136 total steps have happened
[32m[20221213 23:51:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3857 --------------------------#
[32m[20221213 23:51:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:45 @agent_ppo2.py:185][0m |           0.0033 |          60.9067 |          10.9976 |
[32m[20221213 23:51:45 @agent_ppo2.py:185][0m |          -0.0031 |          56.2795 |          11.0339 |
[32m[20221213 23:51:45 @agent_ppo2.py:185][0m |          -0.0086 |          54.5944 |          11.0212 |
[32m[20221213 23:51:46 @agent_ppo2.py:185][0m |          -0.0022 |          55.6955 |          11.0883 |
[32m[20221213 23:51:46 @agent_ppo2.py:185][0m |          -0.0129 |          53.5840 |          11.0876 |
[32m[20221213 23:51:46 @agent_ppo2.py:185][0m |          -0.0037 |          53.9641 |          11.0869 |
[32m[20221213 23:51:46 @agent_ppo2.py:185][0m |          -0.0138 |          53.5862 |          11.1492 |
[32m[20221213 23:51:46 @agent_ppo2.py:185][0m |          -0.0122 |          52.6516 |          11.1361 |
[32m[20221213 23:51:46 @agent_ppo2.py:185][0m |          -0.0077 |          57.6136 |          11.1571 |
[32m[20221213 23:51:46 @agent_ppo2.py:185][0m |          -0.0176 |          52.1706 |          11.1438 |
[32m[20221213 23:51:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.25
[32m[20221213 23:51:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.44
[32m[20221213 23:51:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.01
[32m[20221213 23:51:46 @agent_ppo2.py:143][0m Total time:      39.24 min
[32m[20221213 23:51:46 @agent_ppo2.py:145][0m 3805184 total steps have happened
[32m[20221213 23:51:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3858 --------------------------#
[32m[20221213 23:51:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |           0.0011 |          57.8852 |          11.0151 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0083 |          54.4612 |          11.0192 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0059 |          53.5876 |          10.9990 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0114 |          52.5794 |          10.9720 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0144 |          52.3409 |          10.9865 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0166 |          52.0863 |          10.9658 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0148 |          51.7936 |          10.9245 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0189 |          51.5058 |          10.9093 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0176 |          51.6232 |          10.9183 |
[32m[20221213 23:51:47 @agent_ppo2.py:185][0m |          -0.0188 |          51.3406 |          10.8667 |
[32m[20221213 23:51:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.89
[32m[20221213 23:51:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.03
[32m[20221213 23:51:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.00
[32m[20221213 23:51:47 @agent_ppo2.py:143][0m Total time:      39.26 min
[32m[20221213 23:51:47 @agent_ppo2.py:145][0m 3807232 total steps have happened
[32m[20221213 23:51:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3859 --------------------------#
[32m[20221213 23:51:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |           0.0009 |          48.6935 |          10.6788 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0081 |          44.7611 |          10.6964 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0131 |          43.2085 |          10.6884 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0138 |          42.3378 |          10.6855 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0107 |          41.4146 |          10.6877 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0178 |          40.7401 |          10.6809 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0138 |          40.2254 |          10.6699 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0178 |          39.8367 |          10.6652 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0173 |          39.3736 |          10.6698 |
[32m[20221213 23:51:48 @agent_ppo2.py:185][0m |          -0.0197 |          39.0165 |          10.6439 |
[32m[20221213 23:51:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.85
[32m[20221213 23:51:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.75
[32m[20221213 23:51:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.61
[32m[20221213 23:51:49 @agent_ppo2.py:143][0m Total time:      39.28 min
[32m[20221213 23:51:49 @agent_ppo2.py:145][0m 3809280 total steps have happened
[32m[20221213 23:51:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3860 --------------------------#
[32m[20221213 23:51:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:51:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:49 @agent_ppo2.py:185][0m |          -0.0005 |          66.5500 |          10.9767 |
[32m[20221213 23:51:49 @agent_ppo2.py:185][0m |           0.0033 |          63.1763 |          10.9576 |
[32m[20221213 23:51:49 @agent_ppo2.py:185][0m |          -0.0085 |          56.4578 |          11.0029 |
[32m[20221213 23:51:49 @agent_ppo2.py:185][0m |          -0.0132 |          54.3030 |          11.0218 |
[32m[20221213 23:51:49 @agent_ppo2.py:185][0m |          -0.0137 |          52.7985 |          11.0091 |
[32m[20221213 23:51:49 @agent_ppo2.py:185][0m |          -0.0140 |          51.7010 |          11.0682 |
[32m[20221213 23:51:50 @agent_ppo2.py:185][0m |          -0.0173 |          50.7946 |          11.0559 |
[32m[20221213 23:51:50 @agent_ppo2.py:185][0m |          -0.0164 |          50.0968 |          11.0600 |
[32m[20221213 23:51:50 @agent_ppo2.py:185][0m |          -0.0162 |          49.4407 |          11.0454 |
[32m[20221213 23:51:50 @agent_ppo2.py:185][0m |          -0.0182 |          49.0165 |          11.1169 |
[32m[20221213 23:51:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:51:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.02
[32m[20221213 23:51:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.37
[32m[20221213 23:51:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.79
[32m[20221213 23:51:50 @agent_ppo2.py:143][0m Total time:      39.30 min
[32m[20221213 23:51:50 @agent_ppo2.py:145][0m 3811328 total steps have happened
[32m[20221213 23:51:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3861 --------------------------#
[32m[20221213 23:51:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:50 @agent_ppo2.py:185][0m |          -0.0001 |          64.1982 |          10.9947 |
[32m[20221213 23:51:50 @agent_ppo2.py:185][0m |          -0.0045 |          54.8058 |          11.0342 |
[32m[20221213 23:51:50 @agent_ppo2.py:185][0m |          -0.0066 |          52.7372 |          11.0213 |
[32m[20221213 23:51:51 @agent_ppo2.py:185][0m |          -0.0119 |          51.4815 |          11.0518 |
[32m[20221213 23:51:51 @agent_ppo2.py:185][0m |          -0.0125 |          50.4213 |          11.0866 |
[32m[20221213 23:51:51 @agent_ppo2.py:185][0m |          -0.0142 |          49.5341 |          11.0730 |
[32m[20221213 23:51:51 @agent_ppo2.py:185][0m |          -0.0017 |          56.5955 |          11.0858 |
[32m[20221213 23:51:51 @agent_ppo2.py:185][0m |          -0.0074 |          49.5311 |          11.1479 |
[32m[20221213 23:51:51 @agent_ppo2.py:185][0m |          -0.0127 |          48.5458 |          11.1280 |
[32m[20221213 23:51:51 @agent_ppo2.py:185][0m |          -0.0168 |          47.9384 |          11.1445 |
[32m[20221213 23:51:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:51:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.90
[32m[20221213 23:51:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.50
[32m[20221213 23:51:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.18
[32m[20221213 23:51:51 @agent_ppo2.py:143][0m Total time:      39.32 min
[32m[20221213 23:51:51 @agent_ppo2.py:145][0m 3813376 total steps have happened
[32m[20221213 23:51:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3862 --------------------------#
[32m[20221213 23:51:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |           0.0013 |          51.9093 |          10.9629 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |           0.0002 |          48.7328 |          10.9959 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0023 |          47.6210 |          10.9771 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0087 |          46.8967 |          10.9781 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0072 |          46.5130 |          10.9616 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0074 |          46.2316 |          10.9294 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0133 |          45.6185 |          10.9725 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0067 |          45.3404 |          10.9457 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0137 |          45.0654 |          10.9676 |
[32m[20221213 23:51:52 @agent_ppo2.py:185][0m |          -0.0146 |          44.9555 |          10.9436 |
[32m[20221213 23:51:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:51:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.61
[32m[20221213 23:51:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.60
[32m[20221213 23:51:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.79
[32m[20221213 23:51:53 @agent_ppo2.py:143][0m Total time:      39.34 min
[32m[20221213 23:51:53 @agent_ppo2.py:145][0m 3815424 total steps have happened
[32m[20221213 23:51:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3863 --------------------------#
[32m[20221213 23:51:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0029 |          58.3473 |          10.9119 |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0068 |          53.5969 |          10.9653 |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0083 |          51.0313 |          10.9206 |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0069 |          51.8319 |          11.0010 |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0109 |          49.9100 |          11.0193 |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0157 |          48.8721 |          11.0044 |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0131 |          48.3589 |          11.0412 |
[32m[20221213 23:51:53 @agent_ppo2.py:185][0m |          -0.0151 |          48.1618 |          11.0170 |
[32m[20221213 23:51:54 @agent_ppo2.py:185][0m |          -0.0151 |          47.6831 |          11.0531 |
[32m[20221213 23:51:54 @agent_ppo2.py:185][0m |          -0.0167 |          47.2905 |          11.0614 |
[32m[20221213 23:51:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:51:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.40
[32m[20221213 23:51:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.95
[32m[20221213 23:51:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.08
[32m[20221213 23:51:54 @agent_ppo2.py:143][0m Total time:      39.36 min
[32m[20221213 23:51:54 @agent_ppo2.py:145][0m 3817472 total steps have happened
[32m[20221213 23:51:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3864 --------------------------#
[32m[20221213 23:51:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:54 @agent_ppo2.py:185][0m |           0.0033 |          51.5198 |          11.4056 |
[32m[20221213 23:51:54 @agent_ppo2.py:185][0m |           0.0011 |          45.8642 |          11.4556 |
[32m[20221213 23:51:54 @agent_ppo2.py:185][0m |          -0.0068 |          44.4167 |          11.4508 |
[32m[20221213 23:51:54 @agent_ppo2.py:185][0m |          -0.0126 |          43.8206 |          11.4656 |
[32m[20221213 23:51:54 @agent_ppo2.py:185][0m |          -0.0087 |          45.0351 |          11.4560 |
[32m[20221213 23:51:55 @agent_ppo2.py:185][0m |          -0.0098 |          42.6775 |          11.4364 |
[32m[20221213 23:51:55 @agent_ppo2.py:185][0m |          -0.0129 |          42.5094 |          11.4741 |
[32m[20221213 23:51:55 @agent_ppo2.py:185][0m |          -0.0093 |          42.3944 |          11.4669 |
[32m[20221213 23:51:55 @agent_ppo2.py:185][0m |          -0.0248 |          41.9612 |          11.4564 |
[32m[20221213 23:51:55 @agent_ppo2.py:185][0m |          -0.0148 |          41.6589 |          11.4679 |
[32m[20221213 23:51:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:51:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.75
[32m[20221213 23:51:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.94
[32m[20221213 23:51:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.78
[32m[20221213 23:51:55 @agent_ppo2.py:143][0m Total time:      39.38 min
[32m[20221213 23:51:55 @agent_ppo2.py:145][0m 3819520 total steps have happened
[32m[20221213 23:51:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3865 --------------------------#
[32m[20221213 23:51:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:51:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:55 @agent_ppo2.py:185][0m |          -0.0022 |          67.7558 |          11.1574 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |          -0.0069 |          64.3830 |          11.1696 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |          -0.0080 |          63.5723 |          11.2025 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |           0.0106 |          71.4293 |          11.2044 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |          -0.0095 |          62.6117 |          11.1865 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |           0.0065 |          67.9096 |          11.1824 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |          -0.0100 |          61.4282 |          11.2325 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |          -0.0142 |          61.1551 |          11.2528 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |          -0.0134 |          61.1320 |          11.2819 |
[32m[20221213 23:51:56 @agent_ppo2.py:185][0m |          -0.0157 |          60.6720 |          11.2935 |
[32m[20221213 23:51:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:51:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.37
[32m[20221213 23:51:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.88
[32m[20221213 23:51:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.56
[32m[20221213 23:51:56 @agent_ppo2.py:143][0m Total time:      39.41 min
[32m[20221213 23:51:56 @agent_ppo2.py:145][0m 3821568 total steps have happened
[32m[20221213 23:51:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3866 --------------------------#
[32m[20221213 23:51:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0013 |          52.3235 |          11.2275 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0095 |          49.1178 |          11.2367 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0072 |          47.9726 |          11.2509 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0138 |          46.7405 |          11.2583 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0122 |          46.0391 |          11.2713 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0141 |          45.4435 |          11.2256 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0148 |          44.9837 |          11.2687 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0198 |          44.5291 |          11.2585 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0208 |          44.1397 |          11.2806 |
[32m[20221213 23:51:57 @agent_ppo2.py:185][0m |          -0.0241 |          43.8043 |          11.2840 |
[32m[20221213 23:51:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.09
[32m[20221213 23:51:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.62
[32m[20221213 23:51:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.59
[32m[20221213 23:51:58 @agent_ppo2.py:143][0m Total time:      39.43 min
[32m[20221213 23:51:58 @agent_ppo2.py:145][0m 3823616 total steps have happened
[32m[20221213 23:51:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3867 --------------------------#
[32m[20221213 23:51:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:58 @agent_ppo2.py:185][0m |           0.0031 |          63.3960 |          11.4057 |
[32m[20221213 23:51:58 @agent_ppo2.py:185][0m |          -0.0049 |          52.0919 |          11.3909 |
[32m[20221213 23:51:58 @agent_ppo2.py:185][0m |          -0.0119 |          49.9719 |          11.4336 |
[32m[20221213 23:51:58 @agent_ppo2.py:185][0m |          -0.0057 |          48.9829 |          11.4034 |
[32m[20221213 23:51:58 @agent_ppo2.py:185][0m |          -0.0092 |          48.4039 |          11.4444 |
[32m[20221213 23:51:58 @agent_ppo2.py:185][0m |          -0.0079 |          48.7748 |          11.4400 |
[32m[20221213 23:51:58 @agent_ppo2.py:185][0m |          -0.0144 |          47.4083 |          11.4611 |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |          -0.0191 |          46.8884 |          11.4582 |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |          -0.0121 |          46.8220 |          11.4561 |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |          -0.0163 |          46.5964 |          11.4435 |
[32m[20221213 23:51:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:51:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.62
[32m[20221213 23:51:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.73
[32m[20221213 23:51:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.39
[32m[20221213 23:51:59 @agent_ppo2.py:143][0m Total time:      39.45 min
[32m[20221213 23:51:59 @agent_ppo2.py:145][0m 3825664 total steps have happened
[32m[20221213 23:51:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3868 --------------------------#
[32m[20221213 23:51:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:51:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |           0.0067 |          47.9337 |          11.3314 |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |          -0.0039 |          41.6963 |          11.3345 |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |          -0.0097 |          39.4292 |          11.3535 |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |          -0.0119 |          38.3619 |          11.3365 |
[32m[20221213 23:51:59 @agent_ppo2.py:185][0m |          -0.0120 |          37.5116 |          11.3615 |
[32m[20221213 23:52:00 @agent_ppo2.py:185][0m |          -0.0002 |          41.7220 |          11.3576 |
[32m[20221213 23:52:00 @agent_ppo2.py:185][0m |          -0.0115 |          36.6006 |          11.3983 |
[32m[20221213 23:52:00 @agent_ppo2.py:185][0m |          -0.0171 |          35.9930 |          11.4078 |
[32m[20221213 23:52:00 @agent_ppo2.py:185][0m |          -0.0150 |          35.6380 |          11.3906 |
[32m[20221213 23:52:00 @agent_ppo2.py:185][0m |          -0.0172 |          35.4579 |          11.4204 |
[32m[20221213 23:52:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.95
[32m[20221213 23:52:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.76
[32m[20221213 23:52:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.41
[32m[20221213 23:52:00 @agent_ppo2.py:143][0m Total time:      39.47 min
[32m[20221213 23:52:00 @agent_ppo2.py:145][0m 3827712 total steps have happened
[32m[20221213 23:52:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3869 --------------------------#
[32m[20221213 23:52:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:00 @agent_ppo2.py:185][0m |           0.0079 |          43.5028 |          11.1756 |
[32m[20221213 23:52:00 @agent_ppo2.py:185][0m |          -0.0021 |          37.4687 |          11.1930 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0081 |          35.6865 |          11.1449 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0124 |          34.6623 |          11.1869 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0111 |          33.7678 |          11.2015 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0125 |          33.1660 |          11.1583 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0127 |          32.3099 |          11.1809 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0108 |          31.8104 |          11.1940 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0158 |          31.5651 |          11.1841 |
[32m[20221213 23:52:01 @agent_ppo2.py:185][0m |          -0.0078 |          32.0458 |          11.2014 |
[32m[20221213 23:52:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.71
[32m[20221213 23:52:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.05
[32m[20221213 23:52:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.42
[32m[20221213 23:52:01 @agent_ppo2.py:143][0m Total time:      39.49 min
[32m[20221213 23:52:01 @agent_ppo2.py:145][0m 3829760 total steps have happened
[32m[20221213 23:52:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3870 --------------------------#
[32m[20221213 23:52:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:52:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |           0.0008 |          64.4714 |          11.3251 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0070 |          59.0823 |          11.3336 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0108 |          57.6438 |          11.3025 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0115 |          56.7744 |          11.2514 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0122 |          55.8947 |          11.2459 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0136 |          55.5751 |          11.2573 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0161 |          54.8413 |          11.2583 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0151 |          54.4559 |          11.2278 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0061 |          59.9449 |          11.2495 |
[32m[20221213 23:52:02 @agent_ppo2.py:185][0m |          -0.0174 |          54.4948 |          11.2483 |
[32m[20221213 23:52:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:52:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.60
[32m[20221213 23:52:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.52
[32m[20221213 23:52:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.01
[32m[20221213 23:52:03 @agent_ppo2.py:143][0m Total time:      39.51 min
[32m[20221213 23:52:03 @agent_ppo2.py:145][0m 3831808 total steps have happened
[32m[20221213 23:52:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3871 --------------------------#
[32m[20221213 23:52:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:52:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0042 |          54.9986 |          11.1626 |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0039 |          48.0877 |          11.1879 |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0016 |          47.2138 |          11.1605 |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0117 |          44.9107 |          11.1604 |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0170 |          43.8420 |          11.1923 |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0134 |          42.5561 |          11.2022 |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0142 |          42.8119 |          11.2003 |
[32m[20221213 23:52:03 @agent_ppo2.py:185][0m |          -0.0175 |          41.7450 |          11.1874 |
[32m[20221213 23:52:04 @agent_ppo2.py:185][0m |          -0.0151 |          41.9700 |          11.2027 |
[32m[20221213 23:52:04 @agent_ppo2.py:185][0m |          -0.0169 |          40.8387 |          11.2044 |
[32m[20221213 23:52:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.08
[32m[20221213 23:52:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.38
[32m[20221213 23:52:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.35
[32m[20221213 23:52:04 @agent_ppo2.py:143][0m Total time:      39.53 min
[32m[20221213 23:52:04 @agent_ppo2.py:145][0m 3833856 total steps have happened
[32m[20221213 23:52:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3872 --------------------------#
[32m[20221213 23:52:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:04 @agent_ppo2.py:185][0m |           0.0051 |          77.2489 |          11.3501 |
[32m[20221213 23:52:04 @agent_ppo2.py:185][0m |          -0.0046 |          69.9446 |          11.3905 |
[32m[20221213 23:52:04 @agent_ppo2.py:185][0m |          -0.0081 |          67.1239 |          11.3372 |
[32m[20221213 23:52:04 @agent_ppo2.py:185][0m |          -0.0125 |          65.5936 |          11.3395 |
[32m[20221213 23:52:04 @agent_ppo2.py:185][0m |          -0.0124 |          64.1853 |          11.3875 |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0133 |          63.2305 |          11.3585 |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0156 |          63.0595 |          11.3373 |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0077 |          64.1201 |          11.3614 |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0162 |          61.8519 |          11.3032 |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0072 |          66.4678 |          11.3171 |
[32m[20221213 23:52:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.60
[32m[20221213 23:52:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.64
[32m[20221213 23:52:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.24
[32m[20221213 23:52:05 @agent_ppo2.py:143][0m Total time:      39.55 min
[32m[20221213 23:52:05 @agent_ppo2.py:145][0m 3835904 total steps have happened
[32m[20221213 23:52:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3873 --------------------------#
[32m[20221213 23:52:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0021 |          58.6795 |          11.2607 |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0112 |          52.8445 |          11.3018 |
[32m[20221213 23:52:05 @agent_ppo2.py:185][0m |          -0.0047 |          50.5505 |          11.3140 |
[32m[20221213 23:52:06 @agent_ppo2.py:185][0m |          -0.0118 |          47.0496 |          11.3274 |
[32m[20221213 23:52:06 @agent_ppo2.py:185][0m |          -0.0166 |          45.3878 |          11.3512 |
[32m[20221213 23:52:06 @agent_ppo2.py:185][0m |          -0.0110 |          44.5460 |          11.3553 |
[32m[20221213 23:52:06 @agent_ppo2.py:185][0m |          -0.0178 |          42.9357 |          11.3476 |
[32m[20221213 23:52:06 @agent_ppo2.py:185][0m |          -0.0165 |          42.1114 |          11.3732 |
[32m[20221213 23:52:06 @agent_ppo2.py:185][0m |          -0.0215 |          41.2450 |          11.3977 |
[32m[20221213 23:52:06 @agent_ppo2.py:185][0m |          -0.0208 |          40.6364 |          11.3912 |
[32m[20221213 23:52:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.18
[32m[20221213 23:52:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.51
[32m[20221213 23:52:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.63
[32m[20221213 23:52:06 @agent_ppo2.py:143][0m Total time:      39.57 min
[32m[20221213 23:52:06 @agent_ppo2.py:145][0m 3837952 total steps have happened
[32m[20221213 23:52:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3874 --------------------------#
[32m[20221213 23:52:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0045 |          50.9398 |          11.3898 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0092 |          43.6791 |          11.3511 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0148 |          41.1550 |          11.3585 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0031 |          39.7040 |          11.3592 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0161 |          37.7961 |          11.3606 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0207 |          38.0323 |          11.3481 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0177 |          36.6731 |          11.3937 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0177 |          36.0566 |          11.3336 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0149 |          35.9902 |          11.3267 |
[32m[20221213 23:52:07 @agent_ppo2.py:185][0m |          -0.0185 |          35.5134 |          11.3401 |
[32m[20221213 23:52:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.31
[32m[20221213 23:52:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.72
[32m[20221213 23:52:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.35
[32m[20221213 23:52:07 @agent_ppo2.py:143][0m Total time:      39.59 min
[32m[20221213 23:52:07 @agent_ppo2.py:145][0m 3840000 total steps have happened
[32m[20221213 23:52:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3875 --------------------------#
[32m[20221213 23:52:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:52:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0003 |          66.5581 |          11.1317 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0033 |          61.0519 |          11.1781 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0084 |          59.4736 |          11.1361 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0120 |          58.9732 |          11.1745 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0114 |          58.2539 |          11.0907 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0086 |          57.8040 |          11.1275 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0101 |          57.8103 |          11.1108 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0143 |          57.3123 |          11.0888 |
[32m[20221213 23:52:08 @agent_ppo2.py:185][0m |          -0.0004 |          63.3473 |          11.1066 |
[32m[20221213 23:52:09 @agent_ppo2.py:185][0m |          -0.0122 |          56.7590 |          11.1198 |
[32m[20221213 23:52:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:52:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.17
[32m[20221213 23:52:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 547.25
[32m[20221213 23:52:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.01
[32m[20221213 23:52:09 @agent_ppo2.py:143][0m Total time:      39.61 min
[32m[20221213 23:52:09 @agent_ppo2.py:145][0m 3842048 total steps have happened
[32m[20221213 23:52:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3876 --------------------------#
[32m[20221213 23:52:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:09 @agent_ppo2.py:185][0m |           0.0021 |          70.2659 |          11.5937 |
[32m[20221213 23:52:09 @agent_ppo2.py:185][0m |          -0.0048 |          64.7446 |          11.6116 |
[32m[20221213 23:52:09 @agent_ppo2.py:185][0m |          -0.0034 |          63.1202 |          11.6206 |
[32m[20221213 23:52:09 @agent_ppo2.py:185][0m |           0.0004 |          65.6735 |          11.6633 |
[32m[20221213 23:52:09 @agent_ppo2.py:185][0m |          -0.0096 |          60.1013 |          11.6823 |
[32m[20221213 23:52:09 @agent_ppo2.py:185][0m |          -0.0147 |          58.8252 |          11.6705 |
[32m[20221213 23:52:10 @agent_ppo2.py:185][0m |          -0.0101 |          59.5571 |          11.6969 |
[32m[20221213 23:52:10 @agent_ppo2.py:185][0m |          -0.0099 |          58.5269 |          11.6927 |
[32m[20221213 23:52:10 @agent_ppo2.py:185][0m |          -0.0133 |          57.2057 |          11.7088 |
[32m[20221213 23:52:10 @agent_ppo2.py:185][0m |          -0.0134 |          56.9431 |          11.7242 |
[32m[20221213 23:52:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:52:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.21
[32m[20221213 23:52:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.12
[32m[20221213 23:52:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.82
[32m[20221213 23:52:10 @agent_ppo2.py:143][0m Total time:      39.63 min
[32m[20221213 23:52:10 @agent_ppo2.py:145][0m 3844096 total steps have happened
[32m[20221213 23:52:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3877 --------------------------#
[32m[20221213 23:52:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:10 @agent_ppo2.py:185][0m |           0.0026 |          63.2550 |          11.6969 |
[32m[20221213 23:52:10 @agent_ppo2.py:185][0m |          -0.0041 |          58.1710 |          11.7164 |
[32m[20221213 23:52:10 @agent_ppo2.py:185][0m |          -0.0096 |          56.8406 |          11.7048 |
[32m[20221213 23:52:11 @agent_ppo2.py:185][0m |          -0.0095 |          55.8692 |          11.7279 |
[32m[20221213 23:52:11 @agent_ppo2.py:185][0m |          -0.0104 |          55.0308 |          11.7363 |
[32m[20221213 23:52:11 @agent_ppo2.py:185][0m |          -0.0095 |          54.7854 |          11.7534 |
[32m[20221213 23:52:11 @agent_ppo2.py:185][0m |          -0.0156 |          54.1426 |          11.7135 |
[32m[20221213 23:52:11 @agent_ppo2.py:185][0m |          -0.0166 |          54.0535 |          11.7327 |
[32m[20221213 23:52:11 @agent_ppo2.py:185][0m |          -0.0154 |          53.7486 |          11.7657 |
[32m[20221213 23:52:11 @agent_ppo2.py:185][0m |          -0.0192 |          53.3189 |          11.7185 |
[32m[20221213 23:52:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.57
[32m[20221213 23:52:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.58
[32m[20221213 23:52:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.04
[32m[20221213 23:52:11 @agent_ppo2.py:143][0m Total time:      39.65 min
[32m[20221213 23:52:11 @agent_ppo2.py:145][0m 3846144 total steps have happened
[32m[20221213 23:52:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3878 --------------------------#
[32m[20221213 23:52:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0003 |          53.5005 |          11.5573 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0054 |          47.1243 |          11.5766 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0088 |          45.0202 |          11.5969 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0026 |          44.3849 |          11.6094 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0070 |          43.3731 |          11.6290 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0128 |          42.9680 |          11.6571 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0040 |          42.3304 |          11.6592 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0097 |          41.8451 |          11.6386 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0097 |          41.5521 |          11.6524 |
[32m[20221213 23:52:12 @agent_ppo2.py:185][0m |          -0.0167 |          41.3802 |          11.6475 |
[32m[20221213 23:52:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:52:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.30
[32m[20221213 23:52:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.47
[32m[20221213 23:52:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.53
[32m[20221213 23:52:12 @agent_ppo2.py:143][0m Total time:      39.67 min
[32m[20221213 23:52:12 @agent_ppo2.py:145][0m 3848192 total steps have happened
[32m[20221213 23:52:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3879 --------------------------#
[32m[20221213 23:52:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |           0.0021 |          45.2062 |          11.8412 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0073 |          39.5110 |          11.8596 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0061 |          38.8557 |          11.8151 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0086 |          36.8824 |          11.8527 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0120 |          36.3482 |          11.8021 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0053 |          39.5464 |          11.8214 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0136 |          35.4387 |          11.8166 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0157 |          34.9429 |          11.7947 |
[32m[20221213 23:52:13 @agent_ppo2.py:185][0m |          -0.0171 |          34.6684 |          11.8222 |
[32m[20221213 23:52:14 @agent_ppo2.py:185][0m |          -0.0149 |          34.3853 |          11.7793 |
[32m[20221213 23:52:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.46
[32m[20221213 23:52:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.98
[32m[20221213 23:52:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.16
[32m[20221213 23:52:14 @agent_ppo2.py:143][0m Total time:      39.69 min
[32m[20221213 23:52:14 @agent_ppo2.py:145][0m 3850240 total steps have happened
[32m[20221213 23:52:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3880 --------------------------#
[32m[20221213 23:52:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:52:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:14 @agent_ppo2.py:185][0m |           0.0023 |          64.1529 |          11.6311 |
[32m[20221213 23:52:14 @agent_ppo2.py:185][0m |           0.0025 |          63.5973 |          11.6538 |
[32m[20221213 23:52:14 @agent_ppo2.py:185][0m |          -0.0094 |          57.9344 |          11.6222 |
[32m[20221213 23:52:14 @agent_ppo2.py:185][0m |          -0.0084 |          56.3399 |          11.6379 |
[32m[20221213 23:52:14 @agent_ppo2.py:185][0m |          -0.0094 |          55.0571 |          11.6208 |
[32m[20221213 23:52:14 @agent_ppo2.py:185][0m |          -0.0080 |          54.1920 |          11.6178 |
[32m[20221213 23:52:15 @agent_ppo2.py:185][0m |          -0.0140 |          53.3422 |          11.6049 |
[32m[20221213 23:52:15 @agent_ppo2.py:185][0m |          -0.0096 |          52.8157 |          11.6387 |
[32m[20221213 23:52:15 @agent_ppo2.py:185][0m |          -0.0015 |          55.6481 |          11.5831 |
[32m[20221213 23:52:15 @agent_ppo2.py:185][0m |          -0.0127 |          51.4218 |          11.6248 |
[32m[20221213 23:52:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:52:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.62
[32m[20221213 23:52:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.82
[32m[20221213 23:52:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.84
[32m[20221213 23:52:15 @agent_ppo2.py:143][0m Total time:      39.72 min
[32m[20221213 23:52:15 @agent_ppo2.py:145][0m 3852288 total steps have happened
[32m[20221213 23:52:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3881 --------------------------#
[32m[20221213 23:52:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:15 @agent_ppo2.py:185][0m |          -0.0034 |          72.7544 |          11.6725 |
[32m[20221213 23:52:15 @agent_ppo2.py:185][0m |          -0.0024 |          66.3744 |          11.6558 |
[32m[20221213 23:52:15 @agent_ppo2.py:185][0m |          -0.0086 |          64.3097 |          11.6978 |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0073 |          63.2024 |          11.6129 |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0152 |          62.4714 |          11.6029 |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0098 |          61.9620 |          11.5764 |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0096 |          62.9699 |          11.5605 |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0124 |          61.4733 |          11.5456 |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0143 |          61.0583 |          11.4920 |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0158 |          60.9017 |          11.4973 |
[32m[20221213 23:52:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.09
[32m[20221213 23:52:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.88
[32m[20221213 23:52:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.87
[32m[20221213 23:52:16 @agent_ppo2.py:143][0m Total time:      39.74 min
[32m[20221213 23:52:16 @agent_ppo2.py:145][0m 3854336 total steps have happened
[32m[20221213 23:52:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3882 --------------------------#
[32m[20221213 23:52:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:16 @agent_ppo2.py:185][0m |          -0.0013 |          64.5655 |          11.2101 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |          -0.0082 |          59.7993 |          11.2191 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |          -0.0038 |          58.3834 |          11.2073 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |           0.0002 |          58.1551 |          11.2122 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |          -0.0070 |          57.0934 |          11.2126 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |          -0.0110 |          56.6794 |          11.2041 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |          -0.0109 |          56.2338 |          11.2310 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |           0.0058 |          62.6084 |          11.2273 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |          -0.0154 |          55.6822 |          11.2660 |
[32m[20221213 23:52:17 @agent_ppo2.py:185][0m |          -0.0114 |          55.1950 |          11.2775 |
[32m[20221213 23:52:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.44
[32m[20221213 23:52:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.71
[32m[20221213 23:52:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.43
[32m[20221213 23:52:17 @agent_ppo2.py:143][0m Total time:      39.76 min
[32m[20221213 23:52:17 @agent_ppo2.py:145][0m 3856384 total steps have happened
[32m[20221213 23:52:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3883 --------------------------#
[32m[20221213 23:52:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0019 |          55.4888 |          11.0880 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0073 |          51.1169 |          11.1518 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0104 |          49.7446 |          11.1622 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0141 |          48.8449 |          11.1506 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0139 |          48.0741 |          11.1814 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0132 |          47.6011 |          11.1750 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0094 |          47.2915 |          11.2153 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0173 |          46.7143 |          11.2113 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0185 |          46.4076 |          11.1958 |
[32m[20221213 23:52:18 @agent_ppo2.py:185][0m |          -0.0156 |          46.2086 |          11.2403 |
[32m[20221213 23:52:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.10
[32m[20221213 23:52:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.05
[32m[20221213 23:52:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.12
[32m[20221213 23:52:19 @agent_ppo2.py:143][0m Total time:      39.78 min
[32m[20221213 23:52:19 @agent_ppo2.py:145][0m 3858432 total steps have happened
[32m[20221213 23:52:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3884 --------------------------#
[32m[20221213 23:52:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:19 @agent_ppo2.py:185][0m |           0.0008 |          75.3395 |          11.3855 |
[32m[20221213 23:52:19 @agent_ppo2.py:185][0m |          -0.0056 |          70.9190 |          11.3758 |
[32m[20221213 23:52:19 @agent_ppo2.py:185][0m |          -0.0086 |          69.5426 |          11.3750 |
[32m[20221213 23:52:19 @agent_ppo2.py:185][0m |          -0.0102 |          69.0182 |          11.3777 |
[32m[20221213 23:52:19 @agent_ppo2.py:185][0m |          -0.0108 |          68.5753 |          11.3445 |
[32m[20221213 23:52:19 @agent_ppo2.py:185][0m |          -0.0083 |          70.0791 |          11.3488 |
[32m[20221213 23:52:19 @agent_ppo2.py:185][0m |          -0.0142 |          68.0616 |          11.3804 |
[32m[20221213 23:52:20 @agent_ppo2.py:185][0m |          -0.0095 |          68.6919 |          11.3693 |
[32m[20221213 23:52:20 @agent_ppo2.py:185][0m |          -0.0155 |          68.1107 |          11.3108 |
[32m[20221213 23:52:20 @agent_ppo2.py:185][0m |          -0.0149 |          67.6252 |          11.3507 |
[32m[20221213 23:52:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:52:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.24
[32m[20221213 23:52:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.94
[32m[20221213 23:52:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.79
[32m[20221213 23:52:20 @agent_ppo2.py:143][0m Total time:      39.80 min
[32m[20221213 23:52:20 @agent_ppo2.py:145][0m 3860480 total steps have happened
[32m[20221213 23:52:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3885 --------------------------#
[32m[20221213 23:52:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:20 @agent_ppo2.py:185][0m |           0.0057 |          62.4860 |          11.5372 |
[32m[20221213 23:52:20 @agent_ppo2.py:185][0m |          -0.0041 |          57.7604 |          11.5196 |
[32m[20221213 23:52:20 @agent_ppo2.py:185][0m |          -0.0069 |          56.2389 |          11.5439 |
[32m[20221213 23:52:20 @agent_ppo2.py:185][0m |          -0.0091 |          55.1673 |          11.5674 |
[32m[20221213 23:52:21 @agent_ppo2.py:185][0m |          -0.0124 |          54.3487 |          11.5930 |
[32m[20221213 23:52:21 @agent_ppo2.py:185][0m |          -0.0108 |          53.3536 |          11.5920 |
[32m[20221213 23:52:21 @agent_ppo2.py:185][0m |          -0.0108 |          53.4108 |          11.5776 |
[32m[20221213 23:52:21 @agent_ppo2.py:185][0m |          -0.0122 |          52.4446 |          11.6079 |
[32m[20221213 23:52:21 @agent_ppo2.py:185][0m |          -0.0126 |          52.0176 |          11.5909 |
[32m[20221213 23:52:21 @agent_ppo2.py:185][0m |          -0.0147 |          51.6857 |          11.6022 |
[32m[20221213 23:52:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:52:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.89
[32m[20221213 23:52:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.94
[32m[20221213 23:52:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.18
[32m[20221213 23:52:21 @agent_ppo2.py:143][0m Total time:      39.82 min
[32m[20221213 23:52:21 @agent_ppo2.py:145][0m 3862528 total steps have happened
[32m[20221213 23:52:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3886 --------------------------#
[32m[20221213 23:52:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:21 @agent_ppo2.py:185][0m |           0.0027 |          71.2681 |          11.6003 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |           0.0104 |          75.2454 |          11.5353 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |           0.0069 |          70.0695 |          11.6499 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |           0.0025 |          67.4698 |          11.5968 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |          -0.0070 |          63.1866 |          11.6395 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |          -0.0124 |          62.5683 |          11.6405 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |          -0.0079 |          61.7841 |          11.6948 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |          -0.0083 |          61.2809 |          11.6513 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |          -0.0087 |          60.5537 |          11.6603 |
[32m[20221213 23:52:22 @agent_ppo2.py:185][0m |          -0.0113 |          60.1359 |          11.7174 |
[32m[20221213 23:52:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.41
[32m[20221213 23:52:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.60
[32m[20221213 23:52:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.36
[32m[20221213 23:52:22 @agent_ppo2.py:143][0m Total time:      39.84 min
[32m[20221213 23:52:22 @agent_ppo2.py:145][0m 3864576 total steps have happened
[32m[20221213 23:52:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3887 --------------------------#
[32m[20221213 23:52:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0008 |          65.5794 |          11.6880 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0078 |          60.3625 |          11.6477 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0058 |          61.2113 |          11.6469 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0094 |          58.3510 |          11.6719 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0145 |          57.5035 |          11.6269 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0117 |          56.8623 |          11.6253 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0055 |          62.1520 |          11.6015 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0150 |          56.4046 |          11.6108 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0145 |          55.8659 |          11.6214 |
[32m[20221213 23:52:23 @agent_ppo2.py:185][0m |          -0.0155 |          55.3594 |          11.6190 |
[32m[20221213 23:52:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.58
[32m[20221213 23:52:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.19
[32m[20221213 23:52:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.20
[32m[20221213 23:52:24 @agent_ppo2.py:143][0m Total time:      39.86 min
[32m[20221213 23:52:24 @agent_ppo2.py:145][0m 3866624 total steps have happened
[32m[20221213 23:52:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3888 --------------------------#
[32m[20221213 23:52:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |           0.0082 |          61.8833 |          11.3150 |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |          -0.0072 |          56.2310 |          11.2012 |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |          -0.0057 |          54.3134 |          11.2511 |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |          -0.0086 |          53.1605 |          11.2274 |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |          -0.0097 |          52.1489 |          11.1595 |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |          -0.0135 |          51.7020 |          11.2132 |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |          -0.0147 |          50.9554 |          11.1989 |
[32m[20221213 23:52:24 @agent_ppo2.py:185][0m |          -0.0127 |          50.7083 |          11.1916 |
[32m[20221213 23:52:25 @agent_ppo2.py:185][0m |          -0.0169 |          50.2678 |          11.1607 |
[32m[20221213 23:52:25 @agent_ppo2.py:185][0m |          -0.0167 |          49.6250 |          11.1583 |
[32m[20221213 23:52:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.22
[32m[20221213 23:52:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.31
[32m[20221213 23:52:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.71
[32m[20221213 23:52:25 @agent_ppo2.py:143][0m Total time:      39.88 min
[32m[20221213 23:52:25 @agent_ppo2.py:145][0m 3868672 total steps have happened
[32m[20221213 23:52:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3889 --------------------------#
[32m[20221213 23:52:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:25 @agent_ppo2.py:185][0m |           0.0009 |          58.2327 |          11.6532 |
[32m[20221213 23:52:25 @agent_ppo2.py:185][0m |          -0.0011 |          55.7330 |          11.6365 |
[32m[20221213 23:52:25 @agent_ppo2.py:185][0m |          -0.0065 |          52.4812 |          11.6135 |
[32m[20221213 23:52:25 @agent_ppo2.py:185][0m |          -0.0092 |          51.5026 |          11.6673 |
[32m[20221213 23:52:25 @agent_ppo2.py:185][0m |          -0.0110 |          50.7063 |          11.6916 |
[32m[20221213 23:52:26 @agent_ppo2.py:185][0m |           0.0068 |          54.5659 |          11.6746 |
[32m[20221213 23:52:26 @agent_ppo2.py:185][0m |          -0.0113 |          49.3196 |          11.6605 |
[32m[20221213 23:52:26 @agent_ppo2.py:185][0m |          -0.0111 |          48.8661 |          11.6789 |
[32m[20221213 23:52:26 @agent_ppo2.py:185][0m |          -0.0133 |          48.3543 |          11.6711 |
[32m[20221213 23:52:26 @agent_ppo2.py:185][0m |          -0.0166 |          47.8964 |          11.6949 |
[32m[20221213 23:52:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:52:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.25
[32m[20221213 23:52:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.95
[32m[20221213 23:52:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.72
[32m[20221213 23:52:26 @agent_ppo2.py:143][0m Total time:      39.90 min
[32m[20221213 23:52:26 @agent_ppo2.py:145][0m 3870720 total steps have happened
[32m[20221213 23:52:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3890 --------------------------#
[32m[20221213 23:52:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:52:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:26 @agent_ppo2.py:185][0m |           0.0005 |          68.1553 |          11.4612 |
[32m[20221213 23:52:26 @agent_ppo2.py:185][0m |           0.0082 |          65.4048 |          11.4333 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |           0.0085 |          63.1448 |          11.3747 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |          -0.0037 |          61.6344 |          11.4008 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |          -0.0051 |          61.5067 |          11.3237 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |          -0.0090 |          60.7895 |          11.3595 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |           0.0131 |          69.4634 |          11.3139 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |          -0.0102 |          60.7558 |          11.3231 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |          -0.0095 |          60.0744 |          11.3028 |
[32m[20221213 23:52:27 @agent_ppo2.py:185][0m |          -0.0143 |          60.0498 |          11.2879 |
[32m[20221213 23:52:27 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.73
[32m[20221213 23:52:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.87
[32m[20221213 23:52:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.65
[32m[20221213 23:52:27 @agent_ppo2.py:143][0m Total time:      39.92 min
[32m[20221213 23:52:27 @agent_ppo2.py:145][0m 3872768 total steps have happened
[32m[20221213 23:52:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3891 --------------------------#
[32m[20221213 23:52:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0013 |          55.4257 |          11.5580 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0070 |          50.4171 |          11.4822 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0094 |          49.4556 |          11.5691 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0137 |          47.9973 |          11.5370 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0144 |          47.6195 |          11.5407 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0106 |          47.3104 |          11.5783 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0114 |          46.9104 |          11.5988 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0183 |          46.3370 |          11.5850 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0092 |          48.0341 |          11.6211 |
[32m[20221213 23:52:28 @agent_ppo2.py:185][0m |          -0.0148 |          46.0724 |          11.6431 |
[32m[20221213 23:52:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.40
[32m[20221213 23:52:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.21
[32m[20221213 23:52:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.64
[32m[20221213 23:52:28 @agent_ppo2.py:143][0m Total time:      39.94 min
[32m[20221213 23:52:28 @agent_ppo2.py:145][0m 3874816 total steps have happened
[32m[20221213 23:52:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3892 --------------------------#
[32m[20221213 23:52:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |           0.0013 |          63.3545 |          11.2405 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |           0.0014 |          60.2998 |          11.2091 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |          -0.0069 |          59.0789 |          11.1787 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |          -0.0037 |          58.8251 |          11.1554 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |           0.0043 |          62.4405 |          11.1976 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |           0.0014 |          60.0962 |          11.2101 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |           0.0012 |          61.1890 |          11.1996 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |          -0.0110 |          56.3106 |          11.1809 |
[32m[20221213 23:52:29 @agent_ppo2.py:185][0m |          -0.0083 |          56.4859 |          11.1838 |
[32m[20221213 23:52:30 @agent_ppo2.py:185][0m |          -0.0118 |          56.0912 |          11.2041 |
[32m[20221213 23:52:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.87
[32m[20221213 23:52:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.87
[32m[20221213 23:52:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.63
[32m[20221213 23:52:30 @agent_ppo2.py:143][0m Total time:      39.96 min
[32m[20221213 23:52:30 @agent_ppo2.py:145][0m 3876864 total steps have happened
[32m[20221213 23:52:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3893 --------------------------#
[32m[20221213 23:52:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:30 @agent_ppo2.py:185][0m |          -0.0018 |          69.5707 |          11.5137 |
[32m[20221213 23:52:30 @agent_ppo2.py:185][0m |          -0.0083 |          59.1005 |          11.5186 |
[32m[20221213 23:52:30 @agent_ppo2.py:185][0m |          -0.0127 |          54.6847 |          11.5453 |
[32m[20221213 23:52:30 @agent_ppo2.py:185][0m |          -0.0141 |          52.2935 |          11.5299 |
[32m[20221213 23:52:30 @agent_ppo2.py:185][0m |          -0.0153 |          50.6061 |          11.5642 |
[32m[20221213 23:52:30 @agent_ppo2.py:185][0m |          -0.0158 |          49.7986 |          11.5655 |
[32m[20221213 23:52:31 @agent_ppo2.py:185][0m |          -0.0114 |          48.6471 |          11.5393 |
[32m[20221213 23:52:31 @agent_ppo2.py:185][0m |          -0.0159 |          47.4411 |          11.5667 |
[32m[20221213 23:52:31 @agent_ppo2.py:185][0m |          -0.0174 |          46.7412 |          11.5916 |
[32m[20221213 23:52:31 @agent_ppo2.py:185][0m |          -0.0080 |          48.6584 |          11.5551 |
[32m[20221213 23:52:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.30
[32m[20221213 23:52:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.00
[32m[20221213 23:52:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.12
[32m[20221213 23:52:31 @agent_ppo2.py:143][0m Total time:      39.98 min
[32m[20221213 23:52:31 @agent_ppo2.py:145][0m 3878912 total steps have happened
[32m[20221213 23:52:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3894 --------------------------#
[32m[20221213 23:52:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:31 @agent_ppo2.py:185][0m |          -0.0003 |          66.4513 |          11.4880 |
[32m[20221213 23:52:31 @agent_ppo2.py:185][0m |          -0.0036 |          63.2375 |          11.4848 |
[32m[20221213 23:52:31 @agent_ppo2.py:185][0m |          -0.0085 |          61.9726 |          11.4961 |
[32m[20221213 23:52:32 @agent_ppo2.py:185][0m |          -0.0029 |          61.4941 |          11.4161 |
[32m[20221213 23:52:32 @agent_ppo2.py:185][0m |          -0.0088 |          60.4243 |          11.4688 |
[32m[20221213 23:52:32 @agent_ppo2.py:185][0m |          -0.0057 |          60.2337 |          11.4086 |
[32m[20221213 23:52:32 @agent_ppo2.py:185][0m |          -0.0094 |          59.3622 |          11.3925 |
[32m[20221213 23:52:32 @agent_ppo2.py:185][0m |          -0.0105 |          59.0910 |          11.3286 |
[32m[20221213 23:52:32 @agent_ppo2.py:185][0m |          -0.0109 |          58.8277 |          11.4011 |
[32m[20221213 23:52:32 @agent_ppo2.py:185][0m |          -0.0100 |          58.3619 |          11.3397 |
[32m[20221213 23:52:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:52:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.04
[32m[20221213 23:52:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.98
[32m[20221213 23:52:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.90
[32m[20221213 23:52:32 @agent_ppo2.py:143][0m Total time:      40.00 min
[32m[20221213 23:52:32 @agent_ppo2.py:145][0m 3880960 total steps have happened
[32m[20221213 23:52:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3895 --------------------------#
[32m[20221213 23:52:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0051 |          67.7752 |          11.2089 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0095 |          43.9704 |          11.2089 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0108 |          41.8706 |          11.1693 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0102 |          40.5436 |          11.1512 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0108 |          40.0376 |          11.1156 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0091 |          39.4760 |          11.1097 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0103 |          39.2701 |          11.1269 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0076 |          38.8268 |          11.0797 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0122 |          38.5310 |          11.0989 |
[32m[20221213 23:52:33 @agent_ppo2.py:185][0m |          -0.0058 |          46.1690 |          11.0404 |
[32m[20221213 23:52:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.57
[32m[20221213 23:52:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.94
[32m[20221213 23:52:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.65
[32m[20221213 23:52:33 @agent_ppo2.py:143][0m Total time:      40.02 min
[32m[20221213 23:52:33 @agent_ppo2.py:145][0m 3883008 total steps have happened
[32m[20221213 23:52:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3896 --------------------------#
[32m[20221213 23:52:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |           0.0037 |          57.7514 |          10.8674 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0006 |          49.3874 |          10.8071 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0023 |          45.3932 |          10.8645 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0035 |          44.2058 |          10.8041 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0088 |          43.0075 |          10.7469 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0137 |          42.4931 |          10.8186 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0124 |          41.9943 |          10.8051 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0066 |          47.6878 |          10.7775 |
[32m[20221213 23:52:34 @agent_ppo2.py:185][0m |          -0.0170 |          41.5439 |          10.7878 |
[32m[20221213 23:52:35 @agent_ppo2.py:185][0m |          -0.0087 |          41.1592 |          10.7792 |
[32m[20221213 23:52:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.26
[32m[20221213 23:52:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.53
[32m[20221213 23:52:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.57
[32m[20221213 23:52:35 @agent_ppo2.py:143][0m Total time:      40.04 min
[32m[20221213 23:52:35 @agent_ppo2.py:145][0m 3885056 total steps have happened
[32m[20221213 23:52:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3897 --------------------------#
[32m[20221213 23:52:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:35 @agent_ppo2.py:185][0m |          -0.0022 |          52.4336 |          11.1020 |
[32m[20221213 23:52:35 @agent_ppo2.py:185][0m |          -0.0061 |          47.5403 |          11.0865 |
[32m[20221213 23:52:35 @agent_ppo2.py:185][0m |          -0.0118 |          46.6286 |          11.1493 |
[32m[20221213 23:52:35 @agent_ppo2.py:185][0m |          -0.0145 |          46.0172 |          11.1287 |
[32m[20221213 23:52:35 @agent_ppo2.py:185][0m |          -0.0108 |          45.5763 |          11.1311 |
[32m[20221213 23:52:35 @agent_ppo2.py:185][0m |          -0.0146 |          45.2581 |          11.1175 |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |          -0.0177 |          44.9943 |          11.1600 |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |          -0.0184 |          44.5600 |          11.1465 |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |          -0.0169 |          44.4151 |          11.1078 |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |          -0.0209 |          44.1598 |          11.1307 |
[32m[20221213 23:52:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.27
[32m[20221213 23:52:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.97
[32m[20221213 23:52:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.99
[32m[20221213 23:52:36 @agent_ppo2.py:143][0m Total time:      40.06 min
[32m[20221213 23:52:36 @agent_ppo2.py:145][0m 3887104 total steps have happened
[32m[20221213 23:52:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3898 --------------------------#
[32m[20221213 23:52:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |           0.0031 |          59.2007 |          10.9510 |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |          -0.0027 |          55.9914 |          10.9882 |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |          -0.0061 |          54.7902 |          10.9732 |
[32m[20221213 23:52:36 @agent_ppo2.py:185][0m |          -0.0037 |          54.5582 |          10.9747 |
[32m[20221213 23:52:37 @agent_ppo2.py:185][0m |          -0.0046 |          53.9484 |          11.0219 |
[32m[20221213 23:52:37 @agent_ppo2.py:185][0m |          -0.0112 |          53.3490 |          10.9755 |
[32m[20221213 23:52:37 @agent_ppo2.py:185][0m |          -0.0046 |          54.7558 |          10.9739 |
[32m[20221213 23:52:37 @agent_ppo2.py:185][0m |          -0.0110 |          53.5872 |          10.9843 |
[32m[20221213 23:52:37 @agent_ppo2.py:185][0m |          -0.0148 |          53.0433 |          10.9964 |
[32m[20221213 23:52:37 @agent_ppo2.py:185][0m |          -0.0184 |          52.6225 |          11.0016 |
[32m[20221213 23:52:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.67
[32m[20221213 23:52:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.04
[32m[20221213 23:52:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.00
[32m[20221213 23:52:37 @agent_ppo2.py:143][0m Total time:      40.09 min
[32m[20221213 23:52:37 @agent_ppo2.py:145][0m 3889152 total steps have happened
[32m[20221213 23:52:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3899 --------------------------#
[32m[20221213 23:52:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:37 @agent_ppo2.py:185][0m |           0.0007 |          63.6479 |          10.7328 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0033 |          58.9643 |          10.7610 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0065 |          57.6753 |          10.7706 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0057 |          57.5314 |          10.7683 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0082 |          56.2440 |          10.8028 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0136 |          54.9495 |          10.8483 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0134 |          54.4266 |          10.8140 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0162 |          54.0094 |          10.8279 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0133 |          53.8282 |          10.8505 |
[32m[20221213 23:52:38 @agent_ppo2.py:185][0m |          -0.0144 |          53.2954 |          10.9049 |
[32m[20221213 23:52:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:52:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.76
[32m[20221213 23:52:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.64
[32m[20221213 23:52:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.15
[32m[20221213 23:52:38 @agent_ppo2.py:143][0m Total time:      40.11 min
[32m[20221213 23:52:38 @agent_ppo2.py:145][0m 3891200 total steps have happened
[32m[20221213 23:52:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3900 --------------------------#
[32m[20221213 23:52:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:52:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |           0.0065 |          64.5776 |          10.9726 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0061 |          59.7959 |          10.9399 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0083 |          58.7662 |          10.9307 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0115 |          58.4418 |          10.9110 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0118 |          57.8303 |          10.8968 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0124 |          57.5284 |          10.8898 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0144 |          57.3488 |          10.8868 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0144 |          57.1983 |          10.8909 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0184 |          57.0758 |          10.8575 |
[32m[20221213 23:52:39 @agent_ppo2.py:185][0m |          -0.0162 |          56.8710 |          10.8628 |
[32m[20221213 23:52:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.90
[32m[20221213 23:52:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.26
[32m[20221213 23:52:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.36
[32m[20221213 23:52:40 @agent_ppo2.py:143][0m Total time:      40.13 min
[32m[20221213 23:52:40 @agent_ppo2.py:145][0m 3893248 total steps have happened
[32m[20221213 23:52:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3901 --------------------------#
[32m[20221213 23:52:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:40 @agent_ppo2.py:185][0m |          -0.0025 |          49.0070 |          10.2543 |
[32m[20221213 23:52:40 @agent_ppo2.py:185][0m |          -0.0053 |          44.7590 |          10.2589 |
[32m[20221213 23:52:40 @agent_ppo2.py:185][0m |          -0.0066 |          43.6617 |          10.3027 |
[32m[20221213 23:52:40 @agent_ppo2.py:185][0m |          -0.0030 |          44.3172 |          10.2983 |
[32m[20221213 23:52:40 @agent_ppo2.py:185][0m |          -0.0090 |          42.2824 |          10.3424 |
[32m[20221213 23:52:40 @agent_ppo2.py:185][0m |          -0.0129 |          41.9554 |          10.3712 |
[32m[20221213 23:52:40 @agent_ppo2.py:185][0m |          -0.0151 |          41.3887 |          10.4352 |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |          -0.0125 |          41.2397 |          10.3742 |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |          -0.0171 |          40.9550 |          10.4176 |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |          -0.0135 |          40.8498 |          10.4115 |
[32m[20221213 23:52:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.18
[32m[20221213 23:52:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.71
[32m[20221213 23:52:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.26
[32m[20221213 23:52:41 @agent_ppo2.py:143][0m Total time:      40.15 min
[32m[20221213 23:52:41 @agent_ppo2.py:145][0m 3895296 total steps have happened
[32m[20221213 23:52:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3902 --------------------------#
[32m[20221213 23:52:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |           0.0026 |          60.8272 |          10.6717 |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |          -0.0056 |          58.5286 |          10.6874 |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |          -0.0070 |          57.4914 |          10.6880 |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |          -0.0046 |          57.0160 |          10.7442 |
[32m[20221213 23:52:41 @agent_ppo2.py:185][0m |          -0.0075 |          56.4521 |          10.7526 |
[32m[20221213 23:52:42 @agent_ppo2.py:185][0m |          -0.0095 |          56.0926 |          10.7236 |
[32m[20221213 23:52:42 @agent_ppo2.py:185][0m |          -0.0048 |          57.1800 |          10.7696 |
[32m[20221213 23:52:42 @agent_ppo2.py:185][0m |          -0.0070 |          55.9601 |          10.7683 |
[32m[20221213 23:52:42 @agent_ppo2.py:185][0m |          -0.0076 |          56.2019 |          10.7876 |
[32m[20221213 23:52:42 @agent_ppo2.py:185][0m |          -0.0050 |          57.1000 |          10.8117 |
[32m[20221213 23:52:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.87
[32m[20221213 23:52:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.39
[32m[20221213 23:52:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.14
[32m[20221213 23:52:42 @agent_ppo2.py:143][0m Total time:      40.17 min
[32m[20221213 23:52:42 @agent_ppo2.py:145][0m 3897344 total steps have happened
[32m[20221213 23:52:42 @agent_ppo2.py:121][0m #------------------------ Iteration 3903 --------------------------#
[32m[20221213 23:52:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:42 @agent_ppo2.py:185][0m |           0.0067 |          73.8031 |          10.8008 |
[32m[20221213 23:52:42 @agent_ppo2.py:185][0m |          -0.0040 |          66.7889 |          10.7891 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0052 |          65.0199 |          10.7599 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0088 |          63.3650 |          10.7090 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0091 |          62.2998 |          10.7617 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0068 |          61.5159 |          10.7023 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0078 |          62.0474 |          10.7310 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0106 |          59.7342 |          10.7464 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0128 |          59.3906 |          10.7749 |
[32m[20221213 23:52:43 @agent_ppo2.py:185][0m |          -0.0133 |          58.8883 |          10.7221 |
[32m[20221213 23:52:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 23:52:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.65
[32m[20221213 23:52:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.44
[32m[20221213 23:52:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.55
[32m[20221213 23:52:43 @agent_ppo2.py:143][0m Total time:      40.19 min
[32m[20221213 23:52:43 @agent_ppo2.py:145][0m 3899392 total steps have happened
[32m[20221213 23:52:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3904 --------------------------#
[32m[20221213 23:52:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |           0.0061 |          68.2657 |          11.0686 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0050 |          64.2907 |          11.0662 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0058 |          63.4238 |          11.0098 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0042 |          62.9806 |          11.0443 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0089 |          62.6753 |          10.9871 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0082 |          62.4582 |          10.9369 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0045 |          62.1656 |          10.9403 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0059 |          61.9328 |          10.9252 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0095 |          61.4024 |          10.9696 |
[32m[20221213 23:52:44 @agent_ppo2.py:185][0m |          -0.0034 |          62.8638 |          10.9294 |
[32m[20221213 23:52:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:52:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.67
[32m[20221213 23:52:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.97
[32m[20221213 23:52:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.80
[32m[20221213 23:52:45 @agent_ppo2.py:143][0m Total time:      40.21 min
[32m[20221213 23:52:45 @agent_ppo2.py:145][0m 3901440 total steps have happened
[32m[20221213 23:52:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3905 --------------------------#
[32m[20221213 23:52:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |           0.0088 |          53.4237 |          10.7382 |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |           0.0019 |          48.9596 |          10.7343 |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |          -0.0050 |          46.2354 |          10.7185 |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |          -0.0083 |          45.0937 |          10.7154 |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |          -0.0045 |          45.1256 |          10.7062 |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |          -0.0146 |          44.2766 |          10.7079 |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |          -0.0107 |          44.2005 |          10.6822 |
[32m[20221213 23:52:45 @agent_ppo2.py:185][0m |          -0.0116 |          43.5486 |          10.7191 |
[32m[20221213 23:52:46 @agent_ppo2.py:185][0m |          -0.0180 |          43.2017 |          10.7064 |
[32m[20221213 23:52:46 @agent_ppo2.py:185][0m |          -0.0154 |          43.0276 |          10.7517 |
[32m[20221213 23:52:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.31
[32m[20221213 23:52:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.36
[32m[20221213 23:52:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.39
[32m[20221213 23:52:46 @agent_ppo2.py:143][0m Total time:      40.23 min
[32m[20221213 23:52:46 @agent_ppo2.py:145][0m 3903488 total steps have happened
[32m[20221213 23:52:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3906 --------------------------#
[32m[20221213 23:52:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:46 @agent_ppo2.py:185][0m |           0.0001 |          64.7141 |          10.4132 |
[32m[20221213 23:52:46 @agent_ppo2.py:185][0m |          -0.0076 |          62.0520 |          10.4897 |
[32m[20221213 23:52:46 @agent_ppo2.py:185][0m |          -0.0080 |          61.2529 |          10.4460 |
[32m[20221213 23:52:46 @agent_ppo2.py:185][0m |          -0.0114 |          60.5257 |          10.5018 |
[32m[20221213 23:52:46 @agent_ppo2.py:185][0m |          -0.0098 |          60.0133 |          10.4428 |
[32m[20221213 23:52:47 @agent_ppo2.py:185][0m |          -0.0125 |          59.6891 |          10.5052 |
[32m[20221213 23:52:47 @agent_ppo2.py:185][0m |          -0.0095 |          59.3736 |          10.5559 |
[32m[20221213 23:52:47 @agent_ppo2.py:185][0m |          -0.0060 |          61.2632 |          10.5497 |
[32m[20221213 23:52:47 @agent_ppo2.py:185][0m |          -0.0146 |          58.7202 |          10.5596 |
[32m[20221213 23:52:47 @agent_ppo2.py:185][0m |          -0.0157 |          58.2960 |          10.5825 |
[32m[20221213 23:52:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.11
[32m[20221213 23:52:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.38
[32m[20221213 23:52:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.65
[32m[20221213 23:52:47 @agent_ppo2.py:143][0m Total time:      40.25 min
[32m[20221213 23:52:47 @agent_ppo2.py:145][0m 3905536 total steps have happened
[32m[20221213 23:52:47 @agent_ppo2.py:121][0m #------------------------ Iteration 3907 --------------------------#
[32m[20221213 23:52:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:47 @agent_ppo2.py:185][0m |           0.0006 |          51.7823 |          10.9010 |
[32m[20221213 23:52:47 @agent_ppo2.py:185][0m |          -0.0040 |          47.8275 |          10.8890 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0098 |          46.0055 |          10.8885 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0106 |          44.8287 |          10.9294 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0086 |          44.4900 |          10.9088 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0200 |          43.5657 |          10.8907 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0204 |          42.7838 |          10.8702 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0070 |          42.3803 |          10.8295 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0141 |          42.0198 |          10.8394 |
[32m[20221213 23:52:48 @agent_ppo2.py:185][0m |          -0.0167 |          42.6995 |          10.8427 |
[32m[20221213 23:52:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.54
[32m[20221213 23:52:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.68
[32m[20221213 23:52:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.54
[32m[20221213 23:52:48 @agent_ppo2.py:143][0m Total time:      40.27 min
[32m[20221213 23:52:48 @agent_ppo2.py:145][0m 3907584 total steps have happened
[32m[20221213 23:52:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3908 --------------------------#
[32m[20221213 23:52:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |           0.0105 |          78.2337 |          10.7258 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |          -0.0046 |          72.6876 |          10.7980 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |           0.0016 |          74.3790 |          10.8050 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |          -0.0072 |          71.5749 |          10.8215 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |           0.0042 |          77.8340 |          10.8668 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |          -0.0068 |          71.1047 |          10.8101 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |          -0.0120 |          70.7491 |          10.8716 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |          -0.0012 |          78.1879 |          10.8654 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |          -0.0114 |          70.6515 |          10.8826 |
[32m[20221213 23:52:49 @agent_ppo2.py:185][0m |          -0.0124 |          70.2050 |          10.8801 |
[32m[20221213 23:52:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.68
[32m[20221213 23:52:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.21
[32m[20221213 23:52:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.41
[32m[20221213 23:52:49 @agent_ppo2.py:143][0m Total time:      40.29 min
[32m[20221213 23:52:49 @agent_ppo2.py:145][0m 3909632 total steps have happened
[32m[20221213 23:52:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3909 --------------------------#
[32m[20221213 23:52:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |          -0.0027 |          77.4942 |          11.0522 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |          -0.0026 |          71.3342 |          11.0509 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |          -0.0112 |          69.2373 |          11.0938 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |          -0.0066 |          70.0522 |          11.0581 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |           0.0022 |          76.4526 |          11.1027 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |           0.0029 |          73.5453 |          11.1105 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |          -0.0140 |          65.9713 |          11.1291 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |          -0.0151 |          65.5069 |          11.1993 |
[32m[20221213 23:52:50 @agent_ppo2.py:185][0m |          -0.0181 |          65.3259 |          11.1931 |
[32m[20221213 23:52:51 @agent_ppo2.py:185][0m |          -0.0142 |          64.8465 |          11.2021 |
[32m[20221213 23:52:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:52:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.53
[32m[20221213 23:52:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.32
[32m[20221213 23:52:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.47
[32m[20221213 23:52:51 @agent_ppo2.py:143][0m Total time:      40.31 min
[32m[20221213 23:52:51 @agent_ppo2.py:145][0m 3911680 total steps have happened
[32m[20221213 23:52:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3910 --------------------------#
[32m[20221213 23:52:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:52:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:51 @agent_ppo2.py:185][0m |           0.0000 |          65.1162 |          10.9911 |
[32m[20221213 23:52:51 @agent_ppo2.py:185][0m |          -0.0056 |          60.4015 |          11.0259 |
[32m[20221213 23:52:51 @agent_ppo2.py:185][0m |          -0.0128 |          60.1457 |          11.0212 |
[32m[20221213 23:52:51 @agent_ppo2.py:185][0m |          -0.0118 |          58.7321 |          11.0435 |
[32m[20221213 23:52:51 @agent_ppo2.py:185][0m |          -0.0154 |          58.1750 |          11.0286 |
[32m[20221213 23:52:51 @agent_ppo2.py:185][0m |          -0.0043 |          62.6143 |          11.0439 |
[32m[20221213 23:52:52 @agent_ppo2.py:185][0m |          -0.0122 |          57.4901 |          11.0681 |
[32m[20221213 23:52:52 @agent_ppo2.py:185][0m |          -0.0120 |          57.3897 |          11.0555 |
[32m[20221213 23:52:52 @agent_ppo2.py:185][0m |          -0.0150 |          57.0826 |          11.0498 |
[32m[20221213 23:52:52 @agent_ppo2.py:185][0m |          -0.0149 |          56.8224 |          11.0738 |
[32m[20221213 23:52:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.81
[32m[20221213 23:52:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.09
[32m[20221213 23:52:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.30
[32m[20221213 23:52:52 @agent_ppo2.py:143][0m Total time:      40.33 min
[32m[20221213 23:52:52 @agent_ppo2.py:145][0m 3913728 total steps have happened
[32m[20221213 23:52:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3911 --------------------------#
[32m[20221213 23:52:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:52 @agent_ppo2.py:185][0m |           0.0034 |          74.6389 |          11.1044 |
[32m[20221213 23:52:52 @agent_ppo2.py:185][0m |          -0.0020 |          70.4074 |          11.0993 |
[32m[20221213 23:52:52 @agent_ppo2.py:185][0m |          -0.0099 |          68.9697 |          11.1143 |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0093 |          68.5383 |          11.0926 |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0063 |          68.5663 |          11.1005 |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0148 |          67.5860 |          11.0802 |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0140 |          67.3923 |          11.1227 |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0090 |          67.0310 |          11.1082 |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0123 |          66.6938 |          11.1461 |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0142 |          66.2557 |          11.0860 |
[32m[20221213 23:52:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.05
[32m[20221213 23:52:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.51
[32m[20221213 23:52:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.48
[32m[20221213 23:52:53 @agent_ppo2.py:143][0m Total time:      40.35 min
[32m[20221213 23:52:53 @agent_ppo2.py:145][0m 3915776 total steps have happened
[32m[20221213 23:52:53 @agent_ppo2.py:121][0m #------------------------ Iteration 3912 --------------------------#
[32m[20221213 23:52:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:53 @agent_ppo2.py:185][0m |          -0.0014 |          48.3828 |          11.1732 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |           0.0031 |          41.8200 |          11.1252 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0081 |          38.7395 |          11.2391 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0065 |          37.7036 |          11.1864 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0073 |          36.8836 |          11.1819 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0130 |          36.3583 |          11.1832 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0136 |          35.4196 |          11.1681 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0104 |          34.9472 |          11.2085 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0143 |          34.4079 |          11.1731 |
[32m[20221213 23:52:54 @agent_ppo2.py:185][0m |          -0.0171 |          34.0658 |          11.2056 |
[32m[20221213 23:52:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.19
[32m[20221213 23:52:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.31
[32m[20221213 23:52:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.23
[32m[20221213 23:52:54 @agent_ppo2.py:143][0m Total time:      40.37 min
[32m[20221213 23:52:54 @agent_ppo2.py:145][0m 3917824 total steps have happened
[32m[20221213 23:52:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3913 --------------------------#
[32m[20221213 23:52:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |           0.0113 |          61.6225 |          11.0900 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0015 |          54.4337 |          11.0975 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0071 |          51.6770 |          11.1205 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0084 |          50.8555 |          11.0762 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0117 |          50.2988 |          11.1065 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0109 |          50.0015 |          11.0657 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0125 |          49.7420 |          11.0719 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0097 |          50.9092 |          11.0657 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0121 |          49.0168 |          11.0495 |
[32m[20221213 23:52:55 @agent_ppo2.py:185][0m |          -0.0134 |          48.7355 |          11.0222 |
[32m[20221213 23:52:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.08
[32m[20221213 23:52:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.41
[32m[20221213 23:52:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.93
[32m[20221213 23:52:56 @agent_ppo2.py:143][0m Total time:      40.39 min
[32m[20221213 23:52:56 @agent_ppo2.py:145][0m 3919872 total steps have happened
[32m[20221213 23:52:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3914 --------------------------#
[32m[20221213 23:52:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:56 @agent_ppo2.py:185][0m |          -0.0010 |          71.4290 |          11.2398 |
[32m[20221213 23:52:56 @agent_ppo2.py:185][0m |          -0.0054 |          67.9032 |          11.2673 |
[32m[20221213 23:52:56 @agent_ppo2.py:185][0m |          -0.0086 |          67.0809 |          11.2858 |
[32m[20221213 23:52:56 @agent_ppo2.py:185][0m |          -0.0094 |          66.4522 |          11.2944 |
[32m[20221213 23:52:56 @agent_ppo2.py:185][0m |          -0.0112 |          66.3019 |          11.2684 |
[32m[20221213 23:52:56 @agent_ppo2.py:185][0m |          -0.0098 |          66.1315 |          11.2980 |
[32m[20221213 23:52:56 @agent_ppo2.py:185][0m |          -0.0095 |          65.5288 |          11.2990 |
[32m[20221213 23:52:57 @agent_ppo2.py:185][0m |          -0.0091 |          65.6434 |          11.3114 |
[32m[20221213 23:52:57 @agent_ppo2.py:185][0m |          -0.0045 |          66.7675 |          11.2848 |
[32m[20221213 23:52:57 @agent_ppo2.py:185][0m |          -0.0165 |          65.3246 |          11.3127 |
[32m[20221213 23:52:57 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:52:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.80
[32m[20221213 23:52:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.05
[32m[20221213 23:52:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.96
[32m[20221213 23:52:57 @agent_ppo2.py:143][0m Total time:      40.41 min
[32m[20221213 23:52:57 @agent_ppo2.py:145][0m 3921920 total steps have happened
[32m[20221213 23:52:57 @agent_ppo2.py:121][0m #------------------------ Iteration 3915 --------------------------#
[32m[20221213 23:52:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:57 @agent_ppo2.py:185][0m |           0.0003 |          74.6092 |          11.1507 |
[32m[20221213 23:52:57 @agent_ppo2.py:185][0m |          -0.0034 |          72.6426 |          11.0931 |
[32m[20221213 23:52:57 @agent_ppo2.py:185][0m |          -0.0083 |          71.9656 |          11.1590 |
[32m[20221213 23:52:57 @agent_ppo2.py:185][0m |          -0.0103 |          71.4196 |          11.1766 |
[32m[20221213 23:52:58 @agent_ppo2.py:185][0m |          -0.0107 |          71.4474 |          11.2367 |
[32m[20221213 23:52:58 @agent_ppo2.py:185][0m |          -0.0005 |          77.0022 |          11.2224 |
[32m[20221213 23:52:58 @agent_ppo2.py:185][0m |          -0.0103 |          70.6874 |          11.3156 |
[32m[20221213 23:52:58 @agent_ppo2.py:185][0m |          -0.0138 |          70.4523 |          11.2964 |
[32m[20221213 23:52:58 @agent_ppo2.py:185][0m |          -0.0128 |          70.2303 |          11.3300 |
[32m[20221213 23:52:58 @agent_ppo2.py:185][0m |          -0.0142 |          69.9591 |          11.3302 |
[32m[20221213 23:52:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:52:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.67
[32m[20221213 23:52:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.00
[32m[20221213 23:52:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.06
[32m[20221213 23:52:58 @agent_ppo2.py:143][0m Total time:      40.44 min
[32m[20221213 23:52:58 @agent_ppo2.py:145][0m 3923968 total steps have happened
[32m[20221213 23:52:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3916 --------------------------#
[32m[20221213 23:52:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:52:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:52:58 @agent_ppo2.py:185][0m |           0.0018 |          41.9745 |          11.1382 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0027 |          37.6741 |          11.1756 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0075 |          36.2376 |          11.2003 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0050 |          35.4205 |          11.2226 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0045 |          34.9558 |          11.2037 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0111 |          34.4636 |          11.2660 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0035 |          40.6487 |          11.2509 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0166 |          33.9950 |          11.2694 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0139 |          33.4663 |          11.2817 |
[32m[20221213 23:52:59 @agent_ppo2.py:185][0m |          -0.0141 |          33.0944 |          11.3100 |
[32m[20221213 23:52:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:52:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.39
[32m[20221213 23:52:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.74
[32m[20221213 23:52:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.86
[32m[20221213 23:52:59 @agent_ppo2.py:143][0m Total time:      40.46 min
[32m[20221213 23:52:59 @agent_ppo2.py:145][0m 3926016 total steps have happened
[32m[20221213 23:52:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3917 --------------------------#
[32m[20221213 23:53:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |           0.0023 |          68.7148 |          11.9000 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0049 |          66.6693 |          11.8960 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0109 |          65.7435 |          11.8932 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0096 |          65.2545 |          11.9097 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0101 |          65.0840 |          11.9195 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0115 |          64.6691 |          11.8824 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0090 |          64.7688 |          11.9058 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0155 |          64.5254 |          11.9567 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0148 |          64.1501 |          11.9622 |
[32m[20221213 23:53:00 @agent_ppo2.py:185][0m |          -0.0155 |          64.2573 |          11.9364 |
[32m[20221213 23:53:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.99
[32m[20221213 23:53:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.49
[32m[20221213 23:53:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.52
[32m[20221213 23:53:01 @agent_ppo2.py:143][0m Total time:      40.48 min
[32m[20221213 23:53:01 @agent_ppo2.py:145][0m 3928064 total steps have happened
[32m[20221213 23:53:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3918 --------------------------#
[32m[20221213 23:53:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0010 |          65.2553 |          11.8967 |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0071 |          62.7410 |          11.9244 |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0046 |          64.4396 |          12.0033 |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0133 |          61.5372 |          11.9287 |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0120 |          61.1163 |          11.9871 |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0139 |          60.8878 |          11.9479 |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0077 |          61.4856 |          11.9700 |
[32m[20221213 23:53:01 @agent_ppo2.py:185][0m |          -0.0125 |          60.3343 |          11.9477 |
[32m[20221213 23:53:02 @agent_ppo2.py:185][0m |          -0.0135 |          60.1873 |          11.9754 |
[32m[20221213 23:53:02 @agent_ppo2.py:185][0m |          -0.0136 |          60.0399 |          11.9718 |
[32m[20221213 23:53:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 23:53:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.03
[32m[20221213 23:53:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.02
[32m[20221213 23:53:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.48
[32m[20221213 23:53:02 @agent_ppo2.py:143][0m Total time:      40.50 min
[32m[20221213 23:53:02 @agent_ppo2.py:145][0m 3930112 total steps have happened
[32m[20221213 23:53:02 @agent_ppo2.py:121][0m #------------------------ Iteration 3919 --------------------------#
[32m[20221213 23:53:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:02 @agent_ppo2.py:185][0m |          -0.0003 |          70.1315 |          11.7329 |
[32m[20221213 23:53:02 @agent_ppo2.py:185][0m |           0.0073 |          76.4703 |          11.7703 |
[32m[20221213 23:53:02 @agent_ppo2.py:185][0m |          -0.0030 |          66.6309 |          11.7366 |
[32m[20221213 23:53:02 @agent_ppo2.py:185][0m |          -0.0098 |          64.5708 |          11.7662 |
[32m[20221213 23:53:02 @agent_ppo2.py:185][0m |          -0.0092 |          65.0324 |          11.7781 |
[32m[20221213 23:53:03 @agent_ppo2.py:185][0m |          -0.0044 |          67.1860 |          11.7508 |
[32m[20221213 23:53:03 @agent_ppo2.py:185][0m |          -0.0162 |          63.3839 |          11.7720 |
[32m[20221213 23:53:03 @agent_ppo2.py:185][0m |           0.0006 |          70.3935 |          11.7742 |
[32m[20221213 23:53:03 @agent_ppo2.py:185][0m |          -0.0123 |          62.9652 |          11.7449 |
[32m[20221213 23:53:03 @agent_ppo2.py:185][0m |          -0.0135 |          62.1640 |          11.7688 |
[32m[20221213 23:53:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:53:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.25
[32m[20221213 23:53:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.15
[32m[20221213 23:53:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.18
[32m[20221213 23:53:03 @agent_ppo2.py:143][0m Total time:      40.52 min
[32m[20221213 23:53:03 @agent_ppo2.py:145][0m 3932160 total steps have happened
[32m[20221213 23:53:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3920 --------------------------#
[32m[20221213 23:53:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:53:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:03 @agent_ppo2.py:185][0m |           0.0009 |          62.9597 |          11.4487 |
[32m[20221213 23:53:03 @agent_ppo2.py:185][0m |          -0.0069 |          58.2039 |          11.4417 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0019 |          57.0593 |          11.4369 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0099 |          55.9556 |          11.4392 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0051 |          56.0228 |          11.4283 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0148 |          54.7752 |          11.4616 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0111 |          54.4957 |          11.4607 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0178 |          54.0754 |          11.4413 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0195 |          53.6707 |          11.4831 |
[32m[20221213 23:53:04 @agent_ppo2.py:185][0m |          -0.0188 |          53.4251 |          11.4476 |
[32m[20221213 23:53:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.89
[32m[20221213 23:53:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.30
[32m[20221213 23:53:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.22
[32m[20221213 23:53:04 @agent_ppo2.py:143][0m Total time:      40.54 min
[32m[20221213 23:53:04 @agent_ppo2.py:145][0m 3934208 total steps have happened
[32m[20221213 23:53:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3921 --------------------------#
[32m[20221213 23:53:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0034 |          67.5086 |          11.5205 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0086 |          64.0697 |          11.5978 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0089 |          62.3927 |          11.5584 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0098 |          61.5275 |          11.5370 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0120 |          60.5275 |          11.5730 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0082 |          60.0760 |          11.5361 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0042 |          63.1942 |          11.5746 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0141 |          59.4842 |          11.5333 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0088 |          59.7841 |          11.5302 |
[32m[20221213 23:53:05 @agent_ppo2.py:185][0m |          -0.0145 |          58.7106 |          11.5484 |
[32m[20221213 23:53:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.13
[32m[20221213 23:53:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.53
[32m[20221213 23:53:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.00
[32m[20221213 23:53:06 @agent_ppo2.py:143][0m Total time:      40.56 min
[32m[20221213 23:53:06 @agent_ppo2.py:145][0m 3936256 total steps have happened
[32m[20221213 23:53:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3922 --------------------------#
[32m[20221213 23:53:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |           0.0030 |          54.2402 |          11.5391 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |          -0.0046 |          50.5411 |          11.5270 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |           0.0066 |          55.1578 |          11.5599 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |          -0.0102 |          48.7152 |          11.5154 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |          -0.0028 |          52.3326 |          11.5406 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |          -0.0011 |          50.1802 |          11.5469 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |          -0.0037 |          48.4680 |          11.5349 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |          -0.0138 |          46.8012 |          11.5326 |
[32m[20221213 23:53:06 @agent_ppo2.py:185][0m |          -0.0146 |          46.5179 |          11.5298 |
[32m[20221213 23:53:07 @agent_ppo2.py:185][0m |          -0.0167 |          46.1183 |          11.5062 |
[32m[20221213 23:53:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.59
[32m[20221213 23:53:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.46
[32m[20221213 23:53:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.26
[32m[20221213 23:53:07 @agent_ppo2.py:143][0m Total time:      40.58 min
[32m[20221213 23:53:07 @agent_ppo2.py:145][0m 3938304 total steps have happened
[32m[20221213 23:53:07 @agent_ppo2.py:121][0m #------------------------ Iteration 3923 --------------------------#
[32m[20221213 23:53:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:07 @agent_ppo2.py:185][0m |          -0.0024 |          62.8592 |          11.1847 |
[32m[20221213 23:53:07 @agent_ppo2.py:185][0m |          -0.0052 |          58.0936 |          11.2092 |
[32m[20221213 23:53:07 @agent_ppo2.py:185][0m |          -0.0057 |          56.0763 |          11.2244 |
[32m[20221213 23:53:07 @agent_ppo2.py:185][0m |          -0.0146 |          55.4033 |          11.2225 |
[32m[20221213 23:53:07 @agent_ppo2.py:185][0m |          -0.0175 |          54.9136 |          11.2324 |
[32m[20221213 23:53:07 @agent_ppo2.py:185][0m |          -0.0158 |          54.5173 |          11.2695 |
[32m[20221213 23:53:08 @agent_ppo2.py:185][0m |          -0.0156 |          54.2762 |          11.2947 |
[32m[20221213 23:53:08 @agent_ppo2.py:185][0m |          -0.0175 |          53.9050 |          11.3055 |
[32m[20221213 23:53:08 @agent_ppo2.py:185][0m |          -0.0173 |          53.6682 |          11.2838 |
[32m[20221213 23:53:08 @agent_ppo2.py:185][0m |          -0.0120 |          55.2496 |          11.3252 |
[32m[20221213 23:53:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.84
[32m[20221213 23:53:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.01
[32m[20221213 23:53:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.41
[32m[20221213 23:53:08 @agent_ppo2.py:143][0m Total time:      40.60 min
[32m[20221213 23:53:08 @agent_ppo2.py:145][0m 3940352 total steps have happened
[32m[20221213 23:53:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3924 --------------------------#
[32m[20221213 23:53:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:53:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:08 @agent_ppo2.py:185][0m |           0.0021 |          21.6373 |          11.7698 |
[32m[20221213 23:53:08 @agent_ppo2.py:185][0m |           0.0016 |          18.1728 |          11.7877 |
[32m[20221213 23:53:08 @agent_ppo2.py:185][0m |          -0.0041 |          17.4654 |          11.8037 |
[32m[20221213 23:53:09 @agent_ppo2.py:185][0m |          -0.0030 |          17.4322 |          11.7791 |
[32m[20221213 23:53:09 @agent_ppo2.py:185][0m |          -0.0047 |          17.3006 |          11.8372 |
[32m[20221213 23:53:09 @agent_ppo2.py:185][0m |          -0.0035 |          17.2737 |          11.8114 |
[32m[20221213 23:53:09 @agent_ppo2.py:185][0m |          -0.0031 |          17.2660 |          11.8403 |
[32m[20221213 23:53:09 @agent_ppo2.py:185][0m |          -0.0031 |          17.4196 |          11.8526 |
[32m[20221213 23:53:09 @agent_ppo2.py:185][0m |          -0.0003 |          17.2009 |          11.8674 |
[32m[20221213 23:53:09 @agent_ppo2.py:185][0m |          -0.0018 |          17.3485 |          11.8989 |
[32m[20221213 23:53:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:53:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:53:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:53:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.64
[32m[20221213 23:53:09 @agent_ppo2.py:143][0m Total time:      40.62 min
[32m[20221213 23:53:09 @agent_ppo2.py:145][0m 3942400 total steps have happened
[32m[20221213 23:53:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3925 --------------------------#
[32m[20221213 23:53:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0052 |          60.7790 |          11.9425 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |           0.0010 |          58.3763 |          11.9664 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0074 |          57.0864 |          11.9710 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0022 |          58.2997 |          11.9627 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0075 |          56.5042 |          11.9736 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0025 |          58.5691 |          11.9638 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0087 |          56.1366 |          11.9572 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0095 |          56.0532 |          11.9774 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0111 |          55.9264 |          11.9663 |
[32m[20221213 23:53:10 @agent_ppo2.py:185][0m |          -0.0107 |          55.7936 |          11.9608 |
[32m[20221213 23:53:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.33
[32m[20221213 23:53:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.75
[32m[20221213 23:53:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.71
[32m[20221213 23:53:10 @agent_ppo2.py:143][0m Total time:      40.64 min
[32m[20221213 23:53:10 @agent_ppo2.py:145][0m 3944448 total steps have happened
[32m[20221213 23:53:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3926 --------------------------#
[32m[20221213 23:53:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |           0.0032 |          47.7092 |          11.7468 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0015 |          41.6384 |          11.7924 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0049 |          40.6696 |          11.7675 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0078 |          39.7979 |          11.7823 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0080 |          39.3570 |          11.7726 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0116 |          38.9801 |          11.8158 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0177 |          38.7417 |          11.7918 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0117 |          38.5028 |          11.8193 |
[32m[20221213 23:53:11 @agent_ppo2.py:185][0m |          -0.0069 |          41.1913 |          11.8381 |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |          -0.0132 |          38.2789 |          11.8115 |
[32m[20221213 23:53:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.10
[32m[20221213 23:53:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.49
[32m[20221213 23:53:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.01
[32m[20221213 23:53:12 @agent_ppo2.py:143][0m Total time:      40.66 min
[32m[20221213 23:53:12 @agent_ppo2.py:145][0m 3946496 total steps have happened
[32m[20221213 23:53:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3927 --------------------------#
[32m[20221213 23:53:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |           0.0033 |          60.8927 |          11.6263 |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |           0.0041 |          57.5530 |          11.6337 |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |          -0.0072 |          55.1064 |          11.6203 |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |          -0.0080 |          54.2513 |          11.6349 |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |          -0.0099 |          53.2803 |          11.6298 |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |          -0.0058 |          53.7088 |          11.6343 |
[32m[20221213 23:53:12 @agent_ppo2.py:185][0m |          -0.0129 |          52.6283 |          11.6660 |
[32m[20221213 23:53:13 @agent_ppo2.py:185][0m |          -0.0113 |          51.9724 |          11.6334 |
[32m[20221213 23:53:13 @agent_ppo2.py:185][0m |          -0.0139 |          51.4635 |          11.6292 |
[32m[20221213 23:53:13 @agent_ppo2.py:185][0m |          -0.0142 |          51.3311 |          11.6357 |
[32m[20221213 23:53:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.08
[32m[20221213 23:53:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.29
[32m[20221213 23:53:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.44
[32m[20221213 23:53:13 @agent_ppo2.py:143][0m Total time:      40.68 min
[32m[20221213 23:53:13 @agent_ppo2.py:145][0m 3948544 total steps have happened
[32m[20221213 23:53:13 @agent_ppo2.py:121][0m #------------------------ Iteration 3928 --------------------------#
[32m[20221213 23:53:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:13 @agent_ppo2.py:185][0m |          -0.0013 |          60.8198 |          11.8417 |
[32m[20221213 23:53:13 @agent_ppo2.py:185][0m |          -0.0029 |          58.7515 |          11.8711 |
[32m[20221213 23:53:13 @agent_ppo2.py:185][0m |          -0.0011 |          58.4200 |          11.8732 |
[32m[20221213 23:53:13 @agent_ppo2.py:185][0m |          -0.0150 |          53.3513 |          11.8543 |
[32m[20221213 23:53:14 @agent_ppo2.py:185][0m |          -0.0174 |          52.4464 |          11.8556 |
[32m[20221213 23:53:14 @agent_ppo2.py:185][0m |          -0.0173 |          51.9426 |          11.8407 |
[32m[20221213 23:53:14 @agent_ppo2.py:185][0m |          -0.0174 |          51.2783 |          11.8266 |
[32m[20221213 23:53:14 @agent_ppo2.py:185][0m |          -0.0162 |          51.3564 |          11.7990 |
[32m[20221213 23:53:14 @agent_ppo2.py:185][0m |          -0.0157 |          50.4555 |          11.8156 |
[32m[20221213 23:53:14 @agent_ppo2.py:185][0m |          -0.0176 |          50.5980 |          11.8104 |
[32m[20221213 23:53:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:53:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.53
[32m[20221213 23:53:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.67
[32m[20221213 23:53:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:53:14 @agent_ppo2.py:143][0m Total time:      40.70 min
[32m[20221213 23:53:14 @agent_ppo2.py:145][0m 3950592 total steps have happened
[32m[20221213 23:53:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3929 --------------------------#
[32m[20221213 23:53:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:14 @agent_ppo2.py:185][0m |           0.0006 |          64.2650 |          11.8444 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |          -0.0062 |          59.8335 |          11.7885 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |           0.0036 |          60.1867 |          11.7793 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |          -0.0057 |          57.7211 |          11.7862 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |          -0.0097 |          57.0699 |          11.7880 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |           0.0020 |          65.8955 |          11.8175 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |          -0.0081 |          56.5347 |          11.8367 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |          -0.0184 |          55.8438 |          11.8236 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |          -0.0147 |          55.7329 |          11.8074 |
[32m[20221213 23:53:15 @agent_ppo2.py:185][0m |          -0.0161 |          55.4239 |          11.8181 |
[32m[20221213 23:53:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:53:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.53
[32m[20221213 23:53:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.97
[32m[20221213 23:53:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.89
[32m[20221213 23:53:15 @agent_ppo2.py:143][0m Total time:      40.72 min
[32m[20221213 23:53:15 @agent_ppo2.py:145][0m 3952640 total steps have happened
[32m[20221213 23:53:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3930 --------------------------#
[32m[20221213 23:53:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:53:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |           0.0002 |          61.4085 |          12.0693 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0064 |          55.9055 |          12.1126 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0007 |          54.6694 |          12.1057 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0080 |          52.4104 |          12.1451 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0056 |          51.9051 |          12.1582 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0100 |          50.7096 |          12.1957 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0126 |          50.2299 |          12.1723 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0127 |          49.7235 |          12.1887 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0162 |          49.2302 |          12.2107 |
[32m[20221213 23:53:16 @agent_ppo2.py:185][0m |          -0.0117 |          49.1672 |          12.2119 |
[32m[20221213 23:53:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.04
[32m[20221213 23:53:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.16
[32m[20221213 23:53:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.07
[32m[20221213 23:53:17 @agent_ppo2.py:143][0m Total time:      40.74 min
[32m[20221213 23:53:17 @agent_ppo2.py:145][0m 3954688 total steps have happened
[32m[20221213 23:53:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3931 --------------------------#
[32m[20221213 23:53:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:17 @agent_ppo2.py:185][0m |          -0.0004 |          63.9993 |          11.7470 |
[32m[20221213 23:53:17 @agent_ppo2.py:185][0m |          -0.0044 |          58.9940 |          11.7332 |
[32m[20221213 23:53:17 @agent_ppo2.py:185][0m |          -0.0121 |          56.1727 |          11.7619 |
[32m[20221213 23:53:17 @agent_ppo2.py:185][0m |          -0.0103 |          54.9722 |          11.7350 |
[32m[20221213 23:53:17 @agent_ppo2.py:185][0m |          -0.0156 |          53.7175 |          11.7825 |
[32m[20221213 23:53:17 @agent_ppo2.py:185][0m |          -0.0141 |          53.1553 |          11.7029 |
[32m[20221213 23:53:17 @agent_ppo2.py:185][0m |          -0.0174 |          52.4362 |          11.7702 |
[32m[20221213 23:53:18 @agent_ppo2.py:185][0m |          -0.0167 |          51.7795 |          11.7811 |
[32m[20221213 23:53:18 @agent_ppo2.py:185][0m |          -0.0164 |          51.1657 |          11.8064 |
[32m[20221213 23:53:18 @agent_ppo2.py:185][0m |          -0.0132 |          50.9463 |          11.7911 |
[32m[20221213 23:53:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.16
[32m[20221213 23:53:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.64
[32m[20221213 23:53:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.95
[32m[20221213 23:53:18 @agent_ppo2.py:143][0m Total time:      40.76 min
[32m[20221213 23:53:18 @agent_ppo2.py:145][0m 3956736 total steps have happened
[32m[20221213 23:53:18 @agent_ppo2.py:121][0m #------------------------ Iteration 3932 --------------------------#
[32m[20221213 23:53:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:18 @agent_ppo2.py:185][0m |           0.0019 |          61.4238 |          12.1142 |
[32m[20221213 23:53:18 @agent_ppo2.py:185][0m |          -0.0063 |          52.1111 |          12.0915 |
[32m[20221213 23:53:18 @agent_ppo2.py:185][0m |          -0.0008 |          48.6784 |          12.0978 |
[32m[20221213 23:53:18 @agent_ppo2.py:185][0m |          -0.0119 |          46.6739 |          12.0984 |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0125 |          45.2218 |          12.0725 |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0117 |          44.2709 |          12.1201 |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0084 |          43.3087 |          12.1282 |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0115 |          42.8354 |          12.1212 |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0097 |          42.5522 |          12.1383 |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0125 |          41.8274 |          12.1475 |
[32m[20221213 23:53:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.63
[32m[20221213 23:53:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.60
[32m[20221213 23:53:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.11
[32m[20221213 23:53:19 @agent_ppo2.py:143][0m Total time:      40.78 min
[32m[20221213 23:53:19 @agent_ppo2.py:145][0m 3958784 total steps have happened
[32m[20221213 23:53:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3933 --------------------------#
[32m[20221213 23:53:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0011 |          77.0407 |          12.1224 |
[32m[20221213 23:53:19 @agent_ppo2.py:185][0m |          -0.0043 |          73.8686 |          12.1597 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |          -0.0085 |          72.6145 |          12.2015 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |          -0.0068 |          71.8997 |          12.2281 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |          -0.0091 |          71.1527 |          12.2425 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |           0.0026 |          75.6881 |          12.2969 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |          -0.0092 |          70.7478 |          12.2988 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |           0.0102 |          78.1118 |          12.3062 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |          -0.0122 |          70.4322 |          12.3177 |
[32m[20221213 23:53:20 @agent_ppo2.py:185][0m |          -0.0129 |          69.9422 |          12.3399 |
[32m[20221213 23:53:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:53:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.57
[32m[20221213 23:53:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.08
[32m[20221213 23:53:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.29
[32m[20221213 23:53:20 @agent_ppo2.py:143][0m Total time:      40.81 min
[32m[20221213 23:53:20 @agent_ppo2.py:145][0m 3960832 total steps have happened
[32m[20221213 23:53:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3934 --------------------------#
[32m[20221213 23:53:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |           0.0012 |          49.2240 |          12.5323 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0034 |          40.6785 |          12.5655 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0079 |          38.4271 |          12.5354 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0092 |          37.2871 |          12.5733 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0128 |          36.6654 |          12.5671 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0110 |          36.0558 |          12.5972 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0094 |          35.5265 |          12.5750 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0174 |          34.8479 |          12.5703 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0133 |          34.4755 |          12.5682 |
[32m[20221213 23:53:21 @agent_ppo2.py:185][0m |          -0.0143 |          33.9809 |          12.5640 |
[32m[20221213 23:53:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:53:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.07
[32m[20221213 23:53:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.37
[32m[20221213 23:53:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.34
[32m[20221213 23:53:22 @agent_ppo2.py:143][0m Total time:      40.83 min
[32m[20221213 23:53:22 @agent_ppo2.py:145][0m 3962880 total steps have happened
[32m[20221213 23:53:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3935 --------------------------#
[32m[20221213 23:53:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |           0.0018 |          66.1010 |          12.3106 |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |          -0.0028 |          61.4981 |          12.3488 |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |          -0.0096 |          60.4238 |          12.3041 |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |          -0.0104 |          59.5154 |          12.3282 |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |          -0.0114 |          59.1720 |          12.3264 |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |          -0.0116 |          58.7502 |          12.3069 |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |          -0.0126 |          58.4334 |          12.3140 |
[32m[20221213 23:53:22 @agent_ppo2.py:185][0m |          -0.0146 |          58.4577 |          12.3077 |
[32m[20221213 23:53:23 @agent_ppo2.py:185][0m |          -0.0124 |          58.3845 |          12.3464 |
[32m[20221213 23:53:23 @agent_ppo2.py:185][0m |          -0.0178 |          58.0920 |          12.3268 |
[32m[20221213 23:53:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.54
[32m[20221213 23:53:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.64
[32m[20221213 23:53:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.32
[32m[20221213 23:53:23 @agent_ppo2.py:143][0m Total time:      40.85 min
[32m[20221213 23:53:23 @agent_ppo2.py:145][0m 3964928 total steps have happened
[32m[20221213 23:53:23 @agent_ppo2.py:121][0m #------------------------ Iteration 3936 --------------------------#
[32m[20221213 23:53:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:23 @agent_ppo2.py:185][0m |           0.0068 |          37.0335 |          12.1686 |
[32m[20221213 23:53:23 @agent_ppo2.py:185][0m |          -0.0009 |          32.2660 |          12.2120 |
[32m[20221213 23:53:23 @agent_ppo2.py:185][0m |          -0.0081 |          30.9930 |          12.1956 |
[32m[20221213 23:53:23 @agent_ppo2.py:185][0m |          -0.0060 |          30.3079 |          12.1979 |
[32m[20221213 23:53:23 @agent_ppo2.py:185][0m |          -0.0122 |          30.0549 |          12.2136 |
[32m[20221213 23:53:24 @agent_ppo2.py:185][0m |          -0.0131 |          29.7045 |          12.2598 |
[32m[20221213 23:53:24 @agent_ppo2.py:185][0m |          -0.0108 |          29.5511 |          12.2393 |
[32m[20221213 23:53:24 @agent_ppo2.py:185][0m |          -0.0089 |          29.5322 |          12.2829 |
[32m[20221213 23:53:24 @agent_ppo2.py:185][0m |          -0.0071 |          30.1770 |          12.2888 |
[32m[20221213 23:53:24 @agent_ppo2.py:185][0m |          -0.0114 |          28.9270 |          12.2854 |
[32m[20221213 23:53:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.18
[32m[20221213 23:53:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.04
[32m[20221213 23:53:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.05
[32m[20221213 23:53:24 @agent_ppo2.py:143][0m Total time:      40.87 min
[32m[20221213 23:53:24 @agent_ppo2.py:145][0m 3966976 total steps have happened
[32m[20221213 23:53:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3937 --------------------------#
[32m[20221213 23:53:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:24 @agent_ppo2.py:185][0m |           0.0076 |          47.0149 |          12.2332 |
[32m[20221213 23:53:24 @agent_ppo2.py:185][0m |           0.0040 |          48.2972 |          12.2157 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0087 |          42.2368 |          12.2184 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0086 |          41.4076 |          12.2153 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0119 |          40.8939 |          12.2381 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0075 |          40.3593 |          12.2328 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0126 |          39.9157 |          12.2424 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0127 |          39.5007 |          12.2156 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0135 |          39.1844 |          12.2235 |
[32m[20221213 23:53:25 @agent_ppo2.py:185][0m |          -0.0180 |          39.4463 |          12.2480 |
[32m[20221213 23:53:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.97
[32m[20221213 23:53:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.18
[32m[20221213 23:53:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.54
[32m[20221213 23:53:25 @agent_ppo2.py:143][0m Total time:      40.89 min
[32m[20221213 23:53:25 @agent_ppo2.py:145][0m 3969024 total steps have happened
[32m[20221213 23:53:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3938 --------------------------#
[32m[20221213 23:53:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0003 |          53.0601 |          12.5653 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0060 |          48.5742 |          12.5939 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0066 |          46.6285 |          12.6126 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0079 |          45.6504 |          12.6550 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0092 |          44.7069 |          12.6397 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0091 |          44.1849 |          12.6851 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0131 |          43.6030 |          12.7063 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0144 |          43.4416 |          12.7043 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0066 |          43.9776 |          12.7353 |
[32m[20221213 23:53:26 @agent_ppo2.py:185][0m |          -0.0135 |          43.0919 |          12.7062 |
[32m[20221213 23:53:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:53:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.76
[32m[20221213 23:53:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.74
[32m[20221213 23:53:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.92
[32m[20221213 23:53:27 @agent_ppo2.py:143][0m Total time:      40.91 min
[32m[20221213 23:53:27 @agent_ppo2.py:145][0m 3971072 total steps have happened
[32m[20221213 23:53:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3939 --------------------------#
[32m[20221213 23:53:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:53:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |           0.0047 |          47.3793 |          12.6598 |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |          -0.0033 |          41.5224 |          12.6445 |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |          -0.0146 |          39.7164 |          12.6355 |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |          -0.0181 |          38.3857 |          12.6196 |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |          -0.0124 |          37.5961 |          12.5742 |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |          -0.0175 |          36.5901 |          12.5357 |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |          -0.0164 |          35.8549 |          12.5097 |
[32m[20221213 23:53:27 @agent_ppo2.py:185][0m |          -0.0188 |          35.7769 |          12.4810 |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |          -0.0198 |          35.2389 |          12.4660 |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |          -0.0224 |          34.5305 |          12.4338 |
[32m[20221213 23:53:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.02
[32m[20221213 23:53:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.80
[32m[20221213 23:53:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.55
[32m[20221213 23:53:28 @agent_ppo2.py:143][0m Total time:      40.93 min
[32m[20221213 23:53:28 @agent_ppo2.py:145][0m 3973120 total steps have happened
[32m[20221213 23:53:28 @agent_ppo2.py:121][0m #------------------------ Iteration 3940 --------------------------#
[32m[20221213 23:53:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:53:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |           0.0029 |          53.7732 |          11.7157 |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |          -0.0096 |          46.0785 |          11.6530 |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |          -0.0092 |          43.6255 |          11.6723 |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |          -0.0166 |          42.4105 |          11.6140 |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |          -0.0162 |          41.2499 |          11.5996 |
[32m[20221213 23:53:28 @agent_ppo2.py:185][0m |          -0.0164 |          40.4589 |          11.5857 |
[32m[20221213 23:53:29 @agent_ppo2.py:185][0m |          -0.0138 |          39.8176 |          11.5775 |
[32m[20221213 23:53:29 @agent_ppo2.py:185][0m |          -0.0171 |          39.1544 |          11.5220 |
[32m[20221213 23:53:29 @agent_ppo2.py:185][0m |          -0.0087 |          42.9406 |          11.5072 |
[32m[20221213 23:53:29 @agent_ppo2.py:185][0m |          -0.0232 |          38.4905 |          11.5042 |
[32m[20221213 23:53:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:53:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.64
[32m[20221213 23:53:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.38
[32m[20221213 23:53:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.44
[32m[20221213 23:53:29 @agent_ppo2.py:143][0m Total time:      40.95 min
[32m[20221213 23:53:29 @agent_ppo2.py:145][0m 3975168 total steps have happened
[32m[20221213 23:53:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3941 --------------------------#
[32m[20221213 23:53:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:29 @agent_ppo2.py:185][0m |           0.0020 |          59.2110 |          11.4102 |
[32m[20221213 23:53:29 @agent_ppo2.py:185][0m |          -0.0021 |          56.5262 |          11.3692 |
[32m[20221213 23:53:29 @agent_ppo2.py:185][0m |          -0.0074 |          54.9895 |          11.3643 |
[32m[20221213 23:53:30 @agent_ppo2.py:185][0m |           0.0038 |          61.2662 |          11.3701 |
[32m[20221213 23:53:30 @agent_ppo2.py:185][0m |          -0.0157 |          53.5628 |          11.3859 |
[32m[20221213 23:53:30 @agent_ppo2.py:185][0m |          -0.0105 |          53.4185 |          11.3375 |
[32m[20221213 23:53:30 @agent_ppo2.py:185][0m |          -0.0159 |          52.7299 |          11.3115 |
[32m[20221213 23:53:30 @agent_ppo2.py:185][0m |          -0.0078 |          53.1698 |          11.3435 |
[32m[20221213 23:53:30 @agent_ppo2.py:185][0m |          -0.0153 |          51.8972 |          11.3098 |
[32m[20221213 23:53:30 @agent_ppo2.py:185][0m |          -0.0136 |          51.9750 |          11.3153 |
[32m[20221213 23:53:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.37
[32m[20221213 23:53:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.86
[32m[20221213 23:53:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.15
[32m[20221213 23:53:30 @agent_ppo2.py:143][0m Total time:      40.97 min
[32m[20221213 23:53:30 @agent_ppo2.py:145][0m 3977216 total steps have happened
[32m[20221213 23:53:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3942 --------------------------#
[32m[20221213 23:53:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0006 |          72.9935 |          11.6099 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0071 |          67.3427 |          11.5454 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0060 |          65.6854 |          11.6201 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0127 |          64.7315 |          11.5886 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0099 |          64.4619 |          11.5680 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0007 |          69.4966 |          11.5740 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0101 |          63.4856 |          11.5926 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0124 |          62.7568 |          11.6255 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0110 |          62.7384 |          11.6138 |
[32m[20221213 23:53:31 @agent_ppo2.py:185][0m |          -0.0139 |          62.3661 |          11.6223 |
[32m[20221213 23:53:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.37
[32m[20221213 23:53:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.75
[32m[20221213 23:53:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.19
[32m[20221213 23:53:31 @agent_ppo2.py:143][0m Total time:      40.99 min
[32m[20221213 23:53:31 @agent_ppo2.py:145][0m 3979264 total steps have happened
[32m[20221213 23:53:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3943 --------------------------#
[32m[20221213 23:53:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |           0.0033 |          56.7122 |          11.9350 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0017 |          54.1695 |          11.9231 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0075 |          53.3878 |          11.9287 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0057 |          53.2776 |          11.9186 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0107 |          52.4311 |          11.9047 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0109 |          52.2339 |          11.8868 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0116 |          51.8522 |          11.8941 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0152 |          51.6040 |          11.8756 |
[32m[20221213 23:53:32 @agent_ppo2.py:185][0m |          -0.0054 |          53.4439 |          11.8740 |
[32m[20221213 23:53:33 @agent_ppo2.py:185][0m |          -0.0131 |          51.2606 |          11.9090 |
[32m[20221213 23:53:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:53:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.97
[32m[20221213 23:53:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.59
[32m[20221213 23:53:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.24
[32m[20221213 23:53:33 @agent_ppo2.py:143][0m Total time:      41.01 min
[32m[20221213 23:53:33 @agent_ppo2.py:145][0m 3981312 total steps have happened
[32m[20221213 23:53:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3944 --------------------------#
[32m[20221213 23:53:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:33 @agent_ppo2.py:185][0m |           0.0001 |          64.7546 |          11.6397 |
[32m[20221213 23:53:33 @agent_ppo2.py:185][0m |          -0.0078 |          61.1713 |          11.6745 |
[32m[20221213 23:53:33 @agent_ppo2.py:185][0m |          -0.0080 |          59.9759 |          11.6484 |
[32m[20221213 23:53:33 @agent_ppo2.py:185][0m |          -0.0093 |          59.4714 |          11.6711 |
[32m[20221213 23:53:33 @agent_ppo2.py:185][0m |          -0.0014 |          61.5605 |          11.6879 |
[32m[20221213 23:53:33 @agent_ppo2.py:185][0m |          -0.0105 |          58.2545 |          11.7091 |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |          -0.0026 |          61.5152 |          11.7197 |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |          -0.0138 |          57.9688 |          11.7492 |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |          -0.0151 |          57.5558 |          11.7531 |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |          -0.0158 |          57.4831 |          11.7269 |
[32m[20221213 23:53:34 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.62
[32m[20221213 23:53:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.08
[32m[20221213 23:53:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.35
[32m[20221213 23:53:34 @agent_ppo2.py:143][0m Total time:      41.03 min
[32m[20221213 23:53:34 @agent_ppo2.py:145][0m 3983360 total steps have happened
[32m[20221213 23:53:34 @agent_ppo2.py:121][0m #------------------------ Iteration 3945 --------------------------#
[32m[20221213 23:53:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |           0.0012 |          69.6273 |          11.2437 |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |          -0.0057 |          66.9462 |          11.2397 |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |          -0.0004 |          67.7096 |          11.2689 |
[32m[20221213 23:53:34 @agent_ppo2.py:185][0m |          -0.0078 |          65.7234 |          11.3231 |
[32m[20221213 23:53:35 @agent_ppo2.py:185][0m |          -0.0086 |          65.2677 |          11.3024 |
[32m[20221213 23:53:35 @agent_ppo2.py:185][0m |          -0.0102 |          64.9198 |          11.2993 |
[32m[20221213 23:53:35 @agent_ppo2.py:185][0m |          -0.0100 |          64.8158 |          11.2634 |
[32m[20221213 23:53:35 @agent_ppo2.py:185][0m |          -0.0090 |          64.7215 |          11.3020 |
[32m[20221213 23:53:35 @agent_ppo2.py:185][0m |          -0.0024 |          68.2385 |          11.2919 |
[32m[20221213 23:53:35 @agent_ppo2.py:185][0m |          -0.0010 |          68.9909 |          11.2813 |
[32m[20221213 23:53:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.11
[32m[20221213 23:53:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.24
[32m[20221213 23:53:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.96
[32m[20221213 23:53:35 @agent_ppo2.py:143][0m Total time:      41.05 min
[32m[20221213 23:53:35 @agent_ppo2.py:145][0m 3985408 total steps have happened
[32m[20221213 23:53:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3946 --------------------------#
[32m[20221213 23:53:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:35 @agent_ppo2.py:185][0m |           0.0022 |          45.9768 |          11.5898 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0020 |          41.7728 |          11.6218 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0110 |          41.0595 |          11.6227 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0066 |          40.4241 |          11.6050 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0126 |          39.6644 |          11.5686 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0142 |          39.2608 |          11.5877 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0132 |          38.9378 |          11.5675 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0143 |          38.7356 |          11.5633 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0182 |          38.4826 |          11.5689 |
[32m[20221213 23:53:36 @agent_ppo2.py:185][0m |          -0.0153 |          38.2660 |          11.5777 |
[32m[20221213 23:53:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.56
[32m[20221213 23:53:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.10
[32m[20221213 23:53:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.60
[32m[20221213 23:53:36 @agent_ppo2.py:143][0m Total time:      41.07 min
[32m[20221213 23:53:36 @agent_ppo2.py:145][0m 3987456 total steps have happened
[32m[20221213 23:53:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3947 --------------------------#
[32m[20221213 23:53:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |           0.0035 |          52.6547 |          11.7723 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0094 |          48.9803 |          11.7509 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0074 |          48.0725 |          11.7463 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0076 |          47.4645 |          11.7715 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0107 |          47.0773 |          11.8233 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0143 |          46.7487 |          11.8273 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0093 |          46.6184 |          11.8572 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0124 |          46.2274 |          11.8810 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0172 |          46.1306 |          11.8630 |
[32m[20221213 23:53:37 @agent_ppo2.py:185][0m |          -0.0135 |          46.7361 |          11.8761 |
[32m[20221213 23:53:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.60
[32m[20221213 23:53:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.21
[32m[20221213 23:53:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.95
[32m[20221213 23:53:38 @agent_ppo2.py:143][0m Total time:      41.09 min
[32m[20221213 23:53:38 @agent_ppo2.py:145][0m 3989504 total steps have happened
[32m[20221213 23:53:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3948 --------------------------#
[32m[20221213 23:53:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:38 @agent_ppo2.py:185][0m |          -0.0022 |          77.7636 |          11.8703 |
[32m[20221213 23:53:38 @agent_ppo2.py:185][0m |           0.0030 |          73.1292 |          11.8574 |
[32m[20221213 23:53:38 @agent_ppo2.py:185][0m |          -0.0035 |          70.5606 |          11.8579 |
[32m[20221213 23:53:38 @agent_ppo2.py:185][0m |          -0.0046 |          69.6910 |          11.8193 |
[32m[20221213 23:53:38 @agent_ppo2.py:185][0m |          -0.0077 |          68.5002 |          11.8325 |
[32m[20221213 23:53:38 @agent_ppo2.py:185][0m |          -0.0039 |          68.2863 |          11.8176 |
[32m[20221213 23:53:38 @agent_ppo2.py:185][0m |          -0.0092 |          67.4914 |          11.8163 |
[32m[20221213 23:53:39 @agent_ppo2.py:185][0m |          -0.0096 |          66.5899 |          11.7902 |
[32m[20221213 23:53:39 @agent_ppo2.py:185][0m |          -0.0108 |          66.0509 |          11.7818 |
[32m[20221213 23:53:39 @agent_ppo2.py:185][0m |          -0.0125 |          65.6607 |          11.7834 |
[32m[20221213 23:53:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:53:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.26
[32m[20221213 23:53:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.25
[32m[20221213 23:53:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.78
[32m[20221213 23:53:39 @agent_ppo2.py:143][0m Total time:      41.11 min
[32m[20221213 23:53:39 @agent_ppo2.py:145][0m 3991552 total steps have happened
[32m[20221213 23:53:39 @agent_ppo2.py:121][0m #------------------------ Iteration 3949 --------------------------#
[32m[20221213 23:53:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:39 @agent_ppo2.py:185][0m |           0.0042 |          44.2459 |          11.3525 |
[32m[20221213 23:53:39 @agent_ppo2.py:185][0m |          -0.0045 |          39.6118 |          11.3317 |
[32m[20221213 23:53:39 @agent_ppo2.py:185][0m |          -0.0027 |          38.8794 |          11.3331 |
[32m[20221213 23:53:39 @agent_ppo2.py:185][0m |          -0.0066 |          37.4305 |          11.3228 |
[32m[20221213 23:53:40 @agent_ppo2.py:185][0m |          -0.0074 |          36.6689 |          11.2904 |
[32m[20221213 23:53:40 @agent_ppo2.py:185][0m |          -0.0085 |          36.2038 |          11.2682 |
[32m[20221213 23:53:40 @agent_ppo2.py:185][0m |          -0.0135 |          36.0534 |          11.2444 |
[32m[20221213 23:53:40 @agent_ppo2.py:185][0m |          -0.0133 |          35.6687 |          11.2187 |
[32m[20221213 23:53:40 @agent_ppo2.py:185][0m |           0.0039 |          41.0773 |          11.1883 |
[32m[20221213 23:53:40 @agent_ppo2.py:185][0m |          -0.0088 |          35.5386 |          11.2033 |
[32m[20221213 23:53:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.41
[32m[20221213 23:53:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.83
[32m[20221213 23:53:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.88
[32m[20221213 23:53:40 @agent_ppo2.py:143][0m Total time:      41.13 min
[32m[20221213 23:53:40 @agent_ppo2.py:145][0m 3993600 total steps have happened
[32m[20221213 23:53:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3950 --------------------------#
[32m[20221213 23:53:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:53:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:40 @agent_ppo2.py:185][0m |           0.0052 |          62.1780 |          10.9262 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |           0.0076 |          60.5337 |          10.9161 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0001 |          59.7832 |          10.9232 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0094 |          54.9424 |          10.9518 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0084 |          54.0006 |          10.9195 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0148 |          53.4429 |          10.9350 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0144 |          52.6622 |          10.9032 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0101 |          52.3051 |          10.9225 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0180 |          52.1198 |          10.9226 |
[32m[20221213 23:53:41 @agent_ppo2.py:185][0m |          -0.0178 |          51.5177 |          10.9140 |
[32m[20221213 23:53:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.80
[32m[20221213 23:53:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.96
[32m[20221213 23:53:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.17
[32m[20221213 23:53:41 @agent_ppo2.py:143][0m Total time:      41.16 min
[32m[20221213 23:53:41 @agent_ppo2.py:145][0m 3995648 total steps have happened
[32m[20221213 23:53:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3951 --------------------------#
[32m[20221213 23:53:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |           0.0027 |          66.2082 |          11.3033 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |           0.0010 |          64.8590 |          11.3111 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |           0.0016 |          59.9706 |          11.3031 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |          -0.0100 |          57.5944 |          11.3129 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |          -0.0009 |          59.1870 |          11.3054 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |          -0.0112 |          56.3468 |          11.3046 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |          -0.0124 |          56.0145 |          11.2516 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |          -0.0125 |          55.4792 |          11.2682 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |          -0.0103 |          55.0574 |          11.2086 |
[32m[20221213 23:53:42 @agent_ppo2.py:185][0m |          -0.0143 |          54.8347 |          11.2320 |
[32m[20221213 23:53:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.98
[32m[20221213 23:53:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.45
[32m[20221213 23:53:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.29
[32m[20221213 23:53:43 @agent_ppo2.py:143][0m Total time:      41.18 min
[32m[20221213 23:53:43 @agent_ppo2.py:145][0m 3997696 total steps have happened
[32m[20221213 23:53:43 @agent_ppo2.py:121][0m #------------------------ Iteration 3952 --------------------------#
[32m[20221213 23:53:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0015 |          40.5963 |          11.4539 |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0038 |          33.3161 |          11.4946 |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0048 |          32.2379 |          11.4997 |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0170 |          31.4609 |          11.4437 |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0063 |          31.0468 |          11.4629 |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0155 |          30.7063 |          11.4266 |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0118 |          30.3564 |          11.4719 |
[32m[20221213 23:53:43 @agent_ppo2.py:185][0m |          -0.0098 |          29.8194 |          11.4442 |
[32m[20221213 23:53:44 @agent_ppo2.py:185][0m |          -0.0190 |          29.5211 |          11.4433 |
[32m[20221213 23:53:44 @agent_ppo2.py:185][0m |          -0.0169 |          29.2919 |          11.4481 |
[32m[20221213 23:53:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.09
[32m[20221213 23:53:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.25
[32m[20221213 23:53:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.01
[32m[20221213 23:53:44 @agent_ppo2.py:143][0m Total time:      41.20 min
[32m[20221213 23:53:44 @agent_ppo2.py:145][0m 3999744 total steps have happened
[32m[20221213 23:53:44 @agent_ppo2.py:121][0m #------------------------ Iteration 3953 --------------------------#
[32m[20221213 23:53:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:44 @agent_ppo2.py:185][0m |          -0.0022 |          37.3739 |          11.3028 |
[32m[20221213 23:53:44 @agent_ppo2.py:185][0m |          -0.0038 |          30.5129 |          11.2959 |
[32m[20221213 23:53:44 @agent_ppo2.py:185][0m |          -0.0047 |          28.4519 |          11.3261 |
[32m[20221213 23:53:44 @agent_ppo2.py:185][0m |          -0.0153 |          26.5858 |          11.2712 |
[32m[20221213 23:53:44 @agent_ppo2.py:185][0m |          -0.0151 |          25.7951 |          11.3052 |
[32m[20221213 23:53:45 @agent_ppo2.py:185][0m |          -0.0190 |          24.7654 |          11.2753 |
[32m[20221213 23:53:45 @agent_ppo2.py:185][0m |          -0.0154 |          24.0746 |          11.2458 |
[32m[20221213 23:53:45 @agent_ppo2.py:185][0m |          -0.0211 |          23.5751 |          11.2054 |
[32m[20221213 23:53:45 @agent_ppo2.py:185][0m |          -0.0195 |          23.1109 |          11.2186 |
[32m[20221213 23:53:45 @agent_ppo2.py:185][0m |          -0.0241 |          22.7876 |          11.2189 |
[32m[20221213 23:53:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:53:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.90
[32m[20221213 23:53:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.62
[32m[20221213 23:53:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.14
[32m[20221213 23:53:45 @agent_ppo2.py:143][0m Total time:      41.22 min
[32m[20221213 23:53:45 @agent_ppo2.py:145][0m 4001792 total steps have happened
[32m[20221213 23:53:45 @agent_ppo2.py:121][0m #------------------------ Iteration 3954 --------------------------#
[32m[20221213 23:53:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:45 @agent_ppo2.py:185][0m |           0.0027 |          37.6759 |          11.1138 |
[32m[20221213 23:53:45 @agent_ppo2.py:185][0m |          -0.0047 |          31.9474 |          11.1204 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0098 |          29.9111 |          11.0869 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0102 |          28.8119 |          11.0373 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0174 |          28.1471 |          11.0511 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0185 |          27.4324 |          11.0198 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0167 |          27.0854 |          11.0081 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0171 |          26.7058 |          11.0313 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0241 |          26.5079 |          10.9513 |
[32m[20221213 23:53:46 @agent_ppo2.py:185][0m |          -0.0210 |          27.0094 |          10.9634 |
[32m[20221213 23:53:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:53:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.01
[32m[20221213 23:53:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.65
[32m[20221213 23:53:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.30
[32m[20221213 23:53:46 @agent_ppo2.py:143][0m Total time:      41.24 min
[32m[20221213 23:53:46 @agent_ppo2.py:145][0m 4003840 total steps have happened
[32m[20221213 23:53:46 @agent_ppo2.py:121][0m #------------------------ Iteration 3955 --------------------------#
[32m[20221213 23:53:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |           0.0081 |          40.8576 |          10.8654 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0070 |          36.1956 |          10.8602 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0096 |          34.8956 |          10.8678 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0118 |          34.0945 |          10.8980 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0014 |          37.9371 |          10.8597 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0155 |          33.1240 |          10.8805 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0172 |          32.6182 |          10.9103 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0148 |          32.3010 |          10.8810 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0179 |          32.1020 |          10.9061 |
[32m[20221213 23:53:47 @agent_ppo2.py:185][0m |          -0.0126 |          31.8041 |          10.8736 |
[32m[20221213 23:53:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.01
[32m[20221213 23:53:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.17
[32m[20221213 23:53:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.01
[32m[20221213 23:53:48 @agent_ppo2.py:143][0m Total time:      41.26 min
[32m[20221213 23:53:48 @agent_ppo2.py:145][0m 4005888 total steps have happened
[32m[20221213 23:53:48 @agent_ppo2.py:121][0m #------------------------ Iteration 3956 --------------------------#
[32m[20221213 23:53:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |           0.0016 |          48.0784 |          10.9029 |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |          -0.0095 |          44.2642 |          10.8437 |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |          -0.0040 |          42.9564 |          10.7644 |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |          -0.0079 |          43.1632 |          10.7845 |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |          -0.0150 |          40.8733 |          10.7481 |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |          -0.0158 |          39.7214 |          10.7056 |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |          -0.0158 |          39.2879 |          10.6903 |
[32m[20221213 23:53:48 @agent_ppo2.py:185][0m |          -0.0196 |          38.8489 |          10.6881 |
[32m[20221213 23:53:49 @agent_ppo2.py:185][0m |          -0.0157 |          40.5430 |          10.6743 |
[32m[20221213 23:53:49 @agent_ppo2.py:185][0m |          -0.0257 |          38.1522 |          10.6076 |
[32m[20221213 23:53:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.39
[32m[20221213 23:53:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.37
[32m[20221213 23:53:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.23
[32m[20221213 23:53:49 @agent_ppo2.py:143][0m Total time:      41.28 min
[32m[20221213 23:53:49 @agent_ppo2.py:145][0m 4007936 total steps have happened
[32m[20221213 23:53:49 @agent_ppo2.py:121][0m #------------------------ Iteration 3957 --------------------------#
[32m[20221213 23:53:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:49 @agent_ppo2.py:185][0m |           0.0087 |          57.3575 |          10.7227 |
[32m[20221213 23:53:49 @agent_ppo2.py:185][0m |          -0.0057 |          49.8202 |          10.6742 |
[32m[20221213 23:53:49 @agent_ppo2.py:185][0m |          -0.0019 |          47.1119 |          10.6704 |
[32m[20221213 23:53:49 @agent_ppo2.py:185][0m |          -0.0067 |          45.6388 |          10.6586 |
[32m[20221213 23:53:49 @agent_ppo2.py:185][0m |          -0.0092 |          43.9131 |          10.6492 |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0162 |          42.9489 |          10.6521 |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0157 |          42.0998 |          10.6460 |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0151 |          41.3176 |          10.6113 |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0132 |          40.8660 |          10.6352 |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0148 |          40.1842 |          10.6001 |
[32m[20221213 23:53:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:53:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.23
[32m[20221213 23:53:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.25
[32m[20221213 23:53:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.70
[32m[20221213 23:53:50 @agent_ppo2.py:143][0m Total time:      41.30 min
[32m[20221213 23:53:50 @agent_ppo2.py:145][0m 4009984 total steps have happened
[32m[20221213 23:53:50 @agent_ppo2.py:121][0m #------------------------ Iteration 3958 --------------------------#
[32m[20221213 23:53:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0027 |          56.4573 |          10.0805 |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0052 |          53.2309 |          10.1120 |
[32m[20221213 23:53:50 @agent_ppo2.py:185][0m |          -0.0107 |          50.1103 |          10.0714 |
[32m[20221213 23:53:51 @agent_ppo2.py:185][0m |          -0.0101 |          48.7713 |          10.0464 |
[32m[20221213 23:53:51 @agent_ppo2.py:185][0m |          -0.0168 |          47.6006 |          10.0427 |
[32m[20221213 23:53:51 @agent_ppo2.py:185][0m |          -0.0208 |          47.0430 |          10.0468 |
[32m[20221213 23:53:51 @agent_ppo2.py:185][0m |          -0.0155 |          46.3822 |           9.9940 |
[32m[20221213 23:53:51 @agent_ppo2.py:185][0m |          -0.0162 |          45.7076 |          10.0051 |
[32m[20221213 23:53:51 @agent_ppo2.py:185][0m |          -0.0192 |          45.3004 |          10.0175 |
[32m[20221213 23:53:51 @agent_ppo2.py:185][0m |          -0.0159 |          44.8432 |           9.9863 |
[32m[20221213 23:53:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:53:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.71
[32m[20221213 23:53:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.41
[32m[20221213 23:53:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.19
[32m[20221213 23:53:51 @agent_ppo2.py:143][0m Total time:      41.32 min
[32m[20221213 23:53:51 @agent_ppo2.py:145][0m 4012032 total steps have happened
[32m[20221213 23:53:51 @agent_ppo2.py:121][0m #------------------------ Iteration 3959 --------------------------#
[32m[20221213 23:53:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0015 |          52.1726 |          10.9426 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |           0.0063 |          50.8724 |          10.9491 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0076 |          42.9290 |          10.9680 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0129 |          40.8572 |          10.9009 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0126 |          39.8676 |          10.8839 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0186 |          38.9973 |          10.9161 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0129 |          38.2389 |          10.9700 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0150 |          37.5160 |          10.9383 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0178 |          36.8973 |          10.9464 |
[32m[20221213 23:53:52 @agent_ppo2.py:185][0m |          -0.0148 |          36.5448 |          10.9628 |
[32m[20221213 23:53:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:53:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.97
[32m[20221213 23:53:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.78
[32m[20221213 23:53:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.78
[32m[20221213 23:53:52 @agent_ppo2.py:143][0m Total time:      41.34 min
[32m[20221213 23:53:52 @agent_ppo2.py:145][0m 4014080 total steps have happened
[32m[20221213 23:53:52 @agent_ppo2.py:121][0m #------------------------ Iteration 3960 --------------------------#
[32m[20221213 23:53:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:53:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0027 |          56.8541 |          10.2432 |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0104 |          49.9147 |          10.2556 |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0093 |          46.9755 |          10.2878 |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0127 |          45.0041 |          10.2606 |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0205 |          44.0588 |          10.3140 |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0100 |          43.0350 |          10.2627 |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0191 |          41.7559 |          10.3295 |
[32m[20221213 23:53:53 @agent_ppo2.py:185][0m |          -0.0191 |          41.0620 |          10.2951 |
[32m[20221213 23:53:54 @agent_ppo2.py:185][0m |          -0.0220 |          40.4834 |          10.3016 |
[32m[20221213 23:53:54 @agent_ppo2.py:185][0m |          -0.0220 |          39.8971 |          10.3001 |
[32m[20221213 23:53:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:53:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.81
[32m[20221213 23:53:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.65
[32m[20221213 23:53:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.30
[32m[20221213 23:53:54 @agent_ppo2.py:143][0m Total time:      41.36 min
[32m[20221213 23:53:54 @agent_ppo2.py:145][0m 4016128 total steps have happened
[32m[20221213 23:53:54 @agent_ppo2.py:121][0m #------------------------ Iteration 3961 --------------------------#
[32m[20221213 23:53:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:53:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:54 @agent_ppo2.py:185][0m |          -0.0007 |          17.0462 |          10.5528 |
[32m[20221213 23:53:54 @agent_ppo2.py:185][0m |          -0.0004 |          13.5732 |          10.4903 |
[32m[20221213 23:53:54 @agent_ppo2.py:185][0m |          -0.0023 |          13.5096 |          10.4591 |
[32m[20221213 23:53:54 @agent_ppo2.py:185][0m |          -0.0057 |          13.4452 |          10.4981 |
[32m[20221213 23:53:54 @agent_ppo2.py:185][0m |          -0.0039 |          13.4263 |          10.4896 |
[32m[20221213 23:53:55 @agent_ppo2.py:185][0m |           0.0072 |          13.4214 |          10.4623 |
[32m[20221213 23:53:55 @agent_ppo2.py:185][0m |          -0.0003 |          13.4337 |          10.4919 |
[32m[20221213 23:53:55 @agent_ppo2.py:185][0m |          -0.0031 |          13.3959 |          10.4448 |
[32m[20221213 23:53:55 @agent_ppo2.py:185][0m |           0.0011 |          13.4058 |          10.4812 |
[32m[20221213 23:53:55 @agent_ppo2.py:185][0m |           0.0005 |          13.6532 |          10.4787 |
[32m[20221213 23:53:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:53:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:53:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:53:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.49
[32m[20221213 23:53:55 @agent_ppo2.py:143][0m Total time:      41.38 min
[32m[20221213 23:53:55 @agent_ppo2.py:145][0m 4018176 total steps have happened
[32m[20221213 23:53:55 @agent_ppo2.py:121][0m #------------------------ Iteration 3962 --------------------------#
[32m[20221213 23:53:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:53:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:55 @agent_ppo2.py:185][0m |          -0.0027 |          47.3424 |          10.2498 |
[32m[20221213 23:53:55 @agent_ppo2.py:185][0m |          -0.0049 |          41.1234 |          10.2142 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |          -0.0097 |          39.3005 |          10.2540 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |           0.0038 |          39.8796 |          10.2321 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |          -0.0105 |          37.8774 |          10.2518 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |          -0.0118 |          36.8927 |          10.2451 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |          -0.0107 |          36.7898 |          10.1991 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |          -0.0188 |          35.8203 |          10.2158 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |          -0.0164 |          35.3272 |          10.2364 |
[32m[20221213 23:53:56 @agent_ppo2.py:185][0m |          -0.0107 |          35.0554 |          10.1835 |
[32m[20221213 23:53:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:53:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.30
[32m[20221213 23:53:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.59
[32m[20221213 23:53:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.96
[32m[20221213 23:53:56 @agent_ppo2.py:143][0m Total time:      41.41 min
[32m[20221213 23:53:56 @agent_ppo2.py:145][0m 4020224 total steps have happened
[32m[20221213 23:53:56 @agent_ppo2.py:121][0m #------------------------ Iteration 3963 --------------------------#
[32m[20221213 23:53:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |           0.0028 |          63.2495 |          10.1807 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |           0.0108 |          63.2714 |          10.2043 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |           0.0011 |          57.8684 |          10.2564 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |          -0.0063 |          56.9195 |          10.2245 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |          -0.0119 |          56.0085 |          10.1886 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |          -0.0090 |          55.8443 |          10.1737 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |          -0.0116 |          55.6879 |          10.2233 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |          -0.0114 |          55.4782 |          10.1625 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |          -0.0102 |          55.1505 |          10.1904 |
[32m[20221213 23:53:57 @agent_ppo2.py:185][0m |          -0.0144 |          54.9497 |          10.1562 |
[32m[20221213 23:53:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:53:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.30
[32m[20221213 23:53:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.80
[32m[20221213 23:53:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.89
[32m[20221213 23:53:58 @agent_ppo2.py:143][0m Total time:      41.43 min
[32m[20221213 23:53:58 @agent_ppo2.py:145][0m 4022272 total steps have happened
[32m[20221213 23:53:58 @agent_ppo2.py:121][0m #------------------------ Iteration 3964 --------------------------#
[32m[20221213 23:53:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:58 @agent_ppo2.py:185][0m |           0.0071 |          59.9721 |          10.2905 |
[32m[20221213 23:53:58 @agent_ppo2.py:185][0m |          -0.0078 |          57.2203 |          10.3398 |
[32m[20221213 23:53:58 @agent_ppo2.py:185][0m |          -0.0102 |          55.8720 |          10.3118 |
[32m[20221213 23:53:58 @agent_ppo2.py:185][0m |          -0.0076 |          55.0502 |          10.3492 |
[32m[20221213 23:53:58 @agent_ppo2.py:185][0m |          -0.0126 |          54.3937 |          10.3296 |
[32m[20221213 23:53:58 @agent_ppo2.py:185][0m |          -0.0142 |          53.5028 |          10.3681 |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |          -0.0123 |          53.2533 |          10.4234 |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |          -0.0159 |          52.9854 |          10.3921 |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |          -0.0148 |          52.4710 |          10.4387 |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |          -0.0164 |          52.0605 |          10.4725 |
[32m[20221213 23:53:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:53:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.18
[32m[20221213 23:53:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.19
[32m[20221213 23:53:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.25
[32m[20221213 23:53:59 @agent_ppo2.py:143][0m Total time:      41.45 min
[32m[20221213 23:53:59 @agent_ppo2.py:145][0m 4024320 total steps have happened
[32m[20221213 23:53:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3965 --------------------------#
[32m[20221213 23:53:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:53:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |           0.0022 |          46.8487 |          10.3291 |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |          -0.0002 |          36.0492 |          10.2835 |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |          -0.0058 |          32.8907 |          10.2392 |
[32m[20221213 23:53:59 @agent_ppo2.py:185][0m |          -0.0077 |          31.7050 |          10.3194 |
[32m[20221213 23:54:00 @agent_ppo2.py:185][0m |          -0.0167 |          30.6710 |          10.2941 |
[32m[20221213 23:54:00 @agent_ppo2.py:185][0m |          -0.0102 |          30.4128 |          10.3333 |
[32m[20221213 23:54:00 @agent_ppo2.py:185][0m |          -0.0097 |          29.7068 |          10.3184 |
[32m[20221213 23:54:00 @agent_ppo2.py:185][0m |          -0.0120 |          28.9459 |          10.3497 |
[32m[20221213 23:54:00 @agent_ppo2.py:185][0m |          -0.0142 |          28.8960 |          10.3131 |
[32m[20221213 23:54:00 @agent_ppo2.py:185][0m |          -0.0143 |          28.3735 |          10.3632 |
[32m[20221213 23:54:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.38
[32m[20221213 23:54:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.95
[32m[20221213 23:54:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.24
[32m[20221213 23:54:00 @agent_ppo2.py:143][0m Total time:      41.47 min
[32m[20221213 23:54:00 @agent_ppo2.py:145][0m 4026368 total steps have happened
[32m[20221213 23:54:00 @agent_ppo2.py:121][0m #------------------------ Iteration 3966 --------------------------#
[32m[20221213 23:54:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:00 @agent_ppo2.py:185][0m |           0.0002 |          40.2538 |          10.0344 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0051 |          36.2365 |          10.0932 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0092 |          34.8253 |           9.9963 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0102 |          34.0397 |           9.9745 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0096 |          33.3737 |           9.9998 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0151 |          32.8015 |           9.9739 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0190 |          32.2554 |           9.9519 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0170 |          32.2354 |           9.9390 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0167 |          31.7531 |           9.9689 |
[32m[20221213 23:54:01 @agent_ppo2.py:185][0m |          -0.0161 |          31.5388 |           9.9358 |
[32m[20221213 23:54:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.67
[32m[20221213 23:54:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.10
[32m[20221213 23:54:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.57
[32m[20221213 23:54:01 @agent_ppo2.py:143][0m Total time:      41.49 min
[32m[20221213 23:54:01 @agent_ppo2.py:145][0m 4028416 total steps have happened
[32m[20221213 23:54:01 @agent_ppo2.py:121][0m #------------------------ Iteration 3967 --------------------------#
[32m[20221213 23:54:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |           0.0105 |          69.4190 |           9.7033 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |          -0.0055 |          62.5425 |           9.8535 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |          -0.0096 |          61.2757 |           9.9214 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |          -0.0105 |          60.6115 |           9.9420 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |          -0.0061 |          60.6427 |           9.9878 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |          -0.0121 |          59.4643 |          10.0168 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |          -0.0144 |          58.9938 |          10.0128 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |           0.0007 |          67.0901 |          10.0700 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |           0.0047 |          65.0758 |          10.0693 |
[32m[20221213 23:54:02 @agent_ppo2.py:185][0m |          -0.0164 |          57.9543 |          10.0985 |
[32m[20221213 23:54:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:54:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.85
[32m[20221213 23:54:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.23
[32m[20221213 23:54:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.96
[32m[20221213 23:54:03 @agent_ppo2.py:143][0m Total time:      41.51 min
[32m[20221213 23:54:03 @agent_ppo2.py:145][0m 4030464 total steps have happened
[32m[20221213 23:54:03 @agent_ppo2.py:121][0m #------------------------ Iteration 3968 --------------------------#
[32m[20221213 23:54:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:03 @agent_ppo2.py:185][0m |          -0.0026 |          59.6769 |          11.3016 |
[32m[20221213 23:54:03 @agent_ppo2.py:185][0m |          -0.0074 |          55.6129 |          11.3321 |
[32m[20221213 23:54:03 @agent_ppo2.py:185][0m |          -0.0089 |          53.2189 |          11.3800 |
[32m[20221213 23:54:03 @agent_ppo2.py:185][0m |          -0.0101 |          51.6973 |          11.3246 |
[32m[20221213 23:54:03 @agent_ppo2.py:185][0m |          -0.0076 |          50.7469 |          11.3444 |
[32m[20221213 23:54:03 @agent_ppo2.py:185][0m |          -0.0106 |          49.7623 |          11.3616 |
[32m[20221213 23:54:03 @agent_ppo2.py:185][0m |          -0.0057 |          52.5478 |          11.3514 |
[32m[20221213 23:54:04 @agent_ppo2.py:185][0m |          -0.0138 |          48.6818 |          11.3681 |
[32m[20221213 23:54:04 @agent_ppo2.py:185][0m |          -0.0172 |          47.9108 |          11.3560 |
[32m[20221213 23:54:04 @agent_ppo2.py:185][0m |          -0.0165 |          47.6837 |          11.3547 |
[32m[20221213 23:54:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.78
[32m[20221213 23:54:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.24
[32m[20221213 23:54:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.99
[32m[20221213 23:54:04 @agent_ppo2.py:143][0m Total time:      41.53 min
[32m[20221213 23:54:04 @agent_ppo2.py:145][0m 4032512 total steps have happened
[32m[20221213 23:54:04 @agent_ppo2.py:121][0m #------------------------ Iteration 3969 --------------------------#
[32m[20221213 23:54:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:04 @agent_ppo2.py:185][0m |           0.0007 |          57.8544 |          10.8475 |
[32m[20221213 23:54:04 @agent_ppo2.py:185][0m |          -0.0060 |          55.2695 |          10.8074 |
[32m[20221213 23:54:04 @agent_ppo2.py:185][0m |          -0.0091 |          54.3527 |          10.8091 |
[32m[20221213 23:54:04 @agent_ppo2.py:185][0m |          -0.0101 |          53.7716 |          10.7888 |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |          -0.0126 |          53.2962 |          10.7831 |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |          -0.0125 |          53.0809 |          10.7492 |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |          -0.0083 |          53.3064 |          10.7265 |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |          -0.0153 |          52.7433 |          10.7198 |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |          -0.0164 |          52.4621 |          10.7381 |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |          -0.0135 |          52.2579 |          10.7202 |
[32m[20221213 23:54:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.08
[32m[20221213 23:54:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.93
[32m[20221213 23:54:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.85
[32m[20221213 23:54:05 @agent_ppo2.py:143][0m Total time:      41.55 min
[32m[20221213 23:54:05 @agent_ppo2.py:145][0m 4034560 total steps have happened
[32m[20221213 23:54:05 @agent_ppo2.py:121][0m #------------------------ Iteration 3970 --------------------------#
[32m[20221213 23:54:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:54:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |           0.0029 |          70.8667 |          10.5351 |
[32m[20221213 23:54:05 @agent_ppo2.py:185][0m |          -0.0050 |          64.3632 |          10.6397 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0081 |          62.0584 |          10.5654 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0081 |          61.0736 |          10.6014 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0096 |          60.4784 |          10.6267 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0177 |          58.9920 |          10.6007 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0043 |          58.2140 |          10.6398 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0114 |          58.3700 |          10.6474 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0169 |          56.9015 |          10.6343 |
[32m[20221213 23:54:06 @agent_ppo2.py:185][0m |          -0.0184 |          56.3105 |          10.6207 |
[32m[20221213 23:54:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.87
[32m[20221213 23:54:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.13
[32m[20221213 23:54:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.27
[32m[20221213 23:54:06 @agent_ppo2.py:143][0m Total time:      41.57 min
[32m[20221213 23:54:06 @agent_ppo2.py:145][0m 4036608 total steps have happened
[32m[20221213 23:54:06 @agent_ppo2.py:121][0m #------------------------ Iteration 3971 --------------------------#
[32m[20221213 23:54:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0001 |          34.1490 |          10.5668 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0108 |          30.2354 |          10.6404 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0071 |          28.7784 |          10.5998 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0115 |          27.8683 |          10.5300 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0173 |          27.1560 |          10.4835 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0101 |          26.6213 |          10.4707 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0117 |          26.4971 |          10.4284 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0178 |          25.7178 |          10.4048 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0147 |          25.9607 |          10.3806 |
[32m[20221213 23:54:07 @agent_ppo2.py:185][0m |          -0.0167 |          25.2346 |          10.3276 |
[32m[20221213 23:54:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.55
[32m[20221213 23:54:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.79
[32m[20221213 23:54:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.86
[32m[20221213 23:54:08 @agent_ppo2.py:143][0m Total time:      41.59 min
[32m[20221213 23:54:08 @agent_ppo2.py:145][0m 4038656 total steps have happened
[32m[20221213 23:54:08 @agent_ppo2.py:121][0m #------------------------ Iteration 3972 --------------------------#
[32m[20221213 23:54:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |           0.0000 |          71.1065 |           9.9542 |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |          -0.0047 |          68.4910 |           9.9825 |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |          -0.0062 |          67.5710 |          10.0554 |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |          -0.0089 |          66.9404 |           9.9905 |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |          -0.0111 |          66.4411 |          10.0271 |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |           0.0026 |          75.4760 |          10.0822 |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |          -0.0127 |          65.5588 |          10.1171 |
[32m[20221213 23:54:08 @agent_ppo2.py:185][0m |          -0.0145 |          65.4151 |          10.1192 |
[32m[20221213 23:54:09 @agent_ppo2.py:185][0m |          -0.0135 |          65.2502 |          10.1147 |
[32m[20221213 23:54:09 @agent_ppo2.py:185][0m |          -0.0006 |          69.7363 |          10.1710 |
[32m[20221213 23:54:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:54:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.93
[32m[20221213 23:54:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.62
[32m[20221213 23:54:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.30
[32m[20221213 23:54:09 @agent_ppo2.py:143][0m Total time:      41.61 min
[32m[20221213 23:54:09 @agent_ppo2.py:145][0m 4040704 total steps have happened
[32m[20221213 23:54:09 @agent_ppo2.py:121][0m #------------------------ Iteration 3973 --------------------------#
[32m[20221213 23:54:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:54:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:09 @agent_ppo2.py:185][0m |           0.0231 |          75.0684 |          10.0304 |
[32m[20221213 23:54:09 @agent_ppo2.py:185][0m |          -0.0046 |          61.7817 |          10.0024 |
[32m[20221213 23:54:09 @agent_ppo2.py:185][0m |          -0.0052 |          60.8346 |           9.9873 |
[32m[20221213 23:54:09 @agent_ppo2.py:185][0m |          -0.0086 |          59.9691 |          10.0137 |
[32m[20221213 23:54:09 @agent_ppo2.py:185][0m |          -0.0108 |          59.5867 |          10.0039 |
[32m[20221213 23:54:10 @agent_ppo2.py:185][0m |          -0.0101 |          59.6652 |          10.0064 |
[32m[20221213 23:54:10 @agent_ppo2.py:185][0m |          -0.0137 |          59.0801 |          10.0334 |
[32m[20221213 23:54:10 @agent_ppo2.py:185][0m |          -0.0114 |          59.0683 |          10.0457 |
[32m[20221213 23:54:10 @agent_ppo2.py:185][0m |          -0.0128 |          58.6131 |          10.0021 |
[32m[20221213 23:54:10 @agent_ppo2.py:185][0m |          -0.0095 |          58.6967 |          10.0100 |
[32m[20221213 23:54:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.35
[32m[20221213 23:54:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.17
[32m[20221213 23:54:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.80
[32m[20221213 23:54:10 @agent_ppo2.py:143][0m Total time:      41.63 min
[32m[20221213 23:54:10 @agent_ppo2.py:145][0m 4042752 total steps have happened
[32m[20221213 23:54:10 @agent_ppo2.py:121][0m #------------------------ Iteration 3974 --------------------------#
[32m[20221213 23:54:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:10 @agent_ppo2.py:185][0m |           0.0010 |          61.7911 |          10.5054 |
[32m[20221213 23:54:10 @agent_ppo2.py:185][0m |           0.0007 |          56.4553 |          10.5150 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0047 |          53.3644 |          10.4888 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0046 |          51.4467 |          10.5548 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0092 |          50.0290 |          10.5270 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0061 |          49.2348 |          10.5400 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0103 |          48.8652 |          10.5435 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0091 |          48.1978 |          10.5620 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0094 |          47.7965 |          10.5865 |
[32m[20221213 23:54:11 @agent_ppo2.py:185][0m |          -0.0115 |          47.4001 |          10.6077 |
[32m[20221213 23:54:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.98
[32m[20221213 23:54:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.26
[32m[20221213 23:54:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.97
[32m[20221213 23:54:11 @agent_ppo2.py:143][0m Total time:      41.65 min
[32m[20221213 23:54:11 @agent_ppo2.py:145][0m 4044800 total steps have happened
[32m[20221213 23:54:11 @agent_ppo2.py:121][0m #------------------------ Iteration 3975 --------------------------#
[32m[20221213 23:54:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |           0.0067 |          66.9921 |          10.7917 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0005 |          51.9134 |          10.8293 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0075 |          49.3171 |          10.9287 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0053 |          48.3281 |          10.9475 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0111 |          47.6818 |          11.0124 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0118 |          47.2932 |          11.0478 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0124 |          46.9593 |          11.1117 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0147 |          46.5338 |          11.1403 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0094 |          48.1386 |          11.1720 |
[32m[20221213 23:54:12 @agent_ppo2.py:185][0m |          -0.0180 |          46.2067 |          11.2148 |
[32m[20221213 23:54:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:54:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.77
[32m[20221213 23:54:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.04
[32m[20221213 23:54:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.68
[32m[20221213 23:54:12 @agent_ppo2.py:143][0m Total time:      41.67 min
[32m[20221213 23:54:12 @agent_ppo2.py:145][0m 4046848 total steps have happened
[32m[20221213 23:54:12 @agent_ppo2.py:121][0m #------------------------ Iteration 3976 --------------------------#
[32m[20221213 23:54:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |           0.0009 |          53.2700 |          10.7698 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |           0.0014 |          45.5927 |          10.7961 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |          -0.0092 |          44.0623 |          10.7708 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |          -0.0113 |          43.1155 |          10.8228 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |          -0.0169 |          42.2512 |          10.8490 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |          -0.0202 |          41.8209 |          10.9088 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |          -0.0158 |          41.2358 |          10.8885 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |          -0.0176 |          41.0984 |          10.9099 |
[32m[20221213 23:54:13 @agent_ppo2.py:185][0m |          -0.0162 |          40.6965 |          10.9616 |
[32m[20221213 23:54:14 @agent_ppo2.py:185][0m |          -0.0134 |          42.6988 |          10.9536 |
[32m[20221213 23:54:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:54:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.72
[32m[20221213 23:54:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.43
[32m[20221213 23:54:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.25
[32m[20221213 23:54:14 @agent_ppo2.py:143][0m Total time:      41.70 min
[32m[20221213 23:54:14 @agent_ppo2.py:145][0m 4048896 total steps have happened
[32m[20221213 23:54:14 @agent_ppo2.py:121][0m #------------------------ Iteration 3977 --------------------------#
[32m[20221213 23:54:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:14 @agent_ppo2.py:185][0m |           0.0065 |          60.6424 |          11.0069 |
[32m[20221213 23:54:14 @agent_ppo2.py:185][0m |          -0.0039 |          54.2008 |          11.0114 |
[32m[20221213 23:54:14 @agent_ppo2.py:185][0m |          -0.0057 |          51.2815 |          10.9954 |
[32m[20221213 23:54:14 @agent_ppo2.py:185][0m |           0.0141 |          58.9537 |          11.0023 |
[32m[20221213 23:54:14 @agent_ppo2.py:185][0m |          -0.0091 |          49.6880 |          10.9652 |
[32m[20221213 23:54:14 @agent_ppo2.py:185][0m |          -0.0153 |          49.2116 |          10.9370 |
[32m[20221213 23:54:15 @agent_ppo2.py:185][0m |          -0.0126 |          49.0339 |          10.9239 |
[32m[20221213 23:54:15 @agent_ppo2.py:185][0m |          -0.0128 |          48.5617 |          10.9488 |
[32m[20221213 23:54:15 @agent_ppo2.py:185][0m |          -0.0143 |          48.3969 |          10.9455 |
[32m[20221213 23:54:15 @agent_ppo2.py:185][0m |          -0.0133 |          48.2013 |          10.9252 |
[32m[20221213 23:54:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:54:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.19
[32m[20221213 23:54:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.83
[32m[20221213 23:54:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.92
[32m[20221213 23:54:15 @agent_ppo2.py:143][0m Total time:      41.72 min
[32m[20221213 23:54:15 @agent_ppo2.py:145][0m 4050944 total steps have happened
[32m[20221213 23:54:15 @agent_ppo2.py:121][0m #------------------------ Iteration 3978 --------------------------#
[32m[20221213 23:54:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:15 @agent_ppo2.py:185][0m |           0.0000 |          64.3472 |          10.6532 |
[32m[20221213 23:54:15 @agent_ppo2.py:185][0m |          -0.0047 |          59.8619 |          10.6885 |
[32m[20221213 23:54:15 @agent_ppo2.py:185][0m |          -0.0093 |          58.5448 |          10.6493 |
[32m[20221213 23:54:16 @agent_ppo2.py:185][0m |          -0.0040 |          60.0940 |          10.6604 |
[32m[20221213 23:54:16 @agent_ppo2.py:185][0m |          -0.0121 |          57.7493 |          10.6680 |
[32m[20221213 23:54:16 @agent_ppo2.py:185][0m |          -0.0105 |          57.3318 |          10.6665 |
[32m[20221213 23:54:16 @agent_ppo2.py:185][0m |          -0.0132 |          57.0706 |          10.7160 |
[32m[20221213 23:54:16 @agent_ppo2.py:185][0m |          -0.0142 |          56.5876 |          10.7166 |
[32m[20221213 23:54:16 @agent_ppo2.py:185][0m |          -0.0147 |          56.1070 |          10.7509 |
[32m[20221213 23:54:16 @agent_ppo2.py:185][0m |          -0.0165 |          55.8506 |          10.7942 |
[32m[20221213 23:54:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:54:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.87
[32m[20221213 23:54:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.15
[32m[20221213 23:54:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.96
[32m[20221213 23:54:16 @agent_ppo2.py:143][0m Total time:      41.74 min
[32m[20221213 23:54:16 @agent_ppo2.py:145][0m 4052992 total steps have happened
[32m[20221213 23:54:16 @agent_ppo2.py:121][0m #------------------------ Iteration 3979 --------------------------#
[32m[20221213 23:54:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |           0.0014 |          56.3051 |          10.8731 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0008 |          47.1396 |          10.8548 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0118 |          44.7736 |          10.8511 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0050 |          44.1545 |          10.8566 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0113 |          42.7348 |          10.8635 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0066 |          42.2085 |          10.8810 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0150 |          41.4517 |          10.9305 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0100 |          41.0682 |          10.9165 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0145 |          41.0507 |          10.8737 |
[32m[20221213 23:54:17 @agent_ppo2.py:185][0m |          -0.0130 |          40.5532 |          10.8999 |
[32m[20221213 23:54:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.02
[32m[20221213 23:54:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.48
[32m[20221213 23:54:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.00
[32m[20221213 23:54:17 @agent_ppo2.py:143][0m Total time:      41.76 min
[32m[20221213 23:54:17 @agent_ppo2.py:145][0m 4055040 total steps have happened
[32m[20221213 23:54:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3980 --------------------------#
[32m[20221213 23:54:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:54:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |           0.0052 |          62.1322 |          10.4127 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0044 |          60.0572 |          10.4148 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0051 |          59.4947 |          10.4819 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0063 |          58.8429 |          10.4533 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0087 |          58.5893 |          10.5097 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0095 |          58.3754 |          10.4184 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0102 |          58.0856 |          10.4121 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0100 |          57.9468 |          10.4291 |
[32m[20221213 23:54:18 @agent_ppo2.py:185][0m |          -0.0067 |          58.6294 |          10.4197 |
[32m[20221213 23:54:19 @agent_ppo2.py:185][0m |          -0.0113 |          57.7905 |          10.4429 |
[32m[20221213 23:54:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 23:54:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.66
[32m[20221213 23:54:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.31
[32m[20221213 23:54:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 175.41
[32m[20221213 23:54:19 @agent_ppo2.py:143][0m Total time:      41.78 min
[32m[20221213 23:54:19 @agent_ppo2.py:145][0m 4057088 total steps have happened
[32m[20221213 23:54:19 @agent_ppo2.py:121][0m #------------------------ Iteration 3981 --------------------------#
[32m[20221213 23:54:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:19 @agent_ppo2.py:185][0m |           0.0009 |          56.1875 |          11.1175 |
[32m[20221213 23:54:19 @agent_ppo2.py:185][0m |          -0.0057 |          52.1327 |          11.1064 |
[32m[20221213 23:54:19 @agent_ppo2.py:185][0m |          -0.0102 |          50.7711 |          11.1505 |
[32m[20221213 23:54:19 @agent_ppo2.py:185][0m |          -0.0105 |          50.0411 |          11.1675 |
[32m[20221213 23:54:19 @agent_ppo2.py:185][0m |          -0.0097 |          49.2749 |          11.1766 |
[32m[20221213 23:54:19 @agent_ppo2.py:185][0m |          -0.0154 |          48.8968 |          11.1773 |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |          -0.0149 |          48.4908 |          11.1998 |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |          -0.0176 |          48.3280 |          11.2388 |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |          -0.0131 |          48.9974 |          11.2317 |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |          -0.0157 |          48.0582 |          11.2523 |
[32m[20221213 23:54:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.03
[32m[20221213 23:54:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.30
[32m[20221213 23:54:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.88
[32m[20221213 23:54:20 @agent_ppo2.py:143][0m Total time:      41.80 min
[32m[20221213 23:54:20 @agent_ppo2.py:145][0m 4059136 total steps have happened
[32m[20221213 23:54:20 @agent_ppo2.py:121][0m #------------------------ Iteration 3982 --------------------------#
[32m[20221213 23:54:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |           0.0020 |          50.9567 |          11.0982 |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |          -0.0016 |          43.0861 |          11.0874 |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |          -0.0084 |          41.1481 |          11.1378 |
[32m[20221213 23:54:20 @agent_ppo2.py:185][0m |          -0.0103 |          39.8017 |          11.1270 |
[32m[20221213 23:54:21 @agent_ppo2.py:185][0m |          -0.0100 |          39.3545 |          11.1452 |
[32m[20221213 23:54:21 @agent_ppo2.py:185][0m |          -0.0096 |          39.0913 |          11.1458 |
[32m[20221213 23:54:21 @agent_ppo2.py:185][0m |          -0.0128 |          38.0006 |          11.1519 |
[32m[20221213 23:54:21 @agent_ppo2.py:185][0m |          -0.0180 |          37.5289 |          11.1127 |
[32m[20221213 23:54:21 @agent_ppo2.py:185][0m |          -0.0222 |          37.1057 |          11.1796 |
[32m[20221213 23:54:21 @agent_ppo2.py:185][0m |          -0.0196 |          36.6914 |          11.1649 |
[32m[20221213 23:54:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:54:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.57
[32m[20221213 23:54:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.17
[32m[20221213 23:54:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.90
[32m[20221213 23:54:21 @agent_ppo2.py:143][0m Total time:      41.82 min
[32m[20221213 23:54:21 @agent_ppo2.py:145][0m 4061184 total steps have happened
[32m[20221213 23:54:21 @agent_ppo2.py:121][0m #------------------------ Iteration 3983 --------------------------#
[32m[20221213 23:54:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:21 @agent_ppo2.py:185][0m |           0.0041 |          40.4849 |          11.0883 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0064 |          34.1145 |          11.0673 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0039 |          32.4295 |          11.0698 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0057 |          31.5566 |          11.0740 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0067 |          31.2338 |          11.0758 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |           0.0014 |          31.7247 |          11.0915 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0173 |          29.7537 |          11.1122 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0087 |          29.3863 |          11.1216 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0188 |          28.9423 |          11.0927 |
[32m[20221213 23:54:22 @agent_ppo2.py:185][0m |          -0.0056 |          30.7601 |          11.0970 |
[32m[20221213 23:54:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.25
[32m[20221213 23:54:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.19
[32m[20221213 23:54:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.50
[32m[20221213 23:54:22 @agent_ppo2.py:143][0m Total time:      41.84 min
[32m[20221213 23:54:22 @agent_ppo2.py:145][0m 4063232 total steps have happened
[32m[20221213 23:54:22 @agent_ppo2.py:121][0m #------------------------ Iteration 3984 --------------------------#
[32m[20221213 23:54:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0017 |          60.3131 |          11.4025 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0028 |          57.5624 |          11.4404 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0101 |          55.3419 |          11.4673 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0110 |          54.6182 |          11.4652 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0074 |          54.8514 |          11.4568 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0087 |          54.8871 |          11.4588 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0173 |          53.2449 |          11.4991 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0145 |          53.0320 |          11.4854 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0084 |          53.9147 |          11.5137 |
[32m[20221213 23:54:23 @agent_ppo2.py:185][0m |          -0.0166 |          52.6455 |          11.5512 |
[32m[20221213 23:54:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.39
[32m[20221213 23:54:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.00
[32m[20221213 23:54:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.18
[32m[20221213 23:54:24 @agent_ppo2.py:143][0m Total time:      41.86 min
[32m[20221213 23:54:24 @agent_ppo2.py:145][0m 4065280 total steps have happened
[32m[20221213 23:54:24 @agent_ppo2.py:121][0m #------------------------ Iteration 3985 --------------------------#
[32m[20221213 23:54:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:24 @agent_ppo2.py:185][0m |           0.0003 |          64.8677 |          11.6864 |
[32m[20221213 23:54:24 @agent_ppo2.py:185][0m |          -0.0051 |          61.7132 |          11.6563 |
[32m[20221213 23:54:24 @agent_ppo2.py:185][0m |          -0.0087 |          60.7508 |          11.6745 |
[32m[20221213 23:54:24 @agent_ppo2.py:185][0m |          -0.0141 |          59.9343 |          11.7260 |
[32m[20221213 23:54:24 @agent_ppo2.py:185][0m |          -0.0105 |          59.1583 |          11.6967 |
[32m[20221213 23:54:24 @agent_ppo2.py:185][0m |          -0.0123 |          58.8078 |          11.7136 |
[32m[20221213 23:54:24 @agent_ppo2.py:185][0m |          -0.0160 |          58.3967 |          11.7354 |
[32m[20221213 23:54:25 @agent_ppo2.py:185][0m |          -0.0138 |          57.9635 |          11.7197 |
[32m[20221213 23:54:25 @agent_ppo2.py:185][0m |          -0.0149 |          57.7384 |          11.6708 |
[32m[20221213 23:54:25 @agent_ppo2.py:185][0m |          -0.0181 |          57.5529 |          11.7170 |
[32m[20221213 23:54:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.24
[32m[20221213 23:54:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.11
[32m[20221213 23:54:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.73
[32m[20221213 23:54:25 @agent_ppo2.py:143][0m Total time:      41.88 min
[32m[20221213 23:54:25 @agent_ppo2.py:145][0m 4067328 total steps have happened
[32m[20221213 23:54:25 @agent_ppo2.py:121][0m #------------------------ Iteration 3986 --------------------------#
[32m[20221213 23:54:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:25 @agent_ppo2.py:185][0m |           0.0022 |          39.2580 |          11.5874 |
[32m[20221213 23:54:25 @agent_ppo2.py:185][0m |          -0.0075 |          35.2127 |          11.5507 |
[32m[20221213 23:54:25 @agent_ppo2.py:185][0m |          -0.0086 |          33.8115 |          11.5133 |
[32m[20221213 23:54:25 @agent_ppo2.py:185][0m |          -0.0106 |          33.0616 |          11.5435 |
[32m[20221213 23:54:26 @agent_ppo2.py:185][0m |          -0.0125 |          32.4651 |          11.5429 |
[32m[20221213 23:54:26 @agent_ppo2.py:185][0m |          -0.0154 |          31.8533 |          11.5112 |
[32m[20221213 23:54:26 @agent_ppo2.py:185][0m |          -0.0131 |          31.5410 |          11.5077 |
[32m[20221213 23:54:26 @agent_ppo2.py:185][0m |           0.0013 |          33.6668 |          11.5477 |
[32m[20221213 23:54:26 @agent_ppo2.py:185][0m |          -0.0177 |          30.8823 |          11.5206 |
[32m[20221213 23:54:26 @agent_ppo2.py:185][0m |          -0.0240 |          30.6264 |          11.5355 |
[32m[20221213 23:54:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.16
[32m[20221213 23:54:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 362.59
[32m[20221213 23:54:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.54
[32m[20221213 23:54:26 @agent_ppo2.py:143][0m Total time:      41.90 min
[32m[20221213 23:54:26 @agent_ppo2.py:145][0m 4069376 total steps have happened
[32m[20221213 23:54:26 @agent_ppo2.py:121][0m #------------------------ Iteration 3987 --------------------------#
[32m[20221213 23:54:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:26 @agent_ppo2.py:185][0m |           0.0008 |          52.4438 |          11.3138 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0044 |          46.4199 |          11.2524 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0080 |          44.1963 |          11.1821 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0140 |          42.3038 |          11.2011 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0135 |          41.5185 |          11.1857 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0148 |          40.8665 |          11.1044 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0150 |          39.7498 |          11.1076 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0190 |          39.3819 |          11.0818 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0179 |          38.7472 |          11.0720 |
[32m[20221213 23:54:27 @agent_ppo2.py:185][0m |          -0.0174 |          38.6363 |          11.0687 |
[32m[20221213 23:54:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:54:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.58
[32m[20221213 23:54:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.40
[32m[20221213 23:54:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.59
[32m[20221213 23:54:27 @agent_ppo2.py:143][0m Total time:      41.92 min
[32m[20221213 23:54:27 @agent_ppo2.py:145][0m 4071424 total steps have happened
[32m[20221213 23:54:27 @agent_ppo2.py:121][0m #------------------------ Iteration 3988 --------------------------#
[32m[20221213 23:54:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |           0.0070 |          61.5149 |          10.6487 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0033 |          59.1817 |          10.7260 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0073 |          58.5245 |          10.7154 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0087 |          58.1661 |          10.7398 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0071 |          57.6036 |          10.7423 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0133 |          57.4949 |          10.7691 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0127 |          57.1271 |          10.7873 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0121 |          57.1674 |          10.8129 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0133 |          56.8550 |          10.8597 |
[32m[20221213 23:54:28 @agent_ppo2.py:185][0m |          -0.0138 |          56.7676 |          10.8576 |
[32m[20221213 23:54:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.14
[32m[20221213 23:54:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.87
[32m[20221213 23:54:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.92
[32m[20221213 23:54:29 @agent_ppo2.py:143][0m Total time:      41.94 min
[32m[20221213 23:54:29 @agent_ppo2.py:145][0m 4073472 total steps have happened
[32m[20221213 23:54:29 @agent_ppo2.py:121][0m #------------------------ Iteration 3989 --------------------------#
[32m[20221213 23:54:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |           0.0062 |          43.8841 |          11.1854 |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |          -0.0084 |          40.0227 |          11.1797 |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |          -0.0004 |          39.6987 |          11.1953 |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |          -0.0083 |          37.8711 |          11.1987 |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |          -0.0017 |          38.8975 |          11.2196 |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |           0.0039 |          43.0570 |          11.2170 |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |          -0.0149 |          36.9569 |          11.2691 |
[32m[20221213 23:54:29 @agent_ppo2.py:185][0m |          -0.0093 |          36.4013 |          11.2543 |
[32m[20221213 23:54:30 @agent_ppo2.py:185][0m |          -0.0149 |          36.2427 |          11.2885 |
[32m[20221213 23:54:30 @agent_ppo2.py:185][0m |          -0.0236 |          35.9967 |          11.3059 |
[32m[20221213 23:54:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.22
[32m[20221213 23:54:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.22
[32m[20221213 23:54:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.21
[32m[20221213 23:54:30 @agent_ppo2.py:143][0m Total time:      41.96 min
[32m[20221213 23:54:30 @agent_ppo2.py:145][0m 4075520 total steps have happened
[32m[20221213 23:54:30 @agent_ppo2.py:121][0m #------------------------ Iteration 3990 --------------------------#
[32m[20221213 23:54:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:54:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:30 @agent_ppo2.py:185][0m |           0.0003 |          53.0699 |          11.3292 |
[32m[20221213 23:54:30 @agent_ppo2.py:185][0m |          -0.0053 |          52.5463 |          11.2877 |
[32m[20221213 23:54:30 @agent_ppo2.py:185][0m |           0.0009 |          53.6592 |          11.2796 |
[32m[20221213 23:54:30 @agent_ppo2.py:185][0m |          -0.0118 |          50.1110 |          11.2631 |
[32m[20221213 23:54:30 @agent_ppo2.py:185][0m |          -0.0124 |          49.5940 |          11.3000 |
[32m[20221213 23:54:31 @agent_ppo2.py:185][0m |          -0.0143 |          49.2538 |          11.2721 |
[32m[20221213 23:54:31 @agent_ppo2.py:185][0m |          -0.0128 |          49.9244 |          11.2667 |
[32m[20221213 23:54:31 @agent_ppo2.py:185][0m |          -0.0160 |          49.3199 |          11.2880 |
[32m[20221213 23:54:31 @agent_ppo2.py:185][0m |          -0.0037 |          50.4181 |          11.2703 |
[32m[20221213 23:54:31 @agent_ppo2.py:185][0m |          -0.0162 |          48.9887 |          11.2473 |
[32m[20221213 23:54:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.58
[32m[20221213 23:54:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.31
[32m[20221213 23:54:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.59
[32m[20221213 23:54:31 @agent_ppo2.py:143][0m Total time:      41.98 min
[32m[20221213 23:54:31 @agent_ppo2.py:145][0m 4077568 total steps have happened
[32m[20221213 23:54:31 @agent_ppo2.py:121][0m #------------------------ Iteration 3991 --------------------------#
[32m[20221213 23:54:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:31 @agent_ppo2.py:185][0m |          -0.0010 |          45.6925 |          11.4667 |
[32m[20221213 23:54:31 @agent_ppo2.py:185][0m |          -0.0059 |          41.4612 |          11.4605 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0120 |          40.1334 |          11.4606 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0094 |          39.3312 |          11.4719 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0120 |          38.8153 |          11.4591 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0155 |          38.2416 |          11.4523 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0195 |          37.9601 |          11.4254 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0151 |          37.5658 |          11.4178 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0168 |          37.3291 |          11.4337 |
[32m[20221213 23:54:32 @agent_ppo2.py:185][0m |          -0.0194 |          37.0628 |          11.4617 |
[32m[20221213 23:54:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.96
[32m[20221213 23:54:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.72
[32m[20221213 23:54:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.52
[32m[20221213 23:54:32 @agent_ppo2.py:143][0m Total time:      42.00 min
[32m[20221213 23:54:32 @agent_ppo2.py:145][0m 4079616 total steps have happened
[32m[20221213 23:54:32 @agent_ppo2.py:121][0m #------------------------ Iteration 3992 --------------------------#
[32m[20221213 23:54:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |           0.0026 |          56.3629 |          11.3644 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0057 |          53.2931 |          11.3442 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0010 |          53.5180 |          11.3167 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0104 |          51.7872 |          11.3126 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |           0.0003 |          57.6606 |          11.3543 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0090 |          51.5795 |          11.3609 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0146 |          51.0416 |          11.3506 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0144 |          51.0025 |          11.3551 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0161 |          50.7255 |          11.3598 |
[32m[20221213 23:54:33 @agent_ppo2.py:185][0m |          -0.0154 |          50.8186 |          11.3993 |
[32m[20221213 23:54:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:54:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.65
[32m[20221213 23:54:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.53
[32m[20221213 23:54:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.64
[32m[20221213 23:54:33 @agent_ppo2.py:143][0m Total time:      42.02 min
[32m[20221213 23:54:33 @agent_ppo2.py:145][0m 4081664 total steps have happened
[32m[20221213 23:54:33 @agent_ppo2.py:121][0m #------------------------ Iteration 3993 --------------------------#
[32m[20221213 23:54:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0018 |          60.9428 |          11.3430 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0041 |          53.7880 |          11.2841 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0064 |          51.2548 |          11.2287 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0078 |          50.4084 |          11.2317 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0067 |          48.9222 |          11.2140 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0084 |          47.9748 |          11.1799 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0119 |          47.2410 |          11.2000 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0123 |          46.5052 |          11.1463 |
[32m[20221213 23:54:34 @agent_ppo2.py:185][0m |          -0.0107 |          46.1346 |          11.1340 |
[32m[20221213 23:54:35 @agent_ppo2.py:185][0m |          -0.0134 |          45.5019 |          11.1266 |
[32m[20221213 23:54:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.91
[32m[20221213 23:54:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.23
[32m[20221213 23:54:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.45
[32m[20221213 23:54:35 @agent_ppo2.py:143][0m Total time:      42.05 min
[32m[20221213 23:54:35 @agent_ppo2.py:145][0m 4083712 total steps have happened
[32m[20221213 23:54:35 @agent_ppo2.py:121][0m #------------------------ Iteration 3994 --------------------------#
[32m[20221213 23:54:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:35 @agent_ppo2.py:185][0m |           0.0004 |          70.0381 |          11.3033 |
[32m[20221213 23:54:35 @agent_ppo2.py:185][0m |           0.0053 |          73.1796 |          11.3020 |
[32m[20221213 23:54:35 @agent_ppo2.py:185][0m |          -0.0059 |          66.2788 |          11.3490 |
[32m[20221213 23:54:35 @agent_ppo2.py:185][0m |          -0.0096 |          65.7192 |          11.3621 |
[32m[20221213 23:54:35 @agent_ppo2.py:185][0m |          -0.0041 |          65.9000 |          11.4006 |
[32m[20221213 23:54:35 @agent_ppo2.py:185][0m |          -0.0119 |          65.2439 |          11.3912 |
[32m[20221213 23:54:36 @agent_ppo2.py:185][0m |          -0.0103 |          64.8250 |          11.3990 |
[32m[20221213 23:54:36 @agent_ppo2.py:185][0m |          -0.0146 |          64.6339 |          11.3909 |
[32m[20221213 23:54:36 @agent_ppo2.py:185][0m |          -0.0123 |          64.2968 |          11.4050 |
[32m[20221213 23:54:36 @agent_ppo2.py:185][0m |          -0.0119 |          64.0372 |          11.4090 |
[32m[20221213 23:54:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 23:54:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.14
[32m[20221213 23:54:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.41
[32m[20221213 23:54:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.62
[32m[20221213 23:54:36 @agent_ppo2.py:143][0m Total time:      42.07 min
[32m[20221213 23:54:36 @agent_ppo2.py:145][0m 4085760 total steps have happened
[32m[20221213 23:54:36 @agent_ppo2.py:121][0m #------------------------ Iteration 3995 --------------------------#
[32m[20221213 23:54:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:36 @agent_ppo2.py:185][0m |           0.0002 |          40.3699 |          11.1736 |
[32m[20221213 23:54:36 @agent_ppo2.py:185][0m |          -0.0017 |          35.2527 |          11.1171 |
[32m[20221213 23:54:36 @agent_ppo2.py:185][0m |          -0.0171 |          34.0119 |          11.1599 |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |          -0.0138 |          32.4343 |          11.1205 |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |          -0.0165 |          31.6411 |          11.1255 |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |          -0.0201 |          30.8007 |          11.1091 |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |          -0.0054 |          34.0655 |          11.0771 |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |          -0.0249 |          29.8678 |          11.1387 |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |          -0.0209 |          29.3860 |          11.1675 |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |          -0.0197 |          29.0106 |          11.1182 |
[32m[20221213 23:54:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.11
[32m[20221213 23:54:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.04
[32m[20221213 23:54:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.17
[32m[20221213 23:54:37 @agent_ppo2.py:143][0m Total time:      42.09 min
[32m[20221213 23:54:37 @agent_ppo2.py:145][0m 4087808 total steps have happened
[32m[20221213 23:54:37 @agent_ppo2.py:121][0m #------------------------ Iteration 3996 --------------------------#
[32m[20221213 23:54:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:54:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:37 @agent_ppo2.py:185][0m |           0.0012 |          60.9927 |          11.1591 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0052 |          58.9581 |          11.1126 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0018 |          58.3264 |          11.1260 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0051 |          57.6552 |          11.1576 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0100 |          57.5414 |          11.1512 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0093 |          57.0236 |          11.1518 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0079 |          57.2087 |          11.1692 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0107 |          56.7143 |          11.1672 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0088 |          56.7145 |          11.1652 |
[32m[20221213 23:54:38 @agent_ppo2.py:185][0m |          -0.0095 |          56.5107 |          11.1416 |
[32m[20221213 23:54:38 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.86
[32m[20221213 23:54:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.67
[32m[20221213 23:54:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.05
[32m[20221213 23:54:38 @agent_ppo2.py:143][0m Total time:      42.11 min
[32m[20221213 23:54:38 @agent_ppo2.py:145][0m 4089856 total steps have happened
[32m[20221213 23:54:38 @agent_ppo2.py:121][0m #------------------------ Iteration 3997 --------------------------#
[32m[20221213 23:54:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:54:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |           0.0040 |          42.2200 |          10.7826 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0085 |          37.2229 |          10.8235 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0038 |          35.8812 |          10.8865 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0102 |          36.0450 |          10.8840 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0138 |          34.4248 |          10.9291 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0057 |          37.9333 |          10.9303 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0213 |          33.4337 |          10.8922 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0195 |          32.9173 |          10.9841 |
[32m[20221213 23:54:39 @agent_ppo2.py:185][0m |          -0.0214 |          32.7145 |          10.9358 |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |          -0.0241 |          32.2230 |          10.9593 |
[32m[20221213 23:54:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:54:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.88
[32m[20221213 23:54:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.90
[32m[20221213 23:54:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.49
[32m[20221213 23:54:40 @agent_ppo2.py:143][0m Total time:      42.13 min
[32m[20221213 23:54:40 @agent_ppo2.py:145][0m 4091904 total steps have happened
[32m[20221213 23:54:40 @agent_ppo2.py:121][0m #------------------------ Iteration 3998 --------------------------#
[32m[20221213 23:54:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |           0.0021 |          58.5606 |          11.3679 |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |          -0.0047 |          55.6141 |          11.3151 |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |          -0.0056 |          54.5879 |          11.2985 |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |          -0.0109 |          53.9128 |          11.3218 |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |          -0.0139 |          53.5326 |          11.2931 |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |          -0.0115 |          53.1765 |          11.2532 |
[32m[20221213 23:54:40 @agent_ppo2.py:185][0m |          -0.0116 |          52.9069 |          11.3068 |
[32m[20221213 23:54:41 @agent_ppo2.py:185][0m |          -0.0089 |          54.7572 |          11.2923 |
[32m[20221213 23:54:41 @agent_ppo2.py:185][0m |          -0.0147 |          52.6194 |          11.3052 |
[32m[20221213 23:54:41 @agent_ppo2.py:185][0m |          -0.0150 |          52.2689 |          11.2923 |
[32m[20221213 23:54:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.73
[32m[20221213 23:54:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.64
[32m[20221213 23:54:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.81
[32m[20221213 23:54:41 @agent_ppo2.py:143][0m Total time:      42.15 min
[32m[20221213 23:54:41 @agent_ppo2.py:145][0m 4093952 total steps have happened
[32m[20221213 23:54:41 @agent_ppo2.py:121][0m #------------------------ Iteration 3999 --------------------------#
[32m[20221213 23:54:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:54:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:54:41 @agent_ppo2.py:185][0m |          -0.0006 |          57.3334 |          11.5361 |
[32m[20221213 23:54:41 @agent_ppo2.py:185][0m |          -0.0085 |          53.7903 |          11.5591 |
[32m[20221213 23:54:41 @agent_ppo2.py:185][0m |          -0.0138 |          52.1361 |          11.5057 |
[32m[20221213 23:54:41 @agent_ppo2.py:185][0m |          -0.0110 |          51.0329 |          11.5563 |
[32m[20221213 23:54:42 @agent_ppo2.py:185][0m |          -0.0141 |          50.1947 |          11.5440 |
[32m[20221213 23:54:42 @agent_ppo2.py:185][0m |          -0.0120 |          49.6637 |          11.5195 |
[32m[20221213 23:54:42 @agent_ppo2.py:185][0m |          -0.0052 |          56.1796 |          11.5275 |
[32m[20221213 23:54:42 @agent_ppo2.py:185][0m |          -0.0135 |          49.9653 |          11.5064 |
[32m[20221213 23:54:42 @agent_ppo2.py:185][0m |          -0.0172 |          48.4425 |          11.5393 |
[32m[20221213 23:54:42 @agent_ppo2.py:185][0m |          -0.0076 |          50.8695 |          11.5001 |
[32m[20221213 23:54:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 23:54:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.09
[32m[20221213 23:54:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.80
[32m[20221213 23:54:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.06
[32m[20221213 23:54:42 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 602.07
[32m[20221213 23:54:42 @agent_ppo2.py:143][0m Total time:      42.17 min
[32m[20221213 23:54:42 @agent_ppo2.py:145][0m 4096000 total steps have happened
[32m[20221213 23:54:42 @train.py:54][0m [4m[34mCRITICAL[0m Training completed!
