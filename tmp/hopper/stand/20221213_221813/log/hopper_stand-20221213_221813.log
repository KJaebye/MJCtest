[32m[20221213 22:18:13 @logger.py:105][0m Log file set to ./tmp/hopper/stand/20221213_221813/log/hopper_stand-20221213_221813.log
[32m[20221213 22:18:13 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 22:18:13 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 22:18:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:13 @agent_ppo2.py:185][0m |           0.0012 |           0.4713 |           1.8725 |
[32m[20221213 22:18:13 @agent_ppo2.py:185][0m |           0.0007 |           0.4573 |           1.8716 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0124 |           0.4858 |           1.8705 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0009 |           0.4334 |           1.8688 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0014 |           0.4265 |           1.8676 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0021 |           0.4215 |           1.8663 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0177 |           0.4686 |           1.8653 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0032 |           0.4157 |           1.8640 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0036 |           0.4141 |           1.8630 |
[32m[20221213 22:18:14 @agent_ppo2.py:185][0m |          -0.0039 |           0.4123 |           1.8623 |
[32m[20221213 22:18:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:18:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 11.35
[32m[20221213 22:18:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 15.76
[32m[20221213 22:18:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:14 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 0.00
[32m[20221213 22:18:14 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 0.00
[32m[20221213 22:18:14 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221213 22:18:14 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 22:18:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 22:18:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:18:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |           0.0039 |           0.0384 |           1.8504 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0000 |           0.0274 |           1.8499 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |           0.0005 |           0.0235 |           1.8499 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0061 |           0.0214 |           1.8496 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0106 |           0.0198 |           1.8487 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0137 |           0.0185 |           1.8479 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0100 |           0.0175 |           1.8474 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0065 |           0.0165 |           1.8466 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0100 |           0.0157 |           1.8465 |
[32m[20221213 22:18:15 @agent_ppo2.py:185][0m |          -0.0115 |           0.0149 |           1.8465 |
[32m[20221213 22:18:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:18:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:16 @agent_ppo2.py:143][0m Total time:       0.04 min
[32m[20221213 22:18:16 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 22:18:16 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 22:18:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:16 @agent_ppo2.py:185][0m |           0.0054 |           0.0129 |           1.8378 |
[32m[20221213 22:18:16 @agent_ppo2.py:185][0m |          -0.0041 |           0.0120 |           1.8371 |
[32m[20221213 22:18:16 @agent_ppo2.py:185][0m |          -0.0074 |           0.0114 |           1.8364 |
[32m[20221213 22:18:16 @agent_ppo2.py:185][0m |          -0.0168 |           0.0107 |           1.8351 |
[32m[20221213 22:18:16 @agent_ppo2.py:185][0m |           0.0025 |           0.0102 |           1.8341 |
[32m[20221213 22:18:16 @agent_ppo2.py:185][0m |          -0.0097 |           0.0096 |           1.8329 |
[32m[20221213 22:18:16 @agent_ppo2.py:185][0m |          -0.0021 |           0.0091 |           1.8324 |
[32m[20221213 22:18:17 @agent_ppo2.py:185][0m |          -0.0131 |           0.0086 |           1.8323 |
[32m[20221213 22:18:17 @agent_ppo2.py:185][0m |          -0.0188 |           0.0081 |           1.8321 |
[32m[20221213 22:18:17 @agent_ppo2.py:185][0m |          -0.0103 |           0.0076 |           1.8324 |
[32m[20221213 22:18:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:18:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:17 @agent_ppo2.py:143][0m Total time:       0.07 min
[32m[20221213 22:18:17 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 22:18:17 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 22:18:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:17 @agent_ppo2.py:185][0m |           0.0030 |           0.0070 |           1.8319 |
[32m[20221213 22:18:17 @agent_ppo2.py:185][0m |          -0.0025 |           0.0066 |           1.8302 |
[32m[20221213 22:18:17 @agent_ppo2.py:185][0m |          -0.0174 |           0.0061 |           1.8297 |
[32m[20221213 22:18:17 @agent_ppo2.py:185][0m |          -0.0050 |           0.0058 |           1.8294 |
[32m[20221213 22:18:18 @agent_ppo2.py:185][0m |          -0.0195 |           0.0054 |           1.8304 |
[32m[20221213 22:18:18 @agent_ppo2.py:185][0m |          -0.0057 |           0.0050 |           1.8313 |
[32m[20221213 22:18:18 @agent_ppo2.py:185][0m |          -0.0145 |           0.0047 |           1.8318 |
[32m[20221213 22:18:18 @agent_ppo2.py:185][0m |          -0.0156 |           0.0044 |           1.8329 |
[32m[20221213 22:18:18 @agent_ppo2.py:185][0m |          -0.0056 |           0.0041 |           1.8341 |
[32m[20221213 22:18:18 @agent_ppo2.py:185][0m |          -0.0135 |           0.0038 |           1.8342 |
[32m[20221213 22:18:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:18:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:18 @agent_ppo2.py:143][0m Total time:       0.09 min
[32m[20221213 22:18:18 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 22:18:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 22:18:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:18 @agent_ppo2.py:185][0m |          -0.0057 |           0.0035 |           1.8953 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0066 |           0.0033 |           1.8939 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0110 |           0.0030 |           1.8930 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0095 |           0.0028 |           1.8936 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0147 |           0.0026 |           1.8945 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0103 |           0.0024 |           1.8952 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0127 |           0.0022 |           1.8955 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0195 |           0.0021 |           1.8968 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0225 |           0.0019 |           1.8980 |
[32m[20221213 22:18:19 @agent_ppo2.py:185][0m |          -0.0136 |           0.0017 |           1.8985 |
[32m[20221213 22:18:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:18:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:19 @agent_ppo2.py:143][0m Total time:       0.11 min
[32m[20221213 22:18:19 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 22:18:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 22:18:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:18:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |           0.0011 |           0.5757 |           1.8772 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0012 |           0.2880 |           1.8776 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0019 |           0.2216 |           1.8775 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0129 |           0.1756 |           1.8781 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0029 |           0.1473 |           1.8790 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0031 |           0.1318 |           1.8797 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0034 |           0.1172 |           1.8804 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0037 |           0.1067 |           1.8809 |
[32m[20221213 22:18:20 @agent_ppo2.py:185][0m |          -0.0055 |           0.0974 |           1.8818 |
[32m[20221213 22:18:21 @agent_ppo2.py:185][0m |          -0.0043 |           0.0914 |           1.8825 |
[32m[20221213 22:18:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:18:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 15.51
[32m[20221213 22:18:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 17.03
[32m[20221213 22:18:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:21 @agent_ppo2.py:143][0m Total time:       0.13 min
[32m[20221213 22:18:21 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 22:18:21 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 22:18:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:21 @agent_ppo2.py:185][0m |          -0.0038 |           0.2646 |           1.9018 |
[32m[20221213 22:18:21 @agent_ppo2.py:185][0m |          -0.0071 |           0.1756 |           1.9008 |
[32m[20221213 22:18:21 @agent_ppo2.py:185][0m |          -0.0039 |           0.1585 |           1.8992 |
[32m[20221213 22:18:21 @agent_ppo2.py:185][0m |          -0.0149 |           0.1474 |           1.8978 |
[32m[20221213 22:18:21 @agent_ppo2.py:185][0m |          -0.0091 |           0.1429 |           1.8973 |
[32m[20221213 22:18:21 @agent_ppo2.py:185][0m |          -0.0103 |           0.1388 |           1.8970 |
[32m[20221213 22:18:22 @agent_ppo2.py:185][0m |          -0.0079 |           0.1354 |           1.8963 |
[32m[20221213 22:18:22 @agent_ppo2.py:185][0m |          -0.0094 |           0.1387 |           1.8959 |
[32m[20221213 22:18:22 @agent_ppo2.py:185][0m |          -0.0139 |           0.1352 |           1.8957 |
[32m[20221213 22:18:22 @agent_ppo2.py:185][0m |          -0.0115 |           0.1315 |           1.8947 |
[32m[20221213 22:18:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:18:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:22 @agent_ppo2.py:143][0m Total time:       0.15 min
[32m[20221213 22:18:22 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 22:18:22 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 22:18:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:18:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:22 @agent_ppo2.py:185][0m |           0.0018 |           0.1317 |           1.9065 |
[32m[20221213 22:18:22 @agent_ppo2.py:185][0m |          -0.0029 |           0.1026 |           1.9057 |
[32m[20221213 22:18:22 @agent_ppo2.py:185][0m |          -0.0086 |           0.0980 |           1.9047 |
[32m[20221213 22:18:23 @agent_ppo2.py:185][0m |          -0.0044 |           0.0944 |           1.9042 |
[32m[20221213 22:18:23 @agent_ppo2.py:185][0m |           0.0048 |           0.0926 |           1.9032 |
[32m[20221213 22:18:23 @agent_ppo2.py:185][0m |          -0.0079 |           0.0918 |           1.9025 |
[32m[20221213 22:18:23 @agent_ppo2.py:185][0m |          -0.0068 |           0.0893 |           1.9016 |
[32m[20221213 22:18:23 @agent_ppo2.py:185][0m |          -0.0062 |           0.0891 |           1.9005 |
[32m[20221213 22:18:23 @agent_ppo2.py:185][0m |          -0.0110 |           0.0879 |           1.8999 |
[32m[20221213 22:18:23 @agent_ppo2.py:185][0m |          -0.0104 |           0.0877 |           1.8996 |
[32m[20221213 22:18:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:18:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:23 @agent_ppo2.py:143][0m Total time:       0.17 min
[32m[20221213 22:18:23 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 22:18:23 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 22:18:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0033 |           0.0279 |           1.9085 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0024 |           0.0199 |           1.9080 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0012 |           0.0194 |           1.9067 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0047 |           0.0189 |           1.9055 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0090 |           0.0186 |           1.9042 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0063 |           0.0184 |           1.9033 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0032 |           0.0179 |           1.9023 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0060 |           0.0177 |           1.9012 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0094 |           0.0173 |           1.9004 |
[32m[20221213 22:18:24 @agent_ppo2.py:185][0m |          -0.0067 |           0.0170 |           1.9004 |
[32m[20221213 22:18:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:18:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:24 @agent_ppo2.py:143][0m Total time:       0.19 min
[32m[20221213 22:18:24 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 22:18:24 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 22:18:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0047 |           0.0157 |           1.9265 |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0029 |           0.0139 |           1.9250 |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0028 |           0.0136 |           1.9227 |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0164 |           0.0128 |           1.9207 |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0314 |           0.0128 |           1.9199 |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0158 |           0.0122 |           1.9197 |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0185 |           0.0118 |           1.9203 |
[32m[20221213 22:18:25 @agent_ppo2.py:185][0m |          -0.0151 |           0.0114 |           1.9201 |
[32m[20221213 22:18:26 @agent_ppo2.py:185][0m |          -0.0188 |           0.0113 |           1.9203 |
[32m[20221213 22:18:26 @agent_ppo2.py:185][0m |          -0.0250 |           0.0111 |           1.9199 |
[32m[20221213 22:18:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:18:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:26 @agent_ppo2.py:143][0m Total time:       0.21 min
[32m[20221213 22:18:26 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 22:18:26 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 22:18:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:26 @agent_ppo2.py:185][0m |           0.0005 |           0.0059 |           1.8955 |
[32m[20221213 22:18:26 @agent_ppo2.py:185][0m |          -0.0090 |           0.0054 |           1.8939 |
[32m[20221213 22:18:26 @agent_ppo2.py:185][0m |          -0.0133 |           0.0053 |           1.8913 |
[32m[20221213 22:18:26 @agent_ppo2.py:185][0m |          -0.0151 |           0.0052 |           1.8887 |
[32m[20221213 22:18:27 @agent_ppo2.py:185][0m |          -0.0151 |           0.0051 |           1.8874 |
[32m[20221213 22:18:27 @agent_ppo2.py:185][0m |          -0.0181 |           0.0049 |           1.8863 |
[32m[20221213 22:18:27 @agent_ppo2.py:185][0m |          -0.0164 |           0.0048 |           1.8859 |
[32m[20221213 22:18:27 @agent_ppo2.py:185][0m |          -0.0220 |           0.0047 |           1.8854 |
[32m[20221213 22:18:27 @agent_ppo2.py:185][0m |          -0.0121 |           0.0046 |           1.8853 |
[32m[20221213 22:18:27 @agent_ppo2.py:185][0m |          -0.0159 |           0.0045 |           1.8850 |
[32m[20221213 22:18:27 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 22:18:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 27.84
[32m[20221213 22:18:27 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 27.84
[32m[20221213 22:18:27 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 27.84
[32m[20221213 22:18:27 @agent_ppo2.py:143][0m Total time:       0.24 min
[32m[20221213 22:18:27 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 22:18:27 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 22:18:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |           0.0015 |           0.5255 |           1.9361 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |           0.0006 |           0.3029 |           1.9359 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0006 |           0.2502 |           1.9358 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0016 |           0.2050 |           1.9354 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0092 |           0.1869 |           1.9350 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0028 |           0.1687 |           1.9349 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0033 |           0.1582 |           1.9347 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0038 |           0.1479 |           1.9348 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0040 |           0.1419 |           1.9347 |
[32m[20221213 22:18:28 @agent_ppo2.py:185][0m |          -0.0045 |           0.1373 |           1.9348 |
[32m[20221213 22:18:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:18:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 16.95
[32m[20221213 22:18:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 25.06
[32m[20221213 22:18:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:29 @agent_ppo2.py:143][0m Total time:       0.26 min
[32m[20221213 22:18:29 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 22:18:29 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 22:18:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:29 @agent_ppo2.py:185][0m |          -0.0030 |           0.2626 |           1.9109 |
[32m[20221213 22:18:29 @agent_ppo2.py:185][0m |          -0.0040 |           0.1912 |           1.9097 |
[32m[20221213 22:18:29 @agent_ppo2.py:185][0m |          -0.0078 |           0.1813 |           1.9078 |
[32m[20221213 22:18:29 @agent_ppo2.py:185][0m |          -0.0024 |           0.1778 |           1.9068 |
[32m[20221213 22:18:29 @agent_ppo2.py:185][0m |           0.0014 |           0.1785 |           1.9048 |
[32m[20221213 22:18:29 @agent_ppo2.py:185][0m |          -0.0052 |           0.1719 |           1.9038 |
[32m[20221213 22:18:29 @agent_ppo2.py:185][0m |          -0.0108 |           0.1691 |           1.9034 |
[32m[20221213 22:18:30 @agent_ppo2.py:185][0m |          -0.0138 |           0.1708 |           1.9029 |
[32m[20221213 22:18:30 @agent_ppo2.py:185][0m |          -0.0148 |           0.1681 |           1.9021 |
[32m[20221213 22:18:30 @agent_ppo2.py:185][0m |          -0.0128 |           0.1675 |           1.9011 |
[32m[20221213 22:18:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:18:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.92
[32m[20221213 22:18:30 @agent_ppo2.py:143][0m Total time:       0.28 min
[32m[20221213 22:18:30 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 22:18:30 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 22:18:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:30 @agent_ppo2.py:185][0m |          -0.0041 |           0.1090 |           1.8699 |
[32m[20221213 22:18:30 @agent_ppo2.py:185][0m |          -0.0123 |           0.0745 |           1.8680 |
[32m[20221213 22:18:30 @agent_ppo2.py:185][0m |          -0.0102 |           0.0729 |           1.8658 |
[32m[20221213 22:18:30 @agent_ppo2.py:185][0m |          -0.0162 |           0.0712 |           1.8642 |
[32m[20221213 22:18:31 @agent_ppo2.py:185][0m |          -0.0238 |           0.0702 |           1.8633 |
[32m[20221213 22:18:31 @agent_ppo2.py:185][0m |          -0.0185 |           0.0702 |           1.8624 |
[32m[20221213 22:18:31 @agent_ppo2.py:185][0m |          -0.0136 |           0.0691 |           1.8619 |
[32m[20221213 22:18:31 @agent_ppo2.py:185][0m |          -0.0142 |           0.0682 |           1.8614 |
[32m[20221213 22:18:31 @agent_ppo2.py:185][0m |          -0.0085 |           0.0691 |           1.8610 |
[32m[20221213 22:18:31 @agent_ppo2.py:185][0m |          -0.0109 |           0.0695 |           1.8608 |
[32m[20221213 22:18:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:18:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.50
[32m[20221213 22:18:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.50
[32m[20221213 22:18:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.87
[32m[20221213 22:18:31 @agent_ppo2.py:143][0m Total time:       0.31 min
[32m[20221213 22:18:31 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 22:18:31 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 22:18:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |           0.0152 |           0.0552 |           1.9331 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0090 |           0.0423 |           1.9314 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0065 |           0.0412 |           1.9301 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0025 |           0.0412 |           1.9281 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0116 |           0.0411 |           1.9261 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0063 |           0.0407 |           1.9255 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0114 |           0.0406 |           1.9249 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0102 |           0.0400 |           1.9246 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0120 |           0.0400 |           1.9240 |
[32m[20221213 22:18:32 @agent_ppo2.py:185][0m |          -0.0102 |           0.0398 |           1.9238 |
[32m[20221213 22:18:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:18:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:33 @agent_ppo2.py:143][0m Total time:       0.33 min
[32m[20221213 22:18:33 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 22:18:33 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 22:18:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |           0.0057 |           0.0271 |           1.8750 |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |          -0.0034 |           0.0238 |           1.8725 |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |          -0.0108 |           0.0234 |           1.8694 |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |          -0.0107 |           0.0232 |           1.8675 |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |          -0.0150 |           0.0227 |           1.8668 |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |          -0.0027 |           0.0237 |           1.8665 |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |          -0.0106 |           0.0225 |           1.8661 |
[32m[20221213 22:18:33 @agent_ppo2.py:185][0m |          -0.0159 |           0.0222 |           1.8664 |
[32m[20221213 22:18:34 @agent_ppo2.py:185][0m |          -0.0116 |           0.0220 |           1.8657 |
[32m[20221213 22:18:34 @agent_ppo2.py:185][0m |          -0.0197 |           0.0219 |           1.8658 |
[32m[20221213 22:18:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:34 @agent_ppo2.py:143][0m Total time:       0.35 min
[32m[20221213 22:18:34 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 22:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 22:18:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:34 @agent_ppo2.py:185][0m |          -0.0040 |           0.0214 |           1.8749 |
[32m[20221213 22:18:34 @agent_ppo2.py:185][0m |          -0.0054 |           0.0197 |           1.8742 |
[32m[20221213 22:18:34 @agent_ppo2.py:185][0m |          -0.0091 |           0.0196 |           1.8732 |
[32m[20221213 22:18:34 @agent_ppo2.py:185][0m |          -0.0105 |           0.0194 |           1.8719 |
[32m[20221213 22:18:34 @agent_ppo2.py:185][0m |          -0.0073 |           0.0195 |           1.8705 |
[32m[20221213 22:18:35 @agent_ppo2.py:185][0m |          -0.0030 |           0.0201 |           1.8696 |
[32m[20221213 22:18:35 @agent_ppo2.py:185][0m |          -0.0156 |           0.0191 |           1.8692 |
[32m[20221213 22:18:35 @agent_ppo2.py:185][0m |          -0.0148 |           0.0192 |           1.8690 |
[32m[20221213 22:18:35 @agent_ppo2.py:185][0m |          -0.0133 |           0.0191 |           1.8688 |
[32m[20221213 22:18:35 @agent_ppo2.py:185][0m |          -0.0139 |           0.0189 |           1.8682 |
[32m[20221213 22:18:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:18:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:35 @agent_ppo2.py:143][0m Total time:       0.37 min
[32m[20221213 22:18:35 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 22:18:35 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 22:18:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:35 @agent_ppo2.py:185][0m |          -0.0038 |           0.0111 |           1.8939 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0095 |           0.0103 |           1.8924 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0155 |           0.0101 |           1.8902 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0104 |           0.0100 |           1.8882 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0057 |           0.0101 |           1.8870 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0129 |           0.0099 |           1.8866 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0130 |           0.0097 |           1.8870 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0124 |           0.0097 |           1.8865 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0169 |           0.0096 |           1.8863 |
[32m[20221213 22:18:36 @agent_ppo2.py:185][0m |          -0.0150 |           0.0095 |           1.8867 |
[32m[20221213 22:18:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:18:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:36 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 22:18:36 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 22:18:36 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 22:18:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:18:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |           0.0125 |           0.0090 |           1.8933 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0029 |           0.0078 |           1.8913 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0052 |           0.0077 |           1.8892 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0076 |           0.0076 |           1.8883 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0076 |           0.0075 |           1.8876 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0086 |           0.0074 |           1.8874 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0120 |           0.0074 |           1.8875 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0084 |           0.0073 |           1.8884 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0111 |           0.0072 |           1.8889 |
[32m[20221213 22:18:37 @agent_ppo2.py:185][0m |          -0.0076 |           0.0073 |           1.8896 |
[32m[20221213 22:18:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:18:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.85
[32m[20221213 22:18:38 @agent_ppo2.py:143][0m Total time:       0.41 min
[32m[20221213 22:18:38 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 22:18:38 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 22:18:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:38 @agent_ppo2.py:185][0m |          -0.0027 |           0.0065 |           1.8949 |
[32m[20221213 22:18:38 @agent_ppo2.py:185][0m |          -0.0059 |           0.0060 |           1.8944 |
[32m[20221213 22:18:38 @agent_ppo2.py:185][0m |          -0.0044 |           0.0060 |           1.8933 |
[32m[20221213 22:18:38 @agent_ppo2.py:185][0m |          -0.0022 |           0.0060 |           1.8922 |
[32m[20221213 22:18:38 @agent_ppo2.py:185][0m |          -0.0089 |           0.0059 |           1.8904 |
[32m[20221213 22:18:38 @agent_ppo2.py:185][0m |          -0.0100 |           0.0058 |           1.8900 |
[32m[20221213 22:18:38 @agent_ppo2.py:185][0m |          -0.0103 |           0.0058 |           1.8918 |
[32m[20221213 22:18:39 @agent_ppo2.py:185][0m |          -0.0094 |           0.0057 |           1.8909 |
[32m[20221213 22:18:39 @agent_ppo2.py:185][0m |          -0.0098 |           0.0056 |           1.8911 |
[32m[20221213 22:18:39 @agent_ppo2.py:185][0m |          -0.0070 |           0.0056 |           1.8905 |
[32m[20221213 22:18:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:18:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:39 @agent_ppo2.py:143][0m Total time:       0.43 min
[32m[20221213 22:18:39 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 22:18:39 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 22:18:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:18:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:39 @agent_ppo2.py:185][0m |          -0.0014 |           0.0052 |           1.9330 |
[32m[20221213 22:18:39 @agent_ppo2.py:185][0m |           0.0090 |           0.0051 |           1.9331 |
[32m[20221213 22:18:39 @agent_ppo2.py:185][0m |           0.0116 |           0.0051 |           1.9333 |
[32m[20221213 22:18:39 @agent_ppo2.py:185][0m |          -0.0038 |           0.0048 |           1.9322 |
[32m[20221213 22:18:40 @agent_ppo2.py:185][0m |          -0.0040 |           0.0047 |           1.9317 |
[32m[20221213 22:18:40 @agent_ppo2.py:185][0m |          -0.0060 |           0.0046 |           1.9315 |
[32m[20221213 22:18:40 @agent_ppo2.py:185][0m |          -0.0061 |           0.0046 |           1.9309 |
[32m[20221213 22:18:40 @agent_ppo2.py:185][0m |          -0.0063 |           0.0045 |           1.9312 |
[32m[20221213 22:18:40 @agent_ppo2.py:185][0m |           0.0055 |           0.0048 |           1.9303 |
[32m[20221213 22:18:40 @agent_ppo2.py:185][0m |          -0.0076 |           0.0045 |           1.9312 |
[32m[20221213 22:18:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:18:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:40 @agent_ppo2.py:143][0m Total time:       0.45 min
[32m[20221213 22:18:40 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 22:18:40 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 22:18:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:40 @agent_ppo2.py:185][0m |          -0.0027 |           0.0038 |           1.8720 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0038 |           0.0035 |           1.8718 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0050 |           0.0035 |           1.8708 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0057 |           0.0034 |           1.8699 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0071 |           0.0034 |           1.8688 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0065 |           0.0033 |           1.8687 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0081 |           0.0033 |           1.8681 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0078 |           0.0032 |           1.8678 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0079 |           0.0032 |           1.8675 |
[32m[20221213 22:18:41 @agent_ppo2.py:185][0m |          -0.0082 |           0.0031 |           1.8675 |
[32m[20221213 22:18:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:18:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:41 @agent_ppo2.py:143][0m Total time:       0.48 min
[32m[20221213 22:18:41 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 22:18:41 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 22:18:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0012 |           0.0028 |           1.8971 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0030 |           0.0025 |           1.8977 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0048 |           0.0025 |           1.8974 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0046 |           0.0025 |           1.8975 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0049 |           0.0024 |           1.8975 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0045 |           0.0024 |           1.8986 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0064 |           0.0024 |           1.8988 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0063 |           0.0023 |           1.8993 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0068 |           0.0023 |           1.8999 |
[32m[20221213 22:18:42 @agent_ppo2.py:185][0m |          -0.0079 |           0.0023 |           1.9005 |
[32m[20221213 22:18:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:18:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:43 @agent_ppo2.py:143][0m Total time:       0.50 min
[32m[20221213 22:18:43 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 22:18:43 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 22:18:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:43 @agent_ppo2.py:185][0m |          -0.0020 |           0.0020 |           1.9353 |
[32m[20221213 22:18:43 @agent_ppo2.py:185][0m |          -0.0007 |           0.0018 |           1.9346 |
[32m[20221213 22:18:43 @agent_ppo2.py:185][0m |           0.0069 |           0.0018 |           1.9340 |
[32m[20221213 22:18:43 @agent_ppo2.py:185][0m |          -0.0037 |           0.0017 |           1.9334 |
[32m[20221213 22:18:43 @agent_ppo2.py:185][0m |          -0.0047 |           0.0017 |           1.9328 |
[32m[20221213 22:18:43 @agent_ppo2.py:185][0m |          -0.0047 |           0.0017 |           1.9327 |
[32m[20221213 22:18:43 @agent_ppo2.py:185][0m |          -0.0043 |           0.0016 |           1.9325 |
[32m[20221213 22:18:44 @agent_ppo2.py:185][0m |          -0.0053 |           0.0016 |           1.9332 |
[32m[20221213 22:18:44 @agent_ppo2.py:185][0m |          -0.0042 |           0.0016 |           1.9334 |
[32m[20221213 22:18:44 @agent_ppo2.py:185][0m |          -0.0056 |           0.0016 |           1.9331 |
[32m[20221213 22:18:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:18:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:44 @agent_ppo2.py:143][0m Total time:       0.52 min
[32m[20221213 22:18:44 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 22:18:44 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 22:18:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:44 @agent_ppo2.py:185][0m |          -0.0025 |           0.0014 |           1.9257 |
[32m[20221213 22:18:44 @agent_ppo2.py:185][0m |          -0.0045 |           0.0013 |           1.9266 |
[32m[20221213 22:18:44 @agent_ppo2.py:185][0m |          -0.0064 |           0.0012 |           1.9261 |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |          -0.0065 |           0.0012 |           1.9247 |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |          -0.0033 |           0.0012 |           1.9245 |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |          -0.0088 |           0.0012 |           1.9244 |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |          -0.0087 |           0.0012 |           1.9252 |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |          -0.0091 |           0.0012 |           1.9248 |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |          -0.0088 |           0.0012 |           1.9253 |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |          -0.0038 |           0.0012 |           1.9259 |
[32m[20221213 22:18:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:18:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:45 @agent_ppo2.py:143][0m Total time:       0.54 min
[32m[20221213 22:18:45 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 22:18:45 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 22:18:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:45 @agent_ppo2.py:185][0m |           0.0009 |           0.2310 |           1.9368 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0018 |           0.1571 |           1.9376 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0028 |           0.1522 |           1.9385 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0033 |           0.1489 |           1.9400 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0037 |           0.1469 |           1.9412 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0041 |           0.1439 |           1.9428 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0037 |           0.1408 |           1.9443 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0046 |           0.1388 |           1.9459 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0047 |           0.1360 |           1.9472 |
[32m[20221213 22:18:46 @agent_ppo2.py:185][0m |          -0.0046 |           0.1327 |           1.9489 |
[32m[20221213 22:18:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:18:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.71
[32m[20221213 22:18:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 13.56
[32m[20221213 22:18:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:46 @agent_ppo2.py:143][0m Total time:       0.56 min
[32m[20221213 22:18:46 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 22:18:46 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 22:18:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |          -0.0014 |           0.0270 |           1.9536 |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |          -0.0015 |           0.0078 |           1.9519 |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |          -0.0019 |           0.0070 |           1.9500 |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |          -0.0058 |           0.0067 |           1.9478 |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |          -0.0060 |           0.0066 |           1.9465 |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |          -0.0040 |           0.0065 |           1.9446 |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |          -0.0047 |           0.0064 |           1.9439 |
[32m[20221213 22:18:47 @agent_ppo2.py:185][0m |           0.0015 |           0.0064 |           1.9432 |
[32m[20221213 22:18:48 @agent_ppo2.py:185][0m |          -0.0069 |           0.0063 |           1.9418 |
[32m[20221213 22:18:48 @agent_ppo2.py:185][0m |          -0.0078 |           0.0063 |           1.9407 |
[32m[20221213 22:18:48 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:18:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:48 @agent_ppo2.py:143][0m Total time:       0.58 min
[32m[20221213 22:18:48 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 22:18:48 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 22:18:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:48 @agent_ppo2.py:185][0m |          -0.0033 |           0.0062 |           1.9483 |
[32m[20221213 22:18:48 @agent_ppo2.py:185][0m |          -0.0032 |           0.0034 |           1.9476 |
[32m[20221213 22:18:48 @agent_ppo2.py:185][0m |          -0.0101 |           0.0031 |           1.9459 |
[32m[20221213 22:18:48 @agent_ppo2.py:185][0m |          -0.0107 |           0.0031 |           1.9445 |
[32m[20221213 22:18:48 @agent_ppo2.py:185][0m |          -0.0152 |           0.0030 |           1.9439 |
[32m[20221213 22:18:49 @agent_ppo2.py:185][0m |          -0.0160 |           0.0030 |           1.9429 |
[32m[20221213 22:18:49 @agent_ppo2.py:185][0m |          -0.0105 |           0.0029 |           1.9428 |
[32m[20221213 22:18:49 @agent_ppo2.py:185][0m |          -0.0165 |           0.0029 |           1.9430 |
[32m[20221213 22:18:49 @agent_ppo2.py:185][0m |          -0.0123 |           0.0028 |           1.9422 |
[32m[20221213 22:18:49 @agent_ppo2.py:185][0m |          -0.0155 |           0.0028 |           1.9422 |
[32m[20221213 22:18:49 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:18:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:49 @agent_ppo2.py:143][0m Total time:       0.60 min
[32m[20221213 22:18:49 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 22:18:49 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 22:18:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:49 @agent_ppo2.py:185][0m |          -0.0025 |           0.0029 |           1.9904 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0005 |           0.0025 |           1.9899 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0015 |           0.0024 |           1.9896 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0092 |           0.0024 |           1.9886 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0080 |           0.0024 |           1.9882 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0103 |           0.0023 |           1.9875 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |           0.0010 |           0.0023 |           1.9873 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0113 |           0.0023 |           1.9873 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0118 |           0.0023 |           1.9874 |
[32m[20221213 22:18:50 @agent_ppo2.py:185][0m |          -0.0072 |           0.0023 |           1.9871 |
[32m[20221213 22:18:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:18:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:50 @agent_ppo2.py:143][0m Total time:       0.62 min
[32m[20221213 22:18:50 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 22:18:50 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 22:18:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0016 |           0.0021 |           1.9558 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |           0.0068 |           0.0019 |           1.9558 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0040 |           0.0018 |           1.9548 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0084 |           0.0018 |           1.9537 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0060 |           0.0018 |           1.9522 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0094 |           0.0018 |           1.9511 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0096 |           0.0018 |           1.9509 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0114 |           0.0017 |           1.9494 |
[32m[20221213 22:18:51 @agent_ppo2.py:185][0m |          -0.0098 |           0.0017 |           1.9494 |
[32m[20221213 22:18:52 @agent_ppo2.py:185][0m |          -0.0087 |           0.0017 |           1.9487 |
[32m[20221213 22:18:52 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:18:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:52 @agent_ppo2.py:143][0m Total time:       0.65 min
[32m[20221213 22:18:52 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 22:18:52 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 22:18:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:18:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:52 @agent_ppo2.py:185][0m |          -0.0021 |           0.0016 |           1.9476 |
[32m[20221213 22:18:52 @agent_ppo2.py:185][0m |          -0.0037 |           0.0015 |           1.9463 |
[32m[20221213 22:18:52 @agent_ppo2.py:185][0m |          -0.0078 |           0.0014 |           1.9429 |
[32m[20221213 22:18:52 @agent_ppo2.py:185][0m |          -0.0061 |           0.0014 |           1.9413 |
[32m[20221213 22:18:52 @agent_ppo2.py:185][0m |          -0.0125 |           0.0014 |           1.9403 |
[32m[20221213 22:18:52 @agent_ppo2.py:185][0m |          -0.0106 |           0.0014 |           1.9389 |
[32m[20221213 22:18:53 @agent_ppo2.py:185][0m |          -0.0128 |           0.0014 |           1.9383 |
[32m[20221213 22:18:53 @agent_ppo2.py:185][0m |          -0.0124 |           0.0013 |           1.9378 |
[32m[20221213 22:18:53 @agent_ppo2.py:185][0m |          -0.0112 |           0.0013 |           1.9372 |
[32m[20221213 22:18:53 @agent_ppo2.py:185][0m |          -0.0115 |           0.0013 |           1.9363 |
[32m[20221213 22:18:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:18:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:53 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221213 22:18:53 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 22:18:53 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 22:18:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:18:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:53 @agent_ppo2.py:185][0m |          -0.0031 |           0.0013 |           1.9564 |
[32m[20221213 22:18:53 @agent_ppo2.py:185][0m |           0.0066 |           0.0012 |           1.9553 |
[32m[20221213 22:18:53 @agent_ppo2.py:185][0m |          -0.0061 |           0.0012 |           1.9531 |
[32m[20221213 22:18:54 @agent_ppo2.py:185][0m |          -0.0042 |           0.0011 |           1.9518 |
[32m[20221213 22:18:54 @agent_ppo2.py:185][0m |           0.0057 |           0.0012 |           1.9503 |
[32m[20221213 22:18:54 @agent_ppo2.py:185][0m |          -0.0066 |           0.0011 |           1.9500 |
[32m[20221213 22:18:54 @agent_ppo2.py:185][0m |          -0.0088 |           0.0011 |           1.9489 |
[32m[20221213 22:18:54 @agent_ppo2.py:185][0m |          -0.0097 |           0.0011 |           1.9481 |
[32m[20221213 22:18:54 @agent_ppo2.py:185][0m |          -0.0084 |           0.0010 |           1.9469 |
[32m[20221213 22:18:54 @agent_ppo2.py:185][0m |          -0.0022 |           0.0011 |           1.9464 |
[32m[20221213 22:18:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:18:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:54 @agent_ppo2.py:143][0m Total time:       0.69 min
[32m[20221213 22:18:54 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 22:18:54 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 22:18:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:18:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0020 |           0.0010 |           1.9645 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0031 |           0.0009 |           1.9625 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0049 |           0.0009 |           1.9604 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0063 |           0.0009 |           1.9579 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0068 |           0.0009 |           1.9559 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0099 |           0.0009 |           1.9540 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0083 |           0.0009 |           1.9518 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0098 |           0.0008 |           1.9497 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0118 |           0.0008 |           1.9480 |
[32m[20221213 22:18:55 @agent_ppo2.py:185][0m |          -0.0091 |           0.0008 |           1.9464 |
[32m[20221213 22:18:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 24.84
[32m[20221213 22:18:55 @agent_ppo2.py:143][0m Total time:       0.71 min
[32m[20221213 22:18:55 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 22:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 22:18:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:18:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0010 |           0.0007 |           1.9077 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0039 |           0.0007 |           1.9072 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0016 |           0.0006 |           1.9060 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0015 |           0.0006 |           1.9053 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0032 |           0.0006 |           1.9043 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0065 |           0.0006 |           1.9033 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0006 |           0.0006 |           1.9024 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0010 |           0.0006 |           1.9017 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0065 |           0.0006 |           1.9014 |
[32m[20221213 22:18:56 @agent_ppo2.py:185][0m |          -0.0005 |           0.0006 |           1.9007 |
[32m[20221213 22:18:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:18:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:57 @agent_ppo2.py:143][0m Total time:       0.73 min
[32m[20221213 22:18:57 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 22:18:57 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 22:18:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:18:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:57 @agent_ppo2.py:185][0m |          -0.0029 |           0.0005 |           1.9122 |
[32m[20221213 22:18:57 @agent_ppo2.py:185][0m |          -0.0039 |           0.0004 |           1.9121 |
[32m[20221213 22:18:57 @agent_ppo2.py:185][0m |          -0.0067 |           0.0004 |           1.9109 |
[32m[20221213 22:18:57 @agent_ppo2.py:185][0m |          -0.0068 |           0.0004 |           1.9096 |
[32m[20221213 22:18:57 @agent_ppo2.py:185][0m |          -0.0006 |           0.0004 |           1.9080 |
[32m[20221213 22:18:57 @agent_ppo2.py:185][0m |          -0.0061 |           0.0004 |           1.9071 |
[32m[20221213 22:18:57 @agent_ppo2.py:185][0m |          -0.0097 |           0.0004 |           1.9058 |
[32m[20221213 22:18:58 @agent_ppo2.py:185][0m |          -0.0097 |           0.0004 |           1.9044 |
[32m[20221213 22:18:58 @agent_ppo2.py:185][0m |          -0.0101 |           0.0004 |           1.9044 |
[32m[20221213 22:18:58 @agent_ppo2.py:185][0m |          -0.0102 |           0.0004 |           1.9041 |
[32m[20221213 22:18:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:18:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:18:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:18:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:18:58 @agent_ppo2.py:143][0m Total time:       0.75 min
[32m[20221213 22:18:58 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 22:18:58 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 22:18:58 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 22:18:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:18:58 @agent_ppo2.py:185][0m |          -0.0015 |           1.7005 |           1.8802 |
[32m[20221213 22:18:58 @agent_ppo2.py:185][0m |          -0.0114 |           1.0152 |           1.8792 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0029 |           0.8911 |           1.8793 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0037 |           0.8458 |           1.8790 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0040 |           0.7984 |           1.8790 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0048 |           0.7827 |           1.8788 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0051 |           0.7262 |           1.8792 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0055 |           0.6862 |           1.8798 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0057 |           0.6531 |           1.8802 |
[32m[20221213 22:18:59 @agent_ppo2.py:185][0m |          -0.0061 |           0.6281 |           1.8805 |
[32m[20221213 22:18:59 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:18:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 26.08
[32m[20221213 22:18:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 30.82
[32m[20221213 22:18:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 16.98
[32m[20221213 22:18:59 @agent_ppo2.py:143][0m Total time:       0.77 min
[32m[20221213 22:18:59 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 22:18:59 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 22:18:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |           0.0016 |           0.1613 |           1.9352 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0018 |           0.0601 |           1.9343 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0066 |           0.0566 |           1.9330 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0025 |           0.0542 |           1.9321 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0084 |           0.0524 |           1.9306 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0106 |           0.0523 |           1.9287 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |           0.0004 |           0.0518 |           1.9277 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0082 |           0.0508 |           1.9271 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0098 |           0.0513 |           1.9264 |
[32m[20221213 22:19:00 @agent_ppo2.py:185][0m |          -0.0120 |           0.0504 |           1.9260 |
[32m[20221213 22:19:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:19:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.50
[32m[20221213 22:19:01 @agent_ppo2.py:143][0m Total time:       0.80 min
[32m[20221213 22:19:01 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 22:19:01 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 22:19:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:01 @agent_ppo2.py:185][0m |           0.0049 |           0.0518 |           1.9189 |
[32m[20221213 22:19:01 @agent_ppo2.py:185][0m |          -0.0057 |           0.0235 |           1.9183 |
[32m[20221213 22:19:01 @agent_ppo2.py:185][0m |          -0.0029 |           0.0211 |           1.9180 |
[32m[20221213 22:19:01 @agent_ppo2.py:185][0m |          -0.0076 |           0.0203 |           1.9168 |
[32m[20221213 22:19:01 @agent_ppo2.py:185][0m |          -0.0098 |           0.0197 |           1.9173 |
[32m[20221213 22:19:01 @agent_ppo2.py:185][0m |          -0.0099 |           0.0194 |           1.9169 |
[32m[20221213 22:19:02 @agent_ppo2.py:185][0m |          -0.0109 |           0.0193 |           1.9174 |
[32m[20221213 22:19:02 @agent_ppo2.py:185][0m |          -0.0160 |           0.0192 |           1.9170 |
[32m[20221213 22:19:02 @agent_ppo2.py:185][0m |          -0.0041 |           0.0191 |           1.9172 |
[32m[20221213 22:19:02 @agent_ppo2.py:185][0m |          -0.0119 |           0.0190 |           1.9177 |
[32m[20221213 22:19:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:19:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:02 @agent_ppo2.py:143][0m Total time:       0.82 min
[32m[20221213 22:19:02 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 22:19:02 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 22:19:02 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 22:19:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:02 @agent_ppo2.py:185][0m |          -0.0043 |           0.0147 |           1.9004 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0024 |           0.0108 |           1.8975 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0084 |           0.0107 |           1.8950 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0093 |           0.0105 |           1.8926 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0084 |           0.0103 |           1.8925 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0125 |           0.0103 |           1.8918 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0070 |           0.0101 |           1.8910 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0117 |           0.0101 |           1.8912 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0169 |           0.0102 |           1.8906 |
[32m[20221213 22:19:03 @agent_ppo2.py:185][0m |          -0.0202 |           0.0100 |           1.8913 |
[32m[20221213 22:19:03 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 22:19:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:04 @agent_ppo2.py:143][0m Total time:       0.85 min
[32m[20221213 22:19:04 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 22:19:04 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 22:19:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:04 @agent_ppo2.py:185][0m |          -0.0092 |           0.0077 |           1.8876 |
[32m[20221213 22:19:04 @agent_ppo2.py:185][0m |          -0.0094 |           0.0058 |           1.8855 |
[32m[20221213 22:19:04 @agent_ppo2.py:185][0m |          -0.0136 |           0.0056 |           1.8835 |
[32m[20221213 22:19:04 @agent_ppo2.py:185][0m |          -0.0131 |           0.0056 |           1.8824 |
[32m[20221213 22:19:04 @agent_ppo2.py:185][0m |          -0.0132 |           0.0055 |           1.8817 |
[32m[20221213 22:19:04 @agent_ppo2.py:185][0m |          -0.0134 |           0.0055 |           1.8812 |
[32m[20221213 22:19:05 @agent_ppo2.py:185][0m |          -0.0142 |           0.0054 |           1.8800 |
[32m[20221213 22:19:05 @agent_ppo2.py:185][0m |          -0.0168 |           0.0054 |           1.8798 |
[32m[20221213 22:19:05 @agent_ppo2.py:185][0m |          -0.0168 |           0.0054 |           1.8798 |
[32m[20221213 22:19:05 @agent_ppo2.py:185][0m |          -0.0167 |           0.0053 |           1.8790 |
[32m[20221213 22:19:05 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:19:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:05 @agent_ppo2.py:143][0m Total time:       0.87 min
[32m[20221213 22:19:05 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 22:19:05 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 22:19:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 22:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:05 @agent_ppo2.py:185][0m |          -0.0005 |           0.0049 |           1.9266 |
[32m[20221213 22:19:05 @agent_ppo2.py:185][0m |          -0.0066 |           0.0044 |           1.9255 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |          -0.0059 |           0.0044 |           1.9235 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |          -0.0112 |           0.0043 |           1.9225 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |          -0.0181 |           0.0043 |           1.9228 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |           0.0018 |           0.0043 |           1.9243 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |          -0.0092 |           0.0042 |           1.9246 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |          -0.0149 |           0.0042 |           1.9255 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |          -0.0062 |           0.0042 |           1.9262 |
[32m[20221213 22:19:06 @agent_ppo2.py:185][0m |          -0.0118 |           0.0042 |           1.9271 |
[32m[20221213 22:19:06 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:19:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:06 @agent_ppo2.py:143][0m Total time:       0.89 min
[32m[20221213 22:19:06 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 22:19:06 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 22:19:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0049 |           0.0038 |           1.9081 |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0101 |           0.0036 |           1.9079 |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0049 |           0.0036 |           1.9066 |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0075 |           0.0036 |           1.9063 |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0039 |           0.0036 |           1.9079 |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0113 |           0.0035 |           1.9084 |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0122 |           0.0035 |           1.9093 |
[32m[20221213 22:19:07 @agent_ppo2.py:185][0m |          -0.0124 |           0.0035 |           1.9106 |
[32m[20221213 22:19:08 @agent_ppo2.py:185][0m |          -0.0129 |           0.0035 |           1.9115 |
[32m[20221213 22:19:08 @agent_ppo2.py:185][0m |          -0.0140 |           0.0035 |           1.9126 |
[32m[20221213 22:19:08 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:19:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:08 @agent_ppo2.py:143][0m Total time:       0.91 min
[32m[20221213 22:19:08 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 22:19:08 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 22:19:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:08 @agent_ppo2.py:185][0m |          -0.0037 |           0.0027 |           1.9197 |
[32m[20221213 22:19:08 @agent_ppo2.py:185][0m |          -0.0126 |           0.0026 |           1.9144 |
[32m[20221213 22:19:08 @agent_ppo2.py:185][0m |          -0.0072 |           0.0026 |           1.9130 |
[32m[20221213 22:19:08 @agent_ppo2.py:185][0m |          -0.0134 |           0.0026 |           1.9126 |
[32m[20221213 22:19:08 @agent_ppo2.py:185][0m |          -0.0139 |           0.0025 |           1.9120 |
[32m[20221213 22:19:09 @agent_ppo2.py:185][0m |          -0.0160 |           0.0025 |           1.9120 |
[32m[20221213 22:19:09 @agent_ppo2.py:185][0m |          -0.0136 |           0.0025 |           1.9115 |
[32m[20221213 22:19:09 @agent_ppo2.py:185][0m |          -0.0134 |           0.0025 |           1.9117 |
[32m[20221213 22:19:09 @agent_ppo2.py:185][0m |          -0.0180 |           0.0024 |           1.9116 |
[32m[20221213 22:19:09 @agent_ppo2.py:185][0m |          -0.0163 |           0.0024 |           1.9112 |
[32m[20221213 22:19:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:19:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:09 @agent_ppo2.py:143][0m Total time:       0.94 min
[32m[20221213 22:19:09 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 22:19:09 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 22:19:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:19:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:09 @agent_ppo2.py:185][0m |          -0.0047 |           0.0026 |           1.9947 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0054 |           0.0024 |           1.9917 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0117 |           0.0024 |           1.9895 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0094 |           0.0024 |           1.9885 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0081 |           0.0023 |           1.9891 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0111 |           0.0023 |           1.9890 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0105 |           0.0023 |           1.9892 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0112 |           0.0023 |           1.9892 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0096 |           0.0022 |           1.9904 |
[32m[20221213 22:19:10 @agent_ppo2.py:185][0m |          -0.0129 |           0.0022 |           1.9906 |
[32m[20221213 22:19:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:19:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:10 @agent_ppo2.py:143][0m Total time:       0.96 min
[32m[20221213 22:19:10 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 22:19:10 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 22:19:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |           0.0006 |           0.3718 |           1.9431 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0029 |           0.2060 |           1.9425 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0041 |           0.1725 |           1.9418 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0049 |           0.1568 |           1.9415 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0052 |           0.1445 |           1.9415 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0052 |           0.1400 |           1.9407 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0055 |           0.1332 |           1.9406 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0059 |           0.1321 |           1.9406 |
[32m[20221213 22:19:11 @agent_ppo2.py:185][0m |          -0.0062 |           0.1276 |           1.9403 |
[32m[20221213 22:19:12 @agent_ppo2.py:185][0m |          -0.0058 |           0.1276 |           1.9400 |
[32m[20221213 22:19:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:19:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 14.26
[32m[20221213 22:19:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 14.83
[32m[20221213 22:19:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:12 @agent_ppo2.py:143][0m Total time:       0.98 min
[32m[20221213 22:19:12 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 22:19:12 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 22:19:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:12 @agent_ppo2.py:185][0m |          -0.0004 |           0.0813 |           1.9937 |
[32m[20221213 22:19:12 @agent_ppo2.py:185][0m |          -0.0038 |           0.0225 |           1.9951 |
[32m[20221213 22:19:12 @agent_ppo2.py:185][0m |          -0.0055 |           0.0196 |           1.9958 |
[32m[20221213 22:19:12 @agent_ppo2.py:185][0m |          -0.0062 |           0.0184 |           1.9964 |
[32m[20221213 22:19:12 @agent_ppo2.py:185][0m |          -0.0072 |           0.0178 |           1.9966 |
[32m[20221213 22:19:12 @agent_ppo2.py:185][0m |          -0.0072 |           0.0175 |           1.9971 |
[32m[20221213 22:19:13 @agent_ppo2.py:185][0m |          -0.0095 |           0.0171 |           1.9976 |
[32m[20221213 22:19:13 @agent_ppo2.py:185][0m |          -0.0081 |           0.0169 |           1.9982 |
[32m[20221213 22:19:13 @agent_ppo2.py:185][0m |          -0.0061 |           0.0167 |           1.9991 |
[32m[20221213 22:19:13 @agent_ppo2.py:185][0m |          -0.0100 |           0.0166 |           1.9995 |
[32m[20221213 22:19:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:19:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:13 @agent_ppo2.py:143][0m Total time:       1.00 min
[32m[20221213 22:19:13 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 22:19:13 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 22:19:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:13 @agent_ppo2.py:185][0m |          -0.0003 |           0.0276 |           1.9972 |
[32m[20221213 22:19:13 @agent_ppo2.py:185][0m |          -0.0077 |           0.0116 |           1.9968 |
[32m[20221213 22:19:13 @agent_ppo2.py:185][0m |          -0.0020 |           0.0100 |           1.9944 |
[32m[20221213 22:19:14 @agent_ppo2.py:185][0m |          -0.0060 |           0.0093 |           1.9929 |
[32m[20221213 22:19:14 @agent_ppo2.py:185][0m |          -0.0078 |           0.0094 |           1.9911 |
[32m[20221213 22:19:14 @agent_ppo2.py:185][0m |          -0.0086 |           0.0090 |           1.9892 |
[32m[20221213 22:19:14 @agent_ppo2.py:185][0m |          -0.0089 |           0.0088 |           1.9881 |
[32m[20221213 22:19:14 @agent_ppo2.py:185][0m |          -0.0111 |           0.0087 |           1.9870 |
[32m[20221213 22:19:14 @agent_ppo2.py:185][0m |          -0.0033 |           0.0087 |           1.9854 |
[32m[20221213 22:19:14 @agent_ppo2.py:185][0m |          -0.0097 |           0.0086 |           1.9846 |
[32m[20221213 22:19:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:19:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:14 @agent_ppo2.py:143][0m Total time:       1.02 min
[32m[20221213 22:19:14 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 22:19:14 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 22:19:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0013 |           0.1698 |           1.9772 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0037 |           0.1557 |           1.9776 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0061 |           0.1552 |           1.9773 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0047 |           0.1547 |           1.9780 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0067 |           0.1506 |           1.9782 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0073 |           0.1486 |           1.9787 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0267 |           0.1646 |           1.9789 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0076 |           0.1466 |           1.9795 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0075 |           0.1462 |           1.9803 |
[32m[20221213 22:19:15 @agent_ppo2.py:185][0m |          -0.0084 |           0.1440 |           1.9808 |
[32m[20221213 22:19:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:19:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.36
[32m[20221213 22:19:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 9.77
[32m[20221213 22:19:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:15 @agent_ppo2.py:143][0m Total time:       1.04 min
[32m[20221213 22:19:15 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 22:19:15 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 22:19:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0020 |           0.0262 |           1.9530 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0053 |           0.0147 |           1.9514 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0055 |           0.0137 |           1.9491 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0005 |           0.0133 |           1.9474 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0129 |           0.0131 |           1.9454 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0005 |           0.0129 |           1.9443 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0155 |           0.0127 |           1.9440 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0022 |           0.0126 |           1.9427 |
[32m[20221213 22:19:16 @agent_ppo2.py:185][0m |          -0.0121 |           0.0125 |           1.9419 |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |          -0.0121 |           0.0122 |           1.9407 |
[32m[20221213 22:19:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:19:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:17 @agent_ppo2.py:143][0m Total time:       1.06 min
[32m[20221213 22:19:17 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 22:19:17 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 22:19:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:19:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |           0.0035 |           0.0129 |           1.9558 |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |          -0.0053 |           0.0081 |           1.9542 |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |          -0.0057 |           0.0073 |           1.9523 |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |          -0.0042 |           0.0069 |           1.9508 |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |          -0.0088 |           0.0067 |           1.9502 |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |          -0.0060 |           0.0065 |           1.9492 |
[32m[20221213 22:19:17 @agent_ppo2.py:185][0m |          -0.0111 |           0.0063 |           1.9485 |
[32m[20221213 22:19:18 @agent_ppo2.py:185][0m |          -0.0072 |           0.0062 |           1.9477 |
[32m[20221213 22:19:18 @agent_ppo2.py:185][0m |          -0.0056 |           0.0062 |           1.9467 |
[32m[20221213 22:19:18 @agent_ppo2.py:185][0m |          -0.0060 |           0.0061 |           1.9462 |
[32m[20221213 22:19:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:19:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:18 @agent_ppo2.py:143][0m Total time:       1.08 min
[32m[20221213 22:19:18 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 22:19:18 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 22:19:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:19:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:18 @agent_ppo2.py:185][0m |          -0.0013 |           0.0042 |           1.9754 |
[32m[20221213 22:19:18 @agent_ppo2.py:185][0m |          -0.0025 |           0.0035 |           1.9747 |
[32m[20221213 22:19:18 @agent_ppo2.py:185][0m |          -0.0007 |           0.0034 |           1.9731 |
[32m[20221213 22:19:19 @agent_ppo2.py:185][0m |          -0.0062 |           0.0034 |           1.9714 |
[32m[20221213 22:19:19 @agent_ppo2.py:185][0m |          -0.0067 |           0.0033 |           1.9708 |
[32m[20221213 22:19:19 @agent_ppo2.py:185][0m |          -0.0078 |           0.0033 |           1.9699 |
[32m[20221213 22:19:19 @agent_ppo2.py:185][0m |          -0.0046 |           0.0033 |           1.9693 |
[32m[20221213 22:19:19 @agent_ppo2.py:185][0m |          -0.0084 |           0.0032 |           1.9694 |
[32m[20221213 22:19:19 @agent_ppo2.py:185][0m |          -0.0097 |           0.0032 |           1.9692 |
[32m[20221213 22:19:19 @agent_ppo2.py:185][0m |          -0.0081 |           0.0031 |           1.9682 |
[32m[20221213 22:19:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:19:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:19 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 22:19:19 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 22:19:19 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 22:19:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0033 |           0.0030 |           1.9701 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |           0.0008 |           0.0028 |           1.9681 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |           0.0048 |           0.0028 |           1.9656 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0152 |           0.0027 |           1.9642 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0065 |           0.0027 |           1.9625 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0058 |           0.0026 |           1.9618 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0109 |           0.0026 |           1.9616 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0114 |           0.0026 |           1.9604 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0079 |           0.0025 |           1.9600 |
[32m[20221213 22:19:20 @agent_ppo2.py:185][0m |          -0.0161 |           0.0025 |           1.9585 |
[32m[20221213 22:19:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:19:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.22
[32m[20221213 22:19:20 @agent_ppo2.py:143][0m Total time:       1.13 min
[32m[20221213 22:19:20 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 22:19:20 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 22:19:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0044 |           0.0024 |           1.9200 |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0110 |           0.0022 |           1.9195 |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0028 |           0.0022 |           1.9191 |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0061 |           0.0021 |           1.9194 |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0123 |           0.0021 |           1.9193 |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0053 |           0.0020 |           1.9192 |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0120 |           0.0020 |           1.9195 |
[32m[20221213 22:19:21 @agent_ppo2.py:185][0m |          -0.0134 |           0.0020 |           1.9200 |
[32m[20221213 22:19:22 @agent_ppo2.py:185][0m |          -0.0187 |           0.0019 |           1.9203 |
[32m[20221213 22:19:22 @agent_ppo2.py:185][0m |          -0.0159 |           0.0019 |           1.9206 |
[32m[20221213 22:19:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:19:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 14.02
[32m[20221213 22:19:22 @agent_ppo2.py:143][0m Total time:       1.15 min
[32m[20221213 22:19:22 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 22:19:22 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 22:19:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:22 @agent_ppo2.py:185][0m |          -0.0016 |           0.0017 |           1.9455 |
[32m[20221213 22:19:22 @agent_ppo2.py:185][0m |          -0.0117 |           0.0016 |           1.9422 |
[32m[20221213 22:19:22 @agent_ppo2.py:185][0m |          -0.0056 |           0.0015 |           1.9395 |
[32m[20221213 22:19:22 @agent_ppo2.py:185][0m |          -0.0086 |           0.0015 |           1.9380 |
[32m[20221213 22:19:22 @agent_ppo2.py:185][0m |          -0.0133 |           0.0014 |           1.9380 |
[32m[20221213 22:19:23 @agent_ppo2.py:185][0m |          -0.0063 |           0.0014 |           1.9377 |
[32m[20221213 22:19:23 @agent_ppo2.py:185][0m |          -0.0137 |           0.0014 |           1.9369 |
[32m[20221213 22:19:23 @agent_ppo2.py:185][0m |          -0.0122 |           0.0013 |           1.9370 |
[32m[20221213 22:19:23 @agent_ppo2.py:185][0m |          -0.0140 |           0.0013 |           1.9376 |
[32m[20221213 22:19:23 @agent_ppo2.py:185][0m |          -0.0131 |           0.0013 |           1.9371 |
[32m[20221213 22:19:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:19:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:23 @agent_ppo2.py:143][0m Total time:       1.17 min
[32m[20221213 22:19:23 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 22:19:23 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 22:19:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:23 @agent_ppo2.py:185][0m |           0.0057 |           0.0011 |           1.9583 |
[32m[20221213 22:19:23 @agent_ppo2.py:185][0m |          -0.0037 |           0.0010 |           1.9546 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0065 |           0.0010 |           1.9525 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0111 |           0.0010 |           1.9522 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0094 |           0.0009 |           1.9513 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0161 |           0.0009 |           1.9506 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0142 |           0.0009 |           1.9505 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0066 |           0.0008 |           1.9504 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0144 |           0.0008 |           1.9502 |
[32m[20221213 22:19:24 @agent_ppo2.py:185][0m |          -0.0126 |           0.0008 |           1.9496 |
[32m[20221213 22:19:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:19:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:24 @agent_ppo2.py:143][0m Total time:       1.19 min
[32m[20221213 22:19:24 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 22:19:24 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 22:19:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |           0.0125 |           0.0007 |           1.9594 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0025 |           0.0007 |           1.9559 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0066 |           0.0007 |           1.9520 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0017 |           0.0006 |           1.9499 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0100 |           0.0006 |           1.9481 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0139 |           0.0006 |           1.9460 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0113 |           0.0006 |           1.9444 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0094 |           0.0005 |           1.9430 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0077 |           0.0005 |           1.9412 |
[32m[20221213 22:19:25 @agent_ppo2.py:185][0m |          -0.0080 |           0.0005 |           1.9397 |
[32m[20221213 22:19:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:19:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:26 @agent_ppo2.py:143][0m Total time:       1.21 min
[32m[20221213 22:19:26 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 22:19:26 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 22:19:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:26 @agent_ppo2.py:185][0m |          -0.0053 |           0.0004 |           1.8897 |
[32m[20221213 22:19:26 @agent_ppo2.py:185][0m |          -0.0063 |           0.0004 |           1.8886 |
[32m[20221213 22:19:26 @agent_ppo2.py:185][0m |          -0.0091 |           0.0003 |           1.8862 |
[32m[20221213 22:19:26 @agent_ppo2.py:185][0m |          -0.0050 |           0.0003 |           1.8845 |
[32m[20221213 22:19:26 @agent_ppo2.py:185][0m |          -0.0069 |           0.0003 |           1.8846 |
[32m[20221213 22:19:26 @agent_ppo2.py:185][0m |          -0.0116 |           0.0003 |           1.8839 |
[32m[20221213 22:19:26 @agent_ppo2.py:185][0m |          -0.0070 |           0.0003 |           1.8842 |
[32m[20221213 22:19:27 @agent_ppo2.py:185][0m |          -0.0062 |           0.0003 |           1.8847 |
[32m[20221213 22:19:27 @agent_ppo2.py:185][0m |          -0.0152 |           0.0002 |           1.8857 |
[32m[20221213 22:19:27 @agent_ppo2.py:185][0m |          -0.0152 |           0.0002 |           1.8857 |
[32m[20221213 22:19:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:19:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:27 @agent_ppo2.py:143][0m Total time:       1.23 min
[32m[20221213 22:19:27 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 22:19:27 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 22:19:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:27 @agent_ppo2.py:185][0m |          -0.0037 |           0.0002 |           1.9024 |
[32m[20221213 22:19:27 @agent_ppo2.py:185][0m |          -0.0058 |           0.0002 |           1.9021 |
[32m[20221213 22:19:27 @agent_ppo2.py:185][0m |          -0.0069 |           0.0001 |           1.9006 |
[32m[20221213 22:19:27 @agent_ppo2.py:185][0m |          -0.0090 |           0.0001 |           1.9005 |
[32m[20221213 22:19:28 @agent_ppo2.py:185][0m |          -0.0098 |           0.0001 |           1.9000 |
[32m[20221213 22:19:28 @agent_ppo2.py:185][0m |          -0.0063 |           0.0001 |           1.9006 |
[32m[20221213 22:19:28 @agent_ppo2.py:185][0m |          -0.0115 |           0.0001 |           1.9005 |
[32m[20221213 22:19:28 @agent_ppo2.py:185][0m |          -0.0119 |           0.0001 |           1.9014 |
[32m[20221213 22:19:28 @agent_ppo2.py:185][0m |          -0.0107 |           0.0001 |           1.9011 |
[32m[20221213 22:19:28 @agent_ppo2.py:185][0m |          -0.0104 |           0.0001 |           1.9010 |
[32m[20221213 22:19:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:19:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:28 @agent_ppo2.py:143][0m Total time:       1.25 min
[32m[20221213 22:19:28 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 22:19:28 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 22:19:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0017 |           0.0001 |           1.9049 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0025 |           0.0001 |           1.9052 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0012 |           0.0001 |           1.9053 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0050 |           0.0001 |           1.9035 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0004 |           0.0001 |           1.9032 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0043 |           0.0001 |           1.9040 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0052 |           0.0001 |           1.9038 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0052 |           0.0000 |           1.9037 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0071 |           0.0000 |           1.9040 |
[32m[20221213 22:19:29 @agent_ppo2.py:185][0m |          -0.0073 |           0.0000 |           1.9040 |
[32m[20221213 22:19:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:19:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:29 @agent_ppo2.py:143][0m Total time:       1.28 min
[32m[20221213 22:19:29 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 22:19:29 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 22:19:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |           1.9333 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0046 |           0.0000 |           1.9330 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |           1.9320 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0080 |           0.0000 |           1.9311 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0089 |           0.0000 |           1.9313 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0096 |           0.0000 |           1.9298 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0101 |           0.0000 |           1.9295 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0106 |           0.0000 |           1.9287 |
[32m[20221213 22:19:30 @agent_ppo2.py:185][0m |          -0.0080 |           0.0000 |           1.9295 |
[32m[20221213 22:19:31 @agent_ppo2.py:185][0m |          -0.0112 |           0.0000 |           1.9295 |
[32m[20221213 22:19:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:19:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:31 @agent_ppo2.py:143][0m Total time:       1.30 min
[32m[20221213 22:19:31 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 22:19:31 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 22:19:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:19:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:31 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |           1.9370 |
[32m[20221213 22:19:31 @agent_ppo2.py:185][0m |          -0.0056 |           0.0000 |           1.9360 |
[32m[20221213 22:19:31 @agent_ppo2.py:185][0m |          -0.0084 |           0.0000 |           1.9338 |
[32m[20221213 22:19:31 @agent_ppo2.py:185][0m |          -0.0092 |           0.0000 |           1.9319 |
[32m[20221213 22:19:31 @agent_ppo2.py:185][0m |          -0.0099 |           0.0000 |           1.9305 |
[32m[20221213 22:19:31 @agent_ppo2.py:185][0m |          -0.0108 |           0.0000 |           1.9310 |
[32m[20221213 22:19:32 @agent_ppo2.py:185][0m |          -0.0111 |           0.0000 |           1.9301 |
[32m[20221213 22:19:32 @agent_ppo2.py:185][0m |          -0.0111 |           0.0000 |           1.9295 |
[32m[20221213 22:19:32 @agent_ppo2.py:185][0m |          -0.0118 |           0.0000 |           1.9295 |
[32m[20221213 22:19:32 @agent_ppo2.py:185][0m |          -0.0122 |           0.0000 |           1.9287 |
[32m[20221213 22:19:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:19:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:32 @agent_ppo2.py:143][0m Total time:       1.32 min
[32m[20221213 22:19:32 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 22:19:32 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 22:19:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:32 @agent_ppo2.py:185][0m |           0.0012 |           0.7964 |           1.9407 |
[32m[20221213 22:19:32 @agent_ppo2.py:185][0m |          -0.0019 |           0.5745 |           1.9403 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0030 |           0.4580 |           1.9406 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0090 |           0.4432 |           1.9410 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0165 |           0.4669 |           1.9405 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0036 |           0.4397 |           1.9412 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0043 |           0.4270 |           1.9415 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0043 |           0.4259 |           1.9415 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0048 |           0.4216 |           1.9418 |
[32m[20221213 22:19:33 @agent_ppo2.py:185][0m |          -0.0049 |           0.4184 |           1.9420 |
[32m[20221213 22:19:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:19:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 14.54
[32m[20221213 22:19:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 21.87
[32m[20221213 22:19:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:33 @agent_ppo2.py:143][0m Total time:       1.34 min
[32m[20221213 22:19:33 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 22:19:33 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 22:19:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |           0.0009 |           0.0465 |           1.9342 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0038 |           0.0234 |           1.9345 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0093 |           0.0228 |           1.9344 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0080 |           0.0227 |           1.9342 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0050 |           0.0228 |           1.9347 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0074 |           0.0225 |           1.9353 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0129 |           0.0224 |           1.9362 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0161 |           0.0228 |           1.9365 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0151 |           0.0225 |           1.9372 |
[32m[20221213 22:19:34 @agent_ppo2.py:185][0m |          -0.0118 |           0.0225 |           1.9382 |
[32m[20221213 22:19:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:19:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.72
[32m[20221213 22:19:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 1.86
[32m[20221213 22:19:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:35 @agent_ppo2.py:143][0m Total time:       1.36 min
[32m[20221213 22:19:35 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 22:19:35 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 22:19:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:35 @agent_ppo2.py:185][0m |          -0.0021 |           0.0131 |           1.9856 |
[32m[20221213 22:19:35 @agent_ppo2.py:185][0m |          -0.0036 |           0.0045 |           1.9845 |
[32m[20221213 22:19:35 @agent_ppo2.py:185][0m |          -0.0023 |           0.0039 |           1.9839 |
[32m[20221213 22:19:35 @agent_ppo2.py:185][0m |          -0.0064 |           0.0037 |           1.9841 |
[32m[20221213 22:19:35 @agent_ppo2.py:185][0m |          -0.0117 |           0.0037 |           1.9843 |
[32m[20221213 22:19:35 @agent_ppo2.py:185][0m |          -0.0140 |           0.0036 |           1.9847 |
[32m[20221213 22:19:35 @agent_ppo2.py:185][0m |          -0.0041 |           0.0037 |           1.9853 |
[32m[20221213 22:19:36 @agent_ppo2.py:185][0m |          -0.0095 |           0.0036 |           1.9842 |
[32m[20221213 22:19:36 @agent_ppo2.py:185][0m |          -0.0127 |           0.0036 |           1.9846 |
[32m[20221213 22:19:36 @agent_ppo2.py:185][0m |          -0.0122 |           0.0036 |           1.9844 |
[32m[20221213 22:19:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:19:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:36 @agent_ppo2.py:143][0m Total time:       1.38 min
[32m[20221213 22:19:36 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 22:19:36 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 22:19:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:36 @agent_ppo2.py:185][0m |          -0.0015 |           0.0029 |           1.9959 |
[32m[20221213 22:19:36 @agent_ppo2.py:185][0m |          -0.0084 |           0.0020 |           1.9945 |
[32m[20221213 22:19:36 @agent_ppo2.py:185][0m |          -0.0044 |           0.0020 |           1.9933 |
[32m[20221213 22:19:36 @agent_ppo2.py:185][0m |          -0.0078 |           0.0019 |           1.9914 |
[32m[20221213 22:19:37 @agent_ppo2.py:185][0m |          -0.0047 |           0.0019 |           1.9900 |
[32m[20221213 22:19:37 @agent_ppo2.py:185][0m |          -0.0142 |           0.0019 |           1.9885 |
[32m[20221213 22:19:37 @agent_ppo2.py:185][0m |          -0.0123 |           0.0019 |           1.9879 |
[32m[20221213 22:19:37 @agent_ppo2.py:185][0m |          -0.0149 |           0.0019 |           1.9875 |
[32m[20221213 22:19:37 @agent_ppo2.py:185][0m |          -0.0130 |           0.0018 |           1.9878 |
[32m[20221213 22:19:37 @agent_ppo2.py:185][0m |          -0.0191 |           0.0018 |           1.9875 |
[32m[20221213 22:19:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:19:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:37 @agent_ppo2.py:143][0m Total time:       1.40 min
[32m[20221213 22:19:37 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 22:19:37 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 22:19:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:19:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:37 @agent_ppo2.py:185][0m |          -0.0010 |           0.0012 |           2.0318 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0019 |           0.0010 |           2.0308 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0060 |           0.0010 |           2.0304 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0105 |           0.0010 |           2.0308 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0021 |           0.0010 |           2.0308 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0110 |           0.0010 |           2.0305 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0105 |           0.0010 |           2.0315 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0137 |           0.0010 |           2.0310 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0089 |           0.0010 |           2.0317 |
[32m[20221213 22:19:38 @agent_ppo2.py:185][0m |          -0.0071 |           0.0010 |           2.0319 |
[32m[20221213 22:19:38 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:19:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:38 @agent_ppo2.py:143][0m Total time:       1.43 min
[32m[20221213 22:19:38 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 22:19:38 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 22:19:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0022 |           0.0009 |           2.0026 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0088 |           0.0009 |           2.0020 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0001 |           0.0008 |           2.0013 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0070 |           0.0008 |           2.0010 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0115 |           0.0008 |           2.0013 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0044 |           0.0008 |           2.0018 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0139 |           0.0008 |           2.0014 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0119 |           0.0008 |           2.0021 |
[32m[20221213 22:19:39 @agent_ppo2.py:185][0m |          -0.0142 |           0.0008 |           2.0016 |
[32m[20221213 22:19:40 @agent_ppo2.py:185][0m |          -0.0111 |           0.0008 |           2.0012 |
[32m[20221213 22:19:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:19:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:40 @agent_ppo2.py:143][0m Total time:       1.45 min
[32m[20221213 22:19:40 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 22:19:40 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 22:19:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:40 @agent_ppo2.py:185][0m |          -0.0011 |           0.0008 |           1.9870 |
[32m[20221213 22:19:40 @agent_ppo2.py:185][0m |          -0.0034 |           0.0007 |           1.9846 |
[32m[20221213 22:19:40 @agent_ppo2.py:185][0m |          -0.0101 |           0.0007 |           1.9830 |
[32m[20221213 22:19:40 @agent_ppo2.py:185][0m |          -0.0051 |           0.0007 |           1.9829 |
[32m[20221213 22:19:40 @agent_ppo2.py:185][0m |          -0.0094 |           0.0007 |           1.9821 |
[32m[20221213 22:19:41 @agent_ppo2.py:185][0m |          -0.0049 |           0.0007 |           1.9815 |
[32m[20221213 22:19:41 @agent_ppo2.py:185][0m |          -0.0031 |           0.0007 |           1.9813 |
[32m[20221213 22:19:41 @agent_ppo2.py:185][0m |          -0.0115 |           0.0007 |           1.9812 |
[32m[20221213 22:19:41 @agent_ppo2.py:185][0m |          -0.0085 |           0.0007 |           1.9822 |
[32m[20221213 22:19:41 @agent_ppo2.py:185][0m |          -0.0127 |           0.0007 |           1.9815 |
[32m[20221213 22:19:41 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:19:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.16
[32m[20221213 22:19:41 @agent_ppo2.py:143][0m Total time:       1.47 min
[32m[20221213 22:19:41 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 22:19:41 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 22:19:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:41 @agent_ppo2.py:185][0m |          -0.0019 |           0.0006 |           2.0139 |
[32m[20221213 22:19:41 @agent_ppo2.py:185][0m |          -0.0051 |           0.0006 |           2.0143 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0073 |           0.0006 |           2.0137 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0086 |           0.0006 |           2.0138 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0053 |           0.0006 |           2.0132 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0095 |           0.0006 |           2.0135 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0089 |           0.0006 |           2.0135 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0098 |           0.0005 |           2.0135 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0006 |           0.0005 |           2.0137 |
[32m[20221213 22:19:42 @agent_ppo2.py:185][0m |          -0.0103 |           0.0005 |           2.0130 |
[32m[20221213 22:19:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:19:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:42 @agent_ppo2.py:143][0m Total time:       1.49 min
[32m[20221213 22:19:42 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 22:19:42 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 22:19:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |           0.0063 |           0.0005 |           2.0578 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0059 |           0.0005 |           2.0571 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0005 |           0.0005 |           2.0548 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0098 |           0.0005 |           2.0528 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |           0.0045 |           0.0004 |           2.0519 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0025 |           0.0004 |           2.0503 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0083 |           0.0004 |           2.0489 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0089 |           0.0004 |           2.0497 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0127 |           0.0004 |           2.0489 |
[32m[20221213 22:19:43 @agent_ppo2.py:185][0m |          -0.0123 |           0.0004 |           2.0494 |
[32m[20221213 22:19:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:19:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:44 @agent_ppo2.py:143][0m Total time:       1.51 min
[32m[20221213 22:19:44 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 22:19:44 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 22:19:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:19:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:44 @agent_ppo2.py:185][0m |          -0.0018 |           0.0004 |           2.0242 |
[32m[20221213 22:19:44 @agent_ppo2.py:185][0m |          -0.0046 |           0.0003 |           2.0216 |
[32m[20221213 22:19:44 @agent_ppo2.py:185][0m |          -0.0073 |           0.0003 |           2.0188 |
[32m[20221213 22:19:44 @agent_ppo2.py:185][0m |          -0.0085 |           0.0003 |           2.0163 |
[32m[20221213 22:19:44 @agent_ppo2.py:185][0m |          -0.0096 |           0.0003 |           2.0146 |
[32m[20221213 22:19:44 @agent_ppo2.py:185][0m |          -0.0096 |           0.0003 |           2.0131 |
[32m[20221213 22:19:45 @agent_ppo2.py:185][0m |          -0.0109 |           0.0003 |           2.0114 |
[32m[20221213 22:19:45 @agent_ppo2.py:185][0m |          -0.0112 |           0.0003 |           2.0109 |
[32m[20221213 22:19:45 @agent_ppo2.py:185][0m |          -0.0080 |           0.0002 |           2.0103 |
[32m[20221213 22:19:45 @agent_ppo2.py:185][0m |          -0.0120 |           0.0002 |           2.0083 |
[32m[20221213 22:19:45 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:19:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:45 @agent_ppo2.py:143][0m Total time:       1.53 min
[32m[20221213 22:19:45 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 22:19:45 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 22:19:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:19:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:45 @agent_ppo2.py:185][0m |          -0.0018 |           0.0002 |           2.0274 |
[32m[20221213 22:19:45 @agent_ppo2.py:185][0m |          -0.0033 |           0.0002 |           2.0282 |
[32m[20221213 22:19:45 @agent_ppo2.py:185][0m |          -0.0019 |           0.0002 |           2.0291 |
[32m[20221213 22:19:46 @agent_ppo2.py:185][0m |          -0.0065 |           0.0002 |           2.0290 |
[32m[20221213 22:19:46 @agent_ppo2.py:185][0m |          -0.0072 |           0.0002 |           2.0292 |
[32m[20221213 22:19:46 @agent_ppo2.py:185][0m |          -0.0018 |           0.0002 |           2.0294 |
[32m[20221213 22:19:46 @agent_ppo2.py:185][0m |          -0.0012 |           0.0002 |           2.0291 |
[32m[20221213 22:19:46 @agent_ppo2.py:185][0m |          -0.0091 |           0.0001 |           2.0295 |
[32m[20221213 22:19:46 @agent_ppo2.py:185][0m |          -0.0098 |           0.0001 |           2.0301 |
[32m[20221213 22:19:46 @agent_ppo2.py:185][0m |          -0.0085 |           0.0001 |           2.0300 |
[32m[20221213 22:19:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:19:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:46 @agent_ppo2.py:143][0m Total time:       1.56 min
[32m[20221213 22:19:46 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 22:19:46 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 22:19:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |           0.0022 |           0.0001 |           2.0049 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0039 |           0.0001 |           2.0046 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |           0.0072 |           0.0001 |           2.0042 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0061 |           0.0001 |           2.0024 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0075 |           0.0001 |           2.0026 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0066 |           0.0001 |           2.0017 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0084 |           0.0001 |           2.0020 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0078 |           0.0001 |           2.0015 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0093 |           0.0001 |           2.0019 |
[32m[20221213 22:19:47 @agent_ppo2.py:185][0m |          -0.0078 |           0.0000 |           2.0016 |
[32m[20221213 22:19:47 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:19:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:48 @agent_ppo2.py:143][0m Total time:       1.58 min
[32m[20221213 22:19:48 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 22:19:48 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 22:19:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:48 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |           2.0256 |
[32m[20221213 22:19:48 @agent_ppo2.py:185][0m |          -0.0051 |           0.0000 |           2.0264 |
[32m[20221213 22:19:48 @agent_ppo2.py:185][0m |           0.0049 |           0.0000 |           2.0255 |
[32m[20221213 22:19:48 @agent_ppo2.py:185][0m |          -0.0086 |           0.0000 |           2.0244 |
[32m[20221213 22:19:48 @agent_ppo2.py:185][0m |          -0.0098 |           0.0000 |           2.0233 |
[32m[20221213 22:19:48 @agent_ppo2.py:185][0m |          -0.0106 |           0.0000 |           2.0226 |
[32m[20221213 22:19:48 @agent_ppo2.py:185][0m |          -0.0032 |           0.0000 |           2.0225 |
[32m[20221213 22:19:49 @agent_ppo2.py:185][0m |          -0.0040 |           0.0000 |           2.0221 |
[32m[20221213 22:19:49 @agent_ppo2.py:185][0m |          -0.0114 |           0.0000 |           2.0225 |
[32m[20221213 22:19:49 @agent_ppo2.py:185][0m |          -0.0121 |           0.0000 |           2.0215 |
[32m[20221213 22:19:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:19:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 28.00
[32m[20221213 22:19:49 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 28.00
[32m[20221213 22:19:49 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 28.00
[32m[20221213 22:19:49 @agent_ppo2.py:143][0m Total time:       1.60 min
[32m[20221213 22:19:49 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 22:19:49 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 22:19:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:49 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |           2.0631 |
[32m[20221213 22:19:49 @agent_ppo2.py:185][0m |          -0.0014 |           0.0000 |           2.0628 |
[32m[20221213 22:19:49 @agent_ppo2.py:185][0m |          -0.0055 |           0.0000 |           2.0597 |
[32m[20221213 22:19:50 @agent_ppo2.py:185][0m |           0.0078 |           0.0000 |           2.0576 |
[32m[20221213 22:19:50 @agent_ppo2.py:185][0m |          -0.0044 |           0.0000 |           2.0565 |
[32m[20221213 22:19:50 @agent_ppo2.py:185][0m |          -0.0067 |           0.0000 |           2.0575 |
[32m[20221213 22:19:50 @agent_ppo2.py:185][0m |          -0.0070 |           0.0000 |           2.0559 |
[32m[20221213 22:19:50 @agent_ppo2.py:185][0m |          -0.0072 |           0.0000 |           2.0553 |
[32m[20221213 22:19:50 @agent_ppo2.py:185][0m |          -0.0071 |           0.0000 |           2.0563 |
[32m[20221213 22:19:50 @agent_ppo2.py:185][0m |          -0.0047 |           0.0000 |           2.0552 |
[32m[20221213 22:19:50 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:19:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:50 @agent_ppo2.py:143][0m Total time:       1.62 min
[32m[20221213 22:19:50 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 22:19:50 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 22:19:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |           0.0058 |           0.0000 |           2.0506 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |           2.0516 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |           0.0033 |           0.0000 |           2.0515 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |          -0.0069 |           0.0000 |           2.0513 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |           0.0050 |           0.0000 |           2.0514 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |          -0.0082 |           0.0000 |           2.0507 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |          -0.0087 |           0.0000 |           2.0503 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |          -0.0091 |           0.0000 |           2.0505 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |          -0.0094 |           0.0000 |           2.0498 |
[32m[20221213 22:19:51 @agent_ppo2.py:185][0m |          -0.0098 |           0.0000 |           2.0503 |
[32m[20221213 22:19:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:19:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:52 @agent_ppo2.py:143][0m Total time:       1.64 min
[32m[20221213 22:19:52 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 22:19:52 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 22:19:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |           2.0189 |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |          -0.0038 |           0.0000 |           2.0192 |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |          -0.0049 |           0.0000 |           2.0189 |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |          -0.0065 |           0.0000 |           2.0188 |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |          -0.0029 |           0.0000 |           2.0190 |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |           0.0082 |           0.0000 |           2.0192 |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |          -0.0082 |           0.0000 |           2.0181 |
[32m[20221213 22:19:52 @agent_ppo2.py:185][0m |          -0.0017 |           0.0000 |           2.0182 |
[32m[20221213 22:19:53 @agent_ppo2.py:185][0m |          -0.0093 |           0.0000 |           2.0184 |
[32m[20221213 22:19:53 @agent_ppo2.py:185][0m |          -0.0091 |           0.0000 |           2.0185 |
[32m[20221213 22:19:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:19:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:53 @agent_ppo2.py:143][0m Total time:       1.66 min
[32m[20221213 22:19:53 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 22:19:53 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 22:19:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:19:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:53 @agent_ppo2.py:185][0m |           0.0013 |           0.1595 |           2.0511 |
[32m[20221213 22:19:53 @agent_ppo2.py:185][0m |          -0.0001 |           0.1592 |           2.0512 |
[32m[20221213 22:19:53 @agent_ppo2.py:185][0m |          -0.0013 |           0.1592 |           2.0512 |
[32m[20221213 22:19:53 @agent_ppo2.py:185][0m |          -0.0137 |           0.1770 |           2.0509 |
[32m[20221213 22:19:53 @agent_ppo2.py:185][0m |          -0.0025 |           0.1592 |           2.0513 |
[32m[20221213 22:19:54 @agent_ppo2.py:185][0m |          -0.0031 |           0.1592 |           2.0519 |
[32m[20221213 22:19:54 @agent_ppo2.py:185][0m |          -0.0036 |           0.1592 |           2.0524 |
[32m[20221213 22:19:54 @agent_ppo2.py:185][0m |          -0.0037 |           0.1592 |           2.0531 |
[32m[20221213 22:19:54 @agent_ppo2.py:185][0m |          -0.0040 |           0.1592 |           2.0537 |
[32m[20221213 22:19:54 @agent_ppo2.py:185][0m |          -0.0044 |           0.1591 |           2.0541 |
[32m[20221213 22:19:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:19:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.57
[32m[20221213 22:19:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.78
[32m[20221213 22:19:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:54 @agent_ppo2.py:143][0m Total time:       1.69 min
[32m[20221213 22:19:54 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 22:19:54 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 22:19:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:54 @agent_ppo2.py:185][0m |          -0.0022 |           0.0000 |           2.0510 |
[32m[20221213 22:19:54 @agent_ppo2.py:185][0m |          -0.0045 |           0.0000 |           2.0497 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0042 |           0.0000 |           2.0478 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |           2.0465 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0081 |           0.0000 |           2.0446 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0091 |           0.0000 |           2.0432 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0078 |           0.0000 |           2.0425 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0061 |           0.0000 |           2.0415 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0103 |           0.0000 |           2.0401 |
[32m[20221213 22:19:55 @agent_ppo2.py:185][0m |          -0.0110 |           0.0000 |           2.0389 |
[32m[20221213 22:19:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:19:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:55 @agent_ppo2.py:143][0m Total time:       1.71 min
[32m[20221213 22:19:55 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 22:19:55 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 22:19:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:19:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |           0.0001 |           0.9112 |           2.0783 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0030 |           0.6085 |           2.0791 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0036 |           0.4920 |           2.0803 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0146 |           0.4823 |           2.0826 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0043 |           0.4574 |           2.0843 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0046 |           0.4551 |           2.0866 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0048 |           0.4479 |           2.0880 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0051 |           0.4455 |           2.0896 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0056 |           0.4432 |           2.0910 |
[32m[20221213 22:19:56 @agent_ppo2.py:185][0m |          -0.0056 |           0.4425 |           2.0922 |
[32m[20221213 22:19:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:19:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 16.34
[32m[20221213 22:19:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 19.35
[32m[20221213 22:19:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:57 @agent_ppo2.py:143][0m Total time:       1.73 min
[32m[20221213 22:19:57 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 22:19:57 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 22:19:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:19:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0047 |           0.0825 |           2.0700 |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0046 |           0.0346 |           2.0688 |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0079 |           0.0321 |           2.0695 |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0109 |           0.0311 |           2.0694 |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0045 |           0.0306 |           2.0706 |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0102 |           0.0299 |           2.0719 |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0107 |           0.0298 |           2.0715 |
[32m[20221213 22:19:57 @agent_ppo2.py:185][0m |          -0.0080 |           0.0297 |           2.0719 |
[32m[20221213 22:19:58 @agent_ppo2.py:185][0m |          -0.0111 |           0.0296 |           2.0729 |
[32m[20221213 22:19:58 @agent_ppo2.py:185][0m |          -0.0136 |           0.0296 |           2.0741 |
[32m[20221213 22:19:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:19:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:58 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221213 22:19:58 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 22:19:58 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 22:19:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:58 @agent_ppo2.py:185][0m |          -0.0010 |           0.0161 |           2.1047 |
[32m[20221213 22:19:58 @agent_ppo2.py:185][0m |          -0.0069 |           0.0072 |           2.1041 |
[32m[20221213 22:19:58 @agent_ppo2.py:185][0m |          -0.0091 |           0.0067 |           2.1033 |
[32m[20221213 22:19:58 @agent_ppo2.py:185][0m |          -0.0137 |           0.0067 |           2.1030 |
[32m[20221213 22:19:58 @agent_ppo2.py:185][0m |          -0.0096 |           0.0066 |           2.1032 |
[32m[20221213 22:19:59 @agent_ppo2.py:185][0m |          -0.0087 |           0.0065 |           2.1034 |
[32m[20221213 22:19:59 @agent_ppo2.py:185][0m |          -0.0120 |           0.0065 |           2.1043 |
[32m[20221213 22:19:59 @agent_ppo2.py:185][0m |          -0.0140 |           0.0065 |           2.1041 |
[32m[20221213 22:19:59 @agent_ppo2.py:185][0m |          -0.0003 |           0.0065 |           2.1044 |
[32m[20221213 22:19:59 @agent_ppo2.py:185][0m |          -0.0152 |           0.0065 |           2.1045 |
[32m[20221213 22:19:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:19:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:19:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:19:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:19:59 @agent_ppo2.py:143][0m Total time:       1.77 min
[32m[20221213 22:19:59 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 22:19:59 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 22:19:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:19:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:19:59 @agent_ppo2.py:185][0m |           0.0040 |           0.0036 |           2.1289 |
[32m[20221213 22:19:59 @agent_ppo2.py:185][0m |          -0.0060 |           0.0029 |           2.1282 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0033 |           0.0029 |           2.1280 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0105 |           0.0028 |           2.1276 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0074 |           0.0028 |           2.1277 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0037 |           0.0027 |           2.1280 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0057 |           0.0027 |           2.1286 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0100 |           0.0027 |           2.1292 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0140 |           0.0027 |           2.1297 |
[32m[20221213 22:20:00 @agent_ppo2.py:185][0m |          -0.0095 |           0.0026 |           2.1298 |
[32m[20221213 22:20:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:20:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:00 @agent_ppo2.py:143][0m Total time:       1.79 min
[32m[20221213 22:20:00 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 22:20:00 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 22:20:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:20:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |           0.0001 |           0.3153 |           2.1004 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0023 |           0.2784 |           2.0995 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0034 |           0.2736 |           2.0999 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0040 |           0.2713 |           2.0999 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0046 |           0.2689 |           2.1004 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0048 |           0.2705 |           2.1014 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0054 |           0.2682 |           2.1018 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0254 |           0.3350 |           2.1017 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0052 |           0.2753 |           2.1030 |
[32m[20221213 22:20:01 @agent_ppo2.py:185][0m |          -0.0255 |           0.3296 |           2.1041 |
[32m[20221213 22:20:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:20:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.65
[32m[20221213 22:20:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 14.50
[32m[20221213 22:20:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:02 @agent_ppo2.py:143][0m Total time:       1.81 min
[32m[20221213 22:20:02 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 22:20:02 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 22:20:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:20:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:02 @agent_ppo2.py:185][0m |           0.0012 |           0.0337 |           2.1441 |
[32m[20221213 22:20:02 @agent_ppo2.py:185][0m |          -0.0019 |           0.0094 |           2.1430 |
[32m[20221213 22:20:02 @agent_ppo2.py:185][0m |          -0.0075 |           0.0078 |           2.1416 |
[32m[20221213 22:20:02 @agent_ppo2.py:185][0m |          -0.0042 |           0.0075 |           2.1406 |
[32m[20221213 22:20:02 @agent_ppo2.py:185][0m |          -0.0123 |           0.0073 |           2.1392 |
[32m[20221213 22:20:02 @agent_ppo2.py:185][0m |          -0.0072 |           0.0073 |           2.1385 |
[32m[20221213 22:20:02 @agent_ppo2.py:185][0m |          -0.0121 |           0.0072 |           2.1386 |
[32m[20221213 22:20:03 @agent_ppo2.py:185][0m |          -0.0048 |           0.0072 |           2.1381 |
[32m[20221213 22:20:03 @agent_ppo2.py:185][0m |          -0.0127 |           0.0071 |           2.1376 |
[32m[20221213 22:20:03 @agent_ppo2.py:185][0m |          -0.0109 |           0.0071 |           2.1371 |
[32m[20221213 22:20:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:20:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:03 @agent_ppo2.py:143][0m Total time:       1.83 min
[32m[20221213 22:20:03 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 22:20:03 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 22:20:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:20:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:03 @agent_ppo2.py:185][0m |          -0.0015 |           0.0053 |           2.1594 |
[32m[20221213 22:20:03 @agent_ppo2.py:185][0m |          -0.0010 |           0.0030 |           2.1584 |
[32m[20221213 22:20:03 @agent_ppo2.py:185][0m |          -0.0052 |           0.0025 |           2.1565 |
[32m[20221213 22:20:03 @agent_ppo2.py:185][0m |          -0.0049 |           0.0023 |           2.1551 |
[32m[20221213 22:20:04 @agent_ppo2.py:185][0m |          -0.0082 |           0.0022 |           2.1540 |
[32m[20221213 22:20:04 @agent_ppo2.py:185][0m |          -0.0083 |           0.0021 |           2.1538 |
[32m[20221213 22:20:04 @agent_ppo2.py:185][0m |          -0.0112 |           0.0021 |           2.1530 |
[32m[20221213 22:20:04 @agent_ppo2.py:185][0m |          -0.0098 |           0.0021 |           2.1523 |
[32m[20221213 22:20:04 @agent_ppo2.py:185][0m |          -0.0128 |           0.0021 |           2.1519 |
[32m[20221213 22:20:04 @agent_ppo2.py:185][0m |          -0.0117 |           0.0020 |           2.1511 |
[32m[20221213 22:20:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:20:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.01
[32m[20221213 22:20:04 @agent_ppo2.py:143][0m Total time:       1.85 min
[32m[20221213 22:20:04 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 22:20:04 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 22:20:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:20:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:04 @agent_ppo2.py:185][0m |           0.0027 |           0.0014 |           2.1377 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |           0.0008 |           0.0013 |           2.1370 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |          -0.0059 |           0.0013 |           2.1364 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |          -0.0062 |           0.0013 |           2.1345 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |          -0.0033 |           0.0012 |           2.1349 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |           0.0013 |           0.0012 |           2.1351 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |          -0.0055 |           0.0012 |           2.1358 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |          -0.0035 |           0.0012 |           2.1357 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |          -0.0136 |           0.0012 |           2.1360 |
[32m[20221213 22:20:05 @agent_ppo2.py:185][0m |           0.0081 |           0.0012 |           2.1359 |
[32m[20221213 22:20:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:20:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:05 @agent_ppo2.py:143][0m Total time:       1.87 min
[32m[20221213 22:20:05 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 22:20:05 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 22:20:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0040 |           0.0011 |           2.1394 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0048 |           0.0011 |           2.1387 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0028 |           0.0011 |           2.1370 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0094 |           0.0011 |           2.1360 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0092 |           0.0011 |           2.1352 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0091 |           0.0010 |           2.1342 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0126 |           0.0010 |           2.1338 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0067 |           0.0010 |           2.1337 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0051 |           0.0010 |           2.1342 |
[32m[20221213 22:20:06 @agent_ppo2.py:185][0m |          -0.0050 |           0.0010 |           2.1334 |
[32m[20221213 22:20:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:20:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.55
[32m[20221213 22:20:07 @agent_ppo2.py:143][0m Total time:       1.90 min
[32m[20221213 22:20:07 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 22:20:07 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 22:20:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:20:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:07 @agent_ppo2.py:185][0m |           0.0005 |           0.0009 |           2.1702 |
[32m[20221213 22:20:07 @agent_ppo2.py:185][0m |          -0.0000 |           0.0009 |           2.1705 |
[32m[20221213 22:20:07 @agent_ppo2.py:185][0m |          -0.0042 |           0.0009 |           2.1705 |
[32m[20221213 22:20:07 @agent_ppo2.py:185][0m |          -0.0049 |           0.0009 |           2.1702 |
[32m[20221213 22:20:07 @agent_ppo2.py:185][0m |          -0.0164 |           0.0008 |           2.1701 |
[32m[20221213 22:20:07 @agent_ppo2.py:185][0m |          -0.0082 |           0.0008 |           2.1714 |
[32m[20221213 22:20:07 @agent_ppo2.py:185][0m |          -0.0106 |           0.0008 |           2.1723 |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |          -0.0110 |           0.0008 |           2.1726 |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |          -0.0226 |           0.0008 |           2.1736 |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |          -0.0092 |           0.0008 |           2.1743 |
[32m[20221213 22:20:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:20:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:08 @agent_ppo2.py:143][0m Total time:       1.92 min
[32m[20221213 22:20:08 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 22:20:08 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 22:20:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |           0.0023 |           0.0007 |           2.1663 |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |          -0.0016 |           0.0007 |           2.1659 |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |           0.0012 |           0.0007 |           2.1641 |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |          -0.0058 |           0.0007 |           2.1641 |
[32m[20221213 22:20:08 @agent_ppo2.py:185][0m |          -0.0073 |           0.0007 |           2.1639 |
[32m[20221213 22:20:09 @agent_ppo2.py:185][0m |          -0.0062 |           0.0006 |           2.1637 |
[32m[20221213 22:20:09 @agent_ppo2.py:185][0m |          -0.0101 |           0.0006 |           2.1640 |
[32m[20221213 22:20:09 @agent_ppo2.py:185][0m |          -0.0082 |           0.0006 |           2.1640 |
[32m[20221213 22:20:09 @agent_ppo2.py:185][0m |          -0.0076 |           0.0006 |           2.1648 |
[32m[20221213 22:20:09 @agent_ppo2.py:185][0m |          -0.0091 |           0.0006 |           2.1652 |
[32m[20221213 22:20:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:20:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.00
[32m[20221213 22:20:09 @agent_ppo2.py:143][0m Total time:       1.94 min
[32m[20221213 22:20:09 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 22:20:09 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 22:20:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:20:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:09 @agent_ppo2.py:185][0m |           0.0002 |           0.4421 |           2.2132 |
[32m[20221213 22:20:09 @agent_ppo2.py:185][0m |          -0.0029 |           0.4017 |           2.2128 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0037 |           0.3994 |           2.2136 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0043 |           0.3948 |           2.2141 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0049 |           0.3916 |           2.2147 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0054 |           0.4019 |           2.2154 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0184 |           0.4125 |           2.2156 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0062 |           0.3909 |           2.2171 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0066 |           0.3914 |           2.2173 |
[32m[20221213 22:20:10 @agent_ppo2.py:185][0m |          -0.0066 |           0.3884 |           2.2182 |
[32m[20221213 22:20:10 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.61
[32m[20221213 22:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 21.08
[32m[20221213 22:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:10 @agent_ppo2.py:143][0m Total time:       1.96 min
[32m[20221213 22:20:10 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 22:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 22:20:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0023 |           0.0205 |           2.2269 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0057 |           0.0081 |           2.2268 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0036 |           0.0073 |           2.2260 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0044 |           0.0071 |           2.2266 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0067 |           0.0069 |           2.2273 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0005 |           0.0069 |           2.2269 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0017 |           0.0068 |           2.2278 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0105 |           0.0067 |           2.2278 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0113 |           0.0066 |           2.2285 |
[32m[20221213 22:20:11 @agent_ppo2.py:185][0m |          -0.0119 |           0.0066 |           2.2283 |
[32m[20221213 22:20:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:20:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.69
[32m[20221213 22:20:11 @agent_ppo2.py:143][0m Total time:       1.98 min
[32m[20221213 22:20:11 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 22:20:11 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 22:20:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |           0.0044 |           0.0052 |           2.2400 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |          -0.0068 |           0.0028 |           2.2380 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |           0.0050 |           0.0024 |           2.2359 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |          -0.0031 |           0.0023 |           2.2342 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |          -0.0067 |           0.0023 |           2.2328 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |          -0.0042 |           0.0023 |           2.2313 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |          -0.0095 |           0.0023 |           2.2304 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |          -0.0127 |           0.0023 |           2.2297 |
[32m[20221213 22:20:12 @agent_ppo2.py:185][0m |          -0.0096 |           0.0022 |           2.2288 |
[32m[20221213 22:20:13 @agent_ppo2.py:185][0m |          -0.0003 |           0.0022 |           2.2284 |
[32m[20221213 22:20:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:20:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:13 @agent_ppo2.py:143][0m Total time:       2.00 min
[32m[20221213 22:20:13 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 22:20:13 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 22:20:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:13 @agent_ppo2.py:185][0m |          -0.0025 |           0.0015 |           2.2393 |
[32m[20221213 22:20:13 @agent_ppo2.py:185][0m |           0.0012 |           0.0012 |           2.2389 |
[32m[20221213 22:20:13 @agent_ppo2.py:185][0m |          -0.0055 |           0.0012 |           2.2369 |
[32m[20221213 22:20:13 @agent_ppo2.py:185][0m |          -0.0001 |           0.0012 |           2.2347 |
[32m[20221213 22:20:13 @agent_ppo2.py:185][0m |          -0.0102 |           0.0012 |           2.2342 |
[32m[20221213 22:20:13 @agent_ppo2.py:185][0m |          -0.0087 |           0.0012 |           2.2344 |
[32m[20221213 22:20:14 @agent_ppo2.py:185][0m |          -0.0071 |           0.0012 |           2.2339 |
[32m[20221213 22:20:14 @agent_ppo2.py:185][0m |          -0.0035 |           0.0012 |           2.2337 |
[32m[20221213 22:20:14 @agent_ppo2.py:185][0m |          -0.0043 |           0.0012 |           2.2327 |
[32m[20221213 22:20:14 @agent_ppo2.py:185][0m |          -0.0063 |           0.0011 |           2.2330 |
[32m[20221213 22:20:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:20:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:14 @agent_ppo2.py:143][0m Total time:       2.02 min
[32m[20221213 22:20:14 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 22:20:14 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 22:20:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:14 @agent_ppo2.py:185][0m |           0.0078 |           0.0010 |           2.2242 |
[32m[20221213 22:20:14 @agent_ppo2.py:185][0m |          -0.0038 |           0.0009 |           2.2226 |
[32m[20221213 22:20:14 @agent_ppo2.py:185][0m |          -0.0094 |           0.0009 |           2.2213 |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |          -0.0031 |           0.0009 |           2.2218 |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |          -0.0053 |           0.0008 |           2.2215 |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |          -0.0161 |           0.0008 |           2.2225 |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |          -0.0154 |           0.0008 |           2.2220 |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |          -0.0031 |           0.0008 |           2.2224 |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |          -0.0063 |           0.0008 |           2.2235 |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |          -0.0156 |           0.0008 |           2.2244 |
[32m[20221213 22:20:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:20:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:15 @agent_ppo2.py:143][0m Total time:       2.04 min
[32m[20221213 22:20:15 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 22:20:15 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 22:20:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:15 @agent_ppo2.py:185][0m |           0.0032 |           0.0007 |           2.2188 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0003 |           0.0007 |           2.2170 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0086 |           0.0007 |           2.2148 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0028 |           0.0007 |           2.2144 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0071 |           0.0007 |           2.2135 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0050 |           0.0006 |           2.2126 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0073 |           0.0006 |           2.2128 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0074 |           0.0006 |           2.2138 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0095 |           0.0006 |           2.2130 |
[32m[20221213 22:20:16 @agent_ppo2.py:185][0m |          -0.0039 |           0.0006 |           2.2130 |
[32m[20221213 22:20:16 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:20:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:16 @agent_ppo2.py:143][0m Total time:       2.06 min
[32m[20221213 22:20:16 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 22:20:16 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 22:20:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0005 |           0.0006 |           2.2491 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |           0.0014 |           0.0006 |           2.2485 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |           0.0131 |           0.0005 |           2.2465 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0063 |           0.0005 |           2.2452 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0049 |           0.0005 |           2.2440 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0038 |           0.0005 |           2.2435 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0057 |           0.0005 |           2.2435 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0091 |           0.0005 |           2.2430 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0118 |           0.0005 |           2.2425 |
[32m[20221213 22:20:17 @agent_ppo2.py:185][0m |          -0.0099 |           0.0004 |           2.2419 |
[32m[20221213 22:20:17 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:18 @agent_ppo2.py:143][0m Total time:       2.08 min
[32m[20221213 22:20:18 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 22:20:18 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 22:20:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |          -0.0025 |           0.0004 |           2.2382 |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |           0.0005 |           0.0004 |           2.2391 |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |          -0.0073 |           0.0004 |           2.2400 |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |          -0.0035 |           0.0004 |           2.2394 |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |          -0.0061 |           0.0004 |           2.2396 |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |          -0.0089 |           0.0004 |           2.2388 |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |           0.0040 |           0.0004 |           2.2389 |
[32m[20221213 22:20:18 @agent_ppo2.py:185][0m |          -0.0103 |           0.0003 |           2.2373 |
[32m[20221213 22:20:19 @agent_ppo2.py:185][0m |          -0.0106 |           0.0003 |           2.2375 |
[32m[20221213 22:20:19 @agent_ppo2.py:185][0m |          -0.0112 |           0.0003 |           2.2372 |
[32m[20221213 22:20:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:20:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:19 @agent_ppo2.py:143][0m Total time:       2.10 min
[32m[20221213 22:20:19 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 22:20:19 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 22:20:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:19 @agent_ppo2.py:185][0m |          -0.0022 |           0.0003 |           2.1783 |
[32m[20221213 22:20:19 @agent_ppo2.py:185][0m |          -0.0043 |           0.0003 |           2.1756 |
[32m[20221213 22:20:19 @agent_ppo2.py:185][0m |          -0.0064 |           0.0003 |           2.1734 |
[32m[20221213 22:20:19 @agent_ppo2.py:185][0m |          -0.0013 |           0.0003 |           2.1713 |
[32m[20221213 22:20:19 @agent_ppo2.py:185][0m |          -0.0078 |           0.0003 |           2.1698 |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0084 |           0.0002 |           2.1677 |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0084 |           0.0002 |           2.1679 |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0046 |           0.0002 |           2.1660 |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0089 |           0.0002 |           2.1670 |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0094 |           0.0002 |           2.1651 |
[32m[20221213 22:20:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:20:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.76
[32m[20221213 22:20:20 @agent_ppo2.py:143][0m Total time:       2.12 min
[32m[20221213 22:20:20 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 22:20:20 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 22:20:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0019 |           0.0002 |           2.2770 |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0003 |           0.0002 |           2.2755 |
[32m[20221213 22:20:20 @agent_ppo2.py:185][0m |          -0.0052 |           0.0002 |           2.2726 |
[32m[20221213 22:20:21 @agent_ppo2.py:185][0m |          -0.0065 |           0.0002 |           2.2702 |
[32m[20221213 22:20:21 @agent_ppo2.py:185][0m |          -0.0071 |           0.0001 |           2.2688 |
[32m[20221213 22:20:21 @agent_ppo2.py:185][0m |          -0.0038 |           0.0001 |           2.2674 |
[32m[20221213 22:20:21 @agent_ppo2.py:185][0m |          -0.0076 |           0.0001 |           2.2651 |
[32m[20221213 22:20:21 @agent_ppo2.py:185][0m |          -0.0079 |           0.0001 |           2.2640 |
[32m[20221213 22:20:21 @agent_ppo2.py:185][0m |          -0.0076 |           0.0001 |           2.2642 |
[32m[20221213 22:20:21 @agent_ppo2.py:185][0m |           0.0102 |           0.0001 |           2.2638 |
[32m[20221213 22:20:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:21 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 22:20:21 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 22:20:21 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 22:20:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |           0.0022 |           0.3522 |           2.1797 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0005 |           0.3060 |           2.1774 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0122 |           0.3444 |           2.1756 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0017 |           0.2978 |           2.1734 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0016 |           0.2932 |           2.1703 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0080 |           0.2929 |           2.1694 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0030 |           0.2946 |           2.1682 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0034 |           0.2902 |           2.1672 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0057 |           0.2926 |           2.1659 |
[32m[20221213 22:20:22 @agent_ppo2.py:185][0m |          -0.0033 |           0.2889 |           2.1652 |
[32m[20221213 22:20:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.21
[32m[20221213 22:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 19.51
[32m[20221213 22:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 14.04
[32m[20221213 22:20:22 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221213 22:20:22 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 22:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 22:20:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0000 |           0.0307 |           2.1891 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0037 |           0.0096 |           2.1875 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0081 |           0.0077 |           2.1852 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0076 |           0.0073 |           2.1836 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0048 |           0.0073 |           2.1828 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0108 |           0.0072 |           2.1821 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0072 |           0.0072 |           2.1813 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0087 |           0.0072 |           2.1814 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0088 |           0.0072 |           2.1802 |
[32m[20221213 22:20:23 @agent_ppo2.py:185][0m |          -0.0048 |           0.0072 |           2.1804 |
[32m[20221213 22:20:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:24 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221213 22:20:24 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 22:20:24 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 22:20:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:24 @agent_ppo2.py:185][0m |           0.0040 |           0.0049 |           2.2338 |
[32m[20221213 22:20:24 @agent_ppo2.py:185][0m |          -0.0039 |           0.0022 |           2.2341 |
[32m[20221213 22:20:24 @agent_ppo2.py:185][0m |           0.0007 |           0.0019 |           2.2326 |
[32m[20221213 22:20:24 @agent_ppo2.py:185][0m |          -0.0091 |           0.0019 |           2.2324 |
[32m[20221213 22:20:24 @agent_ppo2.py:185][0m |          -0.0034 |           0.0019 |           2.2322 |
[32m[20221213 22:20:24 @agent_ppo2.py:185][0m |          -0.0113 |           0.0018 |           2.2323 |
[32m[20221213 22:20:24 @agent_ppo2.py:185][0m |          -0.0109 |           0.0018 |           2.2330 |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |          -0.0077 |           0.0018 |           2.2323 |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |          -0.0011 |           0.0018 |           2.2330 |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |          -0.0136 |           0.0018 |           2.2340 |
[32m[20221213 22:20:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:25 @agent_ppo2.py:143][0m Total time:       2.20 min
[32m[20221213 22:20:25 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 22:20:25 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 22:20:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |           0.0017 |           0.0438 |           2.1652 |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |           0.0000 |           0.0426 |           2.1643 |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |          -0.0017 |           0.0422 |           2.1648 |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |          -0.0016 |           0.0423 |           2.1644 |
[32m[20221213 22:20:25 @agent_ppo2.py:185][0m |          -0.0038 |           0.0424 |           2.1635 |
[32m[20221213 22:20:26 @agent_ppo2.py:185][0m |          -0.0042 |           0.0418 |           2.1634 |
[32m[20221213 22:20:26 @agent_ppo2.py:185][0m |          -0.0053 |           0.0421 |           2.1634 |
[32m[20221213 22:20:26 @agent_ppo2.py:185][0m |          -0.0053 |           0.0419 |           2.1626 |
[32m[20221213 22:20:26 @agent_ppo2.py:185][0m |          -0.0055 |           0.0418 |           2.1621 |
[32m[20221213 22:20:26 @agent_ppo2.py:185][0m |          -0.0064 |           0.0419 |           2.1618 |
[32m[20221213 22:20:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:20:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.23
[32m[20221213 22:20:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.15
[32m[20221213 22:20:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:26 @agent_ppo2.py:143][0m Total time:       2.22 min
[32m[20221213 22:20:26 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 22:20:26 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 22:20:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |           0.0012 |           0.0015 |           2.2244 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |           0.0014 |           0.0009 |           2.2245 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0055 |           0.0009 |           2.2255 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0082 |           0.0008 |           2.2253 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0112 |           0.0008 |           2.2262 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0083 |           0.0008 |           2.2262 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0105 |           0.0008 |           2.2274 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0097 |           0.0008 |           2.2278 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0053 |           0.0008 |           2.2293 |
[32m[20221213 22:20:27 @agent_ppo2.py:185][0m |          -0.0121 |           0.0008 |           2.2293 |
[32m[20221213 22:20:27 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 22:20:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 21.35
[32m[20221213 22:20:28 @agent_ppo2.py:143][0m Total time:       2.25 min
[32m[20221213 22:20:28 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 22:20:28 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 22:20:28 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 22:20:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:28 @agent_ppo2.py:185][0m |           0.0099 |           0.0006 |           2.2364 |
[32m[20221213 22:20:28 @agent_ppo2.py:185][0m |          -0.0065 |           0.0005 |           2.2355 |
[32m[20221213 22:20:28 @agent_ppo2.py:185][0m |          -0.0094 |           0.0005 |           2.2353 |
[32m[20221213 22:20:29 @agent_ppo2.py:185][0m |          -0.0048 |           0.0005 |           2.2352 |
[32m[20221213 22:20:29 @agent_ppo2.py:185][0m |           0.0013 |           0.0005 |           2.2352 |
[32m[20221213 22:20:29 @agent_ppo2.py:185][0m |          -0.0092 |           0.0005 |           2.2340 |
[32m[20221213 22:20:29 @agent_ppo2.py:185][0m |          -0.0124 |           0.0005 |           2.2345 |
[32m[20221213 22:20:29 @agent_ppo2.py:185][0m |          -0.0096 |           0.0005 |           2.2345 |
[32m[20221213 22:20:29 @agent_ppo2.py:185][0m |          -0.0125 |           0.0005 |           2.2345 |
[32m[20221213 22:20:29 @agent_ppo2.py:185][0m |          -0.0079 |           0.0005 |           2.2344 |
[32m[20221213 22:20:29 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 22:20:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:29 @agent_ppo2.py:143][0m Total time:       2.27 min
[32m[20221213 22:20:29 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 22:20:29 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 22:20:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:20:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |          -0.0019 |           0.0004 |           2.2895 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |          -0.0051 |           0.0004 |           2.2867 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |          -0.0061 |           0.0004 |           2.2865 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |           0.0011 |           0.0004 |           2.2862 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |          -0.0033 |           0.0004 |           2.2885 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |          -0.0019 |           0.0004 |           2.2875 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |           0.0021 |           0.0004 |           2.2889 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |          -0.0027 |           0.0004 |           2.2892 |
[32m[20221213 22:20:30 @agent_ppo2.py:185][0m |           0.0005 |           0.0004 |           2.2914 |
[32m[20221213 22:20:31 @agent_ppo2.py:185][0m |          -0.0075 |           0.0004 |           2.2922 |
[32m[20221213 22:20:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:20:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:31 @agent_ppo2.py:143][0m Total time:       2.30 min
[32m[20221213 22:20:31 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 22:20:31 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 22:20:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:31 @agent_ppo2.py:185][0m |          -0.0005 |           0.0004 |           2.2946 |
[32m[20221213 22:20:31 @agent_ppo2.py:185][0m |           0.0021 |           0.0003 |           2.2917 |
[32m[20221213 22:20:31 @agent_ppo2.py:185][0m |          -0.0086 |           0.0003 |           2.2891 |
[32m[20221213 22:20:31 @agent_ppo2.py:185][0m |          -0.0050 |           0.0003 |           2.2872 |
[32m[20221213 22:20:31 @agent_ppo2.py:185][0m |          -0.0074 |           0.0003 |           2.2856 |
[32m[20221213 22:20:32 @agent_ppo2.py:185][0m |          -0.0073 |           0.0003 |           2.2855 |
[32m[20221213 22:20:32 @agent_ppo2.py:185][0m |          -0.0046 |           0.0003 |           2.2849 |
[32m[20221213 22:20:32 @agent_ppo2.py:185][0m |          -0.0120 |           0.0003 |           2.2848 |
[32m[20221213 22:20:32 @agent_ppo2.py:185][0m |          -0.0106 |           0.0003 |           2.2838 |
[32m[20221213 22:20:32 @agent_ppo2.py:185][0m |          -0.0086 |           0.0003 |           2.2841 |
[32m[20221213 22:20:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:20:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:32 @agent_ppo2.py:143][0m Total time:       2.32 min
[32m[20221213 22:20:32 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 22:20:32 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 22:20:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:32 @agent_ppo2.py:185][0m |          -0.0002 |           0.0003 |           2.2465 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0021 |           0.0003 |           2.2433 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0086 |           0.0002 |           2.2409 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0088 |           0.0002 |           2.2393 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0097 |           0.0002 |           2.2375 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0035 |           0.0002 |           2.2358 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0105 |           0.0002 |           2.2352 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0120 |           0.0002 |           2.2342 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0147 |           0.0002 |           2.2329 |
[32m[20221213 22:20:33 @agent_ppo2.py:185][0m |          -0.0108 |           0.0002 |           2.2314 |
[32m[20221213 22:20:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:20:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.87
[32m[20221213 22:20:33 @agent_ppo2.py:143][0m Total time:       2.34 min
[32m[20221213 22:20:33 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221213 22:20:33 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 22:20:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |          -0.0022 |           0.0002 |           2.1861 |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |          -0.0035 |           0.0002 |           2.1861 |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |           0.0016 |           0.0002 |           2.1857 |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |          -0.0062 |           0.0001 |           2.1851 |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |          -0.0023 |           0.0001 |           2.1837 |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |          -0.0079 |           0.0001 |           2.1832 |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |          -0.0036 |           0.0001 |           2.1834 |
[32m[20221213 22:20:34 @agent_ppo2.py:185][0m |          -0.0090 |           0.0001 |           2.1820 |
[32m[20221213 22:20:35 @agent_ppo2.py:185][0m |          -0.0094 |           0.0001 |           2.1817 |
[32m[20221213 22:20:35 @agent_ppo2.py:185][0m |          -0.0092 |           0.0001 |           2.1820 |
[32m[20221213 22:20:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:20:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:35 @agent_ppo2.py:143][0m Total time:       2.37 min
[32m[20221213 22:20:35 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 22:20:35 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 22:20:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:20:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:35 @agent_ppo2.py:185][0m |          -0.0022 |           0.0001 |           2.2631 |
[32m[20221213 22:20:35 @agent_ppo2.py:185][0m |          -0.0017 |           0.0001 |           2.2620 |
[32m[20221213 22:20:35 @agent_ppo2.py:185][0m |          -0.0059 |           0.0001 |           2.2621 |
[32m[20221213 22:20:35 @agent_ppo2.py:185][0m |          -0.0021 |           0.0001 |           2.2620 |
[32m[20221213 22:20:36 @agent_ppo2.py:185][0m |          -0.0079 |           0.0001 |           2.2616 |
[32m[20221213 22:20:36 @agent_ppo2.py:185][0m |          -0.0084 |           0.0001 |           2.2611 |
[32m[20221213 22:20:36 @agent_ppo2.py:185][0m |          -0.0064 |           0.0001 |           2.2610 |
[32m[20221213 22:20:36 @agent_ppo2.py:185][0m |           0.0032 |           0.0001 |           2.2608 |
[32m[20221213 22:20:36 @agent_ppo2.py:185][0m |          -0.0096 |           0.0000 |           2.2594 |
[32m[20221213 22:20:36 @agent_ppo2.py:185][0m |          -0.0072 |           0.0000 |           2.2599 |
[32m[20221213 22:20:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:20:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:36 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221213 22:20:36 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221213 22:20:36 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 22:20:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0021 |           0.0000 |           2.2628 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0062 |           0.0000 |           2.2622 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0073 |           0.0000 |           2.2615 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0083 |           0.0000 |           2.2606 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |           0.0024 |           0.0000 |           2.2588 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0110 |           0.0000 |           2.2575 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0115 |           0.0000 |           2.2574 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0119 |           0.0000 |           2.2561 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0118 |           0.0000 |           2.2560 |
[32m[20221213 22:20:37 @agent_ppo2.py:185][0m |          -0.0124 |           0.0000 |           2.2553 |
[32m[20221213 22:20:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:20:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:38 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 22:20:38 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 22:20:38 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 22:20:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:38 @agent_ppo2.py:185][0m |           0.0017 |           0.1889 |           2.2885 |
[32m[20221213 22:20:38 @agent_ppo2.py:185][0m |          -0.0008 |           0.1398 |           2.2896 |
[32m[20221213 22:20:38 @agent_ppo2.py:185][0m |          -0.0014 |           0.1371 |           2.2909 |
[32m[20221213 22:20:38 @agent_ppo2.py:185][0m |          -0.0024 |           0.1378 |           2.2922 |
[32m[20221213 22:20:38 @agent_ppo2.py:185][0m |          -0.0023 |           0.1363 |           2.2929 |
[32m[20221213 22:20:38 @agent_ppo2.py:185][0m |          -0.0028 |           0.1305 |           2.2934 |
[32m[20221213 22:20:38 @agent_ppo2.py:185][0m |          -0.0035 |           0.1311 |           2.2943 |
[32m[20221213 22:20:39 @agent_ppo2.py:185][0m |          -0.0033 |           0.1298 |           2.2950 |
[32m[20221213 22:20:39 @agent_ppo2.py:185][0m |          -0.0038 |           0.1290 |           2.2958 |
[32m[20221213 22:20:39 @agent_ppo2.py:185][0m |          -0.0041 |           0.1309 |           2.2969 |
[32m[20221213 22:20:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:20:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.84
[32m[20221213 22:20:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.53
[32m[20221213 22:20:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:39 @agent_ppo2.py:143][0m Total time:       2.43 min
[32m[20221213 22:20:39 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221213 22:20:39 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 22:20:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:39 @agent_ppo2.py:185][0m |          -0.0025 |           0.0262 |           2.2508 |
[32m[20221213 22:20:39 @agent_ppo2.py:185][0m |          -0.0059 |           0.0153 |           2.2504 |
[32m[20221213 22:20:39 @agent_ppo2.py:185][0m |          -0.0105 |           0.0143 |           2.2497 |
[32m[20221213 22:20:40 @agent_ppo2.py:185][0m |          -0.0056 |           0.0140 |           2.2492 |
[32m[20221213 22:20:40 @agent_ppo2.py:185][0m |          -0.0052 |           0.0139 |           2.2495 |
[32m[20221213 22:20:40 @agent_ppo2.py:185][0m |          -0.0128 |           0.0139 |           2.2497 |
[32m[20221213 22:20:40 @agent_ppo2.py:185][0m |          -0.0124 |           0.0139 |           2.2486 |
[32m[20221213 22:20:40 @agent_ppo2.py:185][0m |          -0.0132 |           0.0138 |           2.2489 |
[32m[20221213 22:20:40 @agent_ppo2.py:185][0m |          -0.0114 |           0.0137 |           2.2497 |
[32m[20221213 22:20:40 @agent_ppo2.py:185][0m |          -0.0065 |           0.0137 |           2.2485 |
[32m[20221213 22:20:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:20:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:40 @agent_ppo2.py:143][0m Total time:       2.46 min
[32m[20221213 22:20:40 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 22:20:40 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 22:20:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |           0.0029 |           0.0105 |           2.2587 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0008 |           0.0090 |           2.2576 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0048 |           0.0088 |           2.2557 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0059 |           0.0087 |           2.2547 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0019 |           0.0085 |           2.2541 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0045 |           0.0085 |           2.2521 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0063 |           0.0087 |           2.2513 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0044 |           0.0083 |           2.2511 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0099 |           0.0083 |           2.2509 |
[32m[20221213 22:20:41 @agent_ppo2.py:185][0m |          -0.0124 |           0.0083 |           2.2502 |
[32m[20221213 22:20:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:42 @agent_ppo2.py:143][0m Total time:       2.48 min
[32m[20221213 22:20:42 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221213 22:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 22:20:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:20:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:42 @agent_ppo2.py:185][0m |          -0.0037 |           0.0062 |           2.3036 |
[32m[20221213 22:20:42 @agent_ppo2.py:185][0m |          -0.0053 |           0.0057 |           2.3021 |
[32m[20221213 22:20:42 @agent_ppo2.py:185][0m |          -0.0078 |           0.0056 |           2.2999 |
[32m[20221213 22:20:42 @agent_ppo2.py:185][0m |          -0.0063 |           0.0056 |           2.2999 |
[32m[20221213 22:20:42 @agent_ppo2.py:185][0m |          -0.0078 |           0.0056 |           2.2991 |
[32m[20221213 22:20:42 @agent_ppo2.py:185][0m |          -0.0077 |           0.0055 |           2.2991 |
[32m[20221213 22:20:43 @agent_ppo2.py:185][0m |          -0.0023 |           0.0058 |           2.2986 |
[32m[20221213 22:20:43 @agent_ppo2.py:185][0m |          -0.0114 |           0.0055 |           2.2986 |
[32m[20221213 22:20:43 @agent_ppo2.py:185][0m |          -0.0104 |           0.0055 |           2.2995 |
[32m[20221213 22:20:43 @agent_ppo2.py:185][0m |          -0.0063 |           0.0056 |           2.2994 |
[32m[20221213 22:20:43 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:20:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:43 @agent_ppo2.py:143][0m Total time:       2.50 min
[32m[20221213 22:20:43 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 22:20:43 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 22:20:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:43 @agent_ppo2.py:185][0m |          -0.0018 |           0.0025 |           2.2925 |
[32m[20221213 22:20:43 @agent_ppo2.py:185][0m |           0.0021 |           0.0022 |           2.2893 |
[32m[20221213 22:20:43 @agent_ppo2.py:185][0m |          -0.0020 |           0.0021 |           2.2863 |
[32m[20221213 22:20:44 @agent_ppo2.py:185][0m |           0.0018 |           0.0022 |           2.2844 |
[32m[20221213 22:20:44 @agent_ppo2.py:185][0m |          -0.0047 |           0.0021 |           2.2821 |
[32m[20221213 22:20:44 @agent_ppo2.py:185][0m |          -0.0087 |           0.0020 |           2.2816 |
[32m[20221213 22:20:44 @agent_ppo2.py:185][0m |           0.0058 |           0.0022 |           2.2821 |
[32m[20221213 22:20:44 @agent_ppo2.py:185][0m |          -0.0096 |           0.0020 |           2.2808 |
[32m[20221213 22:20:44 @agent_ppo2.py:185][0m |          -0.0106 |           0.0020 |           2.2796 |
[32m[20221213 22:20:44 @agent_ppo2.py:185][0m |          -0.0079 |           0.0020 |           2.2801 |
[32m[20221213 22:20:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:20:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:44 @agent_ppo2.py:143][0m Total time:       2.52 min
[32m[20221213 22:20:44 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221213 22:20:44 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 22:20:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0025 |           0.0026 |           2.2689 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0062 |           0.0024 |           2.2695 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0065 |           0.0024 |           2.2692 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0083 |           0.0024 |           2.2681 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0076 |           0.0024 |           2.2679 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0080 |           0.0024 |           2.2675 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0023 |           0.0024 |           2.2669 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0117 |           0.0023 |           2.2667 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0132 |           0.0023 |           2.2662 |
[32m[20221213 22:20:45 @agent_ppo2.py:185][0m |          -0.0129 |           0.0023 |           2.2661 |
[32m[20221213 22:20:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:45 @agent_ppo2.py:143][0m Total time:       2.54 min
[32m[20221213 22:20:45 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 22:20:45 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 22:20:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |           0.0054 |           0.0015 |           2.2530 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |           0.0083 |           0.0014 |           2.2530 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |           0.0064 |           0.0013 |           2.2500 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |          -0.0044 |           0.0012 |           2.2487 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |          -0.0048 |           0.0012 |           2.2482 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |          -0.0012 |           0.0012 |           2.2478 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |          -0.0050 |           0.0012 |           2.2474 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |          -0.0051 |           0.0012 |           2.2473 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |          -0.0056 |           0.0011 |           2.2461 |
[32m[20221213 22:20:46 @agent_ppo2.py:185][0m |          -0.0058 |           0.0011 |           2.2461 |
[32m[20221213 22:20:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.67
[32m[20221213 22:20:47 @agent_ppo2.py:143][0m Total time:       2.56 min
[32m[20221213 22:20:47 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221213 22:20:47 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 22:20:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0025 |           0.0009 |           2.2772 |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0060 |           0.0008 |           2.2762 |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0069 |           0.0008 |           2.2737 |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0095 |           0.0008 |           2.2732 |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0120 |           0.0008 |           2.2712 |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0113 |           0.0008 |           2.2694 |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0011 |           0.0008 |           2.2687 |
[32m[20221213 22:20:47 @agent_ppo2.py:185][0m |          -0.0124 |           0.0008 |           2.2670 |
[32m[20221213 22:20:48 @agent_ppo2.py:185][0m |          -0.0120 |           0.0007 |           2.2663 |
[32m[20221213 22:20:48 @agent_ppo2.py:185][0m |          -0.0060 |           0.0008 |           2.2650 |
[32m[20221213 22:20:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:48 @agent_ppo2.py:143][0m Total time:       2.58 min
[32m[20221213 22:20:48 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 22:20:48 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 22:20:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:20:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:48 @agent_ppo2.py:185][0m |           0.0016 |           0.3294 |           2.2434 |
[32m[20221213 22:20:48 @agent_ppo2.py:185][0m |          -0.0009 |           0.2715 |           2.2424 |
[32m[20221213 22:20:48 @agent_ppo2.py:185][0m |          -0.0080 |           0.2605 |           2.2415 |
[32m[20221213 22:20:48 @agent_ppo2.py:185][0m |          -0.0028 |           0.2593 |           2.2416 |
[32m[20221213 22:20:48 @agent_ppo2.py:185][0m |          -0.0026 |           0.2605 |           2.2411 |
[32m[20221213 22:20:49 @agent_ppo2.py:185][0m |          -0.0031 |           0.2581 |           2.2406 |
[32m[20221213 22:20:49 @agent_ppo2.py:185][0m |          -0.0031 |           0.2576 |           2.2409 |
[32m[20221213 22:20:49 @agent_ppo2.py:185][0m |          -0.0032 |           0.2550 |           2.2411 |
[32m[20221213 22:20:49 @agent_ppo2.py:185][0m |          -0.0037 |           0.2537 |           2.2409 |
[32m[20221213 22:20:49 @agent_ppo2.py:185][0m |          -0.0040 |           0.2541 |           2.2405 |
[32m[20221213 22:20:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:20:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.47
[32m[20221213 22:20:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 14.65
[32m[20221213 22:20:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:49 @agent_ppo2.py:143][0m Total time:       2.60 min
[32m[20221213 22:20:49 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221213 22:20:49 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 22:20:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:49 @agent_ppo2.py:185][0m |          -0.0026 |           0.0361 |           2.2802 |
[32m[20221213 22:20:49 @agent_ppo2.py:185][0m |          -0.0013 |           0.0141 |           2.2791 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0056 |           0.0135 |           2.2768 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0127 |           0.0132 |           2.2755 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0179 |           0.0129 |           2.2745 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0091 |           0.0129 |           2.2738 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0158 |           0.0128 |           2.2737 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0134 |           0.0127 |           2.2735 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0084 |           0.0129 |           2.2731 |
[32m[20221213 22:20:50 @agent_ppo2.py:185][0m |          -0.0190 |           0.0127 |           2.2722 |
[32m[20221213 22:20:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:20:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:50 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221213 22:20:50 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 22:20:50 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 22:20:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0013 |           0.0067 |           2.2875 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0038 |           0.0041 |           2.2898 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0054 |           0.0039 |           2.2899 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0059 |           0.0037 |           2.2903 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0030 |           0.0036 |           2.2919 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0084 |           0.0036 |           2.2926 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0120 |           0.0035 |           2.2947 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0062 |           0.0035 |           2.2954 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0139 |           0.0034 |           2.2965 |
[32m[20221213 22:20:51 @agent_ppo2.py:185][0m |          -0.0118 |           0.0034 |           2.2983 |
[32m[20221213 22:20:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:20:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:52 @agent_ppo2.py:143][0m Total time:       2.65 min
[32m[20221213 22:20:52 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221213 22:20:52 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 22:20:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:52 @agent_ppo2.py:185][0m |          -0.0012 |           0.0019 |           2.3398 |
[32m[20221213 22:20:52 @agent_ppo2.py:185][0m |          -0.0016 |           0.0016 |           2.3389 |
[32m[20221213 22:20:52 @agent_ppo2.py:185][0m |          -0.0045 |           0.0015 |           2.3372 |
[32m[20221213 22:20:52 @agent_ppo2.py:185][0m |          -0.0005 |           0.0015 |           2.3373 |
[32m[20221213 22:20:52 @agent_ppo2.py:185][0m |          -0.0051 |           0.0015 |           2.3362 |
[32m[20221213 22:20:52 @agent_ppo2.py:185][0m |          -0.0054 |           0.0015 |           2.3362 |
[32m[20221213 22:20:52 @agent_ppo2.py:185][0m |          -0.0045 |           0.0015 |           2.3360 |
[32m[20221213 22:20:53 @agent_ppo2.py:185][0m |          -0.0039 |           0.0014 |           2.3369 |
[32m[20221213 22:20:53 @agent_ppo2.py:185][0m |          -0.0060 |           0.0014 |           2.3364 |
[32m[20221213 22:20:53 @agent_ppo2.py:185][0m |          -0.0116 |           0.0014 |           2.3369 |
[32m[20221213 22:20:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:20:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:53 @agent_ppo2.py:143][0m Total time:       2.67 min
[32m[20221213 22:20:53 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 22:20:53 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 22:20:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:53 @agent_ppo2.py:185][0m |           0.0074 |           0.0011 |           2.3379 |
[32m[20221213 22:20:53 @agent_ppo2.py:185][0m |          -0.0004 |           0.0010 |           2.3357 |
[32m[20221213 22:20:53 @agent_ppo2.py:185][0m |          -0.0033 |           0.0010 |           2.3329 |
[32m[20221213 22:20:53 @agent_ppo2.py:185][0m |          -0.0117 |           0.0010 |           2.3308 |
[32m[20221213 22:20:54 @agent_ppo2.py:185][0m |          -0.0103 |           0.0010 |           2.3301 |
[32m[20221213 22:20:54 @agent_ppo2.py:185][0m |          -0.0031 |           0.0010 |           2.3293 |
[32m[20221213 22:20:54 @agent_ppo2.py:185][0m |          -0.0091 |           0.0010 |           2.3279 |
[32m[20221213 22:20:54 @agent_ppo2.py:185][0m |          -0.0002 |           0.0010 |           2.3263 |
[32m[20221213 22:20:54 @agent_ppo2.py:185][0m |          -0.0114 |           0.0009 |           2.3249 |
[32m[20221213 22:20:54 @agent_ppo2.py:185][0m |          -0.0084 |           0.0009 |           2.3242 |
[32m[20221213 22:20:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.97
[32m[20221213 22:20:54 @agent_ppo2.py:143][0m Total time:       2.69 min
[32m[20221213 22:20:54 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221213 22:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 22:20:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:54 @agent_ppo2.py:185][0m |          -0.0012 |           0.0009 |           2.2403 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |           0.0018 |           0.0008 |           2.2400 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |           0.0023 |           0.0008 |           2.2387 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |          -0.0054 |           0.0008 |           2.2375 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |          -0.0062 |           0.0008 |           2.2361 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |          -0.0065 |           0.0008 |           2.2353 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |          -0.0059 |           0.0008 |           2.2348 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |          -0.0096 |           0.0008 |           2.2341 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |           0.0106 |           0.0008 |           2.2333 |
[32m[20221213 22:20:55 @agent_ppo2.py:185][0m |          -0.0103 |           0.0007 |           2.2312 |
[32m[20221213 22:20:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:20:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:55 @agent_ppo2.py:143][0m Total time:       2.71 min
[32m[20221213 22:20:55 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 22:20:55 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 22:20:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0021 |           0.0007 |           2.2603 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0068 |           0.0007 |           2.2582 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0061 |           0.0007 |           2.2565 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0091 |           0.0006 |           2.2551 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0067 |           0.0006 |           2.2533 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0064 |           0.0006 |           2.2523 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0046 |           0.0006 |           2.2524 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0110 |           0.0006 |           2.2514 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0026 |           0.0006 |           2.2513 |
[32m[20221213 22:20:56 @agent_ppo2.py:185][0m |          -0.0090 |           0.0006 |           2.2511 |
[32m[20221213 22:20:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:57 @agent_ppo2.py:143][0m Total time:       2.73 min
[32m[20221213 22:20:57 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221213 22:20:57 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 22:20:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0008 |           0.2786 |           2.2812 |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0032 |           0.2198 |           2.2819 |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0037 |           0.2203 |           2.2833 |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0043 |           0.2133 |           2.2852 |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0046 |           0.2134 |           2.2865 |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0051 |           0.2097 |           2.2871 |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0053 |           0.2094 |           2.2896 |
[32m[20221213 22:20:57 @agent_ppo2.py:185][0m |          -0.0054 |           0.2087 |           2.2912 |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0059 |           0.2073 |           2.2919 |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0108 |           0.2074 |           2.2939 |
[32m[20221213 22:20:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 11.51
[32m[20221213 22:20:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 13.16
[32m[20221213 22:20:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:58 @agent_ppo2.py:143][0m Total time:       2.75 min
[32m[20221213 22:20:58 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 22:20:58 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 22:20:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0032 |           0.0346 |           2.3036 |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0033 |           0.0133 |           2.3029 |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0084 |           0.0127 |           2.3025 |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0083 |           0.0126 |           2.3017 |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0040 |           0.0126 |           2.3011 |
[32m[20221213 22:20:58 @agent_ppo2.py:185][0m |          -0.0146 |           0.0128 |           2.3007 |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |          -0.0079 |           0.0124 |           2.3006 |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |          -0.0078 |           0.0123 |           2.3004 |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |          -0.0120 |           0.0124 |           2.2999 |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |          -0.0082 |           0.0122 |           2.2994 |
[32m[20221213 22:20:59 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:20:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:20:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:20:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:20:59 @agent_ppo2.py:143][0m Total time:       2.77 min
[32m[20221213 22:20:59 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221213 22:20:59 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 22:20:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:20:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |           0.0015 |           0.2078 |           2.3276 |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |          -0.0196 |           0.2137 |           2.3282 |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |          -0.0010 |           0.1859 |           2.3285 |
[32m[20221213 22:20:59 @agent_ppo2.py:185][0m |          -0.0027 |           0.1794 |           2.3304 |
[32m[20221213 22:21:00 @agent_ppo2.py:185][0m |          -0.0022 |           0.1791 |           2.3308 |
[32m[20221213 22:21:00 @agent_ppo2.py:185][0m |          -0.0036 |           0.1782 |           2.3312 |
[32m[20221213 22:21:00 @agent_ppo2.py:185][0m |          -0.0034 |           0.1806 |           2.3317 |
[32m[20221213 22:21:00 @agent_ppo2.py:185][0m |          -0.0039 |           0.1799 |           2.3323 |
[32m[20221213 22:21:00 @agent_ppo2.py:185][0m |          -0.0042 |           0.1796 |           2.3330 |
[32m[20221213 22:21:00 @agent_ppo2.py:185][0m |          -0.0050 |           0.1789 |           2.3340 |
[32m[20221213 22:21:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:21:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.89
[32m[20221213 22:21:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.17
[32m[20221213 22:21:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:00 @agent_ppo2.py:143][0m Total time:       2.79 min
[32m[20221213 22:21:00 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 22:21:00 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 22:21:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:00 @agent_ppo2.py:185][0m |           0.0065 |           0.0352 |           2.3683 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0047 |           0.0158 |           2.3671 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0049 |           0.0151 |           2.3684 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0105 |           0.0150 |           2.3686 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0004 |           0.0150 |           2.3678 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0047 |           0.0150 |           2.3690 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0079 |           0.0150 |           2.3696 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0107 |           0.0148 |           2.3699 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0110 |           0.0149 |           2.3705 |
[32m[20221213 22:21:01 @agent_ppo2.py:185][0m |          -0.0161 |           0.0149 |           2.3702 |
[32m[20221213 22:21:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:21:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:01 @agent_ppo2.py:143][0m Total time:       2.81 min
[32m[20221213 22:21:01 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221213 22:21:01 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 22:21:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |           0.0025 |           2.1197 |           2.3591 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0039 |           1.6736 |           2.3578 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0011 |           1.6196 |           2.3566 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0043 |           1.5970 |           2.3570 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0030 |           1.5940 |           2.3562 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0033 |           1.5714 |           2.3563 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0043 |           1.5450 |           2.3565 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0035 |           1.5356 |           2.3567 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0039 |           1.5174 |           2.3569 |
[32m[20221213 22:21:02 @agent_ppo2.py:185][0m |          -0.0040 |           1.5313 |           2.3560 |
[32m[20221213 22:21:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 31.43
[32m[20221213 22:21:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.67
[32m[20221213 22:21:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:03 @agent_ppo2.py:143][0m Total time:       2.83 min
[32m[20221213 22:21:03 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 22:21:03 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 22:21:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:21:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |          -0.0003 |           0.1586 |           2.4336 |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |          -0.0034 |           0.0318 |           2.4335 |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |          -0.0043 |           0.0280 |           2.4334 |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |          -0.0032 |           0.0271 |           2.4336 |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |           0.0257 |           0.0281 |           2.4329 |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |          -0.0054 |           0.0263 |           2.4338 |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |          -0.0054 |           0.0262 |           2.4325 |
[32m[20221213 22:21:03 @agent_ppo2.py:185][0m |          -0.0080 |           0.0262 |           2.4327 |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |          -0.0058 |           0.0258 |           2.4327 |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |          -0.0056 |           0.0257 |           2.4331 |
[32m[20221213 22:21:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:04 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221213 22:21:04 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221213 22:21:04 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 22:21:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |           0.0114 |           0.2522 |           2.4198 |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |           0.0029 |           0.2274 |           2.4185 |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |          -0.0055 |           0.2267 |           2.4180 |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |          -0.0096 |           0.2303 |           2.4185 |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |          -0.0092 |           0.2272 |           2.4185 |
[32m[20221213 22:21:04 @agent_ppo2.py:185][0m |          -0.0228 |           0.2785 |           2.4200 |
[32m[20221213 22:21:05 @agent_ppo2.py:185][0m |          -0.0049 |           0.2272 |           2.4201 |
[32m[20221213 22:21:05 @agent_ppo2.py:185][0m |          -0.0059 |           0.2248 |           2.4203 |
[32m[20221213 22:21:05 @agent_ppo2.py:185][0m |          -0.0047 |           0.2249 |           2.4210 |
[32m[20221213 22:21:05 @agent_ppo2.py:185][0m |          -0.0035 |           0.2290 |           2.4210 |
[32m[20221213 22:21:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:21:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 9.85
[32m[20221213 22:21:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.34
[32m[20221213 22:21:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:05 @agent_ppo2.py:143][0m Total time:       2.87 min
[32m[20221213 22:21:05 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 22:21:05 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 22:21:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:05 @agent_ppo2.py:185][0m |          -0.0024 |           0.0596 |           2.4267 |
[32m[20221213 22:21:05 @agent_ppo2.py:185][0m |          -0.0015 |           0.0233 |           2.4252 |
[32m[20221213 22:21:05 @agent_ppo2.py:185][0m |          -0.0052 |           0.0196 |           2.4225 |
[32m[20221213 22:21:06 @agent_ppo2.py:185][0m |          -0.0167 |           0.0185 |           2.4225 |
[32m[20221213 22:21:06 @agent_ppo2.py:185][0m |          -0.0117 |           0.0180 |           2.4232 |
[32m[20221213 22:21:06 @agent_ppo2.py:185][0m |          -0.0137 |           0.0178 |           2.4233 |
[32m[20221213 22:21:06 @agent_ppo2.py:185][0m |          -0.0162 |           0.0176 |           2.4247 |
[32m[20221213 22:21:06 @agent_ppo2.py:185][0m |          -0.0135 |           0.0175 |           2.4249 |
[32m[20221213 22:21:06 @agent_ppo2.py:185][0m |          -0.0150 |           0.0178 |           2.4255 |
[32m[20221213 22:21:06 @agent_ppo2.py:185][0m |          -0.0110 |           0.0174 |           2.4263 |
[32m[20221213 22:21:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.60
[32m[20221213 22:21:06 @agent_ppo2.py:143][0m Total time:       2.89 min
[32m[20221213 22:21:06 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221213 22:21:06 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 22:21:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |           0.0038 |           0.0085 |           2.4080 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0024 |           0.0055 |           2.4057 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0143 |           0.0048 |           2.4041 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0023 |           0.0046 |           2.4033 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0121 |           0.0044 |           2.4028 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0117 |           0.0044 |           2.4031 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0027 |           0.0044 |           2.4013 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0047 |           0.0043 |           2.4011 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0172 |           0.0043 |           2.4005 |
[32m[20221213 22:21:07 @agent_ppo2.py:185][0m |          -0.0116 |           0.0043 |           2.4003 |
[32m[20221213 22:21:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:21:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:07 @agent_ppo2.py:143][0m Total time:       2.91 min
[32m[20221213 22:21:07 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 22:21:07 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 22:21:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |           0.0013 |           0.0280 |           2.4040 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |           0.0004 |           0.0274 |           2.4033 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |          -0.0007 |           0.0272 |           2.4029 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |          -0.0012 |           0.0271 |           2.4031 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |          -0.0004 |           0.0271 |           2.4028 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |           0.0001 |           0.0271 |           2.4028 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |          -0.0009 |           0.0269 |           2.4026 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |          -0.0012 |           0.0269 |           2.4028 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |          -0.0021 |           0.0269 |           2.4033 |
[32m[20221213 22:21:08 @agent_ppo2.py:185][0m |          -0.0015 |           0.0269 |           2.4033 |
[32m[20221213 22:21:08 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:21:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.04
[32m[20221213 22:21:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 5.20
[32m[20221213 22:21:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:09 @agent_ppo2.py:143][0m Total time:       2.93 min
[32m[20221213 22:21:09 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221213 22:21:09 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 22:21:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |          -0.0118 |           0.3523 |           2.3977 |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |           0.0010 |           0.1974 |           2.3957 |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |          -0.0009 |           0.1883 |           2.3959 |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |          -0.0014 |           0.1834 |           2.3949 |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |          -0.0019 |           0.1802 |           2.3951 |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |          -0.0023 |           0.1816 |           2.3949 |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |          -0.0024 |           0.1762 |           2.3948 |
[32m[20221213 22:21:09 @agent_ppo2.py:185][0m |          -0.0026 |           0.1759 |           2.3951 |
[32m[20221213 22:21:10 @agent_ppo2.py:185][0m |          -0.0022 |           0.1762 |           2.3951 |
[32m[20221213 22:21:10 @agent_ppo2.py:185][0m |          -0.0030 |           0.1743 |           2.3960 |
[32m[20221213 22:21:10 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:21:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 12.07
[32m[20221213 22:21:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.30
[32m[20221213 22:21:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 14.05
[32m[20221213 22:21:10 @agent_ppo2.py:143][0m Total time:       2.95 min
[32m[20221213 22:21:10 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 22:21:10 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 22:21:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:10 @agent_ppo2.py:185][0m |          -0.0048 |           0.0717 |           2.4648 |
[32m[20221213 22:21:10 @agent_ppo2.py:185][0m |          -0.0011 |           0.0275 |           2.4637 |
[32m[20221213 22:21:10 @agent_ppo2.py:185][0m |          -0.0078 |           0.0250 |           2.4638 |
[32m[20221213 22:21:10 @agent_ppo2.py:185][0m |          -0.0074 |           0.0241 |           2.4639 |
[32m[20221213 22:21:10 @agent_ppo2.py:185][0m |          -0.0113 |           0.0241 |           2.4636 |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0078 |           0.0237 |           2.4641 |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0098 |           0.0233 |           2.4638 |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0144 |           0.0232 |           2.4640 |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0100 |           0.0230 |           2.4649 |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0096 |           0.0230 |           2.4662 |
[32m[20221213 22:21:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.44
[32m[20221213 22:21:11 @agent_ppo2.py:143][0m Total time:       2.97 min
[32m[20221213 22:21:11 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221213 22:21:11 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 22:21:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0071 |           0.0184 |           2.4810 |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0100 |           0.0116 |           2.4820 |
[32m[20221213 22:21:11 @agent_ppo2.py:185][0m |          -0.0075 |           0.0103 |           2.4829 |
[32m[20221213 22:21:12 @agent_ppo2.py:185][0m |          -0.0112 |           0.0099 |           2.4837 |
[32m[20221213 22:21:12 @agent_ppo2.py:185][0m |          -0.0132 |           0.0099 |           2.4860 |
[32m[20221213 22:21:12 @agent_ppo2.py:185][0m |          -0.0110 |           0.0096 |           2.4872 |
[32m[20221213 22:21:12 @agent_ppo2.py:185][0m |          -0.0073 |           0.0096 |           2.4885 |
[32m[20221213 22:21:12 @agent_ppo2.py:185][0m |          -0.0028 |           0.0099 |           2.4897 |
[32m[20221213 22:21:12 @agent_ppo2.py:185][0m |          -0.0138 |           0.0095 |           2.4909 |
[32m[20221213 22:21:12 @agent_ppo2.py:185][0m |          -0.0138 |           0.0094 |           2.4927 |
[32m[20221213 22:21:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:12 @agent_ppo2.py:143][0m Total time:       2.99 min
[32m[20221213 22:21:12 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 22:21:12 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 22:21:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0014 |           0.0077 |           2.5364 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0025 |           0.0067 |           2.5342 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0060 |           0.0067 |           2.5311 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0108 |           0.0066 |           2.5291 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0061 |           0.0066 |           2.5275 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0089 |           0.0065 |           2.5276 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0054 |           0.0065 |           2.5274 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0101 |           0.0064 |           2.5264 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0103 |           0.0064 |           2.5258 |
[32m[20221213 22:21:13 @agent_ppo2.py:185][0m |          -0.0153 |           0.0063 |           2.5249 |
[32m[20221213 22:21:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:13 @agent_ppo2.py:143][0m Total time:       3.01 min
[32m[20221213 22:21:13 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221213 22:21:13 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 22:21:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0014 |           0.0059 |           2.5279 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0062 |           0.0054 |           2.5237 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0024 |           0.0054 |           2.5211 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0080 |           0.0053 |           2.5187 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0000 |           0.0055 |           2.5178 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0083 |           0.0052 |           2.5167 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0105 |           0.0052 |           2.5134 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0103 |           0.0051 |           2.5130 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0097 |           0.0051 |           2.5119 |
[32m[20221213 22:21:14 @agent_ppo2.py:185][0m |          -0.0095 |           0.0050 |           2.5101 |
[32m[20221213 22:21:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:15 @agent_ppo2.py:143][0m Total time:       3.03 min
[32m[20221213 22:21:15 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 22:21:15 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 22:21:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |          -0.0019 |           0.0050 |           2.5629 |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |          -0.0030 |           0.0046 |           2.5621 |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |          -0.0054 |           0.0046 |           2.5631 |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |          -0.0043 |           0.0045 |           2.5624 |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |          -0.0030 |           0.0045 |           2.5647 |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |          -0.0071 |           0.0044 |           2.5625 |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |           0.0040 |           0.0047 |           2.5646 |
[32m[20221213 22:21:15 @agent_ppo2.py:185][0m |          -0.0090 |           0.0043 |           2.5636 |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |          -0.0092 |           0.0043 |           2.5634 |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |           0.0023 |           0.0046 |           2.5650 |
[32m[20221213 22:21:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.83
[32m[20221213 22:21:16 @agent_ppo2.py:143][0m Total time:       3.05 min
[32m[20221213 22:21:16 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221213 22:21:16 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 22:21:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |           0.0017 |           0.0037 |           2.5134 |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |          -0.0048 |           0.0035 |           2.5114 |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |          -0.0069 |           0.0034 |           2.5105 |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |          -0.0049 |           0.0034 |           2.5085 |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |          -0.0034 |           0.0033 |           2.5086 |
[32m[20221213 22:21:16 @agent_ppo2.py:185][0m |          -0.0069 |           0.0033 |           2.5087 |
[32m[20221213 22:21:17 @agent_ppo2.py:185][0m |          -0.0106 |           0.0032 |           2.5084 |
[32m[20221213 22:21:17 @agent_ppo2.py:185][0m |          -0.0101 |           0.0032 |           2.5082 |
[32m[20221213 22:21:17 @agent_ppo2.py:185][0m |          -0.0107 |           0.0031 |           2.5088 |
[32m[20221213 22:21:17 @agent_ppo2.py:185][0m |           0.0009 |           0.0033 |           2.5084 |
[32m[20221213 22:21:17 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:21:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:17 @agent_ppo2.py:143][0m Total time:       3.07 min
[32m[20221213 22:21:17 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 22:21:17 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 22:21:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:17 @agent_ppo2.py:185][0m |          -0.0139 |           0.0352 |           2.5808 |
[32m[20221213 22:21:17 @agent_ppo2.py:185][0m |          -0.0012 |           0.0276 |           2.5815 |
[32m[20221213 22:21:17 @agent_ppo2.py:185][0m |          -0.0026 |           0.0263 |           2.5814 |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0036 |           0.0258 |           2.5802 |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0028 |           0.0262 |           2.5803 |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0053 |           0.0257 |           2.5789 |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0036 |           0.0257 |           2.5788 |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0061 |           0.0260 |           2.5781 |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0065 |           0.0260 |           2.5776 |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0068 |           0.0258 |           2.5770 |
[32m[20221213 22:21:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.55
[32m[20221213 22:21:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 3.50
[32m[20221213 22:21:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:18 @agent_ppo2.py:143][0m Total time:       3.09 min
[32m[20221213 22:21:18 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221213 22:21:18 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 22:21:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:18 @agent_ppo2.py:185][0m |          -0.0001 |           0.0041 |           2.5787 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0065 |           0.0026 |           2.5771 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0014 |           0.0026 |           2.5753 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0003 |           0.0025 |           2.5732 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0056 |           0.0025 |           2.5732 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0107 |           0.0025 |           2.5721 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |           0.0004 |           0.0025 |           2.5719 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0078 |           0.0024 |           2.5718 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0096 |           0.0024 |           2.5705 |
[32m[20221213 22:21:19 @agent_ppo2.py:185][0m |          -0.0118 |           0.0024 |           2.5710 |
[32m[20221213 22:21:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:21:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 6.08
[32m[20221213 22:21:19 @agent_ppo2.py:143][0m Total time:       3.11 min
[32m[20221213 22:21:19 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 22:21:19 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 22:21:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0026 |           0.0019 |           2.5670 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |           0.0010 |           0.0015 |           2.5621 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0120 |           0.0015 |           2.5597 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0110 |           0.0014 |           2.5580 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0098 |           0.0014 |           2.5569 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0094 |           0.0014 |           2.5581 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0055 |           0.0014 |           2.5567 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0131 |           0.0013 |           2.5557 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0135 |           0.0013 |           2.5556 |
[32m[20221213 22:21:20 @agent_ppo2.py:185][0m |          -0.0111 |           0.0013 |           2.5549 |
[32m[20221213 22:21:20 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:21:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 18.76
[32m[20221213 22:21:21 @agent_ppo2.py:143][0m Total time:       3.13 min
[32m[20221213 22:21:21 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221213 22:21:21 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 22:21:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0017 |           0.0012 |           2.5135 |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0054 |           0.0011 |           2.5115 |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0082 |           0.0011 |           2.5092 |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0079 |           0.0011 |           2.5066 |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0056 |           0.0011 |           2.5058 |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0089 |           0.0010 |           2.5048 |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0087 |           0.0010 |           2.5034 |
[32m[20221213 22:21:21 @agent_ppo2.py:185][0m |          -0.0099 |           0.0010 |           2.5029 |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |          -0.0012 |           0.0010 |           2.5019 |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |          -0.0100 |           0.0010 |           2.5002 |
[32m[20221213 22:21:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:22 @agent_ppo2.py:143][0m Total time:       3.15 min
[32m[20221213 22:21:22 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 22:21:22 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 22:21:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |          -0.0011 |           0.0009 |           2.5452 |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |           0.0005 |           0.0008 |           2.5454 |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |          -0.0050 |           0.0008 |           2.5444 |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |          -0.0056 |           0.0008 |           2.5430 |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |          -0.0061 |           0.0007 |           2.5426 |
[32m[20221213 22:21:22 @agent_ppo2.py:185][0m |          -0.0072 |           0.0007 |           2.5423 |
[32m[20221213 22:21:23 @agent_ppo2.py:185][0m |          -0.0090 |           0.0007 |           2.5422 |
[32m[20221213 22:21:23 @agent_ppo2.py:185][0m |          -0.0094 |           0.0007 |           2.5425 |
[32m[20221213 22:21:23 @agent_ppo2.py:185][0m |          -0.0098 |           0.0007 |           2.5429 |
[32m[20221213 22:21:23 @agent_ppo2.py:185][0m |          -0.0099 |           0.0007 |           2.5425 |
[32m[20221213 22:21:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:23 @agent_ppo2.py:143][0m Total time:       3.17 min
[32m[20221213 22:21:23 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221213 22:21:23 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 22:21:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:23 @agent_ppo2.py:185][0m |          -0.0013 |           0.0007 |           2.5486 |
[32m[20221213 22:21:23 @agent_ppo2.py:185][0m |          -0.0077 |           0.0006 |           2.5456 |
[32m[20221213 22:21:23 @agent_ppo2.py:185][0m |          -0.0099 |           0.0005 |           2.5412 |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0105 |           0.0005 |           2.5402 |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0113 |           0.0005 |           2.5397 |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0064 |           0.0005 |           2.5374 |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0124 |           0.0005 |           2.5367 |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0131 |           0.0005 |           2.5370 |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0134 |           0.0005 |           2.5358 |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0135 |           0.0005 |           2.5355 |
[32m[20221213 22:21:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:21:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:24 @agent_ppo2.py:143][0m Total time:       3.19 min
[32m[20221213 22:21:24 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 22:21:24 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 22:21:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:24 @agent_ppo2.py:185][0m |          -0.0019 |           0.0005 |           2.4721 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0047 |           0.0004 |           2.4699 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0064 |           0.0004 |           2.4681 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0073 |           0.0004 |           2.4675 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0082 |           0.0004 |           2.4647 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0089 |           0.0004 |           2.4642 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0094 |           0.0004 |           2.4634 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0069 |           0.0004 |           2.4625 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0101 |           0.0004 |           2.4621 |
[32m[20221213 22:21:25 @agent_ppo2.py:185][0m |          -0.0101 |           0.0004 |           2.4593 |
[32m[20221213 22:21:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:21:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:25 @agent_ppo2.py:143][0m Total time:       3.21 min
[32m[20221213 22:21:25 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221213 22:21:25 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 22:21:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0020 |           0.0004 |           2.5173 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |           0.0165 |           0.0004 |           2.5167 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0059 |           0.0003 |           2.5171 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0032 |           0.0003 |           2.5170 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0072 |           0.0003 |           2.5175 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0080 |           0.0003 |           2.5178 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0082 |           0.0003 |           2.5189 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0081 |           0.0003 |           2.5196 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0090 |           0.0003 |           2.5209 |
[32m[20221213 22:21:26 @agent_ppo2.py:185][0m |          -0.0091 |           0.0003 |           2.5219 |
[32m[20221213 22:21:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:21:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.95
[32m[20221213 22:21:27 @agent_ppo2.py:143][0m Total time:       3.23 min
[32m[20221213 22:21:27 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 22:21:27 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 22:21:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |          -0.0023 |           0.0003 |           2.5724 |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |          -0.0051 |           0.0002 |           2.5706 |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |          -0.0065 |           0.0002 |           2.5688 |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |          -0.0064 |           0.0002 |           2.5669 |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |          -0.0062 |           0.0002 |           2.5653 |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |          -0.0092 |           0.0002 |           2.5631 |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |           0.0028 |           0.0003 |           2.5620 |
[32m[20221213 22:21:27 @agent_ppo2.py:185][0m |           0.0023 |           0.0003 |           2.5605 |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0107 |           0.0002 |           2.5587 |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0112 |           0.0002 |           2.5585 |
[32m[20221213 22:21:28 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:21:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:28 @agent_ppo2.py:143][0m Total time:       3.25 min
[32m[20221213 22:21:28 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221213 22:21:28 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 22:21:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0019 |           0.0002 |           2.5389 |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0051 |           0.0002 |           2.5332 |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0069 |           0.0002 |           2.5293 |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0080 |           0.0002 |           2.5281 |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0087 |           0.0002 |           2.5259 |
[32m[20221213 22:21:28 @agent_ppo2.py:185][0m |          -0.0070 |           0.0002 |           2.5233 |
[32m[20221213 22:21:29 @agent_ppo2.py:185][0m |          -0.0095 |           0.0002 |           2.5227 |
[32m[20221213 22:21:29 @agent_ppo2.py:185][0m |          -0.0096 |           0.0002 |           2.5218 |
[32m[20221213 22:21:29 @agent_ppo2.py:185][0m |          -0.0102 |           0.0002 |           2.5191 |
[32m[20221213 22:21:29 @agent_ppo2.py:185][0m |          -0.0076 |           0.0002 |           2.5192 |
[32m[20221213 22:21:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:21:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:29 @agent_ppo2.py:143][0m Total time:       3.27 min
[32m[20221213 22:21:29 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 22:21:29 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 22:21:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:29 @agent_ppo2.py:185][0m |           0.0091 |           0.0002 |           2.5211 |
[32m[20221213 22:21:29 @agent_ppo2.py:185][0m |          -0.0024 |           0.0002 |           2.5184 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0051 |           0.0002 |           2.5176 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0039 |           0.0002 |           2.5158 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0038 |           0.0002 |           2.5137 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0081 |           0.0002 |           2.5125 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0085 |           0.0002 |           2.5114 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0086 |           0.0002 |           2.5081 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0066 |           0.0002 |           2.5086 |
[32m[20221213 22:21:30 @agent_ppo2.py:185][0m |          -0.0098 |           0.0002 |           2.5060 |
[32m[20221213 22:21:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:21:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:30 @agent_ppo2.py:143][0m Total time:       3.29 min
[32m[20221213 22:21:30 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221213 22:21:30 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 22:21:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0009 |           0.0001 |           2.4836 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0046 |           0.0001 |           2.4808 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0065 |           0.0001 |           2.4779 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |           0.0071 |           0.0001 |           2.4757 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0082 |           0.0001 |           2.4716 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0084 |           0.0001 |           2.4704 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0094 |           0.0001 |           2.4695 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0097 |           0.0001 |           2.4685 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0101 |           0.0001 |           2.4666 |
[32m[20221213 22:21:31 @agent_ppo2.py:185][0m |          -0.0064 |           0.0001 |           2.4656 |
[32m[20221213 22:21:31 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 22:21:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:32 @agent_ppo2.py:143][0m Total time:       3.31 min
[32m[20221213 22:21:32 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 22:21:32 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 22:21:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:21:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:32 @agent_ppo2.py:185][0m |           0.0021 |           1.2129 |           2.4702 |
[32m[20221213 22:21:32 @agent_ppo2.py:185][0m |           0.0009 |           0.9883 |           2.4686 |
[32m[20221213 22:21:32 @agent_ppo2.py:185][0m |          -0.0134 |           0.6746 |           2.4682 |
[32m[20221213 22:21:32 @agent_ppo2.py:185][0m |          -0.0100 |           0.5938 |           2.4680 |
[32m[20221213 22:21:32 @agent_ppo2.py:185][0m |          -0.0010 |           0.5686 |           2.4671 |
[32m[20221213 22:21:33 @agent_ppo2.py:185][0m |          -0.0013 |           0.5495 |           2.4671 |
[32m[20221213 22:21:33 @agent_ppo2.py:185][0m |          -0.0015 |           0.5425 |           2.4668 |
[32m[20221213 22:21:33 @agent_ppo2.py:185][0m |          -0.0020 |           0.5375 |           2.4665 |
[32m[20221213 22:21:33 @agent_ppo2.py:185][0m |          -0.0140 |           0.5638 |           2.4657 |
[32m[20221213 22:21:33 @agent_ppo2.py:185][0m |          -0.0023 |           0.5316 |           2.4634 |
[32m[20221213 22:21:33 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221213 22:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 15.93
[32m[20221213 22:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 19.03
[32m[20221213 22:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:33 @agent_ppo2.py:143][0m Total time:       3.34 min
[32m[20221213 22:21:33 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221213 22:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 22:21:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 22:21:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0057 |           0.3658 |           2.4637 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0074 |           0.3662 |           2.4653 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0057 |           0.3594 |           2.4669 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0052 |           0.3600 |           2.4679 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0096 |           0.3599 |           2.4694 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0063 |           0.3581 |           2.4708 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0103 |           0.3593 |           2.4714 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0070 |           0.3642 |           2.4737 |
[32m[20221213 22:21:34 @agent_ppo2.py:185][0m |          -0.0089 |           0.3596 |           2.4746 |
[32m[20221213 22:21:35 @agent_ppo2.py:185][0m |          -0.0104 |           0.3605 |           2.4754 |
[32m[20221213 22:21:35 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.13
[32m[20221213 22:21:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 18.53
[32m[20221213 22:21:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:35 @agent_ppo2.py:143][0m Total time:       3.36 min
[32m[20221213 22:21:35 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 22:21:35 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 22:21:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:21:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:35 @agent_ppo2.py:185][0m |           0.0036 |           0.1723 |           2.5206 |
[32m[20221213 22:21:35 @agent_ppo2.py:185][0m |          -0.0028 |           0.1198 |           2.5194 |
[32m[20221213 22:21:35 @agent_ppo2.py:185][0m |          -0.0044 |           0.1120 |           2.5181 |
[32m[20221213 22:21:35 @agent_ppo2.py:185][0m |          -0.0090 |           0.1100 |           2.5175 |
[32m[20221213 22:21:35 @agent_ppo2.py:185][0m |          -0.0081 |           0.1111 |           2.5164 |
[32m[20221213 22:21:36 @agent_ppo2.py:185][0m |          -0.0081 |           0.1097 |           2.5163 |
[32m[20221213 22:21:36 @agent_ppo2.py:185][0m |          -0.0098 |           0.1082 |           2.5158 |
[32m[20221213 22:21:36 @agent_ppo2.py:185][0m |          -0.0055 |           0.1132 |           2.5162 |
[32m[20221213 22:21:36 @agent_ppo2.py:185][0m |          -0.0076 |           0.1087 |           2.5173 |
[32m[20221213 22:21:36 @agent_ppo2.py:185][0m |          -0.0119 |           0.1079 |           2.5176 |
[32m[20221213 22:21:36 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:21:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.10
[32m[20221213 22:21:36 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 58.10
[32m[20221213 22:21:36 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 58.10
[32m[20221213 22:21:36 @agent_ppo2.py:143][0m Total time:       3.39 min
[32m[20221213 22:21:36 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221213 22:21:36 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 22:21:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:21:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0074 |           0.1900 |           2.4902 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0070 |           0.1697 |           2.4886 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |           0.0038 |           0.1734 |           2.4869 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0109 |           0.1683 |           2.4848 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0069 |           0.1666 |           2.4846 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0094 |           0.1665 |           2.4838 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0096 |           0.1666 |           2.4846 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0090 |           0.1697 |           2.4846 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0091 |           0.1670 |           2.4842 |
[32m[20221213 22:21:37 @agent_ppo2.py:185][0m |          -0.0087 |           0.1670 |           2.4845 |
[32m[20221213 22:21:37 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.19
[32m[20221213 22:21:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 6.90
[32m[20221213 22:21:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:38 @agent_ppo2.py:143][0m Total time:       3.41 min
[32m[20221213 22:21:38 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 22:21:38 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 22:21:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:21:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:38 @agent_ppo2.py:185][0m |          -0.0064 |           0.0546 |           2.5274 |
[32m[20221213 22:21:38 @agent_ppo2.py:185][0m |          -0.0046 |           0.0415 |           2.5272 |
[32m[20221213 22:21:38 @agent_ppo2.py:185][0m |          -0.0050 |           0.0393 |           2.5260 |
[32m[20221213 22:21:38 @agent_ppo2.py:185][0m |          -0.0083 |           0.0386 |           2.5251 |
[32m[20221213 22:21:38 @agent_ppo2.py:185][0m |          -0.0079 |           0.0380 |           2.5240 |
[32m[20221213 22:21:39 @agent_ppo2.py:185][0m |          -0.0036 |           0.0378 |           2.5231 |
[32m[20221213 22:21:39 @agent_ppo2.py:185][0m |          -0.0061 |           0.0375 |           2.5232 |
[32m[20221213 22:21:39 @agent_ppo2.py:185][0m |          -0.0057 |           0.0374 |           2.5218 |
[32m[20221213 22:21:39 @agent_ppo2.py:185][0m |          -0.0120 |           0.0373 |           2.5224 |
[32m[20221213 22:21:39 @agent_ppo2.py:185][0m |          -0.0097 |           0.0374 |           2.5215 |
[32m[20221213 22:21:39 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:39 @agent_ppo2.py:143][0m Total time:       3.44 min
[32m[20221213 22:21:39 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221213 22:21:39 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 22:21:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:39 @agent_ppo2.py:185][0m |           0.0003 |           1.1947 |           2.5210 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0056 |           0.8737 |           2.5167 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0023 |           0.8217 |           2.5159 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0034 |           0.8120 |           2.5152 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0042 |           0.8012 |           2.5129 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0148 |           0.7789 |           2.5121 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0043 |           0.7504 |           2.5105 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0052 |           0.7432 |           2.5109 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0058 |           0.7377 |           2.5098 |
[32m[20221213 22:21:40 @agent_ppo2.py:185][0m |          -0.0204 |           0.8007 |           2.5089 |
[32m[20221213 22:21:40 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 20.13
[32m[20221213 22:21:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 32.93
[32m[20221213 22:21:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:41 @agent_ppo2.py:143][0m Total time:       3.46 min
[32m[20221213 22:21:41 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 22:21:41 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 22:21:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:41 @agent_ppo2.py:185][0m |           0.0012 |           0.1774 |           2.5376 |
[32m[20221213 22:21:41 @agent_ppo2.py:185][0m |          -0.0060 |           0.1034 |           2.5356 |
[32m[20221213 22:21:41 @agent_ppo2.py:185][0m |          -0.0058 |           0.0953 |           2.5340 |
[32m[20221213 22:21:41 @agent_ppo2.py:185][0m |          -0.0117 |           0.0928 |           2.5339 |
[32m[20221213 22:21:41 @agent_ppo2.py:185][0m |          -0.0088 |           0.0908 |           2.5340 |
[32m[20221213 22:21:41 @agent_ppo2.py:185][0m |          -0.0065 |           0.0901 |           2.5327 |
[32m[20221213 22:21:41 @agent_ppo2.py:185][0m |          -0.0105 |           0.0911 |           2.5334 |
[32m[20221213 22:21:42 @agent_ppo2.py:185][0m |          -0.0089 |           0.0896 |           2.5339 |
[32m[20221213 22:21:42 @agent_ppo2.py:185][0m |          -0.0102 |           0.0893 |           2.5361 |
[32m[20221213 22:21:42 @agent_ppo2.py:185][0m |          -0.0099 |           0.0891 |           2.5355 |
[32m[20221213 22:21:42 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:21:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:42 @agent_ppo2.py:143][0m Total time:       3.48 min
[32m[20221213 22:21:42 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221213 22:21:42 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 22:21:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:42 @agent_ppo2.py:185][0m |          -0.0000 |           0.0765 |           2.5429 |
[32m[20221213 22:21:42 @agent_ppo2.py:185][0m |           0.0003 |           0.0645 |           2.5410 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |          -0.0052 |           0.0638 |           2.5407 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |          -0.0050 |           0.0636 |           2.5398 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |          -0.0115 |           0.0633 |           2.5390 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |          -0.0065 |           0.0633 |           2.5394 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |          -0.0088 |           0.0631 |           2.5389 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |          -0.0033 |           0.0632 |           2.5394 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |           0.0169 |           0.0791 |           2.5395 |
[32m[20221213 22:21:43 @agent_ppo2.py:185][0m |          -0.0037 |           0.0645 |           2.5389 |
[32m[20221213 22:21:43 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:43 @agent_ppo2.py:143][0m Total time:       3.51 min
[32m[20221213 22:21:43 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 22:21:43 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 22:21:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:21:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |           0.0075 |           0.0546 |           2.5552 |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |          -0.0061 |           0.0472 |           2.5532 |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |          -0.0058 |           0.0471 |           2.5526 |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |          -0.0114 |           0.0471 |           2.5503 |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |           0.0032 |           0.0519 |           2.5500 |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |          -0.0074 |           0.0473 |           2.5474 |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |          -0.0014 |           0.0477 |           2.5469 |
[32m[20221213 22:21:44 @agent_ppo2.py:185][0m |          -0.0045 |           0.0476 |           2.5487 |
[32m[20221213 22:21:45 @agent_ppo2.py:185][0m |          -0.0075 |           0.0475 |           2.5480 |
[32m[20221213 22:21:45 @agent_ppo2.py:185][0m |          -0.0111 |           0.0471 |           2.5477 |
[32m[20221213 22:21:45 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:45 @agent_ppo2.py:143][0m Total time:       3.53 min
[32m[20221213 22:21:45 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221213 22:21:45 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 22:21:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:21:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:45 @agent_ppo2.py:185][0m |           0.0043 |           0.0482 |           2.5912 |
[32m[20221213 22:21:45 @agent_ppo2.py:185][0m |          -0.0071 |           0.0415 |           2.5886 |
[32m[20221213 22:21:45 @agent_ppo2.py:185][0m |          -0.0056 |           0.0411 |           2.5854 |
[32m[20221213 22:21:46 @agent_ppo2.py:185][0m |          -0.0046 |           0.0414 |           2.5856 |
[32m[20221213 22:21:46 @agent_ppo2.py:185][0m |          -0.0095 |           0.0408 |           2.5850 |
[32m[20221213 22:21:46 @agent_ppo2.py:185][0m |          -0.0094 |           0.0407 |           2.5854 |
[32m[20221213 22:21:46 @agent_ppo2.py:185][0m |          -0.0091 |           0.0407 |           2.5857 |
[32m[20221213 22:21:46 @agent_ppo2.py:185][0m |          -0.0102 |           0.0407 |           2.5858 |
[32m[20221213 22:21:46 @agent_ppo2.py:185][0m |          -0.0110 |           0.0406 |           2.5852 |
[32m[20221213 22:21:46 @agent_ppo2.py:185][0m |          -0.0084 |           0.0407 |           2.5856 |
[32m[20221213 22:21:46 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 22:21:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.11
[32m[20221213 22:21:46 @agent_ppo2.py:143][0m Total time:       3.56 min
[32m[20221213 22:21:46 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 22:21:46 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 22:21:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |           0.0044 |           1.9604 |           2.5580 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |           0.0015 |           1.4394 |           2.5578 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |           0.0007 |           1.3871 |           2.5565 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |          -0.0001 |           1.3629 |           2.5562 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |          -0.0037 |           1.3446 |           2.5560 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |          -0.0084 |           1.3191 |           2.5544 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |          -0.0007 |           1.2960 |           2.5517 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |          -0.0088 |           1.2866 |           2.5519 |
[32m[20221213 22:21:47 @agent_ppo2.py:185][0m |          -0.0026 |           1.2309 |           2.5504 |
[32m[20221213 22:21:48 @agent_ppo2.py:185][0m |          -0.0029 |           1.1991 |           2.5508 |
[32m[20221213 22:21:48 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 32.67
[32m[20221213 22:21:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.28
[32m[20221213 22:21:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:48 @agent_ppo2.py:143][0m Total time:       3.58 min
[32m[20221213 22:21:48 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221213 22:21:48 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 22:21:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:48 @agent_ppo2.py:185][0m |          -0.0051 |           0.4004 |           2.5232 |
[32m[20221213 22:21:48 @agent_ppo2.py:185][0m |          -0.0033 |           0.3246 |           2.5229 |
[32m[20221213 22:21:48 @agent_ppo2.py:185][0m |          -0.0063 |           0.3203 |           2.5212 |
[32m[20221213 22:21:48 @agent_ppo2.py:185][0m |          -0.0056 |           0.3181 |           2.5204 |
[32m[20221213 22:21:49 @agent_ppo2.py:185][0m |          -0.0019 |           0.3176 |           2.5202 |
[32m[20221213 22:21:49 @agent_ppo2.py:185][0m |          -0.0113 |           0.3148 |           2.5195 |
[32m[20221213 22:21:49 @agent_ppo2.py:185][0m |          -0.0055 |           0.3118 |           2.5200 |
[32m[20221213 22:21:49 @agent_ppo2.py:185][0m |          -0.0106 |           0.3102 |           2.5192 |
[32m[20221213 22:21:49 @agent_ppo2.py:185][0m |          -0.0111 |           0.3099 |           2.5198 |
[32m[20221213 22:21:49 @agent_ppo2.py:185][0m |          -0.0071 |           0.3091 |           2.5194 |
[32m[20221213 22:21:49 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 1.74
[32m[20221213 22:21:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.70
[32m[20221213 22:21:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.98
[32m[20221213 22:21:49 @agent_ppo2.py:143][0m Total time:       3.60 min
[32m[20221213 22:21:49 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 22:21:49 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 22:21:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0056 |           0.1170 |           2.5997 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0014 |           0.0844 |           2.5992 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0031 |           0.0826 |           2.5946 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0071 |           0.0822 |           2.5936 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0069 |           0.0820 |           2.5920 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0065 |           0.0814 |           2.5893 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0099 |           0.0815 |           2.5902 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0059 |           0.0807 |           2.5888 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0142 |           0.0813 |           2.5867 |
[32m[20221213 22:21:50 @agent_ppo2.py:185][0m |          -0.0111 |           0.0816 |           2.5886 |
[32m[20221213 22:21:50 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:21:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:51 @agent_ppo2.py:143][0m Total time:       3.63 min
[32m[20221213 22:21:51 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221213 22:21:51 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 22:21:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:21:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:51 @agent_ppo2.py:185][0m |          -0.0061 |           0.0706 |           2.5726 |
[32m[20221213 22:21:51 @agent_ppo2.py:185][0m |          -0.0074 |           0.0617 |           2.5678 |
[32m[20221213 22:21:51 @agent_ppo2.py:185][0m |          -0.0069 |           0.0609 |           2.5656 |
[32m[20221213 22:21:51 @agent_ppo2.py:185][0m |          -0.0053 |           0.0604 |           2.5651 |
[32m[20221213 22:21:51 @agent_ppo2.py:185][0m |          -0.0102 |           0.0602 |           2.5652 |
[32m[20221213 22:21:51 @agent_ppo2.py:185][0m |          -0.0020 |           0.0615 |           2.5625 |
[32m[20221213 22:21:52 @agent_ppo2.py:185][0m |          -0.0088 |           0.0596 |           2.5625 |
[32m[20221213 22:21:52 @agent_ppo2.py:185][0m |          -0.0130 |           0.0602 |           2.5622 |
[32m[20221213 22:21:52 @agent_ppo2.py:185][0m |          -0.0125 |           0.0605 |           2.5623 |
[32m[20221213 22:21:52 @agent_ppo2.py:185][0m |          -0.0129 |           0.0589 |           2.5628 |
[32m[20221213 22:21:52 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 15.39
[32m[20221213 22:21:52 @agent_ppo2.py:143][0m Total time:       3.65 min
[32m[20221213 22:21:52 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 22:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 22:21:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:21:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:52 @agent_ppo2.py:185][0m |          -0.0051 |           0.0584 |           2.5927 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0037 |           0.0511 |           2.5927 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0094 |           0.0515 |           2.5921 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0079 |           0.0507 |           2.5908 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0099 |           0.0509 |           2.5905 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0059 |           0.0509 |           2.5904 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0143 |           0.0508 |           2.5898 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0024 |           0.0509 |           2.5898 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0106 |           0.0505 |           2.5895 |
[32m[20221213 22:21:53 @agent_ppo2.py:185][0m |          -0.0106 |           0.0506 |           2.5891 |
[32m[20221213 22:21:53 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:54 @agent_ppo2.py:143][0m Total time:       3.68 min
[32m[20221213 22:21:54 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221213 22:21:54 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 22:21:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:54 @agent_ppo2.py:185][0m |          -0.0010 |           0.1397 |           2.5345 |
[32m[20221213 22:21:54 @agent_ppo2.py:185][0m |          -0.0044 |           0.1268 |           2.5317 |
[32m[20221213 22:21:54 @agent_ppo2.py:185][0m |          -0.0054 |           0.1265 |           2.5326 |
[32m[20221213 22:21:54 @agent_ppo2.py:185][0m |          -0.0020 |           0.1281 |           2.5327 |
[32m[20221213 22:21:54 @agent_ppo2.py:185][0m |          -0.0064 |           0.1260 |           2.5328 |
[32m[20221213 22:21:54 @agent_ppo2.py:185][0m |          -0.0067 |           0.1259 |           2.5335 |
[32m[20221213 22:21:55 @agent_ppo2.py:185][0m |          -0.0082 |           0.1256 |           2.5353 |
[32m[20221213 22:21:55 @agent_ppo2.py:185][0m |          -0.0112 |           0.1256 |           2.5369 |
[32m[20221213 22:21:55 @agent_ppo2.py:185][0m |          -0.0071 |           0.1253 |           2.5371 |
[32m[20221213 22:21:55 @agent_ppo2.py:185][0m |          -0.0091 |           0.1267 |           2.5389 |
[32m[20221213 22:21:55 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:21:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:21:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:55 @agent_ppo2.py:143][0m Total time:       3.70 min
[32m[20221213 22:21:55 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 22:21:55 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 22:21:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:21:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:55 @agent_ppo2.py:185][0m |          -0.0025 |           0.1657 |           2.5865 |
[32m[20221213 22:21:55 @agent_ppo2.py:185][0m |          -0.0054 |           0.1552 |           2.5867 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0065 |           0.1523 |           2.5861 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0074 |           0.1499 |           2.5854 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0072 |           0.1484 |           2.5855 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0053 |           0.1493 |           2.5848 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0099 |           0.1471 |           2.5852 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0088 |           0.1484 |           2.5851 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0071 |           0.1484 |           2.5850 |
[32m[20221213 22:21:56 @agent_ppo2.py:185][0m |          -0.0107 |           0.1469 |           2.5848 |
[32m[20221213 22:21:56 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:21:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.15
[32m[20221213 22:21:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.72
[32m[20221213 22:21:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:56 @agent_ppo2.py:143][0m Total time:       3.72 min
[32m[20221213 22:21:56 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221213 22:21:56 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 22:21:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |          -0.0041 |           0.1471 |           2.6465 |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |          -0.0022 |           0.1359 |           2.6440 |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |          -0.0039 |           0.1349 |           2.6414 |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |          -0.0076 |           0.1350 |           2.6410 |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |           0.0020 |           0.1374 |           2.6418 |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |          -0.0051 |           0.1348 |           2.6398 |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |          -0.0054 |           0.1343 |           2.6424 |
[32m[20221213 22:21:57 @agent_ppo2.py:185][0m |          -0.0066 |           0.1344 |           2.6410 |
[32m[20221213 22:21:58 @agent_ppo2.py:185][0m |          -0.0062 |           0.1346 |           2.6413 |
[32m[20221213 22:21:58 @agent_ppo2.py:185][0m |          -0.0030 |           0.1365 |           2.6409 |
[32m[20221213 22:21:58 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:21:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.53
[32m[20221213 22:21:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.64
[32m[20221213 22:21:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:58 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221213 22:21:58 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 22:21:58 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 22:21:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:21:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:21:58 @agent_ppo2.py:185][0m |           0.0022 |           0.7854 |           2.6914 |
[32m[20221213 22:21:58 @agent_ppo2.py:185][0m |          -0.0003 |           0.6771 |           2.6931 |
[32m[20221213 22:21:58 @agent_ppo2.py:185][0m |          -0.0020 |           0.6600 |           2.6929 |
[32m[20221213 22:21:59 @agent_ppo2.py:185][0m |          -0.0024 |           0.6425 |           2.6943 |
[32m[20221213 22:21:59 @agent_ppo2.py:185][0m |          -0.0045 |           0.6512 |           2.6955 |
[32m[20221213 22:21:59 @agent_ppo2.py:185][0m |          -0.0038 |           0.6367 |           2.6963 |
[32m[20221213 22:21:59 @agent_ppo2.py:185][0m |          -0.0030 |           0.6462 |           2.6965 |
[32m[20221213 22:21:59 @agent_ppo2.py:185][0m |          -0.0040 |           0.6282 |           2.6972 |
[32m[20221213 22:21:59 @agent_ppo2.py:185][0m |          -0.0040 |           0.6110 |           2.6977 |
[32m[20221213 22:21:59 @agent_ppo2.py:185][0m |          -0.0171 |           0.6475 |           2.6987 |
[32m[20221213 22:21:59 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 22:21:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 17.45
[32m[20221213 22:21:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 23.27
[32m[20221213 22:21:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:21:59 @agent_ppo2.py:143][0m Total time:       3.77 min
[32m[20221213 22:21:59 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221213 22:21:59 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 22:22:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:22:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |           0.0024 |           0.5309 |           2.6760 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |           0.0004 |           0.5013 |           2.6740 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |          -0.0020 |           0.4778 |           2.6729 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |          -0.0013 |           0.4748 |           2.6731 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |          -0.0024 |           0.4471 |           2.6736 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |          -0.0046 |           0.4412 |           2.6729 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |          -0.0030 |           0.4230 |           2.6738 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |          -0.0046 |           0.4172 |           2.6744 |
[32m[20221213 22:22:00 @agent_ppo2.py:185][0m |          -0.0065 |           0.4034 |           2.6735 |
[32m[20221213 22:22:01 @agent_ppo2.py:185][0m |          -0.0079 |           0.3938 |           2.6746 |
[32m[20221213 22:22:01 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 22:22:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 17.05
[32m[20221213 22:22:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 19.16
[32m[20221213 22:22:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.10
[32m[20221213 22:22:01 @agent_ppo2.py:143][0m Total time:       3.80 min
[32m[20221213 22:22:01 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 22:22:01 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 22:22:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:01 @agent_ppo2.py:185][0m |          -0.0037 |           0.4747 |           2.6709 |
[32m[20221213 22:22:01 @agent_ppo2.py:185][0m |          -0.0001 |           0.3683 |           2.6681 |
[32m[20221213 22:22:01 @agent_ppo2.py:185][0m |          -0.0049 |           0.3471 |           2.6699 |
[32m[20221213 22:22:01 @agent_ppo2.py:185][0m |           0.0023 |           0.3585 |           2.6684 |
[32m[20221213 22:22:01 @agent_ppo2.py:185][0m |          -0.0073 |           0.3390 |           2.6684 |
[32m[20221213 22:22:01 @agent_ppo2.py:185][0m |          -0.0043 |           0.3335 |           2.6697 |
[32m[20221213 22:22:02 @agent_ppo2.py:185][0m |          -0.0087 |           0.3269 |           2.6690 |
[32m[20221213 22:22:02 @agent_ppo2.py:185][0m |          -0.0042 |           0.3393 |           2.6687 |
[32m[20221213 22:22:02 @agent_ppo2.py:185][0m |          -0.0167 |           0.3270 |           2.6686 |
[32m[20221213 22:22:02 @agent_ppo2.py:185][0m |           0.0014 |           0.3698 |           2.6689 |
[32m[20221213 22:22:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:22:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:02 @agent_ppo2.py:143][0m Total time:       3.82 min
[32m[20221213 22:22:02 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221213 22:22:02 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 22:22:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:02 @agent_ppo2.py:185][0m |           0.0018 |           0.7438 |           2.6436 |
[32m[20221213 22:22:02 @agent_ppo2.py:185][0m |          -0.0014 |           0.5740 |           2.6418 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0128 |           0.5919 |           2.6408 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0024 |           0.5767 |           2.6412 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0104 |           0.5589 |           2.6403 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0142 |           0.5554 |           2.6405 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0054 |           0.5454 |           2.6405 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0182 |           0.5659 |           2.6402 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0045 |           0.5334 |           2.6399 |
[32m[20221213 22:22:03 @agent_ppo2.py:185][0m |          -0.0049 |           0.5298 |           2.6401 |
[32m[20221213 22:22:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 18.53
[32m[20221213 22:22:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 33.52
[32m[20221213 22:22:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:03 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221213 22:22:03 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 22:22:03 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 22:22:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0037 |           0.3875 |           2.6396 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0044 |           0.3105 |           2.6389 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0158 |           0.3364 |           2.6365 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0062 |           0.3100 |           2.6361 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0112 |           0.3024 |           2.6341 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0114 |           0.3005 |           2.6346 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0116 |           0.3013 |           2.6341 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0129 |           0.3011 |           2.6349 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0147 |           0.2966 |           2.6343 |
[32m[20221213 22:22:04 @agent_ppo2.py:185][0m |          -0.0086 |           0.2966 |           2.6326 |
[32m[20221213 22:22:04 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.08
[32m[20221213 22:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.38
[32m[20221213 22:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:05 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221213 22:22:05 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221213 22:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 22:22:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0042 |           0.4741 |           2.6656 |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0112 |           0.4789 |           2.6628 |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0135 |           0.4796 |           2.6601 |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0099 |           0.4427 |           2.6591 |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0076 |           0.4789 |           2.6578 |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0112 |           0.4376 |           2.6568 |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0086 |           0.4332 |           2.6566 |
[32m[20221213 22:22:05 @agent_ppo2.py:185][0m |          -0.0110 |           0.4304 |           2.6571 |
[32m[20221213 22:22:06 @agent_ppo2.py:185][0m |          -0.0090 |           0.4301 |           2.6577 |
[32m[20221213 22:22:06 @agent_ppo2.py:185][0m |          -0.0130 |           0.4281 |           2.6573 |
[32m[20221213 22:22:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:22:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.15
[32m[20221213 22:22:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 10.37
[32m[20221213 22:22:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:06 @agent_ppo2.py:143][0m Total time:       3.88 min
[32m[20221213 22:22:06 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 22:22:06 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 22:22:06 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 22:22:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:06 @agent_ppo2.py:185][0m |           0.0036 |           1.0064 |           2.7118 |
[32m[20221213 22:22:06 @agent_ppo2.py:185][0m |           0.0019 |           0.8966 |           2.7112 |
[32m[20221213 22:22:06 @agent_ppo2.py:185][0m |          -0.0002 |           0.8521 |           2.7105 |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |          -0.0104 |           0.8614 |           2.7099 |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |          -0.0038 |           0.8332 |           2.7098 |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |          -0.0067 |           0.8214 |           2.7102 |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |          -0.0024 |           0.8040 |           2.7100 |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |          -0.0057 |           0.7918 |           2.7099 |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |          -0.0120 |           0.7984 |           2.7099 |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |          -0.0047 |           0.7665 |           2.7106 |
[32m[20221213 22:22:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 22.14
[32m[20221213 22:22:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 24.09
[32m[20221213 22:22:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:07 @agent_ppo2.py:143][0m Total time:       3.90 min
[32m[20221213 22:22:07 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221213 22:22:07 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 22:22:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:07 @agent_ppo2.py:185][0m |           0.0025 |           0.1827 |           2.7471 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0012 |           0.1194 |           2.7450 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0073 |           0.1095 |           2.7413 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0041 |           0.1048 |           2.7410 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0024 |           0.1024 |           2.7395 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0052 |           0.1013 |           2.7389 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0078 |           0.0991 |           2.7386 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0104 |           0.0977 |           2.7396 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0089 |           0.0979 |           2.7378 |
[32m[20221213 22:22:08 @agent_ppo2.py:185][0m |          -0.0085 |           0.0964 |           2.7370 |
[32m[20221213 22:22:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:22:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 3.36
[32m[20221213 22:22:08 @agent_ppo2.py:143][0m Total time:       3.92 min
[32m[20221213 22:22:08 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 22:22:08 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 22:22:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |          -0.0061 |           0.2368 |           2.7230 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |           0.0014 |           0.2168 |           2.7210 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |           0.0001 |           0.2160 |           2.7193 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |          -0.0103 |           0.2146 |           2.7183 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |          -0.0058 |           0.2100 |           2.7172 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |          -0.0063 |           0.2072 |           2.7169 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |          -0.0037 |           0.2069 |           2.7163 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |          -0.0063 |           0.2063 |           2.7151 |
[32m[20221213 22:22:09 @agent_ppo2.py:185][0m |          -0.0067 |           0.2055 |           2.7151 |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |          -0.0074 |           0.2048 |           2.7151 |
[32m[20221213 22:22:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:22:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.46
[32m[20221213 22:22:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 7.10
[32m[20221213 22:22:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 38.07
[32m[20221213 22:22:10 @agent_ppo2.py:143][0m Total time:       3.95 min
[32m[20221213 22:22:10 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221213 22:22:10 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 22:22:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |          -0.0010 |           0.2374 |           2.7037 |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |          -0.0046 |           0.2020 |           2.7019 |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |          -0.0055 |           0.2009 |           2.7005 |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |          -0.0101 |           0.2000 |           2.6991 |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |          -0.0071 |           0.1991 |           2.7013 |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |          -0.0042 |           0.2014 |           2.7005 |
[32m[20221213 22:22:10 @agent_ppo2.py:185][0m |           0.0035 |           0.2142 |           2.7003 |
[32m[20221213 22:22:11 @agent_ppo2.py:185][0m |          -0.0103 |           0.1976 |           2.7003 |
[32m[20221213 22:22:11 @agent_ppo2.py:185][0m |          -0.0118 |           0.1974 |           2.7008 |
[32m[20221213 22:22:11 @agent_ppo2.py:185][0m |          -0.0071 |           0.1955 |           2.7037 |
[32m[20221213 22:22:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:22:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:11 @agent_ppo2.py:143][0m Total time:       3.97 min
[32m[20221213 22:22:11 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 22:22:11 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 22:22:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:11 @agent_ppo2.py:185][0m |           0.0038 |           0.9536 |           2.7533 |
[32m[20221213 22:22:11 @agent_ppo2.py:185][0m |          -0.0074 |           0.8041 |           2.7495 |
[32m[20221213 22:22:11 @agent_ppo2.py:185][0m |          -0.0027 |           0.7870 |           2.7460 |
[32m[20221213 22:22:11 @agent_ppo2.py:185][0m |          -0.0034 |           0.7767 |           2.7424 |
[32m[20221213 22:22:12 @agent_ppo2.py:185][0m |          -0.0045 |           0.7799 |           2.7420 |
[32m[20221213 22:22:12 @agent_ppo2.py:185][0m |          -0.0060 |           0.7855 |           2.7409 |
[32m[20221213 22:22:12 @agent_ppo2.py:185][0m |          -0.0055 |           0.7697 |           2.7424 |
[32m[20221213 22:22:12 @agent_ppo2.py:185][0m |          -0.0054 |           0.7617 |           2.7407 |
[32m[20221213 22:22:12 @agent_ppo2.py:185][0m |          -0.0068 |           0.7613 |           2.7423 |
[32m[20221213 22:22:12 @agent_ppo2.py:185][0m |          -0.0080 |           0.7858 |           2.7424 |
[32m[20221213 22:22:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 12.36
[32m[20221213 22:22:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 23.61
[32m[20221213 22:22:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.79
[32m[20221213 22:22:12 @agent_ppo2.py:143][0m Total time:       3.99 min
[32m[20221213 22:22:12 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221213 22:22:12 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 22:22:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:12 @agent_ppo2.py:185][0m |          -0.0035 |           0.2941 |           2.7356 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0026 |           0.2394 |           2.7334 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0036 |           0.2239 |           2.7302 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0076 |           0.2201 |           2.7286 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0069 |           0.2220 |           2.7271 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0066 |           0.2187 |           2.7260 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0039 |           0.2226 |           2.7247 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0114 |           0.2162 |           2.7242 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0084 |           0.2156 |           2.7227 |
[32m[20221213 22:22:13 @agent_ppo2.py:185][0m |          -0.0128 |           0.2160 |           2.7204 |
[32m[20221213 22:22:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:22:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:13 @agent_ppo2.py:143][0m Total time:       4.01 min
[32m[20221213 22:22:13 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 22:22:13 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 22:22:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0033 |           0.3211 |           2.7006 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0017 |           0.2982 |           2.7003 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0041 |           0.2972 |           2.6996 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0007 |           0.2963 |           2.6972 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0011 |           0.2964 |           2.6977 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0043 |           0.2921 |           2.6977 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0060 |           0.2928 |           2.6982 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0070 |           0.2913 |           2.6982 |
[32m[20221213 22:22:14 @agent_ppo2.py:185][0m |          -0.0100 |           0.2900 |           2.6982 |
[32m[20221213 22:22:15 @agent_ppo2.py:185][0m |          -0.0069 |           0.2888 |           2.6975 |
[32m[20221213 22:22:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:22:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 2.28
[32m[20221213 22:22:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.42
[32m[20221213 22:22:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 16.50
[32m[20221213 22:22:15 @agent_ppo2.py:143][0m Total time:       4.03 min
[32m[20221213 22:22:15 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221213 22:22:15 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 22:22:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:15 @agent_ppo2.py:185][0m |           0.0028 |           0.3512 |           2.7244 |
[32m[20221213 22:22:15 @agent_ppo2.py:185][0m |          -0.0029 |           0.3390 |           2.7222 |
[32m[20221213 22:22:15 @agent_ppo2.py:185][0m |          -0.0022 |           0.3379 |           2.7205 |
[32m[20221213 22:22:15 @agent_ppo2.py:185][0m |          -0.0029 |           0.3363 |           2.7193 |
[32m[20221213 22:22:15 @agent_ppo2.py:185][0m |          -0.0015 |           0.3384 |           2.7186 |
[32m[20221213 22:22:15 @agent_ppo2.py:185][0m |          -0.0155 |           0.3557 |           2.7170 |
[32m[20221213 22:22:16 @agent_ppo2.py:185][0m |          -0.0024 |           0.3438 |           2.7177 |
[32m[20221213 22:22:16 @agent_ppo2.py:185][0m |          -0.0113 |           0.3294 |           2.7172 |
[32m[20221213 22:22:16 @agent_ppo2.py:185][0m |          -0.0104 |           0.3314 |           2.7182 |
[32m[20221213 22:22:16 @agent_ppo2.py:185][0m |          -0.0053 |           0.3249 |           2.7171 |
[32m[20221213 22:22:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:22:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.12
[32m[20221213 22:22:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 12.41
[32m[20221213 22:22:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:16 @agent_ppo2.py:143][0m Total time:       4.05 min
[32m[20221213 22:22:16 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 22:22:16 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 22:22:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:22:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:16 @agent_ppo2.py:185][0m |           0.0029 |           2.3513 |           2.6999 |
[32m[20221213 22:22:16 @agent_ppo2.py:185][0m |           0.0012 |           1.7390 |           2.7007 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0007 |           1.5552 |           2.7000 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0080 |           1.3934 |           2.7003 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0028 |           1.2457 |           2.7002 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0024 |           1.1231 |           2.7009 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0032 |           1.0305 |           2.7014 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0044 |           0.9429 |           2.7010 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0043 |           0.8816 |           2.7015 |
[32m[20221213 22:22:17 @agent_ppo2.py:185][0m |          -0.0130 |           0.8398 |           2.7024 |
[32m[20221213 22:22:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:22:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 35.81
[32m[20221213 22:22:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.61
[32m[20221213 22:22:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:17 @agent_ppo2.py:143][0m Total time:       4.07 min
[32m[20221213 22:22:17 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221213 22:22:17 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 22:22:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0017 |           0.3862 |           2.6949 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0079 |           0.2492 |           2.6939 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0094 |           0.2233 |           2.6912 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0057 |           0.2074 |           2.6899 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0046 |           0.1920 |           2.6893 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0067 |           0.1829 |           2.6893 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0108 |           0.1752 |           2.6874 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0078 |           0.1699 |           2.6871 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0059 |           0.1659 |           2.6865 |
[32m[20221213 22:22:18 @agent_ppo2.py:185][0m |          -0.0075 |           0.1635 |           2.6861 |
[32m[20221213 22:22:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:22:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.51
[32m[20221213 22:22:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 2.57
[32m[20221213 22:22:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 23.96
[32m[20221213 22:22:19 @agent_ppo2.py:143][0m Total time:       4.09 min
[32m[20221213 22:22:19 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 22:22:19 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 22:22:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:22:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:19 @agent_ppo2.py:185][0m |          -0.0003 |           0.5422 |           2.7861 |
[32m[20221213 22:22:19 @agent_ppo2.py:185][0m |          -0.0063 |           0.5151 |           2.7838 |
[32m[20221213 22:22:19 @agent_ppo2.py:185][0m |          -0.0054 |           0.5017 |           2.7809 |
[32m[20221213 22:22:19 @agent_ppo2.py:185][0m |          -0.0098 |           0.5009 |           2.7811 |
[32m[20221213 22:22:19 @agent_ppo2.py:185][0m |          -0.0080 |           0.5007 |           2.7809 |
[32m[20221213 22:22:19 @agent_ppo2.py:185][0m |          -0.0008 |           0.4962 |           2.7827 |
[32m[20221213 22:22:19 @agent_ppo2.py:185][0m |          -0.0040 |           0.4952 |           2.7828 |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0054 |           0.4901 |           2.7834 |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0104 |           0.4829 |           2.7845 |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0098 |           0.4831 |           2.7839 |
[32m[20221213 22:22:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:22:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.41
[32m[20221213 22:22:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 15.00
[32m[20221213 22:22:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 27.23
[32m[20221213 22:22:20 @agent_ppo2.py:143][0m Total time:       4.12 min
[32m[20221213 22:22:20 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221213 22:22:20 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 22:22:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0072 |           1.0143 |           2.6949 |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0071 |           0.8573 |           2.6955 |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0031 |           0.8419 |           2.6949 |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0043 |           0.8296 |           2.6965 |
[32m[20221213 22:22:20 @agent_ppo2.py:185][0m |          -0.0099 |           0.8159 |           2.6976 |
[32m[20221213 22:22:21 @agent_ppo2.py:185][0m |          -0.0067 |           0.8093 |           2.6971 |
[32m[20221213 22:22:21 @agent_ppo2.py:185][0m |          -0.0055 |           0.8059 |           2.6967 |
[32m[20221213 22:22:21 @agent_ppo2.py:185][0m |          -0.0098 |           0.8027 |           2.6975 |
[32m[20221213 22:22:21 @agent_ppo2.py:185][0m |          -0.0070 |           0.7904 |           2.6983 |
[32m[20221213 22:22:21 @agent_ppo2.py:185][0m |           0.0021 |           0.8469 |           2.6985 |
[32m[20221213 22:22:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:22:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.36
[32m[20221213 22:22:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 8.95
[32m[20221213 22:22:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:21 @agent_ppo2.py:143][0m Total time:       4.14 min
[32m[20221213 22:22:21 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 22:22:21 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 22:22:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:21 @agent_ppo2.py:185][0m |           0.0033 |           0.9210 |           2.7148 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0039 |           0.8591 |           2.7150 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0031 |           0.8386 |           2.7140 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0036 |           0.8249 |           2.7121 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0084 |           0.8124 |           2.7099 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0051 |           0.8204 |           2.7081 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0071 |           0.7943 |           2.7081 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0094 |           0.7776 |           2.7073 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0035 |           0.8034 |           2.7063 |
[32m[20221213 22:22:22 @agent_ppo2.py:185][0m |          -0.0070 |           0.7579 |           2.7047 |
[32m[20221213 22:22:22 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:22:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.30
[32m[20221213 22:22:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 27.81
[32m[20221213 22:22:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.56
[32m[20221213 22:22:22 @agent_ppo2.py:143][0m Total time:       4.16 min
[32m[20221213 22:22:22 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221213 22:22:22 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 22:22:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |          -0.0023 |           0.4527 |           2.7859 |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |           0.0020 |           0.4075 |           2.7854 |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |          -0.0034 |           0.4004 |           2.7831 |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |          -0.0029 |           0.3968 |           2.7829 |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |          -0.0021 |           0.3966 |           2.7806 |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |          -0.0063 |           0.3931 |           2.7813 |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |          -0.0061 |           0.3933 |           2.7809 |
[32m[20221213 22:22:23 @agent_ppo2.py:185][0m |          -0.0000 |           0.3885 |           2.7809 |
[32m[20221213 22:22:24 @agent_ppo2.py:185][0m |          -0.0073 |           0.3865 |           2.7805 |
[32m[20221213 22:22:24 @agent_ppo2.py:185][0m |          -0.0124 |           0.3851 |           2.7815 |
[32m[20221213 22:22:24 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:22:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.17
[32m[20221213 22:22:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.87
[32m[20221213 22:22:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 20.60
[32m[20221213 22:22:24 @agent_ppo2.py:143][0m Total time:       4.18 min
[32m[20221213 22:22:24 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 22:22:24 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 22:22:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:24 @agent_ppo2.py:185][0m |           0.0013 |           1.0230 |           2.7426 |
[32m[20221213 22:22:24 @agent_ppo2.py:185][0m |           0.0025 |           0.8865 |           2.7411 |
[32m[20221213 22:22:24 @agent_ppo2.py:185][0m |          -0.0035 |           0.8483 |           2.7400 |
[32m[20221213 22:22:24 @agent_ppo2.py:185][0m |          -0.0045 |           0.8199 |           2.7388 |
[32m[20221213 22:22:24 @agent_ppo2.py:185][0m |          -0.0041 |           0.7906 |           2.7374 |
[32m[20221213 22:22:25 @agent_ppo2.py:185][0m |          -0.0179 |           0.8531 |           2.7362 |
[32m[20221213 22:22:25 @agent_ppo2.py:185][0m |          -0.0078 |           0.7517 |           2.7354 |
[32m[20221213 22:22:25 @agent_ppo2.py:185][0m |          -0.0075 |           0.7249 |           2.7349 |
[32m[20221213 22:22:25 @agent_ppo2.py:185][0m |          -0.0114 |           0.7044 |           2.7347 |
[32m[20221213 22:22:25 @agent_ppo2.py:185][0m |          -0.0187 |           0.7020 |           2.7339 |
[32m[20221213 22:22:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 12.12
[32m[20221213 22:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 27.95
[32m[20221213 22:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:25 @agent_ppo2.py:143][0m Total time:       4.20 min
[32m[20221213 22:22:25 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221213 22:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 22:22:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:25 @agent_ppo2.py:185][0m |          -0.0099 |           1.2164 |           2.7986 |
[32m[20221213 22:22:25 @agent_ppo2.py:185][0m |           0.0008 |           0.9630 |           2.7999 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0046 |           0.8317 |           2.8006 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0037 |           0.7678 |           2.8028 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0033 |           0.7024 |           2.8046 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0159 |           0.6704 |           2.8063 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0030 |           0.6414 |           2.8087 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0091 |           0.6215 |           2.8112 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0097 |           0.6174 |           2.8124 |
[32m[20221213 22:22:26 @agent_ppo2.py:185][0m |          -0.0071 |           0.5962 |           2.8153 |
[32m[20221213 22:22:26 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:22:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 26.65
[32m[20221213 22:22:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.05
[32m[20221213 22:22:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:26 @agent_ppo2.py:143][0m Total time:       4.23 min
[32m[20221213 22:22:26 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 22:22:26 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 22:22:27 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 22:22:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |           0.0006 |           0.5906 |           2.7961 |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |          -0.0037 |           0.3651 |           2.7933 |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |          -0.0040 |           0.3366 |           2.7928 |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |          -0.0086 |           0.3238 |           2.7926 |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |          -0.0067 |           0.3163 |           2.7919 |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |          -0.0016 |           0.3224 |           2.7913 |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |          -0.0011 |           0.3016 |           2.7918 |
[32m[20221213 22:22:27 @agent_ppo2.py:185][0m |          -0.0079 |           0.2962 |           2.7902 |
[32m[20221213 22:22:28 @agent_ppo2.py:185][0m |          -0.0078 |           0.2912 |           2.7886 |
[32m[20221213 22:22:28 @agent_ppo2.py:185][0m |          -0.0034 |           0.2907 |           2.7872 |
[32m[20221213 22:22:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:22:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:28 @agent_ppo2.py:143][0m Total time:       4.25 min
[32m[20221213 22:22:28 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221213 22:22:28 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 22:22:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:28 @agent_ppo2.py:185][0m |          -0.0030 |           0.8700 |           2.8385 |
[32m[20221213 22:22:28 @agent_ppo2.py:185][0m |           0.0010 |           0.7755 |           2.8374 |
[32m[20221213 22:22:28 @agent_ppo2.py:185][0m |          -0.0038 |           0.7469 |           2.8357 |
[32m[20221213 22:22:28 @agent_ppo2.py:185][0m |          -0.0019 |           0.7308 |           2.8352 |
[32m[20221213 22:22:29 @agent_ppo2.py:185][0m |          -0.0110 |           0.7217 |           2.8336 |
[32m[20221213 22:22:29 @agent_ppo2.py:185][0m |          -0.0088 |           0.7012 |           2.8328 |
[32m[20221213 22:22:29 @agent_ppo2.py:185][0m |          -0.0049 |           0.6936 |           2.8320 |
[32m[20221213 22:22:29 @agent_ppo2.py:185][0m |          -0.0091 |           0.6868 |           2.8319 |
[32m[20221213 22:22:29 @agent_ppo2.py:185][0m |          -0.0045 |           0.6824 |           2.8316 |
[32m[20221213 22:22:29 @agent_ppo2.py:185][0m |          -0.0074 |           0.6791 |           2.8315 |
[32m[20221213 22:22:29 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 11.26
[32m[20221213 22:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 22.38
[32m[20221213 22:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.22
[32m[20221213 22:22:29 @agent_ppo2.py:143][0m Total time:       4.27 min
[32m[20221213 22:22:29 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 22:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 22:22:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |           0.0038 |           0.8731 |           2.7916 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0030 |           0.8504 |           2.7892 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0075 |           0.8361 |           2.7885 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0044 |           0.8198 |           2.7889 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0065 |           0.8040 |           2.7884 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0052 |           0.7896 |           2.7887 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0083 |           0.7816 |           2.7895 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0094 |           0.7663 |           2.7894 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0060 |           0.7617 |           2.7914 |
[32m[20221213 22:22:30 @agent_ppo2.py:185][0m |          -0.0111 |           0.7673 |           2.7904 |
[32m[20221213 22:22:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:22:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 11.49
[32m[20221213 22:22:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 34.38
[32m[20221213 22:22:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:31 @agent_ppo2.py:143][0m Total time:       4.29 min
[32m[20221213 22:22:31 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221213 22:22:31 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 22:22:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0015 |           1.6752 |           2.7952 |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0021 |           1.4442 |           2.7911 |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0079 |           1.3743 |           2.7907 |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0132 |           1.3328 |           2.7903 |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0108 |           1.3046 |           2.7908 |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0159 |           1.2704 |           2.7923 |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0108 |           1.2439 |           2.7919 |
[32m[20221213 22:22:31 @agent_ppo2.py:185][0m |          -0.0087 |           1.2598 |           2.7936 |
[32m[20221213 22:22:32 @agent_ppo2.py:185][0m |          -0.0127 |           1.2337 |           2.7943 |
[32m[20221213 22:22:32 @agent_ppo2.py:185][0m |          -0.0244 |           1.2583 |           2.7948 |
[32m[20221213 22:22:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:22:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 12.00
[32m[20221213 22:22:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 32.99
[32m[20221213 22:22:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 26.54
[32m[20221213 22:22:32 @agent_ppo2.py:143][0m Total time:       4.31 min
[32m[20221213 22:22:32 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 22:22:32 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 22:22:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:22:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:32 @agent_ppo2.py:185][0m |           0.0037 |           1.3396 |           2.8087 |
[32m[20221213 22:22:32 @agent_ppo2.py:185][0m |          -0.0112 |           1.1192 |           2.8045 |
[32m[20221213 22:22:32 @agent_ppo2.py:185][0m |          -0.0006 |           1.0420 |           2.8025 |
[32m[20221213 22:22:32 @agent_ppo2.py:185][0m |          -0.0075 |           1.0114 |           2.8035 |
[32m[20221213 22:22:32 @agent_ppo2.py:185][0m |          -0.0063 |           0.9760 |           2.8027 |
[32m[20221213 22:22:33 @agent_ppo2.py:185][0m |          -0.0109 |           0.9503 |           2.8035 |
[32m[20221213 22:22:33 @agent_ppo2.py:185][0m |          -0.0135 |           0.9536 |           2.8025 |
[32m[20221213 22:22:33 @agent_ppo2.py:185][0m |          -0.0045 |           0.9167 |           2.8033 |
[32m[20221213 22:22:33 @agent_ppo2.py:185][0m |          -0.0202 |           0.9452 |           2.8035 |
[32m[20221213 22:22:33 @agent_ppo2.py:185][0m |          -0.0125 |           0.9033 |           2.8041 |
[32m[20221213 22:22:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:22:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 16.59
[32m[20221213 22:22:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 29.99
[32m[20221213 22:22:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:33 @agent_ppo2.py:143][0m Total time:       4.34 min
[32m[20221213 22:22:33 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221213 22:22:33 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 22:22:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:33 @agent_ppo2.py:185][0m |           0.0019 |           1.1035 |           2.8002 |
[32m[20221213 22:22:33 @agent_ppo2.py:185][0m |           0.0017 |           0.9459 |           2.7972 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0100 |           0.9200 |           2.7956 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0112 |           0.9074 |           2.7955 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0104 |           0.8945 |           2.7952 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0113 |           0.8854 |           2.7967 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0171 |           0.9436 |           2.7965 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0085 |           0.8903 |           2.7962 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0154 |           0.8616 |           2.7979 |
[32m[20221213 22:22:34 @agent_ppo2.py:185][0m |          -0.0072 |           0.8568 |           2.7976 |
[32m[20221213 22:22:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:22:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 3.74
[32m[20221213 22:22:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 11.82
[32m[20221213 22:22:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:34 @agent_ppo2.py:143][0m Total time:       4.36 min
[32m[20221213 22:22:34 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 22:22:34 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 22:22:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |           0.0026 |           1.0080 |           2.8590 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0044 |           0.9102 |           2.8565 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0031 |           0.8954 |           2.8541 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0042 |           0.8846 |           2.8526 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0100 |           0.8719 |           2.8511 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0080 |           0.8564 |           2.8508 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0112 |           0.8519 |           2.8490 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0090 |           0.8551 |           2.8483 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0088 |           0.8408 |           2.8476 |
[32m[20221213 22:22:35 @agent_ppo2.py:185][0m |          -0.0126 |           0.8480 |           2.8465 |
[32m[20221213 22:22:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:22:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.72
[32m[20221213 22:22:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 14.86
[32m[20221213 22:22:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:35 @agent_ppo2.py:143][0m Total time:       4.37 min
[32m[20221213 22:22:35 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221213 22:22:35 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 22:22:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |           0.0033 |           0.2991 |           2.8616 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |           0.0027 |           0.2386 |           2.8572 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0053 |           0.2208 |           2.8561 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0072 |           0.2108 |           2.8546 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0042 |           0.2035 |           2.8542 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0079 |           0.1982 |           2.8541 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0139 |           0.1939 |           2.8544 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0061 |           0.1904 |           2.8533 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0131 |           0.1879 |           2.8534 |
[32m[20221213 22:22:36 @agent_ppo2.py:185][0m |          -0.0088 |           0.1847 |           2.8528 |
[32m[20221213 22:22:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.36
[32m[20221213 22:22:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 63.36
[32m[20221213 22:22:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 63.36
[32m[20221213 22:22:37 @agent_ppo2.py:143][0m Total time:       4.40 min
[32m[20221213 22:22:37 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 22:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 22:22:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:37 @agent_ppo2.py:185][0m |           0.0009 |           1.8794 |           2.8234 |
[32m[20221213 22:22:37 @agent_ppo2.py:185][0m |          -0.0085 |           1.5176 |           2.8250 |
[32m[20221213 22:22:37 @agent_ppo2.py:185][0m |          -0.0009 |           1.3701 |           2.8268 |
[32m[20221213 22:22:37 @agent_ppo2.py:185][0m |          -0.0015 |           1.2744 |           2.8278 |
[32m[20221213 22:22:37 @agent_ppo2.py:185][0m |          -0.0026 |           1.1372 |           2.8289 |
[32m[20221213 22:22:37 @agent_ppo2.py:185][0m |          -0.0054 |           0.9977 |           2.8296 |
[32m[20221213 22:22:37 @agent_ppo2.py:185][0m |          -0.0042 |           0.8381 |           2.8296 |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0164 |           0.6902 |           2.8307 |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0038 |           0.5712 |           2.8320 |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0048 |           0.4592 |           2.8331 |
[32m[20221213 22:22:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:22:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 32.80
[32m[20221213 22:22:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 37.36
[32m[20221213 22:22:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 37.48
[32m[20221213 22:22:38 @agent_ppo2.py:143][0m Total time:       4.42 min
[32m[20221213 22:22:38 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221213 22:22:38 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 22:22:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0057 |           2.0008 |           2.8678 |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0147 |           1.6440 |           2.8631 |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0041 |           1.4810 |           2.8614 |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0048 |           1.4221 |           2.8616 |
[32m[20221213 22:22:38 @agent_ppo2.py:185][0m |          -0.0132 |           1.4409 |           2.8614 |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0080 |           1.3648 |           2.8612 |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0051 |           1.3357 |           2.8614 |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0029 |           1.3209 |           2.8615 |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0041 |           1.3093 |           2.8620 |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0023 |           1.3111 |           2.8625 |
[32m[20221213 22:22:39 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:22:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 10.52
[32m[20221213 22:22:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 30.67
[32m[20221213 22:22:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:39 @agent_ppo2.py:143][0m Total time:       4.44 min
[32m[20221213 22:22:39 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 22:22:39 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 22:22:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0011 |           0.7646 |           2.9022 |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0037 |           0.5027 |           2.9003 |
[32m[20221213 22:22:39 @agent_ppo2.py:185][0m |          -0.0082 |           0.4512 |           2.8978 |
[32m[20221213 22:22:40 @agent_ppo2.py:185][0m |          -0.0023 |           0.4256 |           2.8973 |
[32m[20221213 22:22:40 @agent_ppo2.py:185][0m |          -0.0134 |           0.4017 |           2.8969 |
[32m[20221213 22:22:40 @agent_ppo2.py:185][0m |          -0.0123 |           0.3912 |           2.8952 |
[32m[20221213 22:22:40 @agent_ppo2.py:185][0m |          -0.0013 |           0.3802 |           2.8945 |
[32m[20221213 22:22:40 @agent_ppo2.py:185][0m |          -0.0036 |           0.3763 |           2.8944 |
[32m[20221213 22:22:40 @agent_ppo2.py:185][0m |          -0.0105 |           0.3716 |           2.8947 |
[32m[20221213 22:22:40 @agent_ppo2.py:185][0m |          -0.0057 |           0.3792 |           2.8936 |
[32m[20221213 22:22:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:22:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.17
[32m[20221213 22:22:40 @agent_ppo2.py:143][0m Total time:       4.46 min
[32m[20221213 22:22:40 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221213 22:22:40 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 22:22:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0021 |           0.3283 |           2.8812 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0073 |           0.2765 |           2.8818 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |           0.0021 |           0.2676 |           2.8818 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0104 |           0.2525 |           2.8828 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0021 |           0.2474 |           2.8838 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0117 |           0.2408 |           2.8844 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0080 |           0.2370 |           2.8861 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0101 |           0.2330 |           2.8868 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0034 |           0.2343 |           2.8876 |
[32m[20221213 22:22:41 @agent_ppo2.py:185][0m |          -0.0030 |           0.2461 |           2.8880 |
[32m[20221213 22:22:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:22:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:41 @agent_ppo2.py:143][0m Total time:       4.48 min
[32m[20221213 22:22:41 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 22:22:41 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 22:22:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0042 |           3.7213 |           2.8844 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0008 |           2.5854 |           2.8849 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0013 |           2.0916 |           2.8815 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0095 |           1.9047 |           2.8788 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0085 |           1.7060 |           2.8780 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0036 |           1.5962 |           2.8790 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0127 |           1.5163 |           2.8795 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0033 |           1.4452 |           2.8780 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0048 |           1.3921 |           2.8768 |
[32m[20221213 22:22:42 @agent_ppo2.py:185][0m |          -0.0128 |           1.3576 |           2.8779 |
[32m[20221213 22:22:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.92
[32m[20221213 22:22:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 75.31
[32m[20221213 22:22:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.74
[32m[20221213 22:22:43 @agent_ppo2.py:143][0m Total time:       4.50 min
[32m[20221213 22:22:43 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221213 22:22:43 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 22:22:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:43 @agent_ppo2.py:185][0m |           0.0003 |           1.6322 |           2.9525 |
[32m[20221213 22:22:43 @agent_ppo2.py:185][0m |          -0.0002 |           1.3055 |           2.9502 |
[32m[20221213 22:22:43 @agent_ppo2.py:185][0m |          -0.0071 |           1.2316 |           2.9488 |
[32m[20221213 22:22:43 @agent_ppo2.py:185][0m |          -0.0124 |           1.2166 |           2.9475 |
[32m[20221213 22:22:43 @agent_ppo2.py:185][0m |          -0.0036 |           1.1869 |           2.9469 |
[32m[20221213 22:22:43 @agent_ppo2.py:185][0m |          -0.0116 |           1.1331 |           2.9466 |
[32m[20221213 22:22:43 @agent_ppo2.py:185][0m |          -0.0133 |           1.0931 |           2.9456 |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |          -0.0059 |           1.0761 |           2.9461 |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |          -0.0105 |           1.0507 |           2.9462 |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |          -0.0118 |           1.0473 |           2.9447 |
[32m[20221213 22:22:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:22:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.87
[32m[20221213 22:22:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 29.37
[32m[20221213 22:22:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.70
[32m[20221213 22:22:44 @agent_ppo2.py:143][0m Total time:       4.52 min
[32m[20221213 22:22:44 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 22:22:44 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 22:22:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:22:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |           0.0015 |           0.3998 |           2.9116 |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |          -0.0024 |           0.3123 |           2.9079 |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |          -0.0039 |           0.2866 |           2.9045 |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |          -0.0087 |           0.2649 |           2.9042 |
[32m[20221213 22:22:44 @agent_ppo2.py:185][0m |          -0.0044 |           0.2553 |           2.9037 |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |          -0.0037 |           0.2434 |           2.9010 |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |          -0.0044 |           0.2358 |           2.9015 |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |           0.0010 |           0.2529 |           2.8994 |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |          -0.0118 |           0.2306 |           2.8991 |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |          -0.0126 |           0.2207 |           2.8993 |
[32m[20221213 22:22:45 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:22:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:22:45 @agent_ppo2.py:143][0m Total time:       4.54 min
[32m[20221213 22:22:45 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221213 22:22:45 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 22:22:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |          -0.0046 |           2.2194 |           2.9168 |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |           0.0002 |           2.0084 |           2.9140 |
[32m[20221213 22:22:45 @agent_ppo2.py:185][0m |           0.0002 |           1.9133 |           2.9144 |
[32m[20221213 22:22:46 @agent_ppo2.py:185][0m |          -0.0067 |           1.8653 |           2.9140 |
[32m[20221213 22:22:46 @agent_ppo2.py:185][0m |          -0.0113 |           1.8357 |           2.9160 |
[32m[20221213 22:22:46 @agent_ppo2.py:185][0m |          -0.0021 |           1.7841 |           2.9164 |
[32m[20221213 22:22:46 @agent_ppo2.py:185][0m |          -0.0068 |           1.7599 |           2.9180 |
[32m[20221213 22:22:46 @agent_ppo2.py:185][0m |          -0.0111 |           1.7879 |           2.9181 |
[32m[20221213 22:22:46 @agent_ppo2.py:185][0m |          -0.0118 |           1.7348 |           2.9186 |
[32m[20221213 22:22:46 @agent_ppo2.py:185][0m |          -0.0064 |           1.7162 |           2.9206 |
[32m[20221213 22:22:46 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:22:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 26.84
[32m[20221213 22:22:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.46
[32m[20221213 22:22:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 109.73
[32m[20221213 22:22:46 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 109.73
[32m[20221213 22:22:46 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 109.73
[32m[20221213 22:22:46 @agent_ppo2.py:143][0m Total time:       4.56 min
[32m[20221213 22:22:46 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 22:22:46 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 22:22:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |           0.0066 |           1.1738 |           2.9011 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0008 |           0.9014 |           2.9029 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0026 |           0.7947 |           2.9033 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0044 |           0.7325 |           2.9037 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0058 |           0.6473 |           2.9051 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0073 |           0.6065 |           2.9058 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0028 |           0.5596 |           2.9063 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0003 |           0.5303 |           2.9079 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0074 |           0.5085 |           2.9082 |
[32m[20221213 22:22:47 @agent_ppo2.py:185][0m |          -0.0046 |           0.4869 |           2.9094 |
[32m[20221213 22:22:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:22:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 15.04
[32m[20221213 22:22:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 20.54
[32m[20221213 22:22:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 105.66
[32m[20221213 22:22:47 @agent_ppo2.py:143][0m Total time:       4.57 min
[32m[20221213 22:22:47 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221213 22:22:47 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 22:22:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0036 |           0.3008 |           2.9764 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0065 |           0.2251 |           2.9735 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0074 |           0.1981 |           2.9724 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0074 |           0.1845 |           2.9729 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0090 |           0.1724 |           2.9724 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0110 |           0.1657 |           2.9728 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0086 |           0.1588 |           2.9724 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0093 |           0.1556 |           2.9720 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0114 |           0.1506 |           2.9712 |
[32m[20221213 22:22:48 @agent_ppo2.py:185][0m |          -0.0110 |           0.1493 |           2.9706 |
[32m[20221213 22:22:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 101.38
[32m[20221213 22:22:49 @agent_ppo2.py:143][0m Total time:       4.60 min
[32m[20221213 22:22:49 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 22:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 22:22:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:49 @agent_ppo2.py:185][0m |           0.0004 |           1.2913 |           2.9687 |
[32m[20221213 22:22:49 @agent_ppo2.py:185][0m |          -0.0021 |           0.9869 |           2.9663 |
[32m[20221213 22:22:49 @agent_ppo2.py:185][0m |          -0.0045 |           0.9082 |           2.9631 |
[32m[20221213 22:22:49 @agent_ppo2.py:185][0m |          -0.0122 |           0.8807 |           2.9612 |
[32m[20221213 22:22:49 @agent_ppo2.py:185][0m |          -0.0081 |           0.8363 |           2.9597 |
[32m[20221213 22:22:49 @agent_ppo2.py:185][0m |          -0.0122 |           0.8291 |           2.9591 |
[32m[20221213 22:22:49 @agent_ppo2.py:185][0m |          -0.0035 |           0.8043 |           2.9589 |
[32m[20221213 22:22:50 @agent_ppo2.py:185][0m |          -0.0077 |           0.7961 |           2.9574 |
[32m[20221213 22:22:50 @agent_ppo2.py:185][0m |          -0.0084 |           0.7818 |           2.9572 |
[32m[20221213 22:22:50 @agent_ppo2.py:185][0m |          -0.0109 |           0.7771 |           2.9569 |
[32m[20221213 22:22:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:22:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 34.97
[32m[20221213 22:22:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 41.93
[32m[20221213 22:22:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 21.05
[32m[20221213 22:22:50 @agent_ppo2.py:143][0m Total time:       4.62 min
[32m[20221213 22:22:50 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221213 22:22:50 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 22:22:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:50 @agent_ppo2.py:185][0m |           0.0011 |           2.6657 |           2.9476 |
[32m[20221213 22:22:50 @agent_ppo2.py:185][0m |          -0.0043 |           2.5039 |           2.9478 |
[32m[20221213 22:22:50 @agent_ppo2.py:185][0m |          -0.0171 |           2.4822 |           2.9473 |
[32m[20221213 22:22:50 @agent_ppo2.py:185][0m |          -0.0028 |           2.4345 |           2.9470 |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |          -0.0086 |           2.3804 |           2.9479 |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |           0.0016 |           2.5918 |           2.9473 |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |          -0.0033 |           2.3735 |           2.9477 |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |          -0.0091 |           2.3281 |           2.9474 |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |          -0.0059 |           2.2857 |           2.9474 |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |          -0.0081 |           2.3027 |           2.9477 |
[32m[20221213 22:22:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:22:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 33.52
[32m[20221213 22:22:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.56
[32m[20221213 22:22:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.83
[32m[20221213 22:22:51 @agent_ppo2.py:143][0m Total time:       4.64 min
[32m[20221213 22:22:51 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 22:22:51 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 22:22:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |          -0.0046 |           1.6094 |           2.9837 |
[32m[20221213 22:22:51 @agent_ppo2.py:185][0m |          -0.0038 |           1.3474 |           2.9815 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0034 |           1.2920 |           2.9819 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0081 |           1.2532 |           2.9812 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0038 |           1.2299 |           2.9816 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0045 |           1.2017 |           2.9826 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0159 |           1.2634 |           2.9827 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0111 |           1.1693 |           2.9835 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0021 |           1.1519 |           2.9841 |
[32m[20221213 22:22:52 @agent_ppo2.py:185][0m |          -0.0112 |           1.1411 |           2.9851 |
[32m[20221213 22:22:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:22:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 5.96
[32m[20221213 22:22:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 29.78
[32m[20221213 22:22:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.15
[32m[20221213 22:22:52 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221213 22:22:52 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221213 22:22:52 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 22:22:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |           0.0026 |           0.3769 |           2.9516 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0019 |           0.3122 |           2.9484 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |           0.0003 |           0.2936 |           2.9453 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0010 |           0.2780 |           2.9448 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0070 |           0.2661 |           2.9425 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0058 |           0.2589 |           2.9425 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0132 |           0.2474 |           2.9418 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0094 |           0.2419 |           2.9410 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0096 |           0.2362 |           2.9414 |
[32m[20221213 22:22:53 @agent_ppo2.py:185][0m |          -0.0114 |           0.2301 |           2.9406 |
[32m[20221213 22:22:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:22:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.59
[32m[20221213 22:22:54 @agent_ppo2.py:143][0m Total time:       4.68 min
[32m[20221213 22:22:54 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 22:22:54 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 22:22:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:22:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:54 @agent_ppo2.py:185][0m |          -0.0002 |           2.1858 |           3.0299 |
[32m[20221213 22:22:54 @agent_ppo2.py:185][0m |          -0.0043 |           1.7804 |           3.0241 |
[32m[20221213 22:22:54 @agent_ppo2.py:185][0m |          -0.0018 |           1.6936 |           3.0214 |
[32m[20221213 22:22:54 @agent_ppo2.py:185][0m |          -0.0051 |           1.6040 |           3.0196 |
[32m[20221213 22:22:54 @agent_ppo2.py:185][0m |          -0.0071 |           1.5893 |           3.0195 |
[32m[20221213 22:22:54 @agent_ppo2.py:185][0m |          -0.0069 |           1.5856 |           3.0201 |
[32m[20221213 22:22:54 @agent_ppo2.py:185][0m |          -0.0127 |           1.5271 |           3.0192 |
[32m[20221213 22:22:55 @agent_ppo2.py:185][0m |          -0.0108 |           1.4986 |           3.0199 |
[32m[20221213 22:22:55 @agent_ppo2.py:185][0m |          -0.0023 |           1.4728 |           3.0213 |
[32m[20221213 22:22:55 @agent_ppo2.py:185][0m |          -0.0134 |           1.4671 |           3.0213 |
[32m[20221213 22:22:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:22:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.33
[32m[20221213 22:22:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 78.26
[32m[20221213 22:22:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.15
[32m[20221213 22:22:55 @agent_ppo2.py:143][0m Total time:       4.70 min
[32m[20221213 22:22:55 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221213 22:22:55 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 22:22:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:55 @agent_ppo2.py:185][0m |           0.0002 |           0.3599 |           2.9081 |
[32m[20221213 22:22:55 @agent_ppo2.py:185][0m |          -0.0015 |           0.3019 |           2.9075 |
[32m[20221213 22:22:55 @agent_ppo2.py:185][0m |          -0.0076 |           0.2813 |           2.9053 |
[32m[20221213 22:22:55 @agent_ppo2.py:185][0m |          -0.0010 |           0.2737 |           2.9037 |
[32m[20221213 22:22:56 @agent_ppo2.py:185][0m |          -0.0031 |           0.2571 |           2.9029 |
[32m[20221213 22:22:56 @agent_ppo2.py:185][0m |          -0.0085 |           0.2494 |           2.9030 |
[32m[20221213 22:22:56 @agent_ppo2.py:185][0m |          -0.0067 |           0.2415 |           2.9010 |
[32m[20221213 22:22:56 @agent_ppo2.py:185][0m |          -0.0076 |           0.2370 |           2.9000 |
[32m[20221213 22:22:56 @agent_ppo2.py:185][0m |          -0.0112 |           0.2319 |           2.9009 |
[32m[20221213 22:22:56 @agent_ppo2.py:185][0m |          -0.0067 |           0.2252 |           2.9001 |
[32m[20221213 22:22:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:22:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:22:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:22:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.60
[32m[20221213 22:22:56 @agent_ppo2.py:143][0m Total time:       4.72 min
[32m[20221213 22:22:56 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 22:22:56 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 22:22:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:22:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:56 @agent_ppo2.py:185][0m |          -0.0034 |           2.2808 |           2.9911 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0048 |           2.0818 |           2.9910 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0023 |           2.0282 |           2.9888 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0044 |           1.9962 |           2.9899 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0034 |           2.0189 |           2.9912 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0076 |           1.9579 |           2.9910 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |           0.0093 |           2.1631 |           2.9917 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0075 |           1.9975 |           2.9940 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0091 |           1.9798 |           2.9946 |
[32m[20221213 22:22:57 @agent_ppo2.py:185][0m |          -0.0121 |           1.9055 |           2.9943 |
[32m[20221213 22:22:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:22:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 16.43
[32m[20221213 22:22:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 38.08
[32m[20221213 22:22:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.77
[32m[20221213 22:22:57 @agent_ppo2.py:143][0m Total time:       4.74 min
[32m[20221213 22:22:57 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221213 22:22:57 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 22:22:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:22:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |           0.0038 |           1.4771 |           3.0415 |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |           0.0012 |           1.1804 |           3.0408 |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |           0.0014 |           1.1507 |           3.0380 |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |          -0.0128 |           1.1163 |           3.0378 |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |          -0.0085 |           1.1054 |           3.0375 |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |          -0.0016 |           1.0732 |           3.0386 |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |          -0.0048 |           1.0567 |           3.0392 |
[32m[20221213 22:22:58 @agent_ppo2.py:185][0m |          -0.0022 |           1.0446 |           3.0391 |
[32m[20221213 22:22:59 @agent_ppo2.py:185][0m |          -0.0086 |           1.0279 |           3.0398 |
[32m[20221213 22:22:59 @agent_ppo2.py:185][0m |          -0.0073 |           1.0159 |           3.0413 |
[32m[20221213 22:22:59 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 22:22:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.06
[32m[20221213 22:22:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 80.30
[32m[20221213 22:22:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.30
[32m[20221213 22:22:59 @agent_ppo2.py:143][0m Total time:       4.77 min
[32m[20221213 22:22:59 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 22:22:59 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 22:22:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:22:59 @agent_ppo2.py:185][0m |          -0.0146 |           2.2090 |           3.1152 |
[32m[20221213 22:22:59 @agent_ppo2.py:185][0m |          -0.0040 |           1.8796 |           3.1148 |
[32m[20221213 22:22:59 @agent_ppo2.py:185][0m |          -0.0040 |           1.6949 |           3.1132 |
[32m[20221213 22:23:00 @agent_ppo2.py:185][0m |          -0.0108 |           1.5984 |           3.1124 |
[32m[20221213 22:23:00 @agent_ppo2.py:185][0m |          -0.0104 |           1.4838 |           3.1141 |
[32m[20221213 22:23:00 @agent_ppo2.py:185][0m |          -0.0031 |           1.4185 |           3.1134 |
[32m[20221213 22:23:00 @agent_ppo2.py:185][0m |          -0.0092 |           1.3558 |           3.1156 |
[32m[20221213 22:23:00 @agent_ppo2.py:185][0m |          -0.0124 |           1.3165 |           3.1160 |
[32m[20221213 22:23:00 @agent_ppo2.py:185][0m |          -0.0083 |           1.2720 |           3.1178 |
[32m[20221213 22:23:00 @agent_ppo2.py:185][0m |          -0.0068 |           1.2283 |           3.1170 |
[32m[20221213 22:23:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:23:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 20.07
[32m[20221213 22:23:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 39.13
[32m[20221213 22:23:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 34.47
[32m[20221213 22:23:00 @agent_ppo2.py:143][0m Total time:       4.79 min
[32m[20221213 22:23:00 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221213 22:23:00 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 22:23:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:23:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0006 |           3.7003 |           3.1266 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0044 |           2.9737 |           3.1257 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0074 |           2.7750 |           3.1230 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0053 |           2.6918 |           3.1251 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0071 |           2.5688 |           3.1255 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0065 |           2.5103 |           3.1268 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0076 |           2.4629 |           3.1265 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0098 |           2.4118 |           3.1278 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0052 |           2.4279 |           3.1294 |
[32m[20221213 22:23:01 @agent_ppo2.py:185][0m |          -0.0069 |           2.3484 |           3.1289 |
[32m[20221213 22:23:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:23:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.41
[32m[20221213 22:23:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.91
[32m[20221213 22:23:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 116.25
[32m[20221213 22:23:02 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 116.25
[32m[20221213 22:23:02 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 116.25
[32m[20221213 22:23:02 @agent_ppo2.py:143][0m Total time:       4.81 min
[32m[20221213 22:23:02 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 22:23:02 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 22:23:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |          -0.0013 |           3.4552 |           3.1438 |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |          -0.0009 |           2.9423 |           3.1443 |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |          -0.0054 |           2.7379 |           3.1450 |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |           0.0011 |           2.6243 |           3.1451 |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |          -0.0039 |           2.5203 |           3.1475 |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |          -0.0098 |           2.4636 |           3.1488 |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |          -0.0070 |           2.4298 |           3.1492 |
[32m[20221213 22:23:02 @agent_ppo2.py:185][0m |          -0.0163 |           2.4991 |           3.1510 |
[32m[20221213 22:23:03 @agent_ppo2.py:185][0m |          -0.0046 |           2.3842 |           3.1526 |
[32m[20221213 22:23:03 @agent_ppo2.py:185][0m |          -0.0118 |           2.3161 |           3.1525 |
[32m[20221213 22:23:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:23:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.93
[32m[20221213 22:23:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.82
[32m[20221213 22:23:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.05
[32m[20221213 22:23:03 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 124.05
[32m[20221213 22:23:03 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 124.05
[32m[20221213 22:23:03 @agent_ppo2.py:143][0m Total time:       4.83 min
[32m[20221213 22:23:03 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221213 22:23:03 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 22:23:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:03 @agent_ppo2.py:185][0m |          -0.0052 |           4.6396 |           3.1067 |
[32m[20221213 22:23:03 @agent_ppo2.py:185][0m |          -0.0065 |           4.2503 |           3.1044 |
[32m[20221213 22:23:03 @agent_ppo2.py:185][0m |          -0.0065 |           4.2094 |           3.1027 |
[32m[20221213 22:23:03 @agent_ppo2.py:185][0m |          -0.0135 |           4.1323 |           3.1027 |
[32m[20221213 22:23:03 @agent_ppo2.py:185][0m |           0.0000 |           4.1695 |           3.1042 |
[32m[20221213 22:23:04 @agent_ppo2.py:185][0m |          -0.0044 |           4.1188 |           3.1049 |
[32m[20221213 22:23:04 @agent_ppo2.py:185][0m |          -0.0120 |           4.0271 |           3.1056 |
[32m[20221213 22:23:04 @agent_ppo2.py:185][0m |          -0.0109 |           3.9707 |           3.1067 |
[32m[20221213 22:23:04 @agent_ppo2.py:185][0m |          -0.0091 |           3.9622 |           3.1066 |
[32m[20221213 22:23:04 @agent_ppo2.py:185][0m |          -0.0091 |           3.9251 |           3.1094 |
[32m[20221213 22:23:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:23:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.21
[32m[20221213 22:23:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.04
[32m[20221213 22:23:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 38.76
[32m[20221213 22:23:04 @agent_ppo2.py:143][0m Total time:       4.85 min
[32m[20221213 22:23:04 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 22:23:04 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 22:23:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:04 @agent_ppo2.py:185][0m |          -0.0045 |           4.5607 |           3.1843 |
[32m[20221213 22:23:04 @agent_ppo2.py:185][0m |          -0.0056 |           4.2041 |           3.1805 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0047 |           3.9468 |           3.1784 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0154 |           3.9257 |           3.1774 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0061 |           3.8242 |           3.1766 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0043 |           3.8064 |           3.1768 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0027 |           3.7892 |           3.1779 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0058 |           3.6793 |           3.1789 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0036 |           3.7616 |           3.1777 |
[32m[20221213 22:23:05 @agent_ppo2.py:185][0m |          -0.0100 |           3.6982 |           3.1763 |
[32m[20221213 22:23:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:23:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.91
[32m[20221213 22:23:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.05
[32m[20221213 22:23:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 106.64
[32m[20221213 22:23:05 @agent_ppo2.py:143][0m Total time:       4.87 min
[32m[20221213 22:23:05 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221213 22:23:05 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 22:23:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |           0.0021 |           7.7542 |           3.0787 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0058 |           6.7811 |           3.0737 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0006 |           6.6706 |           3.0713 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0016 |           6.4357 |           3.0702 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0073 |           6.3298 |           3.0693 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0084 |           6.2340 |           3.0669 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0153 |           6.2131 |           3.0665 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0098 |           6.1404 |           3.0652 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0054 |           6.1272 |           3.0645 |
[32m[20221213 22:23:06 @agent_ppo2.py:185][0m |          -0.0134 |           6.1017 |           3.0638 |
[32m[20221213 22:23:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:23:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 31.99
[32m[20221213 22:23:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.88
[32m[20221213 22:23:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.89
[32m[20221213 22:23:06 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 127.89
[32m[20221213 22:23:06 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 127.89
[32m[20221213 22:23:06 @agent_ppo2.py:143][0m Total time:       4.89 min
[32m[20221213 22:23:06 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 22:23:06 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 22:23:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |           0.0011 |           2.8369 |           3.0816 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0015 |           2.3177 |           3.0796 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0048 |           2.2161 |           3.0781 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0027 |           2.1450 |           3.0780 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0020 |           2.1181 |           3.0783 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0128 |           2.0655 |           3.0775 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |           0.0017 |           2.0658 |           3.0762 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0111 |           2.0515 |           3.0754 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0108 |           2.1093 |           3.0743 |
[32m[20221213 22:23:07 @agent_ppo2.py:185][0m |          -0.0130 |           2.0359 |           3.0743 |
[32m[20221213 22:23:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:23:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.51
[32m[20221213 22:23:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.82
[32m[20221213 22:23:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.92
[32m[20221213 22:23:08 @agent_ppo2.py:143][0m Total time:       4.91 min
[32m[20221213 22:23:08 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221213 22:23:08 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 22:23:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:08 @agent_ppo2.py:185][0m |          -0.0008 |           3.5133 |           3.1434 |
[32m[20221213 22:23:08 @agent_ppo2.py:185][0m |           0.0035 |           3.0683 |           3.1439 |
[32m[20221213 22:23:08 @agent_ppo2.py:185][0m |          -0.0069 |           3.0506 |           3.1428 |
[32m[20221213 22:23:08 @agent_ppo2.py:185][0m |          -0.0061 |           2.9422 |           3.1420 |
[32m[20221213 22:23:08 @agent_ppo2.py:185][0m |          -0.0114 |           2.9085 |           3.1417 |
[32m[20221213 22:23:08 @agent_ppo2.py:185][0m |          -0.0042 |           2.9136 |           3.1429 |
[32m[20221213 22:23:08 @agent_ppo2.py:185][0m |           0.0007 |           2.8803 |           3.1439 |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0069 |           2.8158 |           3.1444 |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0065 |           2.7738 |           3.1452 |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0090 |           2.7646 |           3.1448 |
[32m[20221213 22:23:09 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:23:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 17.42
[32m[20221213 22:23:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.09
[32m[20221213 22:23:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.07
[32m[20221213 22:23:09 @agent_ppo2.py:143][0m Total time:       4.93 min
[32m[20221213 22:23:09 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 22:23:09 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 22:23:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0057 |           5.8138 |           3.1695 |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0005 |           4.9240 |           3.1669 |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0018 |           4.7783 |           3.1644 |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0019 |           4.7158 |           3.1630 |
[32m[20221213 22:23:09 @agent_ppo2.py:185][0m |          -0.0020 |           4.6542 |           3.1603 |
[32m[20221213 22:23:10 @agent_ppo2.py:185][0m |          -0.0055 |           4.5312 |           3.1584 |
[32m[20221213 22:23:10 @agent_ppo2.py:185][0m |          -0.0080 |           4.5161 |           3.1566 |
[32m[20221213 22:23:10 @agent_ppo2.py:185][0m |          -0.0022 |           4.5149 |           3.1549 |
[32m[20221213 22:23:10 @agent_ppo2.py:185][0m |          -0.0047 |           4.4336 |           3.1553 |
[32m[20221213 22:23:10 @agent_ppo2.py:185][0m |          -0.0089 |           4.4265 |           3.1532 |
[32m[20221213 22:23:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:23:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.22
[32m[20221213 22:23:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.11
[32m[20221213 22:23:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.32
[32m[20221213 22:23:10 @agent_ppo2.py:143][0m Total time:       4.95 min
[32m[20221213 22:23:10 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221213 22:23:10 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 22:23:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:10 @agent_ppo2.py:185][0m |           0.0016 |           4.9563 |           3.1520 |
[32m[20221213 22:23:10 @agent_ppo2.py:185][0m |          -0.0024 |           4.7064 |           3.1463 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0121 |           4.7183 |           3.1413 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0154 |           4.6524 |           3.1408 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0170 |           4.5942 |           3.1391 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0036 |           4.5085 |           3.1392 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0137 |           4.5176 |           3.1373 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0137 |           4.4427 |           3.1382 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0061 |           4.3955 |           3.1374 |
[32m[20221213 22:23:11 @agent_ppo2.py:185][0m |          -0.0108 |           4.3909 |           3.1364 |
[32m[20221213 22:23:11 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:23:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.93
[32m[20221213 22:23:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.79
[32m[20221213 22:23:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.05
[32m[20221213 22:23:11 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 130.05
[32m[20221213 22:23:11 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 130.05
[32m[20221213 22:23:11 @agent_ppo2.py:143][0m Total time:       4.97 min
[32m[20221213 22:23:11 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 22:23:11 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 22:23:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0028 |           6.0176 |           3.2154 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0082 |           5.9249 |           3.2137 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0027 |           5.8090 |           3.2115 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0074 |           5.6963 |           3.2107 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0028 |           5.5953 |           3.2120 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0115 |           5.6737 |           3.2127 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0128 |           5.6096 |           3.2136 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0106 |           5.5761 |           3.2150 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0099 |           5.4711 |           3.2151 |
[32m[20221213 22:23:12 @agent_ppo2.py:185][0m |          -0.0099 |           5.4799 |           3.2159 |
[32m[20221213 22:23:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:23:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.90
[32m[20221213 22:23:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.22
[32m[20221213 22:23:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.72
[32m[20221213 22:23:12 @agent_ppo2.py:143][0m Total time:       4.99 min
[32m[20221213 22:23:12 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221213 22:23:12 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 22:23:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |           0.0014 |           7.8351 |           3.1994 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |           0.0007 |           6.1954 |           3.1970 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0013 |           5.9159 |           3.1968 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0100 |           5.8011 |           3.1982 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0053 |           5.7358 |           3.1986 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0084 |           5.6880 |           3.2004 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0114 |           5.6132 |           3.2019 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0088 |           5.5906 |           3.2024 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0053 |           5.5852 |           3.2025 |
[32m[20221213 22:23:13 @agent_ppo2.py:185][0m |          -0.0048 |           5.5252 |           3.2049 |
[32m[20221213 22:23:13 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:23:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.95
[32m[20221213 22:23:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 34.74
[32m[20221213 22:23:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.63
[32m[20221213 22:23:14 @agent_ppo2.py:143][0m Total time:       5.01 min
[32m[20221213 22:23:14 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 22:23:14 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 22:23:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:14 @agent_ppo2.py:185][0m |          -0.0038 |           9.6477 |           3.1419 |
[32m[20221213 22:23:14 @agent_ppo2.py:185][0m |          -0.0022 |           9.0671 |           3.1381 |
[32m[20221213 22:23:14 @agent_ppo2.py:185][0m |          -0.0141 |           8.9640 |           3.1346 |
[32m[20221213 22:23:14 @agent_ppo2.py:185][0m |          -0.0087 |           8.7298 |           3.1328 |
[32m[20221213 22:23:14 @agent_ppo2.py:185][0m |          -0.0070 |           8.6334 |           3.1323 |
[32m[20221213 22:23:14 @agent_ppo2.py:185][0m |          -0.0077 |           8.5388 |           3.1303 |
[32m[20221213 22:23:14 @agent_ppo2.py:185][0m |          -0.0076 |           8.7529 |           3.1322 |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0145 |           8.4323 |           3.1293 |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0095 |           8.3243 |           3.1295 |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0158 |           8.2670 |           3.1286 |
[32m[20221213 22:23:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:23:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.62
[32m[20221213 22:23:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.18
[32m[20221213 22:23:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 219.63
[32m[20221213 22:23:15 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 219.63
[32m[20221213 22:23:15 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 219.63
[32m[20221213 22:23:15 @agent_ppo2.py:143][0m Total time:       5.03 min
[32m[20221213 22:23:15 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221213 22:23:15 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 22:23:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0015 |           8.3992 |           3.1760 |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0061 |           8.0087 |           3.1748 |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0005 |           7.9601 |           3.1726 |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0103 |           7.7672 |           3.1721 |
[32m[20221213 22:23:15 @agent_ppo2.py:185][0m |          -0.0060 |           7.7036 |           3.1712 |
[32m[20221213 22:23:16 @agent_ppo2.py:185][0m |          -0.0083 |           7.6189 |           3.1701 |
[32m[20221213 22:23:16 @agent_ppo2.py:185][0m |          -0.0094 |           7.5511 |           3.1695 |
[32m[20221213 22:23:16 @agent_ppo2.py:185][0m |          -0.0124 |           7.5756 |           3.1693 |
[32m[20221213 22:23:16 @agent_ppo2.py:185][0m |          -0.0144 |           7.5264 |           3.1690 |
[32m[20221213 22:23:16 @agent_ppo2.py:185][0m |          -0.0069 |           7.4569 |           3.1691 |
[32m[20221213 22:23:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:23:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.63
[32m[20221213 22:23:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.85
[32m[20221213 22:23:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.78
[32m[20221213 22:23:16 @agent_ppo2.py:143][0m Total time:       5.05 min
[32m[20221213 22:23:16 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 22:23:16 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 22:23:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:16 @agent_ppo2.py:185][0m |           0.0019 |           6.3481 |           3.1935 |
[32m[20221213 22:23:16 @agent_ppo2.py:185][0m |           0.0003 |           6.1832 |           3.1905 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |           0.0002 |           6.2136 |           3.1885 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |          -0.0082 |           5.9008 |           3.1890 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |          -0.0075 |           5.7703 |           3.1898 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |          -0.0096 |           5.7386 |           3.1885 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |          -0.0115 |           5.7027 |           3.1884 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |          -0.0097 |           5.6738 |           3.1886 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |          -0.0105 |           5.6670 |           3.1901 |
[32m[20221213 22:23:17 @agent_ppo2.py:185][0m |          -0.0069 |           5.6059 |           3.1893 |
[32m[20221213 22:23:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:23:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 24.84
[32m[20221213 22:23:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.30
[32m[20221213 22:23:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.93
[32m[20221213 22:23:17 @agent_ppo2.py:143][0m Total time:       5.07 min
[32m[20221213 22:23:17 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221213 22:23:17 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 22:23:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0007 |           3.4189 |           3.1943 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0050 |           3.1308 |           3.1943 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |           0.0041 |           3.1681 |           3.1926 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0097 |           2.9842 |           3.1935 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0051 |           2.9278 |           3.1948 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0052 |           2.8714 |           3.1949 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0025 |           2.8936 |           3.1933 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0066 |           2.8514 |           3.1937 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0096 |           2.8072 |           3.1941 |
[32m[20221213 22:23:18 @agent_ppo2.py:185][0m |          -0.0083 |           2.7815 |           3.1944 |
[32m[20221213 22:23:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.81
[32m[20221213 22:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 24.06
[32m[20221213 22:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 122.21
[32m[20221213 22:23:18 @agent_ppo2.py:143][0m Total time:       5.09 min
[32m[20221213 22:23:18 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 22:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 22:23:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0057 |           8.5094 |           3.2424 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0055 |           8.0351 |           3.2411 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0035 |           7.8488 |           3.2404 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0092 |           7.7798 |           3.2402 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |           0.0015 |           8.3033 |           3.2403 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0147 |           7.7573 |           3.2414 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0051 |           7.6513 |           3.2424 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0066 |           7.5333 |           3.2427 |
[32m[20221213 22:23:19 @agent_ppo2.py:185][0m |          -0.0090 |           7.4001 |           3.2432 |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0105 |           7.4040 |           3.2433 |
[32m[20221213 22:23:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:23:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.73
[32m[20221213 22:23:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 154.49
[32m[20221213 22:23:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.00
[32m[20221213 22:23:20 @agent_ppo2.py:143][0m Total time:       5.11 min
[32m[20221213 22:23:20 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221213 22:23:20 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 22:23:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0072 |           4.8753 |           3.2890 |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0048 |           3.4375 |           3.2893 |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0053 |           3.2811 |           3.2857 |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0049 |           3.1764 |           3.2864 |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0087 |           3.0918 |           3.2852 |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0066 |           3.0297 |           3.2854 |
[32m[20221213 22:23:20 @agent_ppo2.py:185][0m |          -0.0051 |           3.0079 |           3.2863 |
[32m[20221213 22:23:21 @agent_ppo2.py:185][0m |          -0.0111 |           2.9758 |           3.2857 |
[32m[20221213 22:23:21 @agent_ppo2.py:185][0m |          -0.0058 |           2.9259 |           3.2841 |
[32m[20221213 22:23:21 @agent_ppo2.py:185][0m |          -0.0129 |           2.9005 |           3.2835 |
[32m[20221213 22:23:21 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:23:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.37
[32m[20221213 22:23:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 154.46
[32m[20221213 22:23:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.95
[32m[20221213 22:23:21 @agent_ppo2.py:143][0m Total time:       5.13 min
[32m[20221213 22:23:21 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 22:23:21 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 22:23:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:21 @agent_ppo2.py:185][0m |          -0.0054 |           7.4184 |           3.2205 |
[32m[20221213 22:23:21 @agent_ppo2.py:185][0m |          -0.0071 |           7.1229 |           3.2188 |
[32m[20221213 22:23:21 @agent_ppo2.py:185][0m |          -0.0045 |           7.0298 |           3.2191 |
[32m[20221213 22:23:21 @agent_ppo2.py:185][0m |          -0.0103 |           6.9439 |           3.2190 |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |          -0.0054 |           6.9663 |           3.2202 |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |          -0.0058 |           6.8070 |           3.2217 |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |          -0.0045 |           6.7745 |           3.2212 |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |          -0.0081 |           6.8302 |           3.2229 |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |          -0.0093 |           6.6982 |           3.2237 |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |          -0.0079 |           6.7267 |           3.2234 |
[32m[20221213 22:23:22 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:23:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.17
[32m[20221213 22:23:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 154.74
[32m[20221213 22:23:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.48
[32m[20221213 22:23:22 @agent_ppo2.py:143][0m Total time:       5.15 min
[32m[20221213 22:23:22 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221213 22:23:22 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 22:23:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |           0.0055 |           7.5192 |           3.2620 |
[32m[20221213 22:23:22 @agent_ppo2.py:185][0m |          -0.0039 |           6.9034 |           3.2605 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0015 |           6.8072 |           3.2588 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0003 |           6.7442 |           3.2571 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0041 |           6.8431 |           3.2569 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0071 |           6.6221 |           3.2575 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0069 |           6.6162 |           3.2586 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0027 |           6.5141 |           3.2587 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0091 |           6.5003 |           3.2587 |
[32m[20221213 22:23:23 @agent_ppo2.py:185][0m |          -0.0085 |           6.5310 |           3.2586 |
[32m[20221213 22:23:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:23:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.77
[32m[20221213 22:23:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.13
[32m[20221213 22:23:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.78
[32m[20221213 22:23:23 @agent_ppo2.py:143][0m Total time:       5.17 min
[32m[20221213 22:23:23 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 22:23:23 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 22:23:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |           0.0019 |          10.2717 |           3.3317 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0037 |           8.2852 |           3.3318 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0110 |           8.0030 |           3.3296 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0015 |           7.8376 |           3.3333 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |           0.0003 |           7.9400 |           3.3346 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0144 |           7.7491 |           3.3367 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0028 |           7.5846 |           3.3364 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0085 |           7.5219 |           3.3401 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0093 |           7.4657 |           3.3418 |
[32m[20221213 22:23:24 @agent_ppo2.py:185][0m |          -0.0058 |           7.3958 |           3.3448 |
[32m[20221213 22:23:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:23:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 17.31
[32m[20221213 22:23:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 81.52
[32m[20221213 22:23:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.44
[32m[20221213 22:23:25 @agent_ppo2.py:143][0m Total time:       5.19 min
[32m[20221213 22:23:25 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221213 22:23:25 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 22:23:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |           0.0003 |           4.8288 |           3.3234 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0026 |           3.9817 |           3.3218 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0091 |           3.8709 |           3.3200 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0092 |           3.8263 |           3.3214 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0162 |           3.8165 |           3.3221 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0129 |           3.7779 |           3.3244 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0127 |           3.7266 |           3.3256 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0046 |           3.6357 |           3.3266 |
[32m[20221213 22:23:25 @agent_ppo2.py:185][0m |          -0.0133 |           3.6174 |           3.3281 |
[32m[20221213 22:23:26 @agent_ppo2.py:185][0m |          -0.0123 |           3.6672 |           3.3291 |
[32m[20221213 22:23:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:23:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 76.87
[32m[20221213 22:23:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 186.84
[32m[20221213 22:23:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 205.75
[32m[20221213 22:23:26 @agent_ppo2.py:143][0m Total time:       5.21 min
[32m[20221213 22:23:26 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 22:23:26 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 22:23:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:26 @agent_ppo2.py:185][0m |           0.0025 |           9.5619 |           3.3351 |
[32m[20221213 22:23:26 @agent_ppo2.py:185][0m |          -0.0091 |           8.8880 |           3.3329 |
[32m[20221213 22:23:26 @agent_ppo2.py:185][0m |          -0.0079 |           8.8294 |           3.3292 |
[32m[20221213 22:23:26 @agent_ppo2.py:185][0m |          -0.0016 |           9.3045 |           3.3269 |
[32m[20221213 22:23:26 @agent_ppo2.py:185][0m |          -0.0063 |           8.7596 |           3.3249 |
[32m[20221213 22:23:26 @agent_ppo2.py:185][0m |          -0.0111 |           8.6240 |           3.3222 |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |          -0.0073 |           8.6203 |           3.3208 |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |          -0.0093 |           8.5814 |           3.3203 |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |          -0.0085 |           8.5334 |           3.3184 |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |          -0.0075 |           8.5372 |           3.3179 |
[32m[20221213 22:23:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:23:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.50
[32m[20221213 22:23:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 157.63
[32m[20221213 22:23:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.82
[32m[20221213 22:23:27 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 224.82
[32m[20221213 22:23:27 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 224.82
[32m[20221213 22:23:27 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221213 22:23:27 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221213 22:23:27 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 22:23:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |          -0.0016 |           5.0985 |           3.3130 |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |           0.0013 |           3.9288 |           3.3145 |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |          -0.0053 |           3.8743 |           3.3179 |
[32m[20221213 22:23:27 @agent_ppo2.py:185][0m |           0.0043 |           3.7898 |           3.3201 |
[32m[20221213 22:23:28 @agent_ppo2.py:185][0m |          -0.0009 |           3.7800 |           3.3212 |
[32m[20221213 22:23:28 @agent_ppo2.py:185][0m |          -0.0135 |           3.7721 |           3.3212 |
[32m[20221213 22:23:28 @agent_ppo2.py:185][0m |          -0.0030 |           3.7019 |           3.3236 |
[32m[20221213 22:23:28 @agent_ppo2.py:185][0m |           0.0008 |           3.6741 |           3.3242 |
[32m[20221213 22:23:28 @agent_ppo2.py:185][0m |           0.0000 |           3.6890 |           3.3252 |
[32m[20221213 22:23:28 @agent_ppo2.py:185][0m |          -0.0090 |           3.6626 |           3.3274 |
[32m[20221213 22:23:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:23:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 170.40
[32m[20221213 22:23:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 195.35
[32m[20221213 22:23:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.33
[32m[20221213 22:23:28 @agent_ppo2.py:143][0m Total time:       5.25 min
[32m[20221213 22:23:28 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 22:23:28 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 22:23:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:23:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:28 @agent_ppo2.py:185][0m |           0.0027 |          12.0922 |           3.3559 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0035 |          11.2802 |           3.3554 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0068 |          11.1035 |           3.3532 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0034 |          10.9146 |           3.3534 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0033 |          11.1472 |           3.3535 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0095 |          10.7425 |           3.3518 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0115 |          10.6897 |           3.3522 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0075 |          10.5979 |           3.3526 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0093 |          10.5774 |           3.3511 |
[32m[20221213 22:23:29 @agent_ppo2.py:185][0m |          -0.0043 |          10.5429 |           3.3519 |
[32m[20221213 22:23:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:23:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.46
[32m[20221213 22:23:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 196.83
[32m[20221213 22:23:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.24
[32m[20221213 22:23:29 @agent_ppo2.py:143][0m Total time:       5.28 min
[32m[20221213 22:23:29 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221213 22:23:29 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 22:23:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:23:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |           0.0016 |           7.7735 |           3.3522 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0052 |           6.9057 |           3.3531 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0009 |           6.8114 |           3.3501 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0067 |           6.7892 |           3.3523 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0058 |           6.7170 |           3.3520 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0064 |           6.6344 |           3.3539 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0074 |           6.5291 |           3.3544 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0107 |           6.5264 |           3.3557 |
[32m[20221213 22:23:30 @agent_ppo2.py:185][0m |          -0.0065 |           6.5093 |           3.3567 |
[32m[20221213 22:23:31 @agent_ppo2.py:185][0m |          -0.0100 |           6.4145 |           3.3579 |
[32m[20221213 22:23:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:23:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 18.68
[32m[20221213 22:23:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.39
[32m[20221213 22:23:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 5.06
[32m[20221213 22:23:31 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221213 22:23:31 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 22:23:31 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 22:23:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:23:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:31 @agent_ppo2.py:185][0m |           0.0015 |          10.2480 |           3.4779 |
[32m[20221213 22:23:31 @agent_ppo2.py:185][0m |          -0.0067 |           9.7659 |           3.4755 |
[32m[20221213 22:23:31 @agent_ppo2.py:185][0m |          -0.0097 |           9.6870 |           3.4712 |
[32m[20221213 22:23:31 @agent_ppo2.py:185][0m |          -0.0063 |           9.6650 |           3.4717 |
[32m[20221213 22:23:31 @agent_ppo2.py:185][0m |          -0.0101 |           9.4786 |           3.4688 |
[32m[20221213 22:23:32 @agent_ppo2.py:185][0m |          -0.0055 |           9.4674 |           3.4693 |
[32m[20221213 22:23:32 @agent_ppo2.py:185][0m |          -0.0088 |           9.3572 |           3.4675 |
[32m[20221213 22:23:32 @agent_ppo2.py:185][0m |          -0.0084 |           9.4034 |           3.4671 |
[32m[20221213 22:23:32 @agent_ppo2.py:185][0m |          -0.0050 |           9.4233 |           3.4658 |
[32m[20221213 22:23:32 @agent_ppo2.py:185][0m |          -0.0075 |           9.2570 |           3.4654 |
[32m[20221213 22:23:32 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 22:23:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.90
[32m[20221213 22:23:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.31
[32m[20221213 22:23:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 201.26
[32m[20221213 22:23:32 @agent_ppo2.py:143][0m Total time:       5.32 min
[32m[20221213 22:23:32 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221213 22:23:32 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 22:23:32 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 22:23:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |           0.0014 |           6.1597 |           3.4327 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |           0.0013 |           4.7764 |           3.4275 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |          -0.0101 |           4.3343 |           3.4241 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |          -0.0125 |           4.2295 |           3.4238 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |           0.0006 |           4.0902 |           3.4258 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |          -0.0019 |           4.0803 |           3.4276 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |          -0.0044 |           4.0372 |           3.4265 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |          -0.0057 |           3.9561 |           3.4273 |
[32m[20221213 22:23:33 @agent_ppo2.py:185][0m |          -0.0011 |           4.0505 |           3.4284 |
[32m[20221213 22:23:34 @agent_ppo2.py:185][0m |          -0.0079 |           3.9698 |           3.4279 |
[32m[20221213 22:23:34 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 22:23:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 152.12
[32m[20221213 22:23:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.55
[32m[20221213 22:23:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 24.48
[32m[20221213 22:23:34 @agent_ppo2.py:143][0m Total time:       5.35 min
[32m[20221213 22:23:34 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 22:23:34 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 22:23:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:23:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:34 @agent_ppo2.py:185][0m |          -0.0051 |           8.4623 |           3.4284 |
[32m[20221213 22:23:34 @agent_ppo2.py:185][0m |          -0.0055 |           7.8176 |           3.4257 |
[32m[20221213 22:23:34 @agent_ppo2.py:185][0m |          -0.0076 |           7.6062 |           3.4242 |
[32m[20221213 22:23:34 @agent_ppo2.py:185][0m |          -0.0075 |           7.6207 |           3.4244 |
[32m[20221213 22:23:35 @agent_ppo2.py:185][0m |          -0.0061 |           7.4422 |           3.4227 |
[32m[20221213 22:23:35 @agent_ppo2.py:185][0m |          -0.0079 |           7.3485 |           3.4250 |
[32m[20221213 22:23:35 @agent_ppo2.py:185][0m |          -0.0068 |           7.3188 |           3.4245 |
[32m[20221213 22:23:35 @agent_ppo2.py:185][0m |          -0.0093 |           7.3088 |           3.4247 |
[32m[20221213 22:23:35 @agent_ppo2.py:185][0m |          -0.0135 |           7.2788 |           3.4257 |
[32m[20221213 22:23:35 @agent_ppo2.py:185][0m |          -0.0070 |           7.2480 |           3.4278 |
[32m[20221213 22:23:35 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 22:23:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.57
[32m[20221213 22:23:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 193.25
[32m[20221213 22:23:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.29
[32m[20221213 22:23:35 @agent_ppo2.py:143][0m Total time:       5.37 min
[32m[20221213 22:23:35 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221213 22:23:35 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 22:23:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |          -0.0000 |           8.5814 |           3.4548 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |           0.0014 |           8.1433 |           3.4518 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |          -0.0057 |           7.9940 |           3.4490 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |          -0.0072 |           7.8714 |           3.4495 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |          -0.0006 |           7.8615 |           3.4474 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |           0.0077 |           8.6061 |           3.4467 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |          -0.0081 |           8.0057 |           3.4458 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |          -0.0161 |           7.7648 |           3.4450 |
[32m[20221213 22:23:36 @agent_ppo2.py:185][0m |          -0.0136 |           7.7360 |           3.4443 |
[32m[20221213 22:23:37 @agent_ppo2.py:185][0m |          -0.0072 |           7.8274 |           3.4446 |
[32m[20221213 22:23:37 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:23:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.99
[32m[20221213 22:23:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 193.44
[32m[20221213 22:23:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.06
[32m[20221213 22:23:37 @agent_ppo2.py:143][0m Total time:       5.40 min
[32m[20221213 22:23:37 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 22:23:37 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 22:23:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:37 @agent_ppo2.py:185][0m |          -0.0025 |          14.3652 |           3.5280 |
[32m[20221213 22:23:37 @agent_ppo2.py:185][0m |          -0.0021 |          12.2838 |           3.5240 |
[32m[20221213 22:23:37 @agent_ppo2.py:185][0m |          -0.0030 |          11.8823 |           3.5216 |
[32m[20221213 22:23:37 @agent_ppo2.py:185][0m |          -0.0136 |          11.7019 |           3.5203 |
[32m[20221213 22:23:37 @agent_ppo2.py:185][0m |          -0.0123 |          11.5991 |           3.5220 |
[32m[20221213 22:23:38 @agent_ppo2.py:185][0m |          -0.0057 |          11.4985 |           3.5212 |
[32m[20221213 22:23:38 @agent_ppo2.py:185][0m |          -0.0131 |          11.4379 |           3.5224 |
[32m[20221213 22:23:38 @agent_ppo2.py:185][0m |          -0.0117 |          11.4010 |           3.5228 |
[32m[20221213 22:23:38 @agent_ppo2.py:185][0m |          -0.0092 |          11.3370 |           3.5247 |
[32m[20221213 22:23:38 @agent_ppo2.py:185][0m |          -0.0178 |          11.3162 |           3.5240 |
[32m[20221213 22:23:38 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:23:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.42
[32m[20221213 22:23:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.93
[32m[20221213 22:23:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.39
[32m[20221213 22:23:38 @agent_ppo2.py:143][0m Total time:       5.42 min
[32m[20221213 22:23:38 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221213 22:23:38 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 22:23:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:38 @agent_ppo2.py:185][0m |          -0.0055 |          12.9556 |           3.5050 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0092 |          12.3499 |           3.5039 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |           0.0024 |          13.0757 |           3.5032 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0074 |          12.1258 |           3.5029 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0076 |          11.9306 |           3.5047 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0058 |          11.8737 |           3.5057 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0091 |          11.7776 |           3.5083 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0102 |          11.6913 |           3.5067 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0099 |          11.6870 |           3.5071 |
[32m[20221213 22:23:39 @agent_ppo2.py:185][0m |          -0.0103 |          11.6180 |           3.5058 |
[32m[20221213 22:23:39 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:23:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.59
[32m[20221213 22:23:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 105.59
[32m[20221213 22:23:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:23:40 @agent_ppo2.py:143][0m Total time:       5.44 min
[32m[20221213 22:23:40 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 22:23:40 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 22:23:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:40 @agent_ppo2.py:185][0m |           0.0061 |           8.0999 |           3.4858 |
[32m[20221213 22:23:40 @agent_ppo2.py:185][0m |          -0.0038 |           6.0828 |           3.4830 |
[32m[20221213 22:23:40 @agent_ppo2.py:185][0m |          -0.0039 |           5.9577 |           3.4804 |
[32m[20221213 22:23:40 @agent_ppo2.py:185][0m |          -0.0070 |           5.7565 |           3.4791 |
[32m[20221213 22:23:40 @agent_ppo2.py:185][0m |          -0.0052 |           5.6247 |           3.4793 |
[32m[20221213 22:23:40 @agent_ppo2.py:185][0m |          -0.0010 |           5.6404 |           3.4793 |
[32m[20221213 22:23:41 @agent_ppo2.py:185][0m |          -0.0033 |           5.5139 |           3.4806 |
[32m[20221213 22:23:41 @agent_ppo2.py:185][0m |          -0.0129 |           5.5439 |           3.4819 |
[32m[20221213 22:23:41 @agent_ppo2.py:185][0m |          -0.0079 |           5.4572 |           3.4822 |
[32m[20221213 22:23:41 @agent_ppo2.py:185][0m |          -0.0067 |           5.4256 |           3.4826 |
[32m[20221213 22:23:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:23:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.65
[32m[20221213 22:23:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.46
[32m[20221213 22:23:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.90
[32m[20221213 22:23:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 247.90
[32m[20221213 22:23:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 247.90
[32m[20221213 22:23:41 @agent_ppo2.py:143][0m Total time:       5.47 min
[32m[20221213 22:23:41 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221213 22:23:41 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 22:23:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:23:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:41 @agent_ppo2.py:185][0m |          -0.0006 |          11.5340 |           3.5228 |
[32m[20221213 22:23:41 @agent_ppo2.py:185][0m |          -0.0049 |          10.4660 |           3.5203 |
[32m[20221213 22:23:41 @agent_ppo2.py:185][0m |          -0.0060 |          10.1676 |           3.5171 |
[32m[20221213 22:23:42 @agent_ppo2.py:185][0m |          -0.0034 |          10.1316 |           3.5183 |
[32m[20221213 22:23:42 @agent_ppo2.py:185][0m |          -0.0104 |           9.7236 |           3.5181 |
[32m[20221213 22:23:42 @agent_ppo2.py:185][0m |          -0.0073 |           9.6636 |           3.5195 |
[32m[20221213 22:23:42 @agent_ppo2.py:185][0m |          -0.0058 |           9.6015 |           3.5192 |
[32m[20221213 22:23:42 @agent_ppo2.py:185][0m |          -0.0057 |           9.4865 |           3.5207 |
[32m[20221213 22:23:42 @agent_ppo2.py:185][0m |          -0.0079 |           9.4707 |           3.5227 |
[32m[20221213 22:23:42 @agent_ppo2.py:185][0m |          -0.0121 |           9.3141 |           3.5238 |
[32m[20221213 22:23:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:23:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 205.98
[32m[20221213 22:23:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.10
[32m[20221213 22:23:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.79
[32m[20221213 22:23:42 @agent_ppo2.py:143][0m Total time:       5.49 min
[32m[20221213 22:23:42 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 22:23:42 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 22:23:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0024 |          10.1421 |           3.5302 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0105 |           9.8409 |           3.5254 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0060 |           9.8008 |           3.5253 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0120 |           9.6787 |           3.5260 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0119 |           9.5883 |           3.5245 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0054 |           9.9321 |           3.5240 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0120 |           9.5098 |           3.5235 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0006 |          10.3824 |           3.5228 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0132 |           9.5652 |           3.5248 |
[32m[20221213 22:23:43 @agent_ppo2.py:185][0m |          -0.0102 |           9.4096 |           3.5245 |
[32m[20221213 22:23:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:23:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.15
[32m[20221213 22:23:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.93
[32m[20221213 22:23:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.90
[32m[20221213 22:23:44 @agent_ppo2.py:143][0m Total time:       5.51 min
[32m[20221213 22:23:44 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221213 22:23:44 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 22:23:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:23:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:44 @agent_ppo2.py:185][0m |          -0.0056 |          11.0955 |           3.4850 |
[32m[20221213 22:23:44 @agent_ppo2.py:185][0m |          -0.0012 |          10.5857 |           3.4836 |
[32m[20221213 22:23:44 @agent_ppo2.py:185][0m |          -0.0102 |          10.6290 |           3.4850 |
[32m[20221213 22:23:44 @agent_ppo2.py:185][0m |           0.0006 |          10.4641 |           3.4858 |
[32m[20221213 22:23:44 @agent_ppo2.py:185][0m |          -0.0124 |          10.3561 |           3.4867 |
[32m[20221213 22:23:44 @agent_ppo2.py:185][0m |          -0.0111 |          10.3014 |           3.4866 |
[32m[20221213 22:23:45 @agent_ppo2.py:185][0m |          -0.0029 |          10.0043 |           3.4884 |
[32m[20221213 22:23:45 @agent_ppo2.py:185][0m |          -0.0111 |           9.9873 |           3.4891 |
[32m[20221213 22:23:45 @agent_ppo2.py:185][0m |          -0.0151 |           9.9013 |           3.4892 |
[32m[20221213 22:23:45 @agent_ppo2.py:185][0m |          -0.0022 |           9.7681 |           3.4906 |
[32m[20221213 22:23:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:23:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.90
[32m[20221213 22:23:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 184.72
[32m[20221213 22:23:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 173.88
[32m[20221213 22:23:45 @agent_ppo2.py:143][0m Total time:       5.54 min
[32m[20221213 22:23:45 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 22:23:45 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 22:23:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:23:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:45 @agent_ppo2.py:185][0m |           0.0020 |           9.3902 |           3.5683 |
[32m[20221213 22:23:45 @agent_ppo2.py:185][0m |           0.0010 |           9.1267 |           3.5648 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0038 |           9.4292 |           3.5654 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0094 |           9.0857 |           3.5620 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0050 |           9.0088 |           3.5631 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0100 |           9.0642 |           3.5625 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0045 |           9.0110 |           3.5641 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0061 |           8.9893 |           3.5629 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0148 |           8.9098 |           3.5646 |
[32m[20221213 22:23:46 @agent_ppo2.py:185][0m |          -0.0043 |           9.2744 |           3.5631 |
[32m[20221213 22:23:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:23:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.76
[32m[20221213 22:23:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.46
[32m[20221213 22:23:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.93
[32m[20221213 22:23:46 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 249.93
[32m[20221213 22:23:46 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 249.93
[32m[20221213 22:23:46 @agent_ppo2.py:143][0m Total time:       5.56 min
[32m[20221213 22:23:46 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221213 22:23:46 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 22:23:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:23:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |           0.0001 |          22.5756 |           3.5146 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |          -0.0026 |          22.8119 |           3.5128 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |          -0.0062 |          21.1292 |           3.5136 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |          -0.0056 |          20.8076 |           3.5143 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |          -0.0065 |          20.7424 |           3.5136 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |          -0.0078 |          20.6626 |           3.5124 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |          -0.0097 |          20.5629 |           3.5109 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |           0.0019 |          21.4210 |           3.5118 |
[32m[20221213 22:23:47 @agent_ppo2.py:185][0m |          -0.0094 |          20.5447 |           3.5108 |
[32m[20221213 22:23:48 @agent_ppo2.py:185][0m |          -0.0065 |          20.3824 |           3.5114 |
[32m[20221213 22:23:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:23:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.34
[32m[20221213 22:23:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 193.58
[32m[20221213 22:23:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.17
[32m[20221213 22:23:48 @agent_ppo2.py:143][0m Total time:       5.58 min
[32m[20221213 22:23:48 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 22:23:48 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 22:23:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:48 @agent_ppo2.py:185][0m |           0.0002 |          12.6879 |           3.5718 |
[32m[20221213 22:23:48 @agent_ppo2.py:185][0m |          -0.0024 |          10.4428 |           3.5684 |
[32m[20221213 22:23:48 @agent_ppo2.py:185][0m |          -0.0063 |           9.9192 |           3.5642 |
[32m[20221213 22:23:48 @agent_ppo2.py:185][0m |          -0.0109 |           9.7041 |           3.5627 |
[32m[20221213 22:23:48 @agent_ppo2.py:185][0m |          -0.0069 |           9.5250 |           3.5597 |
[32m[20221213 22:23:49 @agent_ppo2.py:185][0m |          -0.0082 |           9.7813 |           3.5584 |
[32m[20221213 22:23:49 @agent_ppo2.py:185][0m |          -0.0042 |           9.2781 |           3.5546 |
[32m[20221213 22:23:49 @agent_ppo2.py:185][0m |          -0.0093 |           9.2011 |           3.5545 |
[32m[20221213 22:23:49 @agent_ppo2.py:185][0m |          -0.0057 |           9.3280 |           3.5515 |
[32m[20221213 22:23:49 @agent_ppo2.py:185][0m |          -0.0137 |           9.1860 |           3.5531 |
[32m[20221213 22:23:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:23:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.32
[32m[20221213 22:23:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 41.59
[32m[20221213 22:23:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.92
[32m[20221213 22:23:49 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 257.92
[32m[20221213 22:23:49 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 257.92
[32m[20221213 22:23:49 @agent_ppo2.py:143][0m Total time:       5.60 min
[32m[20221213 22:23:49 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221213 22:23:49 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 22:23:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:49 @agent_ppo2.py:185][0m |          -0.0002 |           7.8135 |           3.5145 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0079 |           6.8701 |           3.5136 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0034 |           6.6691 |           3.5124 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0031 |           6.5061 |           3.5119 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0055 |           6.4895 |           3.5120 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0104 |           6.4812 |           3.5115 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0057 |           6.5561 |           3.5123 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0113 |           6.4702 |           3.5120 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0153 |           6.4387 |           3.5112 |
[32m[20221213 22:23:50 @agent_ppo2.py:185][0m |          -0.0066 |           6.4454 |           3.5103 |
[32m[20221213 22:23:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:23:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.19
[32m[20221213 22:23:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.75
[32m[20221213 22:23:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 93.21
[32m[20221213 22:23:50 @agent_ppo2.py:143][0m Total time:       5.63 min
[32m[20221213 22:23:50 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 22:23:50 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 22:23:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0016 |          15.6766 |           3.5677 |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0055 |          14.1266 |           3.5629 |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0075 |          13.9516 |           3.5600 |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0090 |          13.9363 |           3.5562 |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0147 |          13.7436 |           3.5548 |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0123 |          13.7111 |           3.5542 |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0104 |          13.6904 |           3.5524 |
[32m[20221213 22:23:51 @agent_ppo2.py:185][0m |          -0.0118 |          13.6516 |           3.5493 |
[32m[20221213 22:23:52 @agent_ppo2.py:185][0m |          -0.0111 |          13.6192 |           3.5498 |
[32m[20221213 22:23:52 @agent_ppo2.py:185][0m |          -0.0118 |          13.5731 |           3.5497 |
[32m[20221213 22:23:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:23:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.54
[32m[20221213 22:23:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.84
[32m[20221213 22:23:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.47
[32m[20221213 22:23:52 @agent_ppo2.py:143][0m Total time:       5.65 min
[32m[20221213 22:23:52 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221213 22:23:52 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 22:23:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:52 @agent_ppo2.py:185][0m |           0.0031 |           8.0700 |           3.5320 |
[32m[20221213 22:23:52 @agent_ppo2.py:185][0m |           0.0004 |           6.7346 |           3.5277 |
[32m[20221213 22:23:52 @agent_ppo2.py:185][0m |          -0.0019 |           6.6781 |           3.5271 |
[32m[20221213 22:23:52 @agent_ppo2.py:185][0m |          -0.0119 |           6.5455 |           3.5301 |
[32m[20221213 22:23:52 @agent_ppo2.py:185][0m |          -0.0025 |           6.4179 |           3.5318 |
[32m[20221213 22:23:53 @agent_ppo2.py:185][0m |          -0.0006 |           6.4620 |           3.5300 |
[32m[20221213 22:23:53 @agent_ppo2.py:185][0m |          -0.0048 |           6.4089 |           3.5310 |
[32m[20221213 22:23:53 @agent_ppo2.py:185][0m |          -0.0072 |           6.3781 |           3.5323 |
[32m[20221213 22:23:53 @agent_ppo2.py:185][0m |          -0.0036 |           6.3337 |           3.5325 |
[32m[20221213 22:23:53 @agent_ppo2.py:185][0m |          -0.0096 |           6.3600 |           3.5329 |
[32m[20221213 22:23:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:23:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.71
[32m[20221213 22:23:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.39
[32m[20221213 22:23:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.12
[32m[20221213 22:23:53 @agent_ppo2.py:143][0m Total time:       5.67 min
[32m[20221213 22:23:53 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 22:23:53 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 22:23:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |           0.0063 |           8.5175 |           3.5334 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |           0.0001 |           7.7978 |           3.5336 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |           0.0029 |           7.7885 |           3.5305 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |          -0.0069 |           7.5993 |           3.5302 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |          -0.0017 |           7.5046 |           3.5295 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |          -0.0008 |           7.6594 |           3.5307 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |          -0.0061 |           7.5071 |           3.5312 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |          -0.0054 |           7.3740 |           3.5334 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |          -0.0069 |           7.3487 |           3.5312 |
[32m[20221213 22:23:54 @agent_ppo2.py:185][0m |          -0.0050 |           7.5049 |           3.5327 |
[32m[20221213 22:23:54 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.41
[32m[20221213 22:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.87
[32m[20221213 22:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.69
[32m[20221213 22:23:55 @agent_ppo2.py:143][0m Total time:       5.69 min
[32m[20221213 22:23:55 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221213 22:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 22:23:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:55 @agent_ppo2.py:185][0m |          -0.0086 |           9.6502 |           3.5751 |
[32m[20221213 22:23:55 @agent_ppo2.py:185][0m |           0.0056 |           9.4501 |           3.5768 |
[32m[20221213 22:23:55 @agent_ppo2.py:185][0m |          -0.0021 |           9.0755 |           3.5773 |
[32m[20221213 22:23:55 @agent_ppo2.py:185][0m |          -0.0057 |           9.0586 |           3.5801 |
[32m[20221213 22:23:55 @agent_ppo2.py:185][0m |          -0.0108 |           8.8628 |           3.5818 |
[32m[20221213 22:23:55 @agent_ppo2.py:185][0m |          -0.0097 |           8.7879 |           3.5831 |
[32m[20221213 22:23:55 @agent_ppo2.py:185][0m |          -0.0117 |           8.7533 |           3.5855 |
[32m[20221213 22:23:56 @agent_ppo2.py:185][0m |          -0.0029 |           8.8463 |           3.5871 |
[32m[20221213 22:23:56 @agent_ppo2.py:185][0m |          -0.0073 |           8.8180 |           3.5882 |
[32m[20221213 22:23:56 @agent_ppo2.py:185][0m |          -0.0059 |           8.6919 |           3.5910 |
[32m[20221213 22:23:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:23:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 173.65
[32m[20221213 22:23:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 205.81
[32m[20221213 22:23:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:23:56 @agent_ppo2.py:143][0m Total time:       5.72 min
[32m[20221213 22:23:56 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 22:23:56 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 22:23:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:56 @agent_ppo2.py:185][0m |          -0.0061 |          21.3629 |           3.5840 |
[32m[20221213 22:23:56 @agent_ppo2.py:185][0m |          -0.0088 |          17.4073 |           3.5794 |
[32m[20221213 22:23:56 @agent_ppo2.py:185][0m |          -0.0096 |          16.9822 |           3.5773 |
[32m[20221213 22:23:56 @agent_ppo2.py:185][0m |          -0.0106 |          16.7589 |           3.5771 |
[32m[20221213 22:23:57 @agent_ppo2.py:185][0m |          -0.0115 |          16.5371 |           3.5764 |
[32m[20221213 22:23:57 @agent_ppo2.py:185][0m |           0.0025 |          17.5168 |           3.5773 |
[32m[20221213 22:23:57 @agent_ppo2.py:185][0m |          -0.0083 |          16.3762 |           3.5725 |
[32m[20221213 22:23:57 @agent_ppo2.py:185][0m |          -0.0051 |          16.0413 |           3.5726 |
[32m[20221213 22:23:57 @agent_ppo2.py:185][0m |          -0.0123 |          15.9836 |           3.5726 |
[32m[20221213 22:23:57 @agent_ppo2.py:185][0m |          -0.0082 |          15.8948 |           3.5718 |
[32m[20221213 22:23:57 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:23:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.38
[32m[20221213 22:23:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.86
[32m[20221213 22:23:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.94
[32m[20221213 22:23:57 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 22:23:57 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221213 22:23:57 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 22:23:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |           0.0023 |          13.1268 |           3.6288 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0005 |          11.9567 |           3.6243 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0054 |          11.7689 |           3.6228 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0070 |          11.6695 |           3.6171 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0082 |          11.5705 |           3.6178 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0027 |          11.5376 |           3.6169 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0078 |          11.4807 |           3.6171 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0084 |          11.3720 |           3.6171 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0052 |          11.4054 |           3.6163 |
[32m[20221213 22:23:58 @agent_ppo2.py:185][0m |          -0.0077 |          11.2920 |           3.6165 |
[32m[20221213 22:23:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.61
[32m[20221213 22:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.37
[32m[20221213 22:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.91
[32m[20221213 22:23:59 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221213 22:23:59 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 22:23:59 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 22:23:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:23:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:23:59 @agent_ppo2.py:185][0m |           0.0010 |          12.2910 |           3.5354 |
[32m[20221213 22:23:59 @agent_ppo2.py:185][0m |          -0.0018 |          11.7934 |           3.5338 |
[32m[20221213 22:23:59 @agent_ppo2.py:185][0m |           0.0039 |          12.7916 |           3.5351 |
[32m[20221213 22:23:59 @agent_ppo2.py:185][0m |          -0.0099 |          11.6443 |           3.5391 |
[32m[20221213 22:23:59 @agent_ppo2.py:185][0m |          -0.0040 |          11.4825 |           3.5405 |
[32m[20221213 22:23:59 @agent_ppo2.py:185][0m |          -0.0084 |          11.4935 |           3.5421 |
[32m[20221213 22:23:59 @agent_ppo2.py:185][0m |          -0.0061 |          11.4069 |           3.5441 |
[32m[20221213 22:24:00 @agent_ppo2.py:185][0m |          -0.0054 |          11.2352 |           3.5449 |
[32m[20221213 22:24:00 @agent_ppo2.py:185][0m |          -0.0053 |          11.2118 |           3.5470 |
[32m[20221213 22:24:00 @agent_ppo2.py:185][0m |          -0.0032 |          11.5540 |           3.5490 |
[32m[20221213 22:24:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.54
[32m[20221213 22:24:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.35
[32m[20221213 22:24:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.69
[32m[20221213 22:24:00 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.69
[32m[20221213 22:24:00 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.69
[32m[20221213 22:24:00 @agent_ppo2.py:143][0m Total time:       5.78 min
[32m[20221213 22:24:00 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221213 22:24:00 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 22:24:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:00 @agent_ppo2.py:185][0m |           0.0030 |          21.0606 |           3.7074 |
[32m[20221213 22:24:00 @agent_ppo2.py:185][0m |          -0.0048 |          19.1382 |           3.7036 |
[32m[20221213 22:24:00 @agent_ppo2.py:185][0m |          -0.0056 |          18.9349 |           3.7018 |
[32m[20221213 22:24:01 @agent_ppo2.py:185][0m |          -0.0108 |          18.7607 |           3.6997 |
[32m[20221213 22:24:01 @agent_ppo2.py:185][0m |          -0.0049 |          18.6204 |           3.6973 |
[32m[20221213 22:24:01 @agent_ppo2.py:185][0m |          -0.0055 |          18.5629 |           3.6980 |
[32m[20221213 22:24:01 @agent_ppo2.py:185][0m |          -0.0088 |          18.5908 |           3.6975 |
[32m[20221213 22:24:01 @agent_ppo2.py:185][0m |           0.0029 |          18.8723 |           3.6973 |
[32m[20221213 22:24:01 @agent_ppo2.py:185][0m |          -0.0151 |          18.4680 |           3.6959 |
[32m[20221213 22:24:01 @agent_ppo2.py:185][0m |          -0.0090 |          18.6033 |           3.6970 |
[32m[20221213 22:24:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.23
[32m[20221213 22:24:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.51
[32m[20221213 22:24:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.06
[32m[20221213 22:24:01 @agent_ppo2.py:143][0m Total time:       5.81 min
[32m[20221213 22:24:01 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 22:24:01 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 22:24:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:24:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |           0.0122 |          17.1651 |           3.6430 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0053 |          14.4784 |           3.6422 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0058 |          14.1290 |           3.6392 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0032 |          13.9664 |           3.6353 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0092 |          13.8693 |           3.6354 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0071 |          13.6044 |           3.6329 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0082 |          13.5814 |           3.6321 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0072 |          13.4399 |           3.6321 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0062 |          13.3299 |           3.6308 |
[32m[20221213 22:24:02 @agent_ppo2.py:185][0m |          -0.0072 |          13.1636 |           3.6315 |
[32m[20221213 22:24:02 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.67
[32m[20221213 22:24:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.40
[32m[20221213 22:24:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.76
[32m[20221213 22:24:03 @agent_ppo2.py:143][0m Total time:       5.83 min
[32m[20221213 22:24:03 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221213 22:24:03 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 22:24:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:03 @agent_ppo2.py:185][0m |           0.0009 |          13.7279 |           3.6420 |
[32m[20221213 22:24:03 @agent_ppo2.py:185][0m |          -0.0064 |          12.9015 |           3.6392 |
[32m[20221213 22:24:03 @agent_ppo2.py:185][0m |          -0.0028 |          12.6330 |           3.6337 |
[32m[20221213 22:24:03 @agent_ppo2.py:185][0m |          -0.0068 |          12.6053 |           3.6323 |
[32m[20221213 22:24:03 @agent_ppo2.py:185][0m |           0.0085 |          15.6640 |           3.6357 |
[32m[20221213 22:24:03 @agent_ppo2.py:185][0m |          -0.0057 |          12.5034 |           3.6314 |
[32m[20221213 22:24:04 @agent_ppo2.py:185][0m |           0.0195 |          16.2247 |           3.6342 |
[32m[20221213 22:24:04 @agent_ppo2.py:185][0m |          -0.0066 |          12.6300 |           3.6352 |
[32m[20221213 22:24:04 @agent_ppo2.py:185][0m |          -0.0074 |          12.2718 |           3.6356 |
[32m[20221213 22:24:04 @agent_ppo2.py:185][0m |          -0.0097 |          12.2406 |           3.6348 |
[32m[20221213 22:24:04 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.05
[32m[20221213 22:24:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 158.88
[32m[20221213 22:24:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:04 @agent_ppo2.py:143][0m Total time:       5.85 min
[32m[20221213 22:24:04 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 22:24:04 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 22:24:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:04 @agent_ppo2.py:185][0m |          -0.0047 |          11.3379 |           3.7183 |
[32m[20221213 22:24:04 @agent_ppo2.py:185][0m |          -0.0091 |           9.5473 |           3.7178 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |           0.0024 |           9.0708 |           3.7155 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |          -0.0070 |           8.9327 |           3.7201 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |          -0.0083 |           8.7417 |           3.7217 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |          -0.0093 |           8.6559 |           3.7213 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |          -0.0015 |           8.7487 |           3.7245 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |          -0.0064 |           8.8745 |           3.7233 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |          -0.0002 |           8.5013 |           3.7259 |
[32m[20221213 22:24:05 @agent_ppo2.py:185][0m |          -0.0152 |           8.4725 |           3.7284 |
[32m[20221213 22:24:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.08
[32m[20221213 22:24:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.54
[32m[20221213 22:24:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:05 @agent_ppo2.py:143][0m Total time:       5.87 min
[32m[20221213 22:24:05 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221213 22:24:05 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 22:24:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0017 |          12.3648 |           3.7087 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0100 |          11.5268 |           3.7004 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0037 |          11.2520 |           3.7041 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0098 |          11.0656 |           3.7009 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0058 |          10.8685 |           3.7065 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0101 |          10.6750 |           3.7076 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0082 |          10.6711 |           3.7071 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0123 |          10.5352 |           3.7116 |
[32m[20221213 22:24:06 @agent_ppo2.py:185][0m |          -0.0105 |          10.5108 |           3.7110 |
[32m[20221213 22:24:07 @agent_ppo2.py:185][0m |          -0.0156 |          10.4083 |           3.7123 |
[32m[20221213 22:24:07 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:24:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.47
[32m[20221213 22:24:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.55
[32m[20221213 22:24:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:07 @agent_ppo2.py:143][0m Total time:       5.90 min
[32m[20221213 22:24:07 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 22:24:07 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 22:24:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:07 @agent_ppo2.py:185][0m |          -0.0033 |          16.6104 |           3.7047 |
[32m[20221213 22:24:07 @agent_ppo2.py:185][0m |          -0.0046 |          14.5098 |           3.7065 |
[32m[20221213 22:24:07 @agent_ppo2.py:185][0m |          -0.0060 |          14.2107 |           3.7068 |
[32m[20221213 22:24:07 @agent_ppo2.py:185][0m |          -0.0089 |          14.0415 |           3.7076 |
[32m[20221213 22:24:07 @agent_ppo2.py:185][0m |          -0.0048 |          13.8873 |           3.7096 |
[32m[20221213 22:24:08 @agent_ppo2.py:185][0m |          -0.0101 |          13.7780 |           3.7093 |
[32m[20221213 22:24:08 @agent_ppo2.py:185][0m |          -0.0087 |          13.7055 |           3.7103 |
[32m[20221213 22:24:08 @agent_ppo2.py:185][0m |          -0.0062 |          13.5844 |           3.7103 |
[32m[20221213 22:24:08 @agent_ppo2.py:185][0m |          -0.0063 |          13.4592 |           3.7090 |
[32m[20221213 22:24:08 @agent_ppo2.py:185][0m |          -0.0112 |          13.3840 |           3.7093 |
[32m[20221213 22:24:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 33.83
[32m[20221213 22:24:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.04
[32m[20221213 22:24:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.14
[32m[20221213 22:24:08 @agent_ppo2.py:143][0m Total time:       5.92 min
[32m[20221213 22:24:08 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221213 22:24:08 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 22:24:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:08 @agent_ppo2.py:185][0m |           0.0010 |          18.4807 |           3.7817 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0081 |          17.6872 |           3.7784 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0121 |          17.4233 |           3.7785 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0099 |          17.2621 |           3.7776 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0117 |          17.2144 |           3.7783 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0107 |          17.0221 |           3.7779 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0084 |          17.0366 |           3.7785 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0121 |          16.8790 |           3.7791 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0084 |          16.8753 |           3.7804 |
[32m[20221213 22:24:09 @agent_ppo2.py:185][0m |          -0.0079 |          16.7973 |           3.7788 |
[32m[20221213 22:24:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.70
[32m[20221213 22:24:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 151.14
[32m[20221213 22:24:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.33
[32m[20221213 22:24:09 @agent_ppo2.py:143][0m Total time:       5.94 min
[32m[20221213 22:24:09 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 22:24:09 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 22:24:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |           0.0025 |          13.5111 |           3.8095 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |          -0.0072 |          12.4150 |           3.8055 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |           0.0064 |          13.0925 |           3.8046 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |          -0.0052 |          12.1135 |           3.8026 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |          -0.0072 |          11.9743 |           3.8020 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |          -0.0127 |          12.0078 |           3.8046 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |          -0.0054 |          12.0333 |           3.8021 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |          -0.0096 |          11.7845 |           3.8044 |
[32m[20221213 22:24:10 @agent_ppo2.py:185][0m |          -0.0119 |          11.7495 |           3.8045 |
[32m[20221213 22:24:11 @agent_ppo2.py:185][0m |          -0.0079 |          11.6628 |           3.8044 |
[32m[20221213 22:24:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 36.34
[32m[20221213 22:24:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.95
[32m[20221213 22:24:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:11 @agent_ppo2.py:143][0m Total time:       5.96 min
[32m[20221213 22:24:11 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221213 22:24:11 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 22:24:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:11 @agent_ppo2.py:185][0m |           0.0004 |           7.7046 |           3.7971 |
[32m[20221213 22:24:11 @agent_ppo2.py:185][0m |          -0.0074 |           5.1052 |           3.7918 |
[32m[20221213 22:24:11 @agent_ppo2.py:185][0m |          -0.0078 |           4.7405 |           3.7931 |
[32m[20221213 22:24:11 @agent_ppo2.py:185][0m |          -0.0031 |           4.5628 |           3.7921 |
[32m[20221213 22:24:11 @agent_ppo2.py:185][0m |          -0.0037 |           4.6210 |           3.7956 |
[32m[20221213 22:24:12 @agent_ppo2.py:185][0m |          -0.0033 |           4.4203 |           3.8000 |
[32m[20221213 22:24:12 @agent_ppo2.py:185][0m |          -0.0031 |           4.3983 |           3.7954 |
[32m[20221213 22:24:12 @agent_ppo2.py:185][0m |          -0.0100 |           4.3513 |           3.7982 |
[32m[20221213 22:24:12 @agent_ppo2.py:185][0m |          -0.0112 |           4.3304 |           3.7977 |
[32m[20221213 22:24:12 @agent_ppo2.py:185][0m |          -0.0113 |           4.3129 |           3.8029 |
[32m[20221213 22:24:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 193.67
[32m[20221213 22:24:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.95
[32m[20221213 22:24:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:12 @agent_ppo2.py:143][0m Total time:       5.99 min
[32m[20221213 22:24:12 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 22:24:12 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 22:24:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:12 @agent_ppo2.py:185][0m |           0.0008 |          14.4400 |           3.8006 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0034 |          12.9585 |           3.8007 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0071 |          12.6485 |           3.8003 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0078 |          12.4320 |           3.7991 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0069 |          12.3809 |           3.7969 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0007 |          12.2408 |           3.7980 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0076 |          12.2417 |           3.7990 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0084 |          12.2260 |           3.8005 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0071 |          12.2064 |           3.7984 |
[32m[20221213 22:24:13 @agent_ppo2.py:185][0m |          -0.0144 |          12.1875 |           3.8000 |
[32m[20221213 22:24:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 36.73
[32m[20221213 22:24:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.99
[32m[20221213 22:24:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 234.01
[32m[20221213 22:24:13 @agent_ppo2.py:143][0m Total time:       6.01 min
[32m[20221213 22:24:13 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221213 22:24:13 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 22:24:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |           0.0024 |           7.9687 |           3.8278 |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |          -0.0055 |           7.1756 |           3.8247 |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |          -0.0071 |           6.8414 |           3.8236 |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |          -0.0015 |           6.7911 |           3.8226 |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |          -0.0061 |           6.7794 |           3.8222 |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |          -0.0039 |           6.8203 |           3.8233 |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |          -0.0058 |           6.7777 |           3.8229 |
[32m[20221213 22:24:14 @agent_ppo2.py:185][0m |          -0.0032 |           6.6446 |           3.8215 |
[32m[20221213 22:24:15 @agent_ppo2.py:185][0m |          -0.0021 |           6.5857 |           3.8221 |
[32m[20221213 22:24:15 @agent_ppo2.py:185][0m |          -0.0136 |           6.5773 |           3.8216 |
[32m[20221213 22:24:15 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.41
[32m[20221213 22:24:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.84
[32m[20221213 22:24:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.19
[32m[20221213 22:24:15 @agent_ppo2.py:143][0m Total time:       6.03 min
[32m[20221213 22:24:15 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 22:24:15 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 22:24:15 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 22:24:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:15 @agent_ppo2.py:185][0m |          -0.0015 |          10.5916 |           3.8882 |
[32m[20221213 22:24:15 @agent_ppo2.py:185][0m |          -0.0029 |           9.5560 |           3.8809 |
[32m[20221213 22:24:15 @agent_ppo2.py:185][0m |          -0.0044 |           9.5415 |           3.8832 |
[32m[20221213 22:24:15 @agent_ppo2.py:185][0m |          -0.0028 |           9.5107 |           3.8831 |
[32m[20221213 22:24:16 @agent_ppo2.py:185][0m |          -0.0052 |           9.4454 |           3.8792 |
[32m[20221213 22:24:16 @agent_ppo2.py:185][0m |          -0.0025 |           9.4736 |           3.8845 |
[32m[20221213 22:24:16 @agent_ppo2.py:185][0m |          -0.0043 |           9.4743 |           3.8833 |
[32m[20221213 22:24:16 @agent_ppo2.py:185][0m |          -0.0051 |           9.5093 |           3.8790 |
[32m[20221213 22:24:16 @agent_ppo2.py:185][0m |          -0.0039 |           9.3997 |           3.8829 |
[32m[20221213 22:24:16 @agent_ppo2.py:185][0m |          -0.0055 |           9.4171 |           3.8784 |
[32m[20221213 22:24:16 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:24:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:24:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.74
[32m[20221213 22:24:16 @agent_ppo2.py:143][0m Total time:       6.05 min
[32m[20221213 22:24:16 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221213 22:24:16 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 22:24:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |           0.0018 |          17.3716 |           3.8654 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0002 |          15.5671 |           3.8622 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0080 |          15.3941 |           3.8588 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0036 |          15.2246 |           3.8577 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0063 |          15.3021 |           3.8539 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0095 |          15.0922 |           3.8508 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0059 |          15.2594 |           3.8507 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0118 |          14.9387 |           3.8482 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0135 |          14.9095 |           3.8475 |
[32m[20221213 22:24:17 @agent_ppo2.py:185][0m |          -0.0101 |          14.8463 |           3.8483 |
[32m[20221213 22:24:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 34.29
[32m[20221213 22:24:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 63.36
[32m[20221213 22:24:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:18 @agent_ppo2.py:143][0m Total time:       6.08 min
[32m[20221213 22:24:18 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 22:24:18 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 22:24:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:18 @agent_ppo2.py:185][0m |           0.0035 |           7.8654 |           3.8210 |
[32m[20221213 22:24:18 @agent_ppo2.py:185][0m |          -0.0089 |           6.3442 |           3.8151 |
[32m[20221213 22:24:18 @agent_ppo2.py:185][0m |          -0.0053 |           6.1460 |           3.8114 |
[32m[20221213 22:24:18 @agent_ppo2.py:185][0m |          -0.0081 |           6.1603 |           3.8081 |
[32m[20221213 22:24:18 @agent_ppo2.py:185][0m |          -0.0095 |           6.1244 |           3.8068 |
[32m[20221213 22:24:18 @agent_ppo2.py:185][0m |          -0.0088 |           6.0178 |           3.8081 |
[32m[20221213 22:24:18 @agent_ppo2.py:185][0m |          -0.0001 |           6.7644 |           3.8086 |
[32m[20221213 22:24:19 @agent_ppo2.py:185][0m |          -0.0112 |           6.0839 |           3.8073 |
[32m[20221213 22:24:19 @agent_ppo2.py:185][0m |          -0.0087 |           6.0003 |           3.8071 |
[32m[20221213 22:24:19 @agent_ppo2.py:185][0m |          -0.0171 |           5.9909 |           3.8048 |
[32m[20221213 22:24:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:24:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.12
[32m[20221213 22:24:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.19
[32m[20221213 22:24:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:19 @agent_ppo2.py:143][0m Total time:       6.10 min
[32m[20221213 22:24:19 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221213 22:24:19 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 22:24:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:19 @agent_ppo2.py:185][0m |          -0.0004 |          13.0287 |           3.8143 |
[32m[20221213 22:24:19 @agent_ppo2.py:185][0m |          -0.0015 |          11.4783 |           3.8087 |
[32m[20221213 22:24:19 @agent_ppo2.py:185][0m |          -0.0014 |          11.3523 |           3.8023 |
[32m[20221213 22:24:20 @agent_ppo2.py:185][0m |          -0.0073 |          11.1827 |           3.8026 |
[32m[20221213 22:24:20 @agent_ppo2.py:185][0m |          -0.0108 |          11.1052 |           3.7980 |
[32m[20221213 22:24:20 @agent_ppo2.py:185][0m |          -0.0016 |          11.3988 |           3.7976 |
[32m[20221213 22:24:20 @agent_ppo2.py:185][0m |           0.0013 |          11.7284 |           3.7985 |
[32m[20221213 22:24:20 @agent_ppo2.py:185][0m |          -0.0054 |          10.9761 |           3.7965 |
[32m[20221213 22:24:20 @agent_ppo2.py:185][0m |          -0.0107 |          10.9376 |           3.7975 |
[32m[20221213 22:24:20 @agent_ppo2.py:185][0m |          -0.0180 |          10.9772 |           3.7962 |
[32m[20221213 22:24:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 35.38
[32m[20221213 22:24:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.89
[32m[20221213 22:24:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.12
[32m[20221213 22:24:20 @agent_ppo2.py:143][0m Total time:       6.12 min
[32m[20221213 22:24:20 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 22:24:20 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 22:24:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0011 |           6.7827 |           3.8727 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0018 |           5.5419 |           3.8735 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0074 |           5.4709 |           3.8730 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0038 |           5.3206 |           3.8733 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0040 |           5.2392 |           3.8754 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0076 |           5.1992 |           3.8747 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0068 |           5.1258 |           3.8772 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0076 |           5.1604 |           3.8788 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |          -0.0023 |           5.0870 |           3.8814 |
[32m[20221213 22:24:21 @agent_ppo2.py:185][0m |           0.0024 |           5.2700 |           3.8802 |
[32m[20221213 22:24:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.00
[32m[20221213 22:24:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.23
[32m[20221213 22:24:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.53
[32m[20221213 22:24:22 @agent_ppo2.py:143][0m Total time:       6.15 min
[32m[20221213 22:24:22 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221213 22:24:22 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 22:24:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:22 @agent_ppo2.py:185][0m |          -0.0022 |          11.6899 |           3.9092 |
[32m[20221213 22:24:22 @agent_ppo2.py:185][0m |           0.0004 |          10.6225 |           3.9056 |
[32m[20221213 22:24:22 @agent_ppo2.py:185][0m |          -0.0060 |          10.1367 |           3.9030 |
[32m[20221213 22:24:22 @agent_ppo2.py:185][0m |          -0.0067 |           9.8065 |           3.9026 |
[32m[20221213 22:24:22 @agent_ppo2.py:185][0m |          -0.0075 |           9.6900 |           3.9007 |
[32m[20221213 22:24:22 @agent_ppo2.py:185][0m |          -0.0071 |           9.5284 |           3.9012 |
[32m[20221213 22:24:23 @agent_ppo2.py:185][0m |          -0.0068 |           9.4529 |           3.8999 |
[32m[20221213 22:24:23 @agent_ppo2.py:185][0m |          -0.0047 |           9.5272 |           3.8966 |
[32m[20221213 22:24:23 @agent_ppo2.py:185][0m |          -0.0059 |           9.4393 |           3.8985 |
[32m[20221213 22:24:23 @agent_ppo2.py:185][0m |          -0.0081 |           9.2601 |           3.8960 |
[32m[20221213 22:24:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 23.47
[32m[20221213 22:24:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 117.35
[32m[20221213 22:24:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.56
[32m[20221213 22:24:23 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 268.56
[32m[20221213 22:24:23 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 268.56
[32m[20221213 22:24:23 @agent_ppo2.py:143][0m Total time:       6.17 min
[32m[20221213 22:24:23 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 22:24:23 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 22:24:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:23 @agent_ppo2.py:185][0m |           0.0021 |          14.8709 |           3.8829 |
[32m[20221213 22:24:23 @agent_ppo2.py:185][0m |          -0.0046 |          14.0839 |           3.8754 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |           0.0028 |          15.6903 |           3.8709 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |           0.0071 |          15.1251 |           3.8707 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |          -0.0105 |          13.6773 |           3.8679 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |          -0.0030 |          13.7118 |           3.8690 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |          -0.0097 |          13.5753 |           3.8714 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |          -0.0124 |          13.5300 |           3.8730 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |          -0.0115 |          13.5045 |           3.8734 |
[32m[20221213 22:24:24 @agent_ppo2.py:185][0m |          -0.0105 |          13.4743 |           3.8733 |
[32m[20221213 22:24:24 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:24:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.30
[32m[20221213 22:24:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 146.14
[32m[20221213 22:24:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.76
[32m[20221213 22:24:24 @agent_ppo2.py:143][0m Total time:       6.19 min
[32m[20221213 22:24:24 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221213 22:24:24 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 22:24:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |           0.0011 |          12.2590 |           3.8846 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0006 |          11.5605 |           3.8864 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0079 |          11.4488 |           3.8871 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0000 |          11.3101 |           3.8849 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0071 |          11.0683 |           3.8844 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0106 |          11.0169 |           3.8859 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0073 |          10.9349 |           3.8894 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0043 |          10.8542 |           3.8885 |
[32m[20221213 22:24:25 @agent_ppo2.py:185][0m |          -0.0072 |          10.9053 |           3.8914 |
[32m[20221213 22:24:26 @agent_ppo2.py:185][0m |          -0.0088 |          10.7449 |           3.8945 |
[32m[20221213 22:24:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.18
[32m[20221213 22:24:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 171.50
[32m[20221213 22:24:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.25
[32m[20221213 22:24:26 @agent_ppo2.py:143][0m Total time:       6.21 min
[32m[20221213 22:24:26 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 22:24:26 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 22:24:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:26 @agent_ppo2.py:185][0m |          -0.0050 |           6.3162 |           3.8788 |
[32m[20221213 22:24:26 @agent_ppo2.py:185][0m |          -0.0066 |           4.4766 |           3.8796 |
[32m[20221213 22:24:26 @agent_ppo2.py:185][0m |           0.0027 |           4.6020 |           3.8796 |
[32m[20221213 22:24:26 @agent_ppo2.py:185][0m |          -0.0094 |           4.2881 |           3.8832 |
[32m[20221213 22:24:26 @agent_ppo2.py:185][0m |          -0.0050 |           4.1015 |           3.8817 |
[32m[20221213 22:24:27 @agent_ppo2.py:185][0m |          -0.0050 |           4.0675 |           3.8867 |
[32m[20221213 22:24:27 @agent_ppo2.py:185][0m |          -0.0140 |           3.9889 |           3.8847 |
[32m[20221213 22:24:27 @agent_ppo2.py:185][0m |          -0.0062 |           3.9990 |           3.8863 |
[32m[20221213 22:24:27 @agent_ppo2.py:185][0m |          -0.0073 |           3.9623 |           3.8885 |
[32m[20221213 22:24:27 @agent_ppo2.py:185][0m |          -0.0093 |           3.9125 |           3.8901 |
[32m[20221213 22:24:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.44
[32m[20221213 22:24:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 211.18
[32m[20221213 22:24:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.19
[32m[20221213 22:24:27 @agent_ppo2.py:143][0m Total time:       6.24 min
[32m[20221213 22:24:27 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221213 22:24:27 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 22:24:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:27 @agent_ppo2.py:185][0m |           0.0017 |           8.2617 |           3.9827 |
[32m[20221213 22:24:27 @agent_ppo2.py:185][0m |           0.0029 |           7.3172 |           3.9779 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |           0.0006 |           7.2745 |           3.9752 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |          -0.0055 |           7.1730 |           3.9764 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |          -0.0026 |           7.4452 |           3.9767 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |           0.0010 |           7.2300 |           3.9749 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |          -0.0029 |           7.1171 |           3.9807 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |          -0.0114 |           7.0610 |           3.9799 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |          -0.0036 |           7.0466 |           3.9821 |
[32m[20221213 22:24:28 @agent_ppo2.py:185][0m |          -0.0079 |           7.0593 |           3.9825 |
[32m[20221213 22:24:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.22
[32m[20221213 22:24:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.71
[32m[20221213 22:24:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.45
[32m[20221213 22:24:28 @agent_ppo2.py:143][0m Total time:       6.26 min
[32m[20221213 22:24:28 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 22:24:28 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 22:24:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:24:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0067 |          13.9285 |           4.0380 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |           0.0033 |          11.1955 |           4.0330 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0086 |          10.9099 |           4.0283 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0063 |          10.7373 |           4.0254 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0065 |          10.7051 |           4.0235 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0097 |          10.6100 |           4.0237 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0147 |          10.5261 |           4.0188 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0023 |          10.4506 |           4.0187 |
[32m[20221213 22:24:29 @agent_ppo2.py:185][0m |          -0.0144 |          10.4530 |           4.0188 |
[32m[20221213 22:24:30 @agent_ppo2.py:185][0m |          -0.0096 |          10.3971 |           4.0153 |
[32m[20221213 22:24:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 6.97
[32m[20221213 22:24:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 21.77
[32m[20221213 22:24:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 157.56
[32m[20221213 22:24:30 @agent_ppo2.py:143][0m Total time:       6.28 min
[32m[20221213 22:24:30 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221213 22:24:30 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 22:24:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:30 @agent_ppo2.py:185][0m |           0.0004 |          16.0788 |           3.9742 |
[32m[20221213 22:24:30 @agent_ppo2.py:185][0m |          -0.0064 |          15.5911 |           3.9659 |
[32m[20221213 22:24:30 @agent_ppo2.py:185][0m |           0.0019 |          15.4416 |           3.9628 |
[32m[20221213 22:24:30 @agent_ppo2.py:185][0m |          -0.0075 |          15.2361 |           3.9629 |
[32m[20221213 22:24:30 @agent_ppo2.py:185][0m |          -0.0048 |          15.1878 |           3.9628 |
[32m[20221213 22:24:31 @agent_ppo2.py:185][0m |          -0.0079 |          15.0963 |           3.9618 |
[32m[20221213 22:24:31 @agent_ppo2.py:185][0m |          -0.0048 |          15.1230 |           3.9634 |
[32m[20221213 22:24:31 @agent_ppo2.py:185][0m |          -0.0072 |          15.0863 |           3.9634 |
[32m[20221213 22:24:31 @agent_ppo2.py:185][0m |          -0.0115 |          14.9772 |           3.9639 |
[32m[20221213 22:24:31 @agent_ppo2.py:185][0m |          -0.0101 |          15.0559 |           3.9612 |
[32m[20221213 22:24:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:24:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.38
[32m[20221213 22:24:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.26
[32m[20221213 22:24:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.01
[32m[20221213 22:24:31 @agent_ppo2.py:143][0m Total time:       6.30 min
[32m[20221213 22:24:31 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 22:24:31 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 22:24:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:31 @agent_ppo2.py:185][0m |          -0.0021 |           8.7300 |           3.9462 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0082 |           7.6715 |           3.9369 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0102 |           7.5968 |           3.9369 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |           0.0010 |           7.6933 |           3.9373 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0105 |           7.4550 |           3.9372 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0008 |           7.3738 |           3.9364 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0037 |           7.3976 |           3.9361 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0098 |           7.3422 |           3.9350 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0070 |           7.3564 |           3.9346 |
[32m[20221213 22:24:32 @agent_ppo2.py:185][0m |          -0.0048 |           7.2619 |           3.9358 |
[32m[20221213 22:24:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 200.51
[32m[20221213 22:24:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 224.64
[32m[20221213 22:24:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.65
[32m[20221213 22:24:32 @agent_ppo2.py:143][0m Total time:       6.33 min
[32m[20221213 22:24:32 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221213 22:24:32 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 22:24:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0005 |          16.7191 |           3.9897 |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0026 |          15.0738 |           3.9884 |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0062 |          15.0103 |           3.9865 |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0057 |          14.7938 |           3.9850 |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0031 |          14.7191 |           3.9859 |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0065 |          14.6673 |           3.9868 |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0071 |          14.6205 |           3.9871 |
[32m[20221213 22:24:33 @agent_ppo2.py:185][0m |          -0.0080 |          14.5312 |           3.9865 |
[32m[20221213 22:24:34 @agent_ppo2.py:185][0m |          -0.0078 |          14.4884 |           3.9848 |
[32m[20221213 22:24:34 @agent_ppo2.py:185][0m |          -0.0054 |          14.5383 |           3.9867 |
[32m[20221213 22:24:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.77
[32m[20221213 22:24:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.68
[32m[20221213 22:24:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.94
[32m[20221213 22:24:34 @agent_ppo2.py:143][0m Total time:       6.35 min
[32m[20221213 22:24:34 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 22:24:34 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 22:24:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:34 @agent_ppo2.py:185][0m |           0.0008 |          10.4984 |           3.9963 |
[32m[20221213 22:24:34 @agent_ppo2.py:185][0m |          -0.0053 |           9.8581 |           3.9930 |
[32m[20221213 22:24:34 @agent_ppo2.py:185][0m |          -0.0079 |           9.4581 |           3.9872 |
[32m[20221213 22:24:34 @agent_ppo2.py:185][0m |          -0.0086 |           9.3838 |           3.9876 |
[32m[20221213 22:24:35 @agent_ppo2.py:185][0m |          -0.0068 |           9.2720 |           3.9876 |
[32m[20221213 22:24:35 @agent_ppo2.py:185][0m |          -0.0080 |           9.3294 |           3.9848 |
[32m[20221213 22:24:35 @agent_ppo2.py:185][0m |          -0.0086 |           9.1922 |           3.9850 |
[32m[20221213 22:24:35 @agent_ppo2.py:185][0m |          -0.0114 |           9.1574 |           3.9842 |
[32m[20221213 22:24:35 @agent_ppo2.py:185][0m |          -0.0054 |           9.2708 |           3.9824 |
[32m[20221213 22:24:35 @agent_ppo2.py:185][0m |          -0.0044 |           9.2129 |           3.9843 |
[32m[20221213 22:24:35 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.46
[32m[20221213 22:24:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.54
[32m[20221213 22:24:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.34
[32m[20221213 22:24:35 @agent_ppo2.py:143][0m Total time:       6.37 min
[32m[20221213 22:24:35 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221213 22:24:35 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 22:24:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0052 |           9.2705 |           4.0196 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0025 |           7.7658 |           4.0166 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0063 |           7.5566 |           4.0165 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0034 |           7.4540 |           4.0158 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0063 |           7.4640 |           4.0156 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0057 |           7.4153 |           4.0157 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0044 |           7.4196 |           4.0166 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0035 |           7.3618 |           4.0168 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0044 |           7.6171 |           4.0179 |
[32m[20221213 22:24:36 @agent_ppo2.py:185][0m |          -0.0045 |           7.4007 |           4.0201 |
[32m[20221213 22:24:36 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:24:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.08
[32m[20221213 22:24:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.51
[32m[20221213 22:24:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.28
[32m[20221213 22:24:37 @agent_ppo2.py:143][0m Total time:       6.39 min
[32m[20221213 22:24:37 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 22:24:37 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 22:24:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:24:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:37 @agent_ppo2.py:185][0m |          -0.0022 |          15.0311 |           3.9746 |
[32m[20221213 22:24:37 @agent_ppo2.py:185][0m |          -0.0019 |          13.8056 |           3.9659 |
[32m[20221213 22:24:37 @agent_ppo2.py:185][0m |          -0.0028 |          14.0478 |           3.9581 |
[32m[20221213 22:24:37 @agent_ppo2.py:185][0m |          -0.0049 |          13.6260 |           3.9564 |
[32m[20221213 22:24:37 @agent_ppo2.py:185][0m |          -0.0071 |          13.5863 |           3.9521 |
[32m[20221213 22:24:37 @agent_ppo2.py:185][0m |          -0.0038 |          13.5192 |           3.9501 |
[32m[20221213 22:24:37 @agent_ppo2.py:185][0m |          -0.0038 |          13.5368 |           3.9463 |
[32m[20221213 22:24:38 @agent_ppo2.py:185][0m |          -0.0076 |          13.9839 |           3.9432 |
[32m[20221213 22:24:38 @agent_ppo2.py:185][0m |          -0.0096 |          13.4983 |           3.9386 |
[32m[20221213 22:24:38 @agent_ppo2.py:185][0m |          -0.0106 |          13.4272 |           3.9388 |
[32m[20221213 22:24:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.29
[32m[20221213 22:24:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 204.08
[32m[20221213 22:24:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.14
[32m[20221213 22:24:38 @agent_ppo2.py:143][0m Total time:       6.42 min
[32m[20221213 22:24:38 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221213 22:24:38 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 22:24:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:38 @agent_ppo2.py:185][0m |          -0.0035 |           6.1471 |           3.9074 |
[32m[20221213 22:24:38 @agent_ppo2.py:185][0m |          -0.0064 |           5.3733 |           3.9059 |
[32m[20221213 22:24:38 @agent_ppo2.py:185][0m |          -0.0044 |           5.2837 |           3.9116 |
[32m[20221213 22:24:39 @agent_ppo2.py:185][0m |          -0.0029 |           5.1798 |           3.9122 |
[32m[20221213 22:24:39 @agent_ppo2.py:185][0m |          -0.0013 |           5.3713 |           3.9162 |
[32m[20221213 22:24:39 @agent_ppo2.py:185][0m |          -0.0055 |           5.1090 |           3.9132 |
[32m[20221213 22:24:39 @agent_ppo2.py:185][0m |          -0.0016 |           5.1326 |           3.9154 |
[32m[20221213 22:24:39 @agent_ppo2.py:185][0m |          -0.0077 |           5.0187 |           3.9153 |
[32m[20221213 22:24:39 @agent_ppo2.py:185][0m |          -0.0061 |           4.9642 |           3.9155 |
[32m[20221213 22:24:39 @agent_ppo2.py:185][0m |          -0.0078 |           4.9311 |           3.9195 |
[32m[20221213 22:24:39 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:24:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:24:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:24:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.64
[32m[20221213 22:24:39 @agent_ppo2.py:143][0m Total time:       6.44 min
[32m[20221213 22:24:39 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 22:24:39 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 22:24:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0045 |           9.3706 |           3.9812 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0057 |           8.3614 |           3.9761 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0051 |           8.2166 |           3.9712 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0097 |           8.0363 |           3.9682 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0118 |           7.9577 |           3.9650 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0107 |           7.9084 |           3.9618 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0116 |           7.8696 |           3.9601 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0146 |           7.8024 |           3.9627 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0111 |           7.7009 |           3.9593 |
[32m[20221213 22:24:40 @agent_ppo2.py:185][0m |          -0.0116 |           7.6792 |           3.9582 |
[32m[20221213 22:24:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 41.49
[32m[20221213 22:24:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.67
[32m[20221213 22:24:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.34
[32m[20221213 22:24:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 280.34
[32m[20221213 22:24:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 280.34
[32m[20221213 22:24:41 @agent_ppo2.py:143][0m Total time:       6.46 min
[32m[20221213 22:24:41 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221213 22:24:41 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 22:24:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:24:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:41 @agent_ppo2.py:185][0m |          -0.0031 |          12.3353 |           4.0253 |
[32m[20221213 22:24:41 @agent_ppo2.py:185][0m |          -0.0028 |          12.0051 |           4.0194 |
[32m[20221213 22:24:41 @agent_ppo2.py:185][0m |          -0.0053 |          11.8982 |           4.0199 |
[32m[20221213 22:24:41 @agent_ppo2.py:185][0m |          -0.0064 |          11.8574 |           4.0202 |
[32m[20221213 22:24:41 @agent_ppo2.py:185][0m |          -0.0059 |          11.9160 |           4.0214 |
[32m[20221213 22:24:41 @agent_ppo2.py:185][0m |          -0.0058 |          11.8702 |           4.0173 |
[32m[20221213 22:24:41 @agent_ppo2.py:185][0m |          -0.0004 |          12.0011 |           4.0184 |
[32m[20221213 22:24:42 @agent_ppo2.py:185][0m |          -0.0092 |          11.6678 |           4.0202 |
[32m[20221213 22:24:42 @agent_ppo2.py:185][0m |          -0.0094 |          11.6096 |           4.0218 |
[32m[20221213 22:24:42 @agent_ppo2.py:185][0m |          -0.0061 |          11.5190 |           4.0200 |
[32m[20221213 22:24:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:24:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.91
[32m[20221213 22:24:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.94
[32m[20221213 22:24:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.20
[32m[20221213 22:24:42 @agent_ppo2.py:143][0m Total time:       6.48 min
[32m[20221213 22:24:42 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 22:24:42 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 22:24:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:24:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:42 @agent_ppo2.py:185][0m |          -0.0050 |           7.3498 |           3.9861 |
[32m[20221213 22:24:42 @agent_ppo2.py:185][0m |           0.0020 |           5.6044 |           3.9840 |
[32m[20221213 22:24:42 @agent_ppo2.py:185][0m |          -0.0059 |           5.3587 |           3.9809 |
[32m[20221213 22:24:42 @agent_ppo2.py:185][0m |          -0.0024 |           5.2194 |           3.9809 |
[32m[20221213 22:24:43 @agent_ppo2.py:185][0m |          -0.0156 |           5.0735 |           3.9810 |
[32m[20221213 22:24:43 @agent_ppo2.py:185][0m |           0.0003 |           4.9367 |           3.9802 |
[32m[20221213 22:24:43 @agent_ppo2.py:185][0m |          -0.0003 |           4.7798 |           3.9795 |
[32m[20221213 22:24:43 @agent_ppo2.py:185][0m |          -0.0055 |           4.8505 |           3.9759 |
[32m[20221213 22:24:43 @agent_ppo2.py:185][0m |          -0.0054 |           4.6929 |           3.9771 |
[32m[20221213 22:24:43 @agent_ppo2.py:185][0m |          -0.0055 |           4.6495 |           3.9777 |
[32m[20221213 22:24:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:24:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.89
[32m[20221213 22:24:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.62
[32m[20221213 22:24:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.95
[32m[20221213 22:24:43 @agent_ppo2.py:143][0m Total time:       6.50 min
[32m[20221213 22:24:43 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221213 22:24:43 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 22:24:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:24:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:43 @agent_ppo2.py:185][0m |           0.0052 |          18.3816 |           3.9118 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |           0.0006 |          17.4195 |           3.9086 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0070 |          17.1992 |           3.9051 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0076 |          17.1690 |           3.9052 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0091 |          16.9970 |           3.9044 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0119 |          17.0118 |           3.9011 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0070 |          16.9676 |           3.9001 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0028 |          17.0998 |           3.8997 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0034 |          16.9171 |           3.8999 |
[32m[20221213 22:24:44 @agent_ppo2.py:185][0m |          -0.0097 |          16.8425 |           3.8989 |
[32m[20221213 22:24:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:24:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.26
[32m[20221213 22:24:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 215.71
[32m[20221213 22:24:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.77
[32m[20221213 22:24:44 @agent_ppo2.py:143][0m Total time:       6.52 min
[32m[20221213 22:24:44 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 22:24:44 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 22:24:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0018 |           7.2604 |           3.9440 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0063 |           5.8587 |           3.9426 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0047 |           5.6998 |           3.9384 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0079 |           5.6707 |           3.9399 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |           0.0046 |           5.8798 |           3.9383 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0047 |           5.6570 |           3.9383 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0011 |           5.9576 |           3.9393 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0121 |           5.5413 |           3.9377 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0021 |           5.5124 |           3.9378 |
[32m[20221213 22:24:45 @agent_ppo2.py:185][0m |          -0.0100 |           5.4779 |           3.9382 |
[32m[20221213 22:24:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:24:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.66
[32m[20221213 22:24:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.68
[32m[20221213 22:24:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.70
[32m[20221213 22:24:46 @agent_ppo2.py:143][0m Total time:       6.55 min
[32m[20221213 22:24:46 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221213 22:24:46 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 22:24:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:24:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:46 @agent_ppo2.py:185][0m |          -0.0015 |           9.4658 |           3.9636 |
[32m[20221213 22:24:46 @agent_ppo2.py:185][0m |          -0.0037 |           8.4485 |           3.9596 |
[32m[20221213 22:24:46 @agent_ppo2.py:185][0m |          -0.0044 |           8.1995 |           3.9562 |
[32m[20221213 22:24:46 @agent_ppo2.py:185][0m |          -0.0137 |           8.0344 |           3.9557 |
[32m[20221213 22:24:46 @agent_ppo2.py:185][0m |          -0.0110 |           7.9495 |           3.9565 |
[32m[20221213 22:24:46 @agent_ppo2.py:185][0m |          -0.0133 |           7.8722 |           3.9562 |
[32m[20221213 22:24:47 @agent_ppo2.py:185][0m |          -0.0074 |           7.7818 |           3.9589 |
[32m[20221213 22:24:47 @agent_ppo2.py:185][0m |          -0.0139 |           7.8126 |           3.9580 |
[32m[20221213 22:24:47 @agent_ppo2.py:185][0m |          -0.0058 |           7.7660 |           3.9603 |
[32m[20221213 22:24:47 @agent_ppo2.py:185][0m |          -0.0107 |           7.7505 |           3.9607 |
[32m[20221213 22:24:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:24:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 33.87
[32m[20221213 22:24:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.35
[32m[20221213 22:24:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:47 @agent_ppo2.py:143][0m Total time:       6.57 min
[32m[20221213 22:24:47 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221213 22:24:47 @agent_ppo2.py:121][0m #------------------------ Iteration 304 --------------------------#
[32m[20221213 22:24:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:24:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:47 @agent_ppo2.py:185][0m |           0.0016 |           7.3476 |           3.9807 |
[32m[20221213 22:24:47 @agent_ppo2.py:185][0m |          -0.0024 |           5.8879 |           3.9734 |
[32m[20221213 22:24:47 @agent_ppo2.py:185][0m |          -0.0006 |           5.6936 |           3.9674 |
[32m[20221213 22:24:48 @agent_ppo2.py:185][0m |           0.0075 |           6.4352 |           3.9662 |
[32m[20221213 22:24:48 @agent_ppo2.py:185][0m |          -0.0113 |           5.6787 |           3.9627 |
[32m[20221213 22:24:48 @agent_ppo2.py:185][0m |          -0.0038 |           5.5674 |           3.9594 |
[32m[20221213 22:24:48 @agent_ppo2.py:185][0m |          -0.0079 |           5.7128 |           3.9609 |
[32m[20221213 22:24:48 @agent_ppo2.py:185][0m |          -0.0067 |           5.5375 |           3.9552 |
[32m[20221213 22:24:48 @agent_ppo2.py:185][0m |          -0.0082 |           5.4973 |           3.9581 |
[32m[20221213 22:24:48 @agent_ppo2.py:185][0m |          -0.0133 |           5.6639 |           3.9566 |
[32m[20221213 22:24:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:24:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.34
[32m[20221213 22:24:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.76
[32m[20221213 22:24:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.36
[32m[20221213 22:24:48 @agent_ppo2.py:143][0m Total time:       6.59 min
[32m[20221213 22:24:48 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221213 22:24:48 @agent_ppo2.py:121][0m #------------------------ Iteration 305 --------------------------#
[32m[20221213 22:24:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |           0.0056 |          13.3254 |           3.9232 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |          -0.0028 |          11.2186 |           3.9167 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |          -0.0109 |          11.1125 |           3.9136 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |          -0.0047 |          11.0280 |           3.9120 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |          -0.0081 |          10.9203 |           3.9101 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |           0.0019 |          11.5071 |           3.9080 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |          -0.0076 |          10.7797 |           3.9067 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |          -0.0078 |          10.6786 |           3.9066 |
[32m[20221213 22:24:49 @agent_ppo2.py:185][0m |          -0.0121 |          10.7026 |           3.9068 |
[32m[20221213 22:24:50 @agent_ppo2.py:185][0m |          -0.0051 |          11.0294 |           3.9066 |
[32m[20221213 22:24:50 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 22:24:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.95
[32m[20221213 22:24:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.86
[32m[20221213 22:24:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.84
[32m[20221213 22:24:50 @agent_ppo2.py:143][0m Total time:       6.61 min
[32m[20221213 22:24:50 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221213 22:24:50 @agent_ppo2.py:121][0m #------------------------ Iteration 306 --------------------------#
[32m[20221213 22:24:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:24:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:50 @agent_ppo2.py:185][0m |           0.0027 |          11.3467 |           4.0169 |
[32m[20221213 22:24:50 @agent_ppo2.py:185][0m |          -0.0020 |          10.6082 |           4.0125 |
[32m[20221213 22:24:50 @agent_ppo2.py:185][0m |          -0.0036 |          10.4766 |           4.0115 |
[32m[20221213 22:24:50 @agent_ppo2.py:185][0m |          -0.0091 |          10.4231 |           4.0096 |
[32m[20221213 22:24:50 @agent_ppo2.py:185][0m |          -0.0075 |          10.3116 |           4.0087 |
[32m[20221213 22:24:51 @agent_ppo2.py:185][0m |          -0.0055 |          10.2746 |           4.0077 |
[32m[20221213 22:24:51 @agent_ppo2.py:185][0m |          -0.0081 |          10.1601 |           4.0083 |
[32m[20221213 22:24:51 @agent_ppo2.py:185][0m |          -0.0010 |          10.1417 |           4.0084 |
[32m[20221213 22:24:51 @agent_ppo2.py:185][0m |          -0.0087 |          10.2878 |           4.0102 |
[32m[20221213 22:24:51 @agent_ppo2.py:185][0m |          -0.0064 |          10.1642 |           4.0097 |
[32m[20221213 22:24:51 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:24:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.33
[32m[20221213 22:24:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 174.52
[32m[20221213 22:24:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.08
[32m[20221213 22:24:51 @agent_ppo2.py:143][0m Total time:       6.64 min
[32m[20221213 22:24:51 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221213 22:24:51 @agent_ppo2.py:121][0m #------------------------ Iteration 307 --------------------------#
[32m[20221213 22:24:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:51 @agent_ppo2.py:185][0m |           0.0050 |          19.2443 |           3.9215 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0057 |          18.0419 |           3.9145 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0092 |          17.8211 |           3.9163 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0113 |          17.8937 |           3.9168 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |           0.0015 |          19.8333 |           3.9162 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0104 |          17.9366 |           3.9145 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0082 |          17.7628 |           3.9162 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0112 |          17.7485 |           3.9168 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0130 |          17.7288 |           3.9169 |
[32m[20221213 22:24:52 @agent_ppo2.py:185][0m |          -0.0020 |          18.7745 |           3.9172 |
[32m[20221213 22:24:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:24:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.94
[32m[20221213 22:24:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.89
[32m[20221213 22:24:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:52 @agent_ppo2.py:143][0m Total time:       6.66 min
[32m[20221213 22:24:52 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221213 22:24:52 @agent_ppo2.py:121][0m #------------------------ Iteration 308 --------------------------#
[32m[20221213 22:24:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:53 @agent_ppo2.py:185][0m |           0.0024 |           7.4085 |           3.9519 |
[32m[20221213 22:24:53 @agent_ppo2.py:185][0m |          -0.0066 |           4.5522 |           3.9513 |
[32m[20221213 22:24:53 @agent_ppo2.py:185][0m |          -0.0085 |           4.3935 |           3.9490 |
[32m[20221213 22:24:53 @agent_ppo2.py:185][0m |           0.0013 |           4.4078 |           3.9499 |
[32m[20221213 22:24:53 @agent_ppo2.py:185][0m |          -0.0110 |           4.2704 |           3.9513 |
[32m[20221213 22:24:53 @agent_ppo2.py:185][0m |          -0.0057 |           4.1930 |           3.9511 |
[32m[20221213 22:24:53 @agent_ppo2.py:185][0m |          -0.0021 |           4.1546 |           3.9525 |
[32m[20221213 22:24:54 @agent_ppo2.py:185][0m |          -0.0082 |           4.1408 |           3.9519 |
[32m[20221213 22:24:54 @agent_ppo2.py:185][0m |          -0.0059 |           4.1468 |           3.9524 |
[32m[20221213 22:24:54 @agent_ppo2.py:185][0m |          -0.0139 |           4.1055 |           3.9515 |
[32m[20221213 22:24:54 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:24:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.00
[32m[20221213 22:24:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.32
[32m[20221213 22:24:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:24:54 @agent_ppo2.py:143][0m Total time:       6.68 min
[32m[20221213 22:24:54 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221213 22:24:54 @agent_ppo2.py:121][0m #------------------------ Iteration 309 --------------------------#
[32m[20221213 22:24:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:54 @agent_ppo2.py:185][0m |          -0.0005 |           8.0575 |           3.9786 |
[32m[20221213 22:24:54 @agent_ppo2.py:185][0m |          -0.0073 |           6.8343 |           3.9739 |
[32m[20221213 22:24:54 @agent_ppo2.py:185][0m |          -0.0095 |           6.7768 |           3.9703 |
[32m[20221213 22:24:55 @agent_ppo2.py:185][0m |          -0.0087 |           6.7139 |           3.9698 |
[32m[20221213 22:24:55 @agent_ppo2.py:185][0m |          -0.0068 |           6.7293 |           3.9739 |
[32m[20221213 22:24:55 @agent_ppo2.py:185][0m |          -0.0068 |           6.6282 |           3.9728 |
[32m[20221213 22:24:55 @agent_ppo2.py:185][0m |          -0.0078 |           6.6378 |           3.9731 |
[32m[20221213 22:24:55 @agent_ppo2.py:185][0m |          -0.0110 |           6.6185 |           3.9746 |
[32m[20221213 22:24:55 @agent_ppo2.py:185][0m |          -0.0058 |           6.6226 |           3.9745 |
[32m[20221213 22:24:55 @agent_ppo2.py:185][0m |          -0.0075 |           6.6516 |           3.9748 |
[32m[20221213 22:24:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:24:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.25
[32m[20221213 22:24:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.18
[32m[20221213 22:24:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 11.39
[32m[20221213 22:24:55 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221213 22:24:55 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221213 22:24:55 @agent_ppo2.py:121][0m #------------------------ Iteration 310 --------------------------#
[32m[20221213 22:24:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:24:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0033 |          10.4493 |           4.0332 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0075 |           8.4289 |           4.0305 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0065 |           8.2991 |           4.0329 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0099 |           8.1850 |           4.0322 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0033 |           8.1051 |           4.0325 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0039 |           8.0217 |           4.0338 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0100 |           8.0325 |           4.0333 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0074 |           7.9154 |           4.0347 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0051 |           7.9231 |           4.0345 |
[32m[20221213 22:24:56 @agent_ppo2.py:185][0m |          -0.0099 |           7.9340 |           4.0347 |
[32m[20221213 22:24:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:24:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 17.67
[32m[20221213 22:24:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.36
[32m[20221213 22:24:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.50
[32m[20221213 22:24:56 @agent_ppo2.py:143][0m Total time:       6.73 min
[32m[20221213 22:24:56 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221213 22:24:57 @agent_ppo2.py:121][0m #------------------------ Iteration 311 --------------------------#
[32m[20221213 22:24:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:24:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |           0.0002 |           9.8619 |           4.0426 |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |          -0.0039 |           9.1887 |           4.0346 |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |           0.0095 |          10.4905 |           4.0311 |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |          -0.0046 |           9.2500 |           4.0335 |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |          -0.0097 |           8.9105 |           4.0291 |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |          -0.0062 |           8.8936 |           4.0252 |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |          -0.0131 |           8.8774 |           4.0246 |
[32m[20221213 22:24:57 @agent_ppo2.py:185][0m |          -0.0084 |           8.8319 |           4.0236 |
[32m[20221213 22:24:58 @agent_ppo2.py:185][0m |          -0.0040 |           9.0670 |           4.0237 |
[32m[20221213 22:24:58 @agent_ppo2.py:185][0m |          -0.0086 |           8.7745 |           4.0223 |
[32m[20221213 22:24:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:24:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 21.32
[32m[20221213 22:24:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.26
[32m[20221213 22:24:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.09
[32m[20221213 22:24:58 @agent_ppo2.py:143][0m Total time:       6.75 min
[32m[20221213 22:24:58 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221213 22:24:58 @agent_ppo2.py:121][0m #------------------------ Iteration 312 --------------------------#
[32m[20221213 22:24:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:24:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:58 @agent_ppo2.py:185][0m |          -0.0038 |           5.6530 |           4.0040 |
[32m[20221213 22:24:58 @agent_ppo2.py:185][0m |          -0.0040 |           4.2167 |           4.0027 |
[32m[20221213 22:24:58 @agent_ppo2.py:185][0m |           0.0022 |           4.1638 |           4.0031 |
[32m[20221213 22:24:58 @agent_ppo2.py:185][0m |          -0.0036 |           4.1535 |           4.0088 |
[32m[20221213 22:24:58 @agent_ppo2.py:185][0m |          -0.0077 |           4.1052 |           4.0024 |
[32m[20221213 22:24:59 @agent_ppo2.py:185][0m |          -0.0058 |           4.0541 |           4.0054 |
[32m[20221213 22:24:59 @agent_ppo2.py:185][0m |          -0.0053 |           4.0929 |           4.0051 |
[32m[20221213 22:24:59 @agent_ppo2.py:185][0m |          -0.0153 |           4.0378 |           4.0071 |
[32m[20221213 22:24:59 @agent_ppo2.py:185][0m |          -0.0080 |           4.0244 |           4.0088 |
[32m[20221213 22:24:59 @agent_ppo2.py:185][0m |          -0.0044 |           4.0154 |           4.0076 |
[32m[20221213 22:24:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:24:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.56
[32m[20221213 22:24:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.74
[32m[20221213 22:24:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.68
[32m[20221213 22:24:59 @agent_ppo2.py:143][0m Total time:       6.77 min
[32m[20221213 22:24:59 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221213 22:24:59 @agent_ppo2.py:121][0m #------------------------ Iteration 313 --------------------------#
[32m[20221213 22:24:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:24:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:24:59 @agent_ppo2.py:185][0m |          -0.0025 |           7.5090 |           4.0171 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0062 |           6.0108 |           4.0148 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0044 |           5.6910 |           4.0118 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0063 |           5.4739 |           4.0096 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0010 |           5.3393 |           4.0069 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0031 |           5.2128 |           4.0054 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |           0.0087 |           5.9192 |           4.0014 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0085 |           5.1376 |           3.9978 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0082 |           5.0194 |           3.9969 |
[32m[20221213 22:25:00 @agent_ppo2.py:185][0m |          -0.0098 |           4.9495 |           3.9982 |
[32m[20221213 22:25:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:25:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:25:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:25:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.98
[32m[20221213 22:25:00 @agent_ppo2.py:143][0m Total time:       6.79 min
[32m[20221213 22:25:00 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221213 22:25:00 @agent_ppo2.py:121][0m #------------------------ Iteration 314 --------------------------#
[32m[20221213 22:25:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0053 |          10.1292 |           3.9430 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0019 |           9.7603 |           3.9421 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0078 |           9.5032 |           3.9389 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0039 |           9.4269 |           3.9408 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0087 |           9.4162 |           3.9421 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0124 |           9.2962 |           3.9432 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0130 |           9.2775 |           3.9446 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0072 |           9.2158 |           3.9447 |
[32m[20221213 22:25:01 @agent_ppo2.py:185][0m |          -0.0082 |           9.2516 |           3.9500 |
[32m[20221213 22:25:02 @agent_ppo2.py:185][0m |          -0.0138 |           9.2063 |           3.9502 |
[32m[20221213 22:25:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:25:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.70
[32m[20221213 22:25:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.92
[32m[20221213 22:25:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.61
[32m[20221213 22:25:02 @agent_ppo2.py:143][0m Total time:       6.81 min
[32m[20221213 22:25:02 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221213 22:25:02 @agent_ppo2.py:121][0m #------------------------ Iteration 315 --------------------------#
[32m[20221213 22:25:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:02 @agent_ppo2.py:185][0m |          -0.0012 |           6.2279 |           3.9966 |
[32m[20221213 22:25:02 @agent_ppo2.py:185][0m |          -0.0030 |           4.8615 |           3.9940 |
[32m[20221213 22:25:02 @agent_ppo2.py:185][0m |          -0.0080 |           4.3891 |           3.9920 |
[32m[20221213 22:25:02 @agent_ppo2.py:185][0m |          -0.0090 |           4.1219 |           3.9895 |
[32m[20221213 22:25:02 @agent_ppo2.py:185][0m |          -0.0119 |           3.9267 |           3.9891 |
[32m[20221213 22:25:02 @agent_ppo2.py:185][0m |          -0.0101 |           3.8240 |           3.9878 |
[32m[20221213 22:25:03 @agent_ppo2.py:185][0m |          -0.0101 |           3.7303 |           3.9849 |
[32m[20221213 22:25:03 @agent_ppo2.py:185][0m |          -0.0121 |           3.6506 |           3.9838 |
[32m[20221213 22:25:03 @agent_ppo2.py:185][0m |          -0.0113 |           3.5905 |           3.9842 |
[32m[20221213 22:25:03 @agent_ppo2.py:185][0m |          -0.0138 |           3.5547 |           3.9814 |
[32m[20221213 22:25:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:25:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 22.36
[32m[20221213 22:25:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 26.21
[32m[20221213 22:25:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:03 @agent_ppo2.py:143][0m Total time:       6.83 min
[32m[20221213 22:25:03 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221213 22:25:03 @agent_ppo2.py:121][0m #------------------------ Iteration 316 --------------------------#
[32m[20221213 22:25:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:03 @agent_ppo2.py:185][0m |           0.0023 |           6.2017 |           4.0047 |
[32m[20221213 22:25:03 @agent_ppo2.py:185][0m |          -0.0073 |           5.4801 |           4.0059 |
[32m[20221213 22:25:03 @agent_ppo2.py:185][0m |          -0.0076 |           5.4122 |           4.0016 |
[32m[20221213 22:25:04 @agent_ppo2.py:185][0m |           0.0051 |           6.2551 |           4.0002 |
[32m[20221213 22:25:04 @agent_ppo2.py:185][0m |          -0.0040 |           5.3499 |           3.9972 |
[32m[20221213 22:25:04 @agent_ppo2.py:185][0m |          -0.0074 |           5.2417 |           3.9989 |
[32m[20221213 22:25:04 @agent_ppo2.py:185][0m |          -0.0065 |           5.1922 |           3.9989 |
[32m[20221213 22:25:04 @agent_ppo2.py:185][0m |          -0.0067 |           5.1909 |           3.9995 |
[32m[20221213 22:25:04 @agent_ppo2.py:185][0m |          -0.0100 |           5.1265 |           3.9992 |
[32m[20221213 22:25:04 @agent_ppo2.py:185][0m |          -0.0068 |           5.1066 |           4.0006 |
[32m[20221213 22:25:04 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 22:25:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.13
[32m[20221213 22:25:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.74
[32m[20221213 22:25:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.51
[32m[20221213 22:25:04 @agent_ppo2.py:143][0m Total time:       6.86 min
[32m[20221213 22:25:04 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221213 22:25:04 @agent_ppo2.py:121][0m #------------------------ Iteration 317 --------------------------#
[32m[20221213 22:25:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0037 |           7.3235 |           3.9950 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0037 |           6.2794 |           3.9930 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0058 |           6.0532 |           3.9930 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0072 |           5.9605 |           3.9914 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |           0.0079 |           6.5348 |           3.9908 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0021 |           5.8768 |           3.9959 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0058 |           5.8061 |           3.9971 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0069 |           5.8108 |           3.9965 |
[32m[20221213 22:25:05 @agent_ppo2.py:185][0m |          -0.0069 |           5.7377 |           3.9969 |
[32m[20221213 22:25:06 @agent_ppo2.py:185][0m |          -0.0018 |           5.9754 |           3.9972 |
[32m[20221213 22:25:06 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:25:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.60
[32m[20221213 22:25:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.15
[32m[20221213 22:25:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:06 @agent_ppo2.py:143][0m Total time:       6.88 min
[32m[20221213 22:25:06 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221213 22:25:06 @agent_ppo2.py:121][0m #------------------------ Iteration 318 --------------------------#
[32m[20221213 22:25:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:06 @agent_ppo2.py:185][0m |          -0.0030 |          12.5013 |           4.0918 |
[32m[20221213 22:25:06 @agent_ppo2.py:185][0m |          -0.0038 |          12.3192 |           4.0884 |
[32m[20221213 22:25:06 @agent_ppo2.py:185][0m |          -0.0016 |          12.2492 |           4.0895 |
[32m[20221213 22:25:06 @agent_ppo2.py:185][0m |          -0.0085 |          12.1360 |           4.0905 |
[32m[20221213 22:25:06 @agent_ppo2.py:185][0m |           0.0037 |          12.5990 |           4.0905 |
[32m[20221213 22:25:06 @agent_ppo2.py:185][0m |          -0.0030 |          12.0341 |           4.0862 |
[32m[20221213 22:25:07 @agent_ppo2.py:185][0m |          -0.0081 |          11.9429 |           4.0930 |
[32m[20221213 22:25:07 @agent_ppo2.py:185][0m |          -0.0109 |          11.9213 |           4.0941 |
[32m[20221213 22:25:07 @agent_ppo2.py:185][0m |          -0.0077 |          11.8985 |           4.0947 |
[32m[20221213 22:25:07 @agent_ppo2.py:185][0m |          -0.0111 |          11.8512 |           4.0948 |
[32m[20221213 22:25:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:25:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.72
[32m[20221213 22:25:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.79
[32m[20221213 22:25:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 18.86
[32m[20221213 22:25:07 @agent_ppo2.py:143][0m Total time:       6.90 min
[32m[20221213 22:25:07 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221213 22:25:07 @agent_ppo2.py:121][0m #------------------------ Iteration 319 --------------------------#
[32m[20221213 22:25:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:07 @agent_ppo2.py:185][0m |          -0.0018 |           7.8879 |           4.0801 |
[32m[20221213 22:25:07 @agent_ppo2.py:185][0m |          -0.0040 |           6.7684 |           4.0748 |
[32m[20221213 22:25:07 @agent_ppo2.py:185][0m |          -0.0048 |           6.5234 |           4.0694 |
[32m[20221213 22:25:08 @agent_ppo2.py:185][0m |          -0.0087 |           6.4400 |           4.0698 |
[32m[20221213 22:25:08 @agent_ppo2.py:185][0m |          -0.0068 |           6.3870 |           4.0688 |
[32m[20221213 22:25:08 @agent_ppo2.py:185][0m |          -0.0066 |           6.3319 |           4.0692 |
[32m[20221213 22:25:08 @agent_ppo2.py:185][0m |          -0.0058 |           6.3901 |           4.0703 |
[32m[20221213 22:25:08 @agent_ppo2.py:185][0m |          -0.0077 |           6.2804 |           4.0704 |
[32m[20221213 22:25:08 @agent_ppo2.py:185][0m |          -0.0081 |           6.3030 |           4.0683 |
[32m[20221213 22:25:08 @agent_ppo2.py:185][0m |          -0.0060 |           6.7581 |           4.0686 |
[32m[20221213 22:25:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:25:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 33.35
[32m[20221213 22:25:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 166.77
[32m[20221213 22:25:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.93
[32m[20221213 22:25:08 @agent_ppo2.py:143][0m Total time:       6.92 min
[32m[20221213 22:25:08 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221213 22:25:08 @agent_ppo2.py:121][0m #------------------------ Iteration 320 --------------------------#
[32m[20221213 22:25:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:25:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0009 |           6.6779 |           4.1377 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0034 |           5.6187 |           4.1341 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0023 |           5.4932 |           4.1327 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0023 |           5.3875 |           4.1308 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0017 |           5.3437 |           4.1288 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0084 |           5.3580 |           4.1275 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0102 |           5.3643 |           4.1257 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0009 |           5.3592 |           4.1283 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0001 |           5.3797 |           4.1250 |
[32m[20221213 22:25:09 @agent_ppo2.py:185][0m |          -0.0049 |           5.3341 |           4.1245 |
[32m[20221213 22:25:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:25:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.27
[32m[20221213 22:25:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.27
[32m[20221213 22:25:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.25
[32m[20221213 22:25:09 @agent_ppo2.py:143][0m Total time:       6.94 min
[32m[20221213 22:25:09 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221213 22:25:10 @agent_ppo2.py:121][0m #------------------------ Iteration 321 --------------------------#
[32m[20221213 22:25:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |           0.0014 |           5.4590 |           4.1568 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0019 |           4.5356 |           4.1540 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0020 |           4.6862 |           4.1503 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0103 |           4.4105 |           4.1493 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0040 |           4.3850 |           4.1478 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0079 |           4.3523 |           4.1475 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0083 |           4.3035 |           4.1441 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0096 |           4.2895 |           4.1440 |
[32m[20221213 22:25:10 @agent_ppo2.py:185][0m |          -0.0112 |           4.2503 |           4.1422 |
[32m[20221213 22:25:11 @agent_ppo2.py:185][0m |          -0.0094 |           4.2426 |           4.1441 |
[32m[20221213 22:25:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:25:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:25:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:25:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.17
[32m[20221213 22:25:11 @agent_ppo2.py:143][0m Total time:       6.96 min
[32m[20221213 22:25:11 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221213 22:25:11 @agent_ppo2.py:121][0m #------------------------ Iteration 322 --------------------------#
[32m[20221213 22:25:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:11 @agent_ppo2.py:185][0m |           0.0002 |           7.7057 |           4.0989 |
[32m[20221213 22:25:11 @agent_ppo2.py:185][0m |           0.0007 |           7.3465 |           4.0926 |
[32m[20221213 22:25:11 @agent_ppo2.py:185][0m |          -0.0100 |           6.8408 |           4.0915 |
[32m[20221213 22:25:11 @agent_ppo2.py:185][0m |          -0.0069 |           6.7510 |           4.0912 |
[32m[20221213 22:25:11 @agent_ppo2.py:185][0m |          -0.0019 |           6.8817 |           4.0916 |
[32m[20221213 22:25:11 @agent_ppo2.py:185][0m |          -0.0058 |           6.6090 |           4.0913 |
[32m[20221213 22:25:12 @agent_ppo2.py:185][0m |          -0.0016 |           6.8759 |           4.0898 |
[32m[20221213 22:25:12 @agent_ppo2.py:185][0m |          -0.0073 |           6.5896 |           4.0924 |
[32m[20221213 22:25:12 @agent_ppo2.py:185][0m |          -0.0035 |           6.7130 |           4.0916 |
[32m[20221213 22:25:12 @agent_ppo2.py:185][0m |          -0.0106 |           6.4849 |           4.0913 |
[32m[20221213 22:25:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:25:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.03
[32m[20221213 22:25:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 248.22
[32m[20221213 22:25:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.62
[32m[20221213 22:25:12 @agent_ppo2.py:143][0m Total time:       6.98 min
[32m[20221213 22:25:12 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221213 22:25:12 @agent_ppo2.py:121][0m #------------------------ Iteration 323 --------------------------#
[32m[20221213 22:25:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:12 @agent_ppo2.py:185][0m |           0.0002 |           3.4043 |           4.1218 |
[32m[20221213 22:25:12 @agent_ppo2.py:185][0m |          -0.0052 |           3.0306 |           4.1202 |
[32m[20221213 22:25:12 @agent_ppo2.py:185][0m |          -0.0050 |           2.9743 |           4.1180 |
[32m[20221213 22:25:13 @agent_ppo2.py:185][0m |          -0.0041 |           2.9673 |           4.1207 |
[32m[20221213 22:25:13 @agent_ppo2.py:185][0m |           0.0034 |           3.2700 |           4.1228 |
[32m[20221213 22:25:13 @agent_ppo2.py:185][0m |          -0.0035 |           2.9764 |           4.1190 |
[32m[20221213 22:25:13 @agent_ppo2.py:185][0m |          -0.0009 |           3.0480 |           4.1260 |
[32m[20221213 22:25:13 @agent_ppo2.py:185][0m |          -0.0095 |           2.8994 |           4.1215 |
[32m[20221213 22:25:13 @agent_ppo2.py:185][0m |          -0.0124 |           2.8653 |           4.1228 |
[32m[20221213 22:25:13 @agent_ppo2.py:185][0m |           0.0007 |           3.0464 |           4.1251 |
[32m[20221213 22:25:13 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:25:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:25:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:25:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.58
[32m[20221213 22:25:13 @agent_ppo2.py:143][0m Total time:       7.01 min
[32m[20221213 22:25:13 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221213 22:25:13 @agent_ppo2.py:121][0m #------------------------ Iteration 324 --------------------------#
[32m[20221213 22:25:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:25:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0041 |           2.6303 |           4.2304 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0086 |           2.3140 |           4.2280 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0105 |           2.2814 |           4.2252 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0086 |           2.2607 |           4.2276 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0108 |           2.2348 |           4.2253 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0114 |           2.2232 |           4.2261 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0127 |           2.2367 |           4.2277 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0099 |           2.2118 |           4.2271 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0120 |           2.2148 |           4.2263 |
[32m[20221213 22:25:14 @agent_ppo2.py:185][0m |          -0.0115 |           2.1903 |           4.2278 |
[32m[20221213 22:25:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:25:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:25:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:15 @agent_ppo2.py:143][0m Total time:       7.03 min
[32m[20221213 22:25:15 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221213 22:25:15 @agent_ppo2.py:121][0m #------------------------ Iteration 325 --------------------------#
[32m[20221213 22:25:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:15 @agent_ppo2.py:185][0m |           0.0007 |           5.8770 |           4.1468 |
[32m[20221213 22:25:15 @agent_ppo2.py:185][0m |          -0.0005 |           4.8206 |           4.1430 |
[32m[20221213 22:25:15 @agent_ppo2.py:185][0m |           0.0002 |           4.7393 |           4.1440 |
[32m[20221213 22:25:15 @agent_ppo2.py:185][0m |          -0.0105 |           4.6847 |           4.1447 |
[32m[20221213 22:25:15 @agent_ppo2.py:185][0m |          -0.0065 |           4.5785 |           4.1432 |
[32m[20221213 22:25:15 @agent_ppo2.py:185][0m |          -0.0126 |           4.6036 |           4.1433 |
[32m[20221213 22:25:15 @agent_ppo2.py:185][0m |          -0.0084 |           4.5175 |           4.1444 |
[32m[20221213 22:25:16 @agent_ppo2.py:185][0m |          -0.0085 |           4.5260 |           4.1449 |
[32m[20221213 22:25:16 @agent_ppo2.py:185][0m |          -0.0090 |           4.5558 |           4.1439 |
[32m[20221213 22:25:16 @agent_ppo2.py:185][0m |          -0.0129 |           4.5165 |           4.1450 |
[32m[20221213 22:25:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:25:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.01
[32m[20221213 22:25:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.43
[32m[20221213 22:25:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:16 @agent_ppo2.py:143][0m Total time:       7.05 min
[32m[20221213 22:25:16 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221213 22:25:16 @agent_ppo2.py:121][0m #------------------------ Iteration 326 --------------------------#
[32m[20221213 22:25:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:16 @agent_ppo2.py:185][0m |          -0.0018 |           5.5614 |           4.1435 |
[32m[20221213 22:25:16 @agent_ppo2.py:185][0m |          -0.0085 |           4.9507 |           4.1416 |
[32m[20221213 22:25:16 @agent_ppo2.py:185][0m |          -0.0082 |           4.8894 |           4.1392 |
[32m[20221213 22:25:16 @agent_ppo2.py:185][0m |          -0.0042 |           4.8319 |           4.1379 |
[32m[20221213 22:25:17 @agent_ppo2.py:185][0m |          -0.0096 |           4.7590 |           4.1361 |
[32m[20221213 22:25:17 @agent_ppo2.py:185][0m |          -0.0117 |           4.6911 |           4.1349 |
[32m[20221213 22:25:17 @agent_ppo2.py:185][0m |          -0.0072 |           4.7375 |           4.1383 |
[32m[20221213 22:25:17 @agent_ppo2.py:185][0m |          -0.0098 |           4.6526 |           4.1386 |
[32m[20221213 22:25:17 @agent_ppo2.py:185][0m |          -0.0116 |           4.6435 |           4.1351 |
[32m[20221213 22:25:17 @agent_ppo2.py:185][0m |          -0.0061 |           4.6040 |           4.1372 |
[32m[20221213 22:25:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 113.32
[32m[20221213 22:25:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 205.27
[32m[20221213 22:25:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.59
[32m[20221213 22:25:17 @agent_ppo2.py:143][0m Total time:       7.07 min
[32m[20221213 22:25:17 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221213 22:25:17 @agent_ppo2.py:121][0m #------------------------ Iteration 327 --------------------------#
[32m[20221213 22:25:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |           0.0022 |           5.4247 |           4.2064 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0054 |           4.3893 |           4.2057 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0026 |           4.4692 |           4.2045 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0133 |           4.2988 |           4.2053 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0105 |           4.2262 |           4.2056 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0048 |           4.3635 |           4.2060 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0186 |           4.2278 |           4.2087 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0063 |           4.1619 |           4.2073 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0115 |           4.1598 |           4.2079 |
[32m[20221213 22:25:18 @agent_ppo2.py:185][0m |          -0.0148 |           4.1068 |           4.2113 |
[32m[20221213 22:25:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:25:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:25:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:25:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.52
[32m[20221213 22:25:18 @agent_ppo2.py:143][0m Total time:       7.09 min
[32m[20221213 22:25:18 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221213 22:25:18 @agent_ppo2.py:121][0m #------------------------ Iteration 328 --------------------------#
[32m[20221213 22:25:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0018 |          11.2001 |           4.2070 |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0067 |          10.9472 |           4.2028 |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0113 |          10.7280 |           4.1991 |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0119 |          10.6640 |           4.2013 |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0018 |          11.0352 |           4.1963 |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0134 |          10.6346 |           4.1995 |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0154 |          10.5388 |           4.1984 |
[32m[20221213 22:25:19 @agent_ppo2.py:185][0m |          -0.0149 |          10.4843 |           4.1987 |
[32m[20221213 22:25:20 @agent_ppo2.py:185][0m |          -0.0103 |          10.4566 |           4.1982 |
[32m[20221213 22:25:20 @agent_ppo2.py:185][0m |          -0.0149 |          10.4233 |           4.1992 |
[32m[20221213 22:25:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:25:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.44
[32m[20221213 22:25:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.81
[32m[20221213 22:25:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 21.75
[32m[20221213 22:25:20 @agent_ppo2.py:143][0m Total time:       7.11 min
[32m[20221213 22:25:20 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221213 22:25:20 @agent_ppo2.py:121][0m #------------------------ Iteration 329 --------------------------#
[32m[20221213 22:25:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:25:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:20 @agent_ppo2.py:185][0m |           0.0022 |           5.3309 |           4.2688 |
[32m[20221213 22:25:20 @agent_ppo2.py:185][0m |          -0.0063 |           4.8350 |           4.2652 |
[32m[20221213 22:25:20 @agent_ppo2.py:185][0m |          -0.0096 |           4.7627 |           4.2636 |
[32m[20221213 22:25:20 @agent_ppo2.py:185][0m |          -0.0069 |           4.7299 |           4.2644 |
[32m[20221213 22:25:21 @agent_ppo2.py:185][0m |          -0.0084 |           4.6660 |           4.2636 |
[32m[20221213 22:25:21 @agent_ppo2.py:185][0m |          -0.0038 |           4.6891 |           4.2651 |
[32m[20221213 22:25:21 @agent_ppo2.py:185][0m |          -0.0178 |           4.6188 |           4.2691 |
[32m[20221213 22:25:21 @agent_ppo2.py:185][0m |          -0.0124 |           4.6289 |           4.2696 |
[32m[20221213 22:25:21 @agent_ppo2.py:185][0m |          -0.0072 |           4.6056 |           4.2707 |
[32m[20221213 22:25:21 @agent_ppo2.py:185][0m |          -0.0115 |           4.5605 |           4.2694 |
[32m[20221213 22:25:21 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:25:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 13.64
[32m[20221213 22:25:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.22
[32m[20221213 22:25:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.84
[32m[20221213 22:25:21 @agent_ppo2.py:143][0m Total time:       7.14 min
[32m[20221213 22:25:21 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221213 22:25:21 @agent_ppo2.py:121][0m #------------------------ Iteration 330 --------------------------#
[32m[20221213 22:25:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:25:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0001 |           6.4887 |           4.3419 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0062 |           6.0718 |           4.3411 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0146 |           5.9305 |           4.3412 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0140 |           5.9740 |           4.3425 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0074 |           5.8049 |           4.3426 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0073 |           5.7578 |           4.3438 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0097 |           5.7220 |           4.3431 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0114 |           5.7400 |           4.3437 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0142 |           5.6811 |           4.3463 |
[32m[20221213 22:25:22 @agent_ppo2.py:185][0m |          -0.0014 |           5.9580 |           4.3469 |
[32m[20221213 22:25:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:25:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.13
[32m[20221213 22:25:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.20
[32m[20221213 22:25:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:22 @agent_ppo2.py:143][0m Total time:       7.16 min
[32m[20221213 22:25:22 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221213 22:25:22 @agent_ppo2.py:121][0m #------------------------ Iteration 331 --------------------------#
[32m[20221213 22:25:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |           0.0034 |           6.3494 |           4.2485 |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |          -0.0003 |           4.1695 |           4.2460 |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |          -0.0080 |           4.0131 |           4.2457 |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |           0.0009 |           3.9585 |           4.2496 |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |          -0.0148 |           3.8571 |           4.2520 |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |          -0.0098 |           3.8495 |           4.2545 |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |          -0.0088 |           3.7704 |           4.2567 |
[32m[20221213 22:25:23 @agent_ppo2.py:185][0m |          -0.0133 |           3.7043 |           4.2597 |
[32m[20221213 22:25:24 @agent_ppo2.py:185][0m |          -0.0088 |           3.6784 |           4.2623 |
[32m[20221213 22:25:24 @agent_ppo2.py:185][0m |          -0.0055 |           3.6418 |           4.2632 |
[32m[20221213 22:25:24 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:25:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.14
[32m[20221213 22:25:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.61
[32m[20221213 22:25:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.25
[32m[20221213 22:25:24 @agent_ppo2.py:143][0m Total time:       7.18 min
[32m[20221213 22:25:24 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221213 22:25:24 @agent_ppo2.py:121][0m #------------------------ Iteration 332 --------------------------#
[32m[20221213 22:25:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:24 @agent_ppo2.py:185][0m |          -0.0059 |          10.8306 |           4.3131 |
[32m[20221213 22:25:24 @agent_ppo2.py:185][0m |          -0.0072 |           9.5595 |           4.3103 |
[32m[20221213 22:25:24 @agent_ppo2.py:185][0m |          -0.0038 |           9.2422 |           4.3086 |
[32m[20221213 22:25:24 @agent_ppo2.py:185][0m |          -0.0032 |           9.0733 |           4.3077 |
[32m[20221213 22:25:25 @agent_ppo2.py:185][0m |          -0.0060 |           9.1018 |           4.3106 |
[32m[20221213 22:25:25 @agent_ppo2.py:185][0m |          -0.0096 |           8.9515 |           4.3089 |
[32m[20221213 22:25:25 @agent_ppo2.py:185][0m |          -0.0025 |           9.2888 |           4.3119 |
[32m[20221213 22:25:25 @agent_ppo2.py:185][0m |          -0.0123 |           8.8558 |           4.3126 |
[32m[20221213 22:25:25 @agent_ppo2.py:185][0m |          -0.0078 |           8.7288 |           4.3125 |
[32m[20221213 22:25:25 @agent_ppo2.py:185][0m |          -0.0068 |           8.6789 |           4.3134 |
[32m[20221213 22:25:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.99
[32m[20221213 22:25:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.40
[32m[20221213 22:25:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.96
[32m[20221213 22:25:25 @agent_ppo2.py:143][0m Total time:       7.20 min
[32m[20221213 22:25:25 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221213 22:25:25 @agent_ppo2.py:121][0m #------------------------ Iteration 333 --------------------------#
[32m[20221213 22:25:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:25 @agent_ppo2.py:185][0m |           0.0039 |           6.2291 |           4.3946 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0023 |           4.2121 |           4.3937 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0022 |           4.1157 |           4.3911 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |           0.0005 |           4.0666 |           4.3901 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0135 |           4.0496 |           4.3870 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0032 |           4.0075 |           4.3848 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0091 |           3.9782 |           4.3841 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0122 |           3.9515 |           4.3840 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0094 |           3.9991 |           4.3808 |
[32m[20221213 22:25:26 @agent_ppo2.py:185][0m |          -0.0050 |           3.9692 |           4.3803 |
[32m[20221213 22:25:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.96
[32m[20221213 22:25:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.06
[32m[20221213 22:25:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.97
[32m[20221213 22:25:26 @agent_ppo2.py:143][0m Total time:       7.22 min
[32m[20221213 22:25:26 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221213 22:25:26 @agent_ppo2.py:121][0m #------------------------ Iteration 334 --------------------------#
[32m[20221213 22:25:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0065 |           7.1436 |           4.4050 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0030 |           6.1842 |           4.4016 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0038 |           6.0830 |           4.4019 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0003 |           6.1485 |           4.4001 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0022 |           6.2537 |           4.4007 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0015 |           6.0104 |           4.4002 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0026 |           6.0264 |           4.4028 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |           0.0034 |           6.1605 |           4.4014 |
[32m[20221213 22:25:27 @agent_ppo2.py:185][0m |          -0.0024 |           6.0205 |           4.4006 |
[32m[20221213 22:25:28 @agent_ppo2.py:185][0m |          -0.0057 |           5.9714 |           4.4018 |
[32m[20221213 22:25:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:25:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.28
[32m[20221213 22:25:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.31
[32m[20221213 22:25:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.95
[32m[20221213 22:25:28 @agent_ppo2.py:143][0m Total time:       7.25 min
[32m[20221213 22:25:28 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221213 22:25:28 @agent_ppo2.py:121][0m #------------------------ Iteration 335 --------------------------#
[32m[20221213 22:25:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:25:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:28 @agent_ppo2.py:185][0m |           0.0056 |          16.5099 |           4.4240 |
[32m[20221213 22:25:28 @agent_ppo2.py:185][0m |          -0.0018 |          14.6430 |           4.4180 |
[32m[20221213 22:25:28 @agent_ppo2.py:185][0m |          -0.0059 |          14.4217 |           4.4197 |
[32m[20221213 22:25:28 @agent_ppo2.py:185][0m |          -0.0095 |          14.3225 |           4.4176 |
[32m[20221213 22:25:28 @agent_ppo2.py:185][0m |          -0.0055 |          14.3606 |           4.4188 |
[32m[20221213 22:25:29 @agent_ppo2.py:185][0m |          -0.0030 |          14.3855 |           4.4186 |
[32m[20221213 22:25:29 @agent_ppo2.py:185][0m |           0.0074 |          16.4292 |           4.4197 |
[32m[20221213 22:25:29 @agent_ppo2.py:185][0m |          -0.0151 |          14.3314 |           4.4167 |
[32m[20221213 22:25:29 @agent_ppo2.py:185][0m |          -0.0105 |          14.2135 |           4.4166 |
[32m[20221213 22:25:29 @agent_ppo2.py:185][0m |          -0.0075 |          14.2008 |           4.4165 |
[32m[20221213 22:25:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:25:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.59
[32m[20221213 22:25:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.08
[32m[20221213 22:25:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.22
[32m[20221213 22:25:29 @agent_ppo2.py:143][0m Total time:       7.27 min
[32m[20221213 22:25:29 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221213 22:25:29 @agent_ppo2.py:121][0m #------------------------ Iteration 336 --------------------------#
[32m[20221213 22:25:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:25:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:29 @agent_ppo2.py:185][0m |          -0.0006 |           8.0422 |           4.3510 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |          -0.0042 |           6.8333 |           4.3482 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |           0.0067 |           6.9411 |           4.3486 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |          -0.0019 |           6.6732 |           4.3448 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |          -0.0054 |           6.6153 |           4.3467 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |           0.0008 |           7.0716 |           4.3463 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |          -0.0059 |           6.6923 |           4.3478 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |          -0.0132 |           6.5534 |           4.3459 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |           0.0016 |           6.9464 |           4.3461 |
[32m[20221213 22:25:30 @agent_ppo2.py:185][0m |          -0.0101 |           6.5222 |           4.3476 |
[32m[20221213 22:25:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:25:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.09
[32m[20221213 22:25:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.43
[32m[20221213 22:25:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 16.64
[32m[20221213 22:25:30 @agent_ppo2.py:143][0m Total time:       7.29 min
[32m[20221213 22:25:30 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221213 22:25:30 @agent_ppo2.py:121][0m #------------------------ Iteration 337 --------------------------#
[32m[20221213 22:25:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0019 |          16.4342 |           4.3963 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0015 |          15.4268 |           4.3916 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0035 |          15.3530 |           4.3896 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0045 |          15.2191 |           4.3900 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0038 |          15.2732 |           4.3879 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0074 |          15.3374 |           4.3857 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0084 |          15.0724 |           4.3882 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0086 |          15.0398 |           4.3846 |
[32m[20221213 22:25:31 @agent_ppo2.py:185][0m |          -0.0053 |          15.0264 |           4.3852 |
[32m[20221213 22:25:32 @agent_ppo2.py:185][0m |          -0.0074 |          14.9750 |           4.3852 |
[32m[20221213 22:25:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:25:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.63
[32m[20221213 22:25:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 248.35
[32m[20221213 22:25:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:32 @agent_ppo2.py:143][0m Total time:       7.31 min
[32m[20221213 22:25:32 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221213 22:25:32 @agent_ppo2.py:121][0m #------------------------ Iteration 338 --------------------------#
[32m[20221213 22:25:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:32 @agent_ppo2.py:185][0m |          -0.0043 |          11.3504 |           4.4111 |
[32m[20221213 22:25:32 @agent_ppo2.py:185][0m |          -0.0035 |          10.6577 |           4.4090 |
[32m[20221213 22:25:32 @agent_ppo2.py:185][0m |          -0.0077 |          10.5663 |           4.4068 |
[32m[20221213 22:25:32 @agent_ppo2.py:185][0m |          -0.0033 |          10.5107 |           4.4067 |
[32m[20221213 22:25:32 @agent_ppo2.py:185][0m |          -0.0097 |          10.4560 |           4.4046 |
[32m[20221213 22:25:32 @agent_ppo2.py:185][0m |          -0.0105 |          10.3717 |           4.4057 |
[32m[20221213 22:25:33 @agent_ppo2.py:185][0m |          -0.0058 |          10.5581 |           4.4038 |
[32m[20221213 22:25:33 @agent_ppo2.py:185][0m |          -0.0129 |          10.3959 |           4.4042 |
[32m[20221213 22:25:33 @agent_ppo2.py:185][0m |          -0.0121 |          10.3308 |           4.4020 |
[32m[20221213 22:25:33 @agent_ppo2.py:185][0m |          -0.0107 |          10.3122 |           4.4015 |
[32m[20221213 22:25:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.05
[32m[20221213 22:25:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.03
[32m[20221213 22:25:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.29
[32m[20221213 22:25:33 @agent_ppo2.py:143][0m Total time:       7.33 min
[32m[20221213 22:25:33 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221213 22:25:33 @agent_ppo2.py:121][0m #------------------------ Iteration 339 --------------------------#
[32m[20221213 22:25:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:33 @agent_ppo2.py:185][0m |          -0.0011 |          10.7450 |           4.3727 |
[32m[20221213 22:25:33 @agent_ppo2.py:185][0m |          -0.0042 |          10.2421 |           4.3669 |
[32m[20221213 22:25:33 @agent_ppo2.py:185][0m |          -0.0079 |          10.1691 |           4.3646 |
[32m[20221213 22:25:34 @agent_ppo2.py:185][0m |          -0.0028 |          10.3359 |           4.3663 |
[32m[20221213 22:25:34 @agent_ppo2.py:185][0m |          -0.0036 |          10.0976 |           4.3643 |
[32m[20221213 22:25:34 @agent_ppo2.py:185][0m |          -0.0033 |          10.4291 |           4.3667 |
[32m[20221213 22:25:34 @agent_ppo2.py:185][0m |          -0.0117 |          10.1028 |           4.3662 |
[32m[20221213 22:25:34 @agent_ppo2.py:185][0m |          -0.0071 |          10.1905 |           4.3663 |
[32m[20221213 22:25:34 @agent_ppo2.py:185][0m |          -0.0052 |           9.9853 |           4.3693 |
[32m[20221213 22:25:34 @agent_ppo2.py:185][0m |          -0.0100 |           9.9572 |           4.3706 |
[32m[20221213 22:25:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:25:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.18
[32m[20221213 22:25:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.91
[32m[20221213 22:25:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.86
[32m[20221213 22:25:34 @agent_ppo2.py:143][0m Total time:       7.36 min
[32m[20221213 22:25:34 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221213 22:25:34 @agent_ppo2.py:121][0m #------------------------ Iteration 340 --------------------------#
[32m[20221213 22:25:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:25:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |           0.0008 |          14.4963 |           4.4349 |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |          -0.0008 |          12.4801 |           4.4283 |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |          -0.0066 |          12.4462 |           4.4292 |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |           0.0054 |          13.5113 |           4.4220 |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |          -0.0068 |          11.9667 |           4.4242 |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |          -0.0102 |          11.6899 |           4.4266 |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |          -0.0103 |          11.5625 |           4.4256 |
[32m[20221213 22:25:35 @agent_ppo2.py:185][0m |          -0.0107 |          11.5160 |           4.4267 |
[32m[20221213 22:25:36 @agent_ppo2.py:185][0m |          -0.0057 |          11.7465 |           4.4276 |
[32m[20221213 22:25:36 @agent_ppo2.py:185][0m |          -0.0122 |          11.4330 |           4.4260 |
[32m[20221213 22:25:36 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 22:25:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.35
[32m[20221213 22:25:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.53
[32m[20221213 22:25:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.62
[32m[20221213 22:25:36 @agent_ppo2.py:143][0m Total time:       7.38 min
[32m[20221213 22:25:36 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221213 22:25:36 @agent_ppo2.py:121][0m #------------------------ Iteration 341 --------------------------#
[32m[20221213 22:25:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:36 @agent_ppo2.py:185][0m |           0.0027 |          13.7568 |           4.3968 |
[32m[20221213 22:25:36 @agent_ppo2.py:185][0m |          -0.0020 |          13.1574 |           4.3945 |
[32m[20221213 22:25:36 @agent_ppo2.py:185][0m |          -0.0026 |          12.9482 |           4.3870 |
[32m[20221213 22:25:36 @agent_ppo2.py:185][0m |          -0.0074 |          12.9559 |           4.3874 |
[32m[20221213 22:25:37 @agent_ppo2.py:185][0m |          -0.0074 |          12.8750 |           4.3851 |
[32m[20221213 22:25:37 @agent_ppo2.py:185][0m |          -0.0048 |          12.8739 |           4.3852 |
[32m[20221213 22:25:37 @agent_ppo2.py:185][0m |          -0.0087 |          12.8538 |           4.3824 |
[32m[20221213 22:25:37 @agent_ppo2.py:185][0m |          -0.0032 |          13.5901 |           4.3809 |
[32m[20221213 22:25:37 @agent_ppo2.py:185][0m |          -0.0064 |          12.9360 |           4.3787 |
[32m[20221213 22:25:37 @agent_ppo2.py:185][0m |          -0.0073 |          12.7323 |           4.3800 |
[32m[20221213 22:25:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.85
[32m[20221213 22:25:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.00
[32m[20221213 22:25:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 177.09
[32m[20221213 22:25:37 @agent_ppo2.py:143][0m Total time:       7.40 min
[32m[20221213 22:25:37 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221213 22:25:37 @agent_ppo2.py:121][0m #------------------------ Iteration 342 --------------------------#
[32m[20221213 22:25:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:37 @agent_ppo2.py:185][0m |           0.0044 |          12.7520 |           4.4751 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0023 |          11.9895 |           4.4720 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0034 |          11.7661 |           4.4678 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0091 |          11.6423 |           4.4667 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0123 |          11.6373 |           4.4636 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0097 |          11.5488 |           4.4655 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0068 |          11.8626 |           4.4618 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0066 |          11.5095 |           4.4634 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0140 |          11.5804 |           4.4602 |
[32m[20221213 22:25:38 @agent_ppo2.py:185][0m |          -0.0051 |          11.4841 |           4.4605 |
[32m[20221213 22:25:38 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:25:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.59
[32m[20221213 22:25:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.41
[32m[20221213 22:25:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:39 @agent_ppo2.py:143][0m Total time:       7.43 min
[32m[20221213 22:25:39 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221213 22:25:39 @agent_ppo2.py:121][0m #------------------------ Iteration 343 --------------------------#
[32m[20221213 22:25:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |           0.0010 |          17.4811 |           4.4328 |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |          -0.0024 |          15.8067 |           4.4311 |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |          -0.0044 |          16.2065 |           4.4232 |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |          -0.0115 |          15.5587 |           4.4240 |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |          -0.0088 |          15.4268 |           4.4216 |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |          -0.0112 |          15.3124 |           4.4258 |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |          -0.0064 |          15.2184 |           4.4232 |
[32m[20221213 22:25:39 @agent_ppo2.py:185][0m |           0.0064 |          17.2897 |           4.4252 |
[32m[20221213 22:25:40 @agent_ppo2.py:185][0m |          -0.0064 |          15.2653 |           4.4204 |
[32m[20221213 22:25:40 @agent_ppo2.py:185][0m |          -0.0091 |          15.0836 |           4.4234 |
[32m[20221213 22:25:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.32
[32m[20221213 22:25:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 162.74
[32m[20221213 22:25:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:40 @agent_ppo2.py:143][0m Total time:       7.45 min
[32m[20221213 22:25:40 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221213 22:25:40 @agent_ppo2.py:121][0m #------------------------ Iteration 344 --------------------------#
[32m[20221213 22:25:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:40 @agent_ppo2.py:185][0m |          -0.0029 |          12.1064 |           4.4475 |
[32m[20221213 22:25:40 @agent_ppo2.py:185][0m |           0.0024 |          11.3461 |           4.4493 |
[32m[20221213 22:25:40 @agent_ppo2.py:185][0m |          -0.0043 |          10.9556 |           4.4458 |
[32m[20221213 22:25:40 @agent_ppo2.py:185][0m |          -0.0120 |          10.8597 |           4.4477 |
[32m[20221213 22:25:41 @agent_ppo2.py:185][0m |          -0.0065 |          10.8303 |           4.4464 |
[32m[20221213 22:25:41 @agent_ppo2.py:185][0m |          -0.0103 |          10.7508 |           4.4474 |
[32m[20221213 22:25:41 @agent_ppo2.py:185][0m |          -0.0007 |          11.1476 |           4.4460 |
[32m[20221213 22:25:41 @agent_ppo2.py:185][0m |          -0.0090 |          10.7214 |           4.4478 |
[32m[20221213 22:25:41 @agent_ppo2.py:185][0m |          -0.0057 |          11.0053 |           4.4496 |
[32m[20221213 22:25:41 @agent_ppo2.py:185][0m |          -0.0098 |          10.6583 |           4.4498 |
[32m[20221213 22:25:41 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 24.92
[32m[20221213 22:25:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.20
[32m[20221213 22:25:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.86
[32m[20221213 22:25:41 @agent_ppo2.py:143][0m Total time:       7.47 min
[32m[20221213 22:25:41 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221213 22:25:41 @agent_ppo2.py:121][0m #------------------------ Iteration 345 --------------------------#
[32m[20221213 22:25:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:41 @agent_ppo2.py:185][0m |          -0.0034 |           9.0599 |           4.3927 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |          -0.0006 |           8.3688 |           4.3871 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |          -0.0050 |           8.1836 |           4.3844 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |          -0.0090 |           8.0395 |           4.3816 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |          -0.0005 |           8.0832 |           4.3832 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |          -0.0072 |           7.9290 |           4.3802 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |           0.0038 |           8.1669 |           4.3783 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |           0.0039 |           7.9299 |           4.3752 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |          -0.0061 |           8.0343 |           4.3753 |
[32m[20221213 22:25:42 @agent_ppo2.py:185][0m |          -0.0051 |           7.8794 |           4.3734 |
[32m[20221213 22:25:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:25:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.73
[32m[20221213 22:25:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.01
[32m[20221213 22:25:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.49
[32m[20221213 22:25:42 @agent_ppo2.py:143][0m Total time:       7.49 min
[32m[20221213 22:25:42 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221213 22:25:42 @agent_ppo2.py:121][0m #------------------------ Iteration 346 --------------------------#
[32m[20221213 22:25:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0048 |          12.9360 |           4.5029 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0056 |          11.5603 |           4.4955 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0037 |          11.4147 |           4.4942 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0013 |          11.9534 |           4.4901 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0072 |          11.3390 |           4.4879 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0120 |          11.2503 |           4.4894 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0058 |          11.5303 |           4.4871 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0126 |          11.0901 |           4.4869 |
[32m[20221213 22:25:43 @agent_ppo2.py:185][0m |          -0.0138 |          11.0040 |           4.4858 |
[32m[20221213 22:25:44 @agent_ppo2.py:185][0m |          -0.0134 |          10.9465 |           4.4882 |
[32m[20221213 22:25:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:25:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.46
[32m[20221213 22:25:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 115.63
[32m[20221213 22:25:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.12
[32m[20221213 22:25:44 @agent_ppo2.py:143][0m Total time:       7.51 min
[32m[20221213 22:25:44 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221213 22:25:44 @agent_ppo2.py:121][0m #------------------------ Iteration 347 --------------------------#
[32m[20221213 22:25:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:44 @agent_ppo2.py:185][0m |           0.0015 |          14.9874 |           4.4752 |
[32m[20221213 22:25:44 @agent_ppo2.py:185][0m |          -0.0003 |          13.6002 |           4.4738 |
[32m[20221213 22:25:44 @agent_ppo2.py:185][0m |          -0.0071 |          13.3226 |           4.4720 |
[32m[20221213 22:25:44 @agent_ppo2.py:185][0m |          -0.0096 |          13.0138 |           4.4721 |
[32m[20221213 22:25:44 @agent_ppo2.py:185][0m |           0.0008 |          13.7069 |           4.4711 |
[32m[20221213 22:25:45 @agent_ppo2.py:185][0m |          -0.0063 |          12.7758 |           4.4718 |
[32m[20221213 22:25:45 @agent_ppo2.py:185][0m |          -0.0084 |          12.6390 |           4.4701 |
[32m[20221213 22:25:45 @agent_ppo2.py:185][0m |          -0.0144 |          12.6812 |           4.4715 |
[32m[20221213 22:25:45 @agent_ppo2.py:185][0m |          -0.0093 |          12.5071 |           4.4725 |
[32m[20221213 22:25:45 @agent_ppo2.py:185][0m |          -0.0117 |          12.4592 |           4.4727 |
[32m[20221213 22:25:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:25:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.03
[32m[20221213 22:25:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 190.68
[32m[20221213 22:25:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:25:45 @agent_ppo2.py:143][0m Total time:       7.54 min
[32m[20221213 22:25:45 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221213 22:25:45 @agent_ppo2.py:121][0m #------------------------ Iteration 348 --------------------------#
[32m[20221213 22:25:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:45 @agent_ppo2.py:185][0m |           0.0018 |          12.8148 |           4.4314 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0062 |           9.2745 |           4.4285 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0023 |           8.8240 |           4.4266 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0075 |           8.6540 |           4.4285 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0118 |           8.6222 |           4.4292 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0044 |           8.4457 |           4.4275 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0129 |           8.4444 |           4.4299 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0168 |           8.3447 |           4.4303 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0077 |           8.3513 |           4.4312 |
[32m[20221213 22:25:46 @agent_ppo2.py:185][0m |          -0.0128 |           8.2405 |           4.4327 |
[32m[20221213 22:25:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:25:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 169.77
[32m[20221213 22:25:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.01
[32m[20221213 22:25:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.76
[32m[20221213 22:25:46 @agent_ppo2.py:143][0m Total time:       7.56 min
[32m[20221213 22:25:46 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221213 22:25:46 @agent_ppo2.py:121][0m #------------------------ Iteration 349 --------------------------#
[32m[20221213 22:25:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |           0.0005 |          10.9916 |           4.4558 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0015 |           8.3886 |           4.4525 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0131 |           8.1139 |           4.4540 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0055 |           7.8314 |           4.4515 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0080 |           7.7464 |           4.4557 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0129 |           7.6816 |           4.4558 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0159 |           7.6116 |           4.4576 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0066 |           7.5068 |           4.4556 |
[32m[20221213 22:25:47 @agent_ppo2.py:185][0m |          -0.0120 |           7.4268 |           4.4582 |
[32m[20221213 22:25:48 @agent_ppo2.py:185][0m |          -0.0114 |           7.4349 |           4.4586 |
[32m[20221213 22:25:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:25:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.69
[32m[20221213 22:25:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.89
[32m[20221213 22:25:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.43
[32m[20221213 22:25:48 @agent_ppo2.py:143][0m Total time:       7.58 min
[32m[20221213 22:25:48 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221213 22:25:48 @agent_ppo2.py:121][0m #------------------------ Iteration 350 --------------------------#
[32m[20221213 22:25:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 22:25:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:48 @agent_ppo2.py:185][0m |          -0.0027 |           9.7379 |           4.5748 |
[32m[20221213 22:25:48 @agent_ppo2.py:185][0m |           0.0021 |           9.1451 |           4.5678 |
[32m[20221213 22:25:48 @agent_ppo2.py:185][0m |           0.0008 |           9.1240 |           4.5630 |
[32m[20221213 22:25:48 @agent_ppo2.py:185][0m |          -0.0089 |           8.9631 |           4.5611 |
[32m[20221213 22:25:48 @agent_ppo2.py:185][0m |          -0.0087 |           8.8943 |           4.5590 |
[32m[20221213 22:25:49 @agent_ppo2.py:185][0m |          -0.0103 |           8.8739 |           4.5565 |
[32m[20221213 22:25:49 @agent_ppo2.py:185][0m |          -0.0116 |           8.8163 |           4.5555 |
[32m[20221213 22:25:49 @agent_ppo2.py:185][0m |          -0.0102 |           8.8354 |           4.5540 |
[32m[20221213 22:25:49 @agent_ppo2.py:185][0m |           0.0003 |           9.2876 |           4.5513 |
[32m[20221213 22:25:49 @agent_ppo2.py:185][0m |          -0.0061 |           8.9141 |           4.5496 |
[32m[20221213 22:25:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:25:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.00
[32m[20221213 22:25:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.02
[32m[20221213 22:25:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.36
[32m[20221213 22:25:49 @agent_ppo2.py:143][0m Total time:       7.60 min
[32m[20221213 22:25:49 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221213 22:25:49 @agent_ppo2.py:121][0m #------------------------ Iteration 351 --------------------------#
[32m[20221213 22:25:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:49 @agent_ppo2.py:185][0m |          -0.0030 |          11.0007 |           4.4966 |
[32m[20221213 22:25:49 @agent_ppo2.py:185][0m |          -0.0011 |          10.6707 |           4.4884 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0076 |          10.5611 |           4.4843 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0080 |          10.4323 |           4.4806 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0082 |          10.4412 |           4.4803 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0046 |          10.5066 |           4.4829 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0067 |          10.4798 |           4.4807 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0072 |          10.3576 |           4.4805 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0026 |          10.4012 |           4.4768 |
[32m[20221213 22:25:50 @agent_ppo2.py:185][0m |          -0.0128 |          10.3670 |           4.4797 |
[32m[20221213 22:25:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:25:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.22
[32m[20221213 22:25:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.53
[32m[20221213 22:25:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.02
[32m[20221213 22:25:50 @agent_ppo2.py:143][0m Total time:       7.62 min
[32m[20221213 22:25:50 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221213 22:25:50 @agent_ppo2.py:121][0m #------------------------ Iteration 352 --------------------------#
[32m[20221213 22:25:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0007 |          11.1437 |           4.5120 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0057 |          10.5205 |           4.5019 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0074 |          10.3918 |           4.4990 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0053 |          10.3493 |           4.4981 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0086 |          10.2062 |           4.4946 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0085 |          10.1920 |           4.4945 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0132 |          10.1231 |           4.4951 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0101 |          10.1147 |           4.4946 |
[32m[20221213 22:25:51 @agent_ppo2.py:185][0m |          -0.0035 |          10.3781 |           4.4939 |
[32m[20221213 22:25:52 @agent_ppo2.py:185][0m |          -0.0077 |          10.2447 |           4.4943 |
[32m[20221213 22:25:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:25:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.18
[32m[20221213 22:25:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.82
[32m[20221213 22:25:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 21.27
[32m[20221213 22:25:52 @agent_ppo2.py:143][0m Total time:       7.65 min
[32m[20221213 22:25:52 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221213 22:25:52 @agent_ppo2.py:121][0m #------------------------ Iteration 353 --------------------------#
[32m[20221213 22:25:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:52 @agent_ppo2.py:185][0m |          -0.0033 |          22.5256 |           4.4644 |
[32m[20221213 22:25:52 @agent_ppo2.py:185][0m |          -0.0052 |          20.9055 |           4.4667 |
[32m[20221213 22:25:52 @agent_ppo2.py:185][0m |          -0.0063 |          20.7507 |           4.4653 |
[32m[20221213 22:25:52 @agent_ppo2.py:185][0m |          -0.0070 |          20.7014 |           4.4604 |
[32m[20221213 22:25:52 @agent_ppo2.py:185][0m |          -0.0061 |          20.6293 |           4.4637 |
[32m[20221213 22:25:52 @agent_ppo2.py:185][0m |          -0.0117 |          20.5538 |           4.4649 |
[32m[20221213 22:25:53 @agent_ppo2.py:185][0m |          -0.0112 |          20.4696 |           4.4629 |
[32m[20221213 22:25:53 @agent_ppo2.py:185][0m |          -0.0057 |          20.4794 |           4.4633 |
[32m[20221213 22:25:53 @agent_ppo2.py:185][0m |          -0.0100 |          20.4092 |           4.4617 |
[32m[20221213 22:25:53 @agent_ppo2.py:185][0m |          -0.0150 |          20.3940 |           4.4665 |
[32m[20221213 22:25:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.17
[32m[20221213 22:25:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.98
[32m[20221213 22:25:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 184.56
[32m[20221213 22:25:53 @agent_ppo2.py:143][0m Total time:       7.67 min
[32m[20221213 22:25:53 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221213 22:25:53 @agent_ppo2.py:121][0m #------------------------ Iteration 354 --------------------------#
[32m[20221213 22:25:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:53 @agent_ppo2.py:185][0m |          -0.0020 |          13.9588 |           4.5254 |
[32m[20221213 22:25:53 @agent_ppo2.py:185][0m |           0.0012 |          12.9130 |           4.5225 |
[32m[20221213 22:25:53 @agent_ppo2.py:185][0m |          -0.0062 |          12.6611 |           4.5205 |
[32m[20221213 22:25:54 @agent_ppo2.py:185][0m |          -0.0079 |          12.6663 |           4.5237 |
[32m[20221213 22:25:54 @agent_ppo2.py:185][0m |          -0.0043 |          12.5414 |           4.5219 |
[32m[20221213 22:25:54 @agent_ppo2.py:185][0m |           0.0067 |          14.2656 |           4.5214 |
[32m[20221213 22:25:54 @agent_ppo2.py:185][0m |          -0.0081 |          12.7244 |           4.5188 |
[32m[20221213 22:25:54 @agent_ppo2.py:185][0m |          -0.0112 |          12.4216 |           4.5224 |
[32m[20221213 22:25:54 @agent_ppo2.py:185][0m |          -0.0162 |          12.4470 |           4.5222 |
[32m[20221213 22:25:54 @agent_ppo2.py:185][0m |          -0.0138 |          12.4193 |           4.5245 |
[32m[20221213 22:25:54 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:25:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.66
[32m[20221213 22:25:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.12
[32m[20221213 22:25:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 209.85
[32m[20221213 22:25:54 @agent_ppo2.py:143][0m Total time:       7.69 min
[32m[20221213 22:25:54 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221213 22:25:54 @agent_ppo2.py:121][0m #------------------------ Iteration 355 --------------------------#
[32m[20221213 22:25:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0056 |          19.6785 |           4.4741 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0072 |          16.9784 |           4.4749 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0038 |          16.6099 |           4.4747 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0084 |          16.4513 |           4.4730 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0104 |          16.3864 |           4.4736 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0100 |          16.2689 |           4.4751 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0106 |          16.2189 |           4.4756 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0099 |          16.2264 |           4.4754 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0112 |          16.1111 |           4.4782 |
[32m[20221213 22:25:55 @agent_ppo2.py:185][0m |          -0.0102 |          16.1051 |           4.4770 |
[32m[20221213 22:25:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:25:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.20
[32m[20221213 22:25:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.72
[32m[20221213 22:25:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.27
[32m[20221213 22:25:56 @agent_ppo2.py:143][0m Total time:       7.71 min
[32m[20221213 22:25:56 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221213 22:25:56 @agent_ppo2.py:121][0m #------------------------ Iteration 356 --------------------------#
[32m[20221213 22:25:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:56 @agent_ppo2.py:185][0m |          -0.0021 |          11.7640 |           4.4911 |
[32m[20221213 22:25:56 @agent_ppo2.py:185][0m |          -0.0034 |          10.4863 |           4.4824 |
[32m[20221213 22:25:56 @agent_ppo2.py:185][0m |          -0.0078 |          10.1952 |           4.4712 |
[32m[20221213 22:25:56 @agent_ppo2.py:185][0m |          -0.0074 |          10.2276 |           4.4715 |
[32m[20221213 22:25:56 @agent_ppo2.py:185][0m |          -0.0108 |          10.0773 |           4.4643 |
[32m[20221213 22:25:56 @agent_ppo2.py:185][0m |          -0.0049 |          10.1053 |           4.4610 |
[32m[20221213 22:25:56 @agent_ppo2.py:185][0m |          -0.0060 |          10.0445 |           4.4586 |
[32m[20221213 22:25:57 @agent_ppo2.py:185][0m |          -0.0089 |          10.0816 |           4.4560 |
[32m[20221213 22:25:57 @agent_ppo2.py:185][0m |          -0.0060 |           9.9977 |           4.4520 |
[32m[20221213 22:25:57 @agent_ppo2.py:185][0m |          -0.0071 |           9.9977 |           4.4525 |
[32m[20221213 22:25:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:25:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.58
[32m[20221213 22:25:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.03
[32m[20221213 22:25:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.80
[32m[20221213 22:25:57 @agent_ppo2.py:143][0m Total time:       7.73 min
[32m[20221213 22:25:57 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221213 22:25:57 @agent_ppo2.py:121][0m #------------------------ Iteration 357 --------------------------#
[32m[20221213 22:25:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:25:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:57 @agent_ppo2.py:185][0m |          -0.0008 |          10.2795 |           4.5516 |
[32m[20221213 22:25:57 @agent_ppo2.py:185][0m |          -0.0103 |           9.3911 |           4.5431 |
[32m[20221213 22:25:57 @agent_ppo2.py:185][0m |          -0.0045 |           9.2378 |           4.5419 |
[32m[20221213 22:25:58 @agent_ppo2.py:185][0m |          -0.0139 |           9.1184 |           4.5415 |
[32m[20221213 22:25:58 @agent_ppo2.py:185][0m |          -0.0044 |           9.0437 |           4.5414 |
[32m[20221213 22:25:58 @agent_ppo2.py:185][0m |          -0.0137 |           8.9555 |           4.5378 |
[32m[20221213 22:25:58 @agent_ppo2.py:185][0m |          -0.0141 |           8.9122 |           4.5383 |
[32m[20221213 22:25:58 @agent_ppo2.py:185][0m |          -0.0051 |           8.9959 |           4.5368 |
[32m[20221213 22:25:58 @agent_ppo2.py:185][0m |          -0.0088 |           8.8733 |           4.5349 |
[32m[20221213 22:25:58 @agent_ppo2.py:185][0m |          -0.0095 |           8.8859 |           4.5395 |
[32m[20221213 22:25:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:25:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.79
[32m[20221213 22:25:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.03
[32m[20221213 22:25:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.41
[32m[20221213 22:25:58 @agent_ppo2.py:143][0m Total time:       7.75 min
[32m[20221213 22:25:58 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221213 22:25:58 @agent_ppo2.py:121][0m #------------------------ Iteration 358 --------------------------#
[32m[20221213 22:25:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:25:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |           0.0028 |          19.7003 |           4.4916 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0020 |          16.3373 |           4.4864 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0069 |          16.0366 |           4.4891 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0056 |          15.8304 |           4.4890 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0057 |          15.7040 |           4.4844 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0102 |          15.6370 |           4.4866 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0019 |          16.6995 |           4.4868 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0054 |          15.5005 |           4.4861 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0099 |          15.4264 |           4.4840 |
[32m[20221213 22:25:59 @agent_ppo2.py:185][0m |          -0.0109 |          15.3430 |           4.4865 |
[32m[20221213 22:25:59 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:25:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.22
[32m[20221213 22:25:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.17
[32m[20221213 22:25:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 171.52
[32m[20221213 22:25:59 @agent_ppo2.py:143][0m Total time:       7.78 min
[32m[20221213 22:25:59 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221213 22:25:59 @agent_ppo2.py:121][0m #------------------------ Iteration 359 --------------------------#
[32m[20221213 22:26:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:26:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |           0.0003 |          12.8781 |           4.6085 |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |          -0.0005 |          10.5031 |           4.6040 |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |           0.0067 |          10.4487 |           4.5999 |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |          -0.0108 |           9.8300 |           4.5939 |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |          -0.0085 |           9.5943 |           4.5966 |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |           0.0007 |           9.5792 |           4.5959 |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |           0.0013 |           9.4344 |           4.5922 |
[32m[20221213 22:26:00 @agent_ppo2.py:185][0m |          -0.0051 |           9.2669 |           4.5951 |
[32m[20221213 22:26:01 @agent_ppo2.py:185][0m |          -0.0118 |           9.1779 |           4.5940 |
[32m[20221213 22:26:01 @agent_ppo2.py:185][0m |          -0.0109 |           9.1908 |           4.5950 |
[32m[20221213 22:26:01 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:26:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.79
[32m[20221213 22:26:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.45
[32m[20221213 22:26:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.43
[32m[20221213 22:26:01 @agent_ppo2.py:143][0m Total time:       7.80 min
[32m[20221213 22:26:01 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221213 22:26:01 @agent_ppo2.py:121][0m #------------------------ Iteration 360 --------------------------#
[32m[20221213 22:26:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:26:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:01 @agent_ppo2.py:185][0m |          -0.0012 |          16.2487 |           4.4863 |
[32m[20221213 22:26:01 @agent_ppo2.py:185][0m |           0.0024 |          13.5871 |           4.4827 |
[32m[20221213 22:26:01 @agent_ppo2.py:185][0m |          -0.0064 |          13.0366 |           4.4763 |
[32m[20221213 22:26:01 @agent_ppo2.py:185][0m |          -0.0042 |          12.9773 |           4.4728 |
[32m[20221213 22:26:02 @agent_ppo2.py:185][0m |          -0.0038 |          12.9458 |           4.4724 |
[32m[20221213 22:26:02 @agent_ppo2.py:185][0m |          -0.0101 |          12.6461 |           4.4696 |
[32m[20221213 22:26:02 @agent_ppo2.py:185][0m |          -0.0099 |          12.6511 |           4.4679 |
[32m[20221213 22:26:02 @agent_ppo2.py:185][0m |          -0.0119 |          12.5811 |           4.4661 |
[32m[20221213 22:26:02 @agent_ppo2.py:185][0m |          -0.0147 |          12.5137 |           4.4659 |
[32m[20221213 22:26:02 @agent_ppo2.py:185][0m |          -0.0136 |          12.3843 |           4.4667 |
[32m[20221213 22:26:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:26:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 11.13
[32m[20221213 22:26:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 41.45
[32m[20221213 22:26:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.43
[32m[20221213 22:26:02 @agent_ppo2.py:143][0m Total time:       7.82 min
[32m[20221213 22:26:02 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221213 22:26:02 @agent_ppo2.py:121][0m #------------------------ Iteration 361 --------------------------#
[32m[20221213 22:26:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:02 @agent_ppo2.py:185][0m |           0.0057 |          12.6431 |           4.4918 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0079 |          10.2787 |           4.4840 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0051 |           9.9799 |           4.4813 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0081 |           9.5589 |           4.4777 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0019 |           9.9231 |           4.4770 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0088 |           9.2520 |           4.4758 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0098 |           9.1767 |           4.4741 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0132 |           9.0966 |           4.4744 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0090 |           9.0079 |           4.4744 |
[32m[20221213 22:26:03 @agent_ppo2.py:185][0m |          -0.0108 |           9.0005 |           4.4742 |
[32m[20221213 22:26:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:26:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.84
[32m[20221213 22:26:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 74.66
[32m[20221213 22:26:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.70
[32m[20221213 22:26:03 @agent_ppo2.py:143][0m Total time:       7.84 min
[32m[20221213 22:26:03 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221213 22:26:03 @agent_ppo2.py:121][0m #------------------------ Iteration 362 --------------------------#
[32m[20221213 22:26:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0022 |          12.5790 |           4.5505 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0062 |          10.1101 |           4.5442 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0055 |           9.3658 |           4.5397 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0110 |           8.9369 |           4.5393 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0011 |           8.6975 |           4.5369 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0042 |           8.5207 |           4.5365 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0117 |           8.4574 |           4.5333 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0074 |           8.3581 |           4.5322 |
[32m[20221213 22:26:04 @agent_ppo2.py:185][0m |          -0.0106 |           8.2553 |           4.5288 |
[32m[20221213 22:26:05 @agent_ppo2.py:185][0m |          -0.0128 |           8.1903 |           4.5262 |
[32m[20221213 22:26:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:26:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.93
[32m[20221213 22:26:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.22
[32m[20221213 22:26:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.62
[32m[20221213 22:26:05 @agent_ppo2.py:143][0m Total time:       7.86 min
[32m[20221213 22:26:05 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221213 22:26:05 @agent_ppo2.py:121][0m #------------------------ Iteration 363 --------------------------#
[32m[20221213 22:26:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:05 @agent_ppo2.py:185][0m |           0.0003 |          15.0311 |           4.4252 |
[32m[20221213 22:26:05 @agent_ppo2.py:185][0m |          -0.0087 |          14.2655 |           4.4197 |
[32m[20221213 22:26:05 @agent_ppo2.py:185][0m |          -0.0040 |          14.1296 |           4.4143 |
[32m[20221213 22:26:05 @agent_ppo2.py:185][0m |          -0.0039 |          13.9492 |           4.4177 |
[32m[20221213 22:26:05 @agent_ppo2.py:185][0m |          -0.0074 |          13.6946 |           4.4136 |
[32m[20221213 22:26:05 @agent_ppo2.py:185][0m |          -0.0118 |          13.5391 |           4.4173 |
[32m[20221213 22:26:06 @agent_ppo2.py:185][0m |           0.0077 |          15.7144 |           4.4143 |
[32m[20221213 22:26:06 @agent_ppo2.py:185][0m |          -0.0072 |          13.6138 |           4.4155 |
[32m[20221213 22:26:06 @agent_ppo2.py:185][0m |          -0.0068 |          13.4415 |           4.4144 |
[32m[20221213 22:26:06 @agent_ppo2.py:185][0m |          -0.0047 |          14.1341 |           4.4158 |
[32m[20221213 22:26:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:26:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.75
[32m[20221213 22:26:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.30
[32m[20221213 22:26:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.15
[32m[20221213 22:26:06 @agent_ppo2.py:143][0m Total time:       7.89 min
[32m[20221213 22:26:06 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221213 22:26:06 @agent_ppo2.py:121][0m #------------------------ Iteration 364 --------------------------#
[32m[20221213 22:26:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:06 @agent_ppo2.py:185][0m |           0.0022 |          13.2583 |           4.5225 |
[32m[20221213 22:26:06 @agent_ppo2.py:185][0m |          -0.0021 |          12.6297 |           4.5202 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0034 |          12.4292 |           4.5187 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0055 |          12.2826 |           4.5188 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0074 |          12.1629 |           4.5149 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0038 |          12.3092 |           4.5156 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0034 |          12.0548 |           4.5133 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0093 |          12.0406 |           4.5149 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0079 |          11.9810 |           4.5106 |
[32m[20221213 22:26:07 @agent_ppo2.py:185][0m |          -0.0069 |          11.9259 |           4.5116 |
[32m[20221213 22:26:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:26:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.12
[32m[20221213 22:26:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 116.00
[32m[20221213 22:26:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.67
[32m[20221213 22:26:07 @agent_ppo2.py:143][0m Total time:       7.91 min
[32m[20221213 22:26:07 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221213 22:26:07 @agent_ppo2.py:121][0m #------------------------ Iteration 365 --------------------------#
[32m[20221213 22:26:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |           0.0075 |           7.3338 |           4.6240 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0021 |           4.0004 |           4.6191 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0008 |           3.7099 |           4.6177 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0012 |           3.5829 |           4.6155 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0024 |           3.4384 |           4.6157 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0079 |           3.3799 |           4.6141 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |           0.0013 |           3.2941 |           4.6134 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0058 |           3.2305 |           4.6135 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0015 |           3.2619 |           4.6148 |
[32m[20221213 22:26:08 @agent_ppo2.py:185][0m |          -0.0100 |           3.1553 |           4.6136 |
[32m[20221213 22:26:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:26:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.49
[32m[20221213 22:26:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.88
[32m[20221213 22:26:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.29
[32m[20221213 22:26:09 @agent_ppo2.py:143][0m Total time:       7.93 min
[32m[20221213 22:26:09 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221213 22:26:09 @agent_ppo2.py:121][0m #------------------------ Iteration 366 --------------------------#
[32m[20221213 22:26:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:09 @agent_ppo2.py:185][0m |          -0.0011 |          13.5231 |           4.5711 |
[32m[20221213 22:26:09 @agent_ppo2.py:185][0m |          -0.0073 |          11.7221 |           4.5645 |
[32m[20221213 22:26:09 @agent_ppo2.py:185][0m |          -0.0012 |          11.0554 |           4.5571 |
[32m[20221213 22:26:09 @agent_ppo2.py:185][0m |          -0.0075 |          10.7607 |           4.5540 |
[32m[20221213 22:26:09 @agent_ppo2.py:185][0m |          -0.0097 |          10.6682 |           4.5556 |
[32m[20221213 22:26:09 @agent_ppo2.py:185][0m |          -0.0030 |          10.5605 |           4.5544 |
[32m[20221213 22:26:10 @agent_ppo2.py:185][0m |          -0.0024 |          10.8580 |           4.5591 |
[32m[20221213 22:26:10 @agent_ppo2.py:185][0m |          -0.0081 |          10.4555 |           4.5597 |
[32m[20221213 22:26:10 @agent_ppo2.py:185][0m |          -0.0184 |          10.2424 |           4.5572 |
[32m[20221213 22:26:10 @agent_ppo2.py:185][0m |          -0.0053 |          10.4917 |           4.5571 |
[32m[20221213 22:26:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:26:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 69.93
[32m[20221213 22:26:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.88
[32m[20221213 22:26:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.51
[32m[20221213 22:26:10 @agent_ppo2.py:143][0m Total time:       7.95 min
[32m[20221213 22:26:10 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221213 22:26:10 @agent_ppo2.py:121][0m #------------------------ Iteration 367 --------------------------#
[32m[20221213 22:26:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:10 @agent_ppo2.py:185][0m |          -0.0003 |           8.7376 |           4.5650 |
[32m[20221213 22:26:10 @agent_ppo2.py:185][0m |          -0.0052 |           5.0107 |           4.5541 |
[32m[20221213 22:26:10 @agent_ppo2.py:185][0m |          -0.0022 |           4.7361 |           4.5511 |
[32m[20221213 22:26:11 @agent_ppo2.py:185][0m |          -0.0115 |           4.6124 |           4.5480 |
[32m[20221213 22:26:11 @agent_ppo2.py:185][0m |          -0.0014 |           4.5353 |           4.5465 |
[32m[20221213 22:26:11 @agent_ppo2.py:185][0m |          -0.0159 |           4.4845 |           4.5431 |
[32m[20221213 22:26:11 @agent_ppo2.py:185][0m |          -0.0030 |           4.4484 |           4.5408 |
[32m[20221213 22:26:11 @agent_ppo2.py:185][0m |          -0.0120 |           4.5179 |           4.5392 |
[32m[20221213 22:26:11 @agent_ppo2.py:185][0m |          -0.0105 |           4.3951 |           4.5399 |
[32m[20221213 22:26:11 @agent_ppo2.py:185][0m |          -0.0090 |           4.4099 |           4.5368 |
[32m[20221213 22:26:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:26:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.54
[32m[20221213 22:26:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.31
[32m[20221213 22:26:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.64
[32m[20221213 22:26:11 @agent_ppo2.py:143][0m Total time:       7.97 min
[32m[20221213 22:26:11 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221213 22:26:11 @agent_ppo2.py:121][0m #------------------------ Iteration 368 --------------------------#
[32m[20221213 22:26:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0029 |          19.6346 |           4.5694 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0033 |          18.3724 |           4.5637 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0090 |          18.0814 |           4.5613 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0068 |          17.9476 |           4.5604 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0075 |          17.8612 |           4.5607 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0069 |          17.7562 |           4.5579 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0050 |          18.9222 |           4.5588 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0091 |          17.6307 |           4.5613 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0106 |          17.4408 |           4.5578 |
[32m[20221213 22:26:12 @agent_ppo2.py:185][0m |          -0.0038 |          18.9698 |           4.5592 |
[32m[20221213 22:26:12 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:26:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.77
[32m[20221213 22:26:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.04
[32m[20221213 22:26:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.60
[32m[20221213 22:26:13 @agent_ppo2.py:143][0m Total time:       7.99 min
[32m[20221213 22:26:13 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221213 22:26:13 @agent_ppo2.py:121][0m #------------------------ Iteration 369 --------------------------#
[32m[20221213 22:26:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |           0.0001 |          21.7757 |           4.5039 |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |          -0.0049 |          19.4181 |           4.5068 |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |          -0.0044 |          19.0950 |           4.5068 |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |          -0.0035 |          18.9050 |           4.5074 |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |          -0.0060 |          18.8388 |           4.5076 |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |          -0.0066 |          18.6905 |           4.5073 |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |          -0.0078 |          18.6213 |           4.5053 |
[32m[20221213 22:26:13 @agent_ppo2.py:185][0m |          -0.0087 |          18.5239 |           4.5050 |
[32m[20221213 22:26:14 @agent_ppo2.py:185][0m |          -0.0099 |          18.4679 |           4.5057 |
[32m[20221213 22:26:14 @agent_ppo2.py:185][0m |          -0.0129 |          18.4695 |           4.5066 |
[32m[20221213 22:26:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:26:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 140.27
[32m[20221213 22:26:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 192.94
[32m[20221213 22:26:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.08
[32m[20221213 22:26:14 @agent_ppo2.py:143][0m Total time:       8.02 min
[32m[20221213 22:26:14 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221213 22:26:14 @agent_ppo2.py:121][0m #------------------------ Iteration 370 --------------------------#
[32m[20221213 22:26:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:26:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:14 @agent_ppo2.py:185][0m |          -0.0008 |          12.9389 |           4.5647 |
[32m[20221213 22:26:14 @agent_ppo2.py:185][0m |          -0.0116 |          11.2263 |           4.5621 |
[32m[20221213 22:26:14 @agent_ppo2.py:185][0m |          -0.0027 |          10.8043 |           4.5609 |
[32m[20221213 22:26:14 @agent_ppo2.py:185][0m |          -0.0074 |          10.4875 |           4.5596 |
[32m[20221213 22:26:15 @agent_ppo2.py:185][0m |          -0.0038 |          10.2995 |           4.5600 |
[32m[20221213 22:26:15 @agent_ppo2.py:185][0m |          -0.0109 |          10.0873 |           4.5622 |
[32m[20221213 22:26:15 @agent_ppo2.py:185][0m |          -0.0070 |           9.8815 |           4.5626 |
[32m[20221213 22:26:15 @agent_ppo2.py:185][0m |          -0.0089 |           9.8119 |           4.5636 |
[32m[20221213 22:26:15 @agent_ppo2.py:185][0m |          -0.0104 |           9.7882 |           4.5620 |
[32m[20221213 22:26:15 @agent_ppo2.py:185][0m |          -0.0107 |           9.5974 |           4.5609 |
[32m[20221213 22:26:15 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:26:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 72.05
[32m[20221213 22:26:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.88
[32m[20221213 22:26:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.38
[32m[20221213 22:26:15 @agent_ppo2.py:143][0m Total time:       8.04 min
[32m[20221213 22:26:15 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221213 22:26:15 @agent_ppo2.py:121][0m #------------------------ Iteration 371 --------------------------#
[32m[20221213 22:26:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:15 @agent_ppo2.py:185][0m |          -0.0025 |           9.8689 |           4.5940 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |          -0.0098 |           9.1010 |           4.5869 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |           0.0047 |           9.1558 |           4.5800 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |          -0.0087 |           8.7050 |           4.5791 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |          -0.0009 |           8.8908 |           4.5779 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |           0.0007 |           8.5595 |           4.5745 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |          -0.0133 |           8.3866 |           4.5748 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |          -0.0127 |           8.2715 |           4.5720 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |          -0.0062 |           8.2009 |           4.5706 |
[32m[20221213 22:26:16 @agent_ppo2.py:185][0m |          -0.0073 |           8.0932 |           4.5707 |
[32m[20221213 22:26:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:26:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.64
[32m[20221213 22:26:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.82
[32m[20221213 22:26:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.39
[32m[20221213 22:26:16 @agent_ppo2.py:143][0m Total time:       8.06 min
[32m[20221213 22:26:16 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221213 22:26:16 @agent_ppo2.py:121][0m #------------------------ Iteration 372 --------------------------#
[32m[20221213 22:26:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0013 |          17.2901 |           4.5217 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0010 |          16.9766 |           4.5161 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0005 |          17.6011 |           4.5118 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0132 |          15.8769 |           4.5154 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |           0.0019 |          16.4773 |           4.5175 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0178 |          15.7796 |           4.5191 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0117 |          15.5865 |           4.5206 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0150 |          15.4604 |           4.5192 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0097 |          15.3166 |           4.5240 |
[32m[20221213 22:26:17 @agent_ppo2.py:185][0m |          -0.0105 |          15.3208 |           4.5213 |
[32m[20221213 22:26:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:26:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.32
[32m[20221213 22:26:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 224.03
[32m[20221213 22:26:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.54
[32m[20221213 22:26:18 @agent_ppo2.py:143][0m Total time:       8.08 min
[32m[20221213 22:26:18 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221213 22:26:18 @agent_ppo2.py:121][0m #------------------------ Iteration 373 --------------------------#
[32m[20221213 22:26:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:18 @agent_ppo2.py:185][0m |           0.0128 |          21.8169 |           4.5454 |
[32m[20221213 22:26:18 @agent_ppo2.py:185][0m |           0.0019 |          20.0080 |           4.5358 |
[32m[20221213 22:26:18 @agent_ppo2.py:185][0m |          -0.0058 |          19.3474 |           4.5325 |
[32m[20221213 22:26:18 @agent_ppo2.py:185][0m |          -0.0103 |          19.2557 |           4.5313 |
[32m[20221213 22:26:18 @agent_ppo2.py:185][0m |          -0.0059 |          19.2506 |           4.5296 |
[32m[20221213 22:26:18 @agent_ppo2.py:185][0m |           0.0005 |          19.7915 |           4.5327 |
[32m[20221213 22:26:19 @agent_ppo2.py:185][0m |           0.0081 |          20.7612 |           4.5318 |
[32m[20221213 22:26:19 @agent_ppo2.py:185][0m |          -0.0075 |          18.9780 |           4.5277 |
[32m[20221213 22:26:19 @agent_ppo2.py:185][0m |          -0.0081 |          18.7729 |           4.5307 |
[32m[20221213 22:26:19 @agent_ppo2.py:185][0m |           0.0013 |          19.3259 |           4.5301 |
[32m[20221213 22:26:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:26:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.45
[32m[20221213 22:26:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.81
[32m[20221213 22:26:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.47
[32m[20221213 22:26:19 @agent_ppo2.py:143][0m Total time:       8.10 min
[32m[20221213 22:26:19 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221213 22:26:19 @agent_ppo2.py:121][0m #------------------------ Iteration 374 --------------------------#
[32m[20221213 22:26:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:19 @agent_ppo2.py:185][0m |          -0.0019 |          19.5570 |           4.5366 |
[32m[20221213 22:26:19 @agent_ppo2.py:185][0m |          -0.0055 |          19.0577 |           4.5299 |
[32m[20221213 22:26:19 @agent_ppo2.py:185][0m |          -0.0076 |          19.0292 |           4.5268 |
[32m[20221213 22:26:20 @agent_ppo2.py:185][0m |          -0.0058 |          18.8871 |           4.5271 |
[32m[20221213 22:26:20 @agent_ppo2.py:185][0m |          -0.0090 |          18.7843 |           4.5249 |
[32m[20221213 22:26:20 @agent_ppo2.py:185][0m |          -0.0028 |          18.7406 |           4.5256 |
[32m[20221213 22:26:20 @agent_ppo2.py:185][0m |          -0.0118 |          18.7480 |           4.5268 |
[32m[20221213 22:26:20 @agent_ppo2.py:185][0m |           0.0095 |          21.0244 |           4.5240 |
[32m[20221213 22:26:20 @agent_ppo2.py:185][0m |          -0.0064 |          18.5916 |           4.5243 |
[32m[20221213 22:26:20 @agent_ppo2.py:185][0m |          -0.0043 |          18.6851 |           4.5218 |
[32m[20221213 22:26:20 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:26:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.14
[32m[20221213 22:26:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.42
[32m[20221213 22:26:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.47
[32m[20221213 22:26:20 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 289.47
[32m[20221213 22:26:20 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 289.47
[32m[20221213 22:26:20 @agent_ppo2.py:143][0m Total time:       8.12 min
[32m[20221213 22:26:20 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221213 22:26:20 @agent_ppo2.py:121][0m #------------------------ Iteration 375 --------------------------#
[32m[20221213 22:26:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |           0.0027 |          14.0756 |           4.5560 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |           0.0064 |           7.1036 |           4.5517 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |          -0.0042 |           6.2420 |           4.5436 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |          -0.0059 |           5.8698 |           4.5449 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |          -0.0010 |           5.6303 |           4.5455 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |          -0.0061 |           5.3239 |           4.5437 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |          -0.0048 |           5.2087 |           4.5436 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |          -0.0144 |           5.1025 |           4.5433 |
[32m[20221213 22:26:21 @agent_ppo2.py:185][0m |           0.0024 |           4.9493 |           4.5419 |
[32m[20221213 22:26:22 @agent_ppo2.py:185][0m |          -0.0094 |           4.8378 |           4.5411 |
[32m[20221213 22:26:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:26:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 111.20
[32m[20221213 22:26:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 192.14
[32m[20221213 22:26:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.03
[32m[20221213 22:26:22 @agent_ppo2.py:143][0m Total time:       8.15 min
[32m[20221213 22:26:22 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221213 22:26:22 @agent_ppo2.py:121][0m #------------------------ Iteration 376 --------------------------#
[32m[20221213 22:26:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:26:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:22 @agent_ppo2.py:185][0m |          -0.0012 |          25.5913 |           4.6304 |
[32m[20221213 22:26:22 @agent_ppo2.py:185][0m |          -0.0119 |          21.2867 |           4.6234 |
[32m[20221213 22:26:22 @agent_ppo2.py:185][0m |          -0.0045 |          20.5169 |           4.6181 |
[32m[20221213 22:26:22 @agent_ppo2.py:185][0m |          -0.0073 |          20.1052 |           4.6198 |
[32m[20221213 22:26:22 @agent_ppo2.py:185][0m |          -0.0113 |          19.7704 |           4.6140 |
[32m[20221213 22:26:22 @agent_ppo2.py:185][0m |          -0.0092 |          19.6709 |           4.6149 |
[32m[20221213 22:26:23 @agent_ppo2.py:185][0m |          -0.0033 |          19.5798 |           4.6127 |
[32m[20221213 22:26:23 @agent_ppo2.py:185][0m |          -0.0096 |          19.3847 |           4.6094 |
[32m[20221213 22:26:23 @agent_ppo2.py:185][0m |          -0.0108 |          19.3712 |           4.6118 |
[32m[20221213 22:26:23 @agent_ppo2.py:185][0m |          -0.0089 |          19.3666 |           4.6093 |
[32m[20221213 22:26:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:26:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.37
[32m[20221213 22:26:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 168.52
[32m[20221213 22:26:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.11
[32m[20221213 22:26:23 @agent_ppo2.py:143][0m Total time:       8.17 min
[32m[20221213 22:26:23 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221213 22:26:23 @agent_ppo2.py:121][0m #------------------------ Iteration 377 --------------------------#
[32m[20221213 22:26:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:26:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:23 @agent_ppo2.py:185][0m |           0.0039 |           9.8560 |           4.5355 |
[32m[20221213 22:26:23 @agent_ppo2.py:185][0m |          -0.0004 |           5.9718 |           4.5269 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0115 |           5.8224 |           4.5287 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0106 |           5.2794 |           4.5301 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0034 |           5.0641 |           4.5328 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0057 |           5.0549 |           4.5332 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0040 |           4.9614 |           4.5342 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0082 |           4.8219 |           4.5353 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0124 |           4.7683 |           4.5370 |
[32m[20221213 22:26:24 @agent_ppo2.py:185][0m |          -0.0076 |           4.7600 |           4.5368 |
[32m[20221213 22:26:24 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:26:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.43
[32m[20221213 22:26:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.87
[32m[20221213 22:26:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 185.20
[32m[20221213 22:26:24 @agent_ppo2.py:143][0m Total time:       8.19 min
[32m[20221213 22:26:24 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221213 22:26:24 @agent_ppo2.py:121][0m #------------------------ Iteration 378 --------------------------#
[32m[20221213 22:26:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:26:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0008 |           9.3494 |           4.6167 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0033 |           7.7060 |           4.6139 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |           0.0012 |           7.4168 |           4.6114 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0062 |           7.3045 |           4.6140 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0029 |           7.1986 |           4.6148 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0067 |           7.1519 |           4.6165 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0006 |           7.0670 |           4.6179 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0113 |           7.0806 |           4.6198 |
[32m[20221213 22:26:25 @agent_ppo2.py:185][0m |          -0.0073 |           6.9268 |           4.6223 |
[32m[20221213 22:26:26 @agent_ppo2.py:185][0m |          -0.0082 |           6.8944 |           4.6278 |
[32m[20221213 22:26:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:26:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.45
[32m[20221213 22:26:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.73
[32m[20221213 22:26:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.46
[32m[20221213 22:26:26 @agent_ppo2.py:143][0m Total time:       8.21 min
[32m[20221213 22:26:26 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221213 22:26:26 @agent_ppo2.py:121][0m #------------------------ Iteration 379 --------------------------#
[32m[20221213 22:26:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:26 @agent_ppo2.py:185][0m |          -0.0011 |           8.1343 |           4.6436 |
[32m[20221213 22:26:26 @agent_ppo2.py:185][0m |          -0.0016 |           6.9868 |           4.6433 |
[32m[20221213 22:26:26 @agent_ppo2.py:185][0m |          -0.0086 |           6.7706 |           4.6402 |
[32m[20221213 22:26:26 @agent_ppo2.py:185][0m |          -0.0052 |           6.7424 |           4.6414 |
[32m[20221213 22:26:26 @agent_ppo2.py:185][0m |          -0.0050 |           6.6895 |           4.6408 |
[32m[20221213 22:26:26 @agent_ppo2.py:185][0m |           0.0008 |           6.7795 |           4.6390 |
[32m[20221213 22:26:27 @agent_ppo2.py:185][0m |          -0.0048 |           6.6240 |           4.6389 |
[32m[20221213 22:26:27 @agent_ppo2.py:185][0m |          -0.0095 |           6.5840 |           4.6411 |
[32m[20221213 22:26:27 @agent_ppo2.py:185][0m |           0.0014 |           6.7624 |           4.6419 |
[32m[20221213 22:26:27 @agent_ppo2.py:185][0m |          -0.0114 |           6.4072 |           4.6419 |
[32m[20221213 22:26:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:26:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.55
[32m[20221213 22:26:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.06
[32m[20221213 22:26:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.60
[32m[20221213 22:26:27 @agent_ppo2.py:143][0m Total time:       8.23 min
[32m[20221213 22:26:27 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221213 22:26:27 @agent_ppo2.py:121][0m #------------------------ Iteration 380 --------------------------#
[32m[20221213 22:26:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:26:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:27 @agent_ppo2.py:185][0m |          -0.0026 |          21.7739 |           4.6944 |
[32m[20221213 22:26:27 @agent_ppo2.py:185][0m |           0.0008 |          17.0748 |           4.6892 |
[32m[20221213 22:26:27 @agent_ppo2.py:185][0m |          -0.0059 |          16.3469 |           4.6880 |
[32m[20221213 22:26:28 @agent_ppo2.py:185][0m |          -0.0088 |          16.0113 |           4.6880 |
[32m[20221213 22:26:28 @agent_ppo2.py:185][0m |          -0.0039 |          15.5465 |           4.6898 |
[32m[20221213 22:26:28 @agent_ppo2.py:185][0m |          -0.0086 |          15.3230 |           4.6899 |
[32m[20221213 22:26:28 @agent_ppo2.py:185][0m |          -0.0093 |          15.1122 |           4.6860 |
[32m[20221213 22:26:28 @agent_ppo2.py:185][0m |          -0.0008 |          16.5892 |           4.6899 |
[32m[20221213 22:26:28 @agent_ppo2.py:185][0m |          -0.0138 |          14.9213 |           4.6889 |
[32m[20221213 22:26:28 @agent_ppo2.py:185][0m |          -0.0059 |          14.6552 |           4.6883 |
[32m[20221213 22:26:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:26:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.74
[32m[20221213 22:26:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.00
[32m[20221213 22:26:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.11
[32m[20221213 22:26:28 @agent_ppo2.py:143][0m Total time:       8.26 min
[32m[20221213 22:26:28 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221213 22:26:28 @agent_ppo2.py:121][0m #------------------------ Iteration 381 --------------------------#
[32m[20221213 22:26:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0061 |          11.7394 |           4.7432 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0023 |          10.0185 |           4.7392 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0128 |           9.8489 |           4.7394 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0033 |           9.7820 |           4.7410 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0021 |           9.9309 |           4.7385 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0072 |           9.7110 |           4.7355 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0048 |           9.6937 |           4.7350 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0071 |           9.6872 |           4.7343 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0084 |           9.6013 |           4.7359 |
[32m[20221213 22:26:29 @agent_ppo2.py:185][0m |          -0.0067 |           9.7550 |           4.7313 |
[32m[20221213 22:26:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:26:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.94
[32m[20221213 22:26:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.92
[32m[20221213 22:26:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 121.80
[32m[20221213 22:26:29 @agent_ppo2.py:143][0m Total time:       8.28 min
[32m[20221213 22:26:29 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221213 22:26:29 @agent_ppo2.py:121][0m #------------------------ Iteration 382 --------------------------#
[32m[20221213 22:26:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0027 |          21.9907 |           4.7048 |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0058 |          20.2101 |           4.6939 |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0077 |          19.9497 |           4.6916 |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0022 |          19.9017 |           4.6939 |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0153 |          19.6966 |           4.6898 |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0065 |          20.0094 |           4.6871 |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0108 |          19.4600 |           4.6882 |
[32m[20221213 22:26:30 @agent_ppo2.py:185][0m |          -0.0124 |          19.3389 |           4.6926 |
[32m[20221213 22:26:31 @agent_ppo2.py:185][0m |          -0.0114 |          19.3688 |           4.6913 |
[32m[20221213 22:26:31 @agent_ppo2.py:185][0m |          -0.0120 |          19.1954 |           4.6925 |
[32m[20221213 22:26:31 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:26:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.09
[32m[20221213 22:26:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.60
[32m[20221213 22:26:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 99.84
[32m[20221213 22:26:31 @agent_ppo2.py:143][0m Total time:       8.30 min
[32m[20221213 22:26:31 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221213 22:26:31 @agent_ppo2.py:121][0m #------------------------ Iteration 383 --------------------------#
[32m[20221213 22:26:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:31 @agent_ppo2.py:185][0m |          -0.0010 |          11.2090 |           4.7141 |
[32m[20221213 22:26:31 @agent_ppo2.py:185][0m |          -0.0069 |           8.7689 |           4.7058 |
[32m[20221213 22:26:31 @agent_ppo2.py:185][0m |          -0.0059 |           8.4811 |           4.6983 |
[32m[20221213 22:26:31 @agent_ppo2.py:185][0m |          -0.0123 |           8.3235 |           4.6950 |
[32m[20221213 22:26:32 @agent_ppo2.py:185][0m |          -0.0047 |           8.4312 |           4.6891 |
[32m[20221213 22:26:32 @agent_ppo2.py:185][0m |          -0.0093 |           8.2483 |           4.6863 |
[32m[20221213 22:26:32 @agent_ppo2.py:185][0m |          -0.0110 |           8.1470 |           4.6823 |
[32m[20221213 22:26:32 @agent_ppo2.py:185][0m |          -0.0039 |           8.3295 |           4.6795 |
[32m[20221213 22:26:32 @agent_ppo2.py:185][0m |          -0.0055 |           8.0920 |           4.6763 |
[32m[20221213 22:26:32 @agent_ppo2.py:185][0m |          -0.0096 |           8.1422 |           4.6768 |
[32m[20221213 22:26:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:26:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.53
[32m[20221213 22:26:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.77
[32m[20221213 22:26:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.64
[32m[20221213 22:26:32 @agent_ppo2.py:143][0m Total time:       8.32 min
[32m[20221213 22:26:32 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221213 22:26:32 @agent_ppo2.py:121][0m #------------------------ Iteration 384 --------------------------#
[32m[20221213 22:26:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:32 @agent_ppo2.py:185][0m |          -0.0029 |          16.9098 |           4.7216 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0068 |          14.6489 |           4.7126 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0045 |          14.1749 |           4.7125 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0005 |          14.0843 |           4.7084 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0007 |          13.6759 |           4.7120 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0050 |          13.4221 |           4.7123 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0048 |          13.7738 |           4.7098 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0047 |          13.1423 |           4.7089 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0057 |          13.1417 |           4.7071 |
[32m[20221213 22:26:33 @agent_ppo2.py:185][0m |          -0.0076 |          12.9343 |           4.7079 |
[32m[20221213 22:26:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:26:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 58.02
[32m[20221213 22:26:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 191.25
[32m[20221213 22:26:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.07
[32m[20221213 22:26:33 @agent_ppo2.py:143][0m Total time:       8.34 min
[32m[20221213 22:26:33 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221213 22:26:33 @agent_ppo2.py:121][0m #------------------------ Iteration 385 --------------------------#
[32m[20221213 22:26:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |           0.0090 |          24.3003 |           4.5954 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0052 |          21.5709 |           4.5899 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0031 |          21.2593 |           4.5925 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0002 |          21.3384 |           4.5924 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0099 |          20.9635 |           4.5939 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0033 |          20.9373 |           4.5953 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0002 |          21.3765 |           4.5984 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0071 |          20.6879 |           4.5989 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0084 |          20.6135 |           4.5992 |
[32m[20221213 22:26:34 @agent_ppo2.py:185][0m |          -0.0096 |          20.5141 |           4.5997 |
[32m[20221213 22:26:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:26:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 141.49
[32m[20221213 22:26:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 201.83
[32m[20221213 22:26:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.11
[32m[20221213 22:26:35 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221213 22:26:35 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221213 22:26:35 @agent_ppo2.py:121][0m #------------------------ Iteration 386 --------------------------#
[32m[20221213 22:26:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:35 @agent_ppo2.py:185][0m |          -0.0043 |          11.3699 |           4.6629 |
[32m[20221213 22:26:35 @agent_ppo2.py:185][0m |          -0.0061 |           9.7514 |           4.6546 |
[32m[20221213 22:26:35 @agent_ppo2.py:185][0m |          -0.0080 |           9.4237 |           4.6513 |
[32m[20221213 22:26:35 @agent_ppo2.py:185][0m |          -0.0065 |           9.2525 |           4.6499 |
[32m[20221213 22:26:35 @agent_ppo2.py:185][0m |          -0.0014 |           9.3668 |           4.6493 |
[32m[20221213 22:26:35 @agent_ppo2.py:185][0m |          -0.0085 |           9.0931 |           4.6473 |
[32m[20221213 22:26:35 @agent_ppo2.py:185][0m |          -0.0075 |           9.1094 |           4.6487 |
[32m[20221213 22:26:36 @agent_ppo2.py:185][0m |          -0.0088 |           9.0358 |           4.6448 |
[32m[20221213 22:26:36 @agent_ppo2.py:185][0m |          -0.0108 |           9.0907 |           4.6465 |
[32m[20221213 22:26:36 @agent_ppo2.py:185][0m |          -0.0043 |           9.3888 |           4.6485 |
[32m[20221213 22:26:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:26:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 7.28
[32m[20221213 22:26:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 36.42
[32m[20221213 22:26:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.10
[32m[20221213 22:26:36 @agent_ppo2.py:143][0m Total time:       8.38 min
[32m[20221213 22:26:36 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221213 22:26:36 @agent_ppo2.py:121][0m #------------------------ Iteration 387 --------------------------#
[32m[20221213 22:26:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:36 @agent_ppo2.py:185][0m |          -0.0033 |          17.4528 |           4.6355 |
[32m[20221213 22:26:36 @agent_ppo2.py:185][0m |          -0.0083 |          15.9663 |           4.6337 |
[32m[20221213 22:26:36 @agent_ppo2.py:185][0m |          -0.0080 |          15.8119 |           4.6292 |
[32m[20221213 22:26:36 @agent_ppo2.py:185][0m |          -0.0088 |          15.5566 |           4.6239 |
[32m[20221213 22:26:37 @agent_ppo2.py:185][0m |          -0.0095 |          15.5815 |           4.6257 |
[32m[20221213 22:26:37 @agent_ppo2.py:185][0m |          -0.0124 |          15.5638 |           4.6242 |
[32m[20221213 22:26:37 @agent_ppo2.py:185][0m |          -0.0112 |          15.4280 |           4.6238 |
[32m[20221213 22:26:37 @agent_ppo2.py:185][0m |          -0.0059 |          15.3532 |           4.6212 |
[32m[20221213 22:26:37 @agent_ppo2.py:185][0m |          -0.0076 |          15.3463 |           4.6255 |
[32m[20221213 22:26:37 @agent_ppo2.py:185][0m |          -0.0091 |          15.2923 |           4.6188 |
[32m[20221213 22:26:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:26:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.53
[32m[20221213 22:26:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 176.79
[32m[20221213 22:26:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.89
[32m[20221213 22:26:37 @agent_ppo2.py:143][0m Total time:       8.40 min
[32m[20221213 22:26:37 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221213 22:26:37 @agent_ppo2.py:121][0m #------------------------ Iteration 388 --------------------------#
[32m[20221213 22:26:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:37 @agent_ppo2.py:185][0m |           0.0042 |          10.1086 |           4.7291 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0048 |           6.9538 |           4.7226 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0030 |           6.3559 |           4.7223 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0065 |           6.0843 |           4.7238 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0036 |           5.8297 |           4.7275 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0051 |           5.7114 |           4.7287 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0133 |           5.5974 |           4.7327 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0113 |           5.5022 |           4.7340 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0155 |           5.5212 |           4.7359 |
[32m[20221213 22:26:38 @agent_ppo2.py:185][0m |          -0.0066 |           5.4079 |           4.7404 |
[32m[20221213 22:26:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:26:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.20
[32m[20221213 22:26:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.40
[32m[20221213 22:26:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.95
[32m[20221213 22:26:38 @agent_ppo2.py:143][0m Total time:       8.42 min
[32m[20221213 22:26:38 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221213 22:26:38 @agent_ppo2.py:121][0m #------------------------ Iteration 389 --------------------------#
[32m[20221213 22:26:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0006 |          25.4663 |           4.7898 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0088 |          23.4880 |           4.7824 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0179 |          23.1499 |           4.7834 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0064 |          22.9022 |           4.7810 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0050 |          22.7990 |           4.7850 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0073 |          22.1003 |           4.7825 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0075 |          21.9931 |           4.7818 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0107 |          21.8309 |           4.7864 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0090 |          21.7690 |           4.7836 |
[32m[20221213 22:26:39 @agent_ppo2.py:185][0m |          -0.0176 |          21.9293 |           4.7856 |
[32m[20221213 22:26:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:26:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 190.36
[32m[20221213 22:26:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.63
[32m[20221213 22:26:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.46
[32m[20221213 22:26:40 @agent_ppo2.py:143][0m Total time:       8.45 min
[32m[20221213 22:26:40 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221213 22:26:40 @agent_ppo2.py:121][0m #------------------------ Iteration 390 --------------------------#
[32m[20221213 22:26:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:26:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:40 @agent_ppo2.py:185][0m |          -0.0026 |          21.5731 |           4.7849 |
[32m[20221213 22:26:40 @agent_ppo2.py:185][0m |          -0.0079 |          18.8318 |           4.7794 |
[32m[20221213 22:26:40 @agent_ppo2.py:185][0m |           0.0004 |          18.1442 |           4.7789 |
[32m[20221213 22:26:40 @agent_ppo2.py:185][0m |          -0.0070 |          17.6881 |           4.7759 |
[32m[20221213 22:26:40 @agent_ppo2.py:185][0m |          -0.0027 |          17.4592 |           4.7758 |
[32m[20221213 22:26:40 @agent_ppo2.py:185][0m |          -0.0057 |          17.1338 |           4.7745 |
[32m[20221213 22:26:40 @agent_ppo2.py:185][0m |          -0.0084 |          16.9788 |           4.7777 |
[32m[20221213 22:26:41 @agent_ppo2.py:185][0m |          -0.0096 |          16.8165 |           4.7767 |
[32m[20221213 22:26:41 @agent_ppo2.py:185][0m |          -0.0135 |          16.6157 |           4.7768 |
[32m[20221213 22:26:41 @agent_ppo2.py:185][0m |          -0.0069 |          16.5300 |           4.7776 |
[32m[20221213 22:26:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:26:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.98
[32m[20221213 22:26:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.35
[32m[20221213 22:26:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.47
[32m[20221213 22:26:41 @agent_ppo2.py:143][0m Total time:       8.47 min
[32m[20221213 22:26:41 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221213 22:26:41 @agent_ppo2.py:121][0m #------------------------ Iteration 391 --------------------------#
[32m[20221213 22:26:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:41 @agent_ppo2.py:185][0m |          -0.0030 |          15.7240 |           4.8652 |
[32m[20221213 22:26:41 @agent_ppo2.py:185][0m |          -0.0044 |          15.0210 |           4.8620 |
[32m[20221213 22:26:41 @agent_ppo2.py:185][0m |          -0.0125 |          14.8117 |           4.8585 |
[32m[20221213 22:26:41 @agent_ppo2.py:185][0m |          -0.0030 |          14.7688 |           4.8551 |
[32m[20221213 22:26:42 @agent_ppo2.py:185][0m |          -0.0131 |          14.5759 |           4.8566 |
[32m[20221213 22:26:42 @agent_ppo2.py:185][0m |          -0.0131 |          14.4380 |           4.8554 |
[32m[20221213 22:26:42 @agent_ppo2.py:185][0m |           0.0045 |          15.1564 |           4.8515 |
[32m[20221213 22:26:42 @agent_ppo2.py:185][0m |          -0.0093 |          14.6566 |           4.8513 |
[32m[20221213 22:26:42 @agent_ppo2.py:185][0m |          -0.0135 |          14.1728 |           4.8514 |
[32m[20221213 22:26:42 @agent_ppo2.py:185][0m |          -0.0062 |          14.0812 |           4.8495 |
[32m[20221213 22:26:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:26:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.29
[32m[20221213 22:26:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 131.38
[32m[20221213 22:26:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:26:42 @agent_ppo2.py:143][0m Total time:       8.49 min
[32m[20221213 22:26:42 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221213 22:26:42 @agent_ppo2.py:121][0m #------------------------ Iteration 392 --------------------------#
[32m[20221213 22:26:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:42 @agent_ppo2.py:185][0m |          -0.0007 |          12.2145 |           4.8843 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0078 |          11.5778 |           4.8788 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0105 |          11.4484 |           4.8745 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0003 |          11.3998 |           4.8689 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0073 |          11.1816 |           4.8731 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0092 |          11.0372 |           4.8736 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0099 |          11.0100 |           4.8727 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0076 |          10.9367 |           4.8730 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0096 |          10.9056 |           4.8723 |
[32m[20221213 22:26:43 @agent_ppo2.py:185][0m |          -0.0124 |          10.8695 |           4.8714 |
[32m[20221213 22:26:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:26:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.78
[32m[20221213 22:26:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 222.45
[32m[20221213 22:26:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.57
[32m[20221213 22:26:43 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 298.57
[32m[20221213 22:26:43 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 298.57
[32m[20221213 22:26:43 @agent_ppo2.py:143][0m Total time:       8.51 min
[32m[20221213 22:26:43 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221213 22:26:43 @agent_ppo2.py:121][0m #------------------------ Iteration 393 --------------------------#
[32m[20221213 22:26:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0005 |          15.1449 |           4.7547 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0062 |          14.4845 |           4.7511 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0093 |          14.3578 |           4.7457 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0048 |          14.1149 |           4.7499 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0073 |          14.0538 |           4.7459 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0127 |          13.9765 |           4.7528 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0089 |          13.8830 |           4.7524 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0141 |          13.7660 |           4.7548 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0126 |          13.7270 |           4.7551 |
[32m[20221213 22:26:44 @agent_ppo2.py:185][0m |          -0.0140 |          13.6744 |           4.7580 |
[32m[20221213 22:26:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:26:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.75
[32m[20221213 22:26:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.08
[32m[20221213 22:26:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:26:45 @agent_ppo2.py:143][0m Total time:       8.53 min
[32m[20221213 22:26:45 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221213 22:26:45 @agent_ppo2.py:121][0m #------------------------ Iteration 394 --------------------------#
[32m[20221213 22:26:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |          -0.0019 |           8.2683 |           4.9674 |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |          -0.0035 |           5.3454 |           4.9642 |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |           0.0036 |           5.1588 |           4.9663 |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |          -0.0020 |           5.3350 |           4.9620 |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |          -0.0132 |           4.9084 |           4.9599 |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |          -0.0074 |           4.8182 |           4.9583 |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |          -0.0156 |           4.8094 |           4.9548 |
[32m[20221213 22:26:45 @agent_ppo2.py:185][0m |          -0.0062 |           4.7288 |           4.9579 |
[32m[20221213 22:26:46 @agent_ppo2.py:185][0m |          -0.0041 |           4.8054 |           4.9572 |
[32m[20221213 22:26:46 @agent_ppo2.py:185][0m |          -0.0115 |           4.8336 |           4.9564 |
[32m[20221213 22:26:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:26:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.07
[32m[20221213 22:26:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.97
[32m[20221213 22:26:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:26:46 @agent_ppo2.py:143][0m Total time:       8.55 min
[32m[20221213 22:26:46 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221213 22:26:46 @agent_ppo2.py:121][0m #------------------------ Iteration 395 --------------------------#
[32m[20221213 22:26:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:46 @agent_ppo2.py:185][0m |          -0.0001 |           9.8773 |           4.8686 |
[32m[20221213 22:26:46 @agent_ppo2.py:185][0m |          -0.0048 |           7.0197 |           4.8644 |
[32m[20221213 22:26:46 @agent_ppo2.py:185][0m |          -0.0054 |           6.5648 |           4.8657 |
[32m[20221213 22:26:46 @agent_ppo2.py:185][0m |          -0.0051 |           6.1779 |           4.8640 |
[32m[20221213 22:26:46 @agent_ppo2.py:185][0m |          -0.0097 |           5.9438 |           4.8664 |
[32m[20221213 22:26:47 @agent_ppo2.py:185][0m |          -0.0070 |           5.7739 |           4.8661 |
[32m[20221213 22:26:47 @agent_ppo2.py:185][0m |          -0.0060 |           5.5901 |           4.8646 |
[32m[20221213 22:26:47 @agent_ppo2.py:185][0m |          -0.0054 |           5.5194 |           4.8647 |
[32m[20221213 22:26:47 @agent_ppo2.py:185][0m |          -0.0150 |           5.2899 |           4.8667 |
[32m[20221213 22:26:47 @agent_ppo2.py:185][0m |          -0.0108 |           5.2582 |           4.8661 |
[32m[20221213 22:26:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:26:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 4.67
[32m[20221213 22:26:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 23.35
[32m[20221213 22:26:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.23
[32m[20221213 22:26:47 @agent_ppo2.py:143][0m Total time:       8.57 min
[32m[20221213 22:26:47 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221213 22:26:47 @agent_ppo2.py:121][0m #------------------------ Iteration 396 --------------------------#
[32m[20221213 22:26:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:26:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:47 @agent_ppo2.py:185][0m |           0.0021 |           7.8127 |           4.8421 |
[32m[20221213 22:26:47 @agent_ppo2.py:185][0m |           0.0013 |           6.5351 |           4.8439 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0110 |           6.0072 |           4.8430 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0115 |           5.8321 |           4.8444 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0049 |           5.7511 |           4.8466 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0048 |           5.6805 |           4.8488 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0003 |           5.8100 |           4.8500 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0089 |           5.6550 |           4.8522 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0008 |           6.1439 |           4.8538 |
[32m[20221213 22:26:48 @agent_ppo2.py:185][0m |          -0.0119 |           5.6622 |           4.8540 |
[32m[20221213 22:26:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:26:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.62
[32m[20221213 22:26:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.57
[32m[20221213 22:26:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 178.04
[32m[20221213 22:26:48 @agent_ppo2.py:143][0m Total time:       8.59 min
[32m[20221213 22:26:48 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221213 22:26:48 @agent_ppo2.py:121][0m #------------------------ Iteration 397 --------------------------#
[32m[20221213 22:26:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0085 |          12.7182 |           4.9558 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0009 |          10.8957 |           4.9510 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0083 |          10.4497 |           4.9516 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0074 |          10.2108 |           4.9510 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0028 |          10.1062 |           4.9511 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0090 |          10.0979 |           4.9520 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0048 |           9.9449 |           4.9501 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0033 |           9.7636 |           4.9512 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0080 |           9.8430 |           4.9523 |
[32m[20221213 22:26:49 @agent_ppo2.py:185][0m |          -0.0101 |           9.7188 |           4.9560 |
[32m[20221213 22:26:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:26:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.00
[32m[20221213 22:26:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 167.94
[32m[20221213 22:26:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.76
[32m[20221213 22:26:50 @agent_ppo2.py:143][0m Total time:       8.61 min
[32m[20221213 22:26:50 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221213 22:26:50 @agent_ppo2.py:121][0m #------------------------ Iteration 398 --------------------------#
[32m[20221213 22:26:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:50 @agent_ppo2.py:185][0m |          -0.0042 |          13.3289 |           5.0032 |
[32m[20221213 22:26:50 @agent_ppo2.py:185][0m |          -0.0003 |          12.7654 |           5.0031 |
[32m[20221213 22:26:50 @agent_ppo2.py:185][0m |          -0.0112 |          12.6364 |           5.0047 |
[32m[20221213 22:26:50 @agent_ppo2.py:185][0m |          -0.0117 |          12.7187 |           5.0055 |
[32m[20221213 22:26:50 @agent_ppo2.py:185][0m |          -0.0018 |          12.4762 |           5.0076 |
[32m[20221213 22:26:50 @agent_ppo2.py:185][0m |          -0.0050 |          12.3453 |           5.0085 |
[32m[20221213 22:26:50 @agent_ppo2.py:185][0m |          -0.0018 |          12.6429 |           5.0093 |
[32m[20221213 22:26:51 @agent_ppo2.py:185][0m |          -0.0075 |          12.2292 |           5.0111 |
[32m[20221213 22:26:51 @agent_ppo2.py:185][0m |          -0.0070 |          12.2234 |           5.0090 |
[32m[20221213 22:26:51 @agent_ppo2.py:185][0m |          -0.0096 |          12.2354 |           5.0081 |
[32m[20221213 22:26:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:26:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.28
[32m[20221213 22:26:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 178.47
[32m[20221213 22:26:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.67
[32m[20221213 22:26:51 @agent_ppo2.py:143][0m Total time:       8.63 min
[32m[20221213 22:26:51 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221213 22:26:51 @agent_ppo2.py:121][0m #------------------------ Iteration 399 --------------------------#
[32m[20221213 22:26:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:51 @agent_ppo2.py:185][0m |          -0.0023 |           7.4676 |           5.0390 |
[32m[20221213 22:26:51 @agent_ppo2.py:185][0m |          -0.0011 |           7.0813 |           5.0326 |
[32m[20221213 22:26:51 @agent_ppo2.py:185][0m |          -0.0040 |           6.9534 |           5.0305 |
[32m[20221213 22:26:51 @agent_ppo2.py:185][0m |          -0.0042 |           6.8805 |           5.0319 |
[32m[20221213 22:26:52 @agent_ppo2.py:185][0m |          -0.0006 |           6.9489 |           5.0310 |
[32m[20221213 22:26:52 @agent_ppo2.py:185][0m |          -0.0111 |           6.7551 |           5.0271 |
[32m[20221213 22:26:52 @agent_ppo2.py:185][0m |          -0.0052 |           6.7088 |           5.0258 |
[32m[20221213 22:26:52 @agent_ppo2.py:185][0m |          -0.0052 |           6.6741 |           5.0280 |
[32m[20221213 22:26:52 @agent_ppo2.py:185][0m |           0.0009 |           6.8559 |           5.0279 |
[32m[20221213 22:26:52 @agent_ppo2.py:185][0m |          -0.0054 |           6.6297 |           5.0273 |
[32m[20221213 22:26:52 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 22:26:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 39.90
[32m[20221213 22:26:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.57
[32m[20221213 22:26:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.29
[32m[20221213 22:26:52 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221213 22:26:52 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221213 22:26:52 @agent_ppo2.py:121][0m #------------------------ Iteration 400 --------------------------#
[32m[20221213 22:26:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:26:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0002 |           8.9659 |           5.0013 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0041 |           8.2140 |           4.9967 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0001 |           8.1721 |           4.9960 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0080 |           7.7083 |           4.9928 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0087 |           7.6097 |           4.9960 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0128 |           7.4581 |           4.9975 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0091 |           7.3291 |           4.9931 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0114 |           7.3023 |           4.9951 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0076 |           7.2215 |           4.9965 |
[32m[20221213 22:26:53 @agent_ppo2.py:185][0m |          -0.0098 |           7.0522 |           4.9963 |
[32m[20221213 22:26:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:26:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.12
[32m[20221213 22:26:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 182.56
[32m[20221213 22:26:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 94.48
[32m[20221213 22:26:53 @agent_ppo2.py:143][0m Total time:       8.68 min
[32m[20221213 22:26:53 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221213 22:26:53 @agent_ppo2.py:121][0m #------------------------ Iteration 401 --------------------------#
[32m[20221213 22:26:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |           0.0034 |           6.9760 |           5.0934 |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |          -0.0057 |           6.3683 |           5.0882 |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |          -0.0107 |           6.2271 |           5.0861 |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |          -0.0121 |           6.3559 |           5.0886 |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |          -0.0133 |           6.0788 |           5.0852 |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |          -0.0104 |           6.0014 |           5.0873 |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |          -0.0085 |           5.9661 |           5.0894 |
[32m[20221213 22:26:54 @agent_ppo2.py:185][0m |          -0.0114 |           5.9454 |           5.0910 |
[32m[20221213 22:26:55 @agent_ppo2.py:185][0m |          -0.0058 |           5.9162 |           5.0901 |
[32m[20221213 22:26:55 @agent_ppo2.py:185][0m |          -0.0097 |           5.8705 |           5.0949 |
[32m[20221213 22:26:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:26:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.30
[32m[20221213 22:26:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 159.95
[32m[20221213 22:26:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:26:55 @agent_ppo2.py:143][0m Total time:       8.70 min
[32m[20221213 22:26:55 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221213 22:26:55 @agent_ppo2.py:121][0m #------------------------ Iteration 402 --------------------------#
[32m[20221213 22:26:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:26:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:55 @agent_ppo2.py:185][0m |           0.0006 |           6.6045 |           5.1485 |
[32m[20221213 22:26:55 @agent_ppo2.py:185][0m |          -0.0034 |           3.7735 |           5.1389 |
[32m[20221213 22:26:55 @agent_ppo2.py:185][0m |          -0.0076 |           3.6754 |           5.1414 |
[32m[20221213 22:26:55 @agent_ppo2.py:185][0m |          -0.0006 |           3.5886 |           5.1424 |
[32m[20221213 22:26:55 @agent_ppo2.py:185][0m |          -0.0011 |           3.5198 |           5.1418 |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |          -0.0046 |           3.4807 |           5.1432 |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |          -0.0057 |           3.4576 |           5.1423 |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |           0.0020 |           3.4016 |           5.1420 |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |          -0.0095 |           3.4001 |           5.1412 |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |          -0.0051 |           3.3531 |           5.1425 |
[32m[20221213 22:26:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:26:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.90
[32m[20221213 22:26:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.57
[32m[20221213 22:26:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:26:56 @agent_ppo2.py:143][0m Total time:       8.72 min
[32m[20221213 22:26:56 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221213 22:26:56 @agent_ppo2.py:121][0m #------------------------ Iteration 403 --------------------------#
[32m[20221213 22:26:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |          -0.0066 |          14.8044 |           5.1504 |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |          -0.0004 |          13.2367 |           5.1442 |
[32m[20221213 22:26:56 @agent_ppo2.py:185][0m |          -0.0042 |          12.8580 |           5.1450 |
[32m[20221213 22:26:57 @agent_ppo2.py:185][0m |          -0.0053 |          12.5203 |           5.1376 |
[32m[20221213 22:26:57 @agent_ppo2.py:185][0m |          -0.0043 |          12.3118 |           5.1405 |
[32m[20221213 22:26:57 @agent_ppo2.py:185][0m |          -0.0110 |          12.2017 |           5.1394 |
[32m[20221213 22:26:57 @agent_ppo2.py:185][0m |          -0.0108 |          12.0814 |           5.1402 |
[32m[20221213 22:26:57 @agent_ppo2.py:185][0m |          -0.0113 |          11.9676 |           5.1371 |
[32m[20221213 22:26:57 @agent_ppo2.py:185][0m |          -0.0067 |          11.8312 |           5.1420 |
[32m[20221213 22:26:57 @agent_ppo2.py:185][0m |          -0.0076 |          12.0284 |           5.1398 |
[32m[20221213 22:26:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:26:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.75
[32m[20221213 22:26:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.92
[32m[20221213 22:26:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 10.24
[32m[20221213 22:26:57 @agent_ppo2.py:143][0m Total time:       8.74 min
[32m[20221213 22:26:57 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221213 22:26:57 @agent_ppo2.py:121][0m #------------------------ Iteration 404 --------------------------#
[32m[20221213 22:26:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0030 |           8.4106 |           5.1450 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0043 |           7.2210 |           5.1434 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0056 |           6.8589 |           5.1434 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0102 |           6.6607 |           5.1431 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0065 |           6.4644 |           5.1420 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0040 |           6.4136 |           5.1441 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0193 |           6.2259 |           5.1415 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0039 |           6.4500 |           5.1425 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |          -0.0090 |           6.0189 |           5.1416 |
[32m[20221213 22:26:58 @agent_ppo2.py:185][0m |           0.0029 |           6.2933 |           5.1403 |
[32m[20221213 22:26:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:26:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.61
[32m[20221213 22:26:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 172.31
[32m[20221213 22:26:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.45
[32m[20221213 22:26:58 @agent_ppo2.py:143][0m Total time:       8.76 min
[32m[20221213 22:26:58 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221213 22:26:58 @agent_ppo2.py:121][0m #------------------------ Iteration 405 --------------------------#
[32m[20221213 22:26:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:26:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |           0.0027 |           7.8404 |           5.2131 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |          -0.0056 |           4.1401 |           5.2045 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |           0.0002 |           3.7411 |           5.2035 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |           0.0006 |           3.6594 |           5.2038 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |          -0.0030 |           3.5273 |           5.2034 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |          -0.0019 |           3.3574 |           5.2036 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |          -0.0026 |           3.3716 |           5.2019 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |          -0.0084 |           3.2767 |           5.2024 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |          -0.0082 |           3.1764 |           5.2026 |
[32m[20221213 22:26:59 @agent_ppo2.py:185][0m |          -0.0035 |           3.2600 |           5.1992 |
[32m[20221213 22:26:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:27:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.40
[32m[20221213 22:27:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.26
[32m[20221213 22:27:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.54
[32m[20221213 22:27:00 @agent_ppo2.py:143][0m Total time:       8.78 min
[32m[20221213 22:27:00 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221213 22:27:00 @agent_ppo2.py:121][0m #------------------------ Iteration 406 --------------------------#
[32m[20221213 22:27:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:00 @agent_ppo2.py:185][0m |          -0.0051 |          10.5407 |           5.2187 |
[32m[20221213 22:27:00 @agent_ppo2.py:185][0m |          -0.0057 |           8.3070 |           5.2140 |
[32m[20221213 22:27:00 @agent_ppo2.py:185][0m |          -0.0089 |           8.0181 |           5.2082 |
[32m[20221213 22:27:00 @agent_ppo2.py:185][0m |          -0.0058 |           7.9639 |           5.2115 |
[32m[20221213 22:27:00 @agent_ppo2.py:185][0m |          -0.0083 |           7.7937 |           5.2141 |
[32m[20221213 22:27:00 @agent_ppo2.py:185][0m |          -0.0066 |           7.9534 |           5.2138 |
[32m[20221213 22:27:00 @agent_ppo2.py:185][0m |          -0.0101 |           7.5548 |           5.2117 |
[32m[20221213 22:27:01 @agent_ppo2.py:185][0m |          -0.0033 |           7.6205 |           5.2138 |
[32m[20221213 22:27:01 @agent_ppo2.py:185][0m |          -0.0123 |           7.3918 |           5.2147 |
[32m[20221213 22:27:01 @agent_ppo2.py:185][0m |          -0.0078 |           7.4245 |           5.2179 |
[32m[20221213 22:27:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:27:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.20
[32m[20221213 22:27:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 189.04
[32m[20221213 22:27:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.65
[32m[20221213 22:27:01 @agent_ppo2.py:143][0m Total time:       8.80 min
[32m[20221213 22:27:01 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221213 22:27:01 @agent_ppo2.py:121][0m #------------------------ Iteration 407 --------------------------#
[32m[20221213 22:27:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:01 @agent_ppo2.py:185][0m |          -0.0027 |           8.6791 |           5.1841 |
[32m[20221213 22:27:01 @agent_ppo2.py:185][0m |          -0.0009 |           6.4791 |           5.1744 |
[32m[20221213 22:27:01 @agent_ppo2.py:185][0m |          -0.0034 |           6.2337 |           5.1727 |
[32m[20221213 22:27:01 @agent_ppo2.py:185][0m |          -0.0092 |           6.1239 |           5.1738 |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |          -0.0052 |           6.5352 |           5.1718 |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |          -0.0066 |           6.0429 |           5.1623 |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |          -0.0083 |           5.9937 |           5.1667 |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |          -0.0074 |           5.9867 |           5.1654 |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |          -0.0060 |           5.9619 |           5.1644 |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |          -0.0092 |           5.9810 |           5.1619 |
[32m[20221213 22:27:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.99
[32m[20221213 22:27:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.24
[32m[20221213 22:27:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.00
[32m[20221213 22:27:02 @agent_ppo2.py:143][0m Total time:       8.82 min
[32m[20221213 22:27:02 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221213 22:27:02 @agent_ppo2.py:121][0m #------------------------ Iteration 408 --------------------------#
[32m[20221213 22:27:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |           0.0030 |          15.4850 |           5.0425 |
[32m[20221213 22:27:02 @agent_ppo2.py:185][0m |          -0.0037 |          13.4994 |           5.0354 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0076 |          12.9274 |           5.0331 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0040 |          12.6641 |           5.0308 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0085 |          12.6544 |           5.0307 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0014 |          13.2579 |           5.0287 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0092 |          12.2676 |           5.0291 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0047 |          12.2484 |           5.0283 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0083 |          12.2932 |           5.0306 |
[32m[20221213 22:27:03 @agent_ppo2.py:185][0m |          -0.0085 |          12.2312 |           5.0292 |
[32m[20221213 22:27:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.69
[32m[20221213 22:27:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 186.79
[32m[20221213 22:27:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 191.91
[32m[20221213 22:27:03 @agent_ppo2.py:143][0m Total time:       8.84 min
[32m[20221213 22:27:03 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221213 22:27:03 @agent_ppo2.py:121][0m #------------------------ Iteration 409 --------------------------#
[32m[20221213 22:27:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |           0.0043 |           7.3850 |           5.1392 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0052 |           5.3784 |           5.1333 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |           0.0035 |           5.2867 |           5.1303 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0098 |           5.1641 |           5.1343 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0067 |           5.1569 |           5.1336 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0007 |           5.5563 |           5.1339 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0100 |           5.1260 |           5.1326 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0109 |           4.9780 |           5.1347 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0030 |           4.9999 |           5.1366 |
[32m[20221213 22:27:04 @agent_ppo2.py:185][0m |          -0.0096 |           4.9682 |           5.1394 |
[32m[20221213 22:27:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:27:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.32
[32m[20221213 22:27:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.30
[32m[20221213 22:27:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 191.14
[32m[20221213 22:27:04 @agent_ppo2.py:143][0m Total time:       8.86 min
[32m[20221213 22:27:04 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221213 22:27:04 @agent_ppo2.py:121][0m #------------------------ Iteration 410 --------------------------#
[32m[20221213 22:27:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:27:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |          -0.0048 |           9.1574 |           5.1547 |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |           0.0046 |           6.7034 |           5.1503 |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |          -0.0061 |           6.2679 |           5.1486 |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |          -0.0082 |           6.2343 |           5.1473 |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |           0.0003 |           5.9181 |           5.1503 |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |          -0.0113 |           5.7538 |           5.1457 |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |          -0.0082 |           5.7689 |           5.1478 |
[32m[20221213 22:27:05 @agent_ppo2.py:185][0m |          -0.0060 |           5.6454 |           5.1474 |
[32m[20221213 22:27:06 @agent_ppo2.py:185][0m |          -0.0146 |           5.4998 |           5.1452 |
[32m[20221213 22:27:06 @agent_ppo2.py:185][0m |          -0.0138 |           5.4325 |           5.1441 |
[32m[20221213 22:27:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:27:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 8.28
[32m[20221213 22:27:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 41.42
[32m[20221213 22:27:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 220.63
[32m[20221213 22:27:06 @agent_ppo2.py:143][0m Total time:       8.88 min
[32m[20221213 22:27:06 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221213 22:27:06 @agent_ppo2.py:121][0m #------------------------ Iteration 411 --------------------------#
[32m[20221213 22:27:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:06 @agent_ppo2.py:185][0m |          -0.0013 |           7.8720 |           5.0825 |
[32m[20221213 22:27:06 @agent_ppo2.py:185][0m |          -0.0030 |           6.8595 |           5.0796 |
[32m[20221213 22:27:06 @agent_ppo2.py:185][0m |          -0.0048 |           6.7965 |           5.0775 |
[32m[20221213 22:27:06 @agent_ppo2.py:185][0m |          -0.0083 |           6.6658 |           5.0785 |
[32m[20221213 22:27:06 @agent_ppo2.py:185][0m |          -0.0036 |           6.6531 |           5.0788 |
[32m[20221213 22:27:07 @agent_ppo2.py:185][0m |          -0.0071 |           6.6012 |           5.0792 |
[32m[20221213 22:27:07 @agent_ppo2.py:185][0m |          -0.0079 |           6.5910 |           5.0818 |
[32m[20221213 22:27:07 @agent_ppo2.py:185][0m |          -0.0097 |           6.5446 |           5.0809 |
[32m[20221213 22:27:07 @agent_ppo2.py:185][0m |          -0.0064 |           6.5185 |           5.0809 |
[32m[20221213 22:27:07 @agent_ppo2.py:185][0m |          -0.0086 |           6.5465 |           5.0856 |
[32m[20221213 22:27:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:27:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.75
[32m[20221213 22:27:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.43
[32m[20221213 22:27:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.39
[32m[20221213 22:27:07 @agent_ppo2.py:143][0m Total time:       8.90 min
[32m[20221213 22:27:07 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221213 22:27:07 @agent_ppo2.py:121][0m #------------------------ Iteration 412 --------------------------#
[32m[20221213 22:27:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:07 @agent_ppo2.py:185][0m |           0.0024 |          17.5988 |           5.2796 |
[32m[20221213 22:27:07 @agent_ppo2.py:185][0m |          -0.0041 |          15.8769 |           5.2738 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0032 |          15.5463 |           5.2679 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0131 |          15.3666 |           5.2644 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0031 |          15.3625 |           5.2612 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0082 |          15.4014 |           5.2575 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0075 |          15.0644 |           5.2572 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0090 |          15.0384 |           5.2533 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0076 |          14.8566 |           5.2517 |
[32m[20221213 22:27:08 @agent_ppo2.py:185][0m |          -0.0034 |          14.7964 |           5.2532 |
[32m[20221213 22:27:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:27:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.87
[32m[20221213 22:27:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.62
[32m[20221213 22:27:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.99
[32m[20221213 22:27:08 @agent_ppo2.py:143][0m Total time:       8.92 min
[32m[20221213 22:27:08 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221213 22:27:08 @agent_ppo2.py:121][0m #------------------------ Iteration 413 --------------------------#
[32m[20221213 22:27:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |           0.0164 |          15.9026 |           5.1919 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |           0.0017 |          12.9444 |           5.1886 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |          -0.0061 |          12.5116 |           5.1859 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |          -0.0061 |          12.4269 |           5.1880 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |           0.0087 |          14.5467 |           5.1827 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |          -0.0086 |          12.1484 |           5.1744 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |          -0.0105 |          11.9185 |           5.1782 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |          -0.0119 |          11.8059 |           5.1830 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |          -0.0088 |          11.7669 |           5.1798 |
[32m[20221213 22:27:09 @agent_ppo2.py:185][0m |          -0.0086 |          11.6210 |           5.1798 |
[32m[20221213 22:27:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:27:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.82
[32m[20221213 22:27:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 115.45
[32m[20221213 22:27:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.71
[32m[20221213 22:27:10 @agent_ppo2.py:143][0m Total time:       8.95 min
[32m[20221213 22:27:10 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221213 22:27:10 @agent_ppo2.py:121][0m #------------------------ Iteration 414 --------------------------#
[32m[20221213 22:27:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:10 @agent_ppo2.py:185][0m |          -0.0021 |           6.5807 |           5.2633 |
[32m[20221213 22:27:10 @agent_ppo2.py:185][0m |          -0.0054 |           4.4200 |           5.2617 |
[32m[20221213 22:27:10 @agent_ppo2.py:185][0m |           0.0005 |           4.3108 |           5.2616 |
[32m[20221213 22:27:10 @agent_ppo2.py:185][0m |          -0.0059 |           4.1630 |           5.2565 |
[32m[20221213 22:27:10 @agent_ppo2.py:185][0m |          -0.0124 |           4.0970 |           5.2569 |
[32m[20221213 22:27:10 @agent_ppo2.py:185][0m |          -0.0036 |           4.0696 |           5.2557 |
[32m[20221213 22:27:10 @agent_ppo2.py:185][0m |          -0.0077 |           4.0093 |           5.2541 |
[32m[20221213 22:27:11 @agent_ppo2.py:185][0m |          -0.0126 |           3.9664 |           5.2501 |
[32m[20221213 22:27:11 @agent_ppo2.py:185][0m |          -0.0199 |           3.9445 |           5.2514 |
[32m[20221213 22:27:11 @agent_ppo2.py:185][0m |          -0.0125 |           3.9354 |           5.2512 |
[32m[20221213 22:27:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:27:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.10
[32m[20221213 22:27:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.32
[32m[20221213 22:27:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.40
[32m[20221213 22:27:11 @agent_ppo2.py:143][0m Total time:       8.97 min
[32m[20221213 22:27:11 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221213 22:27:11 @agent_ppo2.py:121][0m #------------------------ Iteration 415 --------------------------#
[32m[20221213 22:27:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:11 @agent_ppo2.py:185][0m |           0.0044 |          10.8712 |           5.1853 |
[32m[20221213 22:27:11 @agent_ppo2.py:185][0m |          -0.0103 |           9.5777 |           5.1762 |
[32m[20221213 22:27:11 @agent_ppo2.py:185][0m |          -0.0108 |           9.0628 |           5.1691 |
[32m[20221213 22:27:11 @agent_ppo2.py:185][0m |           0.0028 |           9.5277 |           5.1674 |
[32m[20221213 22:27:12 @agent_ppo2.py:185][0m |          -0.0126 |           8.8879 |           5.1622 |
[32m[20221213 22:27:12 @agent_ppo2.py:185][0m |          -0.0118 |           8.6216 |           5.1618 |
[32m[20221213 22:27:12 @agent_ppo2.py:185][0m |          -0.0052 |           8.4882 |           5.1600 |
[32m[20221213 22:27:12 @agent_ppo2.py:185][0m |          -0.0124 |           8.3636 |           5.1602 |
[32m[20221213 22:27:12 @agent_ppo2.py:185][0m |          -0.0132 |           8.3927 |           5.1565 |
[32m[20221213 22:27:12 @agent_ppo2.py:185][0m |          -0.0112 |           8.2928 |           5.1595 |
[32m[20221213 22:27:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:27:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.35
[32m[20221213 22:27:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 161.89
[32m[20221213 22:27:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.03
[32m[20221213 22:27:12 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 304.03
[32m[20221213 22:27:12 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 304.03
[32m[20221213 22:27:12 @agent_ppo2.py:143][0m Total time:       8.99 min
[32m[20221213 22:27:12 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221213 22:27:12 @agent_ppo2.py:121][0m #------------------------ Iteration 416 --------------------------#
[32m[20221213 22:27:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:27:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |           0.0005 |          17.4379 |           5.1922 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0042 |          16.6749 |           5.1925 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0102 |          16.5458 |           5.1906 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |           0.0008 |          18.2507 |           5.1888 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0029 |          16.1651 |           5.1904 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0058 |          15.9762 |           5.1909 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0040 |          15.8143 |           5.1922 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0097 |          15.8909 |           5.1907 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0108 |          15.7443 |           5.1919 |
[32m[20221213 22:27:13 @agent_ppo2.py:185][0m |          -0.0159 |          15.6554 |           5.1921 |
[32m[20221213 22:27:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:27:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 111.45
[32m[20221213 22:27:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 157.96
[32m[20221213 22:27:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 184.76
[32m[20221213 22:27:14 @agent_ppo2.py:143][0m Total time:       9.01 min
[32m[20221213 22:27:14 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221213 22:27:14 @agent_ppo2.py:121][0m #------------------------ Iteration 417 --------------------------#
[32m[20221213 22:27:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |           0.0038 |          17.3091 |           5.2000 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |           0.0012 |          16.6142 |           5.1964 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |          -0.0101 |          16.2103 |           5.1977 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |          -0.0027 |          15.9683 |           5.1947 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |          -0.0043 |          15.6928 |           5.1945 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |          -0.0042 |          15.7668 |           5.1915 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |          -0.0066 |          15.5734 |           5.1926 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |          -0.0077 |          15.5173 |           5.1897 |
[32m[20221213 22:27:14 @agent_ppo2.py:185][0m |          -0.0093 |          15.5444 |           5.1900 |
[32m[20221213 22:27:15 @agent_ppo2.py:185][0m |          -0.0027 |          15.4610 |           5.1896 |
[32m[20221213 22:27:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:27:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 94.60
[32m[20221213 22:27:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.62
[32m[20221213 22:27:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.41
[32m[20221213 22:27:15 @agent_ppo2.py:143][0m Total time:       9.03 min
[32m[20221213 22:27:15 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221213 22:27:15 @agent_ppo2.py:121][0m #------------------------ Iteration 418 --------------------------#
[32m[20221213 22:27:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:15 @agent_ppo2.py:185][0m |          -0.0005 |           7.4455 |           5.2552 |
[32m[20221213 22:27:15 @agent_ppo2.py:185][0m |          -0.0001 |           4.9088 |           5.2554 |
[32m[20221213 22:27:15 @agent_ppo2.py:185][0m |           0.0013 |           4.6881 |           5.2584 |
[32m[20221213 22:27:15 @agent_ppo2.py:185][0m |          -0.0051 |           4.5674 |           5.2609 |
[32m[20221213 22:27:15 @agent_ppo2.py:185][0m |          -0.0068 |           4.5361 |           5.2579 |
[32m[20221213 22:27:15 @agent_ppo2.py:185][0m |          -0.0086 |           4.4438 |           5.2640 |
[32m[20221213 22:27:16 @agent_ppo2.py:185][0m |          -0.0127 |           4.4440 |           5.2650 |
[32m[20221213 22:27:16 @agent_ppo2.py:185][0m |          -0.0079 |           4.3928 |           5.2658 |
[32m[20221213 22:27:16 @agent_ppo2.py:185][0m |          -0.0136 |           4.4410 |           5.2680 |
[32m[20221213 22:27:16 @agent_ppo2.py:185][0m |          -0.0080 |           4.5072 |           5.2686 |
[32m[20221213 22:27:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:27:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.92
[32m[20221213 22:27:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.40
[32m[20221213 22:27:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 205.99
[32m[20221213 22:27:16 @agent_ppo2.py:143][0m Total time:       9.05 min
[32m[20221213 22:27:16 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221213 22:27:16 @agent_ppo2.py:121][0m #------------------------ Iteration 419 --------------------------#
[32m[20221213 22:27:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:16 @agent_ppo2.py:185][0m |          -0.0036 |          10.6742 |           5.2850 |
[32m[20221213 22:27:16 @agent_ppo2.py:185][0m |          -0.0095 |           8.8799 |           5.2846 |
[32m[20221213 22:27:16 @agent_ppo2.py:185][0m |           0.0062 |           9.0285 |           5.2843 |
[32m[20221213 22:27:17 @agent_ppo2.py:185][0m |          -0.0074 |           8.1242 |           5.2841 |
[32m[20221213 22:27:17 @agent_ppo2.py:185][0m |          -0.0075 |           8.1156 |           5.2853 |
[32m[20221213 22:27:17 @agent_ppo2.py:185][0m |          -0.0017 |           8.1275 |           5.2860 |
[32m[20221213 22:27:17 @agent_ppo2.py:185][0m |          -0.0068 |           7.9664 |           5.2874 |
[32m[20221213 22:27:17 @agent_ppo2.py:185][0m |          -0.0080 |           7.8510 |           5.2899 |
[32m[20221213 22:27:17 @agent_ppo2.py:185][0m |          -0.0054 |           7.7828 |           5.2877 |
[32m[20221213 22:27:17 @agent_ppo2.py:185][0m |          -0.0029 |           7.7236 |           5.2914 |
[32m[20221213 22:27:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:27:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.61
[32m[20221213 22:27:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.59
[32m[20221213 22:27:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.21
[32m[20221213 22:27:17 @agent_ppo2.py:143][0m Total time:       9.07 min
[32m[20221213 22:27:17 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221213 22:27:17 @agent_ppo2.py:121][0m #------------------------ Iteration 420 --------------------------#
[32m[20221213 22:27:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:27:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |           0.0017 |          10.7703 |           5.4226 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0090 |           8.3042 |           5.4132 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0052 |           7.7668 |           5.4112 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0069 |           7.4307 |           5.4075 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0099 |           7.1907 |           5.4102 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0126 |           7.0618 |           5.4087 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0108 |           6.9515 |           5.4088 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0126 |           6.7546 |           5.4106 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0109 |           6.9249 |           5.4089 |
[32m[20221213 22:27:18 @agent_ppo2.py:185][0m |          -0.0110 |           6.6076 |           5.4094 |
[32m[20221213 22:27:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:27:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.16
[32m[20221213 22:27:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 164.97
[32m[20221213 22:27:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.53
[32m[20221213 22:27:18 @agent_ppo2.py:143][0m Total time:       9.09 min
[32m[20221213 22:27:18 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221213 22:27:18 @agent_ppo2.py:121][0m #------------------------ Iteration 421 --------------------------#
[32m[20221213 22:27:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0011 |           8.4530 |           5.3310 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0044 |           7.1270 |           5.3288 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0043 |           6.9710 |           5.3277 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0099 |           6.8602 |           5.3310 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0034 |           6.8299 |           5.3296 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0059 |           6.7820 |           5.3309 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |           0.0007 |           7.9327 |           5.3285 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0117 |           6.7527 |           5.3345 |
[32m[20221213 22:27:19 @agent_ppo2.py:185][0m |          -0.0073 |           6.7171 |           5.3352 |
[32m[20221213 22:27:20 @agent_ppo2.py:185][0m |          -0.0058 |           6.8101 |           5.3326 |
[32m[20221213 22:27:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:27:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.26
[32m[20221213 22:27:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.19
[32m[20221213 22:27:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.45
[32m[20221213 22:27:20 @agent_ppo2.py:143][0m Total time:       9.11 min
[32m[20221213 22:27:20 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221213 22:27:20 @agent_ppo2.py:121][0m #------------------------ Iteration 422 --------------------------#
[32m[20221213 22:27:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:20 @agent_ppo2.py:185][0m |           0.0053 |          10.3724 |           5.4027 |
[32m[20221213 22:27:20 @agent_ppo2.py:185][0m |           0.0073 |           8.5726 |           5.4008 |
[32m[20221213 22:27:20 @agent_ppo2.py:185][0m |          -0.0032 |           7.8306 |           5.3939 |
[32m[20221213 22:27:20 @agent_ppo2.py:185][0m |          -0.0065 |           7.6577 |           5.3911 |
[32m[20221213 22:27:20 @agent_ppo2.py:185][0m |           0.0015 |           7.3979 |           5.3898 |
[32m[20221213 22:27:20 @agent_ppo2.py:185][0m |          -0.0123 |           7.2431 |           5.3906 |
[32m[20221213 22:27:21 @agent_ppo2.py:185][0m |           0.0119 |           8.2294 |           5.3854 |
[32m[20221213 22:27:21 @agent_ppo2.py:185][0m |          -0.0034 |           7.9469 |           5.3876 |
[32m[20221213 22:27:21 @agent_ppo2.py:185][0m |          -0.0059 |           7.0296 |           5.3868 |
[32m[20221213 22:27:21 @agent_ppo2.py:185][0m |          -0.0037 |           6.9691 |           5.3865 |
[32m[20221213 22:27:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:27:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.52
[32m[20221213 22:27:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.05
[32m[20221213 22:27:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.23
[32m[20221213 22:27:21 @agent_ppo2.py:143][0m Total time:       9.13 min
[32m[20221213 22:27:21 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221213 22:27:21 @agent_ppo2.py:121][0m #------------------------ Iteration 423 --------------------------#
[32m[20221213 22:27:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:21 @agent_ppo2.py:185][0m |          -0.0019 |          10.6194 |           5.4019 |
[32m[20221213 22:27:21 @agent_ppo2.py:185][0m |          -0.0075 |           9.8862 |           5.3935 |
[32m[20221213 22:27:21 @agent_ppo2.py:185][0m |          -0.0052 |           9.8087 |           5.3899 |
[32m[20221213 22:27:22 @agent_ppo2.py:185][0m |          -0.0056 |           9.7320 |           5.3930 |
[32m[20221213 22:27:22 @agent_ppo2.py:185][0m |          -0.0064 |           9.7278 |           5.3946 |
[32m[20221213 22:27:22 @agent_ppo2.py:185][0m |          -0.0068 |           9.6621 |           5.3963 |
[32m[20221213 22:27:22 @agent_ppo2.py:185][0m |          -0.0075 |           9.6402 |           5.3956 |
[32m[20221213 22:27:22 @agent_ppo2.py:185][0m |          -0.0051 |           9.7813 |           5.3958 |
[32m[20221213 22:27:22 @agent_ppo2.py:185][0m |          -0.0065 |           9.6687 |           5.3978 |
[32m[20221213 22:27:22 @agent_ppo2.py:185][0m |          -0.0036 |           9.8625 |           5.4045 |
[32m[20221213 22:27:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:27:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.34
[32m[20221213 22:27:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.32
[32m[20221213 22:27:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.26
[32m[20221213 22:27:22 @agent_ppo2.py:143][0m Total time:       9.16 min
[32m[20221213 22:27:22 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221213 22:27:22 @agent_ppo2.py:121][0m #------------------------ Iteration 424 --------------------------#
[32m[20221213 22:27:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |           0.0000 |          10.1857 |           5.4804 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |           0.0012 |           9.3834 |           5.4752 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |           0.0016 |           9.3139 |           5.4749 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |          -0.0046 |           9.1268 |           5.4731 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |          -0.0092 |           8.9782 |           5.4744 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |           0.0033 |           9.4310 |           5.4712 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |          -0.0093 |           8.8494 |           5.4727 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |          -0.0016 |           8.8889 |           5.4711 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |          -0.0049 |           8.8306 |           5.4718 |
[32m[20221213 22:27:23 @agent_ppo2.py:185][0m |           0.0056 |           9.9414 |           5.4708 |
[32m[20221213 22:27:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:27:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.17
[32m[20221213 22:27:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.14
[32m[20221213 22:27:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 209.32
[32m[20221213 22:27:24 @agent_ppo2.py:143][0m Total time:       9.18 min
[32m[20221213 22:27:24 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221213 22:27:24 @agent_ppo2.py:121][0m #------------------------ Iteration 425 --------------------------#
[32m[20221213 22:27:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |           0.0019 |          23.8957 |           5.5898 |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |          -0.0009 |          18.6796 |           5.5877 |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |           0.0021 |          17.9247 |           5.5895 |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |          -0.0078 |          17.5829 |           5.5908 |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |          -0.0088 |          17.3075 |           5.5906 |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |          -0.0089 |          17.2808 |           5.5880 |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |          -0.0114 |          17.2023 |           5.5902 |
[32m[20221213 22:27:24 @agent_ppo2.py:185][0m |          -0.0098 |          17.2280 |           5.5900 |
[32m[20221213 22:27:25 @agent_ppo2.py:185][0m |          -0.0132 |          16.9553 |           5.5933 |
[32m[20221213 22:27:25 @agent_ppo2.py:185][0m |          -0.0124 |          16.8394 |           5.5946 |
[32m[20221213 22:27:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:27:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.48
[32m[20221213 22:27:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.51
[32m[20221213 22:27:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 113.30
[32m[20221213 22:27:25 @agent_ppo2.py:143][0m Total time:       9.20 min
[32m[20221213 22:27:25 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221213 22:27:25 @agent_ppo2.py:121][0m #------------------------ Iteration 426 --------------------------#
[32m[20221213 22:27:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:25 @agent_ppo2.py:185][0m |           0.0017 |          11.2757 |           5.5905 |
[32m[20221213 22:27:25 @agent_ppo2.py:185][0m |          -0.0032 |          10.2918 |           5.5877 |
[32m[20221213 22:27:25 @agent_ppo2.py:185][0m |           0.0060 |          10.7549 |           5.5840 |
[32m[20221213 22:27:25 @agent_ppo2.py:185][0m |          -0.0051 |          10.1653 |           5.5820 |
[32m[20221213 22:27:25 @agent_ppo2.py:185][0m |          -0.0057 |          10.1293 |           5.5814 |
[32m[20221213 22:27:26 @agent_ppo2.py:185][0m |          -0.0078 |          10.1498 |           5.5795 |
[32m[20221213 22:27:26 @agent_ppo2.py:185][0m |           0.0058 |          10.9690 |           5.5797 |
[32m[20221213 22:27:26 @agent_ppo2.py:185][0m |          -0.0059 |          10.0545 |           5.5769 |
[32m[20221213 22:27:26 @agent_ppo2.py:185][0m |          -0.0053 |          10.0416 |           5.5770 |
[32m[20221213 22:27:26 @agent_ppo2.py:185][0m |          -0.0044 |          10.0053 |           5.5762 |
[32m[20221213 22:27:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:27:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.96
[32m[20221213 22:27:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.78
[32m[20221213 22:27:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.04
[32m[20221213 22:27:26 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 304.04
[32m[20221213 22:27:26 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 304.04
[32m[20221213 22:27:26 @agent_ppo2.py:143][0m Total time:       9.22 min
[32m[20221213 22:27:26 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221213 22:27:26 @agent_ppo2.py:121][0m #------------------------ Iteration 427 --------------------------#
[32m[20221213 22:27:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:26 @agent_ppo2.py:185][0m |          -0.0044 |          11.6007 |           5.5238 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0033 |          10.1867 |           5.5140 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0023 |          10.0706 |           5.5155 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0093 |           9.9037 |           5.5126 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0060 |           9.7956 |           5.5124 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0061 |           9.7086 |           5.5146 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0072 |           9.5653 |           5.5113 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0122 |           9.7302 |           5.5123 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0034 |           9.5496 |           5.5130 |
[32m[20221213 22:27:27 @agent_ppo2.py:185][0m |          -0.0102 |           9.3818 |           5.5103 |
[32m[20221213 22:27:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:27:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.17
[32m[20221213 22:27:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.87
[32m[20221213 22:27:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.25
[32m[20221213 22:27:27 @agent_ppo2.py:143][0m Total time:       9.24 min
[32m[20221213 22:27:27 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221213 22:27:27 @agent_ppo2.py:121][0m #------------------------ Iteration 428 --------------------------#
[32m[20221213 22:27:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |           0.0017 |          18.3991 |           5.5240 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0052 |          17.3329 |           5.5209 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0045 |          16.9366 |           5.5166 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0080 |          16.5441 |           5.5151 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0097 |          16.4634 |           5.5115 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0080 |          16.1821 |           5.5099 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0037 |          15.9506 |           5.5073 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0078 |          16.0384 |           5.5090 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0056 |          15.8258 |           5.5103 |
[32m[20221213 22:27:28 @agent_ppo2.py:185][0m |          -0.0076 |          15.6967 |           5.5098 |
[32m[20221213 22:27:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:27:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 107.67
[32m[20221213 22:27:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.66
[32m[20221213 22:27:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.38
[32m[20221213 22:27:29 @agent_ppo2.py:143][0m Total time:       9.26 min
[32m[20221213 22:27:29 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221213 22:27:29 @agent_ppo2.py:121][0m #------------------------ Iteration 429 --------------------------#
[32m[20221213 22:27:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:29 @agent_ppo2.py:185][0m |          -0.0047 |          12.4253 |           5.6148 |
[32m[20221213 22:27:29 @agent_ppo2.py:185][0m |          -0.0016 |          10.7672 |           5.6096 |
[32m[20221213 22:27:29 @agent_ppo2.py:185][0m |          -0.0098 |          10.6657 |           5.6140 |
[32m[20221213 22:27:29 @agent_ppo2.py:185][0m |          -0.0066 |          10.6506 |           5.6107 |
[32m[20221213 22:27:29 @agent_ppo2.py:185][0m |          -0.0075 |          10.5397 |           5.6115 |
[32m[20221213 22:27:29 @agent_ppo2.py:185][0m |          -0.0060 |          10.5546 |           5.6159 |
[32m[20221213 22:27:29 @agent_ppo2.py:185][0m |          -0.0063 |          10.5594 |           5.6191 |
[32m[20221213 22:27:30 @agent_ppo2.py:185][0m |          -0.0062 |          10.5232 |           5.6188 |
[32m[20221213 22:27:30 @agent_ppo2.py:185][0m |          -0.0016 |          10.9171 |           5.6190 |
[32m[20221213 22:27:30 @agent_ppo2.py:185][0m |          -0.0048 |          10.4407 |           5.6215 |
[32m[20221213 22:27:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:27:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.98
[32m[20221213 22:27:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.92
[32m[20221213 22:27:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 208.33
[32m[20221213 22:27:30 @agent_ppo2.py:143][0m Total time:       9.28 min
[32m[20221213 22:27:30 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221213 22:27:30 @agent_ppo2.py:121][0m #------------------------ Iteration 430 --------------------------#
[32m[20221213 22:27:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:27:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:30 @agent_ppo2.py:185][0m |          -0.0077 |          20.1333 |           5.6203 |
[32m[20221213 22:27:30 @agent_ppo2.py:185][0m |           0.0030 |          19.5215 |           5.6103 |
[32m[20221213 22:27:30 @agent_ppo2.py:185][0m |           0.0002 |          19.9851 |           5.6011 |
[32m[20221213 22:27:30 @agent_ppo2.py:185][0m |          -0.0039 |          18.9469 |           5.6019 |
[32m[20221213 22:27:31 @agent_ppo2.py:185][0m |          -0.0004 |          18.5891 |           5.5988 |
[32m[20221213 22:27:31 @agent_ppo2.py:185][0m |          -0.0075 |          18.2802 |           5.5990 |
[32m[20221213 22:27:31 @agent_ppo2.py:185][0m |          -0.0100 |          18.2260 |           5.6010 |
[32m[20221213 22:27:31 @agent_ppo2.py:185][0m |          -0.0057 |          18.2349 |           5.5986 |
[32m[20221213 22:27:31 @agent_ppo2.py:185][0m |          -0.0112 |          18.1212 |           5.5989 |
[32m[20221213 22:27:31 @agent_ppo2.py:185][0m |          -0.0095 |          18.1077 |           5.5985 |
[32m[20221213 22:27:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:27:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 170.81
[32m[20221213 22:27:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.96
[32m[20221213 22:27:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:27:31 @agent_ppo2.py:143][0m Total time:       9.30 min
[32m[20221213 22:27:31 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221213 22:27:31 @agent_ppo2.py:121][0m #------------------------ Iteration 431 --------------------------#
[32m[20221213 22:27:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:31 @agent_ppo2.py:185][0m |          -0.0062 |          18.6429 |           5.6752 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0016 |          18.2877 |           5.6633 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0065 |          17.8487 |           5.6579 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0047 |          17.5799 |           5.6564 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |           0.0032 |          18.4386 |           5.6546 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0070 |          17.4513 |           5.6532 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0103 |          17.2283 |           5.6497 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0088 |          17.1110 |           5.6468 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0103 |          17.0994 |           5.6452 |
[32m[20221213 22:27:32 @agent_ppo2.py:185][0m |          -0.0117 |          17.0047 |           5.6429 |
[32m[20221213 22:27:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:27:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.35
[32m[20221213 22:27:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 233.56
[32m[20221213 22:27:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 196.09
[32m[20221213 22:27:32 @agent_ppo2.py:143][0m Total time:       9.32 min
[32m[20221213 22:27:32 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221213 22:27:32 @agent_ppo2.py:121][0m #------------------------ Iteration 432 --------------------------#
[32m[20221213 22:27:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0007 |          16.9424 |           5.6570 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0037 |          15.6810 |           5.6491 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0051 |          15.3366 |           5.6434 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0034 |          15.2854 |           5.6396 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0096 |          14.9715 |           5.6401 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0093 |          14.7761 |           5.6427 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0040 |          14.8091 |           5.6418 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0115 |          14.5662 |           5.6431 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0008 |          15.4594 |           5.6423 |
[32m[20221213 22:27:33 @agent_ppo2.py:185][0m |          -0.0100 |          14.4254 |           5.6410 |
[32m[20221213 22:27:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:27:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 76.45
[32m[20221213 22:27:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 193.81
[32m[20221213 22:27:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.96
[32m[20221213 22:27:34 @agent_ppo2.py:143][0m Total time:       9.34 min
[32m[20221213 22:27:34 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221213 22:27:34 @agent_ppo2.py:121][0m #------------------------ Iteration 433 --------------------------#
[32m[20221213 22:27:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:34 @agent_ppo2.py:185][0m |           0.0094 |          17.2799 |           5.5932 |
[32m[20221213 22:27:34 @agent_ppo2.py:185][0m |          -0.0046 |          15.7578 |           5.5876 |
[32m[20221213 22:27:34 @agent_ppo2.py:185][0m |          -0.0104 |          15.3527 |           5.5829 |
[32m[20221213 22:27:34 @agent_ppo2.py:185][0m |          -0.0079 |          15.1662 |           5.5811 |
[32m[20221213 22:27:34 @agent_ppo2.py:185][0m |          -0.0052 |          15.1174 |           5.5792 |
[32m[20221213 22:27:34 @agent_ppo2.py:185][0m |          -0.0048 |          15.0210 |           5.5807 |
[32m[20221213 22:27:34 @agent_ppo2.py:185][0m |          -0.0112 |          14.8970 |           5.5821 |
[32m[20221213 22:27:35 @agent_ppo2.py:185][0m |           0.0008 |          15.3235 |           5.5790 |
[32m[20221213 22:27:35 @agent_ppo2.py:185][0m |          -0.0103 |          14.7545 |           5.5801 |
[32m[20221213 22:27:35 @agent_ppo2.py:185][0m |          -0.0088 |          14.6689 |           5.5770 |
[32m[20221213 22:27:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:27:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.04
[32m[20221213 22:27:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.87
[32m[20221213 22:27:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 220.79
[32m[20221213 22:27:35 @agent_ppo2.py:143][0m Total time:       9.37 min
[32m[20221213 22:27:35 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221213 22:27:35 @agent_ppo2.py:121][0m #------------------------ Iteration 434 --------------------------#
[32m[20221213 22:27:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:35 @agent_ppo2.py:185][0m |          -0.0005 |          17.4355 |           5.7400 |
[32m[20221213 22:27:35 @agent_ppo2.py:185][0m |          -0.0073 |          16.6051 |           5.7266 |
[32m[20221213 22:27:35 @agent_ppo2.py:185][0m |          -0.0108 |          16.1405 |           5.7239 |
[32m[20221213 22:27:35 @agent_ppo2.py:185][0m |          -0.0139 |          16.0990 |           5.7226 |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0073 |          15.8682 |           5.7217 |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0090 |          15.7522 |           5.7264 |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0042 |          15.6691 |           5.7266 |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0091 |          15.6274 |           5.7258 |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0011 |          16.0871 |           5.7281 |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0090 |          15.5975 |           5.7305 |
[32m[20221213 22:27:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:27:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 166.45
[32m[20221213 22:27:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.09
[32m[20221213 22:27:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.61
[32m[20221213 22:27:36 @agent_ppo2.py:143][0m Total time:       9.39 min
[32m[20221213 22:27:36 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221213 22:27:36 @agent_ppo2.py:121][0m #------------------------ Iteration 435 --------------------------#
[32m[20221213 22:27:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0007 |          23.1077 |           5.5881 |
[32m[20221213 22:27:36 @agent_ppo2.py:185][0m |          -0.0010 |          22.4210 |           5.5836 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0050 |          22.2019 |           5.5776 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0057 |          22.0390 |           5.5782 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0088 |          21.9306 |           5.5718 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0088 |          22.0617 |           5.5699 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0093 |          21.7843 |           5.5664 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0097 |          21.8694 |           5.5632 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0094 |          21.7376 |           5.5672 |
[32m[20221213 22:27:37 @agent_ppo2.py:185][0m |          -0.0089 |          21.9637 |           5.5642 |
[32m[20221213 22:27:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.25
[32m[20221213 22:27:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.13
[32m[20221213 22:27:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.11
[32m[20221213 22:27:37 @agent_ppo2.py:143][0m Total time:       9.41 min
[32m[20221213 22:27:37 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221213 22:27:37 @agent_ppo2.py:121][0m #------------------------ Iteration 436 --------------------------#
[32m[20221213 22:27:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |           0.0004 |          19.1179 |           5.7041 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0059 |          18.2400 |           5.6963 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0125 |          17.9045 |           5.6961 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0027 |          18.5597 |           5.6942 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0104 |          17.6814 |           5.6904 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0037 |          17.9240 |           5.6925 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0065 |          17.4361 |           5.6879 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0072 |          17.4510 |           5.6889 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0124 |          17.3620 |           5.6883 |
[32m[20221213 22:27:38 @agent_ppo2.py:185][0m |          -0.0081 |          17.4574 |           5.6865 |
[32m[20221213 22:27:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:27:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.86
[32m[20221213 22:27:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.08
[32m[20221213 22:27:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.70
[32m[20221213 22:27:39 @agent_ppo2.py:143][0m Total time:       9.43 min
[32m[20221213 22:27:39 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221213 22:27:39 @agent_ppo2.py:121][0m #------------------------ Iteration 437 --------------------------#
[32m[20221213 22:27:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0019 |          10.7263 |           5.6006 |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0051 |          10.2020 |           5.5919 |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0040 |          10.0573 |           5.5887 |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0066 |           9.9325 |           5.5856 |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0024 |           9.9071 |           5.5836 |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0093 |           9.8225 |           5.5824 |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0085 |           9.7318 |           5.5819 |
[32m[20221213 22:27:39 @agent_ppo2.py:185][0m |          -0.0082 |           9.7305 |           5.5805 |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |          -0.0055 |           9.6827 |           5.5773 |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |          -0.0076 |           9.6325 |           5.5785 |
[32m[20221213 22:27:40 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:27:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.99
[32m[20221213 22:27:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.13
[32m[20221213 22:27:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.08
[32m[20221213 22:27:40 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 316.08
[32m[20221213 22:27:40 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 316.08
[32m[20221213 22:27:40 @agent_ppo2.py:143][0m Total time:       9.45 min
[32m[20221213 22:27:40 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221213 22:27:40 @agent_ppo2.py:121][0m #------------------------ Iteration 438 --------------------------#
[32m[20221213 22:27:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |           0.0001 |          15.4504 |           5.5120 |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |          -0.0041 |          13.8432 |           5.5039 |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |          -0.0071 |          13.0892 |           5.4967 |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |          -0.0066 |          12.8935 |           5.4949 |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |          -0.0145 |          12.6163 |           5.4928 |
[32m[20221213 22:27:40 @agent_ppo2.py:185][0m |          -0.0077 |          12.5292 |           5.4928 |
[32m[20221213 22:27:41 @agent_ppo2.py:185][0m |          -0.0087 |          12.4576 |           5.4905 |
[32m[20221213 22:27:41 @agent_ppo2.py:185][0m |          -0.0136 |          12.4854 |           5.4890 |
[32m[20221213 22:27:41 @agent_ppo2.py:185][0m |          -0.0040 |          12.5074 |           5.4885 |
[32m[20221213 22:27:41 @agent_ppo2.py:185][0m |          -0.0099 |          12.2059 |           5.4853 |
[32m[20221213 22:27:41 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.26
[32m[20221213 22:27:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.17
[32m[20221213 22:27:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 165.69
[32m[20221213 22:27:41 @agent_ppo2.py:143][0m Total time:       9.47 min
[32m[20221213 22:27:41 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221213 22:27:41 @agent_ppo2.py:121][0m #------------------------ Iteration 439 --------------------------#
[32m[20221213 22:27:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:41 @agent_ppo2.py:185][0m |           0.0010 |          10.2108 |           5.6516 |
[32m[20221213 22:27:41 @agent_ppo2.py:185][0m |           0.0001 |           5.9054 |           5.6460 |
[32m[20221213 22:27:41 @agent_ppo2.py:185][0m |           0.0006 |           5.6141 |           5.6458 |
[32m[20221213 22:27:42 @agent_ppo2.py:185][0m |          -0.0031 |           5.4245 |           5.6475 |
[32m[20221213 22:27:42 @agent_ppo2.py:185][0m |          -0.0043 |           5.3389 |           5.6440 |
[32m[20221213 22:27:42 @agent_ppo2.py:185][0m |          -0.0040 |           5.2472 |           5.6447 |
[32m[20221213 22:27:42 @agent_ppo2.py:185][0m |          -0.0100 |           5.2173 |           5.6469 |
[32m[20221213 22:27:42 @agent_ppo2.py:185][0m |          -0.0091 |           5.1717 |           5.6453 |
[32m[20221213 22:27:42 @agent_ppo2.py:185][0m |          -0.0045 |           5.1452 |           5.6474 |
[32m[20221213 22:27:42 @agent_ppo2.py:185][0m |          -0.0098 |           5.0920 |           5.6469 |
[32m[20221213 22:27:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:27:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.44
[32m[20221213 22:27:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.73
[32m[20221213 22:27:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.24
[32m[20221213 22:27:42 @agent_ppo2.py:143][0m Total time:       9.49 min
[32m[20221213 22:27:42 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221213 22:27:42 @agent_ppo2.py:121][0m #------------------------ Iteration 440 --------------------------#
[32m[20221213 22:27:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:27:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0054 |          18.8226 |           5.4925 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0026 |          15.8579 |           5.4896 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0074 |          15.5703 |           5.4900 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0053 |          15.3525 |           5.4869 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0024 |          15.1845 |           5.4895 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0085 |          15.0857 |           5.4882 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0081 |          14.7820 |           5.4903 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0133 |          14.7738 |           5.4901 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0009 |          17.5061 |           5.4896 |
[32m[20221213 22:27:43 @agent_ppo2.py:185][0m |          -0.0097 |          14.6407 |           5.4887 |
[32m[20221213 22:27:43 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 34.38
[32m[20221213 22:27:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.91
[32m[20221213 22:27:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.31
[32m[20221213 22:27:43 @agent_ppo2.py:143][0m Total time:       9.51 min
[32m[20221213 22:27:43 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221213 22:27:43 @agent_ppo2.py:121][0m #------------------------ Iteration 441 --------------------------#
[32m[20221213 22:27:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |           0.0012 |           9.3858 |           5.6734 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |          -0.0051 |           6.6433 |           5.6713 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |           0.0008 |           6.5706 |           5.6630 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |           0.0023 |           6.7544 |           5.6617 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |          -0.0028 |           6.2272 |           5.6625 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |          -0.0049 |           6.0907 |           5.6643 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |          -0.0124 |           6.0701 |           5.6652 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |          -0.0092 |           6.0526 |           5.6628 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |          -0.0072 |           6.0669 |           5.6648 |
[32m[20221213 22:27:44 @agent_ppo2.py:185][0m |          -0.0032 |           6.1135 |           5.6676 |
[32m[20221213 22:27:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.54
[32m[20221213 22:27:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.87
[32m[20221213 22:27:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.73
[32m[20221213 22:27:45 @agent_ppo2.py:143][0m Total time:       9.53 min
[32m[20221213 22:27:45 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221213 22:27:45 @agent_ppo2.py:121][0m #------------------------ Iteration 442 --------------------------#
[32m[20221213 22:27:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0009 |          18.8294 |           5.6764 |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0019 |          17.1013 |           5.6708 |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0085 |          16.7539 |           5.6664 |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0091 |          16.5668 |           5.6689 |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0070 |          16.2116 |           5.6717 |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0133 |          16.2174 |           5.6696 |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0139 |          15.9908 |           5.6719 |
[32m[20221213 22:27:45 @agent_ppo2.py:185][0m |          -0.0112 |          15.8878 |           5.6726 |
[32m[20221213 22:27:46 @agent_ppo2.py:185][0m |          -0.0096 |          15.7749 |           5.6738 |
[32m[20221213 22:27:46 @agent_ppo2.py:185][0m |          -0.0123 |          15.7664 |           5.6739 |
[32m[20221213 22:27:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.03
[32m[20221213 22:27:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.72
[32m[20221213 22:27:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.37
[32m[20221213 22:27:46 @agent_ppo2.py:143][0m Total time:       9.55 min
[32m[20221213 22:27:46 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221213 22:27:46 @agent_ppo2.py:121][0m #------------------------ Iteration 443 --------------------------#
[32m[20221213 22:27:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:46 @agent_ppo2.py:185][0m |           0.0003 |          20.3050 |           5.6303 |
[32m[20221213 22:27:46 @agent_ppo2.py:185][0m |          -0.0075 |          18.6652 |           5.6247 |
[32m[20221213 22:27:46 @agent_ppo2.py:185][0m |          -0.0045 |          18.2846 |           5.6227 |
[32m[20221213 22:27:46 @agent_ppo2.py:185][0m |          -0.0107 |          18.0043 |           5.6219 |
[32m[20221213 22:27:46 @agent_ppo2.py:185][0m |          -0.0005 |          18.9263 |           5.6201 |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |          -0.0123 |          17.9905 |           5.6188 |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |          -0.0126 |          17.5263 |           5.6178 |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |          -0.0053 |          18.4441 |           5.6174 |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |          -0.0093 |          17.2386 |           5.6175 |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |          -0.0086 |          17.1614 |           5.6146 |
[32m[20221213 22:27:47 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:27:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.02
[32m[20221213 22:27:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.81
[32m[20221213 22:27:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 16.03
[32m[20221213 22:27:47 @agent_ppo2.py:143][0m Total time:       9.57 min
[32m[20221213 22:27:47 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221213 22:27:47 @agent_ppo2.py:121][0m #------------------------ Iteration 444 --------------------------#
[32m[20221213 22:27:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |          -0.0007 |          24.7695 |           5.7781 |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |           0.0003 |          23.8101 |           5.7765 |
[32m[20221213 22:27:47 @agent_ppo2.py:185][0m |          -0.0036 |          23.4729 |           5.7782 |
[32m[20221213 22:27:48 @agent_ppo2.py:185][0m |          -0.0053 |          23.2447 |           5.7775 |
[32m[20221213 22:27:48 @agent_ppo2.py:185][0m |          -0.0089 |          22.9900 |           5.7806 |
[32m[20221213 22:27:48 @agent_ppo2.py:185][0m |          -0.0134 |          22.8414 |           5.7813 |
[32m[20221213 22:27:48 @agent_ppo2.py:185][0m |          -0.0061 |          22.6380 |           5.7821 |
[32m[20221213 22:27:48 @agent_ppo2.py:185][0m |          -0.0005 |          24.9445 |           5.7820 |
[32m[20221213 22:27:48 @agent_ppo2.py:185][0m |          -0.0100 |          22.6109 |           5.7821 |
[32m[20221213 22:27:48 @agent_ppo2.py:185][0m |          -0.0078 |          22.3699 |           5.7835 |
[32m[20221213 22:27:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:27:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 149.92
[32m[20221213 22:27:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.46
[32m[20221213 22:27:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.86
[32m[20221213 22:27:48 @agent_ppo2.py:143][0m Total time:       9.59 min
[32m[20221213 22:27:48 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221213 22:27:48 @agent_ppo2.py:121][0m #------------------------ Iteration 445 --------------------------#
[32m[20221213 22:27:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |           0.0006 |           9.6061 |           5.8011 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0072 |           6.9933 |           5.7914 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0032 |           6.7027 |           5.7877 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0111 |           6.5209 |           5.7860 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0067 |           6.4006 |           5.7829 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0067 |           6.3850 |           5.7860 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |           0.0031 |           6.7258 |           5.7820 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0085 |           6.2593 |           5.7806 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0100 |           6.1830 |           5.7776 |
[32m[20221213 22:27:49 @agent_ppo2.py:185][0m |          -0.0050 |           6.1484 |           5.7768 |
[32m[20221213 22:27:49 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.39
[32m[20221213 22:27:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.66
[32m[20221213 22:27:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:27:49 @agent_ppo2.py:143][0m Total time:       9.61 min
[32m[20221213 22:27:49 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221213 22:27:49 @agent_ppo2.py:121][0m #------------------------ Iteration 446 --------------------------#
[32m[20221213 22:27:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |           0.0094 |          22.3142 |           5.7400 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0071 |          19.3211 |           5.7309 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0066 |          18.7839 |           5.7289 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0027 |          18.6473 |           5.7286 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0001 |          19.5522 |           5.7260 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0140 |          18.5760 |           5.7221 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0083 |          18.2904 |           5.7199 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0135 |          18.2767 |           5.7209 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0071 |          18.0546 |           5.7236 |
[32m[20221213 22:27:50 @agent_ppo2.py:185][0m |          -0.0117 |          17.8417 |           5.7225 |
[32m[20221213 22:27:50 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:27:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.86
[32m[20221213 22:27:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.23
[32m[20221213 22:27:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 122.66
[32m[20221213 22:27:51 @agent_ppo2.py:143][0m Total time:       9.63 min
[32m[20221213 22:27:51 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221213 22:27:51 @agent_ppo2.py:121][0m #------------------------ Iteration 447 --------------------------#
[32m[20221213 22:27:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:51 @agent_ppo2.py:185][0m |           0.0104 |          11.2874 |           5.7223 |
[32m[20221213 22:27:51 @agent_ppo2.py:185][0m |          -0.0026 |           8.6928 |           5.7182 |
[32m[20221213 22:27:51 @agent_ppo2.py:185][0m |          -0.0045 |           8.5099 |           5.7177 |
[32m[20221213 22:27:51 @agent_ppo2.py:185][0m |          -0.0064 |           8.5501 |           5.7183 |
[32m[20221213 22:27:51 @agent_ppo2.py:185][0m |          -0.0023 |           8.6723 |           5.7180 |
[32m[20221213 22:27:51 @agent_ppo2.py:185][0m |          -0.0037 |           8.3594 |           5.7187 |
[32m[20221213 22:27:51 @agent_ppo2.py:185][0m |          -0.0040 |           8.3555 |           5.7168 |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0056 |           8.3489 |           5.7173 |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0051 |           8.2823 |           5.7184 |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0086 |           8.3040 |           5.7194 |
[32m[20221213 22:27:52 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.66
[32m[20221213 22:27:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.15
[32m[20221213 22:27:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.12
[32m[20221213 22:27:52 @agent_ppo2.py:143][0m Total time:       9.65 min
[32m[20221213 22:27:52 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221213 22:27:52 @agent_ppo2.py:121][0m #------------------------ Iteration 448 --------------------------#
[32m[20221213 22:27:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0008 |          10.8817 |           5.8120 |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0072 |           9.5097 |           5.8072 |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0031 |           9.3386 |           5.8052 |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0029 |           9.1455 |           5.8006 |
[32m[20221213 22:27:52 @agent_ppo2.py:185][0m |          -0.0069 |           9.1743 |           5.8001 |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0037 |           9.0525 |           5.7971 |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0067 |           9.2908 |           5.7975 |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0148 |           8.9082 |           5.7938 |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0086 |           8.9956 |           5.7968 |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0101 |           8.7540 |           5.7964 |
[32m[20221213 22:27:53 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.61
[32m[20221213 22:27:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.11
[32m[20221213 22:27:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.71
[32m[20221213 22:27:53 @agent_ppo2.py:143][0m Total time:       9.67 min
[32m[20221213 22:27:53 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221213 22:27:53 @agent_ppo2.py:121][0m #------------------------ Iteration 449 --------------------------#
[32m[20221213 22:27:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0052 |          28.2724 |           5.6404 |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0098 |          22.5330 |           5.6290 |
[32m[20221213 22:27:53 @agent_ppo2.py:185][0m |          -0.0047 |          21.0667 |           5.6294 |
[32m[20221213 22:27:54 @agent_ppo2.py:185][0m |          -0.0070 |          20.1455 |           5.6314 |
[32m[20221213 22:27:54 @agent_ppo2.py:185][0m |          -0.0056 |          21.2422 |           5.6283 |
[32m[20221213 22:27:54 @agent_ppo2.py:185][0m |          -0.0130 |          19.4378 |           5.6251 |
[32m[20221213 22:27:54 @agent_ppo2.py:185][0m |          -0.0139 |          18.6307 |           5.6240 |
[32m[20221213 22:27:54 @agent_ppo2.py:185][0m |          -0.0186 |          18.4587 |           5.6219 |
[32m[20221213 22:27:54 @agent_ppo2.py:185][0m |          -0.0135 |          18.4494 |           5.6211 |
[32m[20221213 22:27:54 @agent_ppo2.py:185][0m |          -0.0154 |          17.7475 |           5.6206 |
[32m[20221213 22:27:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:27:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.20
[32m[20221213 22:27:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.74
[32m[20221213 22:27:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.76
[32m[20221213 22:27:54 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221213 22:27:54 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221213 22:27:54 @agent_ppo2.py:121][0m #------------------------ Iteration 450 --------------------------#
[32m[20221213 22:27:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:27:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0028 |          17.4455 |           5.6825 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0016 |          16.2168 |           5.6794 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0044 |          15.9883 |           5.6821 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0068 |          15.8486 |           5.6812 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0129 |          15.7603 |           5.6818 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0047 |          15.9084 |           5.6860 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0095 |          15.5532 |           5.6874 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0030 |          15.5153 |           5.6901 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0086 |          15.4102 |           5.6912 |
[32m[20221213 22:27:55 @agent_ppo2.py:185][0m |          -0.0078 |          15.3228 |           5.6902 |
[32m[20221213 22:27:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 72.55
[32m[20221213 22:27:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.70
[32m[20221213 22:27:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.36
[32m[20221213 22:27:55 @agent_ppo2.py:143][0m Total time:       9.71 min
[32m[20221213 22:27:55 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221213 22:27:55 @agent_ppo2.py:121][0m #------------------------ Iteration 451 --------------------------#
[32m[20221213 22:27:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:27:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |           0.0120 |          12.4921 |           5.8806 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0106 |          10.0197 |           5.8755 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0094 |           9.5390 |           5.8734 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0114 |           9.3410 |           5.8715 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0084 |           9.3067 |           5.8703 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0021 |           9.2502 |           5.8735 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0124 |           8.9870 |           5.8678 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0096 |           8.8828 |           5.8685 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0147 |           8.8218 |           5.8674 |
[32m[20221213 22:27:56 @agent_ppo2.py:185][0m |          -0.0114 |           8.7181 |           5.8663 |
[32m[20221213 22:27:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:27:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.78
[32m[20221213 22:27:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.46
[32m[20221213 22:27:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.18
[32m[20221213 22:27:57 @agent_ppo2.py:143][0m Total time:       9.73 min
[32m[20221213 22:27:57 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221213 22:27:57 @agent_ppo2.py:121][0m #------------------------ Iteration 452 --------------------------#
[32m[20221213 22:27:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:57 @agent_ppo2.py:185][0m |           0.0001 |          10.6902 |           5.9086 |
[32m[20221213 22:27:57 @agent_ppo2.py:185][0m |          -0.0025 |           9.8984 |           5.8895 |
[32m[20221213 22:27:57 @agent_ppo2.py:185][0m |          -0.0044 |           9.7623 |           5.8891 |
[32m[20221213 22:27:57 @agent_ppo2.py:185][0m |          -0.0091 |           9.6687 |           5.8917 |
[32m[20221213 22:27:57 @agent_ppo2.py:185][0m |          -0.0099 |           9.6473 |           5.8885 |
[32m[20221213 22:27:57 @agent_ppo2.py:185][0m |          -0.0153 |           9.6081 |           5.8831 |
[32m[20221213 22:27:57 @agent_ppo2.py:185][0m |          -0.0114 |           9.5705 |           5.8845 |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |          -0.0142 |           9.5251 |           5.8847 |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |          -0.0138 |           9.4959 |           5.8853 |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |          -0.0106 |           9.4831 |           5.8908 |
[32m[20221213 22:27:58 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:27:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.55
[32m[20221213 22:27:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 311.80
[32m[20221213 22:27:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 226.98
[32m[20221213 22:27:58 @agent_ppo2.py:143][0m Total time:       9.75 min
[32m[20221213 22:27:58 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221213 22:27:58 @agent_ppo2.py:121][0m #------------------------ Iteration 453 --------------------------#
[32m[20221213 22:27:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |           0.0033 |          25.4527 |           5.8515 |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |          -0.0032 |          23.3376 |           5.8466 |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |          -0.0085 |          22.8277 |           5.8423 |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |          -0.0098 |          22.4296 |           5.8357 |
[32m[20221213 22:27:58 @agent_ppo2.py:185][0m |          -0.0111 |          22.1896 |           5.8375 |
[32m[20221213 22:27:59 @agent_ppo2.py:185][0m |          -0.0158 |          22.0585 |           5.8393 |
[32m[20221213 22:27:59 @agent_ppo2.py:185][0m |          -0.0114 |          21.9927 |           5.8388 |
[32m[20221213 22:27:59 @agent_ppo2.py:185][0m |          -0.0010 |          21.9779 |           5.8381 |
[32m[20221213 22:27:59 @agent_ppo2.py:185][0m |          -0.0104 |          21.6317 |           5.8403 |
[32m[20221213 22:27:59 @agent_ppo2.py:185][0m |          -0.0110 |          21.5270 |           5.8400 |
[32m[20221213 22:27:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:27:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 159.54
[32m[20221213 22:27:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.57
[32m[20221213 22:27:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.35
[32m[20221213 22:27:59 @agent_ppo2.py:143][0m Total time:       9.77 min
[32m[20221213 22:27:59 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221213 22:27:59 @agent_ppo2.py:121][0m #------------------------ Iteration 454 --------------------------#
[32m[20221213 22:27:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:27:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:27:59 @agent_ppo2.py:185][0m |           0.0055 |          10.7071 |           5.8526 |
[32m[20221213 22:27:59 @agent_ppo2.py:185][0m |          -0.0066 |           9.2781 |           5.8476 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0065 |           9.0920 |           5.8405 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0048 |           8.9953 |           5.8439 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0089 |           8.9241 |           5.8397 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0032 |           9.0154 |           5.8417 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0109 |           8.8792 |           5.8444 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0098 |           8.8302 |           5.8421 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0172 |           8.7877 |           5.8428 |
[32m[20221213 22:28:00 @agent_ppo2.py:185][0m |          -0.0105 |           8.7330 |           5.8436 |
[32m[20221213 22:28:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:28:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.59
[32m[20221213 22:28:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.91
[32m[20221213 22:28:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 30.13
[32m[20221213 22:28:00 @agent_ppo2.py:143][0m Total time:       9.79 min
[32m[20221213 22:28:00 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221213 22:28:00 @agent_ppo2.py:121][0m #------------------------ Iteration 455 --------------------------#
[32m[20221213 22:28:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |           0.0012 |          22.2654 |           5.9598 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0093 |          20.7827 |           5.9605 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0072 |          20.2720 |           5.9597 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0052 |          20.0839 |           5.9536 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0114 |          19.9271 |           5.9518 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0106 |          19.7873 |           5.9501 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0162 |          19.7435 |           5.9508 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0110 |          19.7367 |           5.9499 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0100 |          19.5793 |           5.9467 |
[32m[20221213 22:28:01 @agent_ppo2.py:185][0m |          -0.0067 |          19.4075 |           5.9479 |
[32m[20221213 22:28:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:28:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.09
[32m[20221213 22:28:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.24
[32m[20221213 22:28:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.52
[32m[20221213 22:28:01 @agent_ppo2.py:143][0m Total time:       9.81 min
[32m[20221213 22:28:01 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221213 22:28:01 @agent_ppo2.py:121][0m #------------------------ Iteration 456 --------------------------#
[32m[20221213 22:28:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |           0.0003 |          22.0167 |           5.9213 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0015 |          20.6458 |           5.9125 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0100 |          20.1040 |           5.9051 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0054 |          19.6962 |           5.9078 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0108 |          19.4940 |           5.9073 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0078 |          19.3531 |           5.9072 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0086 |          19.2494 |           5.9060 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0123 |          19.1387 |           5.9078 |
[32m[20221213 22:28:02 @agent_ppo2.py:185][0m |          -0.0135 |          19.0461 |           5.9086 |
[32m[20221213 22:28:03 @agent_ppo2.py:185][0m |          -0.0141 |          18.9082 |           5.9098 |
[32m[20221213 22:28:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:28:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.25
[32m[20221213 22:28:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.81
[32m[20221213 22:28:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 38.74
[32m[20221213 22:28:03 @agent_ppo2.py:143][0m Total time:       9.83 min
[32m[20221213 22:28:03 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221213 22:28:03 @agent_ppo2.py:121][0m #------------------------ Iteration 457 --------------------------#
[32m[20221213 22:28:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:03 @agent_ppo2.py:185][0m |           0.0056 |          23.5632 |           5.9893 |
[32m[20221213 22:28:03 @agent_ppo2.py:185][0m |          -0.0036 |          22.5781 |           5.9788 |
[32m[20221213 22:28:03 @agent_ppo2.py:185][0m |           0.0003 |          23.0973 |           5.9735 |
[32m[20221213 22:28:03 @agent_ppo2.py:185][0m |          -0.0047 |          22.1447 |           5.9735 |
[32m[20221213 22:28:03 @agent_ppo2.py:185][0m |          -0.0091 |          21.9913 |           5.9685 |
[32m[20221213 22:28:03 @agent_ppo2.py:185][0m |          -0.0094 |          21.9130 |           5.9706 |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0096 |          21.8651 |           5.9653 |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0095 |          21.7868 |           5.9652 |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0129 |          21.7564 |           5.9649 |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0091 |          21.6712 |           5.9601 |
[32m[20221213 22:28:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.39
[32m[20221213 22:28:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.15
[32m[20221213 22:28:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.23
[32m[20221213 22:28:04 @agent_ppo2.py:143][0m Total time:       9.85 min
[32m[20221213 22:28:04 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221213 22:28:04 @agent_ppo2.py:121][0m #------------------------ Iteration 458 --------------------------#
[32m[20221213 22:28:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0016 |           9.3056 |           5.9687 |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0005 |           7.8378 |           5.9664 |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0014 |           7.6228 |           5.9647 |
[32m[20221213 22:28:04 @agent_ppo2.py:185][0m |          -0.0074 |           7.5060 |           5.9660 |
[32m[20221213 22:28:05 @agent_ppo2.py:185][0m |          -0.0035 |           7.4592 |           5.9685 |
[32m[20221213 22:28:05 @agent_ppo2.py:185][0m |          -0.0071 |           7.4156 |           5.9652 |
[32m[20221213 22:28:05 @agent_ppo2.py:185][0m |          -0.0023 |           7.4019 |           5.9699 |
[32m[20221213 22:28:05 @agent_ppo2.py:185][0m |           0.0061 |           7.5264 |           5.9710 |
[32m[20221213 22:28:05 @agent_ppo2.py:185][0m |          -0.0039 |           7.3175 |           5.9709 |
[32m[20221213 22:28:05 @agent_ppo2.py:185][0m |          -0.0090 |           7.3345 |           5.9713 |
[32m[20221213 22:28:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:28:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.76
[32m[20221213 22:28:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.17
[32m[20221213 22:28:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.86
[32m[20221213 22:28:05 @agent_ppo2.py:143][0m Total time:       9.87 min
[32m[20221213 22:28:05 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221213 22:28:05 @agent_ppo2.py:121][0m #------------------------ Iteration 459 --------------------------#
[32m[20221213 22:28:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:28:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:05 @agent_ppo2.py:185][0m |           0.0146 |          14.0325 |           5.9326 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |          -0.0063 |          12.3073 |           5.9175 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |           0.0015 |          12.5646 |           5.9206 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |          -0.0093 |          11.9711 |           5.9207 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |           0.0007 |          12.6677 |           5.9153 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |          -0.0061 |          11.8667 |           5.9165 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |          -0.0099 |          11.7758 |           5.9163 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |          -0.0120 |          11.7415 |           5.9191 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |          -0.0105 |          11.6987 |           5.9204 |
[32m[20221213 22:28:06 @agent_ppo2.py:185][0m |          -0.0104 |          11.6031 |           5.9164 |
[32m[20221213 22:28:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:28:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.30
[32m[20221213 22:28:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.41
[32m[20221213 22:28:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.35
[32m[20221213 22:28:06 @agent_ppo2.py:143][0m Total time:       9.89 min
[32m[20221213 22:28:06 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221213 22:28:06 @agent_ppo2.py:121][0m #------------------------ Iteration 460 --------------------------#
[32m[20221213 22:28:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:28:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0017 |          29.5519 |           5.9872 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0027 |          26.6535 |           5.9851 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |           0.0056 |          30.0140 |           5.9789 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0109 |          26.2923 |           5.9813 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0055 |          25.8113 |           5.9820 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0089 |          25.7457 |           5.9838 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0074 |          25.5361 |           5.9855 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0078 |          25.4744 |           5.9838 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |          -0.0061 |          25.2533 |           5.9921 |
[32m[20221213 22:28:07 @agent_ppo2.py:185][0m |           0.0044 |          28.5962 |           5.9898 |
[32m[20221213 22:28:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 168.20
[32m[20221213 22:28:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.23
[32m[20221213 22:28:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 18.77
[32m[20221213 22:28:08 @agent_ppo2.py:143][0m Total time:       9.91 min
[32m[20221213 22:28:08 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221213 22:28:08 @agent_ppo2.py:121][0m #------------------------ Iteration 461 --------------------------#
[32m[20221213 22:28:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:08 @agent_ppo2.py:185][0m |          -0.0031 |          21.0193 |           6.0594 |
[32m[20221213 22:28:08 @agent_ppo2.py:185][0m |          -0.0030 |          19.8553 |           6.0451 |
[32m[20221213 22:28:08 @agent_ppo2.py:185][0m |          -0.0080 |          19.7130 |           6.0444 |
[32m[20221213 22:28:08 @agent_ppo2.py:185][0m |          -0.0063 |          19.5379 |           6.0411 |
[32m[20221213 22:28:08 @agent_ppo2.py:185][0m |          -0.0139 |          19.3278 |           6.0418 |
[32m[20221213 22:28:08 @agent_ppo2.py:185][0m |          -0.0130 |          19.1784 |           6.0375 |
[32m[20221213 22:28:08 @agent_ppo2.py:185][0m |          -0.0126 |          19.1486 |           6.0390 |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |          -0.0139 |          19.0955 |           6.0352 |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |          -0.0147 |          18.9476 |           6.0365 |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |          -0.0103 |          18.8726 |           6.0350 |
[32m[20221213 22:28:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.98
[32m[20221213 22:28:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 225.18
[32m[20221213 22:28:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 21.29
[32m[20221213 22:28:09 @agent_ppo2.py:143][0m Total time:       9.93 min
[32m[20221213 22:28:09 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221213 22:28:09 @agent_ppo2.py:121][0m #------------------------ Iteration 462 --------------------------#
[32m[20221213 22:28:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |           0.0005 |          13.8631 |           6.0154 |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |          -0.0061 |          12.5349 |           6.0006 |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |          -0.0072 |          12.3386 |           6.0011 |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |          -0.0078 |          12.0676 |           6.0049 |
[32m[20221213 22:28:09 @agent_ppo2.py:185][0m |          -0.0056 |          11.9645 |           6.0083 |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |          -0.0043 |          11.8439 |           6.0107 |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |          -0.0102 |          11.7987 |           6.0125 |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |          -0.0102 |          11.6392 |           6.0145 |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |          -0.0049 |          11.6129 |           6.0146 |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |          -0.0059 |          11.5682 |           6.0188 |
[32m[20221213 22:28:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.30
[32m[20221213 22:28:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.52
[32m[20221213 22:28:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.61
[32m[20221213 22:28:10 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 319.61
[32m[20221213 22:28:10 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 319.61
[32m[20221213 22:28:10 @agent_ppo2.py:143][0m Total time:       9.95 min
[32m[20221213 22:28:10 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221213 22:28:10 @agent_ppo2.py:121][0m #------------------------ Iteration 463 --------------------------#
[32m[20221213 22:28:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |           0.0036 |          15.7839 |           6.1492 |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |          -0.0046 |          15.1079 |           6.1401 |
[32m[20221213 22:28:10 @agent_ppo2.py:185][0m |          -0.0068 |          15.0591 |           6.1347 |
[32m[20221213 22:28:11 @agent_ppo2.py:185][0m |          -0.0066 |          15.0112 |           6.1326 |
[32m[20221213 22:28:11 @agent_ppo2.py:185][0m |          -0.0097 |          14.9516 |           6.1266 |
[32m[20221213 22:28:11 @agent_ppo2.py:185][0m |          -0.0036 |          15.0688 |           6.1314 |
[32m[20221213 22:28:11 @agent_ppo2.py:185][0m |          -0.0083 |          14.9106 |           6.1298 |
[32m[20221213 22:28:11 @agent_ppo2.py:185][0m |          -0.0068 |          14.8835 |           6.1299 |
[32m[20221213 22:28:11 @agent_ppo2.py:185][0m |          -0.0101 |          14.8994 |           6.1295 |
[32m[20221213 22:28:11 @agent_ppo2.py:185][0m |          -0.0062 |          14.8471 |           6.1330 |
[32m[20221213 22:28:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.54
[32m[20221213 22:28:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.08
[32m[20221213 22:28:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.15
[32m[20221213 22:28:11 @agent_ppo2.py:143][0m Total time:       9.97 min
[32m[20221213 22:28:11 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221213 22:28:11 @agent_ppo2.py:121][0m #------------------------ Iteration 464 --------------------------#
[32m[20221213 22:28:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0051 |          26.6133 |           6.1357 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |           0.0093 |          29.0234 |           6.1266 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0048 |          25.7668 |           6.1197 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0035 |          25.6241 |           6.1257 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0100 |          25.3957 |           6.1243 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |           0.0031 |          26.5485 |           6.1208 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0125 |          25.3704 |           6.1240 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0053 |          26.7241 |           6.1208 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0109 |          25.2534 |           6.1223 |
[32m[20221213 22:28:12 @agent_ppo2.py:185][0m |          -0.0109 |          25.2224 |           6.1206 |
[32m[20221213 22:28:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:28:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.36
[32m[20221213 22:28:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.43
[32m[20221213 22:28:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.95
[32m[20221213 22:28:12 @agent_ppo2.py:143][0m Total time:       9.99 min
[32m[20221213 22:28:12 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221213 22:28:12 @agent_ppo2.py:121][0m #------------------------ Iteration 465 --------------------------#
[32m[20221213 22:28:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0024 |          27.6644 |           6.1742 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0063 |          26.7422 |           6.1727 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0079 |          26.6251 |           6.1670 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0085 |          26.3746 |           6.1687 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0073 |          26.2352 |           6.1633 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0063 |          26.0979 |           6.1652 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0096 |          26.0122 |           6.1627 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0009 |          27.1938 |           6.1643 |
[32m[20221213 22:28:13 @agent_ppo2.py:185][0m |          -0.0099 |          25.9159 |           6.1625 |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0100 |          25.7846 |           6.1625 |
[32m[20221213 22:28:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:28:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 207.34
[32m[20221213 22:28:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.37
[32m[20221213 22:28:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.28
[32m[20221213 22:28:14 @agent_ppo2.py:143][0m Total time:      10.01 min
[32m[20221213 22:28:14 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221213 22:28:14 @agent_ppo2.py:121][0m #------------------------ Iteration 466 --------------------------#
[32m[20221213 22:28:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0036 |          22.6311 |           6.1166 |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0040 |          21.9682 |           6.1058 |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0117 |          21.1232 |           6.1031 |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0078 |          20.7367 |           6.1046 |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0103 |          20.6688 |           6.1018 |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0123 |          20.4422 |           6.1030 |
[32m[20221213 22:28:14 @agent_ppo2.py:185][0m |          -0.0135 |          20.2895 |           6.1049 |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |          -0.0116 |          20.1998 |           6.1044 |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |          -0.0118 |          19.9913 |           6.1058 |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |          -0.0120 |          19.8587 |           6.1045 |
[32m[20221213 22:28:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.22
[32m[20221213 22:28:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.86
[32m[20221213 22:28:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.77
[32m[20221213 22:28:15 @agent_ppo2.py:143][0m Total time:      10.03 min
[32m[20221213 22:28:15 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221213 22:28:15 @agent_ppo2.py:121][0m #------------------------ Iteration 467 --------------------------#
[32m[20221213 22:28:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |           0.0036 |          14.0855 |           6.2496 |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |           0.0020 |          13.6441 |           6.2483 |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |          -0.0053 |          13.1223 |           6.2442 |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |          -0.0035 |          13.1229 |           6.2443 |
[32m[20221213 22:28:15 @agent_ppo2.py:185][0m |          -0.0047 |          13.0556 |           6.2419 |
[32m[20221213 22:28:16 @agent_ppo2.py:185][0m |           0.0031 |          13.9111 |           6.2412 |
[32m[20221213 22:28:16 @agent_ppo2.py:185][0m |          -0.0051 |          12.9857 |           6.2379 |
[32m[20221213 22:28:16 @agent_ppo2.py:185][0m |          -0.0055 |          12.8954 |           6.2395 |
[32m[20221213 22:28:16 @agent_ppo2.py:185][0m |          -0.0011 |          12.9090 |           6.2384 |
[32m[20221213 22:28:16 @agent_ppo2.py:185][0m |          -0.0060 |          12.9360 |           6.2387 |
[32m[20221213 22:28:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.28
[32m[20221213 22:28:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.80
[32m[20221213 22:28:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 8.52
[32m[20221213 22:28:16 @agent_ppo2.py:143][0m Total time:      10.05 min
[32m[20221213 22:28:16 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221213 22:28:16 @agent_ppo2.py:121][0m #------------------------ Iteration 468 --------------------------#
[32m[20221213 22:28:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:16 @agent_ppo2.py:185][0m |           0.0042 |          14.2536 |           6.1647 |
[32m[20221213 22:28:16 @agent_ppo2.py:185][0m |          -0.0047 |          13.1375 |           6.1596 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |          -0.0052 |          12.9722 |           6.1614 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |          -0.0061 |          12.8477 |           6.1629 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |          -0.0018 |          12.7603 |           6.1574 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |          -0.0035 |          12.6659 |           6.1585 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |          -0.0013 |          12.5593 |           6.1591 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |          -0.0026 |          12.4706 |           6.1586 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |          -0.0081 |          12.4747 |           6.1563 |
[32m[20221213 22:28:17 @agent_ppo2.py:185][0m |           0.0053 |          14.0143 |           6.1553 |
[32m[20221213 22:28:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:28:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.57
[32m[20221213 22:28:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.23
[32m[20221213 22:28:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:28:17 @agent_ppo2.py:143][0m Total time:      10.07 min
[32m[20221213 22:28:17 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221213 22:28:17 @agent_ppo2.py:121][0m #------------------------ Iteration 469 --------------------------#
[32m[20221213 22:28:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0036 |          25.5694 |           6.2074 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0082 |          24.3335 |           6.2009 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0070 |          24.1239 |           6.1957 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0050 |          23.9329 |           6.2033 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0107 |          23.6345 |           6.1990 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0084 |          23.4841 |           6.1984 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0022 |          25.4227 |           6.1953 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0100 |          23.2943 |           6.1968 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0132 |          23.1481 |           6.1964 |
[32m[20221213 22:28:18 @agent_ppo2.py:185][0m |          -0.0109 |          23.0645 |           6.2003 |
[32m[20221213 22:28:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.44
[32m[20221213 22:28:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.81
[32m[20221213 22:28:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.08
[32m[20221213 22:28:18 @agent_ppo2.py:143][0m Total time:      10.09 min
[32m[20221213 22:28:18 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221213 22:28:18 @agent_ppo2.py:121][0m #------------------------ Iteration 470 --------------------------#
[32m[20221213 22:28:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:28:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |           0.0072 |          13.2206 |           6.2603 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |          -0.0052 |          12.0480 |           6.2506 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |          -0.0040 |          11.9024 |           6.2519 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |          -0.0062 |          11.8569 |           6.2473 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |          -0.0034 |          11.7624 |           6.2541 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |          -0.0056 |          11.6373 |           6.2493 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |          -0.0075 |          11.6154 |           6.2489 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |           0.0022 |          11.9473 |           6.2563 |
[32m[20221213 22:28:19 @agent_ppo2.py:185][0m |          -0.0068 |          11.4172 |           6.2545 |
[32m[20221213 22:28:20 @agent_ppo2.py:185][0m |          -0.0025 |          11.7618 |           6.2549 |
[32m[20221213 22:28:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.00
[32m[20221213 22:28:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.05
[32m[20221213 22:28:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.33
[32m[20221213 22:28:20 @agent_ppo2.py:143][0m Total time:      10.11 min
[32m[20221213 22:28:20 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221213 22:28:20 @agent_ppo2.py:121][0m #------------------------ Iteration 471 --------------------------#
[32m[20221213 22:28:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:20 @agent_ppo2.py:185][0m |          -0.0038 |          14.3635 |           6.3196 |
[32m[20221213 22:28:20 @agent_ppo2.py:185][0m |          -0.0081 |          13.6648 |           6.3009 |
[32m[20221213 22:28:20 @agent_ppo2.py:185][0m |          -0.0037 |          14.0015 |           6.2939 |
[32m[20221213 22:28:20 @agent_ppo2.py:185][0m |          -0.0112 |          13.3640 |           6.2882 |
[32m[20221213 22:28:20 @agent_ppo2.py:185][0m |          -0.0104 |          13.2716 |           6.2860 |
[32m[20221213 22:28:20 @agent_ppo2.py:185][0m |          -0.0083 |          13.2551 |           6.2886 |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |          -0.0135 |          13.1898 |           6.2825 |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |          -0.0117 |          13.0978 |           6.2818 |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |          -0.0109 |          13.0813 |           6.2799 |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |          -0.0133 |          13.0427 |           6.2737 |
[32m[20221213 22:28:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.25
[32m[20221213 22:28:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 318.74
[32m[20221213 22:28:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.20
[32m[20221213 22:28:21 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 338.20
[32m[20221213 22:28:21 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 338.20
[32m[20221213 22:28:21 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221213 22:28:21 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221213 22:28:21 @agent_ppo2.py:121][0m #------------------------ Iteration 472 --------------------------#
[32m[20221213 22:28:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |           0.0006 |          18.7390 |           6.3232 |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |          -0.0011 |          16.2881 |           6.3160 |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |           0.0043 |          17.2122 |           6.3114 |
[32m[20221213 22:28:21 @agent_ppo2.py:185][0m |          -0.0038 |          15.1895 |           6.3079 |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |          -0.0095 |          14.7311 |           6.3069 |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |          -0.0108 |          14.5511 |           6.3050 |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |          -0.0066 |          14.2974 |           6.3048 |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |          -0.0064 |          14.2382 |           6.3031 |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |          -0.0106 |          14.0500 |           6.3044 |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |           0.0003 |          15.2632 |           6.3014 |
[32m[20221213 22:28:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.77
[32m[20221213 22:28:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.71
[32m[20221213 22:28:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:28:22 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221213 22:28:22 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221213 22:28:22 @agent_ppo2.py:121][0m #------------------------ Iteration 473 --------------------------#
[32m[20221213 22:28:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |          -0.0017 |          37.9953 |           6.1061 |
[32m[20221213 22:28:22 @agent_ppo2.py:185][0m |           0.0001 |          33.5768 |           6.0981 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0073 |          33.0457 |           6.0911 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0051 |          32.7054 |           6.0912 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0054 |          32.4355 |           6.0918 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0062 |          32.3016 |           6.0910 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0054 |          32.0570 |           6.0925 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0058 |          31.8613 |           6.0919 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0039 |          32.8892 |           6.0879 |
[32m[20221213 22:28:23 @agent_ppo2.py:185][0m |          -0.0053 |          31.5561 |           6.0933 |
[32m[20221213 22:28:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.98
[32m[20221213 22:28:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 229.77
[32m[20221213 22:28:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.25
[32m[20221213 22:28:23 @agent_ppo2.py:143][0m Total time:      10.17 min
[32m[20221213 22:28:23 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221213 22:28:23 @agent_ppo2.py:121][0m #------------------------ Iteration 474 --------------------------#
[32m[20221213 22:28:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0027 |          28.7329 |           6.3017 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |           0.0014 |          26.9767 |           6.2948 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0062 |          26.1503 |           6.2904 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0069 |          25.8902 |           6.2874 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0116 |          25.8389 |           6.2825 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0059 |          25.7081 |           6.2835 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0124 |          25.4680 |           6.2865 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0121 |          25.4311 |           6.2811 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0138 |          25.2618 |           6.2883 |
[32m[20221213 22:28:24 @agent_ppo2.py:185][0m |          -0.0115 |          25.1741 |           6.2846 |
[32m[20221213 22:28:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:28:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.17
[32m[20221213 22:28:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.91
[32m[20221213 22:28:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.40
[32m[20221213 22:28:25 @agent_ppo2.py:143][0m Total time:      10.19 min
[32m[20221213 22:28:25 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221213 22:28:25 @agent_ppo2.py:121][0m #------------------------ Iteration 475 --------------------------#
[32m[20221213 22:28:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0014 |          14.1222 |           6.2613 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0069 |          12.4180 |           6.2579 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0068 |          12.2410 |           6.2525 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0109 |          12.1261 |           6.2553 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0052 |          12.1048 |           6.2512 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0005 |          12.9040 |           6.2503 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0006 |          12.4950 |           6.2503 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0067 |          12.0141 |           6.2507 |
[32m[20221213 22:28:25 @agent_ppo2.py:185][0m |          -0.0037 |          11.9360 |           6.2512 |
[32m[20221213 22:28:26 @agent_ppo2.py:185][0m |          -0.0113 |          11.9292 |           6.2500 |
[32m[20221213 22:28:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:28:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.66
[32m[20221213 22:28:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.11
[32m[20221213 22:28:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 195.60
[32m[20221213 22:28:26 @agent_ppo2.py:143][0m Total time:      10.21 min
[32m[20221213 22:28:26 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221213 22:28:26 @agent_ppo2.py:121][0m #------------------------ Iteration 476 --------------------------#
[32m[20221213 22:28:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:26 @agent_ppo2.py:185][0m |          -0.0006 |          28.9965 |           6.2878 |
[32m[20221213 22:28:26 @agent_ppo2.py:185][0m |          -0.0019 |          27.6094 |           6.2746 |
[32m[20221213 22:28:26 @agent_ppo2.py:185][0m |          -0.0017 |          27.3581 |           6.2685 |
[32m[20221213 22:28:26 @agent_ppo2.py:185][0m |          -0.0103 |          26.6767 |           6.2631 |
[32m[20221213 22:28:26 @agent_ppo2.py:185][0m |          -0.0099 |          26.4318 |           6.2603 |
[32m[20221213 22:28:26 @agent_ppo2.py:185][0m |          -0.0089 |          26.4242 |           6.2612 |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |          -0.0044 |          27.7079 |           6.2605 |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |          -0.0127 |          26.1234 |           6.2528 |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |          -0.0124 |          26.0216 |           6.2548 |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |          -0.0111 |          25.9742 |           6.2523 |
[32m[20221213 22:28:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.01
[32m[20221213 22:28:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.84
[32m[20221213 22:28:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.71
[32m[20221213 22:28:27 @agent_ppo2.py:143][0m Total time:      10.23 min
[32m[20221213 22:28:27 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221213 22:28:27 @agent_ppo2.py:121][0m #------------------------ Iteration 477 --------------------------#
[32m[20221213 22:28:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |           0.0056 |          35.9464 |           6.2866 |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |          -0.0074 |          32.3469 |           6.2826 |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |          -0.0085 |          31.7337 |           6.2787 |
[32m[20221213 22:28:27 @agent_ppo2.py:185][0m |          -0.0098 |          31.3746 |           6.2763 |
[32m[20221213 22:28:28 @agent_ppo2.py:185][0m |          -0.0040 |          31.3389 |           6.2753 |
[32m[20221213 22:28:28 @agent_ppo2.py:185][0m |          -0.0094 |          30.7927 |           6.2729 |
[32m[20221213 22:28:28 @agent_ppo2.py:185][0m |          -0.0131 |          30.8011 |           6.2694 |
[32m[20221213 22:28:28 @agent_ppo2.py:185][0m |          -0.0021 |          32.8765 |           6.2671 |
[32m[20221213 22:28:28 @agent_ppo2.py:185][0m |          -0.0046 |          30.2558 |           6.2623 |
[32m[20221213 22:28:28 @agent_ppo2.py:185][0m |          -0.0115 |          30.3468 |           6.2610 |
[32m[20221213 22:28:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 152.56
[32m[20221213 22:28:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.34
[32m[20221213 22:28:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.73
[32m[20221213 22:28:28 @agent_ppo2.py:143][0m Total time:      10.25 min
[32m[20221213 22:28:28 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221213 22:28:28 @agent_ppo2.py:121][0m #------------------------ Iteration 478 --------------------------#
[32m[20221213 22:28:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:28 @agent_ppo2.py:185][0m |          -0.0011 |          29.3931 |           6.2056 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0005 |          28.1694 |           6.1993 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0077 |          27.7068 |           6.1968 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0072 |          27.3808 |           6.1943 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0097 |          27.1913 |           6.1951 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0008 |          28.4135 |           6.1952 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0105 |          27.0472 |           6.1983 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0084 |          26.9554 |           6.1991 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0114 |          26.8177 |           6.1997 |
[32m[20221213 22:28:29 @agent_ppo2.py:185][0m |          -0.0094 |          26.7068 |           6.2048 |
[32m[20221213 22:28:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.19
[32m[20221213 22:28:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.29
[32m[20221213 22:28:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.93
[32m[20221213 22:28:29 @agent_ppo2.py:143][0m Total time:      10.27 min
[32m[20221213 22:28:29 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221213 22:28:29 @agent_ppo2.py:121][0m #------------------------ Iteration 479 --------------------------#
[32m[20221213 22:28:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0016 |          15.6291 |           6.2335 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0004 |          13.9630 |           6.2329 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0050 |          13.4243 |           6.2279 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0044 |          13.1316 |           6.2272 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |           0.0015 |          13.3118 |           6.2272 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0051 |          12.6984 |           6.2305 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0103 |          12.5886 |           6.2292 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0059 |          12.5606 |           6.2301 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0050 |          12.4963 |           6.2317 |
[32m[20221213 22:28:30 @agent_ppo2.py:185][0m |          -0.0083 |          12.3024 |           6.2333 |
[32m[20221213 22:28:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:28:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.82
[32m[20221213 22:28:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.03
[32m[20221213 22:28:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.33
[32m[20221213 22:28:31 @agent_ppo2.py:143][0m Total time:      10.29 min
[32m[20221213 22:28:31 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221213 22:28:31 @agent_ppo2.py:121][0m #------------------------ Iteration 480 --------------------------#
[32m[20221213 22:28:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:28:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:31 @agent_ppo2.py:185][0m |          -0.0042 |          30.8887 |           6.2790 |
[32m[20221213 22:28:31 @agent_ppo2.py:185][0m |           0.0046 |          32.1607 |           6.2658 |
[32m[20221213 22:28:31 @agent_ppo2.py:185][0m |          -0.0051 |          28.8943 |           6.2644 |
[32m[20221213 22:28:31 @agent_ppo2.py:185][0m |          -0.0085 |          28.3118 |           6.2634 |
[32m[20221213 22:28:31 @agent_ppo2.py:185][0m |           0.0013 |          29.3541 |           6.2632 |
[32m[20221213 22:28:31 @agent_ppo2.py:185][0m |          -0.0047 |          28.1635 |           6.2583 |
[32m[20221213 22:28:31 @agent_ppo2.py:185][0m |          -0.0093 |          27.7800 |           6.2608 |
[32m[20221213 22:28:32 @agent_ppo2.py:185][0m |          -0.0059 |          28.0561 |           6.2643 |
[32m[20221213 22:28:32 @agent_ppo2.py:185][0m |          -0.0062 |          27.8658 |           6.2623 |
[32m[20221213 22:28:32 @agent_ppo2.py:185][0m |          -0.0133 |          27.4677 |           6.2548 |
[32m[20221213 22:28:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:28:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 185.37
[32m[20221213 22:28:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.13
[32m[20221213 22:28:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.70
[32m[20221213 22:28:32 @agent_ppo2.py:143][0m Total time:      10.32 min
[32m[20221213 22:28:32 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221213 22:28:32 @agent_ppo2.py:121][0m #------------------------ Iteration 481 --------------------------#
[32m[20221213 22:28:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:28:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:32 @agent_ppo2.py:185][0m |          -0.0020 |          31.2731 |           6.3261 |
[32m[20221213 22:28:32 @agent_ppo2.py:185][0m |          -0.0089 |          30.3073 |           6.3166 |
[32m[20221213 22:28:32 @agent_ppo2.py:185][0m |          -0.0018 |          29.7170 |           6.3192 |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0087 |          29.4154 |           6.3136 |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0049 |          29.2862 |           6.3123 |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0086 |          29.0383 |           6.3122 |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0048 |          28.9571 |           6.3182 |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0080 |          28.8048 |           6.3134 |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0107 |          28.6267 |           6.3148 |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0066 |          28.4718 |           6.3159 |
[32m[20221213 22:28:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:28:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.55
[32m[20221213 22:28:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.76
[32m[20221213 22:28:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.39
[32m[20221213 22:28:33 @agent_ppo2.py:143][0m Total time:      10.34 min
[32m[20221213 22:28:33 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221213 22:28:33 @agent_ppo2.py:121][0m #------------------------ Iteration 482 --------------------------#
[32m[20221213 22:28:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:33 @agent_ppo2.py:185][0m |          -0.0014 |          22.6018 |           6.2217 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |           0.0007 |          22.5036 |           6.2143 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |          -0.0108 |          21.2744 |           6.2093 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |          -0.0067 |          21.1158 |           6.2079 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |           0.0043 |          23.3682 |           6.2085 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |          -0.0062 |          20.8046 |           6.2074 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |          -0.0081 |          20.6645 |           6.2063 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |          -0.0108 |          20.6160 |           6.2063 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |          -0.0128 |          20.6565 |           6.2049 |
[32m[20221213 22:28:34 @agent_ppo2.py:185][0m |          -0.0046 |          20.6226 |           6.2049 |
[32m[20221213 22:28:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:28:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.57
[32m[20221213 22:28:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.16
[32m[20221213 22:28:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 192.12
[32m[20221213 22:28:34 @agent_ppo2.py:143][0m Total time:      10.36 min
[32m[20221213 22:28:34 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221213 22:28:34 @agent_ppo2.py:121][0m #------------------------ Iteration 483 --------------------------#
[32m[20221213 22:28:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |           0.0001 |          22.1523 |           6.4150 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0010 |          21.4787 |           6.4108 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0051 |          21.2755 |           6.4106 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0087 |          21.2187 |           6.4073 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0047 |          21.4189 |           6.4092 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0061 |          20.8791 |           6.4083 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0117 |          20.8286 |           6.4103 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0094 |          20.7066 |           6.4128 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0068 |          20.6797 |           6.4129 |
[32m[20221213 22:28:35 @agent_ppo2.py:185][0m |          -0.0103 |          20.6098 |           6.4102 |
[32m[20221213 22:28:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:28:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.20
[32m[20221213 22:28:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 173.80
[32m[20221213 22:28:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.97
[32m[20221213 22:28:36 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 357.97
[32m[20221213 22:28:36 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 357.97
[32m[20221213 22:28:36 @agent_ppo2.py:143][0m Total time:      10.38 min
[32m[20221213 22:28:36 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221213 22:28:36 @agent_ppo2.py:121][0m #------------------------ Iteration 484 --------------------------#
[32m[20221213 22:28:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:36 @agent_ppo2.py:185][0m |           0.0194 |          33.0536 |           6.3666 |
[32m[20221213 22:28:36 @agent_ppo2.py:185][0m |          -0.0002 |          26.9157 |           6.3542 |
[32m[20221213 22:28:36 @agent_ppo2.py:185][0m |          -0.0053 |          25.7991 |           6.3543 |
[32m[20221213 22:28:36 @agent_ppo2.py:185][0m |          -0.0044 |          25.0789 |           6.3501 |
[32m[20221213 22:28:36 @agent_ppo2.py:185][0m |          -0.0060 |          24.6350 |           6.3498 |
[32m[20221213 22:28:36 @agent_ppo2.py:185][0m |          -0.0100 |          24.5506 |           6.3445 |
[32m[20221213 22:28:36 @agent_ppo2.py:185][0m |           0.0047 |          26.6944 |           6.3466 |
[32m[20221213 22:28:37 @agent_ppo2.py:185][0m |          -0.0059 |          23.9929 |           6.3413 |
[32m[20221213 22:28:37 @agent_ppo2.py:185][0m |          -0.0112 |          23.7618 |           6.3393 |
[32m[20221213 22:28:37 @agent_ppo2.py:185][0m |          -0.0083 |          23.5744 |           6.3383 |
[32m[20221213 22:28:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.49
[32m[20221213 22:28:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.07
[32m[20221213 22:28:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.45
[32m[20221213 22:28:37 @agent_ppo2.py:143][0m Total time:      10.40 min
[32m[20221213 22:28:37 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221213 22:28:37 @agent_ppo2.py:121][0m #------------------------ Iteration 485 --------------------------#
[32m[20221213 22:28:37 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 22:28:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:37 @agent_ppo2.py:185][0m |           0.0009 |          24.7932 |           6.3224 |
[32m[20221213 22:28:37 @agent_ppo2.py:185][0m |          -0.0029 |          23.3453 |           6.3184 |
[32m[20221213 22:28:37 @agent_ppo2.py:185][0m |          -0.0119 |          22.5274 |           6.3125 |
[32m[20221213 22:28:38 @agent_ppo2.py:185][0m |          -0.0045 |          22.4581 |           6.3115 |
[32m[20221213 22:28:38 @agent_ppo2.py:185][0m |          -0.0072 |          22.1126 |           6.3091 |
[32m[20221213 22:28:38 @agent_ppo2.py:185][0m |           0.0009 |          22.7568 |           6.3087 |
[32m[20221213 22:28:38 @agent_ppo2.py:185][0m |          -0.0148 |          21.8307 |           6.3049 |
[32m[20221213 22:28:38 @agent_ppo2.py:185][0m |          -0.0123 |          21.6847 |           6.3028 |
[32m[20221213 22:28:38 @agent_ppo2.py:185][0m |          -0.0105 |          21.4638 |           6.3033 |
[32m[20221213 22:28:38 @agent_ppo2.py:185][0m |          -0.0099 |          21.4366 |           6.3024 |
[32m[20221213 22:28:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 203.40
[32m[20221213 22:28:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.38
[32m[20221213 22:28:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.76
[32m[20221213 22:28:38 @agent_ppo2.py:143][0m Total time:      10.42 min
[32m[20221213 22:28:38 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221213 22:28:38 @agent_ppo2.py:121][0m #------------------------ Iteration 486 --------------------------#
[32m[20221213 22:28:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |           0.0042 |          11.1138 |           6.2978 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0035 |           8.3332 |           6.2948 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0089 |           8.2113 |           6.2956 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0028 |           8.0767 |           6.2958 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0055 |           8.0086 |           6.2965 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0090 |           7.9749 |           6.2996 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0019 |           8.0823 |           6.3021 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0109 |           7.8508 |           6.2982 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0095 |           7.8234 |           6.3055 |
[32m[20221213 22:28:39 @agent_ppo2.py:185][0m |          -0.0024 |           7.9504 |           6.3063 |
[32m[20221213 22:28:39 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:28:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.75
[32m[20221213 22:28:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 318.81
[32m[20221213 22:28:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.72
[32m[20221213 22:28:39 @agent_ppo2.py:143][0m Total time:      10.44 min
[32m[20221213 22:28:39 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221213 22:28:39 @agent_ppo2.py:121][0m #------------------------ Iteration 487 --------------------------#
[32m[20221213 22:28:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0056 |          28.7969 |           6.3373 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0054 |          26.9092 |           6.3291 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0017 |          26.8318 |           6.3285 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0101 |          25.9477 |           6.3265 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0071 |          25.5311 |           6.3315 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0082 |          25.2403 |           6.3310 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0088 |          25.8758 |           6.3336 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0151 |          25.0135 |           6.3331 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0009 |          26.7058 |           6.3348 |
[32m[20221213 22:28:40 @agent_ppo2.py:185][0m |          -0.0044 |          25.0041 |           6.3396 |
[32m[20221213 22:28:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:28:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.17
[32m[20221213 22:28:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.94
[32m[20221213 22:28:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.05
[32m[20221213 22:28:41 @agent_ppo2.py:143][0m Total time:      10.46 min
[32m[20221213 22:28:41 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221213 22:28:41 @agent_ppo2.py:121][0m #------------------------ Iteration 488 --------------------------#
[32m[20221213 22:28:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:41 @agent_ppo2.py:185][0m |           0.0013 |          27.6375 |           6.4803 |
[32m[20221213 22:28:41 @agent_ppo2.py:185][0m |           0.0019 |          26.3937 |           6.4762 |
[32m[20221213 22:28:41 @agent_ppo2.py:185][0m |          -0.0071 |          25.6312 |           6.4684 |
[32m[20221213 22:28:41 @agent_ppo2.py:185][0m |          -0.0080 |          25.3046 |           6.4669 |
[32m[20221213 22:28:41 @agent_ppo2.py:185][0m |          -0.0058 |          25.0580 |           6.4646 |
[32m[20221213 22:28:41 @agent_ppo2.py:185][0m |          -0.0048 |          24.7612 |           6.4598 |
[32m[20221213 22:28:41 @agent_ppo2.py:185][0m |           0.0001 |          25.3175 |           6.4585 |
[32m[20221213 22:28:42 @agent_ppo2.py:185][0m |          -0.0077 |          24.4258 |           6.4587 |
[32m[20221213 22:28:42 @agent_ppo2.py:185][0m |          -0.0071 |          24.4214 |           6.4559 |
[32m[20221213 22:28:42 @agent_ppo2.py:185][0m |          -0.0097 |          24.5919 |           6.4530 |
[32m[20221213 22:28:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:28:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.05
[32m[20221213 22:28:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.22
[32m[20221213 22:28:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.66
[32m[20221213 22:28:42 @agent_ppo2.py:143][0m Total time:      10.48 min
[32m[20221213 22:28:42 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221213 22:28:42 @agent_ppo2.py:121][0m #------------------------ Iteration 489 --------------------------#
[32m[20221213 22:28:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:42 @agent_ppo2.py:185][0m |          -0.0069 |          10.2751 |           6.2973 |
[32m[20221213 22:28:42 @agent_ppo2.py:185][0m |           0.0077 |           8.5818 |           6.2942 |
[32m[20221213 22:28:42 @agent_ppo2.py:185][0m |           0.0019 |           7.8630 |           6.2933 |
[32m[20221213 22:28:42 @agent_ppo2.py:185][0m |          -0.0062 |           7.6368 |           6.2887 |
[32m[20221213 22:28:43 @agent_ppo2.py:185][0m |          -0.0104 |           7.6170 |           6.2875 |
[32m[20221213 22:28:43 @agent_ppo2.py:185][0m |          -0.0030 |           7.4301 |           6.2859 |
[32m[20221213 22:28:43 @agent_ppo2.py:185][0m |          -0.0024 |           7.3163 |           6.2877 |
[32m[20221213 22:28:43 @agent_ppo2.py:185][0m |          -0.0086 |           7.1724 |           6.2839 |
[32m[20221213 22:28:43 @agent_ppo2.py:185][0m |          -0.0075 |           7.1256 |           6.2851 |
[32m[20221213 22:28:43 @agent_ppo2.py:185][0m |          -0.0047 |           7.1273 |           6.2842 |
[32m[20221213 22:28:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:28:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.35
[32m[20221213 22:28:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.37
[32m[20221213 22:28:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.80
[32m[20221213 22:28:43 @agent_ppo2.py:143][0m Total time:      10.50 min
[32m[20221213 22:28:43 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221213 22:28:43 @agent_ppo2.py:121][0m #------------------------ Iteration 490 --------------------------#
[32m[20221213 22:28:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:28:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:43 @agent_ppo2.py:185][0m |          -0.0011 |          10.9616 |           6.4218 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0065 |           9.5722 |           6.4166 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0101 |           9.3671 |           6.4198 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0050 |           9.2999 |           6.4159 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0106 |           9.2942 |           6.4173 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0037 |           9.2296 |           6.4179 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0071 |           9.0210 |           6.4197 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0084 |           8.9396 |           6.4159 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0090 |           8.8750 |           6.4200 |
[32m[20221213 22:28:44 @agent_ppo2.py:185][0m |          -0.0085 |           8.8365 |           6.4186 |
[32m[20221213 22:28:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:28:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.30
[32m[20221213 22:28:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.96
[32m[20221213 22:28:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.74
[32m[20221213 22:28:44 @agent_ppo2.py:143][0m Total time:      10.52 min
[32m[20221213 22:28:44 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221213 22:28:44 @agent_ppo2.py:121][0m #------------------------ Iteration 491 --------------------------#
[32m[20221213 22:28:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0033 |          17.4103 |           6.4224 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0022 |          16.7338 |           6.4103 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0011 |          16.5094 |           6.4045 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0067 |          16.2808 |           6.4073 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |           0.0020 |          17.6480 |           6.4093 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0061 |          15.9529 |           6.4063 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0079 |          15.8631 |           6.4050 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0101 |          15.6563 |           6.4061 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0055 |          15.5648 |           6.4071 |
[32m[20221213 22:28:45 @agent_ppo2.py:185][0m |          -0.0129 |          15.6131 |           6.4061 |
[32m[20221213 22:28:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:28:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.44
[32m[20221213 22:28:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.35
[32m[20221213 22:28:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.33
[32m[20221213 22:28:46 @agent_ppo2.py:143][0m Total time:      10.54 min
[32m[20221213 22:28:46 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221213 22:28:46 @agent_ppo2.py:121][0m #------------------------ Iteration 492 --------------------------#
[32m[20221213 22:28:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0022 |          29.0013 |           6.3697 |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0069 |          26.1740 |           6.3595 |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0038 |          25.7739 |           6.3558 |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0052 |          25.5429 |           6.3579 |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0078 |          25.1072 |           6.3507 |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0152 |          25.0201 |           6.3508 |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0143 |          24.7282 |           6.3524 |
[32m[20221213 22:28:46 @agent_ppo2.py:185][0m |          -0.0076 |          24.6575 |           6.3523 |
[32m[20221213 22:28:47 @agent_ppo2.py:185][0m |          -0.0051 |          24.5939 |           6.3510 |
[32m[20221213 22:28:47 @agent_ppo2.py:185][0m |          -0.0089 |          24.4791 |           6.3470 |
[32m[20221213 22:28:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:28:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.27
[32m[20221213 22:28:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.80
[32m[20221213 22:28:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.71
[32m[20221213 22:28:47 @agent_ppo2.py:143][0m Total time:      10.56 min
[32m[20221213 22:28:47 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221213 22:28:47 @agent_ppo2.py:121][0m #------------------------ Iteration 493 --------------------------#
[32m[20221213 22:28:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:47 @agent_ppo2.py:185][0m |          -0.0020 |          28.6567 |           6.5381 |
[32m[20221213 22:28:47 @agent_ppo2.py:185][0m |           0.0022 |          26.8410 |           6.5308 |
[32m[20221213 22:28:47 @agent_ppo2.py:185][0m |          -0.0074 |          26.0648 |           6.5238 |
[32m[20221213 22:28:47 @agent_ppo2.py:185][0m |          -0.0069 |          25.6664 |           6.5230 |
[32m[20221213 22:28:47 @agent_ppo2.py:185][0m |          -0.0091 |          25.7745 |           6.5260 |
[32m[20221213 22:28:48 @agent_ppo2.py:185][0m |          -0.0064 |          25.5373 |           6.5220 |
[32m[20221213 22:28:48 @agent_ppo2.py:185][0m |          -0.0093 |          25.3296 |           6.5188 |
[32m[20221213 22:28:48 @agent_ppo2.py:185][0m |          -0.0126 |          25.1417 |           6.5204 |
[32m[20221213 22:28:48 @agent_ppo2.py:185][0m |          -0.0122 |          25.1111 |           6.5188 |
[32m[20221213 22:28:48 @agent_ppo2.py:185][0m |          -0.0095 |          25.0305 |           6.5204 |
[32m[20221213 22:28:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:28:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.24
[32m[20221213 22:28:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 176.40
[32m[20221213 22:28:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.34
[32m[20221213 22:28:48 @agent_ppo2.py:143][0m Total time:      10.59 min
[32m[20221213 22:28:48 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221213 22:28:48 @agent_ppo2.py:121][0m #------------------------ Iteration 494 --------------------------#
[32m[20221213 22:28:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:48 @agent_ppo2.py:185][0m |          -0.0025 |          11.8142 |           6.3974 |
[32m[20221213 22:28:48 @agent_ppo2.py:185][0m |          -0.0050 |           9.2637 |           6.4013 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0074 |           9.1937 |           6.3986 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0079 |           9.0732 |           6.3954 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0041 |           9.1908 |           6.3979 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0116 |           8.9980 |           6.3978 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0069 |           9.2954 |           6.3978 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0096 |           8.9670 |           6.3917 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0045 |           9.0657 |           6.4007 |
[32m[20221213 22:28:49 @agent_ppo2.py:185][0m |          -0.0069 |           8.8915 |           6.4004 |
[32m[20221213 22:28:49 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:28:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.28
[32m[20221213 22:28:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.17
[32m[20221213 22:28:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.76
[32m[20221213 22:28:49 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 362.76
[32m[20221213 22:28:49 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 362.76
[32m[20221213 22:28:49 @agent_ppo2.py:143][0m Total time:      10.61 min
[32m[20221213 22:28:49 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221213 22:28:49 @agent_ppo2.py:121][0m #------------------------ Iteration 495 --------------------------#
[32m[20221213 22:28:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 22:28:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |           0.0019 |          28.7972 |           6.4208 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0063 |          27.4066 |           6.4084 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0102 |          27.1699 |           6.4112 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0155 |          26.8376 |           6.4085 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0115 |          26.6752 |           6.4090 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0135 |          26.5711 |           6.4081 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0060 |          26.4808 |           6.4073 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0100 |          26.3713 |           6.4060 |
[32m[20221213 22:28:50 @agent_ppo2.py:185][0m |          -0.0129 |          26.2157 |           6.4067 |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |          -0.0148 |          26.2088 |           6.4030 |
[32m[20221213 22:28:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:28:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.87
[32m[20221213 22:28:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.75
[32m[20221213 22:28:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.16
[32m[20221213 22:28:51 @agent_ppo2.py:143][0m Total time:      10.63 min
[32m[20221213 22:28:51 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221213 22:28:51 @agent_ppo2.py:121][0m #------------------------ Iteration 496 --------------------------#
[32m[20221213 22:28:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |           0.0015 |          24.5743 |           6.4633 |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |          -0.0022 |          23.4322 |           6.4488 |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |          -0.0062 |          23.0603 |           6.4494 |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |          -0.0057 |          22.9385 |           6.4447 |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |          -0.0071 |          22.6437 |           6.4433 |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |          -0.0073 |          22.4357 |           6.4477 |
[32m[20221213 22:28:51 @agent_ppo2.py:185][0m |          -0.0062 |          22.3476 |           6.4435 |
[32m[20221213 22:28:52 @agent_ppo2.py:185][0m |          -0.0117 |          22.2227 |           6.4435 |
[32m[20221213 22:28:52 @agent_ppo2.py:185][0m |          -0.0088 |          22.1150 |           6.4495 |
[32m[20221213 22:28:52 @agent_ppo2.py:185][0m |          -0.0028 |          22.1734 |           6.4480 |
[32m[20221213 22:28:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:28:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 142.29
[32m[20221213 22:28:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.28
[32m[20221213 22:28:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.70
[32m[20221213 22:28:52 @agent_ppo2.py:143][0m Total time:      10.65 min
[32m[20221213 22:28:52 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221213 22:28:52 @agent_ppo2.py:121][0m #------------------------ Iteration 497 --------------------------#
[32m[20221213 22:28:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:52 @agent_ppo2.py:185][0m |           0.0007 |          13.4004 |           6.4371 |
[32m[20221213 22:28:52 @agent_ppo2.py:185][0m |          -0.0052 |          11.3728 |           6.4335 |
[32m[20221213 22:28:52 @agent_ppo2.py:185][0m |          -0.0028 |          11.6625 |           6.4266 |
[32m[20221213 22:28:52 @agent_ppo2.py:185][0m |          -0.0067 |          10.8790 |           6.4203 |
[32m[20221213 22:28:53 @agent_ppo2.py:185][0m |          -0.0086 |          10.8690 |           6.4362 |
[32m[20221213 22:28:53 @agent_ppo2.py:185][0m |           0.0012 |          11.0562 |           6.4317 |
[32m[20221213 22:28:53 @agent_ppo2.py:185][0m |          -0.0112 |          10.6289 |           6.4431 |
[32m[20221213 22:28:53 @agent_ppo2.py:185][0m |          -0.0073 |          10.6237 |           6.4376 |
[32m[20221213 22:28:53 @agent_ppo2.py:185][0m |          -0.0111 |          10.5002 |           6.4354 |
[32m[20221213 22:28:53 @agent_ppo2.py:185][0m |          -0.0018 |          11.2277 |           6.4423 |
[32m[20221213 22:28:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.84
[32m[20221213 22:28:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.06
[32m[20221213 22:28:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.59
[32m[20221213 22:28:53 @agent_ppo2.py:143][0m Total time:      10.67 min
[32m[20221213 22:28:53 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221213 22:28:53 @agent_ppo2.py:121][0m #------------------------ Iteration 498 --------------------------#
[32m[20221213 22:28:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:53 @agent_ppo2.py:185][0m |          -0.0034 |          16.9795 |           6.5138 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0024 |          16.4702 |           6.5022 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0061 |          16.2251 |           6.4932 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0107 |          15.9715 |           6.4935 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0105 |          15.8904 |           6.4936 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0005 |          16.8114 |           6.4937 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0084 |          15.9126 |           6.4906 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0059 |          15.9064 |           6.4946 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0030 |          15.8559 |           6.4876 |
[32m[20221213 22:28:54 @agent_ppo2.py:185][0m |          -0.0086 |          15.6228 |           6.4943 |
[32m[20221213 22:28:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:28:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.81
[32m[20221213 22:28:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.05
[32m[20221213 22:28:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.64
[32m[20221213 22:28:54 @agent_ppo2.py:143][0m Total time:      10.69 min
[32m[20221213 22:28:54 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221213 22:28:54 @agent_ppo2.py:121][0m #------------------------ Iteration 499 --------------------------#
[32m[20221213 22:28:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0012 |          28.2342 |           6.3196 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0043 |          26.5177 |           6.3135 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0062 |          26.2179 |           6.3157 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0043 |          25.9846 |           6.3138 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0039 |          25.8689 |           6.3118 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0059 |          25.7914 |           6.3124 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0031 |          26.7206 |           6.3147 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0071 |          25.5385 |           6.3135 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |          -0.0058 |          26.1393 |           6.3176 |
[32m[20221213 22:28:55 @agent_ppo2.py:185][0m |           0.0021 |          27.2037 |           6.3130 |
[32m[20221213 22:28:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.04
[32m[20221213 22:28:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.92
[32m[20221213 22:28:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.85
[32m[20221213 22:28:56 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 362.76
[32m[20221213 22:28:56 @agent_ppo2.py:143][0m Total time:      10.71 min
[32m[20221213 22:28:56 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221213 22:28:56 @agent_ppo2.py:121][0m #------------------------ Iteration 500 --------------------------#
[32m[20221213 22:28:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:28:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:56 @agent_ppo2.py:185][0m |          -0.0029 |          27.9369 |           6.4871 |
[32m[20221213 22:28:56 @agent_ppo2.py:185][0m |           0.0027 |          25.3694 |           6.4842 |
[32m[20221213 22:28:56 @agent_ppo2.py:185][0m |          -0.0013 |          24.5816 |           6.4807 |
[32m[20221213 22:28:56 @agent_ppo2.py:185][0m |          -0.0107 |          24.1523 |           6.4793 |
[32m[20221213 22:28:56 @agent_ppo2.py:185][0m |          -0.0073 |          23.8727 |           6.4786 |
[32m[20221213 22:28:56 @agent_ppo2.py:185][0m |          -0.0053 |          24.0959 |           6.4764 |
[32m[20221213 22:28:56 @agent_ppo2.py:185][0m |          -0.0020 |          23.7570 |           6.4754 |
[32m[20221213 22:28:57 @agent_ppo2.py:185][0m |          -0.0086 |          23.2111 |           6.4754 |
[32m[20221213 22:28:57 @agent_ppo2.py:185][0m |          -0.0091 |          23.2446 |           6.4767 |
[32m[20221213 22:28:57 @agent_ppo2.py:185][0m |          -0.0012 |          23.7103 |           6.4771 |
[32m[20221213 22:28:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.11
[32m[20221213 22:28:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.57
[32m[20221213 22:28:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.32
[32m[20221213 22:28:57 @agent_ppo2.py:143][0m Total time:      10.73 min
[32m[20221213 22:28:57 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221213 22:28:57 @agent_ppo2.py:121][0m #------------------------ Iteration 501 --------------------------#
[32m[20221213 22:28:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:57 @agent_ppo2.py:185][0m |          -0.0034 |          15.1175 |           6.5035 |
[32m[20221213 22:28:57 @agent_ppo2.py:185][0m |          -0.0052 |          13.7172 |           6.5053 |
[32m[20221213 22:28:57 @agent_ppo2.py:185][0m |          -0.0064 |          13.5689 |           6.5002 |
[32m[20221213 22:28:57 @agent_ppo2.py:185][0m |          -0.0064 |          13.4582 |           6.5026 |
[32m[20221213 22:28:58 @agent_ppo2.py:185][0m |          -0.0081 |          13.4184 |           6.5045 |
[32m[20221213 22:28:58 @agent_ppo2.py:185][0m |          -0.0109 |          13.3995 |           6.5055 |
[32m[20221213 22:28:58 @agent_ppo2.py:185][0m |          -0.0050 |          13.3461 |           6.5039 |
[32m[20221213 22:28:58 @agent_ppo2.py:185][0m |          -0.0075 |          13.2979 |           6.5114 |
[32m[20221213 22:28:58 @agent_ppo2.py:185][0m |          -0.0099 |          13.2884 |           6.5159 |
[32m[20221213 22:28:58 @agent_ppo2.py:185][0m |          -0.0088 |          13.2641 |           6.5182 |
[32m[20221213 22:28:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:28:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.09
[32m[20221213 22:28:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.10
[32m[20221213 22:28:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.56
[32m[20221213 22:28:58 @agent_ppo2.py:143][0m Total time:      10.75 min
[32m[20221213 22:28:58 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221213 22:28:58 @agent_ppo2.py:121][0m #------------------------ Iteration 502 --------------------------#
[32m[20221213 22:28:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:28:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:28:58 @agent_ppo2.py:185][0m |          -0.0040 |          28.9959 |           6.6570 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0043 |          29.5227 |           6.6494 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0091 |          27.6551 |           6.6457 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0119 |          27.3873 |           6.6465 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0158 |          27.2001 |           6.6448 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0075 |          27.0242 |           6.6448 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0131 |          27.0438 |           6.6470 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0161 |          26.8854 |           6.6465 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0140 |          26.8375 |           6.6467 |
[32m[20221213 22:28:59 @agent_ppo2.py:185][0m |          -0.0120 |          26.6729 |           6.6468 |
[32m[20221213 22:28:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:28:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.31
[32m[20221213 22:28:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.44
[32m[20221213 22:28:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:28:59 @agent_ppo2.py:143][0m Total time:      10.77 min
[32m[20221213 22:28:59 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221213 22:28:59 @agent_ppo2.py:121][0m #------------------------ Iteration 503 --------------------------#
[32m[20221213 22:29:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |           0.0075 |          24.8318 |           6.6265 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0095 |          21.2344 |           6.6180 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0052 |          20.3745 |           6.6134 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0103 |          19.9821 |           6.6069 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0117 |          19.4637 |           6.6106 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0083 |          19.1062 |           6.6068 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0104 |          18.6151 |           6.6071 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0145 |          18.4716 |           6.6020 |
[32m[20221213 22:29:00 @agent_ppo2.py:185][0m |          -0.0120 |          18.1084 |           6.6018 |
[32m[20221213 22:29:01 @agent_ppo2.py:185][0m |          -0.0113 |          17.8718 |           6.6014 |
[32m[20221213 22:29:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:29:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.67
[32m[20221213 22:29:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.13
[32m[20221213 22:29:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.06
[32m[20221213 22:29:01 @agent_ppo2.py:143][0m Total time:      10.80 min
[32m[20221213 22:29:01 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221213 22:29:01 @agent_ppo2.py:121][0m #------------------------ Iteration 504 --------------------------#
[32m[20221213 22:29:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:29:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:01 @agent_ppo2.py:185][0m |          -0.0023 |          26.4353 |           6.6203 |
[32m[20221213 22:29:01 @agent_ppo2.py:185][0m |          -0.0052 |          24.7843 |           6.6108 |
[32m[20221213 22:29:01 @agent_ppo2.py:185][0m |          -0.0055 |          24.2027 |           6.6023 |
[32m[20221213 22:29:01 @agent_ppo2.py:185][0m |          -0.0101 |          23.8669 |           6.6009 |
[32m[20221213 22:29:01 @agent_ppo2.py:185][0m |          -0.0096 |          23.7313 |           6.6013 |
[32m[20221213 22:29:02 @agent_ppo2.py:185][0m |          -0.0066 |          23.4987 |           6.6024 |
[32m[20221213 22:29:02 @agent_ppo2.py:185][0m |          -0.0072 |          23.5971 |           6.6007 |
[32m[20221213 22:29:02 @agent_ppo2.py:185][0m |          -0.0040 |          23.4847 |           6.5938 |
[32m[20221213 22:29:02 @agent_ppo2.py:185][0m |          -0.0117 |          23.1524 |           6.5933 |
[32m[20221213 22:29:02 @agent_ppo2.py:185][0m |          -0.0107 |          23.0025 |           6.6007 |
[32m[20221213 22:29:02 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 22:29:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.98
[32m[20221213 22:29:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.40
[32m[20221213 22:29:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.88
[32m[20221213 22:29:02 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221213 22:29:02 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221213 22:29:02 @agent_ppo2.py:121][0m #------------------------ Iteration 505 --------------------------#
[32m[20221213 22:29:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0014 |          30.0061 |           6.5437 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0086 |          28.3236 |           6.5410 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0061 |          28.1366 |           6.5360 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0071 |          27.7081 |           6.5360 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0033 |          28.1694 |           6.5355 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0060 |          27.4631 |           6.5362 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0070 |          27.2154 |           6.5399 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0095 |          27.1991 |           6.5393 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0076 |          27.1021 |           6.5420 |
[32m[20221213 22:29:03 @agent_ppo2.py:185][0m |          -0.0109 |          27.0043 |           6.5434 |
[32m[20221213 22:29:03 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:29:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 152.71
[32m[20221213 22:29:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.37
[32m[20221213 22:29:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.66
[32m[20221213 22:29:03 @agent_ppo2.py:143][0m Total time:      10.84 min
[32m[20221213 22:29:03 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221213 22:29:03 @agent_ppo2.py:121][0m #------------------------ Iteration 506 --------------------------#
[32m[20221213 22:29:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:29:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0018 |          15.1438 |           6.7317 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0057 |          13.9285 |           6.7218 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0022 |          13.8607 |           6.7174 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0014 |          13.8061 |           6.7179 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0075 |          13.6495 |           6.7173 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0054 |          13.6433 |           6.7133 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0056 |          13.6865 |           6.7089 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0102 |          13.4336 |           6.7099 |
[32m[20221213 22:29:04 @agent_ppo2.py:185][0m |          -0.0120 |          13.4295 |           6.7113 |
[32m[20221213 22:29:05 @agent_ppo2.py:185][0m |          -0.0059 |          13.9483 |           6.7087 |
[32m[20221213 22:29:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:29:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.01
[32m[20221213 22:29:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.22
[32m[20221213 22:29:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.36
[32m[20221213 22:29:05 @agent_ppo2.py:143][0m Total time:      10.86 min
[32m[20221213 22:29:05 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221213 22:29:05 @agent_ppo2.py:121][0m #------------------------ Iteration 507 --------------------------#
[32m[20221213 22:29:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:05 @agent_ppo2.py:185][0m |          -0.0015 |          16.5929 |           6.7028 |
[32m[20221213 22:29:05 @agent_ppo2.py:185][0m |          -0.0008 |          16.0090 |           6.6961 |
[32m[20221213 22:29:05 @agent_ppo2.py:185][0m |          -0.0069 |          15.8467 |           6.6901 |
[32m[20221213 22:29:05 @agent_ppo2.py:185][0m |          -0.0022 |          16.3605 |           6.6782 |
[32m[20221213 22:29:05 @agent_ppo2.py:185][0m |          -0.0080 |          15.6141 |           6.6751 |
[32m[20221213 22:29:05 @agent_ppo2.py:185][0m |          -0.0091 |          15.5017 |           6.6746 |
[32m[20221213 22:29:06 @agent_ppo2.py:185][0m |          -0.0108 |          15.4610 |           6.6741 |
[32m[20221213 22:29:06 @agent_ppo2.py:185][0m |          -0.0104 |          15.4984 |           6.6706 |
[32m[20221213 22:29:06 @agent_ppo2.py:185][0m |          -0.0114 |          15.5194 |           6.6658 |
[32m[20221213 22:29:06 @agent_ppo2.py:185][0m |          -0.0052 |          15.4477 |           6.6655 |
[32m[20221213 22:29:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:29:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.94
[32m[20221213 22:29:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.09
[32m[20221213 22:29:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 51.37
[32m[20221213 22:29:06 @agent_ppo2.py:143][0m Total time:      10.88 min
[32m[20221213 22:29:06 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221213 22:29:06 @agent_ppo2.py:121][0m #------------------------ Iteration 508 --------------------------#
[32m[20221213 22:29:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:06 @agent_ppo2.py:185][0m |          -0.0018 |          29.3983 |           6.7427 |
[32m[20221213 22:29:06 @agent_ppo2.py:185][0m |           0.0067 |          30.1805 |           6.7407 |
[32m[20221213 22:29:06 @agent_ppo2.py:185][0m |          -0.0108 |          27.4057 |           6.7471 |
[32m[20221213 22:29:07 @agent_ppo2.py:185][0m |          -0.0117 |          27.1058 |           6.7446 |
[32m[20221213 22:29:07 @agent_ppo2.py:185][0m |          -0.0131 |          26.9133 |           6.7461 |
[32m[20221213 22:29:07 @agent_ppo2.py:185][0m |          -0.0097 |          26.7951 |           6.7477 |
[32m[20221213 22:29:07 @agent_ppo2.py:185][0m |          -0.0051 |          26.9881 |           6.7500 |
[32m[20221213 22:29:07 @agent_ppo2.py:185][0m |          -0.0114 |          26.5922 |           6.7484 |
[32m[20221213 22:29:07 @agent_ppo2.py:185][0m |          -0.0098 |          26.6108 |           6.7548 |
[32m[20221213 22:29:07 @agent_ppo2.py:185][0m |          -0.0110 |          26.6573 |           6.7570 |
[32m[20221213 22:29:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:29:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.91
[32m[20221213 22:29:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.01
[32m[20221213 22:29:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.62
[32m[20221213 22:29:07 @agent_ppo2.py:143][0m Total time:      10.90 min
[32m[20221213 22:29:07 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221213 22:29:07 @agent_ppo2.py:121][0m #------------------------ Iteration 509 --------------------------#
[32m[20221213 22:29:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0002 |          31.0860 |           6.6313 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0070 |          27.8605 |           6.6259 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0081 |          26.9883 |           6.6217 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0064 |          26.4089 |           6.6247 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0093 |          26.0437 |           6.6208 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0042 |          25.9781 |           6.6189 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0066 |          25.9385 |           6.6188 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0099 |          25.4727 |           6.6195 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0150 |          25.2804 |           6.6183 |
[32m[20221213 22:29:08 @agent_ppo2.py:185][0m |          -0.0109 |          25.2075 |           6.6128 |
[32m[20221213 22:29:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:29:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.07
[32m[20221213 22:29:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.51
[32m[20221213 22:29:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.90
[32m[20221213 22:29:08 @agent_ppo2.py:143][0m Total time:      10.93 min
[32m[20221213 22:29:08 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221213 22:29:08 @agent_ppo2.py:121][0m #------------------------ Iteration 510 --------------------------#
[32m[20221213 22:29:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:29:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |          -0.0016 |          18.6502 |           6.6985 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |          -0.0035 |          16.7152 |           6.7014 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |          -0.0019 |          16.1481 |           6.7011 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |          -0.0043 |          15.9998 |           6.6983 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |           0.0003 |          15.7724 |           6.6946 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |           0.0016 |          16.1626 |           6.6973 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |          -0.0054 |          15.0927 |           6.6957 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |          -0.0057 |          14.8469 |           6.6967 |
[32m[20221213 22:29:09 @agent_ppo2.py:185][0m |          -0.0013 |          14.7651 |           6.6924 |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |          -0.0092 |          14.6095 |           6.6938 |
[32m[20221213 22:29:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:29:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.78
[32m[20221213 22:29:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.46
[32m[20221213 22:29:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 26.53
[32m[20221213 22:29:10 @agent_ppo2.py:143][0m Total time:      10.95 min
[32m[20221213 22:29:10 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221213 22:29:10 @agent_ppo2.py:121][0m #------------------------ Iteration 511 --------------------------#
[32m[20221213 22:29:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |          -0.0023 |          19.3583 |           6.5457 |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |          -0.0088 |          18.4066 |           6.5326 |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |          -0.0065 |          18.0493 |           6.5299 |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |           0.0056 |          18.5819 |           6.5323 |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |          -0.0104 |          17.7495 |           6.5300 |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |          -0.0116 |          17.6534 |           6.5358 |
[32m[20221213 22:29:10 @agent_ppo2.py:185][0m |          -0.0112 |          17.5193 |           6.5318 |
[32m[20221213 22:29:11 @agent_ppo2.py:185][0m |          -0.0081 |          17.6767 |           6.5359 |
[32m[20221213 22:29:11 @agent_ppo2.py:185][0m |          -0.0134 |          17.4172 |           6.5366 |
[32m[20221213 22:29:11 @agent_ppo2.py:185][0m |          -0.0125 |          17.3957 |           6.5372 |
[32m[20221213 22:29:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.29
[32m[20221213 22:29:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.18
[32m[20221213 22:29:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.56
[32m[20221213 22:29:11 @agent_ppo2.py:143][0m Total time:      10.97 min
[32m[20221213 22:29:11 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221213 22:29:11 @agent_ppo2.py:121][0m #------------------------ Iteration 512 --------------------------#
[32m[20221213 22:29:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:11 @agent_ppo2.py:185][0m |          -0.0005 |          26.2283 |           6.7169 |
[32m[20221213 22:29:11 @agent_ppo2.py:185][0m |          -0.0043 |          25.4332 |           6.7174 |
[32m[20221213 22:29:11 @agent_ppo2.py:185][0m |          -0.0078 |          25.2263 |           6.7126 |
[32m[20221213 22:29:11 @agent_ppo2.py:185][0m |          -0.0095 |          25.1521 |           6.7159 |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0096 |          24.9770 |           6.7111 |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0107 |          24.8953 |           6.7120 |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0090 |          24.7574 |           6.7146 |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0080 |          24.7123 |           6.7061 |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0110 |          24.8127 |           6.7127 |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0084 |          24.5665 |           6.7148 |
[32m[20221213 22:29:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.21
[32m[20221213 22:29:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.05
[32m[20221213 22:29:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.65
[32m[20221213 22:29:12 @agent_ppo2.py:143][0m Total time:      10.99 min
[32m[20221213 22:29:12 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221213 22:29:12 @agent_ppo2.py:121][0m #------------------------ Iteration 513 --------------------------#
[32m[20221213 22:29:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0043 |          28.2831 |           6.7348 |
[32m[20221213 22:29:12 @agent_ppo2.py:185][0m |          -0.0050 |          26.8164 |           6.7232 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |          -0.0062 |          26.4840 |           6.7206 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |          -0.0086 |          26.3445 |           6.7177 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |          -0.0071 |          26.0750 |           6.7110 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |          -0.0056 |          25.9678 |           6.7152 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |          -0.0106 |          25.9210 |           6.7165 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |           0.0014 |          28.4686 |           6.7122 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |          -0.0001 |          26.4420 |           6.7018 |
[32m[20221213 22:29:13 @agent_ppo2.py:185][0m |          -0.0156 |          25.5663 |           6.7128 |
[32m[20221213 22:29:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.66
[32m[20221213 22:29:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.85
[32m[20221213 22:29:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.31
[32m[20221213 22:29:13 @agent_ppo2.py:143][0m Total time:      11.01 min
[32m[20221213 22:29:13 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221213 22:29:13 @agent_ppo2.py:121][0m #------------------------ Iteration 514 --------------------------#
[32m[20221213 22:29:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0026 |          31.6932 |           6.7834 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0118 |          31.2038 |           6.7741 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0097 |          30.9614 |           6.7681 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0103 |          30.8104 |           6.7727 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0107 |          30.7277 |           6.7683 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0081 |          30.6963 |           6.7683 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0107 |          30.5813 |           6.7655 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0106 |          30.5587 |           6.7683 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0127 |          30.4977 |           6.7676 |
[32m[20221213 22:29:14 @agent_ppo2.py:185][0m |          -0.0128 |          30.4595 |           6.7681 |
[32m[20221213 22:29:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.70
[32m[20221213 22:29:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.91
[32m[20221213 22:29:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.63
[32m[20221213 22:29:15 @agent_ppo2.py:143][0m Total time:      11.03 min
[32m[20221213 22:29:15 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221213 22:29:15 @agent_ppo2.py:121][0m #------------------------ Iteration 515 --------------------------#
[32m[20221213 22:29:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |           0.0030 |          24.5233 |           6.7423 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0065 |          23.5827 |           6.7417 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0031 |          23.4723 |           6.7367 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0085 |          22.7933 |           6.7363 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0103 |          22.6164 |           6.7304 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0121 |          22.5624 |           6.7280 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0110 |          22.3397 |           6.7282 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0067 |          22.2610 |           6.7260 |
[32m[20221213 22:29:15 @agent_ppo2.py:185][0m |          -0.0115 |          22.0429 |           6.7238 |
[32m[20221213 22:29:16 @agent_ppo2.py:185][0m |          -0.0079 |          22.0231 |           6.7245 |
[32m[20221213 22:29:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.89
[32m[20221213 22:29:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.55
[32m[20221213 22:29:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.78
[32m[20221213 22:29:16 @agent_ppo2.py:143][0m Total time:      11.05 min
[32m[20221213 22:29:16 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221213 22:29:16 @agent_ppo2.py:121][0m #------------------------ Iteration 516 --------------------------#
[32m[20221213 22:29:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:16 @agent_ppo2.py:185][0m |           0.0030 |          25.9186 |           6.7240 |
[32m[20221213 22:29:16 @agent_ppo2.py:185][0m |          -0.0055 |          22.6182 |           6.7147 |
[32m[20221213 22:29:16 @agent_ppo2.py:185][0m |          -0.0090 |          21.4975 |           6.7155 |
[32m[20221213 22:29:16 @agent_ppo2.py:185][0m |          -0.0034 |          20.6041 |           6.7137 |
[32m[20221213 22:29:16 @agent_ppo2.py:185][0m |          -0.0061 |          20.0952 |           6.7118 |
[32m[20221213 22:29:16 @agent_ppo2.py:185][0m |          -0.0065 |          19.7833 |           6.7124 |
[32m[20221213 22:29:17 @agent_ppo2.py:185][0m |          -0.0070 |          19.5601 |           6.7141 |
[32m[20221213 22:29:17 @agent_ppo2.py:185][0m |          -0.0141 |          19.1575 |           6.7120 |
[32m[20221213 22:29:17 @agent_ppo2.py:185][0m |          -0.0102 |          19.0320 |           6.7116 |
[32m[20221213 22:29:17 @agent_ppo2.py:185][0m |          -0.0102 |          18.8292 |           6.7062 |
[32m[20221213 22:29:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.25
[32m[20221213 22:29:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.59
[32m[20221213 22:29:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.40
[32m[20221213 22:29:17 @agent_ppo2.py:143][0m Total time:      11.07 min
[32m[20221213 22:29:17 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221213 22:29:17 @agent_ppo2.py:121][0m #------------------------ Iteration 517 --------------------------#
[32m[20221213 22:29:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:17 @agent_ppo2.py:185][0m |          -0.0036 |          19.5965 |           6.7966 |
[32m[20221213 22:29:17 @agent_ppo2.py:185][0m |          -0.0019 |          17.1496 |           6.7966 |
[32m[20221213 22:29:17 @agent_ppo2.py:185][0m |          -0.0095 |          16.5642 |           6.7952 |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0072 |          16.3442 |           6.7952 |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0086 |          16.1235 |           6.7929 |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0082 |          16.1795 |           6.7908 |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0115 |          15.8361 |           6.7938 |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0068 |          15.6869 |           6.7950 |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0061 |          15.5101 |           6.7876 |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0100 |          15.5648 |           6.7964 |
[32m[20221213 22:29:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.32
[32m[20221213 22:29:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.28
[32m[20221213 22:29:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.94
[32m[20221213 22:29:18 @agent_ppo2.py:143][0m Total time:      11.09 min
[32m[20221213 22:29:18 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221213 22:29:18 @agent_ppo2.py:121][0m #------------------------ Iteration 518 --------------------------#
[32m[20221213 22:29:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:18 @agent_ppo2.py:185][0m |          -0.0011 |          20.0635 |           6.8847 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |           0.0007 |          18.8423 |           6.8758 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |          -0.0033 |          18.3919 |           6.8718 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |          -0.0064 |          18.1819 |           6.8652 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |          -0.0057 |          18.0540 |           6.8621 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |           0.0076 |          19.5617 |           6.8626 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |          -0.0092 |          17.8560 |           6.8609 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |          -0.0071 |          17.6708 |           6.8617 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |          -0.0071 |          17.6305 |           6.8645 |
[32m[20221213 22:29:19 @agent_ppo2.py:185][0m |          -0.0119 |          17.5558 |           6.8604 |
[32m[20221213 22:29:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.84
[32m[20221213 22:29:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.74
[32m[20221213 22:29:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.05
[32m[20221213 22:29:19 @agent_ppo2.py:143][0m Total time:      11.11 min
[32m[20221213 22:29:19 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221213 22:29:19 @agent_ppo2.py:121][0m #------------------------ Iteration 519 --------------------------#
[32m[20221213 22:29:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0012 |          24.2682 |           6.7850 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0078 |          22.4369 |           6.7663 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0103 |          21.8033 |           6.7629 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0108 |          21.3883 |           6.7578 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0079 |          21.0376 |           6.7529 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0139 |          20.8389 |           6.7522 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0141 |          20.8038 |           6.7496 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0124 |          20.6400 |           6.7474 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0093 |          20.3053 |           6.7460 |
[32m[20221213 22:29:20 @agent_ppo2.py:185][0m |          -0.0153 |          20.2189 |           6.7469 |
[32m[20221213 22:29:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.86
[32m[20221213 22:29:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 220.74
[32m[20221213 22:29:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.22
[32m[20221213 22:29:21 @agent_ppo2.py:143][0m Total time:      11.13 min
[32m[20221213 22:29:21 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221213 22:29:21 @agent_ppo2.py:121][0m #------------------------ Iteration 520 --------------------------#
[32m[20221213 22:29:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:29:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |          -0.0021 |          26.9216 |           6.7119 |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |          -0.0091 |          26.1133 |           6.7089 |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |           0.0043 |          26.7038 |           6.7003 |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |          -0.0045 |          25.3817 |           6.6992 |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |          -0.0027 |          25.5575 |           6.6949 |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |          -0.0059 |          25.0444 |           6.6953 |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |          -0.0090 |          24.8071 |           6.6954 |
[32m[20221213 22:29:21 @agent_ppo2.py:185][0m |          -0.0023 |          25.3500 |           6.6946 |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |          -0.0071 |          24.5870 |           6.6923 |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |          -0.0089 |          24.5032 |           6.6911 |
[32m[20221213 22:29:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.22
[32m[20221213 22:29:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.14
[32m[20221213 22:29:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.69
[32m[20221213 22:29:22 @agent_ppo2.py:143][0m Total time:      11.15 min
[32m[20221213 22:29:22 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221213 22:29:22 @agent_ppo2.py:121][0m #------------------------ Iteration 521 --------------------------#
[32m[20221213 22:29:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |          -0.0010 |          26.0439 |           6.7973 |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |          -0.0068 |          24.4625 |           6.7861 |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |           0.0031 |          25.1725 |           6.7842 |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |          -0.0071 |          23.5816 |           6.7810 |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |          -0.0058 |          23.3971 |           6.7834 |
[32m[20221213 22:29:22 @agent_ppo2.py:185][0m |          -0.0052 |          23.2337 |           6.7820 |
[32m[20221213 22:29:23 @agent_ppo2.py:185][0m |          -0.0088 |          22.9813 |           6.7794 |
[32m[20221213 22:29:23 @agent_ppo2.py:185][0m |          -0.0099 |          22.7633 |           6.7786 |
[32m[20221213 22:29:23 @agent_ppo2.py:185][0m |          -0.0074 |          22.8350 |           6.7794 |
[32m[20221213 22:29:23 @agent_ppo2.py:185][0m |          -0.0093 |          22.5679 |           6.7808 |
[32m[20221213 22:29:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.90
[32m[20221213 22:29:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.71
[32m[20221213 22:29:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 177.08
[32m[20221213 22:29:23 @agent_ppo2.py:143][0m Total time:      11.17 min
[32m[20221213 22:29:23 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221213 22:29:23 @agent_ppo2.py:121][0m #------------------------ Iteration 522 --------------------------#
[32m[20221213 22:29:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:23 @agent_ppo2.py:185][0m |          -0.0039 |          27.6782 |           6.6079 |
[32m[20221213 22:29:23 @agent_ppo2.py:185][0m |          -0.0001 |          26.0553 |           6.5997 |
[32m[20221213 22:29:23 @agent_ppo2.py:185][0m |          -0.0021 |          25.4279 |           6.5988 |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |          -0.0010 |          25.0020 |           6.5954 |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |           0.0163 |          29.2015 |           6.5960 |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |           0.0024 |          27.1249 |           6.5926 |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |          -0.0039 |          24.4163 |           6.5907 |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |          -0.0032 |          24.7741 |           6.5909 |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |          -0.0040 |          23.9573 |           6.5906 |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |          -0.0072 |          23.8342 |           6.5859 |
[32m[20221213 22:29:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.00
[32m[20221213 22:29:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.70
[32m[20221213 22:29:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.55
[32m[20221213 22:29:24 @agent_ppo2.py:143][0m Total time:      11.19 min
[32m[20221213 22:29:24 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221213 22:29:24 @agent_ppo2.py:121][0m #------------------------ Iteration 523 --------------------------#
[32m[20221213 22:29:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:24 @agent_ppo2.py:185][0m |          -0.0031 |           9.9685 |           6.7864 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0050 |           7.3758 |           6.7787 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0039 |           7.1178 |           6.7738 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0050 |           6.9495 |           6.7745 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0119 |           6.8952 |           6.7695 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0109 |           6.7830 |           6.7688 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0039 |           6.6769 |           6.7642 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0029 |           7.1759 |           6.7618 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0082 |           7.0887 |           6.7601 |
[32m[20221213 22:29:25 @agent_ppo2.py:185][0m |          -0.0119 |           6.5299 |           6.7632 |
[32m[20221213 22:29:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.28
[32m[20221213 22:29:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 307.71
[32m[20221213 22:29:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.92
[32m[20221213 22:29:25 @agent_ppo2.py:143][0m Total time:      11.21 min
[32m[20221213 22:29:25 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221213 22:29:25 @agent_ppo2.py:121][0m #------------------------ Iteration 524 --------------------------#
[32m[20221213 22:29:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0014 |          16.8580 |           6.6645 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0075 |          13.3500 |           6.6535 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0051 |          12.0193 |           6.6456 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0119 |          11.3449 |           6.6389 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0104 |          10.8653 |           6.6327 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0119 |          10.5987 |           6.6329 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0182 |          10.3855 |           6.6270 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0129 |          10.1504 |           6.6271 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0145 |           9.9326 |           6.6206 |
[32m[20221213 22:29:26 @agent_ppo2.py:185][0m |          -0.0157 |           9.7429 |           6.6204 |
[32m[20221213 22:29:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 112.85
[32m[20221213 22:29:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 191.20
[32m[20221213 22:29:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.14
[32m[20221213 22:29:27 @agent_ppo2.py:143][0m Total time:      11.23 min
[32m[20221213 22:29:27 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221213 22:29:27 @agent_ppo2.py:121][0m #------------------------ Iteration 525 --------------------------#
[32m[20221213 22:29:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0007 |          27.9081 |           6.6118 |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0043 |          26.1211 |           6.6011 |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0119 |          25.8733 |           6.6001 |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0042 |          25.6459 |           6.5957 |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0083 |          25.4155 |           6.6029 |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0102 |          25.2997 |           6.6002 |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0107 |          25.2340 |           6.5981 |
[32m[20221213 22:29:27 @agent_ppo2.py:185][0m |          -0.0046 |          25.2273 |           6.6050 |
[32m[20221213 22:29:28 @agent_ppo2.py:185][0m |          -0.0107 |          25.1683 |           6.6092 |
[32m[20221213 22:29:28 @agent_ppo2.py:185][0m |          -0.0114 |          25.1888 |           6.6114 |
[32m[20221213 22:29:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.87
[32m[20221213 22:29:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.70
[32m[20221213 22:29:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.86
[32m[20221213 22:29:28 @agent_ppo2.py:143][0m Total time:      11.25 min
[32m[20221213 22:29:28 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221213 22:29:28 @agent_ppo2.py:121][0m #------------------------ Iteration 526 --------------------------#
[32m[20221213 22:29:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:28 @agent_ppo2.py:185][0m |          -0.0030 |          15.8410 |           6.6336 |
[32m[20221213 22:29:28 @agent_ppo2.py:185][0m |          -0.0113 |          13.5586 |           6.6231 |
[32m[20221213 22:29:28 @agent_ppo2.py:185][0m |          -0.0071 |          12.3199 |           6.6187 |
[32m[20221213 22:29:28 @agent_ppo2.py:185][0m |          -0.0048 |          11.2141 |           6.6183 |
[32m[20221213 22:29:28 @agent_ppo2.py:185][0m |          -0.0113 |          10.1453 |           6.6177 |
[32m[20221213 22:29:29 @agent_ppo2.py:185][0m |          -0.0114 |           9.4122 |           6.6193 |
[32m[20221213 22:29:29 @agent_ppo2.py:185][0m |          -0.0112 |           8.9264 |           6.6171 |
[32m[20221213 22:29:29 @agent_ppo2.py:185][0m |          -0.0072 |           8.6233 |           6.6201 |
[32m[20221213 22:29:29 @agent_ppo2.py:185][0m |          -0.0082 |           8.5225 |           6.6182 |
[32m[20221213 22:29:29 @agent_ppo2.py:185][0m |          -0.0099 |           8.3119 |           6.6196 |
[32m[20221213 22:29:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.64
[32m[20221213 22:29:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.22
[32m[20221213 22:29:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.55
[32m[20221213 22:29:29 @agent_ppo2.py:143][0m Total time:      11.27 min
[32m[20221213 22:29:29 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221213 22:29:29 @agent_ppo2.py:121][0m #------------------------ Iteration 527 --------------------------#
[32m[20221213 22:29:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:29 @agent_ppo2.py:185][0m |           0.0111 |          16.5443 |           6.7073 |
[32m[20221213 22:29:29 @agent_ppo2.py:185][0m |          -0.0069 |          14.2012 |           6.6892 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0030 |          13.7744 |           6.6898 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0032 |          13.4393 |           6.6845 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0107 |          13.1627 |           6.6842 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0069 |          12.9521 |           6.6844 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0146 |          12.7898 |           6.6783 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0093 |          12.6261 |           6.6815 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0096 |          12.4846 |           6.6796 |
[32m[20221213 22:29:30 @agent_ppo2.py:185][0m |          -0.0097 |          12.3814 |           6.6819 |
[32m[20221213 22:29:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.77
[32m[20221213 22:29:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.47
[32m[20221213 22:29:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.49
[32m[20221213 22:29:30 @agent_ppo2.py:143][0m Total time:      11.29 min
[32m[20221213 22:29:30 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221213 22:29:30 @agent_ppo2.py:121][0m #------------------------ Iteration 528 --------------------------#
[32m[20221213 22:29:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |           0.0006 |          15.2283 |           6.6770 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0047 |          13.6984 |           6.6730 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0003 |          13.2935 |           6.6688 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0101 |          12.9135 |           6.6721 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0120 |          12.6897 |           6.6717 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0134 |          12.5225 |           6.6702 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0090 |          12.3509 |           6.6700 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0155 |          12.2690 |           6.6707 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0167 |          12.2100 |           6.6730 |
[32m[20221213 22:29:31 @agent_ppo2.py:185][0m |          -0.0169 |          12.1081 |           6.6686 |
[32m[20221213 22:29:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.73
[32m[20221213 22:29:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.97
[32m[20221213 22:29:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 191.58
[32m[20221213 22:29:31 @agent_ppo2.py:143][0m Total time:      11.31 min
[32m[20221213 22:29:31 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221213 22:29:31 @agent_ppo2.py:121][0m #------------------------ Iteration 529 --------------------------#
[32m[20221213 22:29:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |           0.0039 |          28.5106 |           6.7797 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0084 |          22.0249 |           6.7723 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0060 |          21.6140 |           6.7705 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0037 |          20.3805 |           6.7716 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0110 |          20.1905 |           6.7688 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0076 |          19.9424 |           6.7690 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0154 |          19.7160 |           6.7714 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0106 |          19.5833 |           6.7763 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0111 |          19.5635 |           6.7715 |
[32m[20221213 22:29:32 @agent_ppo2.py:185][0m |          -0.0107 |          19.4423 |           6.7771 |
[32m[20221213 22:29:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 158.59
[32m[20221213 22:29:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.33
[32m[20221213 22:29:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.43
[32m[20221213 22:29:33 @agent_ppo2.py:143][0m Total time:      11.33 min
[32m[20221213 22:29:33 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221213 22:29:33 @agent_ppo2.py:121][0m #------------------------ Iteration 530 --------------------------#
[32m[20221213 22:29:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:29:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:33 @agent_ppo2.py:185][0m |           0.0025 |          16.2173 |           6.7820 |
[32m[20221213 22:29:33 @agent_ppo2.py:185][0m |          -0.0073 |          14.1869 |           6.7696 |
[32m[20221213 22:29:33 @agent_ppo2.py:185][0m |          -0.0072 |          13.8776 |           6.7719 |
[32m[20221213 22:29:33 @agent_ppo2.py:185][0m |          -0.0098 |          13.7811 |           6.7742 |
[32m[20221213 22:29:33 @agent_ppo2.py:185][0m |          -0.0143 |          13.6330 |           6.7710 |
[32m[20221213 22:29:33 @agent_ppo2.py:185][0m |          -0.0096 |          13.4598 |           6.7737 |
[32m[20221213 22:29:33 @agent_ppo2.py:185][0m |          -0.0154 |          13.4348 |           6.7779 |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |          -0.0029 |          14.0077 |           6.7792 |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |          -0.0105 |          13.3636 |           6.7828 |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |          -0.0102 |          13.2874 |           6.7821 |
[32m[20221213 22:29:34 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:29:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.11
[32m[20221213 22:29:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 317.09
[32m[20221213 22:29:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.70
[32m[20221213 22:29:34 @agent_ppo2.py:143][0m Total time:      11.35 min
[32m[20221213 22:29:34 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221213 22:29:34 @agent_ppo2.py:121][0m #------------------------ Iteration 531 --------------------------#
[32m[20221213 22:29:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |           0.0002 |          16.4824 |           6.7978 |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |           0.0032 |          16.0263 |           6.7968 |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |          -0.0037 |          15.7241 |           6.7916 |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |          -0.0016 |          15.4591 |           6.7995 |
[32m[20221213 22:29:34 @agent_ppo2.py:185][0m |          -0.0038 |          15.3695 |           6.7969 |
[32m[20221213 22:29:35 @agent_ppo2.py:185][0m |          -0.0076 |          15.2983 |           6.8019 |
[32m[20221213 22:29:35 @agent_ppo2.py:185][0m |          -0.0105 |          15.2073 |           6.7974 |
[32m[20221213 22:29:35 @agent_ppo2.py:185][0m |          -0.0010 |          15.4073 |           6.8040 |
[32m[20221213 22:29:35 @agent_ppo2.py:185][0m |          -0.0085 |          15.2904 |           6.8019 |
[32m[20221213 22:29:35 @agent_ppo2.py:185][0m |          -0.0090 |          15.2650 |           6.8006 |
[32m[20221213 22:29:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.19
[32m[20221213 22:29:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 317.76
[32m[20221213 22:29:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.26
[32m[20221213 22:29:35 @agent_ppo2.py:143][0m Total time:      11.37 min
[32m[20221213 22:29:35 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221213 22:29:35 @agent_ppo2.py:121][0m #------------------------ Iteration 532 --------------------------#
[32m[20221213 22:29:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:35 @agent_ppo2.py:185][0m |           0.0066 |          16.8921 |           6.8675 |
[32m[20221213 22:29:35 @agent_ppo2.py:185][0m |          -0.0035 |          13.9106 |           6.8640 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0015 |          13.2586 |           6.8654 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0037 |          12.8130 |           6.8636 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0056 |          12.8827 |           6.8617 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0062 |          12.3463 |           6.8596 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0045 |          12.0903 |           6.8611 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0066 |          11.9690 |           6.8637 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0078 |          11.8018 |           6.8619 |
[32m[20221213 22:29:36 @agent_ppo2.py:185][0m |          -0.0118 |          11.6392 |           6.8620 |
[32m[20221213 22:29:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.38
[32m[20221213 22:29:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.14
[32m[20221213 22:29:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.60
[32m[20221213 22:29:36 @agent_ppo2.py:143][0m Total time:      11.39 min
[32m[20221213 22:29:36 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221213 22:29:36 @agent_ppo2.py:121][0m #------------------------ Iteration 533 --------------------------#
[32m[20221213 22:29:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0043 |          20.3576 |           7.0491 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0089 |          19.8337 |           7.0347 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0069 |          19.6031 |           7.0322 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |           0.0004 |          20.0012 |           7.0276 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0058 |          19.3724 |           7.0227 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0027 |          20.0506 |           7.0240 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0045 |          19.4497 |           7.0227 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0076 |          19.1349 |           7.0219 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0098 |          19.1053 |           7.0155 |
[32m[20221213 22:29:37 @agent_ppo2.py:185][0m |          -0.0114 |          19.1118 |           7.0164 |
[32m[20221213 22:29:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.30
[32m[20221213 22:29:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.77
[32m[20221213 22:29:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.88
[32m[20221213 22:29:38 @agent_ppo2.py:143][0m Total time:      11.41 min
[32m[20221213 22:29:38 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221213 22:29:38 @agent_ppo2.py:121][0m #------------------------ Iteration 534 --------------------------#
[32m[20221213 22:29:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0029 |          32.0131 |           6.7369 |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0033 |          29.1558 |           6.7276 |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0022 |          28.1906 |           6.7231 |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0019 |          28.0990 |           6.7185 |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0110 |          27.3071 |           6.7158 |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0087 |          26.8917 |           6.7140 |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0089 |          26.6024 |           6.7122 |
[32m[20221213 22:29:38 @agent_ppo2.py:185][0m |          -0.0048 |          26.5815 |           6.7107 |
[32m[20221213 22:29:39 @agent_ppo2.py:185][0m |          -0.0088 |          26.4189 |           6.7050 |
[32m[20221213 22:29:39 @agent_ppo2.py:185][0m |          -0.0095 |          26.4480 |           6.7073 |
[32m[20221213 22:29:39 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:29:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.07
[32m[20221213 22:29:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.58
[32m[20221213 22:29:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.33
[32m[20221213 22:29:39 @agent_ppo2.py:143][0m Total time:      11.43 min
[32m[20221213 22:29:39 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221213 22:29:39 @agent_ppo2.py:121][0m #------------------------ Iteration 535 --------------------------#
[32m[20221213 22:29:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:39 @agent_ppo2.py:185][0m |           0.0025 |          17.1071 |           6.7867 |
[32m[20221213 22:29:39 @agent_ppo2.py:185][0m |          -0.0038 |          15.7586 |           6.7886 |
[32m[20221213 22:29:39 @agent_ppo2.py:185][0m |           0.0049 |          17.0827 |           6.7870 |
[32m[20221213 22:29:39 @agent_ppo2.py:185][0m |          -0.0077 |          15.3360 |           6.7887 |
[32m[20221213 22:29:39 @agent_ppo2.py:185][0m |          -0.0049 |          15.2883 |           6.7915 |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |          -0.0084 |          15.2111 |           6.7975 |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |          -0.0014 |          15.3159 |           6.7937 |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |          -0.0091 |          15.1174 |           6.7984 |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |          -0.0105 |          15.1016 |           6.7982 |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |          -0.0060 |          15.1190 |           6.8010 |
[32m[20221213 22:29:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:29:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.62
[32m[20221213 22:29:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.96
[32m[20221213 22:29:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.64
[32m[20221213 22:29:40 @agent_ppo2.py:143][0m Total time:      11.45 min
[32m[20221213 22:29:40 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221213 22:29:40 @agent_ppo2.py:121][0m #------------------------ Iteration 536 --------------------------#
[32m[20221213 22:29:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |           0.0031 |          21.3866 |           6.7934 |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |          -0.0026 |          19.3253 |           6.7838 |
[32m[20221213 22:29:40 @agent_ppo2.py:185][0m |          -0.0045 |          18.7064 |           6.7752 |
[32m[20221213 22:29:41 @agent_ppo2.py:185][0m |          -0.0069 |          18.5299 |           6.7777 |
[32m[20221213 22:29:41 @agent_ppo2.py:185][0m |          -0.0065 |          18.2975 |           6.7798 |
[32m[20221213 22:29:41 @agent_ppo2.py:185][0m |          -0.0047 |          18.3611 |           6.7759 |
[32m[20221213 22:29:41 @agent_ppo2.py:185][0m |          -0.0074 |          18.0187 |           6.7797 |
[32m[20221213 22:29:41 @agent_ppo2.py:185][0m |          -0.0086 |          17.8998 |           6.7803 |
[32m[20221213 22:29:41 @agent_ppo2.py:185][0m |          -0.0076 |          17.8287 |           6.7812 |
[32m[20221213 22:29:41 @agent_ppo2.py:185][0m |          -0.0025 |          17.8948 |           6.7801 |
[32m[20221213 22:29:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 193.31
[32m[20221213 22:29:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.80
[32m[20221213 22:29:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.44
[32m[20221213 22:29:41 @agent_ppo2.py:143][0m Total time:      11.47 min
[32m[20221213 22:29:41 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221213 22:29:41 @agent_ppo2.py:121][0m #------------------------ Iteration 537 --------------------------#
[32m[20221213 22:29:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0008 |          15.1245 |           6.8118 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0088 |          13.9556 |           6.8019 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0047 |          13.8109 |           6.8025 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0029 |          13.6078 |           6.8012 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0132 |          13.4559 |           6.8024 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0076 |          13.3374 |           6.8074 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0062 |          13.2314 |           6.8087 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0010 |          13.4667 |           6.8107 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0119 |          13.1088 |           6.8061 |
[32m[20221213 22:29:42 @agent_ppo2.py:185][0m |          -0.0067 |          13.1158 |           6.8098 |
[32m[20221213 22:29:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:29:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.94
[32m[20221213 22:29:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.24
[32m[20221213 22:29:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.46
[32m[20221213 22:29:42 @agent_ppo2.py:143][0m Total time:      11.49 min
[32m[20221213 22:29:42 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221213 22:29:42 @agent_ppo2.py:121][0m #------------------------ Iteration 538 --------------------------#
[32m[20221213 22:29:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:29:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |           0.0014 |          17.4632 |           6.9055 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0111 |          15.4029 |           6.8990 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0147 |          14.6367 |           6.8998 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0053 |          14.1317 |           6.8949 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0009 |          13.8240 |           6.8953 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0084 |          13.6536 |           6.8959 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0002 |          15.5474 |           6.8931 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0128 |          13.3256 |           6.8900 |
[32m[20221213 22:29:43 @agent_ppo2.py:185][0m |          -0.0107 |          12.9403 |           6.8917 |
[32m[20221213 22:29:44 @agent_ppo2.py:185][0m |          -0.0129 |          12.8508 |           6.8943 |
[32m[20221213 22:29:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.97
[32m[20221213 22:29:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.20
[32m[20221213 22:29:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.52
[32m[20221213 22:29:44 @agent_ppo2.py:143][0m Total time:      11.51 min
[32m[20221213 22:29:44 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221213 22:29:44 @agent_ppo2.py:121][0m #------------------------ Iteration 539 --------------------------#
[32m[20221213 22:29:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:44 @agent_ppo2.py:185][0m |          -0.0069 |          19.7136 |           7.0536 |
[32m[20221213 22:29:44 @agent_ppo2.py:185][0m |          -0.0059 |          17.6956 |           7.0355 |
[32m[20221213 22:29:44 @agent_ppo2.py:185][0m |          -0.0064 |          17.1075 |           7.0389 |
[32m[20221213 22:29:44 @agent_ppo2.py:185][0m |          -0.0070 |          17.3213 |           7.0405 |
[32m[20221213 22:29:44 @agent_ppo2.py:185][0m |          -0.0086 |          16.8196 |           7.0357 |
[32m[20221213 22:29:44 @agent_ppo2.py:185][0m |          -0.0090 |          16.5561 |           7.0321 |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |          -0.0096 |          16.4051 |           7.0346 |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |          -0.0087 |          16.2223 |           7.0374 |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |          -0.0128 |          16.1288 |           7.0407 |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |          -0.0093 |          16.0709 |           7.0330 |
[32m[20221213 22:29:45 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:29:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.38
[32m[20221213 22:29:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.68
[32m[20221213 22:29:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.91
[32m[20221213 22:29:45 @agent_ppo2.py:143][0m Total time:      11.53 min
[32m[20221213 22:29:45 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221213 22:29:45 @agent_ppo2.py:121][0m #------------------------ Iteration 540 --------------------------#
[32m[20221213 22:29:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:29:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |           0.0015 |          16.7601 |           7.0248 |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |          -0.0010 |          15.9596 |           7.0191 |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |          -0.0047 |          15.5980 |           7.0180 |
[32m[20221213 22:29:45 @agent_ppo2.py:185][0m |          -0.0076 |          15.4650 |           7.0196 |
[32m[20221213 22:29:46 @agent_ppo2.py:185][0m |          -0.0077 |          15.1369 |           7.0178 |
[32m[20221213 22:29:46 @agent_ppo2.py:185][0m |          -0.0040 |          15.0023 |           7.0203 |
[32m[20221213 22:29:46 @agent_ppo2.py:185][0m |          -0.0066 |          14.8085 |           7.0197 |
[32m[20221213 22:29:46 @agent_ppo2.py:185][0m |           0.0029 |          15.6376 |           7.0188 |
[32m[20221213 22:29:46 @agent_ppo2.py:185][0m |          -0.0044 |          14.7796 |           7.0232 |
[32m[20221213 22:29:46 @agent_ppo2.py:185][0m |          -0.0154 |          14.6238 |           7.0186 |
[32m[20221213 22:29:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.27
[32m[20221213 22:29:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.10
[32m[20221213 22:29:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 37.05
[32m[20221213 22:29:46 @agent_ppo2.py:143][0m Total time:      11.55 min
[32m[20221213 22:29:46 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221213 22:29:46 @agent_ppo2.py:121][0m #------------------------ Iteration 541 --------------------------#
[32m[20221213 22:29:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:46 @agent_ppo2.py:185][0m |           0.0007 |          19.8339 |           7.0200 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0034 |          18.7502 |           7.0077 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0057 |          18.3671 |           7.0093 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0042 |          18.3865 |           7.0041 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0069 |          17.8779 |           6.9972 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0084 |          17.8110 |           6.9990 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0101 |          17.5114 |           6.9980 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0097 |          17.3941 |           6.9967 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0071 |          17.4098 |           6.9954 |
[32m[20221213 22:29:47 @agent_ppo2.py:185][0m |          -0.0117 |          17.1512 |           6.9930 |
[32m[20221213 22:29:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:29:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.02
[32m[20221213 22:29:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.68
[32m[20221213 22:29:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.53
[32m[20221213 22:29:47 @agent_ppo2.py:143][0m Total time:      11.57 min
[32m[20221213 22:29:47 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221213 22:29:47 @agent_ppo2.py:121][0m #------------------------ Iteration 542 --------------------------#
[32m[20221213 22:29:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |           0.0064 |          18.2441 |           6.9981 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |           0.0002 |          16.7118 |           6.9961 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0099 |          16.0620 |           6.9978 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0119 |          15.8157 |           6.9965 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0064 |          15.4666 |           6.9965 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0038 |          15.8247 |           7.0022 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0073 |          15.2629 |           6.9981 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0086 |          14.9511 |           7.0048 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0088 |          14.8950 |           7.0059 |
[32m[20221213 22:29:48 @agent_ppo2.py:185][0m |          -0.0095 |          14.8605 |           7.0131 |
[32m[20221213 22:29:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:29:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.89
[32m[20221213 22:29:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.77
[32m[20221213 22:29:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.93
[32m[20221213 22:29:49 @agent_ppo2.py:143][0m Total time:      11.60 min
[32m[20221213 22:29:49 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221213 22:29:49 @agent_ppo2.py:121][0m #------------------------ Iteration 543 --------------------------#
[32m[20221213 22:29:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:49 @agent_ppo2.py:185][0m |          -0.0022 |          14.2390 |           7.0448 |
[32m[20221213 22:29:49 @agent_ppo2.py:185][0m |          -0.0047 |          12.1488 |           7.0323 |
[32m[20221213 22:29:49 @agent_ppo2.py:185][0m |           0.0001 |          11.7181 |           7.0288 |
[32m[20221213 22:29:49 @agent_ppo2.py:185][0m |          -0.0068 |          11.4748 |           7.0316 |
[32m[20221213 22:29:49 @agent_ppo2.py:185][0m |          -0.0077 |          11.2352 |           7.0247 |
[32m[20221213 22:29:49 @agent_ppo2.py:185][0m |          -0.0081 |          11.4319 |           7.0250 |
[32m[20221213 22:29:49 @agent_ppo2.py:185][0m |          -0.0006 |          11.5958 |           7.0248 |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |          -0.0169 |          11.0206 |           7.0171 |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |          -0.0139 |          10.9361 |           7.0207 |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |          -0.0137 |          10.7052 |           7.0220 |
[32m[20221213 22:29:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.64
[32m[20221213 22:29:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.08
[32m[20221213 22:29:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.76
[32m[20221213 22:29:50 @agent_ppo2.py:143][0m Total time:      11.62 min
[32m[20221213 22:29:50 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221213 22:29:50 @agent_ppo2.py:121][0m #------------------------ Iteration 544 --------------------------#
[32m[20221213 22:29:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |           0.0015 |          16.8046 |           7.1430 |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |          -0.0046 |          15.5866 |           7.1298 |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |          -0.0045 |          15.3658 |           7.1316 |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |          -0.0118 |          15.2483 |           7.1271 |
[32m[20221213 22:29:50 @agent_ppo2.py:185][0m |          -0.0068 |          15.4218 |           7.1313 |
[32m[20221213 22:29:51 @agent_ppo2.py:185][0m |          -0.0106 |          14.9992 |           7.1255 |
[32m[20221213 22:29:51 @agent_ppo2.py:185][0m |          -0.0105 |          14.9930 |           7.1236 |
[32m[20221213 22:29:51 @agent_ppo2.py:185][0m |          -0.0021 |          15.7154 |           7.1243 |
[32m[20221213 22:29:51 @agent_ppo2.py:185][0m |          -0.0079 |          14.7917 |           7.1199 |
[32m[20221213 22:29:51 @agent_ppo2.py:185][0m |          -0.0021 |          15.0630 |           7.1215 |
[32m[20221213 22:29:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.14
[32m[20221213 22:29:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.91
[32m[20221213 22:29:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.36
[32m[20221213 22:29:51 @agent_ppo2.py:143][0m Total time:      11.64 min
[32m[20221213 22:29:51 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221213 22:29:51 @agent_ppo2.py:121][0m #------------------------ Iteration 545 --------------------------#
[32m[20221213 22:29:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:51 @agent_ppo2.py:185][0m |          -0.0011 |          18.9444 |           7.1780 |
[32m[20221213 22:29:51 @agent_ppo2.py:185][0m |          -0.0044 |          17.5646 |           7.1704 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |          -0.0090 |          17.2477 |           7.1689 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |          -0.0080 |          16.9838 |           7.1699 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |          -0.0066 |          17.0110 |           7.1708 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |          -0.0089 |          16.7890 |           7.1745 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |          -0.0102 |          16.7062 |           7.1699 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |           0.0047 |          18.0171 |           7.1739 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |          -0.0083 |          16.7228 |           7.1775 |
[32m[20221213 22:29:52 @agent_ppo2.py:185][0m |          -0.0119 |          16.5435 |           7.1763 |
[32m[20221213 22:29:52 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.82
[32m[20221213 22:29:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 229.97
[32m[20221213 22:29:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.04
[32m[20221213 22:29:52 @agent_ppo2.py:143][0m Total time:      11.66 min
[32m[20221213 22:29:52 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221213 22:29:52 @agent_ppo2.py:121][0m #------------------------ Iteration 546 --------------------------#
[32m[20221213 22:29:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0001 |          16.7568 |           7.0823 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0003 |          14.5793 |           7.0768 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0037 |          14.2352 |           7.0627 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |           0.0053 |          13.9254 |           7.0599 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0139 |          13.8795 |           7.0585 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0038 |          13.6585 |           7.0547 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0008 |          13.5655 |           7.0474 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0056 |          13.4574 |           7.0506 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0108 |          13.5649 |           7.0472 |
[32m[20221213 22:29:53 @agent_ppo2.py:185][0m |          -0.0066 |          13.3800 |           7.0447 |
[32m[20221213 22:29:53 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.10
[32m[20221213 22:29:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 190.69
[32m[20221213 22:29:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.75
[32m[20221213 22:29:53 @agent_ppo2.py:143][0m Total time:      11.68 min
[32m[20221213 22:29:53 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221213 22:29:53 @agent_ppo2.py:121][0m #------------------------ Iteration 547 --------------------------#
[32m[20221213 22:29:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0018 |          14.8500 |           7.1792 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0130 |          13.9923 |           7.1698 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |           0.0050 |          15.0095 |           7.1668 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0046 |          13.6002 |           7.1676 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0048 |          13.3247 |           7.1638 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0070 |          13.2254 |           7.1672 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0071 |          13.0950 |           7.1597 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0059 |          13.0769 |           7.1630 |
[32m[20221213 22:29:54 @agent_ppo2.py:185][0m |          -0.0105 |          13.0164 |           7.1600 |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |          -0.0080 |          12.9257 |           7.1670 |
[32m[20221213 22:29:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.28
[32m[20221213 22:29:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.85
[32m[20221213 22:29:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.09
[32m[20221213 22:29:55 @agent_ppo2.py:143][0m Total time:      11.70 min
[32m[20221213 22:29:55 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221213 22:29:55 @agent_ppo2.py:121][0m #------------------------ Iteration 548 --------------------------#
[32m[20221213 22:29:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |           0.0018 |          15.1691 |           7.2766 |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |          -0.0020 |          13.7747 |           7.2742 |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |          -0.0039 |          13.6556 |           7.2659 |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |          -0.0089 |          13.5255 |           7.2637 |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |          -0.0120 |          13.4813 |           7.2594 |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |          -0.0059 |          13.4196 |           7.2550 |
[32m[20221213 22:29:55 @agent_ppo2.py:185][0m |          -0.0067 |          13.4121 |           7.2628 |
[32m[20221213 22:29:56 @agent_ppo2.py:185][0m |          -0.0128 |          13.3380 |           7.2629 |
[32m[20221213 22:29:56 @agent_ppo2.py:185][0m |          -0.0048 |          13.3403 |           7.2594 |
[32m[20221213 22:29:56 @agent_ppo2.py:185][0m |          -0.0090 |          13.2238 |           7.2591 |
[32m[20221213 22:29:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.74
[32m[20221213 22:29:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 318.92
[32m[20221213 22:29:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.52
[32m[20221213 22:29:56 @agent_ppo2.py:143][0m Total time:      11.72 min
[32m[20221213 22:29:56 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221213 22:29:56 @agent_ppo2.py:121][0m #------------------------ Iteration 549 --------------------------#
[32m[20221213 22:29:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:56 @agent_ppo2.py:185][0m |          -0.0052 |          13.4781 |           7.1604 |
[32m[20221213 22:29:56 @agent_ppo2.py:185][0m |           0.0004 |          12.6566 |           7.1484 |
[32m[20221213 22:29:56 @agent_ppo2.py:185][0m |          -0.0052 |          12.0999 |           7.1404 |
[32m[20221213 22:29:56 @agent_ppo2.py:185][0m |          -0.0094 |          11.9539 |           7.1402 |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |          -0.0088 |          11.8251 |           7.1388 |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |          -0.0022 |          12.3070 |           7.1366 |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |          -0.0011 |          11.7498 |           7.1333 |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |          -0.0104 |          11.5260 |           7.1329 |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |          -0.0108 |          11.4368 |           7.1279 |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |          -0.0025 |          11.8108 |           7.1242 |
[32m[20221213 22:29:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:29:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.74
[32m[20221213 22:29:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.59
[32m[20221213 22:29:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.29
[32m[20221213 22:29:57 @agent_ppo2.py:143][0m Total time:      11.74 min
[32m[20221213 22:29:57 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221213 22:29:57 @agent_ppo2.py:121][0m #------------------------ Iteration 550 --------------------------#
[32m[20221213 22:29:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:29:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |           0.0102 |          28.6422 |           7.1325 |
[32m[20221213 22:29:57 @agent_ppo2.py:185][0m |          -0.0048 |          25.6933 |           7.1259 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0059 |          25.1019 |           7.1251 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0026 |          25.0499 |           7.1199 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0121 |          24.7777 |           7.1211 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0075 |          24.6742 |           7.1210 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0080 |          24.5893 |           7.1269 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0078 |          24.2482 |           7.1241 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0128 |          24.2097 |           7.1215 |
[32m[20221213 22:29:58 @agent_ppo2.py:185][0m |          -0.0118 |          24.0972 |           7.1174 |
[32m[20221213 22:29:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:29:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.48
[32m[20221213 22:29:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 226.34
[32m[20221213 22:29:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.44
[32m[20221213 22:29:58 @agent_ppo2.py:143][0m Total time:      11.76 min
[32m[20221213 22:29:58 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221213 22:29:58 @agent_ppo2.py:121][0m #------------------------ Iteration 551 --------------------------#
[32m[20221213 22:29:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:29:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0028 |          23.0702 |           7.1459 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0010 |          22.4092 |           7.1332 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0077 |          21.7956 |           7.1350 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |           0.0016 |          22.6729 |           7.1303 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0131 |          21.7355 |           7.1227 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0122 |          21.4314 |           7.1299 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0066 |          21.2848 |           7.1254 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0095 |          21.1074 |           7.1188 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0114 |          20.9381 |           7.1238 |
[32m[20221213 22:29:59 @agent_ppo2.py:185][0m |          -0.0097 |          20.9660 |           7.1183 |
[32m[20221213 22:29:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:29:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.91
[32m[20221213 22:29:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.39
[32m[20221213 22:29:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:29:59 @agent_ppo2.py:143][0m Total time:      11.78 min
[32m[20221213 22:29:59 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221213 22:30:00 @agent_ppo2.py:121][0m #------------------------ Iteration 552 --------------------------#
[32m[20221213 22:30:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0038 |          15.7719 |           7.0857 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0022 |          13.7235 |           7.0788 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0054 |          13.5581 |           7.0702 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0015 |          12.8696 |           7.0663 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0069 |          12.5079 |           7.0646 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0138 |          12.3265 |           7.0639 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0073 |          12.1420 |           7.0632 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0065 |          11.9816 |           7.0614 |
[32m[20221213 22:30:00 @agent_ppo2.py:185][0m |          -0.0068 |          11.8352 |           7.0595 |
[32m[20221213 22:30:01 @agent_ppo2.py:185][0m |          -0.0009 |          11.8204 |           7.0584 |
[32m[20221213 22:30:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:30:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.38
[32m[20221213 22:30:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.20
[32m[20221213 22:30:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.49
[32m[20221213 22:30:01 @agent_ppo2.py:143][0m Total time:      11.80 min
[32m[20221213 22:30:01 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221213 22:30:01 @agent_ppo2.py:121][0m #------------------------ Iteration 553 --------------------------#
[32m[20221213 22:30:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:01 @agent_ppo2.py:185][0m |           0.0060 |          18.9641 |           7.2005 |
[32m[20221213 22:30:01 @agent_ppo2.py:185][0m |          -0.0063 |          17.8180 |           7.1948 |
[32m[20221213 22:30:01 @agent_ppo2.py:185][0m |          -0.0047 |          17.6012 |           7.1819 |
[32m[20221213 22:30:01 @agent_ppo2.py:185][0m |          -0.0073 |          17.4774 |           7.1852 |
[32m[20221213 22:30:01 @agent_ppo2.py:185][0m |          -0.0104 |          17.4552 |           7.1761 |
[32m[20221213 22:30:01 @agent_ppo2.py:185][0m |          -0.0087 |          17.1928 |           7.1778 |
[32m[20221213 22:30:02 @agent_ppo2.py:185][0m |          -0.0103 |          17.1331 |           7.1790 |
[32m[20221213 22:30:02 @agent_ppo2.py:185][0m |          -0.0105 |          17.0691 |           7.1738 |
[32m[20221213 22:30:02 @agent_ppo2.py:185][0m |          -0.0133 |          16.9200 |           7.1751 |
[32m[20221213 22:30:02 @agent_ppo2.py:185][0m |          -0.0114 |          16.9146 |           7.1695 |
[32m[20221213 22:30:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:30:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.95
[32m[20221213 22:30:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.03
[32m[20221213 22:30:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.27
[32m[20221213 22:30:02 @agent_ppo2.py:143][0m Total time:      11.82 min
[32m[20221213 22:30:02 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221213 22:30:02 @agent_ppo2.py:121][0m #------------------------ Iteration 554 --------------------------#
[32m[20221213 22:30:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:02 @agent_ppo2.py:185][0m |          -0.0011 |          18.6643 |           7.1589 |
[32m[20221213 22:30:02 @agent_ppo2.py:185][0m |          -0.0015 |          16.9459 |           7.1413 |
[32m[20221213 22:30:02 @agent_ppo2.py:185][0m |          -0.0079 |          16.4134 |           7.1366 |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |          -0.0086 |          16.4321 |           7.1365 |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |          -0.0089 |          16.1724 |           7.1338 |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |          -0.0086 |          16.1167 |           7.1317 |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |          -0.0044 |          16.8646 |           7.1328 |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |          -0.0119 |          15.7505 |           7.1290 |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |          -0.0124 |          15.7181 |           7.1354 |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |          -0.0107 |          15.7953 |           7.1358 |
[32m[20221213 22:30:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:30:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.58
[32m[20221213 22:30:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.28
[32m[20221213 22:30:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.70
[32m[20221213 22:30:03 @agent_ppo2.py:143][0m Total time:      11.84 min
[32m[20221213 22:30:03 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221213 22:30:03 @agent_ppo2.py:121][0m #------------------------ Iteration 555 --------------------------#
[32m[20221213 22:30:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:03 @agent_ppo2.py:185][0m |           0.0016 |          21.0713 |           7.1259 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0069 |          19.7770 |           7.1259 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0056 |          19.3590 |           7.1210 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0063 |          19.0446 |           7.1171 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0074 |          19.1357 |           7.1209 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0123 |          18.7764 |           7.1184 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0115 |          18.5383 |           7.1152 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0088 |          18.5079 |           7.1166 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0141 |          18.3974 |           7.1175 |
[32m[20221213 22:30:04 @agent_ppo2.py:185][0m |          -0.0133 |          18.3923 |           7.1129 |
[32m[20221213 22:30:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:30:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.67
[32m[20221213 22:30:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.48
[32m[20221213 22:30:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.74
[32m[20221213 22:30:04 @agent_ppo2.py:143][0m Total time:      11.86 min
[32m[20221213 22:30:04 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221213 22:30:04 @agent_ppo2.py:121][0m #------------------------ Iteration 556 --------------------------#
[32m[20221213 22:30:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |           0.0063 |          28.9831 |           7.2302 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0034 |          20.8724 |           7.2194 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0069 |          20.1167 |           7.2104 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0056 |          19.6715 |           7.2077 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0068 |          19.4134 |           7.2073 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0019 |          19.1770 |           7.2073 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0091 |          19.0458 |           7.2090 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0050 |          18.7779 |           7.2039 |
[32m[20221213 22:30:05 @agent_ppo2.py:185][0m |          -0.0082 |          18.7565 |           7.2014 |
[32m[20221213 22:30:06 @agent_ppo2.py:185][0m |          -0.0093 |          18.6524 |           7.2032 |
[32m[20221213 22:30:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:30:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.56
[32m[20221213 22:30:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.97
[32m[20221213 22:30:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.28
[32m[20221213 22:30:06 @agent_ppo2.py:143][0m Total time:      11.88 min
[32m[20221213 22:30:06 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221213 22:30:06 @agent_ppo2.py:121][0m #------------------------ Iteration 557 --------------------------#
[32m[20221213 22:30:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:06 @agent_ppo2.py:185][0m |          -0.0049 |          23.7819 |           7.2140 |
[32m[20221213 22:30:06 @agent_ppo2.py:185][0m |          -0.0026 |          21.6086 |           7.2087 |
[32m[20221213 22:30:06 @agent_ppo2.py:185][0m |           0.0009 |          21.4296 |           7.2115 |
[32m[20221213 22:30:06 @agent_ppo2.py:185][0m |          -0.0042 |          20.6029 |           7.2082 |
[32m[20221213 22:30:06 @agent_ppo2.py:185][0m |          -0.0054 |          20.0246 |           7.2136 |
[32m[20221213 22:30:06 @agent_ppo2.py:185][0m |          -0.0091 |          19.9163 |           7.2093 |
[32m[20221213 22:30:07 @agent_ppo2.py:185][0m |          -0.0134 |          19.5353 |           7.2151 |
[32m[20221213 22:30:07 @agent_ppo2.py:185][0m |          -0.0086 |          19.5027 |           7.2131 |
[32m[20221213 22:30:07 @agent_ppo2.py:185][0m |          -0.0109 |          19.1942 |           7.2158 |
[32m[20221213 22:30:07 @agent_ppo2.py:185][0m |          -0.0111 |          18.9878 |           7.2136 |
[32m[20221213 22:30:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:30:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 154.64
[32m[20221213 22:30:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.76
[32m[20221213 22:30:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:30:07 @agent_ppo2.py:143][0m Total time:      11.90 min
[32m[20221213 22:30:07 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221213 22:30:07 @agent_ppo2.py:121][0m #------------------------ Iteration 558 --------------------------#
[32m[20221213 22:30:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:07 @agent_ppo2.py:185][0m |          -0.0040 |          24.3044 |           7.3770 |
[32m[20221213 22:30:07 @agent_ppo2.py:185][0m |           0.0021 |          24.1328 |           7.3670 |
[32m[20221213 22:30:07 @agent_ppo2.py:185][0m |           0.0025 |          24.2085 |           7.3552 |
[32m[20221213 22:30:08 @agent_ppo2.py:185][0m |          -0.0008 |          22.8776 |           7.3585 |
[32m[20221213 22:30:08 @agent_ppo2.py:185][0m |          -0.0117 |          22.2889 |           7.3615 |
[32m[20221213 22:30:08 @agent_ppo2.py:185][0m |          -0.0082 |          21.9167 |           7.3599 |
[32m[20221213 22:30:08 @agent_ppo2.py:185][0m |          -0.0099 |          21.7272 |           7.3691 |
[32m[20221213 22:30:08 @agent_ppo2.py:185][0m |          -0.0069 |          21.7178 |           7.3663 |
[32m[20221213 22:30:08 @agent_ppo2.py:185][0m |          -0.0099 |          21.5704 |           7.3660 |
[32m[20221213 22:30:08 @agent_ppo2.py:185][0m |          -0.0111 |          21.5967 |           7.3732 |
[32m[20221213 22:30:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:30:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.58
[32m[20221213 22:30:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 307.90
[32m[20221213 22:30:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.35
[32m[20221213 22:30:08 @agent_ppo2.py:143][0m Total time:      11.92 min
[32m[20221213 22:30:08 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221213 22:30:08 @agent_ppo2.py:121][0m #------------------------ Iteration 559 --------------------------#
[32m[20221213 22:30:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:30:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0001 |          16.0868 |           7.2007 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0038 |          13.8831 |           7.1997 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0015 |          13.5230 |           7.1988 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0032 |          13.3813 |           7.1985 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0017 |          13.2581 |           7.2019 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0024 |          13.6918 |           7.2010 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0125 |          13.0666 |           7.1973 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0128 |          12.9539 |           7.1985 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0075 |          12.9091 |           7.2011 |
[32m[20221213 22:30:09 @agent_ppo2.py:185][0m |          -0.0129 |          12.8842 |           7.2044 |
[32m[20221213 22:30:09 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:30:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.06
[32m[20221213 22:30:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.45
[32m[20221213 22:30:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.82
[32m[20221213 22:30:10 @agent_ppo2.py:143][0m Total time:      11.95 min
[32m[20221213 22:30:10 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221213 22:30:10 @agent_ppo2.py:121][0m #------------------------ Iteration 560 --------------------------#
[32m[20221213 22:30:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:30:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:10 @agent_ppo2.py:185][0m |          -0.0017 |          24.1592 |           7.3263 |
[32m[20221213 22:30:10 @agent_ppo2.py:185][0m |           0.0051 |          23.2937 |           7.3161 |
[32m[20221213 22:30:10 @agent_ppo2.py:185][0m |          -0.0078 |          21.2266 |           7.3096 |
[32m[20221213 22:30:10 @agent_ppo2.py:185][0m |          -0.0068 |          20.7442 |           7.3112 |
[32m[20221213 22:30:10 @agent_ppo2.py:185][0m |          -0.0042 |          20.5122 |           7.3072 |
[32m[20221213 22:30:10 @agent_ppo2.py:185][0m |          -0.0082 |          20.2318 |           7.3068 |
[32m[20221213 22:30:10 @agent_ppo2.py:185][0m |           0.0009 |          20.6263 |           7.3114 |
[32m[20221213 22:30:11 @agent_ppo2.py:185][0m |          -0.0124 |          19.9535 |           7.3081 |
[32m[20221213 22:30:11 @agent_ppo2.py:185][0m |          -0.0082 |          19.7972 |           7.3081 |
[32m[20221213 22:30:11 @agent_ppo2.py:185][0m |          -0.0165 |          19.7715 |           7.3097 |
[32m[20221213 22:30:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:30:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.65
[32m[20221213 22:30:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.73
[32m[20221213 22:30:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.76
[32m[20221213 22:30:11 @agent_ppo2.py:143][0m Total time:      11.97 min
[32m[20221213 22:30:11 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221213 22:30:11 @agent_ppo2.py:121][0m #------------------------ Iteration 561 --------------------------#
[32m[20221213 22:30:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:11 @agent_ppo2.py:185][0m |          -0.0020 |          15.6627 |           7.3729 |
[32m[20221213 22:30:11 @agent_ppo2.py:185][0m |           0.0018 |          14.3659 |           7.3583 |
[32m[20221213 22:30:11 @agent_ppo2.py:185][0m |          -0.0060 |          13.7810 |           7.3506 |
[32m[20221213 22:30:11 @agent_ppo2.py:185][0m |          -0.0101 |          13.6317 |           7.3480 |
[32m[20221213 22:30:12 @agent_ppo2.py:185][0m |          -0.0058 |          13.4672 |           7.3515 |
[32m[20221213 22:30:12 @agent_ppo2.py:185][0m |          -0.0190 |          13.3956 |           7.3521 |
[32m[20221213 22:30:12 @agent_ppo2.py:185][0m |          -0.0155 |          13.3761 |           7.3556 |
[32m[20221213 22:30:12 @agent_ppo2.py:185][0m |          -0.0166 |          13.2746 |           7.3551 |
[32m[20221213 22:30:12 @agent_ppo2.py:185][0m |          -0.0079 |          13.2133 |           7.3515 |
[32m[20221213 22:30:12 @agent_ppo2.py:185][0m |          -0.0167 |          13.0615 |           7.3579 |
[32m[20221213 22:30:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:30:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.17
[32m[20221213 22:30:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.06
[32m[20221213 22:30:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:30:12 @agent_ppo2.py:143][0m Total time:      11.99 min
[32m[20221213 22:30:12 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221213 22:30:12 @agent_ppo2.py:121][0m #------------------------ Iteration 562 --------------------------#
[32m[20221213 22:30:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:12 @agent_ppo2.py:185][0m |          -0.0019 |          22.1847 |           7.3920 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0062 |          20.8259 |           7.3834 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0071 |          20.5160 |           7.3817 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0093 |          20.2336 |           7.3797 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0119 |          19.9093 |           7.3792 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0092 |          19.7929 |           7.3823 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0094 |          19.6133 |           7.3834 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0083 |          19.5699 |           7.3857 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0131 |          19.5329 |           7.3874 |
[32m[20221213 22:30:13 @agent_ppo2.py:185][0m |          -0.0090 |          19.5229 |           7.3850 |
[32m[20221213 22:30:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:30:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 217.27
[32m[20221213 22:30:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.43
[32m[20221213 22:30:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 7.74
[32m[20221213 22:30:13 @agent_ppo2.py:143][0m Total time:      12.01 min
[32m[20221213 22:30:13 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221213 22:30:13 @agent_ppo2.py:121][0m #------------------------ Iteration 563 --------------------------#
[32m[20221213 22:30:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0032 |          14.6794 |           7.3870 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0040 |          13.7032 |           7.3689 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0068 |          13.5585 |           7.3743 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0086 |          13.4391 |           7.3645 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0062 |          13.3600 |           7.3638 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0100 |          13.3745 |           7.3623 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0055 |          13.9458 |           7.3580 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0113 |          13.2199 |           7.3625 |
[32m[20221213 22:30:14 @agent_ppo2.py:185][0m |          -0.0127 |          13.3241 |           7.3627 |
[32m[20221213 22:30:15 @agent_ppo2.py:185][0m |          -0.0073 |          13.1602 |           7.3596 |
[32m[20221213 22:30:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:30:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.23
[32m[20221213 22:30:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.43
[32m[20221213 22:30:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.66
[32m[20221213 22:30:15 @agent_ppo2.py:143][0m Total time:      12.03 min
[32m[20221213 22:30:15 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221213 22:30:15 @agent_ppo2.py:121][0m #------------------------ Iteration 564 --------------------------#
[32m[20221213 22:30:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:15 @agent_ppo2.py:185][0m |           0.0001 |          25.3027 |           7.5286 |
[32m[20221213 22:30:15 @agent_ppo2.py:185][0m |          -0.0061 |          24.1534 |           7.5201 |
[32m[20221213 22:30:15 @agent_ppo2.py:185][0m |          -0.0061 |          23.7788 |           7.5140 |
[32m[20221213 22:30:15 @agent_ppo2.py:185][0m |          -0.0084 |          23.5417 |           7.5098 |
[32m[20221213 22:30:15 @agent_ppo2.py:185][0m |          -0.0080 |          23.1325 |           7.5049 |
[32m[20221213 22:30:15 @agent_ppo2.py:185][0m |          -0.0052 |          23.0746 |           7.5121 |
[32m[20221213 22:30:16 @agent_ppo2.py:185][0m |          -0.0113 |          22.8501 |           7.5035 |
[32m[20221213 22:30:16 @agent_ppo2.py:185][0m |          -0.0083 |          23.3254 |           7.5062 |
[32m[20221213 22:30:16 @agent_ppo2.py:185][0m |          -0.0104 |          22.5454 |           7.5100 |
[32m[20221213 22:30:16 @agent_ppo2.py:185][0m |          -0.0130 |          22.6970 |           7.5123 |
[32m[20221213 22:30:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:30:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.19
[32m[20221213 22:30:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.10
[32m[20221213 22:30:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.92
[32m[20221213 22:30:16 @agent_ppo2.py:143][0m Total time:      12.05 min
[32m[20221213 22:30:16 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221213 22:30:16 @agent_ppo2.py:121][0m #------------------------ Iteration 565 --------------------------#
[32m[20221213 22:30:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:16 @agent_ppo2.py:185][0m |          -0.0043 |          23.6307 |           7.6046 |
[32m[20221213 22:30:16 @agent_ppo2.py:185][0m |          -0.0078 |          21.0998 |           7.5935 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |          -0.0047 |          20.8301 |           7.5970 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |          -0.0095 |          20.3910 |           7.5983 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |           0.0046 |          21.6489 |           7.5939 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |           0.0000 |          20.9409 |           7.5981 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |          -0.0109 |          19.7924 |           7.5983 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |          -0.0039 |          19.7632 |           7.6016 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |          -0.0120 |          19.5672 |           7.5995 |
[32m[20221213 22:30:17 @agent_ppo2.py:185][0m |          -0.0075 |          19.4238 |           7.5985 |
[32m[20221213 22:30:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:30:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.80
[32m[20221213 22:30:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 358.37
[32m[20221213 22:30:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.99
[32m[20221213 22:30:17 @agent_ppo2.py:143][0m Total time:      12.07 min
[32m[20221213 22:30:17 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221213 22:30:17 @agent_ppo2.py:121][0m #------------------------ Iteration 566 --------------------------#
[32m[20221213 22:30:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |           0.0065 |          22.3723 |           7.5524 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |           0.0074 |          21.0556 |           7.5546 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0026 |          19.1203 |           7.5461 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0079 |          18.6633 |           7.5494 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0067 |          18.7875 |           7.5521 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0070 |          18.4039 |           7.5496 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0118 |          18.2755 |           7.5542 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0091 |          18.1734 |           7.5551 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0058 |          18.0663 |           7.5549 |
[32m[20221213 22:30:18 @agent_ppo2.py:185][0m |          -0.0042 |          18.1281 |           7.5536 |
[32m[20221213 22:30:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:30:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.72
[32m[20221213 22:30:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.98
[32m[20221213 22:30:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:30:19 @agent_ppo2.py:143][0m Total time:      12.09 min
[32m[20221213 22:30:19 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221213 22:30:19 @agent_ppo2.py:121][0m #------------------------ Iteration 567 --------------------------#
[32m[20221213 22:30:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |           0.0011 |          24.6044 |           7.5541 |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |           0.0016 |          22.2814 |           7.5531 |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |          -0.0057 |          21.5831 |           7.5525 |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |          -0.0008 |          21.0565 |           7.5555 |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |          -0.0040 |          20.6887 |           7.5550 |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |          -0.0041 |          20.4809 |           7.5550 |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |          -0.0051 |          20.7543 |           7.5572 |
[32m[20221213 22:30:19 @agent_ppo2.py:185][0m |          -0.0016 |          21.4275 |           7.5643 |
[32m[20221213 22:30:20 @agent_ppo2.py:185][0m |          -0.0049 |          20.0145 |           7.5563 |
[32m[20221213 22:30:20 @agent_ppo2.py:185][0m |          -0.0038 |          19.8964 |           7.5591 |
[32m[20221213 22:30:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:30:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.59
[32m[20221213 22:30:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.74
[32m[20221213 22:30:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.73
[32m[20221213 22:30:20 @agent_ppo2.py:143][0m Total time:      12.11 min
[32m[20221213 22:30:20 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221213 22:30:20 @agent_ppo2.py:121][0m #------------------------ Iteration 568 --------------------------#
[32m[20221213 22:30:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:20 @agent_ppo2.py:185][0m |          -0.0015 |          28.3377 |           7.6235 |
[32m[20221213 22:30:20 @agent_ppo2.py:185][0m |          -0.0059 |          26.6605 |           7.6114 |
[32m[20221213 22:30:20 @agent_ppo2.py:185][0m |          -0.0071 |          26.3332 |           7.6107 |
[32m[20221213 22:30:20 @agent_ppo2.py:185][0m |          -0.0112 |          25.8795 |           7.6131 |
[32m[20221213 22:30:20 @agent_ppo2.py:185][0m |           0.0055 |          28.0655 |           7.6054 |
[32m[20221213 22:30:21 @agent_ppo2.py:185][0m |          -0.0010 |          26.9205 |           7.6075 |
[32m[20221213 22:30:21 @agent_ppo2.py:185][0m |          -0.0110 |          25.3137 |           7.6188 |
[32m[20221213 22:30:21 @agent_ppo2.py:185][0m |          -0.0100 |          25.2228 |           7.6139 |
[32m[20221213 22:30:21 @agent_ppo2.py:185][0m |          -0.0077 |          25.0690 |           7.6006 |
[32m[20221213 22:30:21 @agent_ppo2.py:185][0m |          -0.0146 |          24.9091 |           7.6139 |
[32m[20221213 22:30:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:30:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.37
[32m[20221213 22:30:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.57
[32m[20221213 22:30:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.75
[32m[20221213 22:30:21 @agent_ppo2.py:143][0m Total time:      12.14 min
[32m[20221213 22:30:21 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221213 22:30:21 @agent_ppo2.py:121][0m #------------------------ Iteration 569 --------------------------#
[32m[20221213 22:30:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:21 @agent_ppo2.py:185][0m |           0.0092 |          19.2105 |           7.7379 |
[32m[20221213 22:30:21 @agent_ppo2.py:185][0m |           0.0032 |          17.2109 |           7.7217 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0117 |          15.8340 |           7.7060 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0069 |          15.5688 |           7.6995 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0026 |          16.0534 |           7.6925 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0120 |          15.0755 |           7.6833 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0047 |          15.1710 |           7.6774 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0154 |          14.8230 |           7.6699 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0131 |          14.6057 |           7.6627 |
[32m[20221213 22:30:22 @agent_ppo2.py:185][0m |          -0.0154 |          14.5332 |           7.6582 |
[32m[20221213 22:30:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:30:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.09
[32m[20221213 22:30:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.13
[32m[20221213 22:30:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.26
[32m[20221213 22:30:22 @agent_ppo2.py:143][0m Total time:      12.16 min
[32m[20221213 22:30:22 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221213 22:30:22 @agent_ppo2.py:121][0m #------------------------ Iteration 570 --------------------------#
[32m[20221213 22:30:22 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:30:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0015 |          23.4794 |           7.4334 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0003 |          22.0290 |           7.4274 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |           0.0020 |          22.2866 |           7.4255 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0029 |          21.1436 |           7.4337 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0111 |          20.8385 |           7.4302 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0085 |          20.4596 |           7.4261 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0097 |          20.2187 |           7.4314 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0107 |          20.0269 |           7.4287 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0013 |          20.2843 |           7.4318 |
[32m[20221213 22:30:23 @agent_ppo2.py:185][0m |          -0.0154 |          19.7882 |           7.4426 |
[32m[20221213 22:30:23 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:30:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.56
[32m[20221213 22:30:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.51
[32m[20221213 22:30:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.77
[32m[20221213 22:30:24 @agent_ppo2.py:143][0m Total time:      12.18 min
[32m[20221213 22:30:24 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221213 22:30:24 @agent_ppo2.py:121][0m #------------------------ Iteration 571 --------------------------#
[32m[20221213 22:30:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:24 @agent_ppo2.py:185][0m |          -0.0022 |          20.2748 |           7.4319 |
[32m[20221213 22:30:24 @agent_ppo2.py:185][0m |          -0.0029 |          18.4008 |           7.4297 |
[32m[20221213 22:30:24 @agent_ppo2.py:185][0m |          -0.0072 |          17.6778 |           7.4279 |
[32m[20221213 22:30:24 @agent_ppo2.py:185][0m |          -0.0103 |          17.1282 |           7.4318 |
[32m[20221213 22:30:24 @agent_ppo2.py:185][0m |          -0.0096 |          16.8757 |           7.4349 |
[32m[20221213 22:30:24 @agent_ppo2.py:185][0m |          -0.0107 |          16.6198 |           7.4349 |
[32m[20221213 22:30:24 @agent_ppo2.py:185][0m |          -0.0132 |          16.4392 |           7.4339 |
[32m[20221213 22:30:25 @agent_ppo2.py:185][0m |          -0.0116 |          16.2230 |           7.4361 |
[32m[20221213 22:30:25 @agent_ppo2.py:185][0m |          -0.0117 |          15.9693 |           7.4328 |
[32m[20221213 22:30:25 @agent_ppo2.py:185][0m |          -0.0138 |          15.9492 |           7.4340 |
[32m[20221213 22:30:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:30:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.42
[32m[20221213 22:30:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.05
[32m[20221213 22:30:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.42
[32m[20221213 22:30:25 @agent_ppo2.py:143][0m Total time:      12.20 min
[32m[20221213 22:30:25 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221213 22:30:25 @agent_ppo2.py:121][0m #------------------------ Iteration 572 --------------------------#
[32m[20221213 22:30:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:25 @agent_ppo2.py:185][0m |           0.0004 |          26.7833 |           7.5089 |
[32m[20221213 22:30:25 @agent_ppo2.py:185][0m |          -0.0036 |          25.2496 |           7.4985 |
[32m[20221213 22:30:25 @agent_ppo2.py:185][0m |          -0.0051 |          24.5758 |           7.4968 |
[32m[20221213 22:30:25 @agent_ppo2.py:185][0m |          -0.0046 |          24.5046 |           7.4898 |
[32m[20221213 22:30:26 @agent_ppo2.py:185][0m |          -0.0073 |          23.9729 |           7.4921 |
[32m[20221213 22:30:26 @agent_ppo2.py:185][0m |          -0.0114 |          23.7891 |           7.4908 |
[32m[20221213 22:30:26 @agent_ppo2.py:185][0m |          -0.0078 |          23.6546 |           7.4849 |
[32m[20221213 22:30:26 @agent_ppo2.py:185][0m |          -0.0096 |          23.8398 |           7.4836 |
[32m[20221213 22:30:26 @agent_ppo2.py:185][0m |          -0.0022 |          23.6723 |           7.4840 |
[32m[20221213 22:30:26 @agent_ppo2.py:185][0m |          -0.0090 |          23.4640 |           7.4825 |
[32m[20221213 22:30:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:30:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.53
[32m[20221213 22:30:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.77
[32m[20221213 22:30:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:30:26 @agent_ppo2.py:143][0m Total time:      12.22 min
[32m[20221213 22:30:26 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221213 22:30:26 @agent_ppo2.py:121][0m #------------------------ Iteration 573 --------------------------#
[32m[20221213 22:30:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:26 @agent_ppo2.py:185][0m |          -0.0003 |          17.7456 |           7.6145 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0061 |          15.4821 |           7.6106 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0079 |          15.1585 |           7.6070 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0114 |          15.0716 |           7.6059 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0076 |          15.0599 |           7.6060 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0105 |          14.9277 |           7.6091 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0094 |          14.8525 |           7.6111 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0090 |          14.7749 |           7.6161 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0093 |          14.6611 |           7.6098 |
[32m[20221213 22:30:27 @agent_ppo2.py:185][0m |          -0.0090 |          14.9811 |           7.6137 |
[32m[20221213 22:30:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:30:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.09
[32m[20221213 22:30:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.19
[32m[20221213 22:30:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.97
[32m[20221213 22:30:27 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 366.97
[32m[20221213 22:30:27 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 366.97
[32m[20221213 22:30:27 @agent_ppo2.py:143][0m Total time:      12.24 min
[32m[20221213 22:30:27 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221213 22:30:27 @agent_ppo2.py:121][0m #------------------------ Iteration 574 --------------------------#
[32m[20221213 22:30:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |           0.0031 |          20.8583 |           7.5893 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0045 |          18.0791 |           7.5874 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0104 |          17.3605 |           7.5808 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0116 |          17.1024 |           7.5787 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0170 |          16.7397 |           7.5797 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0121 |          16.4700 |           7.5776 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0127 |          16.3108 |           7.5782 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0140 |          16.1153 |           7.5782 |
[32m[20221213 22:30:28 @agent_ppo2.py:185][0m |          -0.0132 |          15.9830 |           7.5799 |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |          -0.0141 |          15.8367 |           7.5762 |
[32m[20221213 22:30:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:30:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.69
[32m[20221213 22:30:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.14
[32m[20221213 22:30:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.87
[32m[20221213 22:30:29 @agent_ppo2.py:143][0m Total time:      12.26 min
[32m[20221213 22:30:29 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221213 22:30:29 @agent_ppo2.py:121][0m #------------------------ Iteration 575 --------------------------#
[32m[20221213 22:30:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |          -0.0007 |          20.3872 |           7.5625 |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |          -0.0085 |          16.5555 |           7.5499 |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |           0.0020 |          17.1893 |           7.5592 |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |          -0.0101 |          15.3355 |           7.5590 |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |          -0.0052 |          14.5178 |           7.5519 |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |          -0.0031 |          14.5157 |           7.5572 |
[32m[20221213 22:30:29 @agent_ppo2.py:185][0m |          -0.0111 |          14.1251 |           7.5614 |
[32m[20221213 22:30:30 @agent_ppo2.py:185][0m |          -0.0141 |          13.9590 |           7.5630 |
[32m[20221213 22:30:30 @agent_ppo2.py:185][0m |          -0.0144 |          13.8154 |           7.5637 |
[32m[20221213 22:30:30 @agent_ppo2.py:185][0m |          -0.0127 |          13.7428 |           7.5683 |
[32m[20221213 22:30:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:30:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.75
[32m[20221213 22:30:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.31
[32m[20221213 22:30:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.79
[32m[20221213 22:30:30 @agent_ppo2.py:143][0m Total time:      12.28 min
[32m[20221213 22:30:30 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221213 22:30:30 @agent_ppo2.py:121][0m #------------------------ Iteration 576 --------------------------#
[32m[20221213 22:30:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:30 @agent_ppo2.py:185][0m |           0.0015 |          21.7837 |           7.7744 |
[32m[20221213 22:30:30 @agent_ppo2.py:185][0m |           0.0060 |          21.3031 |           7.7693 |
[32m[20221213 22:30:30 @agent_ppo2.py:185][0m |          -0.0061 |          19.0591 |           7.7587 |
[32m[20221213 22:30:30 @agent_ppo2.py:185][0m |          -0.0058 |          18.6132 |           7.7624 |
[32m[20221213 22:30:31 @agent_ppo2.py:185][0m |          -0.0060 |          18.3269 |           7.7623 |
[32m[20221213 22:30:31 @agent_ppo2.py:185][0m |          -0.0085 |          18.0609 |           7.7605 |
[32m[20221213 22:30:31 @agent_ppo2.py:185][0m |          -0.0127 |          18.0167 |           7.7548 |
[32m[20221213 22:30:31 @agent_ppo2.py:185][0m |          -0.0049 |          18.3444 |           7.7589 |
[32m[20221213 22:30:31 @agent_ppo2.py:185][0m |          -0.0037 |          17.7192 |           7.7592 |
[32m[20221213 22:30:31 @agent_ppo2.py:185][0m |          -0.0057 |          17.6675 |           7.7587 |
[32m[20221213 22:30:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:30:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.81
[32m[20221213 22:30:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.26
[32m[20221213 22:30:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.49
[32m[20221213 22:30:31 @agent_ppo2.py:143][0m Total time:      12.30 min
[32m[20221213 22:30:31 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221213 22:30:31 @agent_ppo2.py:121][0m #------------------------ Iteration 577 --------------------------#
[32m[20221213 22:30:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0033 |          19.3462 |           7.6986 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0063 |          18.4590 |           7.6884 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0082 |          18.2861 |           7.6848 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0105 |          18.1548 |           7.6802 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0099 |          18.0293 |           7.6831 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |           0.0024 |          19.2053 |           7.6782 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0050 |          18.4643 |           7.6800 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0095 |          17.8822 |           7.6788 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0046 |          18.4212 |           7.6758 |
[32m[20221213 22:30:32 @agent_ppo2.py:185][0m |          -0.0106 |          17.6241 |           7.6755 |
[32m[20221213 22:30:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:30:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 308.92
[32m[20221213 22:30:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.35
[32m[20221213 22:30:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.10
[32m[20221213 22:30:32 @agent_ppo2.py:143][0m Total time:      12.33 min
[32m[20221213 22:30:32 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221213 22:30:32 @agent_ppo2.py:121][0m #------------------------ Iteration 578 --------------------------#
[32m[20221213 22:30:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |           0.0066 |          19.8518 |           7.8945 |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |          -0.0077 |          15.0855 |           7.8796 |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |          -0.0055 |          14.1825 |           7.8808 |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |          -0.0090 |          13.8044 |           7.8785 |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |          -0.0041 |          13.4178 |           7.8721 |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |          -0.0090 |          13.1984 |           7.8705 |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |          -0.0018 |          12.9852 |           7.8675 |
[32m[20221213 22:30:33 @agent_ppo2.py:185][0m |          -0.0135 |          12.9958 |           7.8653 |
[32m[20221213 22:30:34 @agent_ppo2.py:185][0m |          -0.0067 |          12.6847 |           7.8676 |
[32m[20221213 22:30:34 @agent_ppo2.py:185][0m |          -0.0044 |          12.7873 |           7.8621 |
[32m[20221213 22:30:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:30:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.44
[32m[20221213 22:30:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.00
[32m[20221213 22:30:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.51
[32m[20221213 22:30:34 @agent_ppo2.py:143][0m Total time:      12.35 min
[32m[20221213 22:30:34 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221213 22:30:34 @agent_ppo2.py:121][0m #------------------------ Iteration 579 --------------------------#
[32m[20221213 22:30:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:34 @agent_ppo2.py:185][0m |           0.0005 |          21.5736 |           7.6788 |
[32m[20221213 22:30:34 @agent_ppo2.py:185][0m |          -0.0023 |          21.1771 |           7.6761 |
[32m[20221213 22:30:34 @agent_ppo2.py:185][0m |          -0.0022 |          21.6478 |           7.6664 |
[32m[20221213 22:30:34 @agent_ppo2.py:185][0m |          -0.0063 |          20.8303 |           7.6628 |
[32m[20221213 22:30:35 @agent_ppo2.py:185][0m |          -0.0111 |          20.5982 |           7.6676 |
[32m[20221213 22:30:35 @agent_ppo2.py:185][0m |          -0.0081 |          20.6062 |           7.6694 |
[32m[20221213 22:30:35 @agent_ppo2.py:185][0m |          -0.0087 |          20.4323 |           7.6678 |
[32m[20221213 22:30:35 @agent_ppo2.py:185][0m |          -0.0111 |          20.5247 |           7.6682 |
[32m[20221213 22:30:35 @agent_ppo2.py:185][0m |          -0.0135 |          20.4636 |           7.6637 |
[32m[20221213 22:30:35 @agent_ppo2.py:185][0m |          -0.0112 |          20.3327 |           7.6676 |
[32m[20221213 22:30:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:30:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.15
[32m[20221213 22:30:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.65
[32m[20221213 22:30:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.88
[32m[20221213 22:30:35 @agent_ppo2.py:143][0m Total time:      12.37 min
[32m[20221213 22:30:35 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221213 22:30:35 @agent_ppo2.py:121][0m #------------------------ Iteration 580 --------------------------#
[32m[20221213 22:30:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:30:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:35 @agent_ppo2.py:185][0m |           0.0038 |          24.4589 |           7.7004 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0014 |          23.2834 |           7.6975 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0082 |          22.4437 |           7.6884 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0088 |          22.2134 |           7.6960 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0079 |          22.0848 |           7.6889 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0070 |          22.0047 |           7.6860 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0107 |          21.6753 |           7.6864 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0098 |          21.7100 |           7.6854 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0139 |          21.3857 |           7.6904 |
[32m[20221213 22:30:36 @agent_ppo2.py:185][0m |          -0.0153 |          21.4715 |           7.6870 |
[32m[20221213 22:30:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:30:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.43
[32m[20221213 22:30:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.50
[32m[20221213 22:30:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.16
[32m[20221213 22:30:36 @agent_ppo2.py:143][0m Total time:      12.39 min
[32m[20221213 22:30:36 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221213 22:30:36 @agent_ppo2.py:121][0m #------------------------ Iteration 581 --------------------------#
[32m[20221213 22:30:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |           0.0007 |          25.4253 |           7.8946 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0034 |          23.4839 |           7.8817 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0079 |          22.9400 |           7.8795 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0042 |          22.4418 |           7.8792 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0036 |          22.2320 |           7.8780 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |           0.0014 |          22.0725 |           7.8777 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0113 |          21.8455 |           7.8792 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0051 |          23.5392 |           7.8790 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0135 |          21.4708 |           7.8681 |
[32m[20221213 22:30:37 @agent_ppo2.py:185][0m |          -0.0189 |          21.5251 |           7.8807 |
[32m[20221213 22:30:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:30:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.50
[32m[20221213 22:30:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.23
[32m[20221213 22:30:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.11
[32m[20221213 22:30:38 @agent_ppo2.py:143][0m Total time:      12.41 min
[32m[20221213 22:30:38 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221213 22:30:38 @agent_ppo2.py:121][0m #------------------------ Iteration 582 --------------------------#
[32m[20221213 22:30:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 22:30:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:38 @agent_ppo2.py:185][0m |           0.0026 |          24.8327 |           7.9603 |
[32m[20221213 22:30:38 @agent_ppo2.py:185][0m |          -0.0028 |          23.4525 |           7.9635 |
[32m[20221213 22:30:38 @agent_ppo2.py:185][0m |          -0.0042 |          22.8391 |           7.9530 |
[32m[20221213 22:30:38 @agent_ppo2.py:185][0m |          -0.0092 |          22.6162 |           7.9635 |
[32m[20221213 22:30:38 @agent_ppo2.py:185][0m |          -0.0052 |          22.4496 |           7.9648 |
[32m[20221213 22:30:38 @agent_ppo2.py:185][0m |          -0.0074 |          22.1930 |           7.9572 |
[32m[20221213 22:30:39 @agent_ppo2.py:185][0m |           0.0007 |          23.2178 |           7.9620 |
[32m[20221213 22:30:39 @agent_ppo2.py:185][0m |          -0.0032 |          21.8844 |           7.9446 |
[32m[20221213 22:30:39 @agent_ppo2.py:185][0m |          -0.0023 |          21.8211 |           7.9601 |
[32m[20221213 22:30:39 @agent_ppo2.py:185][0m |          -0.0069 |          21.6664 |           7.9664 |
[32m[20221213 22:30:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:30:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 299.63
[32m[20221213 22:30:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.49
[32m[20221213 22:30:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.48
[32m[20221213 22:30:39 @agent_ppo2.py:143][0m Total time:      12.43 min
[32m[20221213 22:30:39 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221213 22:30:39 @agent_ppo2.py:121][0m #------------------------ Iteration 583 --------------------------#
[32m[20221213 22:30:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:39 @agent_ppo2.py:185][0m |           0.0022 |          26.6302 |           7.8450 |
[32m[20221213 22:30:39 @agent_ppo2.py:185][0m |          -0.0029 |          25.2131 |           7.8434 |
[32m[20221213 22:30:39 @agent_ppo2.py:185][0m |          -0.0074 |          24.6289 |           7.8445 |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |          -0.0040 |          24.2008 |           7.8420 |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |          -0.0072 |          23.9014 |           7.8397 |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |          -0.0107 |          23.7608 |           7.8436 |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |          -0.0084 |          23.4118 |           7.8403 |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |          -0.0095 |          23.3480 |           7.8481 |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |          -0.0092 |          23.0671 |           7.8493 |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |          -0.0094 |          22.8839 |           7.8512 |
[32m[20221213 22:30:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:30:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.86
[32m[20221213 22:30:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.16
[32m[20221213 22:30:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.54
[32m[20221213 22:30:40 @agent_ppo2.py:143][0m Total time:      12.45 min
[32m[20221213 22:30:40 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221213 22:30:40 @agent_ppo2.py:121][0m #------------------------ Iteration 584 --------------------------#
[32m[20221213 22:30:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:40 @agent_ppo2.py:185][0m |           0.0041 |          27.9657 |           7.9888 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0009 |          26.6242 |           7.9777 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0061 |          26.0489 |           7.9754 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0053 |          25.8525 |           7.9784 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0012 |          25.7691 |           7.9715 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |           0.0052 |          28.7200 |           7.9784 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0061 |          25.6177 |           7.9729 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0105 |          25.4454 |           7.9725 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0114 |          25.3160 |           7.9748 |
[32m[20221213 22:30:41 @agent_ppo2.py:185][0m |          -0.0059 |          25.4995 |           7.9745 |
[32m[20221213 22:30:41 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.03
[32m[20221213 22:30:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.94
[32m[20221213 22:30:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.05
[32m[20221213 22:30:41 @agent_ppo2.py:143][0m Total time:      12.47 min
[32m[20221213 22:30:41 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221213 22:30:41 @agent_ppo2.py:121][0m #------------------------ Iteration 585 --------------------------#
[32m[20221213 22:30:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |           0.0058 |          22.3478 |           7.8649 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0088 |          20.7314 |           7.8602 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0043 |          20.2358 |           7.8555 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0082 |          19.8066 |           7.8554 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0114 |          19.6484 |           7.8523 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0111 |          19.4764 |           7.8531 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0109 |          19.2765 |           7.8531 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0070 |          19.1263 |           7.8550 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0091 |          19.0210 |           7.8485 |
[32m[20221213 22:30:42 @agent_ppo2.py:185][0m |          -0.0079 |          18.8999 |           7.8570 |
[32m[20221213 22:30:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:30:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 152.15
[32m[20221213 22:30:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.92
[32m[20221213 22:30:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.39
[32m[20221213 22:30:43 @agent_ppo2.py:143][0m Total time:      12.49 min
[32m[20221213 22:30:43 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221213 22:30:43 @agent_ppo2.py:121][0m #------------------------ Iteration 586 --------------------------#
[32m[20221213 22:30:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0014 |          27.6427 |           8.0636 |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0089 |          26.4954 |           8.0539 |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0107 |          26.0180 |           8.0480 |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0070 |          26.3071 |           8.0466 |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0130 |          25.6161 |           8.0385 |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0082 |          25.7300 |           8.0474 |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0135 |          25.2176 |           8.0454 |
[32m[20221213 22:30:43 @agent_ppo2.py:185][0m |          -0.0156 |          25.0827 |           8.0440 |
[32m[20221213 22:30:44 @agent_ppo2.py:185][0m |          -0.0095 |          25.0769 |           8.0424 |
[32m[20221213 22:30:44 @agent_ppo2.py:185][0m |          -0.0119 |          24.9621 |           8.0454 |
[32m[20221213 22:30:44 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:30:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.59
[32m[20221213 22:30:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.81
[32m[20221213 22:30:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.24
[32m[20221213 22:30:44 @agent_ppo2.py:143][0m Total time:      12.51 min
[32m[20221213 22:30:44 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221213 22:30:44 @agent_ppo2.py:121][0m #------------------------ Iteration 587 --------------------------#
[32m[20221213 22:30:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:44 @agent_ppo2.py:185][0m |          -0.0010 |          19.4353 |           8.0200 |
[32m[20221213 22:30:44 @agent_ppo2.py:185][0m |          -0.0020 |          16.9182 |           8.0150 |
[32m[20221213 22:30:44 @agent_ppo2.py:185][0m |          -0.0061 |          16.5695 |           8.0085 |
[32m[20221213 22:30:44 @agent_ppo2.py:185][0m |          -0.0057 |          16.3493 |           8.0081 |
[32m[20221213 22:30:44 @agent_ppo2.py:185][0m |          -0.0093 |          16.0692 |           8.0011 |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |          -0.0031 |          15.8716 |           8.0042 |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |          -0.0104 |          15.8282 |           8.0030 |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |          -0.0117 |          15.6914 |           7.9996 |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |          -0.0084 |          15.5639 |           7.9995 |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |           0.0004 |          16.8294 |           7.9974 |
[32m[20221213 22:30:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.48
[32m[20221213 22:30:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.50
[32m[20221213 22:30:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.75
[32m[20221213 22:30:45 @agent_ppo2.py:143][0m Total time:      12.53 min
[32m[20221213 22:30:45 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221213 22:30:45 @agent_ppo2.py:121][0m #------------------------ Iteration 588 --------------------------#
[32m[20221213 22:30:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |           0.0010 |          30.6715 |           8.0697 |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |          -0.0041 |          29.2884 |           8.0741 |
[32m[20221213 22:30:45 @agent_ppo2.py:185][0m |          -0.0108 |          27.7457 |           8.0735 |
[32m[20221213 22:30:46 @agent_ppo2.py:185][0m |          -0.0118 |          27.3064 |           8.0672 |
[32m[20221213 22:30:46 @agent_ppo2.py:185][0m |          -0.0128 |          27.1830 |           8.0638 |
[32m[20221213 22:30:46 @agent_ppo2.py:185][0m |          -0.0141 |          27.0625 |           8.0701 |
[32m[20221213 22:30:46 @agent_ppo2.py:185][0m |          -0.0067 |          26.8093 |           8.0719 |
[32m[20221213 22:30:46 @agent_ppo2.py:185][0m |          -0.0106 |          26.8258 |           8.0689 |
[32m[20221213 22:30:46 @agent_ppo2.py:185][0m |          -0.0128 |          26.4684 |           8.0734 |
[32m[20221213 22:30:46 @agent_ppo2.py:185][0m |          -0.0038 |          28.1997 |           8.0694 |
[32m[20221213 22:30:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.16
[32m[20221213 22:30:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.57
[32m[20221213 22:30:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.62
[32m[20221213 22:30:46 @agent_ppo2.py:143][0m Total time:      12.55 min
[32m[20221213 22:30:46 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221213 22:30:46 @agent_ppo2.py:121][0m #------------------------ Iteration 589 --------------------------#
[32m[20221213 22:30:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0006 |          15.1607 |           8.0199 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0044 |          13.1264 |           8.0093 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0103 |          12.4284 |           8.0007 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |           0.0077 |          13.4054 |           7.9955 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0040 |          11.6191 |           7.9917 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0087 |          11.2630 |           7.9892 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0059 |          11.1134 |           7.9869 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0084 |          10.7195 |           7.9765 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0091 |          10.4847 |           7.9797 |
[32m[20221213 22:30:47 @agent_ppo2.py:185][0m |          -0.0099 |          10.3395 |           7.9795 |
[32m[20221213 22:30:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.35
[32m[20221213 22:30:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.92
[32m[20221213 22:30:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.14
[32m[20221213 22:30:47 @agent_ppo2.py:143][0m Total time:      12.57 min
[32m[20221213 22:30:47 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221213 22:30:47 @agent_ppo2.py:121][0m #------------------------ Iteration 590 --------------------------#
[32m[20221213 22:30:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:30:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |           0.0017 |          21.8162 |           8.2024 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0033 |          18.7203 |           8.1928 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0035 |          17.9072 |           8.1982 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0016 |          17.5014 |           8.1966 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0058 |          17.1972 |           8.1982 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0059 |          16.8648 |           8.1962 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0095 |          16.6527 |           8.1983 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0156 |          16.4009 |           8.1952 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0123 |          16.3401 |           8.1994 |
[32m[20221213 22:30:48 @agent_ppo2.py:185][0m |          -0.0099 |          16.1746 |           8.1991 |
[32m[20221213 22:30:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:30:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.61
[32m[20221213 22:30:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.45
[32m[20221213 22:30:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.32
[32m[20221213 22:30:49 @agent_ppo2.py:143][0m Total time:      12.60 min
[32m[20221213 22:30:49 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221213 22:30:49 @agent_ppo2.py:121][0m #------------------------ Iteration 591 --------------------------#
[32m[20221213 22:30:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:49 @agent_ppo2.py:185][0m |          -0.0073 |          24.5522 |           8.1800 |
[32m[20221213 22:30:49 @agent_ppo2.py:185][0m |          -0.0064 |          23.2901 |           8.1724 |
[32m[20221213 22:30:49 @agent_ppo2.py:185][0m |          -0.0082 |          22.8719 |           8.1629 |
[32m[20221213 22:30:49 @agent_ppo2.py:185][0m |          -0.0081 |          22.6453 |           8.1637 |
[32m[20221213 22:30:49 @agent_ppo2.py:185][0m |          -0.0098 |          22.4284 |           8.1578 |
[32m[20221213 22:30:49 @agent_ppo2.py:185][0m |           0.0015 |          23.6290 |           8.1625 |
[32m[20221213 22:30:49 @agent_ppo2.py:185][0m |           0.0055 |          23.1070 |           8.1579 |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |          -0.0122 |          21.9517 |           8.1506 |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |          -0.0084 |          21.9760 |           8.1562 |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |           0.0004 |          23.2870 |           8.1496 |
[32m[20221213 22:30:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.83
[32m[20221213 22:30:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.63
[32m[20221213 22:30:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.31
[32m[20221213 22:30:50 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 368.31
[32m[20221213 22:30:50 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 368.31
[32m[20221213 22:30:50 @agent_ppo2.py:143][0m Total time:      12.62 min
[32m[20221213 22:30:50 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221213 22:30:50 @agent_ppo2.py:121][0m #------------------------ Iteration 592 --------------------------#
[32m[20221213 22:30:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |           0.0056 |          15.1158 |           8.0564 |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |          -0.0045 |          13.9097 |           8.0448 |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |           0.0012 |          14.6891 |           8.0388 |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |          -0.0074 |          13.6760 |           8.0375 |
[32m[20221213 22:30:50 @agent_ppo2.py:185][0m |          -0.0045 |          13.5285 |           8.0388 |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |          -0.0145 |          13.4139 |           8.0388 |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |          -0.0083 |          13.3558 |           8.0382 |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |          -0.0110 |          13.2831 |           8.0342 |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |          -0.0136 |          13.2868 |           8.0361 |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |          -0.0077 |          13.1782 |           8.0353 |
[32m[20221213 22:30:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.00
[32m[20221213 22:30:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.34
[32m[20221213 22:30:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.25
[32m[20221213 22:30:51 @agent_ppo2.py:143][0m Total time:      12.64 min
[32m[20221213 22:30:51 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221213 22:30:51 @agent_ppo2.py:121][0m #------------------------ Iteration 593 --------------------------#
[32m[20221213 22:30:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |          -0.0034 |          24.2279 |           8.1018 |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |           0.0030 |          23.2935 |           8.0943 |
[32m[20221213 22:30:51 @agent_ppo2.py:185][0m |          -0.0084 |          21.7989 |           8.0851 |
[32m[20221213 22:30:52 @agent_ppo2.py:185][0m |          -0.0062 |          21.3214 |           8.0865 |
[32m[20221213 22:30:52 @agent_ppo2.py:185][0m |          -0.0077 |          21.1696 |           8.0930 |
[32m[20221213 22:30:52 @agent_ppo2.py:185][0m |          -0.0088 |          20.9004 |           8.0910 |
[32m[20221213 22:30:52 @agent_ppo2.py:185][0m |          -0.0125 |          20.7486 |           8.0915 |
[32m[20221213 22:30:52 @agent_ppo2.py:185][0m |          -0.0073 |          20.5532 |           8.0951 |
[32m[20221213 22:30:52 @agent_ppo2.py:185][0m |          -0.0031 |          21.1194 |           8.0961 |
[32m[20221213 22:30:52 @agent_ppo2.py:185][0m |          -0.0125 |          20.4496 |           8.0975 |
[32m[20221213 22:30:52 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 195.52
[32m[20221213 22:30:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 315.82
[32m[20221213 22:30:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.28
[32m[20221213 22:30:52 @agent_ppo2.py:143][0m Total time:      12.66 min
[32m[20221213 22:30:52 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221213 22:30:52 @agent_ppo2.py:121][0m #------------------------ Iteration 594 --------------------------#
[32m[20221213 22:30:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:30:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0027 |           2.7395 |           8.2646 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0011 |           2.3860 |           8.2583 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0048 |           2.3428 |           8.2602 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0004 |           2.4345 |           8.2594 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0064 |           2.3236 |           8.2461 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0059 |           2.3189 |           8.2659 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |           0.0007 |           2.4182 |           8.2589 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0072 |           2.3107 |           8.2508 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0012 |           2.3173 |           8.2656 |
[32m[20221213 22:30:53 @agent_ppo2.py:185][0m |          -0.0084 |           2.3035 |           8.2632 |
[32m[20221213 22:30:53 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:30:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:30:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:30:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.08
[32m[20221213 22:30:53 @agent_ppo2.py:143][0m Total time:      12.68 min
[32m[20221213 22:30:53 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221213 22:30:53 @agent_ppo2.py:121][0m #------------------------ Iteration 595 --------------------------#
[32m[20221213 22:30:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |           0.0008 |          13.7522 |           8.3597 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0059 |          12.8294 |           8.3536 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0079 |          12.7073 |           8.3556 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0038 |          12.6164 |           8.3559 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0072 |          12.4533 |           8.3575 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0029 |          12.6063 |           8.3613 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0089 |          12.2819 |           8.3634 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0087 |          12.2141 |           8.3566 |
[32m[20221213 22:30:54 @agent_ppo2.py:185][0m |          -0.0112 |          12.1773 |           8.3612 |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0126 |          12.1142 |           8.3698 |
[32m[20221213 22:30:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:30:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.78
[32m[20221213 22:30:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.59
[32m[20221213 22:30:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.65
[32m[20221213 22:30:55 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 370.65
[32m[20221213 22:30:55 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 370.65
[32m[20221213 22:30:55 @agent_ppo2.py:143][0m Total time:      12.70 min
[32m[20221213 22:30:55 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221213 22:30:55 @agent_ppo2.py:121][0m #------------------------ Iteration 596 --------------------------#
[32m[20221213 22:30:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0013 |          12.8965 |           8.3298 |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0015 |          11.1493 |           8.3303 |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0050 |          10.8815 |           8.3222 |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0064 |          10.6359 |           8.3275 |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0117 |          10.4507 |           8.3238 |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0078 |          10.3423 |           8.3274 |
[32m[20221213 22:30:55 @agent_ppo2.py:185][0m |          -0.0080 |          10.2889 |           8.3240 |
[32m[20221213 22:30:56 @agent_ppo2.py:185][0m |          -0.0057 |          10.3465 |           8.3273 |
[32m[20221213 22:30:56 @agent_ppo2.py:185][0m |          -0.0102 |          10.1540 |           8.3166 |
[32m[20221213 22:30:56 @agent_ppo2.py:185][0m |          -0.0135 |          10.0776 |           8.3280 |
[32m[20221213 22:30:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.83
[32m[20221213 22:30:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.79
[32m[20221213 22:30:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.55
[32m[20221213 22:30:56 @agent_ppo2.py:143][0m Total time:      12.72 min
[32m[20221213 22:30:56 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221213 22:30:56 @agent_ppo2.py:121][0m #------------------------ Iteration 597 --------------------------#
[32m[20221213 22:30:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:56 @agent_ppo2.py:185][0m |          -0.0018 |          20.4008 |           8.2890 |
[32m[20221213 22:30:56 @agent_ppo2.py:185][0m |          -0.0047 |          18.9750 |           8.2842 |
[32m[20221213 22:30:56 @agent_ppo2.py:185][0m |          -0.0115 |          18.7072 |           8.2778 |
[32m[20221213 22:30:56 @agent_ppo2.py:185][0m |          -0.0059 |          18.4018 |           8.2760 |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |          -0.0129 |          18.3531 |           8.2767 |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |           0.0055 |          19.7348 |           8.2785 |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |          -0.0105 |          18.1487 |           8.2704 |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |          -0.0077 |          18.0206 |           8.2777 |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |          -0.0143 |          17.9757 |           8.2721 |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |          -0.0118 |          17.8411 |           8.2745 |
[32m[20221213 22:30:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.41
[32m[20221213 22:30:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.19
[32m[20221213 22:30:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 37.73
[32m[20221213 22:30:57 @agent_ppo2.py:143][0m Total time:      12.74 min
[32m[20221213 22:30:57 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221213 22:30:57 @agent_ppo2.py:121][0m #------------------------ Iteration 598 --------------------------#
[32m[20221213 22:30:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |          -0.0009 |          19.8898 |           8.3093 |
[32m[20221213 22:30:57 @agent_ppo2.py:185][0m |          -0.0031 |          19.0832 |           8.2938 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |          -0.0039 |          18.9163 |           8.2967 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |           0.0079 |          20.7795 |           8.2936 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |           0.0099 |          20.7043 |           8.2983 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |          -0.0052 |          18.8380 |           8.2813 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |          -0.0091 |          18.6213 |           8.2926 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |          -0.0101 |          18.5760 |           8.2908 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |          -0.0061 |          18.5075 |           8.2874 |
[32m[20221213 22:30:58 @agent_ppo2.py:185][0m |          -0.0059 |          18.6011 |           8.2855 |
[32m[20221213 22:30:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:30:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.87
[32m[20221213 22:30:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.51
[32m[20221213 22:30:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.43
[32m[20221213 22:30:58 @agent_ppo2.py:143][0m Total time:      12.76 min
[32m[20221213 22:30:58 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221213 22:30:58 @agent_ppo2.py:121][0m #------------------------ Iteration 599 --------------------------#
[32m[20221213 22:30:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:30:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |           0.0024 |          22.4633 |           8.3611 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0045 |          21.0182 |           8.3408 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0103 |          20.7101 |           8.3364 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0058 |          20.4031 |           8.3333 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0055 |          20.2312 |           8.3382 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0101 |          20.1470 |           8.3327 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0041 |          19.8524 |           8.3386 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0126 |          19.9126 |           8.3307 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0099 |          19.7390 |           8.3337 |
[32m[20221213 22:30:59 @agent_ppo2.py:185][0m |          -0.0119 |          19.6314 |           8.3325 |
[32m[20221213 22:30:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:30:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.04
[32m[20221213 22:30:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.32
[32m[20221213 22:30:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.54
[32m[20221213 22:30:59 @agent_ppo2.py:143][0m Total time:      12.78 min
[32m[20221213 22:30:59 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221213 22:30:59 @agent_ppo2.py:121][0m #------------------------ Iteration 600 --------------------------#
[32m[20221213 22:31:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:31:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0023 |          21.6339 |           8.5057 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |           0.0004 |          21.1130 |           8.4982 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0007 |          20.4354 |           8.4871 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0021 |          19.8553 |           8.4983 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0074 |          19.5792 |           8.4885 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0077 |          19.5159 |           8.4904 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0108 |          19.3090 |           8.4868 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0102 |          19.1448 |           8.4853 |
[32m[20221213 22:31:00 @agent_ppo2.py:185][0m |          -0.0147 |          19.1013 |           8.4939 |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |          -0.0101 |          18.9305 |           8.4828 |
[32m[20221213 22:31:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:31:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.19
[32m[20221213 22:31:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.67
[32m[20221213 22:31:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.34
[32m[20221213 22:31:01 @agent_ppo2.py:143][0m Total time:      12.80 min
[32m[20221213 22:31:01 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221213 22:31:01 @agent_ppo2.py:121][0m #------------------------ Iteration 601 --------------------------#
[32m[20221213 22:31:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |           0.0030 |          16.6226 |           8.4331 |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |          -0.0030 |          15.8471 |           8.4229 |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |          -0.0041 |          15.6706 |           8.4301 |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |          -0.0078 |          15.5227 |           8.4271 |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |          -0.0025 |          15.5004 |           8.4287 |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |          -0.0087 |          15.4311 |           8.4315 |
[32m[20221213 22:31:01 @agent_ppo2.py:185][0m |          -0.0076 |          15.4259 |           8.4336 |
[32m[20221213 22:31:02 @agent_ppo2.py:185][0m |          -0.0029 |          16.1711 |           8.4348 |
[32m[20221213 22:31:02 @agent_ppo2.py:185][0m |          -0.0065 |          15.2542 |           8.4278 |
[32m[20221213 22:31:02 @agent_ppo2.py:185][0m |           0.0051 |          16.5554 |           8.4306 |
[32m[20221213 22:31:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.53
[32m[20221213 22:31:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.80
[32m[20221213 22:31:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.85
[32m[20221213 22:31:02 @agent_ppo2.py:143][0m Total time:      12.82 min
[32m[20221213 22:31:02 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221213 22:31:02 @agent_ppo2.py:121][0m #------------------------ Iteration 602 --------------------------#
[32m[20221213 22:31:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:02 @agent_ppo2.py:185][0m |          -0.0002 |          22.2607 |           8.4670 |
[32m[20221213 22:31:02 @agent_ppo2.py:185][0m |          -0.0054 |          21.1204 |           8.4655 |
[32m[20221213 22:31:02 @agent_ppo2.py:185][0m |          -0.0063 |          20.5681 |           8.4633 |
[32m[20221213 22:31:02 @agent_ppo2.py:185][0m |          -0.0057 |          20.6182 |           8.4668 |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |          -0.0056 |          19.9453 |           8.4642 |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |           0.0002 |          21.0665 |           8.4641 |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |          -0.0080 |          19.8234 |           8.4653 |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |          -0.0101 |          19.4392 |           8.4644 |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |          -0.0122 |          19.2191 |           8.4657 |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |          -0.0115 |          19.1387 |           8.4661 |
[32m[20221213 22:31:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.40
[32m[20221213 22:31:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.81
[32m[20221213 22:31:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.87
[32m[20221213 22:31:03 @agent_ppo2.py:143][0m Total time:      12.84 min
[32m[20221213 22:31:03 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221213 22:31:03 @agent_ppo2.py:121][0m #------------------------ Iteration 603 --------------------------#
[32m[20221213 22:31:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |          -0.0072 |          17.1404 |           8.4795 |
[32m[20221213 22:31:03 @agent_ppo2.py:185][0m |           0.0006 |          15.8995 |           8.4580 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |          -0.0089 |          15.5004 |           8.4533 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |          -0.0116 |          15.0756 |           8.4565 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |           0.0049 |          16.1547 |           8.4525 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |          -0.0096 |          14.7288 |           8.4628 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |          -0.0051 |          14.5805 |           8.4560 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |          -0.0088 |          14.3645 |           8.4476 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |          -0.0069 |          14.6075 |           8.4577 |
[32m[20221213 22:31:04 @agent_ppo2.py:185][0m |          -0.0118 |          14.2156 |           8.4499 |
[32m[20221213 22:31:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.35
[32m[20221213 22:31:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.43
[32m[20221213 22:31:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.64
[32m[20221213 22:31:04 @agent_ppo2.py:143][0m Total time:      12.86 min
[32m[20221213 22:31:04 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221213 22:31:04 @agent_ppo2.py:121][0m #------------------------ Iteration 604 --------------------------#
[32m[20221213 22:31:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0030 |          18.8407 |           8.4590 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0063 |          17.7331 |           8.4454 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0086 |          17.4870 |           8.4441 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0075 |          17.2794 |           8.4485 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0066 |          17.2442 |           8.4463 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0086 |          17.0952 |           8.4527 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0044 |          17.1068 |           8.4518 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0098 |          17.0061 |           8.4478 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0058 |          17.6046 |           8.4529 |
[32m[20221213 22:31:05 @agent_ppo2.py:185][0m |          -0.0068 |          16.9211 |           8.4534 |
[32m[20221213 22:31:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.36
[32m[20221213 22:31:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.86
[32m[20221213 22:31:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 167.97
[32m[20221213 22:31:05 @agent_ppo2.py:143][0m Total time:      12.88 min
[32m[20221213 22:31:05 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221213 22:31:05 @agent_ppo2.py:121][0m #------------------------ Iteration 605 --------------------------#
[32m[20221213 22:31:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |           0.0002 |          21.3609 |           8.6354 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |          -0.0068 |          19.8560 |           8.6254 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |           0.0102 |          20.9371 |           8.6132 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |          -0.0048 |          19.2664 |           8.5963 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |          -0.0011 |          18.8955 |           8.6129 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |          -0.0101 |          18.7313 |           8.6041 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |          -0.0044 |          18.5478 |           8.6061 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |          -0.0088 |          18.4578 |           8.6031 |
[32m[20221213 22:31:06 @agent_ppo2.py:185][0m |          -0.0056 |          18.3265 |           8.6013 |
[32m[20221213 22:31:07 @agent_ppo2.py:185][0m |          -0.0113 |          18.2424 |           8.5995 |
[32m[20221213 22:31:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:31:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.03
[32m[20221213 22:31:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.04
[32m[20221213 22:31:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.18
[32m[20221213 22:31:07 @agent_ppo2.py:143][0m Total time:      12.90 min
[32m[20221213 22:31:07 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221213 22:31:07 @agent_ppo2.py:121][0m #------------------------ Iteration 606 --------------------------#
[32m[20221213 22:31:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:07 @agent_ppo2.py:185][0m |           0.0026 |          13.4169 |           8.4735 |
[32m[20221213 22:31:07 @agent_ppo2.py:185][0m |           0.0115 |          13.0967 |           8.4647 |
[32m[20221213 22:31:07 @agent_ppo2.py:185][0m |          -0.0027 |          11.7156 |           8.4516 |
[32m[20221213 22:31:07 @agent_ppo2.py:185][0m |          -0.0139 |          11.4934 |           8.4715 |
[32m[20221213 22:31:07 @agent_ppo2.py:185][0m |          -0.0081 |          11.3431 |           8.4681 |
[32m[20221213 22:31:07 @agent_ppo2.py:185][0m |          -0.0089 |          11.2721 |           8.4707 |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0086 |          11.2133 |           8.4758 |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0098 |          11.1676 |           8.4707 |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0066 |          11.1332 |           8.4734 |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0087 |          11.0656 |           8.4777 |
[32m[20221213 22:31:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.78
[32m[20221213 22:31:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.10
[32m[20221213 22:31:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.85
[32m[20221213 22:31:08 @agent_ppo2.py:143][0m Total time:      12.92 min
[32m[20221213 22:31:08 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221213 22:31:08 @agent_ppo2.py:121][0m #------------------------ Iteration 607 --------------------------#
[32m[20221213 22:31:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0019 |          27.5177 |           8.4705 |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0077 |          26.1744 |           8.4582 |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0102 |          25.6592 |           8.4552 |
[32m[20221213 22:31:08 @agent_ppo2.py:185][0m |          -0.0127 |          25.4577 |           8.4503 |
[32m[20221213 22:31:09 @agent_ppo2.py:185][0m |           0.0026 |          27.4134 |           8.4500 |
[32m[20221213 22:31:09 @agent_ppo2.py:185][0m |          -0.0027 |          25.3157 |           8.4493 |
[32m[20221213 22:31:09 @agent_ppo2.py:185][0m |          -0.0084 |          24.7584 |           8.4455 |
[32m[20221213 22:31:09 @agent_ppo2.py:185][0m |          -0.0119 |          24.5830 |           8.4530 |
[32m[20221213 22:31:09 @agent_ppo2.py:185][0m |          -0.0123 |          24.3725 |           8.4499 |
[32m[20221213 22:31:09 @agent_ppo2.py:185][0m |          -0.0150 |          24.4298 |           8.4445 |
[32m[20221213 22:31:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.19
[32m[20221213 22:31:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.93
[32m[20221213 22:31:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.14
[32m[20221213 22:31:09 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221213 22:31:09 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221213 22:31:09 @agent_ppo2.py:121][0m #------------------------ Iteration 608 --------------------------#
[32m[20221213 22:31:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:09 @agent_ppo2.py:185][0m |          -0.0033 |          30.4841 |           8.5988 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0032 |          28.7662 |           8.5870 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0088 |          28.4256 |           8.5807 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0051 |          28.2003 |           8.5777 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0021 |          27.7839 |           8.5777 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0006 |          28.3320 |           8.5755 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0075 |          27.4347 |           8.5775 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0086 |          27.5071 |           8.5716 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0064 |          27.4130 |           8.5757 |
[32m[20221213 22:31:10 @agent_ppo2.py:185][0m |          -0.0069 |          27.1089 |           8.5744 |
[32m[20221213 22:31:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.88
[32m[20221213 22:31:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.13
[32m[20221213 22:31:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.22
[32m[20221213 22:31:10 @agent_ppo2.py:143][0m Total time:      12.96 min
[32m[20221213 22:31:10 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221213 22:31:10 @agent_ppo2.py:121][0m #------------------------ Iteration 609 --------------------------#
[32m[20221213 22:31:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |           0.0004 |          24.4346 |           8.4968 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0057 |          23.2935 |           8.4842 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0088 |          23.0536 |           8.4753 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0073 |          22.5401 |           8.4674 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0032 |          22.3342 |           8.4631 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0076 |          22.0617 |           8.4623 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0078 |          21.8245 |           8.4609 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0089 |          21.7375 |           8.4644 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0087 |          21.4278 |           8.4532 |
[32m[20221213 22:31:11 @agent_ppo2.py:185][0m |          -0.0074 |          21.5424 |           8.4514 |
[32m[20221213 22:31:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.00
[32m[20221213 22:31:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.82
[32m[20221213 22:31:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.74
[32m[20221213 22:31:12 @agent_ppo2.py:143][0m Total time:      12.98 min
[32m[20221213 22:31:12 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221213 22:31:12 @agent_ppo2.py:121][0m #------------------------ Iteration 610 --------------------------#
[32m[20221213 22:31:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:31:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |           0.0038 |          14.4727 |           8.6079 |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |          -0.0039 |          13.3037 |           8.6081 |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |          -0.0031 |          13.1594 |           8.6082 |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |          -0.0040 |          13.0264 |           8.6116 |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |          -0.0017 |          13.0165 |           8.6036 |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |          -0.0095 |          12.8639 |           8.6024 |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |          -0.0065 |          12.7797 |           8.6105 |
[32m[20221213 22:31:12 @agent_ppo2.py:185][0m |          -0.0076 |          12.7078 |           8.6120 |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0005 |          12.8610 |           8.6090 |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0075 |          12.6626 |           8.6065 |
[32m[20221213 22:31:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.50
[32m[20221213 22:31:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.16
[32m[20221213 22:31:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.02
[32m[20221213 22:31:13 @agent_ppo2.py:143][0m Total time:      13.00 min
[32m[20221213 22:31:13 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221213 22:31:13 @agent_ppo2.py:121][0m #------------------------ Iteration 611 --------------------------#
[32m[20221213 22:31:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0005 |          28.0602 |           8.6518 |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0027 |          27.0881 |           8.6432 |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0086 |          26.6736 |           8.6420 |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0061 |          26.4365 |           8.6375 |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0091 |          26.4395 |           8.6360 |
[32m[20221213 22:31:13 @agent_ppo2.py:185][0m |          -0.0089 |          26.1041 |           8.6405 |
[32m[20221213 22:31:14 @agent_ppo2.py:185][0m |          -0.0080 |          25.9576 |           8.6344 |
[32m[20221213 22:31:14 @agent_ppo2.py:185][0m |          -0.0098 |          25.8283 |           8.6394 |
[32m[20221213 22:31:14 @agent_ppo2.py:185][0m |          -0.0117 |          25.8154 |           8.6339 |
[32m[20221213 22:31:14 @agent_ppo2.py:185][0m |          -0.0116 |          25.6902 |           8.6384 |
[32m[20221213 22:31:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.32
[32m[20221213 22:31:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.88
[32m[20221213 22:31:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.86
[32m[20221213 22:31:14 @agent_ppo2.py:143][0m Total time:      13.02 min
[32m[20221213 22:31:14 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221213 22:31:14 @agent_ppo2.py:121][0m #------------------------ Iteration 612 --------------------------#
[32m[20221213 22:31:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:14 @agent_ppo2.py:185][0m |           0.0055 |          25.1183 |           8.5809 |
[32m[20221213 22:31:14 @agent_ppo2.py:185][0m |           0.0003 |          23.7602 |           8.5619 |
[32m[20221213 22:31:14 @agent_ppo2.py:185][0m |          -0.0053 |          23.0391 |           8.5560 |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |          -0.0092 |          22.7080 |           8.5574 |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |          -0.0096 |          22.4256 |           8.5571 |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |          -0.0084 |          22.2885 |           8.5604 |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |          -0.0101 |          22.3763 |           8.5620 |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |          -0.0115 |          22.0761 |           8.5598 |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |          -0.0005 |          23.7719 |           8.5585 |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |          -0.0116 |          21.8541 |           8.5597 |
[32m[20221213 22:31:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.94
[32m[20221213 22:31:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.78
[32m[20221213 22:31:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.90
[32m[20221213 22:31:15 @agent_ppo2.py:143][0m Total time:      13.04 min
[32m[20221213 22:31:15 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221213 22:31:15 @agent_ppo2.py:121][0m #------------------------ Iteration 613 --------------------------#
[32m[20221213 22:31:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:15 @agent_ppo2.py:185][0m |           0.0029 |          24.0731 |           8.6086 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0051 |          22.1960 |           8.5974 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0092 |          21.4688 |           8.5857 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0012 |          21.4035 |           8.5932 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0096 |          20.8132 |           8.5789 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0025 |          21.5668 |           8.5888 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0091 |          20.4232 |           8.5815 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0080 |          20.2803 |           8.5893 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0128 |          20.1631 |           8.5804 |
[32m[20221213 22:31:16 @agent_ppo2.py:185][0m |          -0.0080 |          19.9714 |           8.5859 |
[32m[20221213 22:31:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.55
[32m[20221213 22:31:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.78
[32m[20221213 22:31:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.39
[32m[20221213 22:31:16 @agent_ppo2.py:143][0m Total time:      13.06 min
[32m[20221213 22:31:16 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221213 22:31:16 @agent_ppo2.py:121][0m #------------------------ Iteration 614 --------------------------#
[32m[20221213 22:31:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0022 |          30.5886 |           8.6895 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0050 |          28.5393 |           8.6913 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0040 |          28.3022 |           8.6803 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |           0.0075 |          30.6242 |           8.6857 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0049 |          27.2857 |           8.6743 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0079 |          26.8025 |           8.6857 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0092 |          26.7598 |           8.6808 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0127 |          26.5250 |           8.6817 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0099 |          26.2848 |           8.6846 |
[32m[20221213 22:31:17 @agent_ppo2.py:185][0m |          -0.0101 |          26.1653 |           8.6839 |
[32m[20221213 22:31:17 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 203.53
[32m[20221213 22:31:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.64
[32m[20221213 22:31:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.10
[32m[20221213 22:31:18 @agent_ppo2.py:143][0m Total time:      13.08 min
[32m[20221213 22:31:18 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221213 22:31:18 @agent_ppo2.py:121][0m #------------------------ Iteration 615 --------------------------#
[32m[20221213 22:31:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |          -0.0007 |          27.9595 |           8.4756 |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |           0.0004 |          25.3966 |           8.4771 |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |          -0.0044 |          24.7740 |           8.4688 |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |          -0.0091 |          24.4292 |           8.4691 |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |          -0.0105 |          24.1954 |           8.4683 |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |          -0.0131 |          24.0764 |           8.4689 |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |          -0.0091 |          23.9227 |           8.4643 |
[32m[20221213 22:31:18 @agent_ppo2.py:185][0m |          -0.0119 |          23.9497 |           8.4651 |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |          -0.0113 |          23.8024 |           8.4598 |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |          -0.0005 |          25.0079 |           8.4662 |
[32m[20221213 22:31:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.81
[32m[20221213 22:31:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.22
[32m[20221213 22:31:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.86
[32m[20221213 22:31:19 @agent_ppo2.py:143][0m Total time:      13.10 min
[32m[20221213 22:31:19 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221213 22:31:19 @agent_ppo2.py:121][0m #------------------------ Iteration 616 --------------------------#
[32m[20221213 22:31:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |           0.0118 |          26.4691 |           8.6226 |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |          -0.0011 |          23.6897 |           8.6038 |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |           0.0002 |          23.4352 |           8.6062 |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |          -0.0069 |          22.9616 |           8.6043 |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |          -0.0039 |          22.8565 |           8.6023 |
[32m[20221213 22:31:19 @agent_ppo2.py:185][0m |          -0.0073 |          22.7205 |           8.5963 |
[32m[20221213 22:31:20 @agent_ppo2.py:185][0m |          -0.0076 |          22.6510 |           8.5931 |
[32m[20221213 22:31:20 @agent_ppo2.py:185][0m |          -0.0044 |          23.6621 |           8.5943 |
[32m[20221213 22:31:20 @agent_ppo2.py:185][0m |          -0.0087 |          22.5617 |           8.5867 |
[32m[20221213 22:31:20 @agent_ppo2.py:185][0m |          -0.0101 |          22.3563 |           8.5839 |
[32m[20221213 22:31:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.83
[32m[20221213 22:31:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.50
[32m[20221213 22:31:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.75
[32m[20221213 22:31:20 @agent_ppo2.py:143][0m Total time:      13.12 min
[32m[20221213 22:31:20 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221213 22:31:20 @agent_ppo2.py:121][0m #------------------------ Iteration 617 --------------------------#
[32m[20221213 22:31:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:20 @agent_ppo2.py:185][0m |           0.0026 |          22.1801 |           8.5511 |
[32m[20221213 22:31:20 @agent_ppo2.py:185][0m |          -0.0110 |          20.7114 |           8.5362 |
[32m[20221213 22:31:20 @agent_ppo2.py:185][0m |          -0.0094 |          20.1715 |           8.5382 |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |          -0.0122 |          19.6625 |           8.5323 |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |          -0.0091 |          19.4300 |           8.5379 |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |           0.0035 |          20.8695 |           8.5418 |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |          -0.0124 |          19.1764 |           8.5426 |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |          -0.0128 |          18.8486 |           8.5386 |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |          -0.0079 |          18.7788 |           8.5405 |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |          -0.0133 |          18.5764 |           8.5418 |
[32m[20221213 22:31:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.53
[32m[20221213 22:31:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 311.61
[32m[20221213 22:31:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.66
[32m[20221213 22:31:21 @agent_ppo2.py:143][0m Total time:      13.14 min
[32m[20221213 22:31:21 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221213 22:31:21 @agent_ppo2.py:121][0m #------------------------ Iteration 618 --------------------------#
[32m[20221213 22:31:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:21 @agent_ppo2.py:185][0m |           0.0032 |          16.1535 |           8.6357 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |           0.0098 |          15.8680 |           8.6194 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0041 |          14.6186 |           8.6263 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0059 |          14.4609 |           8.6167 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0085 |          14.3859 |           8.6153 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0086 |          14.3604 |           8.6111 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0060 |          14.3044 |           8.6158 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0067 |          14.3042 |           8.6106 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0050 |          14.4738 |           8.6111 |
[32m[20221213 22:31:22 @agent_ppo2.py:185][0m |          -0.0105 |          14.2288 |           8.6081 |
[32m[20221213 22:31:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.44
[32m[20221213 22:31:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.27
[32m[20221213 22:31:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.70
[32m[20221213 22:31:22 @agent_ppo2.py:143][0m Total time:      13.16 min
[32m[20221213 22:31:22 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221213 22:31:22 @agent_ppo2.py:121][0m #------------------------ Iteration 619 --------------------------#
[32m[20221213 22:31:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0010 |          17.6904 |           8.5246 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0043 |          16.8020 |           8.5131 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0083 |          16.6471 |           8.5076 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0062 |          16.5000 |           8.5030 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0060 |          16.4346 |           8.5000 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0096 |          16.2898 |           8.5015 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0145 |          16.2939 |           8.5009 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0111 |          16.2885 |           8.5067 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |          -0.0088 |          16.3665 |           8.5057 |
[32m[20221213 22:31:23 @agent_ppo2.py:185][0m |           0.0011 |          16.5753 |           8.5045 |
[32m[20221213 22:31:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.75
[32m[20221213 22:31:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.24
[32m[20221213 22:31:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.34
[32m[20221213 22:31:24 @agent_ppo2.py:143][0m Total time:      13.18 min
[32m[20221213 22:31:24 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221213 22:31:24 @agent_ppo2.py:121][0m #------------------------ Iteration 620 --------------------------#
[32m[20221213 22:31:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:31:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |          -0.0027 |          23.9217 |           8.5371 |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |           0.0014 |          22.0539 |           8.5308 |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |          -0.0063 |          21.4687 |           8.5317 |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |          -0.0075 |          21.1025 |           8.5282 |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |          -0.0077 |          20.7448 |           8.5216 |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |          -0.0118 |          20.6004 |           8.5303 |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |          -0.0098 |          20.7433 |           8.5282 |
[32m[20221213 22:31:24 @agent_ppo2.py:185][0m |          -0.0037 |          20.3619 |           8.5240 |
[32m[20221213 22:31:25 @agent_ppo2.py:185][0m |          -0.0101 |          20.1283 |           8.5275 |
[32m[20221213 22:31:25 @agent_ppo2.py:185][0m |          -0.0132 |          20.0335 |           8.5294 |
[32m[20221213 22:31:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.58
[32m[20221213 22:31:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.53
[32m[20221213 22:31:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.16
[32m[20221213 22:31:25 @agent_ppo2.py:143][0m Total time:      13.20 min
[32m[20221213 22:31:25 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221213 22:31:25 @agent_ppo2.py:121][0m #------------------------ Iteration 621 --------------------------#
[32m[20221213 22:31:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:25 @agent_ppo2.py:185][0m |          -0.0065 |          31.0572 |           8.6513 |
[32m[20221213 22:31:25 @agent_ppo2.py:185][0m |          -0.0054 |          29.4067 |           8.6328 |
[32m[20221213 22:31:25 @agent_ppo2.py:185][0m |          -0.0047 |          28.8014 |           8.6299 |
[32m[20221213 22:31:25 @agent_ppo2.py:185][0m |          -0.0055 |          28.5198 |           8.6243 |
[32m[20221213 22:31:25 @agent_ppo2.py:185][0m |          -0.0047 |          28.3453 |           8.6182 |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0084 |          28.0589 |           8.6161 |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0093 |          27.9651 |           8.6093 |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0115 |          27.7394 |           8.6050 |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0108 |          27.5581 |           8.6024 |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0081 |          27.9268 |           8.5982 |
[32m[20221213 22:31:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:31:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.34
[32m[20221213 22:31:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.01
[32m[20221213 22:31:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.16
[32m[20221213 22:31:26 @agent_ppo2.py:143][0m Total time:      13.22 min
[32m[20221213 22:31:26 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221213 22:31:26 @agent_ppo2.py:121][0m #------------------------ Iteration 622 --------------------------#
[32m[20221213 22:31:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0050 |          25.9218 |           8.6377 |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0088 |          24.2372 |           8.6302 |
[32m[20221213 22:31:26 @agent_ppo2.py:185][0m |          -0.0084 |          23.8732 |           8.6218 |
[32m[20221213 22:31:27 @agent_ppo2.py:185][0m |          -0.0095 |          23.4376 |           8.6173 |
[32m[20221213 22:31:27 @agent_ppo2.py:185][0m |          -0.0072 |          23.2339 |           8.6137 |
[32m[20221213 22:31:27 @agent_ppo2.py:185][0m |          -0.0103 |          23.1306 |           8.6164 |
[32m[20221213 22:31:27 @agent_ppo2.py:185][0m |          -0.0113 |          22.9041 |           8.6144 |
[32m[20221213 22:31:27 @agent_ppo2.py:185][0m |          -0.0079 |          22.7804 |           8.6118 |
[32m[20221213 22:31:27 @agent_ppo2.py:185][0m |          -0.0135 |          22.6988 |           8.6054 |
[32m[20221213 22:31:27 @agent_ppo2.py:185][0m |          -0.0090 |          22.5960 |           8.6075 |
[32m[20221213 22:31:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.68
[32m[20221213 22:31:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.92
[32m[20221213 22:31:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.53
[32m[20221213 22:31:27 @agent_ppo2.py:143][0m Total time:      13.24 min
[32m[20221213 22:31:27 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221213 22:31:27 @agent_ppo2.py:121][0m #------------------------ Iteration 623 --------------------------#
[32m[20221213 22:31:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:31:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |           0.0024 |          21.5547 |           8.5830 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0061 |          20.0115 |           8.5759 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0047 |          19.7454 |           8.5674 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0089 |          19.4583 |           8.5660 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0078 |          19.2166 |           8.5635 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0071 |          19.0102 |           8.5664 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0030 |          18.9618 |           8.5626 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0076 |          18.8864 |           8.5685 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0101 |          18.7079 |           8.5635 |
[32m[20221213 22:31:28 @agent_ppo2.py:185][0m |          -0.0052 |          18.7132 |           8.5669 |
[32m[20221213 22:31:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:31:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.57
[32m[20221213 22:31:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.57
[32m[20221213 22:31:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.48
[32m[20221213 22:31:29 @agent_ppo2.py:143][0m Total time:      13.26 min
[32m[20221213 22:31:29 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221213 22:31:29 @agent_ppo2.py:121][0m #------------------------ Iteration 624 --------------------------#
[32m[20221213 22:31:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:29 @agent_ppo2.py:185][0m |          -0.0027 |          18.2987 |           8.5322 |
[32m[20221213 22:31:29 @agent_ppo2.py:185][0m |          -0.0082 |          17.7712 |           8.5198 |
[32m[20221213 22:31:29 @agent_ppo2.py:185][0m |          -0.0086 |          17.5706 |           8.5208 |
[32m[20221213 22:31:29 @agent_ppo2.py:185][0m |          -0.0064 |          17.3850 |           8.5161 |
[32m[20221213 22:31:29 @agent_ppo2.py:185][0m |          -0.0067 |          17.4135 |           8.5161 |
[32m[20221213 22:31:29 @agent_ppo2.py:185][0m |          -0.0062 |          17.3109 |           8.5148 |
[32m[20221213 22:31:29 @agent_ppo2.py:185][0m |          -0.0099 |          17.2702 |           8.5165 |
[32m[20221213 22:31:30 @agent_ppo2.py:185][0m |          -0.0085 |          17.2261 |           8.5191 |
[32m[20221213 22:31:30 @agent_ppo2.py:185][0m |          -0.0067 |          17.2341 |           8.5142 |
[32m[20221213 22:31:30 @agent_ppo2.py:185][0m |          -0.0108 |          17.2386 |           8.5178 |
[32m[20221213 22:31:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:31:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.52
[32m[20221213 22:31:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.10
[32m[20221213 22:31:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 101.38
[32m[20221213 22:31:30 @agent_ppo2.py:143][0m Total time:      13.28 min
[32m[20221213 22:31:30 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221213 22:31:30 @agent_ppo2.py:121][0m #------------------------ Iteration 625 --------------------------#
[32m[20221213 22:31:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:30 @agent_ppo2.py:185][0m |          -0.0029 |          18.2740 |           8.6220 |
[32m[20221213 22:31:30 @agent_ppo2.py:185][0m |          -0.0053 |          17.6027 |           8.6086 |
[32m[20221213 22:31:30 @agent_ppo2.py:185][0m |          -0.0074 |          17.4784 |           8.5965 |
[32m[20221213 22:31:30 @agent_ppo2.py:185][0m |          -0.0071 |          17.5105 |           8.5941 |
[32m[20221213 22:31:31 @agent_ppo2.py:185][0m |           0.0031 |          18.3177 |           8.5893 |
[32m[20221213 22:31:31 @agent_ppo2.py:185][0m |          -0.0055 |          17.6098 |           8.5827 |
[32m[20221213 22:31:31 @agent_ppo2.py:185][0m |          -0.0105 |          17.2711 |           8.5820 |
[32m[20221213 22:31:31 @agent_ppo2.py:185][0m |          -0.0102 |          17.2478 |           8.5803 |
[32m[20221213 22:31:31 @agent_ppo2.py:185][0m |          -0.0123 |          17.1701 |           8.5798 |
[32m[20221213 22:31:31 @agent_ppo2.py:185][0m |          -0.0085 |          17.1687 |           8.5717 |
[32m[20221213 22:31:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:31:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.24
[32m[20221213 22:31:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.23
[32m[20221213 22:31:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.87
[32m[20221213 22:31:31 @agent_ppo2.py:143][0m Total time:      13.30 min
[32m[20221213 22:31:31 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221213 22:31:31 @agent_ppo2.py:121][0m #------------------------ Iteration 626 --------------------------#
[32m[20221213 22:31:31 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 22:31:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |           0.0003 |          20.1345 |           8.4749 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0039 |          19.6686 |           8.4632 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0037 |          19.6177 |           8.4611 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0083 |          19.4988 |           8.4502 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0086 |          19.4408 |           8.4424 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0088 |          19.3672 |           8.4462 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0099 |          19.3418 |           8.4471 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0108 |          19.2738 |           8.4402 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0105 |          19.2965 |           8.4408 |
[32m[20221213 22:31:32 @agent_ppo2.py:185][0m |          -0.0108 |          19.2882 |           8.4361 |
[32m[20221213 22:31:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:31:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.64
[32m[20221213 22:31:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.78
[32m[20221213 22:31:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.72
[32m[20221213 22:31:33 @agent_ppo2.py:143][0m Total time:      13.33 min
[32m[20221213 22:31:33 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221213 22:31:33 @agent_ppo2.py:121][0m #------------------------ Iteration 627 --------------------------#
[32m[20221213 22:31:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:31:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:33 @agent_ppo2.py:185][0m |           0.0070 |          20.8302 |           8.4508 |
[32m[20221213 22:31:33 @agent_ppo2.py:185][0m |          -0.0086 |          19.3160 |           8.4430 |
[32m[20221213 22:31:33 @agent_ppo2.py:185][0m |          -0.0069 |          19.1452 |           8.4318 |
[32m[20221213 22:31:33 @agent_ppo2.py:185][0m |          -0.0082 |          19.0763 |           8.4452 |
[32m[20221213 22:31:33 @agent_ppo2.py:185][0m |          -0.0081 |          19.0323 |           8.4422 |
[32m[20221213 22:31:33 @agent_ppo2.py:185][0m |          -0.0088 |          18.9280 |           8.4573 |
[32m[20221213 22:31:33 @agent_ppo2.py:185][0m |          -0.0100 |          18.9172 |           8.4527 |
[32m[20221213 22:31:34 @agent_ppo2.py:185][0m |          -0.0078 |          18.9942 |           8.4521 |
[32m[20221213 22:31:34 @agent_ppo2.py:185][0m |          -0.0121 |          18.8801 |           8.4543 |
[32m[20221213 22:31:34 @agent_ppo2.py:185][0m |          -0.0004 |          20.2512 |           8.4639 |
[32m[20221213 22:31:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:31:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.68
[32m[20221213 22:31:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.41
[32m[20221213 22:31:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 128.92
[32m[20221213 22:31:34 @agent_ppo2.py:143][0m Total time:      13.35 min
[32m[20221213 22:31:34 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221213 22:31:34 @agent_ppo2.py:121][0m #------------------------ Iteration 628 --------------------------#
[32m[20221213 22:31:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:34 @agent_ppo2.py:185][0m |           0.0000 |          20.6848 |           8.5880 |
[32m[20221213 22:31:34 @agent_ppo2.py:185][0m |          -0.0065 |          19.2595 |           8.5796 |
[32m[20221213 22:31:34 @agent_ppo2.py:185][0m |          -0.0080 |          18.5851 |           8.5742 |
[32m[20221213 22:31:34 @agent_ppo2.py:185][0m |          -0.0073 |          18.2180 |           8.5719 |
[32m[20221213 22:31:35 @agent_ppo2.py:185][0m |          -0.0079 |          18.1062 |           8.5755 |
[32m[20221213 22:31:35 @agent_ppo2.py:185][0m |          -0.0106 |          17.8730 |           8.5756 |
[32m[20221213 22:31:35 @agent_ppo2.py:185][0m |          -0.0082 |          17.7714 |           8.5800 |
[32m[20221213 22:31:35 @agent_ppo2.py:185][0m |          -0.0012 |          17.8349 |           8.5762 |
[32m[20221213 22:31:35 @agent_ppo2.py:185][0m |          -0.0059 |          17.6637 |           8.5696 |
[32m[20221213 22:31:35 @agent_ppo2.py:185][0m |          -0.0098 |          17.5419 |           8.5738 |
[32m[20221213 22:31:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.27
[32m[20221213 22:31:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.50
[32m[20221213 22:31:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.42
[32m[20221213 22:31:35 @agent_ppo2.py:143][0m Total time:      13.37 min
[32m[20221213 22:31:35 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221213 22:31:35 @agent_ppo2.py:121][0m #------------------------ Iteration 629 --------------------------#
[32m[20221213 22:31:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:35 @agent_ppo2.py:185][0m |          -0.0026 |          36.8440 |           8.5678 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |           0.0043 |          37.0281 |           8.5631 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |          -0.0059 |          34.3201 |           8.5587 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |          -0.0053 |          33.9399 |           8.5657 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |           0.0060 |          39.3046 |           8.5691 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |          -0.0110 |          33.6918 |           8.5553 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |          -0.0114 |          33.4024 |           8.5767 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |          -0.0093 |          33.2613 |           8.5785 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |          -0.0102 |          33.0766 |           8.5781 |
[32m[20221213 22:31:36 @agent_ppo2.py:185][0m |          -0.0120 |          32.9940 |           8.5890 |
[32m[20221213 22:31:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:31:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.26
[32m[20221213 22:31:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.15
[32m[20221213 22:31:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.09
[32m[20221213 22:31:36 @agent_ppo2.py:143][0m Total time:      13.39 min
[32m[20221213 22:31:36 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221213 22:31:36 @agent_ppo2.py:121][0m #------------------------ Iteration 630 --------------------------#
[32m[20221213 22:31:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:31:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |           0.0062 |          23.1393 |           8.8355 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |           0.0002 |          20.1189 |           8.8120 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0049 |          19.3792 |           8.8195 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0066 |          19.0750 |           8.8143 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0066 |          18.9358 |           8.8126 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0046 |          18.7287 |           8.8163 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0083 |          18.6248 |           8.8123 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0097 |          18.5423 |           8.8054 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0085 |          18.4596 |           8.8119 |
[32m[20221213 22:31:37 @agent_ppo2.py:185][0m |          -0.0082 |          18.4866 |           8.8053 |
[32m[20221213 22:31:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:31:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.10
[32m[20221213 22:31:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.33
[32m[20221213 22:31:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.23
[32m[20221213 22:31:38 @agent_ppo2.py:143][0m Total time:      13.41 min
[32m[20221213 22:31:38 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221213 22:31:38 @agent_ppo2.py:121][0m #------------------------ Iteration 631 --------------------------#
[32m[20221213 22:31:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:31:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:38 @agent_ppo2.py:185][0m |           0.0085 |          35.2221 |           8.6025 |
[32m[20221213 22:31:38 @agent_ppo2.py:185][0m |          -0.0055 |          31.3554 |           8.5937 |
[32m[20221213 22:31:38 @agent_ppo2.py:185][0m |           0.0051 |          32.7344 |           8.5868 |
[32m[20221213 22:31:38 @agent_ppo2.py:185][0m |          -0.0063 |          30.7471 |           8.6021 |
[32m[20221213 22:31:38 @agent_ppo2.py:185][0m |          -0.0076 |          30.5799 |           8.5955 |
[32m[20221213 22:31:38 @agent_ppo2.py:185][0m |          -0.0060 |          30.6150 |           8.5943 |
[32m[20221213 22:31:38 @agent_ppo2.py:185][0m |          -0.0078 |          30.4593 |           8.5953 |
[32m[20221213 22:31:39 @agent_ppo2.py:185][0m |          -0.0053 |          31.2154 |           8.5978 |
[32m[20221213 22:31:39 @agent_ppo2.py:185][0m |          -0.0090 |          30.2394 |           8.5949 |
[32m[20221213 22:31:39 @agent_ppo2.py:185][0m |          -0.0070 |          30.0711 |           8.5970 |
[32m[20221213 22:31:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:31:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.80
[32m[20221213 22:31:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.01
[32m[20221213 22:31:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.25
[32m[20221213 22:31:39 @agent_ppo2.py:143][0m Total time:      13.43 min
[32m[20221213 22:31:39 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221213 22:31:39 @agent_ppo2.py:121][0m #------------------------ Iteration 632 --------------------------#
[32m[20221213 22:31:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:39 @agent_ppo2.py:185][0m |           0.0030 |          34.2411 |           8.6639 |
[32m[20221213 22:31:39 @agent_ppo2.py:185][0m |          -0.0023 |          33.5315 |           8.6600 |
[32m[20221213 22:31:39 @agent_ppo2.py:185][0m |          -0.0082 |          33.2259 |           8.6574 |
[32m[20221213 22:31:39 @agent_ppo2.py:185][0m |          -0.0069 |          32.8755 |           8.6478 |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |          -0.0082 |          32.7657 |           8.6488 |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |          -0.0049 |          32.6250 |           8.6468 |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |          -0.0054 |          32.5765 |           8.6443 |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |          -0.0077 |          32.5709 |           8.6408 |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |          -0.0104 |          32.4677 |           8.6379 |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |          -0.0043 |          34.1700 |           8.6353 |
[32m[20221213 22:31:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.57
[32m[20221213 22:31:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 346.77
[32m[20221213 22:31:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.54
[32m[20221213 22:31:40 @agent_ppo2.py:143][0m Total time:      13.45 min
[32m[20221213 22:31:40 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221213 22:31:40 @agent_ppo2.py:121][0m #------------------------ Iteration 633 --------------------------#
[32m[20221213 22:31:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |           0.0003 |          19.6378 |           8.6372 |
[32m[20221213 22:31:40 @agent_ppo2.py:185][0m |          -0.0039 |          19.0215 |           8.6311 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0086 |          18.5900 |           8.6249 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0106 |          18.5604 |           8.6168 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0120 |          18.5375 |           8.6171 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0066 |          18.3987 |           8.6149 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0073 |          18.2881 |           8.6104 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0092 |          18.2703 |           8.6140 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0115 |          18.2317 |           8.6101 |
[32m[20221213 22:31:41 @agent_ppo2.py:185][0m |          -0.0107 |          18.1773 |           8.6166 |
[32m[20221213 22:31:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:31:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.55
[32m[20221213 22:31:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.28
[32m[20221213 22:31:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 157.00
[32m[20221213 22:31:41 @agent_ppo2.py:143][0m Total time:      13.47 min
[32m[20221213 22:31:41 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221213 22:31:41 @agent_ppo2.py:121][0m #------------------------ Iteration 634 --------------------------#
[32m[20221213 22:31:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |           0.0002 |          32.3948 |           8.6087 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |           0.0002 |          30.7678 |           8.5976 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0053 |          29.6601 |           8.5962 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0134 |          29.2876 |           8.5954 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0079 |          28.8083 |           8.5854 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0069 |          28.6449 |           8.5924 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0108 |          28.3737 |           8.5944 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0171 |          28.1090 |           8.5942 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0098 |          27.8417 |           8.5948 |
[32m[20221213 22:31:42 @agent_ppo2.py:185][0m |          -0.0151 |          27.7722 |           8.5914 |
[32m[20221213 22:31:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:31:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.24
[32m[20221213 22:31:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.29
[32m[20221213 22:31:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.08
[32m[20221213 22:31:43 @agent_ppo2.py:143][0m Total time:      13.49 min
[32m[20221213 22:31:43 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221213 22:31:43 @agent_ppo2.py:121][0m #------------------------ Iteration 635 --------------------------#
[32m[20221213 22:31:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |          -0.0020 |          21.3064 |           8.8375 |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |           0.0053 |          21.7494 |           8.8183 |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |           0.0011 |          21.0531 |           8.8186 |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |          -0.0063 |          20.1352 |           8.8181 |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |          -0.0083 |          20.1092 |           8.8146 |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |          -0.0065 |          19.9934 |           8.8165 |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |          -0.0080 |          19.9038 |           8.8196 |
[32m[20221213 22:31:43 @agent_ppo2.py:185][0m |          -0.0055 |          19.8187 |           8.8118 |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |          -0.0074 |          19.7719 |           8.8187 |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |          -0.0069 |          19.8656 |           8.8167 |
[32m[20221213 22:31:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.28
[32m[20221213 22:31:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.60
[32m[20221213 22:31:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.40
[32m[20221213 22:31:44 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 383.40
[32m[20221213 22:31:44 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 383.40
[32m[20221213 22:31:44 @agent_ppo2.py:143][0m Total time:      13.51 min
[32m[20221213 22:31:44 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221213 22:31:44 @agent_ppo2.py:121][0m #------------------------ Iteration 636 --------------------------#
[32m[20221213 22:31:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |           0.0185 |          32.1588 |           8.7140 |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |          -0.0080 |          25.2190 |           8.6936 |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |          -0.0054 |          24.1490 |           8.6924 |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |          -0.0057 |          23.6495 |           8.6943 |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |          -0.0070 |          23.3098 |           8.6893 |
[32m[20221213 22:31:44 @agent_ppo2.py:185][0m |          -0.0082 |          22.9848 |           8.6939 |
[32m[20221213 22:31:45 @agent_ppo2.py:185][0m |          -0.0077 |          22.6884 |           8.6955 |
[32m[20221213 22:31:45 @agent_ppo2.py:185][0m |          -0.0134 |          22.7431 |           8.6899 |
[32m[20221213 22:31:45 @agent_ppo2.py:185][0m |          -0.0094 |          22.3259 |           8.6911 |
[32m[20221213 22:31:45 @agent_ppo2.py:185][0m |          -0.0132 |          22.0901 |           8.6874 |
[32m[20221213 22:31:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.45
[32m[20221213 22:31:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.22
[32m[20221213 22:31:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.75
[32m[20221213 22:31:45 @agent_ppo2.py:143][0m Total time:      13.53 min
[32m[20221213 22:31:45 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221213 22:31:45 @agent_ppo2.py:121][0m #------------------------ Iteration 637 --------------------------#
[32m[20221213 22:31:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:31:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:45 @agent_ppo2.py:185][0m |           0.0054 |          39.4962 |           8.7997 |
[32m[20221213 22:31:45 @agent_ppo2.py:185][0m |          -0.0037 |          35.4590 |           8.7825 |
[32m[20221213 22:31:45 @agent_ppo2.py:185][0m |          -0.0141 |          34.7297 |           8.7891 |
[32m[20221213 22:31:46 @agent_ppo2.py:185][0m |          -0.0098 |          34.3782 |           8.7819 |
[32m[20221213 22:31:46 @agent_ppo2.py:185][0m |          -0.0109 |          33.9970 |           8.7870 |
[32m[20221213 22:31:46 @agent_ppo2.py:185][0m |          -0.0080 |          33.8908 |           8.7757 |
[32m[20221213 22:31:46 @agent_ppo2.py:185][0m |          -0.0146 |          33.6292 |           8.7664 |
[32m[20221213 22:31:46 @agent_ppo2.py:185][0m |          -0.0044 |          34.9385 |           8.7800 |
[32m[20221213 22:31:46 @agent_ppo2.py:185][0m |          -0.0143 |          33.3777 |           8.7597 |
[32m[20221213 22:31:46 @agent_ppo2.py:185][0m |          -0.0042 |          35.4568 |           8.7638 |
[32m[20221213 22:31:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:31:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.40
[32m[20221213 22:31:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.78
[32m[20221213 22:31:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.70
[32m[20221213 22:31:46 @agent_ppo2.py:143][0m Total time:      13.56 min
[32m[20221213 22:31:46 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221213 22:31:46 @agent_ppo2.py:121][0m #------------------------ Iteration 638 --------------------------#
[32m[20221213 22:31:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:31:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0002 |          27.4851 |           8.8354 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0003 |          25.0544 |           8.8266 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0043 |          24.2331 |           8.8236 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0077 |          23.0931 |           8.8184 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0137 |          22.7388 |           8.8199 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0132 |          22.4991 |           8.8191 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0097 |          22.5380 |           8.8170 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0085 |          22.1005 |           8.8138 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0048 |          24.6115 |           8.8176 |
[32m[20221213 22:31:47 @agent_ppo2.py:185][0m |          -0.0147 |          21.9405 |           8.8178 |
[32m[20221213 22:31:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:31:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.19
[32m[20221213 22:31:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.39
[32m[20221213 22:31:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.55
[32m[20221213 22:31:48 @agent_ppo2.py:143][0m Total time:      13.58 min
[32m[20221213 22:31:48 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221213 22:31:48 @agent_ppo2.py:121][0m #------------------------ Iteration 639 --------------------------#
[32m[20221213 22:31:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0023 |          21.4763 |           8.6980 |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0070 |          20.4341 |           8.6944 |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0095 |          20.2924 |           8.6798 |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0007 |          20.8943 |           8.6823 |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0052 |          20.0484 |           8.6697 |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0114 |          19.9756 |           8.6674 |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0130 |          19.8457 |           8.6554 |
[32m[20221213 22:31:48 @agent_ppo2.py:185][0m |          -0.0123 |          19.8002 |           8.6590 |
[32m[20221213 22:31:49 @agent_ppo2.py:185][0m |          -0.0108 |          19.6645 |           8.6543 |
[32m[20221213 22:31:49 @agent_ppo2.py:185][0m |          -0.0128 |          19.6528 |           8.6519 |
[32m[20221213 22:31:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:31:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.60
[32m[20221213 22:31:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.47
[32m[20221213 22:31:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.66
[32m[20221213 22:31:49 @agent_ppo2.py:143][0m Total time:      13.60 min
[32m[20221213 22:31:49 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221213 22:31:49 @agent_ppo2.py:121][0m #------------------------ Iteration 640 --------------------------#
[32m[20221213 22:31:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:31:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:49 @agent_ppo2.py:185][0m |          -0.0050 |          23.6583 |           8.7071 |
[32m[20221213 22:31:49 @agent_ppo2.py:185][0m |          -0.0047 |          21.6484 |           8.6980 |
[32m[20221213 22:31:49 @agent_ppo2.py:185][0m |          -0.0089 |          21.0387 |           8.6886 |
[32m[20221213 22:31:49 @agent_ppo2.py:185][0m |          -0.0078 |          20.6294 |           8.6825 |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |          -0.0038 |          20.3028 |           8.6783 |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |          -0.0118 |          20.1322 |           8.6732 |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |          -0.0076 |          19.8571 |           8.6706 |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |          -0.0097 |          19.8013 |           8.6724 |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |          -0.0055 |          19.8506 |           8.6678 |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |          -0.0072 |          19.6158 |           8.6679 |
[32m[20221213 22:31:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:31:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.23
[32m[20221213 22:31:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 196.54
[32m[20221213 22:31:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.57
[32m[20221213 22:31:50 @agent_ppo2.py:143][0m Total time:      13.62 min
[32m[20221213 22:31:50 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221213 22:31:50 @agent_ppo2.py:121][0m #------------------------ Iteration 641 --------------------------#
[32m[20221213 22:31:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |          -0.0003 |          31.7496 |           8.6856 |
[32m[20221213 22:31:50 @agent_ppo2.py:185][0m |           0.0001 |          31.0395 |           8.6915 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |          -0.0071 |          30.3285 |           8.6844 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |          -0.0098 |          30.2024 |           8.6884 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |          -0.0037 |          29.8361 |           8.6832 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |          -0.0031 |          30.5690 |           8.6846 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |           0.0025 |          32.1873 |           8.6793 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |          -0.0065 |          29.5343 |           8.6773 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |          -0.0048 |          29.3735 |           8.6848 |
[32m[20221213 22:31:51 @agent_ppo2.py:185][0m |          -0.0107 |          29.1823 |           8.6853 |
[32m[20221213 22:31:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:31:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.86
[32m[20221213 22:31:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.86
[32m[20221213 22:31:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.74
[32m[20221213 22:31:51 @agent_ppo2.py:143][0m Total time:      13.64 min
[32m[20221213 22:31:51 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221213 22:31:51 @agent_ppo2.py:121][0m #------------------------ Iteration 642 --------------------------#
[32m[20221213 22:31:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:31:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |           0.0013 |          30.7082 |           8.6053 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |          -0.0094 |          29.2069 |           8.5904 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |          -0.0040 |          28.7275 |           8.5832 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |          -0.0078 |          28.6110 |           8.5880 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |          -0.0056 |          28.2286 |           8.5816 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |          -0.0088 |          27.9972 |           8.5820 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |          -0.0126 |          27.8828 |           8.5773 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |          -0.0122 |          27.8007 |           8.5805 |
[32m[20221213 22:31:52 @agent_ppo2.py:185][0m |           0.0014 |          31.4293 |           8.5757 |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |          -0.0076 |          27.9023 |           8.5665 |
[32m[20221213 22:31:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:31:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.72
[32m[20221213 22:31:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.63
[32m[20221213 22:31:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.35
[32m[20221213 22:31:53 @agent_ppo2.py:143][0m Total time:      13.66 min
[32m[20221213 22:31:53 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221213 22:31:53 @agent_ppo2.py:121][0m #------------------------ Iteration 643 --------------------------#
[32m[20221213 22:31:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |           0.0034 |          25.5639 |           8.6435 |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |          -0.0042 |          23.8864 |           8.6335 |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |          -0.0104 |          23.4523 |           8.6284 |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |          -0.0084 |          23.1208 |           8.6301 |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |          -0.0044 |          22.8871 |           8.6328 |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |          -0.0063 |          22.8087 |           8.6308 |
[32m[20221213 22:31:53 @agent_ppo2.py:185][0m |          -0.0050 |          22.7043 |           8.6309 |
[32m[20221213 22:31:54 @agent_ppo2.py:185][0m |          -0.0098 |          22.5803 |           8.6331 |
[32m[20221213 22:31:54 @agent_ppo2.py:185][0m |          -0.0071 |          22.2903 |           8.6308 |
[32m[20221213 22:31:54 @agent_ppo2.py:185][0m |          -0.0152 |          22.2537 |           8.6355 |
[32m[20221213 22:31:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:31:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 205.89
[32m[20221213 22:31:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.65
[32m[20221213 22:31:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.38
[32m[20221213 22:31:54 @agent_ppo2.py:143][0m Total time:      13.68 min
[32m[20221213 22:31:54 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221213 22:31:54 @agent_ppo2.py:121][0m #------------------------ Iteration 644 --------------------------#
[32m[20221213 22:31:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:54 @agent_ppo2.py:185][0m |           0.0014 |          27.7458 |           8.7565 |
[32m[20221213 22:31:54 @agent_ppo2.py:185][0m |          -0.0060 |          25.5183 |           8.7505 |
[32m[20221213 22:31:54 @agent_ppo2.py:185][0m |           0.0020 |          25.7781 |           8.7338 |
[32m[20221213 22:31:55 @agent_ppo2.py:185][0m |          -0.0104 |          24.1961 |           8.7386 |
[32m[20221213 22:31:55 @agent_ppo2.py:185][0m |          -0.0077 |          23.9337 |           8.7365 |
[32m[20221213 22:31:55 @agent_ppo2.py:185][0m |          -0.0092 |          23.6538 |           8.7322 |
[32m[20221213 22:31:55 @agent_ppo2.py:185][0m |          -0.0080 |          23.5298 |           8.7338 |
[32m[20221213 22:31:55 @agent_ppo2.py:185][0m |          -0.0092 |          23.3264 |           8.7323 |
[32m[20221213 22:31:55 @agent_ppo2.py:185][0m |          -0.0145 |          23.1486 |           8.7334 |
[32m[20221213 22:31:55 @agent_ppo2.py:185][0m |          -0.0112 |          22.8887 |           8.7296 |
[32m[20221213 22:31:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:31:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 141.12
[32m[20221213 22:31:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.90
[32m[20221213 22:31:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.71
[32m[20221213 22:31:55 @agent_ppo2.py:143][0m Total time:      13.70 min
[32m[20221213 22:31:55 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221213 22:31:55 @agent_ppo2.py:121][0m #------------------------ Iteration 645 --------------------------#
[32m[20221213 22:31:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |           0.0014 |          29.7936 |           8.6705 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0038 |          28.5963 |           8.6627 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0025 |          28.1394 |           8.6625 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0072 |          27.7931 |           8.6608 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0068 |          27.6254 |           8.6617 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0092 |          27.5512 |           8.6623 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0081 |          27.3777 |           8.6639 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0055 |          27.2298 |           8.6518 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |          -0.0071 |          27.1168 |           8.6593 |
[32m[20221213 22:31:56 @agent_ppo2.py:185][0m |           0.0007 |          29.4634 |           8.6575 |
[32m[20221213 22:31:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:31:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.27
[32m[20221213 22:31:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.63
[32m[20221213 22:31:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.28
[32m[20221213 22:31:56 @agent_ppo2.py:143][0m Total time:      13.72 min
[32m[20221213 22:31:56 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221213 22:31:56 @agent_ppo2.py:121][0m #------------------------ Iteration 646 --------------------------#
[32m[20221213 22:31:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0031 |          22.6301 |           8.7161 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0001 |          20.0525 |           8.7087 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0086 |          19.4851 |           8.7055 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0112 |          19.0112 |           8.6999 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0042 |          19.5396 |           8.6980 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0087 |          18.6850 |           8.6882 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0091 |          18.4889 |           8.6902 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0013 |          18.9026 |           8.6814 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0073 |          18.3658 |           8.6859 |
[32m[20221213 22:31:57 @agent_ppo2.py:185][0m |          -0.0092 |          18.1110 |           8.6759 |
[32m[20221213 22:31:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:31:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.63
[32m[20221213 22:31:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.86
[32m[20221213 22:31:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.52
[32m[20221213 22:31:58 @agent_ppo2.py:143][0m Total time:      13.75 min
[32m[20221213 22:31:58 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221213 22:31:58 @agent_ppo2.py:121][0m #------------------------ Iteration 647 --------------------------#
[32m[20221213 22:31:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:58 @agent_ppo2.py:185][0m |           0.0005 |          17.6530 |           8.5865 |
[32m[20221213 22:31:58 @agent_ppo2.py:185][0m |          -0.0031 |          16.5381 |           8.5714 |
[32m[20221213 22:31:58 @agent_ppo2.py:185][0m |          -0.0067 |          15.5121 |           8.5753 |
[32m[20221213 22:31:58 @agent_ppo2.py:185][0m |          -0.0065 |          15.1322 |           8.5711 |
[32m[20221213 22:31:58 @agent_ppo2.py:185][0m |          -0.0060 |          14.9084 |           8.5768 |
[32m[20221213 22:31:58 @agent_ppo2.py:185][0m |          -0.0006 |          17.3607 |           8.5800 |
[32m[20221213 22:31:58 @agent_ppo2.py:185][0m |           0.0032 |          15.2317 |           8.5692 |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |          -0.0146 |          14.4366 |           8.5777 |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |          -0.0061 |          14.3609 |           8.5721 |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |          -0.0151 |          14.2007 |           8.5742 |
[32m[20221213 22:31:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:31:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.23
[32m[20221213 22:31:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.35
[32m[20221213 22:31:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.62
[32m[20221213 22:31:59 @agent_ppo2.py:143][0m Total time:      13.77 min
[32m[20221213 22:31:59 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221213 22:31:59 @agent_ppo2.py:121][0m #------------------------ Iteration 648 --------------------------#
[32m[20221213 22:31:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:31:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |           0.0067 |          21.1544 |           8.7120 |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |          -0.0060 |          16.3777 |           8.6990 |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |          -0.0028 |          15.5614 |           8.6909 |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |          -0.0045 |          15.1202 |           8.6923 |
[32m[20221213 22:31:59 @agent_ppo2.py:185][0m |          -0.0122 |          14.9151 |           8.6873 |
[32m[20221213 22:32:00 @agent_ppo2.py:185][0m |          -0.0058 |          14.6144 |           8.6883 |
[32m[20221213 22:32:00 @agent_ppo2.py:185][0m |          -0.0071 |          14.3953 |           8.6789 |
[32m[20221213 22:32:00 @agent_ppo2.py:185][0m |          -0.0093 |          14.2288 |           8.6779 |
[32m[20221213 22:32:00 @agent_ppo2.py:185][0m |          -0.0100 |          13.9935 |           8.6786 |
[32m[20221213 22:32:00 @agent_ppo2.py:185][0m |          -0.0136 |          14.0110 |           8.6758 |
[32m[20221213 22:32:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.41
[32m[20221213 22:32:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.41
[32m[20221213 22:32:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.41
[32m[20221213 22:32:00 @agent_ppo2.py:143][0m Total time:      13.79 min
[32m[20221213 22:32:00 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221213 22:32:00 @agent_ppo2.py:121][0m #------------------------ Iteration 649 --------------------------#
[32m[20221213 22:32:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:00 @agent_ppo2.py:185][0m |           0.0059 |          27.3211 |           8.7242 |
[32m[20221213 22:32:00 @agent_ppo2.py:185][0m |          -0.0049 |          24.2331 |           8.7201 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |          -0.0048 |          23.3756 |           8.7107 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |          -0.0064 |          22.9273 |           8.7061 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |          -0.0064 |          22.7362 |           8.7145 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |          -0.0098 |          22.4438 |           8.7007 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |           0.0058 |          24.9672 |           8.7112 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |          -0.0069 |          22.2408 |           8.7062 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |          -0.0103 |          21.8738 |           8.7044 |
[32m[20221213 22:32:01 @agent_ppo2.py:185][0m |          -0.0098 |          21.8970 |           8.7054 |
[32m[20221213 22:32:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:32:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.01
[32m[20221213 22:32:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.99
[32m[20221213 22:32:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.13
[32m[20221213 22:32:01 @agent_ppo2.py:143][0m Total time:      13.81 min
[32m[20221213 22:32:01 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221213 22:32:01 @agent_ppo2.py:121][0m #------------------------ Iteration 650 --------------------------#
[32m[20221213 22:32:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:32:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |           0.0091 |          28.0608 |           8.7404 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0054 |          25.3935 |           8.7198 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0020 |          25.0257 |           8.7186 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0068 |          24.8002 |           8.7145 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0094 |          24.6118 |           8.7130 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |           0.0003 |          25.5460 |           8.7203 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0095 |          24.5844 |           8.7116 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0082 |          24.4627 |           8.7127 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0100 |          24.2073 |           8.7110 |
[32m[20221213 22:32:02 @agent_ppo2.py:185][0m |          -0.0094 |          24.1669 |           8.7103 |
[32m[20221213 22:32:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.05
[32m[20221213 22:32:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.71
[32m[20221213 22:32:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.41
[32m[20221213 22:32:03 @agent_ppo2.py:143][0m Total time:      13.83 min
[32m[20221213 22:32:03 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221213 22:32:03 @agent_ppo2.py:121][0m #------------------------ Iteration 651 --------------------------#
[32m[20221213 22:32:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |          -0.0024 |          24.7146 |           8.7003 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |          -0.0056 |          24.1961 |           8.6840 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |           0.0013 |          25.0640 |           8.6770 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |           0.0011 |          24.4611 |           8.6814 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |          -0.0078 |          23.8991 |           8.6812 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |          -0.0085 |          23.7988 |           8.6735 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |          -0.0110 |          23.7571 |           8.6649 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |          -0.0093 |          23.6967 |           8.6654 |
[32m[20221213 22:32:03 @agent_ppo2.py:185][0m |          -0.0087 |          23.5827 |           8.6702 |
[32m[20221213 22:32:04 @agent_ppo2.py:185][0m |           0.0047 |          26.3364 |           8.6552 |
[32m[20221213 22:32:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.30
[32m[20221213 22:32:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.50
[32m[20221213 22:32:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.49
[32m[20221213 22:32:04 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221213 22:32:04 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221213 22:32:04 @agent_ppo2.py:121][0m #------------------------ Iteration 652 --------------------------#
[32m[20221213 22:32:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:04 @agent_ppo2.py:185][0m |           0.0002 |          32.4362 |           8.7081 |
[32m[20221213 22:32:04 @agent_ppo2.py:185][0m |          -0.0058 |          31.3051 |           8.6924 |
[32m[20221213 22:32:04 @agent_ppo2.py:185][0m |          -0.0091 |          31.1009 |           8.6754 |
[32m[20221213 22:32:04 @agent_ppo2.py:185][0m |          -0.0002 |          34.5589 |           8.6718 |
[32m[20221213 22:32:04 @agent_ppo2.py:185][0m |          -0.0045 |          31.0963 |           8.6492 |
[32m[20221213 22:32:04 @agent_ppo2.py:185][0m |          -0.0067 |          30.6096 |           8.6634 |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |          -0.0035 |          31.3951 |           8.6612 |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |          -0.0030 |          31.8658 |           8.6554 |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |          -0.0061 |          30.5435 |           8.6534 |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |          -0.0147 |          30.4454 |           8.6473 |
[32m[20221213 22:32:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.84
[32m[20221213 22:32:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.01
[32m[20221213 22:32:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.87
[32m[20221213 22:32:05 @agent_ppo2.py:143][0m Total time:      13.87 min
[32m[20221213 22:32:05 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221213 22:32:05 @agent_ppo2.py:121][0m #------------------------ Iteration 653 --------------------------#
[32m[20221213 22:32:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |           0.0072 |          17.3791 |           8.6166 |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |          -0.0082 |          15.2679 |           8.6026 |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |           0.0006 |          15.4983 |           8.5929 |
[32m[20221213 22:32:05 @agent_ppo2.py:185][0m |          -0.0036 |          14.7243 |           8.5877 |
[32m[20221213 22:32:06 @agent_ppo2.py:185][0m |          -0.0078 |          14.6396 |           8.5867 |
[32m[20221213 22:32:06 @agent_ppo2.py:185][0m |          -0.0091 |          14.4977 |           8.5867 |
[32m[20221213 22:32:06 @agent_ppo2.py:185][0m |          -0.0132 |          14.4349 |           8.5825 |
[32m[20221213 22:32:06 @agent_ppo2.py:185][0m |          -0.0092 |          14.4319 |           8.5834 |
[32m[20221213 22:32:06 @agent_ppo2.py:185][0m |          -0.0108 |          14.2043 |           8.5840 |
[32m[20221213 22:32:06 @agent_ppo2.py:185][0m |          -0.0091 |          14.2115 |           8.5813 |
[32m[20221213 22:32:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.83
[32m[20221213 22:32:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.38
[32m[20221213 22:32:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.96
[32m[20221213 22:32:06 @agent_ppo2.py:143][0m Total time:      13.89 min
[32m[20221213 22:32:06 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221213 22:32:06 @agent_ppo2.py:121][0m #------------------------ Iteration 654 --------------------------#
[32m[20221213 22:32:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:06 @agent_ppo2.py:185][0m |           0.0025 |          25.8729 |           8.6259 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0027 |          24.1084 |           8.6132 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0038 |          23.5510 |           8.6122 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0068 |          23.1646 |           8.6092 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0046 |          22.8856 |           8.6012 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0057 |          22.8852 |           8.6062 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0025 |          22.6535 |           8.6124 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0092 |          22.5242 |           8.6027 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |           0.0000 |          23.8632 |           8.6066 |
[32m[20221213 22:32:07 @agent_ppo2.py:185][0m |          -0.0068 |          22.3351 |           8.5968 |
[32m[20221213 22:32:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:32:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.44
[32m[20221213 22:32:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.46
[32m[20221213 22:32:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.71
[32m[20221213 22:32:07 @agent_ppo2.py:143][0m Total time:      13.91 min
[32m[20221213 22:32:07 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221213 22:32:07 @agent_ppo2.py:121][0m #------------------------ Iteration 655 --------------------------#
[32m[20221213 22:32:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |           0.0142 |          20.2965 |           8.6837 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0054 |          17.3039 |           8.6730 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0036 |          16.8511 |           8.6693 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0098 |          16.5939 |           8.6664 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0095 |          16.4890 |           8.6605 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0092 |          16.3181 |           8.6573 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0093 |          16.2203 |           8.6613 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0119 |          16.1494 |           8.6580 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0095 |          16.0549 |           8.6538 |
[32m[20221213 22:32:08 @agent_ppo2.py:185][0m |          -0.0022 |          16.3890 |           8.6578 |
[32m[20221213 22:32:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:32:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.08
[32m[20221213 22:32:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.52
[32m[20221213 22:32:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.49
[32m[20221213 22:32:09 @agent_ppo2.py:143][0m Total time:      13.93 min
[32m[20221213 22:32:09 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221213 22:32:09 @agent_ppo2.py:121][0m #------------------------ Iteration 656 --------------------------#
[32m[20221213 22:32:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |          -0.0038 |          20.4577 |           8.5437 |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |          -0.0122 |          19.8264 |           8.5183 |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |          -0.0105 |          19.6028 |           8.5135 |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |          -0.0043 |          19.5001 |           8.5117 |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |          -0.0072 |          19.9310 |           8.5185 |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |           0.0008 |          20.3354 |           8.5114 |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |          -0.0101 |          19.3240 |           8.5137 |
[32m[20221213 22:32:09 @agent_ppo2.py:185][0m |          -0.0120 |          19.3471 |           8.5174 |
[32m[20221213 22:32:10 @agent_ppo2.py:185][0m |          -0.0059 |          19.3325 |           8.5169 |
[32m[20221213 22:32:10 @agent_ppo2.py:185][0m |          -0.0084 |          19.1093 |           8.5126 |
[32m[20221213 22:32:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.24
[32m[20221213 22:32:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.29
[32m[20221213 22:32:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.82
[32m[20221213 22:32:10 @agent_ppo2.py:143][0m Total time:      13.95 min
[32m[20221213 22:32:10 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221213 22:32:10 @agent_ppo2.py:121][0m #------------------------ Iteration 657 --------------------------#
[32m[20221213 22:32:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:10 @agent_ppo2.py:185][0m |          -0.0024 |          21.3186 |           8.6650 |
[32m[20221213 22:32:10 @agent_ppo2.py:185][0m |          -0.0016 |          20.1474 |           8.6348 |
[32m[20221213 22:32:10 @agent_ppo2.py:185][0m |          -0.0041 |          19.7885 |           8.6487 |
[32m[20221213 22:32:10 @agent_ppo2.py:185][0m |          -0.0090 |          19.5871 |           8.6360 |
[32m[20221213 22:32:10 @agent_ppo2.py:185][0m |          -0.0085 |          19.5126 |           8.6429 |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |          -0.0093 |          19.4047 |           8.6319 |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |          -0.0124 |          19.3394 |           8.6418 |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |          -0.0109 |          19.2094 |           8.6343 |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |          -0.0091 |          19.1953 |           8.6307 |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |          -0.0124 |          19.2087 |           8.6406 |
[32m[20221213 22:32:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:32:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.58
[32m[20221213 22:32:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.15
[32m[20221213 22:32:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.49
[32m[20221213 22:32:11 @agent_ppo2.py:143][0m Total time:      13.97 min
[32m[20221213 22:32:11 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221213 22:32:11 @agent_ppo2.py:121][0m #------------------------ Iteration 658 --------------------------#
[32m[20221213 22:32:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |          -0.0033 |          23.5923 |           8.6747 |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |          -0.0068 |          22.6462 |           8.6594 |
[32m[20221213 22:32:11 @agent_ppo2.py:185][0m |           0.0010 |          22.2702 |           8.6572 |
[32m[20221213 22:32:12 @agent_ppo2.py:185][0m |          -0.0018 |          23.4059 |           8.6614 |
[32m[20221213 22:32:12 @agent_ppo2.py:185][0m |           0.0011 |          22.6190 |           8.6596 |
[32m[20221213 22:32:12 @agent_ppo2.py:185][0m |          -0.0052 |          21.7525 |           8.6575 |
[32m[20221213 22:32:12 @agent_ppo2.py:185][0m |          -0.0007 |          22.0887 |           8.6624 |
[32m[20221213 22:32:12 @agent_ppo2.py:185][0m |           0.0035 |          24.5311 |           8.6573 |
[32m[20221213 22:32:12 @agent_ppo2.py:185][0m |          -0.0035 |          21.9707 |           8.6451 |
[32m[20221213 22:32:12 @agent_ppo2.py:185][0m |          -0.0107 |          21.1730 |           8.6586 |
[32m[20221213 22:32:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.99
[32m[20221213 22:32:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.38
[32m[20221213 22:32:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.07
[32m[20221213 22:32:12 @agent_ppo2.py:143][0m Total time:      13.99 min
[32m[20221213 22:32:12 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221213 22:32:12 @agent_ppo2.py:121][0m #------------------------ Iteration 659 --------------------------#
[32m[20221213 22:32:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0015 |          29.5992 |           8.6692 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0038 |          27.3081 |           8.6524 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0067 |          26.5055 |           8.6517 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0089 |          26.2131 |           8.6481 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0035 |          26.6069 |           8.6533 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0125 |          25.7251 |           8.6447 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0101 |          25.3942 |           8.6479 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0114 |          25.2520 |           8.6477 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0177 |          25.1857 |           8.6459 |
[32m[20221213 22:32:13 @agent_ppo2.py:185][0m |          -0.0080 |          26.1727 |           8.6465 |
[32m[20221213 22:32:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:32:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.41
[32m[20221213 22:32:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.98
[32m[20221213 22:32:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.86
[32m[20221213 22:32:13 @agent_ppo2.py:143][0m Total time:      14.01 min
[32m[20221213 22:32:13 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221213 22:32:13 @agent_ppo2.py:121][0m #------------------------ Iteration 660 --------------------------#
[32m[20221213 22:32:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:32:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0017 |          30.3541 |           8.7439 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0054 |          29.1731 |           8.7352 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0092 |          29.0475 |           8.7340 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0039 |          29.4809 |           8.7375 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0035 |          28.5925 |           8.7223 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0082 |          28.2616 |           8.7264 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0102 |          28.0379 |           8.7284 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0081 |          27.9953 |           8.7284 |
[32m[20221213 22:32:14 @agent_ppo2.py:185][0m |          -0.0053 |          28.0885 |           8.7263 |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |          -0.0102 |          27.9047 |           8.7209 |
[32m[20221213 22:32:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.52
[32m[20221213 22:32:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 307.68
[32m[20221213 22:32:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.23
[32m[20221213 22:32:15 @agent_ppo2.py:143][0m Total time:      14.03 min
[32m[20221213 22:32:15 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221213 22:32:15 @agent_ppo2.py:121][0m #------------------------ Iteration 661 --------------------------#
[32m[20221213 22:32:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |           0.0071 |          28.9399 |           8.5370 |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |          -0.0074 |          27.0518 |           8.5373 |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |          -0.0100 |          26.4360 |           8.5338 |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |          -0.0036 |          26.2511 |           8.5338 |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |          -0.0092 |          25.6658 |           8.5350 |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |          -0.0120 |          25.4142 |           8.5337 |
[32m[20221213 22:32:15 @agent_ppo2.py:185][0m |          -0.0049 |          25.3633 |           8.5410 |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |          -0.0134 |          25.0622 |           8.5331 |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |          -0.0125 |          24.9455 |           8.5426 |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |          -0.0092 |          24.8298 |           8.5479 |
[32m[20221213 22:32:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.55
[32m[20221213 22:32:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.87
[32m[20221213 22:32:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.37
[32m[20221213 22:32:16 @agent_ppo2.py:143][0m Total time:      14.05 min
[32m[20221213 22:32:16 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221213 22:32:16 @agent_ppo2.py:121][0m #------------------------ Iteration 662 --------------------------#
[32m[20221213 22:32:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |           0.0003 |          34.6182 |           8.9471 |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |          -0.0058 |          33.2374 |           8.9404 |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |          -0.0054 |          32.7330 |           8.9352 |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |          -0.0079 |          32.3198 |           8.9466 |
[32m[20221213 22:32:16 @agent_ppo2.py:185][0m |          -0.0098 |          31.9930 |           8.9337 |
[32m[20221213 22:32:17 @agent_ppo2.py:185][0m |          -0.0057 |          32.2436 |           8.9429 |
[32m[20221213 22:32:17 @agent_ppo2.py:185][0m |          -0.0115 |          31.4197 |           8.9414 |
[32m[20221213 22:32:17 @agent_ppo2.py:185][0m |          -0.0110 |          31.2487 |           8.9473 |
[32m[20221213 22:32:17 @agent_ppo2.py:185][0m |          -0.0109 |          31.0908 |           8.9405 |
[32m[20221213 22:32:17 @agent_ppo2.py:185][0m |          -0.0123 |          30.9845 |           8.9472 |
[32m[20221213 22:32:17 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.44
[32m[20221213 22:32:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.63
[32m[20221213 22:32:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 74.99
[32m[20221213 22:32:17 @agent_ppo2.py:143][0m Total time:      14.07 min
[32m[20221213 22:32:17 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221213 22:32:17 @agent_ppo2.py:121][0m #------------------------ Iteration 663 --------------------------#
[32m[20221213 22:32:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:17 @agent_ppo2.py:185][0m |           0.0147 |          34.1391 |           8.8455 |
[32m[20221213 22:32:17 @agent_ppo2.py:185][0m |          -0.0034 |          30.6591 |           8.8351 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |          -0.0062 |          30.2226 |           8.8377 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |          -0.0070 |          29.8228 |           8.8333 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |          -0.0077 |          29.6207 |           8.8378 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |          -0.0109 |          29.5747 |           8.8312 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |           0.0028 |          30.8606 |           8.8335 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |          -0.0101 |          29.3466 |           8.8294 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |          -0.0082 |          29.3095 |           8.8315 |
[32m[20221213 22:32:18 @agent_ppo2.py:185][0m |          -0.0120 |          29.2054 |           8.8352 |
[32m[20221213 22:32:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.25
[32m[20221213 22:32:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.48
[32m[20221213 22:32:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.89
[32m[20221213 22:32:18 @agent_ppo2.py:143][0m Total time:      14.09 min
[32m[20221213 22:32:18 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221213 22:32:18 @agent_ppo2.py:121][0m #------------------------ Iteration 664 --------------------------#
[32m[20221213 22:32:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |           0.0037 |          20.4205 |           8.9328 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0061 |          19.0686 |           8.9205 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0084 |          18.7210 |           8.9194 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0090 |          18.5082 |           8.9216 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0091 |          18.3971 |           8.9220 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0004 |          19.0517 |           8.9192 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0116 |          18.2964 |           8.9128 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0026 |          19.5898 |           8.9200 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |           0.0007 |          19.2256 |           8.9137 |
[32m[20221213 22:32:19 @agent_ppo2.py:185][0m |          -0.0065 |          18.0920 |           8.9143 |
[32m[20221213 22:32:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:32:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.21
[32m[20221213 22:32:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.23
[32m[20221213 22:32:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.29
[32m[20221213 22:32:19 @agent_ppo2.py:143][0m Total time:      14.11 min
[32m[20221213 22:32:19 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221213 22:32:19 @agent_ppo2.py:121][0m #------------------------ Iteration 665 --------------------------#
[32m[20221213 22:32:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |          -0.0030 |          26.2076 |           8.7873 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |          -0.0068 |          25.7229 |           8.7767 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |           0.0009 |          26.3630 |           8.7725 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |           0.0021 |          28.1683 |           8.7713 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |          -0.0079 |          25.3330 |           8.7644 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |           0.0022 |          27.1346 |           8.7636 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |          -0.0091 |          25.2077 |           8.7511 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |          -0.0069 |          25.0148 |           8.7570 |
[32m[20221213 22:32:20 @agent_ppo2.py:185][0m |          -0.0096 |          25.0562 |           8.7531 |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |          -0.0040 |          25.2839 |           8.7605 |
[32m[20221213 22:32:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.85
[32m[20221213 22:32:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.51
[32m[20221213 22:32:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.41
[32m[20221213 22:32:21 @agent_ppo2.py:143][0m Total time:      14.13 min
[32m[20221213 22:32:21 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221213 22:32:21 @agent_ppo2.py:121][0m #------------------------ Iteration 666 --------------------------#
[32m[20221213 22:32:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |           0.0112 |          18.9896 |           8.8207 |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |           0.0054 |          19.2634 |           8.8147 |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |          -0.0016 |          18.0704 |           8.8033 |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |          -0.0055 |          17.3613 |           8.8053 |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |          -0.0077 |          17.3285 |           8.7979 |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |          -0.0076 |          17.2331 |           8.8075 |
[32m[20221213 22:32:21 @agent_ppo2.py:185][0m |          -0.0081 |          17.3180 |           8.8042 |
[32m[20221213 22:32:22 @agent_ppo2.py:185][0m |          -0.0108 |          17.2069 |           8.7955 |
[32m[20221213 22:32:22 @agent_ppo2.py:185][0m |          -0.0069 |          17.2393 |           8.7902 |
[32m[20221213 22:32:22 @agent_ppo2.py:185][0m |          -0.0054 |          17.3434 |           8.8029 |
[32m[20221213 22:32:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.59
[32m[20221213 22:32:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.26
[32m[20221213 22:32:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.04
[32m[20221213 22:32:22 @agent_ppo2.py:143][0m Total time:      14.15 min
[32m[20221213 22:32:22 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221213 22:32:22 @agent_ppo2.py:121][0m #------------------------ Iteration 667 --------------------------#
[32m[20221213 22:32:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:22 @agent_ppo2.py:185][0m |           0.0025 |          29.7229 |           8.8176 |
[32m[20221213 22:32:22 @agent_ppo2.py:185][0m |          -0.0034 |          28.0950 |           8.8057 |
[32m[20221213 22:32:22 @agent_ppo2.py:185][0m |          -0.0110 |          27.8146 |           8.8022 |
[32m[20221213 22:32:22 @agent_ppo2.py:185][0m |           0.0000 |          27.6173 |           8.8014 |
[32m[20221213 22:32:23 @agent_ppo2.py:185][0m |           0.0052 |          29.5618 |           8.8005 |
[32m[20221213 22:32:23 @agent_ppo2.py:185][0m |          -0.0090 |          27.4977 |           8.7929 |
[32m[20221213 22:32:23 @agent_ppo2.py:185][0m |          -0.0090 |          27.1247 |           8.8014 |
[32m[20221213 22:32:23 @agent_ppo2.py:185][0m |           0.0101 |          30.8831 |           8.7942 |
[32m[20221213 22:32:23 @agent_ppo2.py:185][0m |          -0.0079 |          26.9882 |           8.7934 |
[32m[20221213 22:32:23 @agent_ppo2.py:185][0m |          -0.0080 |          26.9611 |           8.8005 |
[32m[20221213 22:32:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.99
[32m[20221213 22:32:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.78
[32m[20221213 22:32:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.10
[32m[20221213 22:32:23 @agent_ppo2.py:143][0m Total time:      14.17 min
[32m[20221213 22:32:23 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221213 22:32:23 @agent_ppo2.py:121][0m #------------------------ Iteration 668 --------------------------#
[32m[20221213 22:32:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:23 @agent_ppo2.py:185][0m |           0.0003 |          19.4490 |           8.8345 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0060 |          18.6193 |           8.8298 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0058 |          18.4235 |           8.8288 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0077 |          18.3561 |           8.8286 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0038 |          18.3175 |           8.8158 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0039 |          18.1900 |           8.8216 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0098 |          18.0974 |           8.8219 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0071 |          18.1605 |           8.8177 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0141 |          18.0113 |           8.8198 |
[32m[20221213 22:32:24 @agent_ppo2.py:185][0m |          -0.0100 |          17.9604 |           8.8129 |
[32m[20221213 22:32:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.17
[32m[20221213 22:32:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.12
[32m[20221213 22:32:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.18
[32m[20221213 22:32:24 @agent_ppo2.py:143][0m Total time:      14.19 min
[32m[20221213 22:32:24 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221213 22:32:24 @agent_ppo2.py:121][0m #------------------------ Iteration 669 --------------------------#
[32m[20221213 22:32:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0013 |          23.7440 |           8.9543 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0052 |          23.1379 |           8.9357 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0041 |          23.0189 |           8.9440 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |           0.0039 |          24.1686 |           8.9295 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |           0.0054 |          25.7806 |           8.9208 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0083 |          22.7119 |           8.9213 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0105 |          22.4752 |           8.9169 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0089 |          22.3380 |           8.9361 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0061 |          22.5419 |           8.9386 |
[32m[20221213 22:32:25 @agent_ppo2.py:185][0m |          -0.0071 |          22.3602 |           8.9318 |
[32m[20221213 22:32:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:32:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.97
[32m[20221213 22:32:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.17
[32m[20221213 22:32:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.25
[32m[20221213 22:32:26 @agent_ppo2.py:143][0m Total time:      14.21 min
[32m[20221213 22:32:26 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221213 22:32:26 @agent_ppo2.py:121][0m #------------------------ Iteration 670 --------------------------#
[32m[20221213 22:32:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:32:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0003 |          22.9831 |           8.8184 |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0012 |          22.7306 |           8.8000 |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0061 |          22.0879 |           8.7965 |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0090 |          21.8969 |           8.8048 |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0085 |          21.7611 |           8.8004 |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0107 |          21.6756 |           8.7961 |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0042 |          21.7906 |           8.7977 |
[32m[20221213 22:32:26 @agent_ppo2.py:185][0m |          -0.0114 |          21.4489 |           8.8068 |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0125 |          21.3037 |           8.8011 |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0054 |          21.6973 |           8.7935 |
[32m[20221213 22:32:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:32:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.21
[32m[20221213 22:32:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.46
[32m[20221213 22:32:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.56
[32m[20221213 22:32:27 @agent_ppo2.py:143][0m Total time:      14.23 min
[32m[20221213 22:32:27 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221213 22:32:27 @agent_ppo2.py:121][0m #------------------------ Iteration 671 --------------------------#
[32m[20221213 22:32:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0019 |          25.6460 |           8.8292 |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0030 |          25.1826 |           8.8207 |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0064 |          24.6947 |           8.8132 |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0081 |          24.4920 |           8.8109 |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0015 |          25.1003 |           8.8002 |
[32m[20221213 22:32:27 @agent_ppo2.py:185][0m |          -0.0022 |          24.6716 |           8.7963 |
[32m[20221213 22:32:28 @agent_ppo2.py:185][0m |          -0.0084 |          24.0041 |           8.8032 |
[32m[20221213 22:32:28 @agent_ppo2.py:185][0m |          -0.0088 |          23.8478 |           8.7978 |
[32m[20221213 22:32:28 @agent_ppo2.py:185][0m |          -0.0013 |          25.2537 |           8.7895 |
[32m[20221213 22:32:28 @agent_ppo2.py:185][0m |          -0.0097 |          23.7427 |           8.7930 |
[32m[20221213 22:32:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.68
[32m[20221213 22:32:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.80
[32m[20221213 22:32:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.41
[32m[20221213 22:32:28 @agent_ppo2.py:143][0m Total time:      14.25 min
[32m[20221213 22:32:28 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221213 22:32:28 @agent_ppo2.py:121][0m #------------------------ Iteration 672 --------------------------#
[32m[20221213 22:32:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:28 @agent_ppo2.py:185][0m |          -0.0036 |          29.0402 |           8.8476 |
[32m[20221213 22:32:28 @agent_ppo2.py:185][0m |          -0.0049 |          26.1907 |           8.8364 |
[32m[20221213 22:32:28 @agent_ppo2.py:185][0m |          -0.0006 |          25.5068 |           8.8342 |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |          -0.0086 |          25.0653 |           8.8377 |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |          -0.0114 |          24.8711 |           8.8343 |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |           0.0047 |          26.4247 |           8.8400 |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |          -0.0069 |          24.3815 |           8.8415 |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |          -0.0022 |          25.3577 |           8.8331 |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |          -0.0081 |          24.1583 |           8.8324 |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |          -0.0046 |          23.8883 |           8.8314 |
[32m[20221213 22:32:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:32:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.21
[32m[20221213 22:32:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.42
[32m[20221213 22:32:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.67
[32m[20221213 22:32:29 @agent_ppo2.py:143][0m Total time:      14.27 min
[32m[20221213 22:32:29 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221213 22:32:29 @agent_ppo2.py:121][0m #------------------------ Iteration 673 --------------------------#
[32m[20221213 22:32:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:29 @agent_ppo2.py:185][0m |           0.0022 |          28.7466 |           8.8510 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |          -0.0001 |          26.9194 |           8.8412 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |          -0.0036 |          26.4200 |           8.8381 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |           0.0004 |          26.4649 |           8.8369 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |          -0.0009 |          26.5429 |           8.8289 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |           0.0023 |          28.9078 |           8.8351 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |          -0.0068 |          25.8503 |           8.8191 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |          -0.0022 |          25.3753 |           8.8235 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |          -0.0046 |          25.8477 |           8.8314 |
[32m[20221213 22:32:30 @agent_ppo2.py:185][0m |          -0.0099 |          25.1879 |           8.8309 |
[32m[20221213 22:32:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:32:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.09
[32m[20221213 22:32:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 315.90
[32m[20221213 22:32:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.59
[32m[20221213 22:32:30 @agent_ppo2.py:143][0m Total time:      14.29 min
[32m[20221213 22:32:30 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221213 22:32:30 @agent_ppo2.py:121][0m #------------------------ Iteration 674 --------------------------#
[32m[20221213 22:32:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |           0.0275 |          45.8734 |           8.7756 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0029 |          34.5127 |           8.7375 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |           0.0053 |          36.1626 |           8.7564 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0079 |          33.1647 |           8.7544 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0083 |          32.7811 |           8.7520 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0069 |          32.6378 |           8.7502 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0083 |          32.5356 |           8.7525 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0103 |          32.4268 |           8.7520 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0050 |          34.5114 |           8.7494 |
[32m[20221213 22:32:31 @agent_ppo2.py:185][0m |          -0.0101 |          32.3088 |           8.7455 |
[32m[20221213 22:32:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:32:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.14
[32m[20221213 22:32:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.62
[32m[20221213 22:32:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.55
[32m[20221213 22:32:32 @agent_ppo2.py:143][0m Total time:      14.31 min
[32m[20221213 22:32:32 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221213 22:32:32 @agent_ppo2.py:121][0m #------------------------ Iteration 675 --------------------------#
[32m[20221213 22:32:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:32 @agent_ppo2.py:185][0m |          -0.0026 |          23.3217 |           8.8578 |
[32m[20221213 22:32:32 @agent_ppo2.py:185][0m |           0.0049 |          23.9061 |           8.8239 |
[32m[20221213 22:32:32 @agent_ppo2.py:185][0m |          -0.0083 |          22.5333 |           8.8174 |
[32m[20221213 22:32:32 @agent_ppo2.py:185][0m |          -0.0087 |          22.2751 |           8.8086 |
[32m[20221213 22:32:32 @agent_ppo2.py:185][0m |          -0.0093 |          22.3655 |           8.7981 |
[32m[20221213 22:32:32 @agent_ppo2.py:185][0m |           0.0001 |          23.4730 |           8.8004 |
[32m[20221213 22:32:32 @agent_ppo2.py:185][0m |          -0.0107 |          22.1294 |           8.7977 |
[32m[20221213 22:32:33 @agent_ppo2.py:185][0m |          -0.0105 |          22.0723 |           8.7897 |
[32m[20221213 22:32:33 @agent_ppo2.py:185][0m |          -0.0120 |          22.0511 |           8.7884 |
[32m[20221213 22:32:33 @agent_ppo2.py:185][0m |          -0.0128 |          22.0631 |           8.7932 |
[32m[20221213 22:32:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.20
[32m[20221213 22:32:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 358.83
[32m[20221213 22:32:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.47
[32m[20221213 22:32:33 @agent_ppo2.py:143][0m Total time:      14.33 min
[32m[20221213 22:32:33 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221213 22:32:33 @agent_ppo2.py:121][0m #------------------------ Iteration 676 --------------------------#
[32m[20221213 22:32:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:33 @agent_ppo2.py:185][0m |          -0.0026 |          22.6681 |           8.9541 |
[32m[20221213 22:32:33 @agent_ppo2.py:185][0m |          -0.0080 |          20.3703 |           8.9323 |
[32m[20221213 22:32:33 @agent_ppo2.py:185][0m |          -0.0089 |          19.1866 |           8.9241 |
[32m[20221213 22:32:33 @agent_ppo2.py:185][0m |          -0.0080 |          18.5383 |           8.9098 |
[32m[20221213 22:32:34 @agent_ppo2.py:185][0m |          -0.0115 |          17.8561 |           8.9074 |
[32m[20221213 22:32:34 @agent_ppo2.py:185][0m |          -0.0137 |          17.5079 |           8.9081 |
[32m[20221213 22:32:34 @agent_ppo2.py:185][0m |          -0.0132 |          17.2695 |           8.9089 |
[32m[20221213 22:32:34 @agent_ppo2.py:185][0m |          -0.0115 |          17.1611 |           8.9025 |
[32m[20221213 22:32:34 @agent_ppo2.py:185][0m |          -0.0132 |          17.0064 |           8.8979 |
[32m[20221213 22:32:34 @agent_ppo2.py:185][0m |          -0.0139 |          16.9635 |           8.8985 |
[32m[20221213 22:32:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:32:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.82
[32m[20221213 22:32:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.12
[32m[20221213 22:32:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.80
[32m[20221213 22:32:34 @agent_ppo2.py:143][0m Total time:      14.35 min
[32m[20221213 22:32:34 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221213 22:32:34 @agent_ppo2.py:121][0m #------------------------ Iteration 677 --------------------------#
[32m[20221213 22:32:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:32:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:34 @agent_ppo2.py:185][0m |          -0.0003 |          29.2317 |           8.7661 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0006 |          26.7287 |           8.7512 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0038 |          25.9011 |           8.7380 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0088 |          26.3302 |           8.7384 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0101 |          25.6121 |           8.7383 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0075 |          25.0594 |           8.7405 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0082 |          24.9819 |           8.7362 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0116 |          24.8291 |           8.7364 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0138 |          24.5325 |           8.7323 |
[32m[20221213 22:32:35 @agent_ppo2.py:185][0m |          -0.0015 |          24.9391 |           8.7303 |
[32m[20221213 22:32:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:32:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.22
[32m[20221213 22:32:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.68
[32m[20221213 22:32:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.65
[32m[20221213 22:32:35 @agent_ppo2.py:143][0m Total time:      14.38 min
[32m[20221213 22:32:35 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221213 22:32:35 @agent_ppo2.py:121][0m #------------------------ Iteration 678 --------------------------#
[32m[20221213 22:32:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0005 |          23.2615 |           8.6647 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0075 |          22.1499 |           8.6513 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0061 |          21.9128 |           8.6581 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0081 |          21.7075 |           8.6600 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0103 |          21.5579 |           8.6591 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0087 |          21.5151 |           8.6579 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0129 |          21.4162 |           8.6629 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0103 |          21.3298 |           8.6680 |
[32m[20221213 22:32:36 @agent_ppo2.py:185][0m |          -0.0072 |          21.4740 |           8.6642 |
[32m[20221213 22:32:37 @agent_ppo2.py:185][0m |          -0.0100 |          21.2312 |           8.6634 |
[32m[20221213 22:32:37 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:32:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.29
[32m[20221213 22:32:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.23
[32m[20221213 22:32:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.44
[32m[20221213 22:32:37 @agent_ppo2.py:143][0m Total time:      14.40 min
[32m[20221213 22:32:37 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221213 22:32:37 @agent_ppo2.py:121][0m #------------------------ Iteration 679 --------------------------#
[32m[20221213 22:32:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:32:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:37 @agent_ppo2.py:185][0m |           0.0043 |          30.2056 |           8.7013 |
[32m[20221213 22:32:37 @agent_ppo2.py:185][0m |           0.0002 |          26.6144 |           8.7034 |
[32m[20221213 22:32:37 @agent_ppo2.py:185][0m |          -0.0007 |          26.1991 |           8.6910 |
[32m[20221213 22:32:37 @agent_ppo2.py:185][0m |          -0.0164 |          25.0388 |           8.6932 |
[32m[20221213 22:32:37 @agent_ppo2.py:185][0m |          -0.0054 |          24.4682 |           8.6939 |
[32m[20221213 22:32:37 @agent_ppo2.py:185][0m |          -0.0067 |          24.1676 |           8.6968 |
[32m[20221213 22:32:38 @agent_ppo2.py:185][0m |          -0.0135 |          23.9595 |           8.6859 |
[32m[20221213 22:32:38 @agent_ppo2.py:185][0m |          -0.0082 |          24.9057 |           8.6956 |
[32m[20221213 22:32:38 @agent_ppo2.py:185][0m |          -0.0076 |          24.1579 |           8.6912 |
[32m[20221213 22:32:38 @agent_ppo2.py:185][0m |          -0.0034 |          25.3064 |           8.6955 |
[32m[20221213 22:32:38 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:32:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.40
[32m[20221213 22:32:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 311.95
[32m[20221213 22:32:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.26
[32m[20221213 22:32:38 @agent_ppo2.py:143][0m Total time:      14.42 min
[32m[20221213 22:32:38 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221213 22:32:38 @agent_ppo2.py:121][0m #------------------------ Iteration 680 --------------------------#
[32m[20221213 22:32:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:32:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:38 @agent_ppo2.py:185][0m |           0.0009 |          28.7261 |           8.5635 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0079 |          25.9072 |           8.5460 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0100 |          25.1570 |           8.5437 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0137 |          24.5234 |           8.5380 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0102 |          24.1984 |           8.5430 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0111 |          23.8516 |           8.5408 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0112 |          23.6761 |           8.5415 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0117 |          23.3670 |           8.5379 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0135 |          23.3109 |           8.5304 |
[32m[20221213 22:32:39 @agent_ppo2.py:185][0m |          -0.0106 |          22.9954 |           8.5388 |
[32m[20221213 22:32:39 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 22:32:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.70
[32m[20221213 22:32:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.90
[32m[20221213 22:32:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.32
[32m[20221213 22:32:39 @agent_ppo2.py:143][0m Total time:      14.44 min
[32m[20221213 22:32:39 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221213 22:32:39 @agent_ppo2.py:121][0m #------------------------ Iteration 681 --------------------------#
[32m[20221213 22:32:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:32:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |          -0.0010 |          30.3631 |           8.7844 |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |          -0.0090 |          29.8657 |           8.7736 |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |          -0.0061 |          28.9528 |           8.7685 |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |          -0.0087 |          28.8010 |           8.7620 |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |          -0.0063 |          28.5843 |           8.7579 |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |          -0.0040 |          28.3930 |           8.7600 |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |           0.0042 |          32.6436 |           8.7533 |
[32m[20221213 22:32:40 @agent_ppo2.py:185][0m |          -0.0045 |          28.5320 |           8.7414 |
[32m[20221213 22:32:41 @agent_ppo2.py:185][0m |          -0.0115 |          27.9578 |           8.7527 |
[32m[20221213 22:32:41 @agent_ppo2.py:185][0m |          -0.0087 |          28.1276 |           8.7521 |
[32m[20221213 22:32:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:32:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.96
[32m[20221213 22:32:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.80
[32m[20221213 22:32:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.89
[32m[20221213 22:32:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 390.89
[32m[20221213 22:32:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 390.89
[32m[20221213 22:32:41 @agent_ppo2.py:143][0m Total time:      14.46 min
[32m[20221213 22:32:41 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221213 22:32:41 @agent_ppo2.py:121][0m #------------------------ Iteration 682 --------------------------#
[32m[20221213 22:32:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:41 @agent_ppo2.py:185][0m |          -0.0030 |          20.8112 |           8.8210 |
[32m[20221213 22:32:41 @agent_ppo2.py:185][0m |          -0.0086 |          20.1102 |           8.8171 |
[32m[20221213 22:32:41 @agent_ppo2.py:185][0m |          -0.0059 |          19.8903 |           8.8141 |
[32m[20221213 22:32:41 @agent_ppo2.py:185][0m |          -0.0045 |          19.9119 |           8.8097 |
[32m[20221213 22:32:41 @agent_ppo2.py:185][0m |          -0.0065 |          19.8785 |           8.8102 |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |           0.0089 |          22.9863 |           8.8149 |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |           0.0002 |          21.1894 |           8.8104 |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |          -0.0119 |          19.7358 |           8.8073 |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |          -0.0099 |          19.6317 |           8.8080 |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |          -0.0068 |          19.7737 |           8.8075 |
[32m[20221213 22:32:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.54
[32m[20221213 22:32:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.86
[32m[20221213 22:32:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.78
[32m[20221213 22:32:42 @agent_ppo2.py:143][0m Total time:      14.48 min
[32m[20221213 22:32:42 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221213 22:32:42 @agent_ppo2.py:121][0m #------------------------ Iteration 683 --------------------------#
[32m[20221213 22:32:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |          -0.0014 |          16.9161 |           8.7406 |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |          -0.0023 |          15.3820 |           8.7146 |
[32m[20221213 22:32:42 @agent_ppo2.py:185][0m |          -0.0143 |          15.0713 |           8.7116 |
[32m[20221213 22:32:43 @agent_ppo2.py:185][0m |          -0.0045 |          14.4530 |           8.7221 |
[32m[20221213 22:32:43 @agent_ppo2.py:185][0m |          -0.0088 |          14.1724 |           8.7169 |
[32m[20221213 22:32:43 @agent_ppo2.py:185][0m |          -0.0114 |          13.9232 |           8.7126 |
[32m[20221213 22:32:43 @agent_ppo2.py:185][0m |          -0.0067 |          13.6859 |           8.7061 |
[32m[20221213 22:32:43 @agent_ppo2.py:185][0m |           0.0064 |          15.8967 |           8.7085 |
[32m[20221213 22:32:43 @agent_ppo2.py:185][0m |          -0.0096 |          13.9081 |           8.7050 |
[32m[20221213 22:32:43 @agent_ppo2.py:185][0m |          -0.0161 |          13.6528 |           8.7081 |
[32m[20221213 22:32:43 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:32:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.10
[32m[20221213 22:32:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.36
[32m[20221213 22:32:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.51
[32m[20221213 22:32:43 @agent_ppo2.py:143][0m Total time:      14.51 min
[32m[20221213 22:32:43 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221213 22:32:43 @agent_ppo2.py:121][0m #------------------------ Iteration 684 --------------------------#
[32m[20221213 22:32:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0019 |          21.5389 |           8.7608 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0064 |          19.2735 |           8.7445 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0068 |          18.0888 |           8.7413 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |           0.0013 |          17.9613 |           8.7438 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0080 |          16.9089 |           8.7363 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0095 |          16.6545 |           8.7420 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0081 |          16.4836 |           8.7369 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0010 |          17.0338 |           8.7385 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0083 |          16.2247 |           8.7346 |
[32m[20221213 22:32:44 @agent_ppo2.py:185][0m |          -0.0104 |          16.1431 |           8.7343 |
[32m[20221213 22:32:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.79
[32m[20221213 22:32:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.00
[32m[20221213 22:32:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 112.66
[32m[20221213 22:32:44 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221213 22:32:44 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221213 22:32:44 @agent_ppo2.py:121][0m #------------------------ Iteration 685 --------------------------#
[32m[20221213 22:32:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0020 |          21.1586 |           8.7840 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0038 |          20.7072 |           8.7750 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0053 |          20.5344 |           8.7740 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0073 |          20.4239 |           8.7708 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0075 |          20.3118 |           8.7626 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0054 |          20.2915 |           8.7677 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0010 |          20.9410 |           8.7622 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0101 |          20.2127 |           8.7588 |
[32m[20221213 22:32:45 @agent_ppo2.py:185][0m |          -0.0094 |          20.0275 |           8.7618 |
[32m[20221213 22:32:46 @agent_ppo2.py:185][0m |          -0.0106 |          20.0532 |           8.7641 |
[32m[20221213 22:32:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:32:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.00
[32m[20221213 22:32:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.82
[32m[20221213 22:32:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.14
[32m[20221213 22:32:46 @agent_ppo2.py:143][0m Total time:      14.55 min
[32m[20221213 22:32:46 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221213 22:32:46 @agent_ppo2.py:121][0m #------------------------ Iteration 686 --------------------------#
[32m[20221213 22:32:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:46 @agent_ppo2.py:185][0m |           0.0032 |          20.0348 |           8.9768 |
[32m[20221213 22:32:46 @agent_ppo2.py:185][0m |          -0.0044 |          13.7344 |           8.9556 |
[32m[20221213 22:32:46 @agent_ppo2.py:185][0m |          -0.0075 |          13.1235 |           8.9630 |
[32m[20221213 22:32:46 @agent_ppo2.py:185][0m |          -0.0001 |          12.8154 |           8.9552 |
[32m[20221213 22:32:46 @agent_ppo2.py:185][0m |           0.0059 |          13.5733 |           8.9465 |
[32m[20221213 22:32:46 @agent_ppo2.py:185][0m |          -0.0038 |          12.5336 |           8.9465 |
[32m[20221213 22:32:47 @agent_ppo2.py:185][0m |          -0.0096 |          12.3582 |           8.9443 |
[32m[20221213 22:32:47 @agent_ppo2.py:185][0m |          -0.0064 |          12.2276 |           8.9387 |
[32m[20221213 22:32:47 @agent_ppo2.py:185][0m |          -0.0073 |          12.1142 |           8.9389 |
[32m[20221213 22:32:47 @agent_ppo2.py:185][0m |          -0.0048 |          12.7242 |           8.9354 |
[32m[20221213 22:32:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:32:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.84
[32m[20221213 22:32:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 317.06
[32m[20221213 22:32:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.74
[32m[20221213 22:32:47 @agent_ppo2.py:143][0m Total time:      14.57 min
[32m[20221213 22:32:47 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221213 22:32:47 @agent_ppo2.py:121][0m #------------------------ Iteration 687 --------------------------#
[32m[20221213 22:32:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:47 @agent_ppo2.py:185][0m |           0.0036 |          21.4002 |           8.7893 |
[32m[20221213 22:32:47 @agent_ppo2.py:185][0m |          -0.0119 |          18.5367 |           8.7804 |
[32m[20221213 22:32:47 @agent_ppo2.py:185][0m |          -0.0048 |          18.3410 |           8.7639 |
[32m[20221213 22:32:48 @agent_ppo2.py:185][0m |          -0.0019 |          17.9760 |           8.7594 |
[32m[20221213 22:32:48 @agent_ppo2.py:185][0m |          -0.0077 |          17.9097 |           8.7575 |
[32m[20221213 22:32:48 @agent_ppo2.py:185][0m |          -0.0049 |          17.9391 |           8.7498 |
[32m[20221213 22:32:48 @agent_ppo2.py:185][0m |          -0.0111 |          17.6123 |           8.7446 |
[32m[20221213 22:32:48 @agent_ppo2.py:185][0m |          -0.0058 |          17.6149 |           8.7411 |
[32m[20221213 22:32:48 @agent_ppo2.py:185][0m |          -0.0116 |          17.4693 |           8.7368 |
[32m[20221213 22:32:48 @agent_ppo2.py:185][0m |          -0.0108 |          17.3930 |           8.7376 |
[32m[20221213 22:32:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:32:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.65
[32m[20221213 22:32:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.02
[32m[20221213 22:32:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.73
[32m[20221213 22:32:48 @agent_ppo2.py:143][0m Total time:      14.59 min
[32m[20221213 22:32:48 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221213 22:32:48 @agent_ppo2.py:121][0m #------------------------ Iteration 688 --------------------------#
[32m[20221213 22:32:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:32:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0027 |          20.5631 |           8.7757 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0004 |          19.0228 |           8.7591 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0075 |          18.6868 |           8.7540 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0035 |          18.5515 |           8.7542 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0099 |          18.3927 |           8.7533 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0016 |          18.3945 |           8.7475 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0075 |          18.2893 |           8.7488 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0057 |          18.2328 |           8.7434 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0047 |          18.0969 |           8.7369 |
[32m[20221213 22:32:49 @agent_ppo2.py:185][0m |          -0.0042 |          18.0949 |           8.7421 |
[32m[20221213 22:32:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:32:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.36
[32m[20221213 22:32:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.11
[32m[20221213 22:32:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.62
[32m[20221213 22:32:49 @agent_ppo2.py:143][0m Total time:      14.61 min
[32m[20221213 22:32:49 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221213 22:32:49 @agent_ppo2.py:121][0m #------------------------ Iteration 689 --------------------------#
[32m[20221213 22:32:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:32:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |           0.0007 |          23.2009 |           8.7632 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0077 |          22.0720 |           8.7591 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0053 |          21.9358 |           8.7549 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0065 |          21.7961 |           8.7522 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0070 |          21.5683 |           8.7588 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0118 |          21.4964 |           8.7538 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0108 |          21.5083 |           8.7562 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0075 |          21.4093 |           8.7505 |
[32m[20221213 22:32:50 @agent_ppo2.py:185][0m |          -0.0108 |          21.4336 |           8.7506 |
[32m[20221213 22:32:51 @agent_ppo2.py:185][0m |          -0.0105 |          21.3492 |           8.7538 |
[32m[20221213 22:32:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:32:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.66
[32m[20221213 22:32:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.32
[32m[20221213 22:32:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.57
[32m[20221213 22:32:51 @agent_ppo2.py:143][0m Total time:      14.63 min
[32m[20221213 22:32:51 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221213 22:32:51 @agent_ppo2.py:121][0m #------------------------ Iteration 690 --------------------------#
[32m[20221213 22:32:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:32:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:51 @agent_ppo2.py:185][0m |           0.0000 |          20.5022 |           8.6166 |
[32m[20221213 22:32:51 @agent_ppo2.py:185][0m |          -0.0028 |          19.2442 |           8.6048 |
[32m[20221213 22:32:51 @agent_ppo2.py:185][0m |          -0.0073 |          18.9243 |           8.6040 |
[32m[20221213 22:32:51 @agent_ppo2.py:185][0m |           0.0046 |          19.4430 |           8.6071 |
[32m[20221213 22:32:51 @agent_ppo2.py:185][0m |          -0.0060 |          18.4335 |           8.5991 |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |          -0.0094 |          18.3671 |           8.5971 |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |          -0.0090 |          18.1556 |           8.6054 |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |          -0.0010 |          19.2666 |           8.6001 |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |          -0.0111 |          18.1408 |           8.6014 |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |           0.0036 |          18.6591 |           8.5948 |
[32m[20221213 22:32:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:32:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.28
[32m[20221213 22:32:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.46
[32m[20221213 22:32:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.36
[32m[20221213 22:32:52 @agent_ppo2.py:143][0m Total time:      14.65 min
[32m[20221213 22:32:52 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221213 22:32:52 @agent_ppo2.py:121][0m #------------------------ Iteration 691 --------------------------#
[32m[20221213 22:32:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |           0.0039 |          25.2016 |           8.7583 |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |          -0.0029 |          23.3979 |           8.7418 |
[32m[20221213 22:32:52 @agent_ppo2.py:185][0m |          -0.0070 |          23.0403 |           8.7465 |
[32m[20221213 22:32:53 @agent_ppo2.py:185][0m |          -0.0069 |          22.7708 |           8.7435 |
[32m[20221213 22:32:53 @agent_ppo2.py:185][0m |          -0.0082 |          22.3937 |           8.7388 |
[32m[20221213 22:32:53 @agent_ppo2.py:185][0m |          -0.0068 |          22.2603 |           8.7390 |
[32m[20221213 22:32:53 @agent_ppo2.py:185][0m |          -0.0053 |          21.9939 |           8.7380 |
[32m[20221213 22:32:53 @agent_ppo2.py:185][0m |          -0.0044 |          22.0331 |           8.7366 |
[32m[20221213 22:32:53 @agent_ppo2.py:185][0m |          -0.0109 |          21.6829 |           8.7379 |
[32m[20221213 22:32:53 @agent_ppo2.py:185][0m |          -0.0050 |          21.6865 |           8.7331 |
[32m[20221213 22:32:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:32:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.28
[32m[20221213 22:32:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.65
[32m[20221213 22:32:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.47
[32m[20221213 22:32:53 @agent_ppo2.py:143][0m Total time:      14.67 min
[32m[20221213 22:32:53 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221213 22:32:53 @agent_ppo2.py:121][0m #------------------------ Iteration 692 --------------------------#
[32m[20221213 22:32:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |           0.0034 |          22.6860 |           8.8343 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0011 |          21.9134 |           8.8353 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |           0.0012 |          22.2881 |           8.8314 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0092 |          21.5468 |           8.8325 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0099 |          21.4006 |           8.8400 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0089 |          21.2955 |           8.8417 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0091 |          21.2770 |           8.8337 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0122 |          21.1794 |           8.8374 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0075 |          21.4234 |           8.8364 |
[32m[20221213 22:32:54 @agent_ppo2.py:185][0m |          -0.0095 |          21.2030 |           8.8370 |
[32m[20221213 22:32:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:32:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.42
[32m[20221213 22:32:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.21
[32m[20221213 22:32:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.95
[32m[20221213 22:32:54 @agent_ppo2.py:143][0m Total time:      14.69 min
[32m[20221213 22:32:54 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221213 22:32:54 @agent_ppo2.py:121][0m #------------------------ Iteration 693 --------------------------#
[32m[20221213 22:32:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:32:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |          -0.0027 |          19.2397 |           8.8083 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |          -0.0040 |          17.2517 |           8.7928 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |           0.0001 |          17.1983 |           8.7967 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |           0.0024 |          17.2029 |           8.7799 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |          -0.0081 |          16.2543 |           8.8075 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |          -0.0087 |          16.0672 |           8.8020 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |          -0.0094 |          16.1185 |           8.8053 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |          -0.0123 |          15.8876 |           8.7980 |
[32m[20221213 22:32:55 @agent_ppo2.py:185][0m |          -0.0051 |          15.8149 |           8.7979 |
[32m[20221213 22:32:56 @agent_ppo2.py:185][0m |          -0.0143 |          15.7261 |           8.8024 |
[32m[20221213 22:32:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:32:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.18
[32m[20221213 22:32:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.25
[32m[20221213 22:32:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.92
[32m[20221213 22:32:56 @agent_ppo2.py:143][0m Total time:      14.71 min
[32m[20221213 22:32:56 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221213 22:32:56 @agent_ppo2.py:121][0m #------------------------ Iteration 694 --------------------------#
[32m[20221213 22:32:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:32:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:56 @agent_ppo2.py:185][0m |          -0.0008 |          22.2982 |           8.8519 |
[32m[20221213 22:32:56 @agent_ppo2.py:185][0m |          -0.0041 |          21.8669 |           8.8387 |
[32m[20221213 22:32:56 @agent_ppo2.py:185][0m |          -0.0054 |          21.7149 |           8.8250 |
[32m[20221213 22:32:56 @agent_ppo2.py:185][0m |          -0.0066 |          21.6058 |           8.8108 |
[32m[20221213 22:32:56 @agent_ppo2.py:185][0m |          -0.0003 |          21.6954 |           8.8191 |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0093 |          21.4978 |           8.8224 |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0090 |          21.4267 |           8.8171 |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0083 |          21.3805 |           8.8166 |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0075 |          21.2989 |           8.8163 |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0091 |          21.2496 |           8.8192 |
[32m[20221213 22:32:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:32:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.68
[32m[20221213 22:32:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.28
[32m[20221213 22:32:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.98
[32m[20221213 22:32:57 @agent_ppo2.py:143][0m Total time:      14.73 min
[32m[20221213 22:32:57 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221213 22:32:57 @agent_ppo2.py:121][0m #------------------------ Iteration 695 --------------------------#
[32m[20221213 22:32:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0021 |          20.6114 |           8.7468 |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0009 |          19.0892 |           8.7454 |
[32m[20221213 22:32:57 @agent_ppo2.py:185][0m |          -0.0099 |          18.3195 |           8.7468 |
[32m[20221213 22:32:58 @agent_ppo2.py:185][0m |          -0.0096 |          17.9402 |           8.7388 |
[32m[20221213 22:32:58 @agent_ppo2.py:185][0m |          -0.0016 |          20.2845 |           8.7428 |
[32m[20221213 22:32:58 @agent_ppo2.py:185][0m |          -0.0066 |          17.6269 |           8.7421 |
[32m[20221213 22:32:58 @agent_ppo2.py:185][0m |          -0.0091 |          17.3964 |           8.7358 |
[32m[20221213 22:32:58 @agent_ppo2.py:185][0m |          -0.0129 |          17.3135 |           8.7419 |
[32m[20221213 22:32:58 @agent_ppo2.py:185][0m |          -0.0099 |          17.2790 |           8.7399 |
[32m[20221213 22:32:58 @agent_ppo2.py:185][0m |          -0.0099 |          17.1280 |           8.7404 |
[32m[20221213 22:32:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:32:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.83
[32m[20221213 22:32:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.77
[32m[20221213 22:32:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.41
[32m[20221213 22:32:58 @agent_ppo2.py:143][0m Total time:      14.76 min
[32m[20221213 22:32:58 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221213 22:32:58 @agent_ppo2.py:121][0m #------------------------ Iteration 696 --------------------------#
[32m[20221213 22:32:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:32:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0017 |          26.3784 |           8.8283 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0062 |          25.2158 |           8.8103 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0085 |          24.5529 |           8.8035 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0131 |          24.4173 |           8.8079 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0077 |          24.2346 |           8.7989 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0111 |          23.9703 |           8.8061 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0103 |          23.9029 |           8.7988 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0147 |          23.7117 |           8.8019 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0145 |          23.6038 |           8.8003 |
[32m[20221213 22:32:59 @agent_ppo2.py:185][0m |          -0.0122 |          23.4339 |           8.8016 |
[32m[20221213 22:32:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:32:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.59
[32m[20221213 22:32:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.63
[32m[20221213 22:32:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.34
[32m[20221213 22:32:59 @agent_ppo2.py:143][0m Total time:      14.78 min
[32m[20221213 22:32:59 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221213 22:32:59 @agent_ppo2.py:121][0m #------------------------ Iteration 697 --------------------------#
[32m[20221213 22:33:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |           0.0023 |          23.7842 |           8.8268 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |           0.0048 |          22.9614 |           8.8053 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0095 |          21.8747 |           8.8081 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0122 |          21.7122 |           8.8018 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0132 |          21.5340 |           8.7964 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0118 |          21.3678 |           8.8022 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0121 |          21.2402 |           8.7988 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0158 |          21.1577 |           8.7966 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0118 |          21.0711 |           8.7872 |
[32m[20221213 22:33:00 @agent_ppo2.py:185][0m |          -0.0125 |          21.0062 |           8.8006 |
[32m[20221213 22:33:00 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.19
[32m[20221213 22:33:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.96
[32m[20221213 22:33:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.14
[32m[20221213 22:33:01 @agent_ppo2.py:143][0m Total time:      14.80 min
[32m[20221213 22:33:01 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221213 22:33:01 @agent_ppo2.py:121][0m #------------------------ Iteration 698 --------------------------#
[32m[20221213 22:33:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:01 @agent_ppo2.py:185][0m |           0.0067 |          25.7842 |           8.8604 |
[32m[20221213 22:33:01 @agent_ppo2.py:185][0m |          -0.0036 |          24.9104 |           8.8431 |
[32m[20221213 22:33:01 @agent_ppo2.py:185][0m |          -0.0035 |          25.0623 |           8.8341 |
[32m[20221213 22:33:01 @agent_ppo2.py:185][0m |           0.0025 |          25.8386 |           8.8286 |
[32m[20221213 22:33:01 @agent_ppo2.py:185][0m |          -0.0068 |          24.6998 |           8.8276 |
[32m[20221213 22:33:01 @agent_ppo2.py:185][0m |          -0.0060 |          24.5599 |           8.8251 |
[32m[20221213 22:33:01 @agent_ppo2.py:185][0m |          -0.0093 |          24.5128 |           8.8234 |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0063 |          24.5097 |           8.8302 |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0069 |          24.4530 |           8.8353 |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0064 |          24.5471 |           8.8262 |
[32m[20221213 22:33:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.20
[32m[20221213 22:33:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.36
[32m[20221213 22:33:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 206.74
[32m[20221213 22:33:02 @agent_ppo2.py:143][0m Total time:      14.82 min
[32m[20221213 22:33:02 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221213 22:33:02 @agent_ppo2.py:121][0m #------------------------ Iteration 699 --------------------------#
[32m[20221213 22:33:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0023 |          23.4688 |           8.8861 |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0098 |          22.1411 |           8.8660 |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0129 |          21.7867 |           8.8578 |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0085 |          21.5793 |           8.8612 |
[32m[20221213 22:33:02 @agent_ppo2.py:185][0m |          -0.0111 |          21.3415 |           8.8636 |
[32m[20221213 22:33:03 @agent_ppo2.py:185][0m |          -0.0146 |          21.2000 |           8.8608 |
[32m[20221213 22:33:03 @agent_ppo2.py:185][0m |          -0.0117 |          21.0985 |           8.8623 |
[32m[20221213 22:33:03 @agent_ppo2.py:185][0m |          -0.0125 |          20.9742 |           8.8661 |
[32m[20221213 22:33:03 @agent_ppo2.py:185][0m |          -0.0146 |          20.8326 |           8.8638 |
[32m[20221213 22:33:03 @agent_ppo2.py:185][0m |          -0.0120 |          20.7247 |           8.8645 |
[32m[20221213 22:33:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.43
[32m[20221213 22:33:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.74
[32m[20221213 22:33:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.55
[32m[20221213 22:33:03 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 404.55
[32m[20221213 22:33:03 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 404.55
[32m[20221213 22:33:03 @agent_ppo2.py:143][0m Total time:      14.84 min
[32m[20221213 22:33:03 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221213 22:33:03 @agent_ppo2.py:121][0m #------------------------ Iteration 700 --------------------------#
[32m[20221213 22:33:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:33:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:03 @agent_ppo2.py:185][0m |          -0.0031 |          30.0399 |           9.0067 |
[32m[20221213 22:33:03 @agent_ppo2.py:185][0m |          -0.0066 |          27.8019 |           8.9928 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0105 |          26.9627 |           8.9952 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0071 |          26.4562 |           8.9983 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0084 |          26.3285 |           9.0053 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0010 |          26.3282 |           9.0006 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0045 |          25.7480 |           9.0004 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0105 |          25.6023 |           9.0070 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0063 |          25.8536 |           9.0005 |
[32m[20221213 22:33:04 @agent_ppo2.py:185][0m |          -0.0069 |          25.3046 |           9.0105 |
[32m[20221213 22:33:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.25
[32m[20221213 22:33:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.03
[32m[20221213 22:33:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.76
[32m[20221213 22:33:04 @agent_ppo2.py:143][0m Total time:      14.86 min
[32m[20221213 22:33:04 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221213 22:33:04 @agent_ppo2.py:121][0m #------------------------ Iteration 701 --------------------------#
[32m[20221213 22:33:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0024 |          24.1380 |           9.0632 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0046 |          22.3038 |           9.0485 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0147 |          21.6724 |           9.0416 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0019 |          21.3168 |           9.0319 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0057 |          21.0768 |           9.0266 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0099 |          20.8263 |           9.0286 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0105 |          20.6088 |           9.0243 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0097 |          20.4417 |           9.0171 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0090 |          20.3148 |           9.0155 |
[32m[20221213 22:33:05 @agent_ppo2.py:185][0m |          -0.0109 |          20.1947 |           9.0194 |
[32m[20221213 22:33:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.34
[32m[20221213 22:33:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.22
[32m[20221213 22:33:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.33
[32m[20221213 22:33:05 @agent_ppo2.py:143][0m Total time:      14.88 min
[32m[20221213 22:33:05 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221213 22:33:05 @agent_ppo2.py:121][0m #------------------------ Iteration 702 --------------------------#
[32m[20221213 22:33:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0012 |          25.6497 |           9.0207 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0018 |          23.5109 |           9.0148 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0044 |          22.8980 |           9.0145 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0045 |          22.2803 |           9.0182 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0051 |          21.9869 |           9.0168 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0101 |          21.7817 |           9.0216 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0089 |          21.7306 |           9.0244 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0069 |          21.5606 |           9.0248 |
[32m[20221213 22:33:06 @agent_ppo2.py:185][0m |          -0.0082 |          21.3825 |           9.0324 |
[32m[20221213 22:33:07 @agent_ppo2.py:185][0m |          -0.0095 |          21.5483 |           9.0269 |
[32m[20221213 22:33:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.36
[32m[20221213 22:33:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.35
[32m[20221213 22:33:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.73
[32m[20221213 22:33:07 @agent_ppo2.py:143][0m Total time:      14.90 min
[32m[20221213 22:33:07 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221213 22:33:07 @agent_ppo2.py:121][0m #------------------------ Iteration 703 --------------------------#
[32m[20221213 22:33:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:07 @agent_ppo2.py:185][0m |           0.0080 |          20.3143 |           9.0720 |
[32m[20221213 22:33:07 @agent_ppo2.py:185][0m |          -0.0068 |          17.4600 |           9.0613 |
[32m[20221213 22:33:07 @agent_ppo2.py:185][0m |          -0.0041 |          16.5772 |           9.0622 |
[32m[20221213 22:33:07 @agent_ppo2.py:185][0m |          -0.0062 |          16.1708 |           9.0560 |
[32m[20221213 22:33:07 @agent_ppo2.py:185][0m |          -0.0044 |          15.7667 |           9.0555 |
[32m[20221213 22:33:07 @agent_ppo2.py:185][0m |          -0.0122 |          15.4238 |           9.0525 |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |          -0.0098 |          15.1814 |           9.0569 |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |          -0.0135 |          14.9840 |           9.0595 |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |          -0.0087 |          14.7883 |           9.0543 |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |           0.0007 |          14.7754 |           9.0554 |
[32m[20221213 22:33:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:33:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.93
[32m[20221213 22:33:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.82
[32m[20221213 22:33:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.53
[32m[20221213 22:33:08 @agent_ppo2.py:143][0m Total time:      14.92 min
[32m[20221213 22:33:08 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221213 22:33:08 @agent_ppo2.py:121][0m #------------------------ Iteration 704 --------------------------#
[32m[20221213 22:33:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |           0.0044 |          32.4042 |           9.2111 |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |           0.0022 |          31.4752 |           9.2140 |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |          -0.0032 |          30.5765 |           9.2081 |
[32m[20221213 22:33:08 @agent_ppo2.py:185][0m |           0.0049 |          31.4264 |           9.2035 |
[32m[20221213 22:33:09 @agent_ppo2.py:185][0m |           0.0022 |          31.2737 |           9.2024 |
[32m[20221213 22:33:09 @agent_ppo2.py:185][0m |          -0.0057 |          29.9219 |           9.2024 |
[32m[20221213 22:33:09 @agent_ppo2.py:185][0m |          -0.0098 |          29.6939 |           9.2051 |
[32m[20221213 22:33:09 @agent_ppo2.py:185][0m |          -0.0131 |          29.7451 |           9.2018 |
[32m[20221213 22:33:09 @agent_ppo2.py:185][0m |          -0.0052 |          30.1385 |           9.2014 |
[32m[20221213 22:33:09 @agent_ppo2.py:185][0m |           0.0076 |          31.7990 |           9.2015 |
[32m[20221213 22:33:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.34
[32m[20221213 22:33:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.76
[32m[20221213 22:33:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 181.13
[32m[20221213 22:33:09 @agent_ppo2.py:143][0m Total time:      14.94 min
[32m[20221213 22:33:09 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221213 22:33:09 @agent_ppo2.py:121][0m #------------------------ Iteration 705 --------------------------#
[32m[20221213 22:33:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:09 @agent_ppo2.py:185][0m |          -0.0016 |          24.5322 |           8.9484 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0051 |          22.9979 |           8.9342 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0077 |          22.6152 |           8.9378 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0093 |          22.3689 |           8.9350 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0098 |          22.1795 |           8.9322 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0073 |          22.0846 |           8.9291 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0080 |          21.9798 |           8.9339 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0064 |          21.8035 |           8.9276 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0162 |          21.8050 |           8.9295 |
[32m[20221213 22:33:10 @agent_ppo2.py:185][0m |          -0.0137 |          21.7530 |           8.9297 |
[32m[20221213 22:33:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.53
[32m[20221213 22:33:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.77
[32m[20221213 22:33:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.79
[32m[20221213 22:33:10 @agent_ppo2.py:143][0m Total time:      14.96 min
[32m[20221213 22:33:10 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221213 22:33:10 @agent_ppo2.py:121][0m #------------------------ Iteration 706 --------------------------#
[32m[20221213 22:33:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0001 |          22.0521 |           8.9745 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0015 |          19.4676 |           8.9732 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0053 |          18.2847 |           8.9697 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0126 |          17.7752 |           8.9689 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0059 |          17.3254 |           8.9740 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0087 |          17.2405 |           8.9739 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0070 |          16.9144 |           8.9747 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0067 |          16.9119 |           8.9781 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0007 |          19.7037 |           8.9685 |
[32m[20221213 22:33:11 @agent_ppo2.py:185][0m |          -0.0025 |          17.9846 |           8.9717 |
[32m[20221213 22:33:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.84
[32m[20221213 22:33:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.98
[32m[20221213 22:33:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.54
[32m[20221213 22:33:12 @agent_ppo2.py:143][0m Total time:      14.98 min
[32m[20221213 22:33:12 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221213 22:33:12 @agent_ppo2.py:121][0m #------------------------ Iteration 707 --------------------------#
[32m[20221213 22:33:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0005 |          25.5919 |           9.2409 |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0044 |          24.3534 |           9.2281 |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0059 |          24.1366 |           9.2170 |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0061 |          23.9192 |           9.2107 |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0077 |          23.7768 |           9.2175 |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0075 |          23.6781 |           9.2193 |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0050 |          23.5956 |           9.2113 |
[32m[20221213 22:33:12 @agent_ppo2.py:185][0m |          -0.0087 |          23.5548 |           9.2184 |
[32m[20221213 22:33:13 @agent_ppo2.py:185][0m |          -0.0093 |          23.4080 |           9.2147 |
[32m[20221213 22:33:13 @agent_ppo2.py:185][0m |          -0.0086 |          23.3529 |           9.2033 |
[32m[20221213 22:33:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.06
[32m[20221213 22:33:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.19
[32m[20221213 22:33:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.95
[32m[20221213 22:33:13 @agent_ppo2.py:143][0m Total time:      15.00 min
[32m[20221213 22:33:13 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221213 22:33:13 @agent_ppo2.py:121][0m #------------------------ Iteration 708 --------------------------#
[32m[20221213 22:33:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:13 @agent_ppo2.py:185][0m |           0.0014 |          24.0138 |           9.1866 |
[32m[20221213 22:33:13 @agent_ppo2.py:185][0m |           0.0097 |          22.1881 |           9.1768 |
[32m[20221213 22:33:13 @agent_ppo2.py:185][0m |          -0.0037 |          21.0662 |           9.1669 |
[32m[20221213 22:33:13 @agent_ppo2.py:185][0m |          -0.0025 |          20.5553 |           9.1687 |
[32m[20221213 22:33:13 @agent_ppo2.py:185][0m |          -0.0044 |          20.2268 |           9.1708 |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0107 |          19.9044 |           9.1694 |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0026 |          19.8210 |           9.1718 |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0108 |          19.5662 |           9.1735 |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0010 |          20.4477 |           9.1743 |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0130 |          19.2433 |           9.1695 |
[32m[20221213 22:33:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:33:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.22
[32m[20221213 22:33:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.48
[32m[20221213 22:33:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.59
[32m[20221213 22:33:14 @agent_ppo2.py:143][0m Total time:      15.02 min
[32m[20221213 22:33:14 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221213 22:33:14 @agent_ppo2.py:121][0m #------------------------ Iteration 709 --------------------------#
[32m[20221213 22:33:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0028 |          25.8205 |           9.2977 |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0057 |          24.7505 |           9.2913 |
[32m[20221213 22:33:14 @agent_ppo2.py:185][0m |          -0.0010 |          24.9234 |           9.2937 |
[32m[20221213 22:33:15 @agent_ppo2.py:185][0m |          -0.0095 |          24.0629 |           9.2922 |
[32m[20221213 22:33:15 @agent_ppo2.py:185][0m |          -0.0115 |          23.9114 |           9.2884 |
[32m[20221213 22:33:15 @agent_ppo2.py:185][0m |          -0.0091 |          23.7488 |           9.2807 |
[32m[20221213 22:33:15 @agent_ppo2.py:185][0m |          -0.0104 |          23.6127 |           9.2760 |
[32m[20221213 22:33:15 @agent_ppo2.py:185][0m |          -0.0011 |          24.5816 |           9.2808 |
[32m[20221213 22:33:15 @agent_ppo2.py:185][0m |          -0.0075 |          23.6681 |           9.2778 |
[32m[20221213 22:33:15 @agent_ppo2.py:185][0m |          -0.0112 |          23.3737 |           9.2798 |
[32m[20221213 22:33:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.97
[32m[20221213 22:33:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.20
[32m[20221213 22:33:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.02
[32m[20221213 22:33:15 @agent_ppo2.py:143][0m Total time:      15.04 min
[32m[20221213 22:33:15 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221213 22:33:15 @agent_ppo2.py:121][0m #------------------------ Iteration 710 --------------------------#
[32m[20221213 22:33:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:33:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |           0.0015 |          28.8088 |           9.1105 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |           0.0016 |          26.4125 |           9.0995 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0086 |          25.7552 |           9.0932 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0088 |          25.3730 |           9.0880 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0042 |          25.1417 |           9.0872 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0097 |          24.8697 |           9.0874 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0108 |          24.6062 |           9.0809 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0127 |          24.4457 |           9.0868 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0070 |          24.4040 |           9.0870 |
[32m[20221213 22:33:16 @agent_ppo2.py:185][0m |          -0.0105 |          24.2631 |           9.0796 |
[32m[20221213 22:33:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.89
[32m[20221213 22:33:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.19
[32m[20221213 22:33:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 192.45
[32m[20221213 22:33:16 @agent_ppo2.py:143][0m Total time:      15.06 min
[32m[20221213 22:33:16 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221213 22:33:16 @agent_ppo2.py:121][0m #------------------------ Iteration 711 --------------------------#
[32m[20221213 22:33:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |           0.0009 |          33.9064 |           9.1962 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |           0.0010 |          32.6544 |           9.1858 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |          -0.0078 |          30.5837 |           9.1853 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |          -0.0046 |          29.9445 |           9.1804 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |          -0.0067 |          29.3744 |           9.1697 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |           0.0161 |          34.9223 |           9.1739 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |          -0.0049 |          28.9984 |           9.1505 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |          -0.0064 |          28.3882 |           9.1563 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |          -0.0055 |          28.3367 |           9.1620 |
[32m[20221213 22:33:17 @agent_ppo2.py:185][0m |          -0.0119 |          28.0121 |           9.1568 |
[32m[20221213 22:33:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.01
[32m[20221213 22:33:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.29
[32m[20221213 22:33:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.42
[32m[20221213 22:33:18 @agent_ppo2.py:143][0m Total time:      15.08 min
[32m[20221213 22:33:18 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221213 22:33:18 @agent_ppo2.py:121][0m #------------------------ Iteration 712 --------------------------#
[32m[20221213 22:33:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:18 @agent_ppo2.py:185][0m |          -0.0002 |          33.4407 |           9.1172 |
[32m[20221213 22:33:18 @agent_ppo2.py:185][0m |          -0.0019 |          32.4415 |           9.0939 |
[32m[20221213 22:33:18 @agent_ppo2.py:185][0m |          -0.0084 |          31.6617 |           9.0870 |
[32m[20221213 22:33:18 @agent_ppo2.py:185][0m |          -0.0099 |          31.3280 |           9.0952 |
[32m[20221213 22:33:18 @agent_ppo2.py:185][0m |           0.0016 |          32.8735 |           9.0929 |
[32m[20221213 22:33:18 @agent_ppo2.py:185][0m |          -0.0099 |          31.1053 |           9.0856 |
[32m[20221213 22:33:18 @agent_ppo2.py:185][0m |          -0.0118 |          30.7912 |           9.0876 |
[32m[20221213 22:33:19 @agent_ppo2.py:185][0m |          -0.0104 |          30.6562 |           9.0887 |
[32m[20221213 22:33:19 @agent_ppo2.py:185][0m |          -0.0052 |          31.5812 |           9.0888 |
[32m[20221213 22:33:19 @agent_ppo2.py:185][0m |          -0.0079 |          30.5087 |           9.0878 |
[32m[20221213 22:33:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:33:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.10
[32m[20221213 22:33:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 357.56
[32m[20221213 22:33:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.47
[32m[20221213 22:33:19 @agent_ppo2.py:143][0m Total time:      15.10 min
[32m[20221213 22:33:19 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221213 22:33:19 @agent_ppo2.py:121][0m #------------------------ Iteration 713 --------------------------#
[32m[20221213 22:33:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:33:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:19 @agent_ppo2.py:185][0m |          -0.0016 |          24.4489 |           9.3466 |
[32m[20221213 22:33:19 @agent_ppo2.py:185][0m |           0.0113 |          22.9661 |           9.3249 |
[32m[20221213 22:33:19 @agent_ppo2.py:185][0m |          -0.0070 |          20.1916 |           9.3275 |
[32m[20221213 22:33:19 @agent_ppo2.py:185][0m |          -0.0135 |          19.5216 |           9.3290 |
[32m[20221213 22:33:20 @agent_ppo2.py:185][0m |          -0.0078 |          19.1417 |           9.3272 |
[32m[20221213 22:33:20 @agent_ppo2.py:185][0m |          -0.0137 |          18.7612 |           9.3279 |
[32m[20221213 22:33:20 @agent_ppo2.py:185][0m |          -0.0101 |          18.5212 |           9.3207 |
[32m[20221213 22:33:20 @agent_ppo2.py:185][0m |          -0.0081 |          18.2536 |           9.3253 |
[32m[20221213 22:33:20 @agent_ppo2.py:185][0m |          -0.0131 |          17.9993 |           9.3192 |
[32m[20221213 22:33:20 @agent_ppo2.py:185][0m |          -0.0116 |          17.7964 |           9.3186 |
[32m[20221213 22:33:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:33:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.23
[32m[20221213 22:33:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.27
[32m[20221213 22:33:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.27
[32m[20221213 22:33:20 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221213 22:33:20 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221213 22:33:20 @agent_ppo2.py:121][0m #------------------------ Iteration 714 --------------------------#
[32m[20221213 22:33:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0052 |          32.4369 |           9.2494 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |           0.0035 |          28.4737 |           9.2360 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0066 |          26.9815 |           9.2324 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0093 |          26.6176 |           9.2325 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0074 |          26.3006 |           9.2218 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0067 |          26.1498 |           9.2345 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0070 |          25.8589 |           9.2337 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0081 |          25.8725 |           9.2360 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0093 |          25.6564 |           9.2392 |
[32m[20221213 22:33:21 @agent_ppo2.py:185][0m |          -0.0051 |          25.6742 |           9.2391 |
[32m[20221213 22:33:21 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:33:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.73
[32m[20221213 22:33:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.54
[32m[20221213 22:33:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.69
[32m[20221213 22:33:22 @agent_ppo2.py:143][0m Total time:      15.14 min
[32m[20221213 22:33:22 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221213 22:33:22 @agent_ppo2.py:121][0m #------------------------ Iteration 715 --------------------------#
[32m[20221213 22:33:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |           0.0005 |          20.3332 |           9.2191 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |          -0.0041 |          19.4301 |           9.2126 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |          -0.0073 |          19.2103 |           9.2051 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |           0.0028 |          20.1089 |           9.2089 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |          -0.0031 |          19.2634 |           9.1953 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |          -0.0085 |          18.9886 |           9.2064 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |          -0.0078 |          18.9041 |           9.2063 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |          -0.0062 |          18.9346 |           9.2019 |
[32m[20221213 22:33:22 @agent_ppo2.py:185][0m |          -0.0063 |          18.8092 |           9.2039 |
[32m[20221213 22:33:23 @agent_ppo2.py:185][0m |          -0.0079 |          18.7877 |           9.1932 |
[32m[20221213 22:33:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.44
[32m[20221213 22:33:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 360.99
[32m[20221213 22:33:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.42
[32m[20221213 22:33:23 @agent_ppo2.py:143][0m Total time:      15.16 min
[32m[20221213 22:33:23 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221213 22:33:23 @agent_ppo2.py:121][0m #------------------------ Iteration 716 --------------------------#
[32m[20221213 22:33:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:23 @agent_ppo2.py:185][0m |           0.0002 |          23.6257 |           9.3569 |
[32m[20221213 22:33:23 @agent_ppo2.py:185][0m |          -0.0012 |          22.8483 |           9.3496 |
[32m[20221213 22:33:23 @agent_ppo2.py:185][0m |          -0.0045 |          22.4911 |           9.3415 |
[32m[20221213 22:33:23 @agent_ppo2.py:185][0m |          -0.0072 |          22.3075 |           9.3400 |
[32m[20221213 22:33:23 @agent_ppo2.py:185][0m |          -0.0054 |          22.2205 |           9.3371 |
[32m[20221213 22:33:23 @agent_ppo2.py:185][0m |           0.0007 |          24.6214 |           9.3305 |
[32m[20221213 22:33:24 @agent_ppo2.py:185][0m |          -0.0085 |          22.0860 |           9.3265 |
[32m[20221213 22:33:24 @agent_ppo2.py:185][0m |          -0.0052 |          22.9181 |           9.3254 |
[32m[20221213 22:33:24 @agent_ppo2.py:185][0m |          -0.0108 |          21.9480 |           9.3173 |
[32m[20221213 22:33:24 @agent_ppo2.py:185][0m |          -0.0064 |          21.9027 |           9.3239 |
[32m[20221213 22:33:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.43
[32m[20221213 22:33:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.84
[32m[20221213 22:33:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.30
[32m[20221213 22:33:24 @agent_ppo2.py:143][0m Total time:      15.18 min
[32m[20221213 22:33:24 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221213 22:33:24 @agent_ppo2.py:121][0m #------------------------ Iteration 717 --------------------------#
[32m[20221213 22:33:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:24 @agent_ppo2.py:185][0m |           0.0013 |          23.7857 |           9.2327 |
[32m[20221213 22:33:24 @agent_ppo2.py:185][0m |          -0.0093 |          22.7847 |           9.2173 |
[32m[20221213 22:33:24 @agent_ppo2.py:185][0m |          -0.0069 |          22.3237 |           9.2201 |
[32m[20221213 22:33:25 @agent_ppo2.py:185][0m |          -0.0050 |          22.1740 |           9.2086 |
[32m[20221213 22:33:25 @agent_ppo2.py:185][0m |          -0.0096 |          21.9011 |           9.2038 |
[32m[20221213 22:33:25 @agent_ppo2.py:185][0m |          -0.0096 |          21.6450 |           9.2107 |
[32m[20221213 22:33:25 @agent_ppo2.py:185][0m |          -0.0026 |          22.5082 |           9.2117 |
[32m[20221213 22:33:25 @agent_ppo2.py:185][0m |          -0.0069 |          21.4150 |           9.2028 |
[32m[20221213 22:33:25 @agent_ppo2.py:185][0m |          -0.0096 |          21.2467 |           9.2015 |
[32m[20221213 22:33:25 @agent_ppo2.py:185][0m |          -0.0100 |          21.1078 |           9.2116 |
[32m[20221213 22:33:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:33:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.27
[32m[20221213 22:33:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.74
[32m[20221213 22:33:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.04
[32m[20221213 22:33:25 @agent_ppo2.py:143][0m Total time:      15.20 min
[32m[20221213 22:33:25 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221213 22:33:25 @agent_ppo2.py:121][0m #------------------------ Iteration 718 --------------------------#
[32m[20221213 22:33:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:33:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |           0.0003 |          26.1060 |           9.2152 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0070 |          25.4066 |           9.2002 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0018 |          25.8663 |           9.2052 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0055 |          24.9140 |           9.2016 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0109 |          24.7781 |           9.1990 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0106 |          24.6734 |           9.1932 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0090 |          24.5582 |           9.1932 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0073 |          24.4611 |           9.1954 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0119 |          24.3134 |           9.1943 |
[32m[20221213 22:33:26 @agent_ppo2.py:185][0m |          -0.0126 |          24.2467 |           9.1905 |
[32m[20221213 22:33:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:33:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.80
[32m[20221213 22:33:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.37
[32m[20221213 22:33:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.11
[32m[20221213 22:33:26 @agent_ppo2.py:143][0m Total time:      15.23 min
[32m[20221213 22:33:26 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221213 22:33:26 @agent_ppo2.py:121][0m #------------------------ Iteration 719 --------------------------#
[32m[20221213 22:33:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |           0.0048 |          28.8154 |           9.2001 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0034 |          23.6554 |           9.1825 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0086 |          23.1182 |           9.1839 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0055 |          22.5562 |           9.1810 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0046 |          22.2486 |           9.1830 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0122 |          22.1551 |           9.1811 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0079 |          22.0185 |           9.1814 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0102 |          21.8467 |           9.1791 |
[32m[20221213 22:33:27 @agent_ppo2.py:185][0m |          -0.0145 |          21.6188 |           9.1811 |
[32m[20221213 22:33:28 @agent_ppo2.py:185][0m |          -0.0126 |          21.4129 |           9.1858 |
[32m[20221213 22:33:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:33:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.16
[32m[20221213 22:33:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.26
[32m[20221213 22:33:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.72
[32m[20221213 22:33:28 @agent_ppo2.py:143][0m Total time:      15.25 min
[32m[20221213 22:33:28 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221213 22:33:28 @agent_ppo2.py:121][0m #------------------------ Iteration 720 --------------------------#
[32m[20221213 22:33:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:33:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:28 @agent_ppo2.py:185][0m |           0.0003 |          21.2000 |           9.2694 |
[32m[20221213 22:33:28 @agent_ppo2.py:185][0m |          -0.0063 |          18.6956 |           9.2490 |
[32m[20221213 22:33:28 @agent_ppo2.py:185][0m |          -0.0067 |          17.9212 |           9.2404 |
[32m[20221213 22:33:28 @agent_ppo2.py:185][0m |          -0.0063 |          17.5778 |           9.2374 |
[32m[20221213 22:33:28 @agent_ppo2.py:185][0m |          -0.0089 |          17.1853 |           9.2428 |
[32m[20221213 22:33:28 @agent_ppo2.py:185][0m |          -0.0102 |          17.0527 |           9.2303 |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |          -0.0144 |          16.6577 |           9.2359 |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |          -0.0091 |          16.5804 |           9.2276 |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |          -0.0131 |          16.3049 |           9.2235 |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |          -0.0104 |          16.1758 |           9.2247 |
[32m[20221213 22:33:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 205.99
[32m[20221213 22:33:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.71
[32m[20221213 22:33:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.08
[32m[20221213 22:33:29 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 406.08
[32m[20221213 22:33:29 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 406.08
[32m[20221213 22:33:29 @agent_ppo2.py:143][0m Total time:      15.27 min
[32m[20221213 22:33:29 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221213 22:33:29 @agent_ppo2.py:121][0m #------------------------ Iteration 721 --------------------------#
[32m[20221213 22:33:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |           0.0001 |          23.7591 |           9.1938 |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |          -0.0066 |          22.3153 |           9.1808 |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |          -0.0084 |          21.8594 |           9.1689 |
[32m[20221213 22:33:29 @agent_ppo2.py:185][0m |          -0.0094 |          21.4791 |           9.1663 |
[32m[20221213 22:33:30 @agent_ppo2.py:185][0m |          -0.0083 |          21.2276 |           9.1728 |
[32m[20221213 22:33:30 @agent_ppo2.py:185][0m |          -0.0132 |          21.0277 |           9.1582 |
[32m[20221213 22:33:30 @agent_ppo2.py:185][0m |          -0.0083 |          20.8330 |           9.1652 |
[32m[20221213 22:33:30 @agent_ppo2.py:185][0m |          -0.0113 |          20.7085 |           9.1577 |
[32m[20221213 22:33:30 @agent_ppo2.py:185][0m |          -0.0127 |          20.5444 |           9.1621 |
[32m[20221213 22:33:30 @agent_ppo2.py:185][0m |          -0.0089 |          20.4265 |           9.1570 |
[32m[20221213 22:33:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.92
[32m[20221213 22:33:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.40
[32m[20221213 22:33:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.42
[32m[20221213 22:33:30 @agent_ppo2.py:143][0m Total time:      15.29 min
[32m[20221213 22:33:30 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221213 22:33:30 @agent_ppo2.py:121][0m #------------------------ Iteration 722 --------------------------#
[32m[20221213 22:33:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:33:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:30 @agent_ppo2.py:185][0m |           0.0009 |          23.0392 |           9.0596 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0069 |          21.0739 |           9.0422 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0058 |          21.6168 |           9.0335 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0136 |          20.6108 |           9.0261 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0090 |          20.5101 |           9.0239 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0116 |          20.3792 |           9.0248 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0084 |          20.3156 |           9.0204 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0134 |          20.1140 |           9.0177 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0112 |          20.0233 |           9.0092 |
[32m[20221213 22:33:31 @agent_ppo2.py:185][0m |          -0.0105 |          20.0096 |           9.0151 |
[32m[20221213 22:33:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:33:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.38
[32m[20221213 22:33:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.81
[32m[20221213 22:33:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.84
[32m[20221213 22:33:31 @agent_ppo2.py:143][0m Total time:      15.31 min
[32m[20221213 22:33:31 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221213 22:33:31 @agent_ppo2.py:121][0m #------------------------ Iteration 723 --------------------------#
[32m[20221213 22:33:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0027 |          21.9499 |           9.1356 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0002 |          20.6075 |           9.1305 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0058 |          20.1043 |           9.1310 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0072 |          19.8680 |           9.1278 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0100 |          19.5909 |           9.1279 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0096 |          19.3277 |           9.1279 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0127 |          19.2240 |           9.1296 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0110 |          19.0975 |           9.1367 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |           0.0002 |          20.7110 |           9.1277 |
[32m[20221213 22:33:32 @agent_ppo2.py:185][0m |          -0.0112 |          18.8538 |           9.1294 |
[32m[20221213 22:33:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.88
[32m[20221213 22:33:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.06
[32m[20221213 22:33:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.49
[32m[20221213 22:33:33 @agent_ppo2.py:143][0m Total time:      15.33 min
[32m[20221213 22:33:33 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221213 22:33:33 @agent_ppo2.py:121][0m #------------------------ Iteration 724 --------------------------#
[32m[20221213 22:33:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |           0.0007 |          31.5386 |           9.0677 |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |          -0.0050 |          30.1145 |           9.0649 |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |          -0.0065 |          29.4666 |           9.0608 |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |          -0.0082 |          29.2910 |           9.0549 |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |          -0.0005 |          29.0972 |           9.0560 |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |          -0.0063 |          28.9532 |           9.0528 |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |          -0.0067 |          29.2847 |           9.0522 |
[32m[20221213 22:33:33 @agent_ppo2.py:185][0m |          -0.0091 |          28.4136 |           9.0518 |
[32m[20221213 22:33:34 @agent_ppo2.py:185][0m |          -0.0113 |          28.3677 |           9.0507 |
[32m[20221213 22:33:34 @agent_ppo2.py:185][0m |          -0.0100 |          28.1601 |           9.0487 |
[32m[20221213 22:33:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.64
[32m[20221213 22:33:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.63
[32m[20221213 22:33:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.93
[32m[20221213 22:33:34 @agent_ppo2.py:143][0m Total time:      15.35 min
[32m[20221213 22:33:34 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221213 22:33:34 @agent_ppo2.py:121][0m #------------------------ Iteration 725 --------------------------#
[32m[20221213 22:33:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:34 @agent_ppo2.py:185][0m |           0.0090 |          31.6474 |           9.3029 |
[32m[20221213 22:33:34 @agent_ppo2.py:185][0m |          -0.0051 |          28.0623 |           9.2915 |
[32m[20221213 22:33:34 @agent_ppo2.py:185][0m |          -0.0080 |          27.7803 |           9.2842 |
[32m[20221213 22:33:34 @agent_ppo2.py:185][0m |          -0.0032 |          27.7383 |           9.2873 |
[32m[20221213 22:33:34 @agent_ppo2.py:185][0m |          -0.0069 |          27.3939 |           9.2845 |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0031 |          27.2959 |           9.2783 |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0059 |          27.3010 |           9.2788 |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0111 |          27.1306 |           9.2653 |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0079 |          26.9826 |           9.2754 |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0086 |          26.9515 |           9.2724 |
[32m[20221213 22:33:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.37
[32m[20221213 22:33:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.21
[32m[20221213 22:33:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.69
[32m[20221213 22:33:35 @agent_ppo2.py:143][0m Total time:      15.37 min
[32m[20221213 22:33:35 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221213 22:33:35 @agent_ppo2.py:121][0m #------------------------ Iteration 726 --------------------------#
[32m[20221213 22:33:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0015 |          31.7102 |           9.1570 |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0031 |          30.8157 |           9.1401 |
[32m[20221213 22:33:35 @agent_ppo2.py:185][0m |          -0.0077 |          30.3744 |           9.1303 |
[32m[20221213 22:33:36 @agent_ppo2.py:185][0m |          -0.0120 |          30.3340 |           9.1355 |
[32m[20221213 22:33:36 @agent_ppo2.py:185][0m |          -0.0101 |          30.2128 |           9.1392 |
[32m[20221213 22:33:36 @agent_ppo2.py:185][0m |          -0.0103 |          30.2437 |           9.1296 |
[32m[20221213 22:33:36 @agent_ppo2.py:185][0m |          -0.0086 |          29.9290 |           9.1312 |
[32m[20221213 22:33:36 @agent_ppo2.py:185][0m |          -0.0096 |          29.8480 |           9.1441 |
[32m[20221213 22:33:36 @agent_ppo2.py:185][0m |          -0.0075 |          30.1277 |           9.1411 |
[32m[20221213 22:33:36 @agent_ppo2.py:185][0m |          -0.0073 |          29.6981 |           9.1361 |
[32m[20221213 22:33:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.77
[32m[20221213 22:33:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.90
[32m[20221213 22:33:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.56
[32m[20221213 22:33:36 @agent_ppo2.py:143][0m Total time:      15.39 min
[32m[20221213 22:33:36 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221213 22:33:36 @agent_ppo2.py:121][0m #------------------------ Iteration 727 --------------------------#
[32m[20221213 22:33:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0007 |          34.7402 |           9.1602 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0053 |          34.2926 |           9.1411 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0049 |          33.9723 |           9.1463 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0075 |          33.7335 |           9.1400 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0059 |          33.7752 |           9.1387 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0062 |          33.4893 |           9.1408 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0083 |          33.4499 |           9.1356 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0101 |          33.3870 |           9.1390 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0107 |          33.4564 |           9.1448 |
[32m[20221213 22:33:37 @agent_ppo2.py:185][0m |          -0.0111 |          33.2781 |           9.1424 |
[32m[20221213 22:33:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.12
[32m[20221213 22:33:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.41
[32m[20221213 22:33:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.80
[32m[20221213 22:33:37 @agent_ppo2.py:143][0m Total time:      15.41 min
[32m[20221213 22:33:37 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221213 22:33:37 @agent_ppo2.py:121][0m #------------------------ Iteration 728 --------------------------#
[32m[20221213 22:33:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:33:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0007 |          30.3413 |           9.2013 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |           0.0001 |          29.2243 |           9.1922 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0070 |          28.8005 |           9.1885 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0060 |          28.4314 |           9.1815 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0079 |          28.1716 |           9.1817 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0038 |          28.0495 |           9.1799 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0024 |          28.2379 |           9.1758 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0093 |          27.8905 |           9.1717 |
[32m[20221213 22:33:38 @agent_ppo2.py:185][0m |          -0.0078 |          27.6786 |           9.1775 |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |          -0.0075 |          27.5172 |           9.1774 |
[32m[20221213 22:33:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.81
[32m[20221213 22:33:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.87
[32m[20221213 22:33:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.10
[32m[20221213 22:33:39 @agent_ppo2.py:143][0m Total time:      15.43 min
[32m[20221213 22:33:39 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221213 22:33:39 @agent_ppo2.py:121][0m #------------------------ Iteration 729 --------------------------#
[32m[20221213 22:33:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |           0.0039 |          24.9040 |           9.3032 |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |          -0.0055 |          23.1076 |           9.2976 |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |          -0.0042 |          22.7073 |           9.2953 |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |          -0.0077 |          22.5267 |           9.2932 |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |          -0.0112 |          22.3235 |           9.2910 |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |          -0.0081 |          22.1691 |           9.2879 |
[32m[20221213 22:33:39 @agent_ppo2.py:185][0m |          -0.0113 |          22.0527 |           9.2909 |
[32m[20221213 22:33:40 @agent_ppo2.py:185][0m |          -0.0091 |          21.9330 |           9.2912 |
[32m[20221213 22:33:40 @agent_ppo2.py:185][0m |          -0.0146 |          21.8690 |           9.2899 |
[32m[20221213 22:33:40 @agent_ppo2.py:185][0m |          -0.0104 |          21.7477 |           9.2906 |
[32m[20221213 22:33:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.54
[32m[20221213 22:33:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.15
[32m[20221213 22:33:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 152.66
[32m[20221213 22:33:40 @agent_ppo2.py:143][0m Total time:      15.45 min
[32m[20221213 22:33:40 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221213 22:33:40 @agent_ppo2.py:121][0m #------------------------ Iteration 730 --------------------------#
[32m[20221213 22:33:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:33:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:40 @agent_ppo2.py:185][0m |           0.0079 |          31.8326 |           9.3430 |
[32m[20221213 22:33:40 @agent_ppo2.py:185][0m |          -0.0035 |          27.7084 |           9.3144 |
[32m[20221213 22:33:40 @agent_ppo2.py:185][0m |          -0.0115 |          26.5817 |           9.3054 |
[32m[20221213 22:33:40 @agent_ppo2.py:185][0m |          -0.0119 |          25.8213 |           9.3090 |
[32m[20221213 22:33:41 @agent_ppo2.py:185][0m |          -0.0020 |          26.2613 |           9.3116 |
[32m[20221213 22:33:41 @agent_ppo2.py:185][0m |          -0.0110 |          24.7361 |           9.3046 |
[32m[20221213 22:33:41 @agent_ppo2.py:185][0m |          -0.0010 |          26.8895 |           9.2971 |
[32m[20221213 22:33:41 @agent_ppo2.py:185][0m |          -0.0128 |          24.0429 |           9.2900 |
[32m[20221213 22:33:41 @agent_ppo2.py:185][0m |          -0.0123 |          23.8864 |           9.3035 |
[32m[20221213 22:33:41 @agent_ppo2.py:185][0m |          -0.0115 |          23.6214 |           9.3010 |
[32m[20221213 22:33:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 156.90
[32m[20221213 22:33:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.29
[32m[20221213 22:33:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.77
[32m[20221213 22:33:41 @agent_ppo2.py:143][0m Total time:      15.47 min
[32m[20221213 22:33:41 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221213 22:33:41 @agent_ppo2.py:121][0m #------------------------ Iteration 731 --------------------------#
[32m[20221213 22:33:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:41 @agent_ppo2.py:185][0m |          -0.0014 |          27.9697 |           9.3928 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0045 |          26.4372 |           9.3855 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0073 |          25.8220 |           9.3833 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0073 |          25.4484 |           9.3751 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0043 |          25.1888 |           9.3693 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0111 |          25.1188 |           9.3709 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0150 |          24.8863 |           9.3692 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0092 |          24.8853 |           9.3729 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0109 |          24.6606 |           9.3671 |
[32m[20221213 22:33:42 @agent_ppo2.py:185][0m |          -0.0127 |          24.5556 |           9.3611 |
[32m[20221213 22:33:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.21
[32m[20221213 22:33:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.79
[32m[20221213 22:33:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.43
[32m[20221213 22:33:42 @agent_ppo2.py:143][0m Total time:      15.49 min
[32m[20221213 22:33:42 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221213 22:33:42 @agent_ppo2.py:121][0m #------------------------ Iteration 732 --------------------------#
[32m[20221213 22:33:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0040 |          31.0525 |           9.3041 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0077 |          29.7668 |           9.2811 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0102 |          29.5155 |           9.2783 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0056 |          29.3628 |           9.2754 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0102 |          29.0760 |           9.2735 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0092 |          29.0650 |           9.2731 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0100 |          28.9529 |           9.2672 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0111 |          28.7837 |           9.2668 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0129 |          28.7175 |           9.2653 |
[32m[20221213 22:33:43 @agent_ppo2.py:185][0m |          -0.0150 |          28.6716 |           9.2702 |
[32m[20221213 22:33:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.20
[32m[20221213 22:33:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.53
[32m[20221213 22:33:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.10
[32m[20221213 22:33:44 @agent_ppo2.py:143][0m Total time:      15.51 min
[32m[20221213 22:33:44 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221213 22:33:44 @agent_ppo2.py:121][0m #------------------------ Iteration 733 --------------------------#
[32m[20221213 22:33:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0014 |          22.2429 |           9.3096 |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0037 |          17.7449 |           9.3004 |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0135 |          16.9632 |           9.2853 |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0094 |          16.6081 |           9.2885 |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0071 |          16.3133 |           9.2854 |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0114 |          16.0916 |           9.2803 |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0032 |          16.8050 |           9.2820 |
[32m[20221213 22:33:44 @agent_ppo2.py:185][0m |          -0.0078 |          15.9571 |           9.2752 |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0116 |          15.7280 |           9.2809 |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0101 |          15.6675 |           9.2795 |
[32m[20221213 22:33:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.66
[32m[20221213 22:33:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.68
[32m[20221213 22:33:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.20
[32m[20221213 22:33:45 @agent_ppo2.py:143][0m Total time:      15.53 min
[32m[20221213 22:33:45 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221213 22:33:45 @agent_ppo2.py:121][0m #------------------------ Iteration 734 --------------------------#
[32m[20221213 22:33:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0030 |          26.3827 |           9.4338 |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0056 |          25.1588 |           9.4266 |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0064 |          24.8633 |           9.4195 |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0098 |          24.5338 |           9.4208 |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0015 |          25.8872 |           9.4105 |
[32m[20221213 22:33:45 @agent_ppo2.py:185][0m |          -0.0077 |          24.3134 |           9.4094 |
[32m[20221213 22:33:46 @agent_ppo2.py:185][0m |          -0.0097 |          24.2380 |           9.4134 |
[32m[20221213 22:33:46 @agent_ppo2.py:185][0m |          -0.0075 |          23.9859 |           9.4122 |
[32m[20221213 22:33:46 @agent_ppo2.py:185][0m |          -0.0087 |          23.8485 |           9.4104 |
[32m[20221213 22:33:46 @agent_ppo2.py:185][0m |          -0.0086 |          23.9289 |           9.4118 |
[32m[20221213 22:33:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:33:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.59
[32m[20221213 22:33:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.22
[32m[20221213 22:33:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.67
[32m[20221213 22:33:46 @agent_ppo2.py:143][0m Total time:      15.55 min
[32m[20221213 22:33:46 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221213 22:33:46 @agent_ppo2.py:121][0m #------------------------ Iteration 735 --------------------------#
[32m[20221213 22:33:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:33:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:46 @agent_ppo2.py:185][0m |           0.0036 |          22.6425 |           9.4802 |
[32m[20221213 22:33:46 @agent_ppo2.py:185][0m |           0.0002 |          22.1689 |           9.4637 |
[32m[20221213 22:33:46 @agent_ppo2.py:185][0m |          -0.0088 |          20.9178 |           9.4671 |
[32m[20221213 22:33:47 @agent_ppo2.py:185][0m |          -0.0059 |          20.5557 |           9.4623 |
[32m[20221213 22:33:47 @agent_ppo2.py:185][0m |          -0.0102 |          20.3377 |           9.4616 |
[32m[20221213 22:33:47 @agent_ppo2.py:185][0m |          -0.0075 |          20.1834 |           9.4666 |
[32m[20221213 22:33:47 @agent_ppo2.py:185][0m |          -0.0090 |          19.9045 |           9.4631 |
[32m[20221213 22:33:47 @agent_ppo2.py:185][0m |          -0.0141 |          19.8835 |           9.4570 |
[32m[20221213 22:33:47 @agent_ppo2.py:185][0m |           0.0008 |          20.8602 |           9.4633 |
[32m[20221213 22:33:47 @agent_ppo2.py:185][0m |          -0.0125 |          19.6848 |           9.4618 |
[32m[20221213 22:33:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:33:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 157.09
[32m[20221213 22:33:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 311.24
[32m[20221213 22:33:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.15
[32m[20221213 22:33:47 @agent_ppo2.py:143][0m Total time:      15.57 min
[32m[20221213 22:33:47 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221213 22:33:47 @agent_ppo2.py:121][0m #------------------------ Iteration 736 --------------------------#
[32m[20221213 22:33:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0016 |          26.3827 |           9.4305 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |           0.0032 |          26.0096 |           9.4270 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0010 |          25.4515 |           9.4274 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0058 |          25.6653 |           9.4236 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0081 |          24.6373 |           9.4325 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0098 |          24.4036 |           9.4256 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0056 |          25.1629 |           9.4329 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0096 |          24.1265 |           9.4289 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0097 |          24.0514 |           9.4306 |
[32m[20221213 22:33:48 @agent_ppo2.py:185][0m |          -0.0114 |          23.8289 |           9.4337 |
[32m[20221213 22:33:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:33:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 205.30
[32m[20221213 22:33:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.88
[32m[20221213 22:33:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.28
[32m[20221213 22:33:49 @agent_ppo2.py:143][0m Total time:      15.59 min
[32m[20221213 22:33:49 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221213 22:33:49 @agent_ppo2.py:121][0m #------------------------ Iteration 737 --------------------------#
[32m[20221213 22:33:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |           0.0016 |          24.2646 |           9.4077 |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |          -0.0032 |          23.0686 |           9.3993 |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |          -0.0064 |          22.6360 |           9.3960 |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |          -0.0054 |          22.5271 |           9.3826 |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |          -0.0089 |          22.4350 |           9.3902 |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |          -0.0073 |          22.3363 |           9.3846 |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |          -0.0061 |          22.3220 |           9.3808 |
[32m[20221213 22:33:49 @agent_ppo2.py:185][0m |          -0.0058 |          22.1466 |           9.3817 |
[32m[20221213 22:33:50 @agent_ppo2.py:185][0m |          -0.0065 |          22.0137 |           9.3775 |
[32m[20221213 22:33:50 @agent_ppo2.py:185][0m |          -0.0032 |          22.3015 |           9.3790 |
[32m[20221213 22:33:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:33:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.68
[32m[20221213 22:33:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.61
[32m[20221213 22:33:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.66
[32m[20221213 22:33:50 @agent_ppo2.py:143][0m Total time:      15.61 min
[32m[20221213 22:33:50 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221213 22:33:50 @agent_ppo2.py:121][0m #------------------------ Iteration 738 --------------------------#
[32m[20221213 22:33:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:50 @agent_ppo2.py:185][0m |          -0.0053 |          30.4644 |           9.4552 |
[32m[20221213 22:33:50 @agent_ppo2.py:185][0m |          -0.0040 |          28.6044 |           9.4505 |
[32m[20221213 22:33:50 @agent_ppo2.py:185][0m |          -0.0071 |          28.0369 |           9.4454 |
[32m[20221213 22:33:50 @agent_ppo2.py:185][0m |          -0.0122 |          27.5447 |           9.4431 |
[32m[20221213 22:33:50 @agent_ppo2.py:185][0m |          -0.0126 |          27.2503 |           9.4395 |
[32m[20221213 22:33:51 @agent_ppo2.py:185][0m |          -0.0121 |          26.9343 |           9.4412 |
[32m[20221213 22:33:51 @agent_ppo2.py:185][0m |          -0.0134 |          26.6248 |           9.4455 |
[32m[20221213 22:33:51 @agent_ppo2.py:185][0m |          -0.0101 |          26.7038 |           9.4454 |
[32m[20221213 22:33:51 @agent_ppo2.py:185][0m |           0.0057 |          29.2866 |           9.4373 |
[32m[20221213 22:33:51 @agent_ppo2.py:185][0m |           0.0059 |          30.1022 |           9.4427 |
[32m[20221213 22:33:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.96
[32m[20221213 22:33:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.28
[32m[20221213 22:33:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.20
[32m[20221213 22:33:51 @agent_ppo2.py:143][0m Total time:      15.64 min
[32m[20221213 22:33:51 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221213 22:33:51 @agent_ppo2.py:121][0m #------------------------ Iteration 739 --------------------------#
[32m[20221213 22:33:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:51 @agent_ppo2.py:185][0m |          -0.0008 |          27.6176 |           9.5940 |
[32m[20221213 22:33:51 @agent_ppo2.py:185][0m |          -0.0040 |          26.6082 |           9.5889 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0064 |          26.3549 |           9.5881 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0081 |          26.1121 |           9.5904 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0044 |          26.2866 |           9.5899 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0077 |          25.8833 |           9.5970 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0097 |          25.8215 |           9.5992 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0100 |          25.7397 |           9.6044 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0106 |          25.6819 |           9.6025 |
[32m[20221213 22:33:52 @agent_ppo2.py:185][0m |          -0.0124 |          25.6279 |           9.6106 |
[32m[20221213 22:33:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:33:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.78
[32m[20221213 22:33:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.40
[32m[20221213 22:33:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.06
[32m[20221213 22:33:52 @agent_ppo2.py:143][0m Total time:      15.66 min
[32m[20221213 22:33:52 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221213 22:33:52 @agent_ppo2.py:121][0m #------------------------ Iteration 740 --------------------------#
[32m[20221213 22:33:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:33:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0012 |          21.2897 |           9.6388 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0092 |          19.7830 |           9.6236 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0082 |          19.2184 |           9.6197 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0050 |          18.9289 |           9.6150 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0040 |          18.7170 |           9.6171 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0134 |          18.4741 |           9.6184 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0133 |          18.2653 |           9.6159 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0076 |          18.2169 |           9.6138 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0159 |          17.9883 |           9.6118 |
[32m[20221213 22:33:53 @agent_ppo2.py:185][0m |          -0.0125 |          17.9240 |           9.6109 |
[32m[20221213 22:33:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.22
[32m[20221213 22:33:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 311.31
[32m[20221213 22:33:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.20
[32m[20221213 22:33:53 @agent_ppo2.py:143][0m Total time:      15.68 min
[32m[20221213 22:33:53 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221213 22:33:53 @agent_ppo2.py:121][0m #------------------------ Iteration 741 --------------------------#
[32m[20221213 22:33:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |          -0.0026 |          28.4295 |           9.7854 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |          -0.0022 |          27.8113 |           9.7691 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |           0.0088 |          29.6515 |           9.7632 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |           0.0093 |          30.1388 |           9.7734 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |          -0.0076 |          27.4244 |           9.7754 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |          -0.0070 |          27.2751 |           9.7663 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |          -0.0082 |          27.2923 |           9.7764 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |          -0.0082 |          27.1432 |           9.7633 |
[32m[20221213 22:33:54 @agent_ppo2.py:185][0m |          -0.0062 |          27.2286 |           9.7565 |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |          -0.0123 |          27.0897 |           9.7605 |
[32m[20221213 22:33:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.33
[32m[20221213 22:33:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.26
[32m[20221213 22:33:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.84
[32m[20221213 22:33:55 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221213 22:33:55 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221213 22:33:55 @agent_ppo2.py:121][0m #------------------------ Iteration 742 --------------------------#
[32m[20221213 22:33:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |           0.0010 |          31.4205 |           9.8120 |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |          -0.0062 |          30.0529 |           9.8046 |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |           0.0042 |          31.5402 |           9.8027 |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |          -0.0105 |          29.3486 |           9.7955 |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |          -0.0108 |          28.9637 |           9.8017 |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |          -0.0093 |          28.7129 |           9.8029 |
[32m[20221213 22:33:55 @agent_ppo2.py:185][0m |          -0.0055 |          29.0218 |           9.7968 |
[32m[20221213 22:33:56 @agent_ppo2.py:185][0m |          -0.0110 |          28.2867 |           9.7996 |
[32m[20221213 22:33:56 @agent_ppo2.py:185][0m |          -0.0106 |          28.1700 |           9.7979 |
[32m[20221213 22:33:56 @agent_ppo2.py:185][0m |           0.0015 |          30.7070 |           9.7946 |
[32m[20221213 22:33:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:33:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.04
[32m[20221213 22:33:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.59
[32m[20221213 22:33:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.96
[32m[20221213 22:33:56 @agent_ppo2.py:143][0m Total time:      15.72 min
[32m[20221213 22:33:56 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221213 22:33:56 @agent_ppo2.py:121][0m #------------------------ Iteration 743 --------------------------#
[32m[20221213 22:33:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:56 @agent_ppo2.py:185][0m |          -0.0026 |          23.4520 |           9.7809 |
[32m[20221213 22:33:56 @agent_ppo2.py:185][0m |           0.0008 |          20.0030 |           9.7699 |
[32m[20221213 22:33:56 @agent_ppo2.py:185][0m |           0.0025 |          18.8970 |           9.7782 |
[32m[20221213 22:33:56 @agent_ppo2.py:185][0m |          -0.0100 |          18.4226 |           9.7607 |
[32m[20221213 22:33:57 @agent_ppo2.py:185][0m |          -0.0094 |          18.1405 |           9.7579 |
[32m[20221213 22:33:57 @agent_ppo2.py:185][0m |          -0.0063 |          17.9652 |           9.7643 |
[32m[20221213 22:33:57 @agent_ppo2.py:185][0m |          -0.0043 |          18.8465 |           9.7573 |
[32m[20221213 22:33:57 @agent_ppo2.py:185][0m |          -0.0153 |          17.7122 |           9.7555 |
[32m[20221213 22:33:57 @agent_ppo2.py:185][0m |          -0.0114 |          17.6181 |           9.7461 |
[32m[20221213 22:33:57 @agent_ppo2.py:185][0m |          -0.0125 |          17.4819 |           9.7509 |
[32m[20221213 22:33:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:33:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.37
[32m[20221213 22:33:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.11
[32m[20221213 22:33:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.36
[32m[20221213 22:33:57 @agent_ppo2.py:143][0m Total time:      15.74 min
[32m[20221213 22:33:57 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221213 22:33:57 @agent_ppo2.py:121][0m #------------------------ Iteration 744 --------------------------#
[32m[20221213 22:33:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:57 @agent_ppo2.py:185][0m |           0.0012 |          28.8659 |           9.6807 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0063 |          25.9242 |           9.6707 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0065 |          25.5031 |           9.6720 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0047 |          25.1337 |           9.6642 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0069 |          24.8436 |           9.6640 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0110 |          24.6570 |           9.6587 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0148 |          24.6065 |           9.6555 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0065 |          24.5469 |           9.6550 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0133 |          24.3845 |           9.6511 |
[32m[20221213 22:33:58 @agent_ppo2.py:185][0m |          -0.0077 |          24.7561 |           9.6542 |
[32m[20221213 22:33:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:33:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.17
[32m[20221213 22:33:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.16
[32m[20221213 22:33:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.97
[32m[20221213 22:33:58 @agent_ppo2.py:143][0m Total time:      15.76 min
[32m[20221213 22:33:58 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221213 22:33:58 @agent_ppo2.py:121][0m #------------------------ Iteration 745 --------------------------#
[32m[20221213 22:33:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:33:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0018 |          32.7421 |           9.6742 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0057 |          31.0104 |           9.6728 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0065 |          30.3225 |           9.6615 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0048 |          30.2568 |           9.6622 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0072 |          29.7631 |           9.6613 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0069 |          29.4750 |           9.6541 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0041 |          29.5255 |           9.6648 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0075 |          29.2995 |           9.6556 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0081 |          29.0540 |           9.6619 |
[32m[20221213 22:33:59 @agent_ppo2.py:185][0m |          -0.0084 |          28.9190 |           9.6533 |
[32m[20221213 22:33:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.14
[32m[20221213 22:34:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.96
[32m[20221213 22:34:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.42
[32m[20221213 22:34:00 @agent_ppo2.py:143][0m Total time:      15.78 min
[32m[20221213 22:34:00 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221213 22:34:00 @agent_ppo2.py:121][0m #------------------------ Iteration 746 --------------------------#
[32m[20221213 22:34:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0001 |          24.4969 |           9.7202 |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0069 |          23.2551 |           9.7090 |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0066 |          22.9928 |           9.7063 |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0091 |          22.5931 |           9.6942 |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0103 |          22.4323 |           9.6914 |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0111 |          22.2873 |           9.6894 |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0128 |          22.1210 |           9.6826 |
[32m[20221213 22:34:00 @agent_ppo2.py:185][0m |          -0.0075 |          22.4900 |           9.6779 |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0037 |          24.0908 |           9.6758 |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0085 |          21.9244 |           9.6676 |
[32m[20221213 22:34:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.91
[32m[20221213 22:34:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.75
[32m[20221213 22:34:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.73
[32m[20221213 22:34:01 @agent_ppo2.py:143][0m Total time:      15.80 min
[32m[20221213 22:34:01 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221213 22:34:01 @agent_ppo2.py:121][0m #------------------------ Iteration 747 --------------------------#
[32m[20221213 22:34:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0010 |          31.8032 |           9.7378 |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0038 |          30.8985 |           9.7403 |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0051 |          30.5655 |           9.7252 |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0045 |          30.4181 |           9.7178 |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0076 |          30.2148 |           9.7322 |
[32m[20221213 22:34:01 @agent_ppo2.py:185][0m |          -0.0070 |          30.0929 |           9.7294 |
[32m[20221213 22:34:02 @agent_ppo2.py:185][0m |           0.0024 |          32.4302 |           9.7270 |
[32m[20221213 22:34:02 @agent_ppo2.py:185][0m |          -0.0076 |          29.7543 |           9.7087 |
[32m[20221213 22:34:02 @agent_ppo2.py:185][0m |          -0.0078 |          29.6608 |           9.7310 |
[32m[20221213 22:34:02 @agent_ppo2.py:185][0m |          -0.0106 |          29.6704 |           9.7253 |
[32m[20221213 22:34:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.73
[32m[20221213 22:34:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.02
[32m[20221213 22:34:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.69
[32m[20221213 22:34:02 @agent_ppo2.py:143][0m Total time:      15.82 min
[32m[20221213 22:34:02 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221213 22:34:02 @agent_ppo2.py:121][0m #------------------------ Iteration 748 --------------------------#
[32m[20221213 22:34:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:02 @agent_ppo2.py:185][0m |           0.0007 |          20.4016 |           9.7403 |
[32m[20221213 22:34:02 @agent_ppo2.py:185][0m |          -0.0086 |          18.4076 |           9.7316 |
[32m[20221213 22:34:02 @agent_ppo2.py:185][0m |          -0.0065 |          17.7982 |           9.7302 |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |          -0.0094 |          17.4250 |           9.7292 |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |          -0.0013 |          19.5780 |           9.7281 |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |          -0.0103 |          17.0762 |           9.7254 |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |          -0.0038 |          16.8514 |           9.7260 |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |          -0.0167 |          16.7770 |           9.7226 |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |          -0.0163 |          16.6240 |           9.7282 |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |          -0.0162 |          16.6418 |           9.7240 |
[32m[20221213 22:34:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.92
[32m[20221213 22:34:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.58
[32m[20221213 22:34:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.18
[32m[20221213 22:34:03 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221213 22:34:03 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221213 22:34:03 @agent_ppo2.py:121][0m #------------------------ Iteration 749 --------------------------#
[32m[20221213 22:34:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:03 @agent_ppo2.py:185][0m |           0.0112 |          17.4707 |           9.6971 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |           0.0012 |          14.9564 |           9.6615 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0099 |          14.4338 |           9.6642 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0063 |          14.3552 |           9.6552 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0071 |          14.0121 |           9.6436 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0106 |          13.9037 |           9.6398 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0137 |          13.7464 |           9.6363 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0082 |          13.6230 |           9.6310 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0086 |          13.4895 |           9.6267 |
[32m[20221213 22:34:04 @agent_ppo2.py:185][0m |          -0.0131 |          13.4162 |           9.6257 |
[32m[20221213 22:34:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:34:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.16
[32m[20221213 22:34:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.64
[32m[20221213 22:34:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 30.76
[32m[20221213 22:34:04 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221213 22:34:04 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221213 22:34:04 @agent_ppo2.py:121][0m #------------------------ Iteration 750 --------------------------#
[32m[20221213 22:34:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:34:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0018 |          21.8939 |           9.5507 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0066 |          20.8584 |           9.5262 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0072 |          20.5371 |           9.5238 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0094 |          20.3624 |           9.5248 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0088 |          20.1714 |           9.5176 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0085 |          19.9379 |           9.5280 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0100 |          19.7307 |           9.5226 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0111 |          19.6391 |           9.5161 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0068 |          19.6321 |           9.5134 |
[32m[20221213 22:34:05 @agent_ppo2.py:185][0m |          -0.0132 |          19.3279 |           9.5121 |
[32m[20221213 22:34:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:34:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 205.13
[32m[20221213 22:34:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.97
[32m[20221213 22:34:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.34
[32m[20221213 22:34:06 @agent_ppo2.py:143][0m Total time:      15.88 min
[32m[20221213 22:34:06 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221213 22:34:06 @agent_ppo2.py:121][0m #------------------------ Iteration 751 --------------------------#
[32m[20221213 22:34:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:06 @agent_ppo2.py:185][0m |           0.0039 |          27.2717 |           9.6228 |
[32m[20221213 22:34:06 @agent_ppo2.py:185][0m |          -0.0082 |          23.5495 |           9.6012 |
[32m[20221213 22:34:06 @agent_ppo2.py:185][0m |          -0.0081 |          22.7148 |           9.6005 |
[32m[20221213 22:34:06 @agent_ppo2.py:185][0m |          -0.0083 |          22.1412 |           9.6059 |
[32m[20221213 22:34:06 @agent_ppo2.py:185][0m |          -0.0122 |          21.8631 |           9.6125 |
[32m[20221213 22:34:06 @agent_ppo2.py:185][0m |          -0.0102 |          21.6364 |           9.6141 |
[32m[20221213 22:34:06 @agent_ppo2.py:185][0m |          -0.0080 |          21.3347 |           9.6184 |
[32m[20221213 22:34:07 @agent_ppo2.py:185][0m |          -0.0120 |          21.1940 |           9.6190 |
[32m[20221213 22:34:07 @agent_ppo2.py:185][0m |          -0.0071 |          21.1817 |           9.6171 |
[32m[20221213 22:34:07 @agent_ppo2.py:185][0m |          -0.0106 |          21.0208 |           9.6159 |
[32m[20221213 22:34:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.82
[32m[20221213 22:34:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 337.87
[32m[20221213 22:34:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.39
[32m[20221213 22:34:07 @agent_ppo2.py:143][0m Total time:      15.90 min
[32m[20221213 22:34:07 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221213 22:34:07 @agent_ppo2.py:121][0m #------------------------ Iteration 752 --------------------------#
[32m[20221213 22:34:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:07 @agent_ppo2.py:185][0m |          -0.0019 |          25.3511 |           9.6650 |
[32m[20221213 22:34:07 @agent_ppo2.py:185][0m |          -0.0012 |          20.2326 |           9.6505 |
[32m[20221213 22:34:07 @agent_ppo2.py:185][0m |          -0.0075 |          19.4231 |           9.6452 |
[32m[20221213 22:34:07 @agent_ppo2.py:185][0m |          -0.0101 |          19.1585 |           9.6332 |
[32m[20221213 22:34:08 @agent_ppo2.py:185][0m |          -0.0128 |          18.7794 |           9.6238 |
[32m[20221213 22:34:08 @agent_ppo2.py:185][0m |          -0.0070 |          18.8259 |           9.6263 |
[32m[20221213 22:34:08 @agent_ppo2.py:185][0m |          -0.0147 |          18.4118 |           9.6202 |
[32m[20221213 22:34:08 @agent_ppo2.py:185][0m |          -0.0162 |          18.1667 |           9.6266 |
[32m[20221213 22:34:08 @agent_ppo2.py:185][0m |          -0.0089 |          18.1671 |           9.6151 |
[32m[20221213 22:34:08 @agent_ppo2.py:185][0m |          -0.0102 |          18.0309 |           9.6227 |
[32m[20221213 22:34:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:34:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.07
[32m[20221213 22:34:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.99
[32m[20221213 22:34:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.66
[32m[20221213 22:34:08 @agent_ppo2.py:143][0m Total time:      15.92 min
[32m[20221213 22:34:08 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221213 22:34:08 @agent_ppo2.py:121][0m #------------------------ Iteration 753 --------------------------#
[32m[20221213 22:34:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:08 @agent_ppo2.py:185][0m |           0.0027 |          17.5105 |           9.5996 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0112 |          14.6703 |           9.5896 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0051 |          14.2695 |           9.5865 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0070 |          14.0782 |           9.5772 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0086 |          14.0109 |           9.5706 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0155 |          13.7909 |           9.5738 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0064 |          13.9162 |           9.5680 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0085 |          13.6460 |           9.5599 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0064 |          13.6126 |           9.5648 |
[32m[20221213 22:34:09 @agent_ppo2.py:185][0m |          -0.0098 |          13.5237 |           9.5653 |
[32m[20221213 22:34:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:34:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.79
[32m[20221213 22:34:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.24
[32m[20221213 22:34:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 210.39
[32m[20221213 22:34:09 @agent_ppo2.py:143][0m Total time:      15.94 min
[32m[20221213 22:34:09 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221213 22:34:09 @agent_ppo2.py:121][0m #------------------------ Iteration 754 --------------------------#
[32m[20221213 22:34:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |           0.0073 |          19.9483 |           9.8475 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0015 |          18.3999 |           9.8350 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0107 |          17.7887 |           9.8324 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0028 |          17.4883 |           9.8328 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0091 |          17.2399 |           9.8341 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0097 |          17.0532 |           9.8416 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0118 |          16.8451 |           9.8340 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0124 |          16.6331 |           9.8327 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0140 |          16.7239 |           9.8408 |
[32m[20221213 22:34:10 @agent_ppo2.py:185][0m |          -0.0062 |          16.4421 |           9.8318 |
[32m[20221213 22:34:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.48
[32m[20221213 22:34:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.68
[32m[20221213 22:34:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.18
[32m[20221213 22:34:11 @agent_ppo2.py:143][0m Total time:      15.96 min
[32m[20221213 22:34:11 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221213 22:34:11 @agent_ppo2.py:121][0m #------------------------ Iteration 755 --------------------------#
[32m[20221213 22:34:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |           0.0013 |          17.5763 |           9.6145 |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |          -0.0071 |          15.7743 |           9.6044 |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |          -0.0096 |          15.2844 |           9.6003 |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |          -0.0068 |          14.9386 |           9.5975 |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |          -0.0100 |          14.7561 |           9.5905 |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |          -0.0085 |          14.6178 |           9.5920 |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |          -0.0151 |          14.4139 |           9.5959 |
[32m[20221213 22:34:11 @agent_ppo2.py:185][0m |          -0.0092 |          14.2539 |           9.5889 |
[32m[20221213 22:34:12 @agent_ppo2.py:185][0m |          -0.0124 |          14.1605 |           9.5888 |
[32m[20221213 22:34:12 @agent_ppo2.py:185][0m |          -0.0151 |          14.1724 |           9.5857 |
[32m[20221213 22:34:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.04
[32m[20221213 22:34:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.47
[32m[20221213 22:34:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.86
[32m[20221213 22:34:12 @agent_ppo2.py:143][0m Total time:      15.98 min
[32m[20221213 22:34:12 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221213 22:34:12 @agent_ppo2.py:121][0m #------------------------ Iteration 756 --------------------------#
[32m[20221213 22:34:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:12 @agent_ppo2.py:185][0m |          -0.0048 |          24.8256 |           9.9586 |
[32m[20221213 22:34:12 @agent_ppo2.py:185][0m |          -0.0066 |          23.7731 |           9.9481 |
[32m[20221213 22:34:12 @agent_ppo2.py:185][0m |          -0.0074 |          23.3816 |           9.9393 |
[32m[20221213 22:34:12 @agent_ppo2.py:185][0m |          -0.0071 |          23.2183 |           9.9408 |
[32m[20221213 22:34:12 @agent_ppo2.py:185][0m |          -0.0073 |          22.9950 |           9.9318 |
[32m[20221213 22:34:13 @agent_ppo2.py:185][0m |          -0.0057 |          22.8903 |           9.9379 |
[32m[20221213 22:34:13 @agent_ppo2.py:185][0m |          -0.0069 |          22.9771 |           9.9319 |
[32m[20221213 22:34:13 @agent_ppo2.py:185][0m |          -0.0112 |          22.7204 |           9.9378 |
[32m[20221213 22:34:13 @agent_ppo2.py:185][0m |          -0.0083 |          22.6944 |           9.9301 |
[32m[20221213 22:34:13 @agent_ppo2.py:185][0m |           0.0012 |          23.8224 |           9.9368 |
[32m[20221213 22:34:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.41
[32m[20221213 22:34:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.25
[32m[20221213 22:34:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.47
[32m[20221213 22:34:13 @agent_ppo2.py:143][0m Total time:      16.00 min
[32m[20221213 22:34:13 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221213 22:34:13 @agent_ppo2.py:121][0m #------------------------ Iteration 757 --------------------------#
[32m[20221213 22:34:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:13 @agent_ppo2.py:185][0m |          -0.0018 |          28.2500 |           9.7480 |
[32m[20221213 22:34:13 @agent_ppo2.py:185][0m |          -0.0052 |          25.2571 |           9.7445 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0058 |          24.3052 |           9.7428 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0019 |          23.9743 |           9.7315 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0084 |          23.3230 |           9.7275 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0087 |          23.1409 |           9.7191 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0149 |          22.8168 |           9.7241 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0096 |          22.6575 |           9.7141 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0112 |          22.5913 |           9.7214 |
[32m[20221213 22:34:14 @agent_ppo2.py:185][0m |          -0.0102 |          22.5891 |           9.7064 |
[32m[20221213 22:34:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:34:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.93
[32m[20221213 22:34:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.82
[32m[20221213 22:34:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.59
[32m[20221213 22:34:14 @agent_ppo2.py:143][0m Total time:      16.02 min
[32m[20221213 22:34:14 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221213 22:34:14 @agent_ppo2.py:121][0m #------------------------ Iteration 758 --------------------------#
[32m[20221213 22:34:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:34:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0012 |          19.6191 |           9.5965 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0008 |          12.4666 |           9.5883 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0093 |          11.8452 |           9.5827 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0094 |          11.5135 |           9.5787 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0102 |          11.2654 |           9.5795 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0083 |          11.8768 |           9.5806 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0096 |          10.9929 |           9.5783 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0095 |          10.8095 |           9.5730 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0041 |          10.7117 |           9.5800 |
[32m[20221213 22:34:15 @agent_ppo2.py:185][0m |          -0.0130 |          10.6567 |           9.5764 |
[32m[20221213 22:34:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.31
[32m[20221213 22:34:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.24
[32m[20221213 22:34:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.68
[32m[20221213 22:34:16 @agent_ppo2.py:143][0m Total time:      16.04 min
[32m[20221213 22:34:16 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221213 22:34:16 @agent_ppo2.py:121][0m #------------------------ Iteration 759 --------------------------#
[32m[20221213 22:34:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:34:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |           0.0061 |          21.5466 |           9.8131 |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |          -0.0080 |          19.5378 |           9.8072 |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |          -0.0057 |          19.0873 |           9.8006 |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |          -0.0067 |          18.7944 |           9.7935 |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |          -0.0028 |          18.6855 |           9.7976 |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |          -0.0047 |          18.5367 |           9.7780 |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |           0.0011 |          19.6348 |           9.7829 |
[32m[20221213 22:34:16 @agent_ppo2.py:185][0m |          -0.0052 |          18.4202 |           9.7750 |
[32m[20221213 22:34:17 @agent_ppo2.py:185][0m |          -0.0088 |          18.1248 |           9.7769 |
[32m[20221213 22:34:17 @agent_ppo2.py:185][0m |          -0.0096 |          18.0513 |           9.7808 |
[32m[20221213 22:34:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:34:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.97
[32m[20221213 22:34:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.83
[32m[20221213 22:34:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.23
[32m[20221213 22:34:17 @agent_ppo2.py:143][0m Total time:      16.06 min
[32m[20221213 22:34:17 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221213 22:34:17 @agent_ppo2.py:121][0m #------------------------ Iteration 760 --------------------------#
[32m[20221213 22:34:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:34:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:17 @agent_ppo2.py:185][0m |          -0.0057 |          30.1505 |           9.8219 |
[32m[20221213 22:34:17 @agent_ppo2.py:185][0m |          -0.0083 |          27.0578 |           9.8110 |
[32m[20221213 22:34:17 @agent_ppo2.py:185][0m |          -0.0070 |          25.8984 |           9.7964 |
[32m[20221213 22:34:17 @agent_ppo2.py:185][0m |           0.0049 |          28.8226 |           9.8070 |
[32m[20221213 22:34:17 @agent_ppo2.py:185][0m |          -0.0058 |          25.3778 |           9.8011 |
[32m[20221213 22:34:18 @agent_ppo2.py:185][0m |          -0.0101 |          24.9063 |           9.8056 |
[32m[20221213 22:34:18 @agent_ppo2.py:185][0m |          -0.0081 |          24.6599 |           9.8061 |
[32m[20221213 22:34:18 @agent_ppo2.py:185][0m |          -0.0095 |          24.3707 |           9.8043 |
[32m[20221213 22:34:18 @agent_ppo2.py:185][0m |          -0.0113 |          24.5280 |           9.8031 |
[32m[20221213 22:34:18 @agent_ppo2.py:185][0m |          -0.0161 |          24.2138 |           9.8052 |
[32m[20221213 22:34:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.03
[32m[20221213 22:34:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.41
[32m[20221213 22:34:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.27
[32m[20221213 22:34:18 @agent_ppo2.py:143][0m Total time:      16.09 min
[32m[20221213 22:34:18 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221213 22:34:18 @agent_ppo2.py:121][0m #------------------------ Iteration 761 --------------------------#
[32m[20221213 22:34:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:18 @agent_ppo2.py:185][0m |           0.0075 |          19.4561 |           9.8762 |
[32m[20221213 22:34:18 @agent_ppo2.py:185][0m |          -0.0014 |          15.4167 |           9.8560 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |           0.0039 |          14.4341 |           9.8407 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |          -0.0049 |          13.9980 |           9.8450 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |          -0.0020 |          13.8349 |           9.8391 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |          -0.0036 |          13.8069 |           9.8503 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |          -0.0135 |          13.7453 |           9.8479 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |          -0.0108 |          13.6438 |           9.8447 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |          -0.0039 |          13.9727 |           9.8516 |
[32m[20221213 22:34:19 @agent_ppo2.py:185][0m |          -0.0142 |          13.5670 |           9.8485 |
[32m[20221213 22:34:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:34:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.41
[32m[20221213 22:34:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.68
[32m[20221213 22:34:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.99
[32m[20221213 22:34:19 @agent_ppo2.py:143][0m Total time:      16.11 min
[32m[20221213 22:34:19 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221213 22:34:19 @agent_ppo2.py:121][0m #------------------------ Iteration 762 --------------------------#
[32m[20221213 22:34:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |           0.0017 |          28.8095 |           9.8040 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0007 |          26.3499 |           9.7906 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0074 |          25.4129 |           9.7851 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |           0.0130 |          28.9435 |           9.7882 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0097 |          24.8241 |           9.7774 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0113 |          24.2589 |           9.7786 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0079 |          24.1875 |           9.7778 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0093 |          23.8498 |           9.7856 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0102 |          23.6041 |           9.7787 |
[32m[20221213 22:34:20 @agent_ppo2.py:185][0m |          -0.0123 |          23.5708 |           9.7853 |
[32m[20221213 22:34:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:34:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.08
[32m[20221213 22:34:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.34
[32m[20221213 22:34:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.53
[32m[20221213 22:34:20 @agent_ppo2.py:143][0m Total time:      16.13 min
[32m[20221213 22:34:20 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221213 22:34:20 @agent_ppo2.py:121][0m #------------------------ Iteration 763 --------------------------#
[32m[20221213 22:34:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |           0.0094 |          29.5367 |           9.8354 |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |          -0.0001 |          24.5203 |           9.8138 |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |          -0.0003 |          24.3246 |           9.8116 |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |          -0.0065 |          23.2442 |           9.8038 |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |          -0.0141 |          22.9001 |           9.8105 |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |          -0.0071 |          22.5990 |           9.8061 |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |           0.0072 |          25.3688 |           9.8129 |
[32m[20221213 22:34:21 @agent_ppo2.py:185][0m |          -0.0067 |          22.2012 |           9.8133 |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0144 |          21.9843 |           9.8081 |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0013 |          23.3690 |           9.8130 |
[32m[20221213 22:34:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:34:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 185.00
[32m[20221213 22:34:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.91
[32m[20221213 22:34:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.87
[32m[20221213 22:34:22 @agent_ppo2.py:143][0m Total time:      16.15 min
[32m[20221213 22:34:22 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221213 22:34:22 @agent_ppo2.py:121][0m #------------------------ Iteration 764 --------------------------#
[32m[20221213 22:34:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0042 |          16.1762 |           9.8217 |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0053 |          14.1300 |           9.8023 |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0112 |          13.7989 |           9.8011 |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0056 |          13.0553 |           9.8092 |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0108 |          12.8000 |           9.8013 |
[32m[20221213 22:34:22 @agent_ppo2.py:185][0m |          -0.0068 |          12.6339 |           9.8054 |
[32m[20221213 22:34:23 @agent_ppo2.py:185][0m |          -0.0141 |          12.5905 |           9.8052 |
[32m[20221213 22:34:23 @agent_ppo2.py:185][0m |          -0.0119 |          12.4196 |           9.8066 |
[32m[20221213 22:34:23 @agent_ppo2.py:185][0m |          -0.0109 |          12.3407 |           9.8140 |
[32m[20221213 22:34:23 @agent_ppo2.py:185][0m |          -0.0163 |          12.2521 |           9.8004 |
[32m[20221213 22:34:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.27
[32m[20221213 22:34:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.34
[32m[20221213 22:34:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.35
[32m[20221213 22:34:23 @agent_ppo2.py:143][0m Total time:      16.17 min
[32m[20221213 22:34:23 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221213 22:34:23 @agent_ppo2.py:121][0m #------------------------ Iteration 765 --------------------------#
[32m[20221213 22:34:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:23 @agent_ppo2.py:185][0m |          -0.0030 |          18.8197 |           9.8652 |
[32m[20221213 22:34:23 @agent_ppo2.py:185][0m |          -0.0057 |          17.7481 |           9.8559 |
[32m[20221213 22:34:23 @agent_ppo2.py:185][0m |          -0.0061 |          17.4235 |           9.8485 |
[32m[20221213 22:34:24 @agent_ppo2.py:185][0m |          -0.0071 |          17.1219 |           9.8409 |
[32m[20221213 22:34:24 @agent_ppo2.py:185][0m |          -0.0071 |          16.9740 |           9.8352 |
[32m[20221213 22:34:24 @agent_ppo2.py:185][0m |          -0.0052 |          16.8977 |           9.8324 |
[32m[20221213 22:34:24 @agent_ppo2.py:185][0m |          -0.0056 |          16.9040 |           9.8262 |
[32m[20221213 22:34:24 @agent_ppo2.py:185][0m |          -0.0118 |          16.7417 |           9.8338 |
[32m[20221213 22:34:24 @agent_ppo2.py:185][0m |          -0.0009 |          17.2305 |           9.8226 |
[32m[20221213 22:34:24 @agent_ppo2.py:185][0m |          -0.0148 |          16.7061 |           9.8222 |
[32m[20221213 22:34:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.68
[32m[20221213 22:34:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.97
[32m[20221213 22:34:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.18
[32m[20221213 22:34:24 @agent_ppo2.py:143][0m Total time:      16.19 min
[32m[20221213 22:34:24 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221213 22:34:24 @agent_ppo2.py:121][0m #------------------------ Iteration 766 --------------------------#
[32m[20221213 22:34:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |           0.0009 |          15.9685 |           9.7807 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0047 |          15.5005 |           9.7772 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0024 |          15.2388 |           9.7789 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0052 |          15.0740 |           9.7827 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0056 |          14.9491 |           9.7814 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0080 |          14.8842 |           9.7772 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0068 |          14.8097 |           9.7806 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0072 |          14.7226 |           9.7770 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0087 |          14.6478 |           9.7784 |
[32m[20221213 22:34:25 @agent_ppo2.py:185][0m |          -0.0090 |          14.6112 |           9.7831 |
[32m[20221213 22:34:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.30
[32m[20221213 22:34:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.92
[32m[20221213 22:34:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.68
[32m[20221213 22:34:25 @agent_ppo2.py:143][0m Total time:      16.21 min
[32m[20221213 22:34:25 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221213 22:34:25 @agent_ppo2.py:121][0m #------------------------ Iteration 767 --------------------------#
[32m[20221213 22:34:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |           0.0004 |          17.8704 |          10.0478 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0042 |          15.8416 |          10.0439 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0077 |          15.2313 |          10.0405 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0074 |          14.9441 |          10.0383 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0085 |          14.6627 |          10.0420 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0017 |          14.3191 |          10.0390 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0092 |          14.1279 |          10.0361 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0070 |          14.1491 |          10.0344 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |           0.0003 |          14.3862 |          10.0295 |
[32m[20221213 22:34:26 @agent_ppo2.py:185][0m |          -0.0092 |          13.7574 |          10.0356 |
[32m[20221213 22:34:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.92
[32m[20221213 22:34:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 317.85
[32m[20221213 22:34:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.48
[32m[20221213 22:34:27 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221213 22:34:27 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221213 22:34:27 @agent_ppo2.py:121][0m #------------------------ Iteration 768 --------------------------#
[32m[20221213 22:34:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:27 @agent_ppo2.py:185][0m |           0.0032 |          22.0347 |           9.6936 |
[32m[20221213 22:34:27 @agent_ppo2.py:185][0m |          -0.0063 |          20.7814 |           9.6765 |
[32m[20221213 22:34:27 @agent_ppo2.py:185][0m |          -0.0044 |          20.3198 |           9.6710 |
[32m[20221213 22:34:27 @agent_ppo2.py:185][0m |          -0.0058 |          19.9278 |           9.6695 |
[32m[20221213 22:34:27 @agent_ppo2.py:185][0m |          -0.0021 |          19.9416 |           9.6691 |
[32m[20221213 22:34:27 @agent_ppo2.py:185][0m |          -0.0104 |          19.5686 |           9.6549 |
[32m[20221213 22:34:27 @agent_ppo2.py:185][0m |          -0.0055 |          19.6804 |           9.6632 |
[32m[20221213 22:34:28 @agent_ppo2.py:185][0m |          -0.0112 |          19.3858 |           9.6575 |
[32m[20221213 22:34:28 @agent_ppo2.py:185][0m |          -0.0102 |          19.2164 |           9.6591 |
[32m[20221213 22:34:28 @agent_ppo2.py:185][0m |          -0.0085 |          19.1844 |           9.6604 |
[32m[20221213 22:34:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.60
[32m[20221213 22:34:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.56
[32m[20221213 22:34:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.88
[32m[20221213 22:34:28 @agent_ppo2.py:143][0m Total time:      16.25 min
[32m[20221213 22:34:28 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221213 22:34:28 @agent_ppo2.py:121][0m #------------------------ Iteration 769 --------------------------#
[32m[20221213 22:34:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:28 @agent_ppo2.py:185][0m |           0.0014 |          17.8846 |           9.9955 |
[32m[20221213 22:34:28 @agent_ppo2.py:185][0m |          -0.0081 |          16.4527 |           9.9815 |
[32m[20221213 22:34:28 @agent_ppo2.py:185][0m |           0.0014 |          16.6746 |           9.9732 |
[32m[20221213 22:34:28 @agent_ppo2.py:185][0m |          -0.0004 |          16.1259 |           9.9725 |
[32m[20221213 22:34:29 @agent_ppo2.py:185][0m |          -0.0077 |          15.4917 |           9.9713 |
[32m[20221213 22:34:29 @agent_ppo2.py:185][0m |          -0.0105 |          15.3280 |           9.9712 |
[32m[20221213 22:34:29 @agent_ppo2.py:185][0m |          -0.0115 |          15.1341 |           9.9660 |
[32m[20221213 22:34:29 @agent_ppo2.py:185][0m |          -0.0049 |          14.9913 |           9.9672 |
[32m[20221213 22:34:29 @agent_ppo2.py:185][0m |          -0.0106 |          14.9169 |           9.9676 |
[32m[20221213 22:34:29 @agent_ppo2.py:185][0m |          -0.0076 |          15.2474 |           9.9625 |
[32m[20221213 22:34:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:34:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.16
[32m[20221213 22:34:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.60
[32m[20221213 22:34:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.48
[32m[20221213 22:34:29 @agent_ppo2.py:143][0m Total time:      16.27 min
[32m[20221213 22:34:29 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221213 22:34:29 @agent_ppo2.py:121][0m #------------------------ Iteration 770 --------------------------#
[32m[20221213 22:34:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:34:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:29 @agent_ppo2.py:185][0m |          -0.0024 |          20.9083 |           9.9271 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0007 |          17.9272 |           9.9272 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0052 |          16.7387 |           9.9291 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0068 |          16.0747 |           9.9296 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0078 |          15.7407 |           9.9233 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0085 |          15.3161 |           9.9271 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0001 |          15.7143 |           9.9275 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0101 |          14.9968 |           9.9231 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0087 |          14.8476 |           9.9247 |
[32m[20221213 22:34:30 @agent_ppo2.py:185][0m |          -0.0098 |          14.7105 |           9.9235 |
[32m[20221213 22:34:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:34:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.42
[32m[20221213 22:34:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.40
[32m[20221213 22:34:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.77
[32m[20221213 22:34:30 @agent_ppo2.py:143][0m Total time:      16.29 min
[32m[20221213 22:34:30 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221213 22:34:30 @agent_ppo2.py:121][0m #------------------------ Iteration 771 --------------------------#
[32m[20221213 22:34:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0044 |          31.6638 |          10.0112 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |           0.0016 |          26.0485 |          10.0019 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0082 |          25.6105 |           9.9911 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0045 |          25.2300 |           9.9902 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0027 |          24.9960 |           9.9958 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0072 |          24.7869 |           9.9878 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0061 |          24.8054 |           9.9869 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0033 |          24.6489 |           9.9926 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0047 |          24.5702 |           9.9828 |
[32m[20221213 22:34:31 @agent_ppo2.py:185][0m |          -0.0016 |          24.5670 |           9.9888 |
[32m[20221213 22:34:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.54
[32m[20221213 22:34:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.19
[32m[20221213 22:34:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.66
[32m[20221213 22:34:32 @agent_ppo2.py:143][0m Total time:      16.31 min
[32m[20221213 22:34:32 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221213 22:34:32 @agent_ppo2.py:121][0m #------------------------ Iteration 772 --------------------------#
[32m[20221213 22:34:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:34:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |           0.0089 |          27.0691 |           9.7846 |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |          -0.0049 |          24.2133 |           9.8080 |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |          -0.0066 |          23.5101 |           9.8022 |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |          -0.0089 |          23.3702 |           9.8054 |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |          -0.0105 |          22.9329 |           9.8097 |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |          -0.0070 |          22.9744 |           9.8072 |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |          -0.0106 |          22.5480 |           9.8082 |
[32m[20221213 22:34:32 @agent_ppo2.py:185][0m |          -0.0077 |          22.5613 |           9.8100 |
[32m[20221213 22:34:33 @agent_ppo2.py:185][0m |          -0.0108 |          22.3451 |           9.8105 |
[32m[20221213 22:34:33 @agent_ppo2.py:185][0m |          -0.0119 |          22.1147 |           9.8078 |
[32m[20221213 22:34:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.13
[32m[20221213 22:34:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.25
[32m[20221213 22:34:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.40
[32m[20221213 22:34:33 @agent_ppo2.py:143][0m Total time:      16.33 min
[32m[20221213 22:34:33 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221213 22:34:33 @agent_ppo2.py:121][0m #------------------------ Iteration 773 --------------------------#
[32m[20221213 22:34:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:33 @agent_ppo2.py:185][0m |           0.0041 |          22.7567 |          10.0310 |
[32m[20221213 22:34:33 @agent_ppo2.py:185][0m |          -0.0058 |          19.3947 |          10.0098 |
[32m[20221213 22:34:33 @agent_ppo2.py:185][0m |          -0.0082 |          18.8854 |           9.9960 |
[32m[20221213 22:34:33 @agent_ppo2.py:185][0m |          -0.0050 |          18.7139 |           9.9967 |
[32m[20221213 22:34:33 @agent_ppo2.py:185][0m |          -0.0023 |          19.9196 |           9.9958 |
[32m[20221213 22:34:34 @agent_ppo2.py:185][0m |          -0.0046 |          18.8191 |          10.0049 |
[32m[20221213 22:34:34 @agent_ppo2.py:185][0m |          -0.0051 |          18.2650 |           9.9940 |
[32m[20221213 22:34:34 @agent_ppo2.py:185][0m |          -0.0069 |          18.1153 |           9.9937 |
[32m[20221213 22:34:34 @agent_ppo2.py:185][0m |          -0.0115 |          18.0199 |           9.9914 |
[32m[20221213 22:34:34 @agent_ppo2.py:185][0m |          -0.0002 |          20.9073 |           9.9877 |
[32m[20221213 22:34:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.83
[32m[20221213 22:34:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 317.54
[32m[20221213 22:34:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.39
[32m[20221213 22:34:34 @agent_ppo2.py:143][0m Total time:      16.35 min
[32m[20221213 22:34:34 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221213 22:34:34 @agent_ppo2.py:121][0m #------------------------ Iteration 774 --------------------------#
[32m[20221213 22:34:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:34 @agent_ppo2.py:185][0m |           0.0036 |          19.5505 |          10.1408 |
[32m[20221213 22:34:34 @agent_ppo2.py:185][0m |          -0.0051 |          16.0285 |          10.1312 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0059 |          15.2979 |          10.1280 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0097 |          14.9133 |          10.1241 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0040 |          15.1168 |          10.1215 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0122 |          14.3671 |          10.1153 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0118 |          14.6558 |          10.1159 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0052 |          14.0367 |          10.1155 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0116 |          13.9500 |          10.1196 |
[32m[20221213 22:34:35 @agent_ppo2.py:185][0m |          -0.0123 |          13.8365 |          10.1130 |
[32m[20221213 22:34:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.87
[32m[20221213 22:34:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.62
[32m[20221213 22:34:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.50
[32m[20221213 22:34:35 @agent_ppo2.py:143][0m Total time:      16.37 min
[32m[20221213 22:34:35 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221213 22:34:35 @agent_ppo2.py:121][0m #------------------------ Iteration 775 --------------------------#
[32m[20221213 22:34:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |           0.0015 |          27.7959 |          10.0520 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |           0.0081 |          27.3751 |          10.0509 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0026 |          26.3399 |          10.0427 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0007 |          26.9502 |          10.0364 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0040 |          25.5722 |          10.0490 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0057 |          24.9975 |          10.0486 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0089 |          24.8424 |          10.0484 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0082 |          24.7044 |          10.0440 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0102 |          24.5388 |          10.0491 |
[32m[20221213 22:34:36 @agent_ppo2.py:185][0m |          -0.0147 |          24.5277 |          10.0406 |
[32m[20221213 22:34:36 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:34:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.67
[32m[20221213 22:34:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.96
[32m[20221213 22:34:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.28
[32m[20221213 22:34:37 @agent_ppo2.py:143][0m Total time:      16.39 min
[32m[20221213 22:34:37 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221213 22:34:37 @agent_ppo2.py:121][0m #------------------------ Iteration 776 --------------------------#
[32m[20221213 22:34:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:34:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:37 @agent_ppo2.py:185][0m |           0.0121 |          28.0033 |          10.1223 |
[32m[20221213 22:34:37 @agent_ppo2.py:185][0m |          -0.0033 |          25.6442 |          10.1082 |
[32m[20221213 22:34:37 @agent_ppo2.py:185][0m |          -0.0054 |          25.4782 |          10.1053 |
[32m[20221213 22:34:37 @agent_ppo2.py:185][0m |          -0.0096 |          25.2103 |          10.1064 |
[32m[20221213 22:34:37 @agent_ppo2.py:185][0m |          -0.0061 |          24.9877 |          10.1053 |
[32m[20221213 22:34:37 @agent_ppo2.py:185][0m |           0.0046 |          27.1196 |          10.1003 |
[32m[20221213 22:34:37 @agent_ppo2.py:185][0m |           0.0092 |          26.7872 |          10.1070 |
[32m[20221213 22:34:38 @agent_ppo2.py:185][0m |          -0.0090 |          24.8798 |          10.0865 |
[32m[20221213 22:34:38 @agent_ppo2.py:185][0m |          -0.0080 |          24.6438 |          10.1072 |
[32m[20221213 22:34:38 @agent_ppo2.py:185][0m |          -0.0092 |          24.5923 |          10.0955 |
[32m[20221213 22:34:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:34:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.49
[32m[20221213 22:34:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.83
[32m[20221213 22:34:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.06
[32m[20221213 22:34:38 @agent_ppo2.py:143][0m Total time:      16.42 min
[32m[20221213 22:34:38 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221213 22:34:38 @agent_ppo2.py:121][0m #------------------------ Iteration 777 --------------------------#
[32m[20221213 22:34:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:38 @agent_ppo2.py:185][0m |           0.0009 |          25.5370 |          10.0729 |
[32m[20221213 22:34:38 @agent_ppo2.py:185][0m |          -0.0039 |          24.9974 |          10.0614 |
[32m[20221213 22:34:38 @agent_ppo2.py:185][0m |          -0.0120 |          24.9715 |          10.0436 |
[32m[20221213 22:34:38 @agent_ppo2.py:185][0m |          -0.0087 |          24.6197 |          10.0450 |
[32m[20221213 22:34:39 @agent_ppo2.py:185][0m |          -0.0093 |          24.4436 |          10.0401 |
[32m[20221213 22:34:39 @agent_ppo2.py:185][0m |          -0.0083 |          24.2651 |          10.0238 |
[32m[20221213 22:34:39 @agent_ppo2.py:185][0m |           0.0002 |          25.5188 |          10.0291 |
[32m[20221213 22:34:39 @agent_ppo2.py:185][0m |          -0.0076 |          24.1553 |          10.0141 |
[32m[20221213 22:34:39 @agent_ppo2.py:185][0m |          -0.0087 |          23.8757 |          10.0218 |
[32m[20221213 22:34:39 @agent_ppo2.py:185][0m |          -0.0054 |          24.0204 |          10.0231 |
[32m[20221213 22:34:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:34:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.57
[32m[20221213 22:34:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.54
[32m[20221213 22:34:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.37
[32m[20221213 22:34:39 @agent_ppo2.py:143][0m Total time:      16.44 min
[32m[20221213 22:34:39 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221213 22:34:39 @agent_ppo2.py:121][0m #------------------------ Iteration 778 --------------------------#
[32m[20221213 22:34:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:39 @agent_ppo2.py:185][0m |           0.0026 |          26.5676 |           9.9717 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0049 |          25.1027 |           9.9549 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0060 |          24.7074 |           9.9540 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0057 |          24.5139 |           9.9511 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0057 |          24.7302 |           9.9529 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0066 |          24.1544 |           9.9406 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0075 |          24.0040 |           9.9526 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0030 |          23.9831 |           9.9498 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0097 |          23.8948 |           9.9455 |
[32m[20221213 22:34:40 @agent_ppo2.py:185][0m |          -0.0087 |          23.8674 |           9.9461 |
[32m[20221213 22:34:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:34:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.80
[32m[20221213 22:34:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 342.42
[32m[20221213 22:34:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.66
[32m[20221213 22:34:40 @agent_ppo2.py:143][0m Total time:      16.46 min
[32m[20221213 22:34:40 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221213 22:34:40 @agent_ppo2.py:121][0m #------------------------ Iteration 779 --------------------------#
[32m[20221213 22:34:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |          -0.0020 |          27.0077 |          10.0762 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |          -0.0053 |          25.8655 |          10.0768 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |          -0.0041 |          25.4045 |          10.0768 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |           0.0043 |          27.2583 |          10.0743 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |          -0.0080 |          24.9100 |          10.0625 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |          -0.0071 |          24.6783 |          10.0696 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |           0.0037 |          28.4435 |          10.0734 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |          -0.0052 |          24.6498 |          10.0651 |
[32m[20221213 22:34:41 @agent_ppo2.py:185][0m |          -0.0082 |          24.2535 |          10.0684 |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |          -0.0081 |          24.1320 |          10.0716 |
[32m[20221213 22:34:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:34:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.00
[32m[20221213 22:34:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.06
[32m[20221213 22:34:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.42
[32m[20221213 22:34:42 @agent_ppo2.py:143][0m Total time:      16.48 min
[32m[20221213 22:34:42 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221213 22:34:42 @agent_ppo2.py:121][0m #------------------------ Iteration 780 --------------------------#
[32m[20221213 22:34:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:34:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |           0.0107 |          24.9873 |          10.0292 |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |          -0.0074 |          21.3033 |          10.0210 |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |          -0.0052 |          20.5757 |          10.0252 |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |          -0.0053 |          20.1107 |          10.0174 |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |          -0.0079 |          19.9610 |          10.0202 |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |          -0.0005 |          20.0237 |          10.0229 |
[32m[20221213 22:34:42 @agent_ppo2.py:185][0m |          -0.0031 |          19.7166 |          10.0280 |
[32m[20221213 22:34:43 @agent_ppo2.py:185][0m |          -0.0093 |          19.4910 |          10.0240 |
[32m[20221213 22:34:43 @agent_ppo2.py:185][0m |          -0.0077 |          19.4885 |          10.0282 |
[32m[20221213 22:34:43 @agent_ppo2.py:185][0m |          -0.0108 |          19.2035 |          10.0266 |
[32m[20221213 22:34:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.67
[32m[20221213 22:34:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.33
[32m[20221213 22:34:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.38
[32m[20221213 22:34:43 @agent_ppo2.py:143][0m Total time:      16.50 min
[32m[20221213 22:34:43 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221213 22:34:43 @agent_ppo2.py:121][0m #------------------------ Iteration 781 --------------------------#
[32m[20221213 22:34:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:43 @agent_ppo2.py:185][0m |          -0.0040 |          25.5799 |           9.9085 |
[32m[20221213 22:34:43 @agent_ppo2.py:185][0m |          -0.0101 |          24.2130 |           9.8940 |
[32m[20221213 22:34:43 @agent_ppo2.py:185][0m |          -0.0120 |          23.5218 |           9.8965 |
[32m[20221213 22:34:43 @agent_ppo2.py:185][0m |          -0.0123 |          23.1375 |           9.9050 |
[32m[20221213 22:34:44 @agent_ppo2.py:185][0m |          -0.0108 |          22.8549 |           9.8961 |
[32m[20221213 22:34:44 @agent_ppo2.py:185][0m |          -0.0106 |          22.5743 |           9.9009 |
[32m[20221213 22:34:44 @agent_ppo2.py:185][0m |          -0.0105 |          22.4651 |           9.9028 |
[32m[20221213 22:34:44 @agent_ppo2.py:185][0m |          -0.0139 |          22.2388 |           9.9078 |
[32m[20221213 22:34:44 @agent_ppo2.py:185][0m |          -0.0018 |          24.8792 |           9.9065 |
[32m[20221213 22:34:44 @agent_ppo2.py:185][0m |           0.0010 |          24.6203 |           9.9040 |
[32m[20221213 22:34:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:34:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.35
[32m[20221213 22:34:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.81
[32m[20221213 22:34:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.77
[32m[20221213 22:34:44 @agent_ppo2.py:143][0m Total time:      16.52 min
[32m[20221213 22:34:44 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221213 22:34:44 @agent_ppo2.py:121][0m #------------------------ Iteration 782 --------------------------#
[32m[20221213 22:34:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:44 @agent_ppo2.py:185][0m |           0.0000 |          26.5059 |          10.1551 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0046 |          25.1134 |          10.1498 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0086 |          24.3556 |          10.1417 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0078 |          23.9251 |          10.1515 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0081 |          23.6227 |          10.1511 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0087 |          23.5323 |          10.1467 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0055 |          24.0993 |          10.1526 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0086 |          23.3033 |          10.1446 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0064 |          23.0559 |          10.1512 |
[32m[20221213 22:34:45 @agent_ppo2.py:185][0m |          -0.0084 |          23.0883 |          10.1543 |
[32m[20221213 22:34:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 205.34
[32m[20221213 22:34:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.86
[32m[20221213 22:34:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.15
[32m[20221213 22:34:45 @agent_ppo2.py:143][0m Total time:      16.54 min
[32m[20221213 22:34:45 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221213 22:34:45 @agent_ppo2.py:121][0m #------------------------ Iteration 783 --------------------------#
[32m[20221213 22:34:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:34:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0014 |          24.6523 |          10.2892 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0089 |          21.5469 |          10.2796 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |           0.0014 |          22.2876 |          10.2746 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0035 |          20.3654 |          10.2667 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0103 |          19.9605 |          10.2652 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0111 |          19.7349 |          10.2665 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0088 |          19.4158 |          10.2656 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0097 |          19.3889 |          10.2678 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0089 |          19.1176 |          10.2633 |
[32m[20221213 22:34:46 @agent_ppo2.py:185][0m |          -0.0143 |          19.0072 |          10.2607 |
[32m[20221213 22:34:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.39
[32m[20221213 22:34:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.07
[32m[20221213 22:34:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.64
[32m[20221213 22:34:47 @agent_ppo2.py:143][0m Total time:      16.56 min
[32m[20221213 22:34:47 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221213 22:34:47 @agent_ppo2.py:121][0m #------------------------ Iteration 784 --------------------------#
[32m[20221213 22:34:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:47 @agent_ppo2.py:185][0m |          -0.0003 |          27.3726 |          10.3871 |
[32m[20221213 22:34:47 @agent_ppo2.py:185][0m |           0.0011 |          27.9517 |          10.3825 |
[32m[20221213 22:34:47 @agent_ppo2.py:185][0m |           0.0060 |          26.3415 |          10.3642 |
[32m[20221213 22:34:47 @agent_ppo2.py:185][0m |          -0.0090 |          24.9113 |          10.3737 |
[32m[20221213 22:34:47 @agent_ppo2.py:185][0m |          -0.0017 |          25.9927 |          10.3669 |
[32m[20221213 22:34:47 @agent_ppo2.py:185][0m |          -0.0107 |          24.4440 |          10.3832 |
[32m[20221213 22:34:47 @agent_ppo2.py:185][0m |          -0.0135 |          24.1108 |          10.3732 |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |           0.0041 |          26.1760 |          10.3723 |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |          -0.0043 |          24.2363 |          10.3781 |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |          -0.0137 |          23.6311 |          10.3751 |
[32m[20221213 22:34:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.81
[32m[20221213 22:34:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.17
[32m[20221213 22:34:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.77
[32m[20221213 22:34:48 @agent_ppo2.py:143][0m Total time:      16.58 min
[32m[20221213 22:34:48 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221213 22:34:48 @agent_ppo2.py:121][0m #------------------------ Iteration 785 --------------------------#
[32m[20221213 22:34:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |          -0.0002 |          23.1940 |          10.2729 |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |          -0.0058 |          20.3915 |          10.2641 |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |          -0.0053 |          18.8446 |          10.2669 |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |          -0.0093 |          18.0196 |          10.2593 |
[32m[20221213 22:34:48 @agent_ppo2.py:185][0m |          -0.0068 |          17.7364 |          10.2548 |
[32m[20221213 22:34:49 @agent_ppo2.py:185][0m |          -0.0104 |          17.1554 |          10.2547 |
[32m[20221213 22:34:49 @agent_ppo2.py:185][0m |          -0.0093 |          16.8842 |          10.2576 |
[32m[20221213 22:34:49 @agent_ppo2.py:185][0m |          -0.0092 |          16.6519 |          10.2445 |
[32m[20221213 22:34:49 @agent_ppo2.py:185][0m |          -0.0067 |          17.1531 |          10.2471 |
[32m[20221213 22:34:49 @agent_ppo2.py:185][0m |          -0.0118 |          16.1608 |          10.2466 |
[32m[20221213 22:34:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.83
[32m[20221213 22:34:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.48
[32m[20221213 22:34:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.32
[32m[20221213 22:34:49 @agent_ppo2.py:143][0m Total time:      16.60 min
[32m[20221213 22:34:49 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221213 22:34:49 @agent_ppo2.py:121][0m #------------------------ Iteration 786 --------------------------#
[32m[20221213 22:34:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:49 @agent_ppo2.py:185][0m |          -0.0020 |          35.7023 |          10.3562 |
[32m[20221213 22:34:49 @agent_ppo2.py:185][0m |          -0.0057 |          33.6826 |          10.3365 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |           0.0002 |          34.1099 |          10.3318 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |          -0.0088 |          33.0698 |          10.3264 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |          -0.0088 |          32.6039 |          10.3223 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |          -0.0024 |          33.1354 |          10.3239 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |          -0.0110 |          32.2220 |          10.3176 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |          -0.0096 |          31.9587 |          10.3213 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |          -0.0000 |          34.8230 |          10.3215 |
[32m[20221213 22:34:50 @agent_ppo2.py:185][0m |          -0.0093 |          31.9148 |          10.3206 |
[32m[20221213 22:34:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.87
[32m[20221213 22:34:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.18
[32m[20221213 22:34:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.30
[32m[20221213 22:34:50 @agent_ppo2.py:143][0m Total time:      16.62 min
[32m[20221213 22:34:50 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221213 22:34:50 @agent_ppo2.py:121][0m #------------------------ Iteration 787 --------------------------#
[32m[20221213 22:34:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |           0.0036 |          27.1183 |          10.2614 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |           0.0002 |          24.7240 |          10.2513 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0055 |          23.5717 |          10.2469 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0058 |          22.7653 |          10.2465 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0070 |          22.5590 |          10.2534 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0070 |          22.0519 |          10.2521 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0077 |          21.9294 |          10.2496 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0109 |          21.7216 |          10.2512 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0071 |          21.7920 |          10.2553 |
[32m[20221213 22:34:51 @agent_ppo2.py:185][0m |          -0.0085 |          21.3868 |          10.2554 |
[32m[20221213 22:34:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.60
[32m[20221213 22:34:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.28
[32m[20221213 22:34:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.57
[32m[20221213 22:34:52 @agent_ppo2.py:143][0m Total time:      16.64 min
[32m[20221213 22:34:52 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221213 22:34:52 @agent_ppo2.py:121][0m #------------------------ Iteration 788 --------------------------#
[32m[20221213 22:34:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:34:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |           0.0044 |           4.5794 |          10.2619 |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |          -0.0005 |           4.1104 |          10.2583 |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |          -0.0016 |           4.0387 |          10.2667 |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |          -0.0048 |           4.0048 |          10.2606 |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |          -0.0030 |           3.9935 |          10.2586 |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |          -0.0040 |           3.9679 |          10.2601 |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |          -0.0036 |           3.9638 |          10.2614 |
[32m[20221213 22:34:52 @agent_ppo2.py:185][0m |           0.0071 |           4.1434 |          10.2654 |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |          -0.0037 |           3.9465 |          10.2604 |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |          -0.0028 |           3.9545 |          10.2882 |
[32m[20221213 22:34:53 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:34:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:34:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:34:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.88
[32m[20221213 22:34:53 @agent_ppo2.py:143][0m Total time:      16.66 min
[32m[20221213 22:34:53 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221213 22:34:53 @agent_ppo2.py:121][0m #------------------------ Iteration 789 --------------------------#
[32m[20221213 22:34:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:34:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |           0.0028 |          17.1077 |          10.4018 |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |          -0.0024 |          15.1217 |          10.3998 |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |          -0.0097 |          14.4415 |          10.4039 |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |          -0.0016 |          14.0686 |          10.4011 |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |          -0.0037 |          13.8215 |          10.3979 |
[32m[20221213 22:34:53 @agent_ppo2.py:185][0m |          -0.0036 |          13.5904 |          10.3986 |
[32m[20221213 22:34:54 @agent_ppo2.py:185][0m |          -0.0018 |          13.3505 |          10.3936 |
[32m[20221213 22:34:54 @agent_ppo2.py:185][0m |          -0.0050 |          13.1801 |          10.4005 |
[32m[20221213 22:34:54 @agent_ppo2.py:185][0m |          -0.0115 |          13.0648 |          10.4018 |
[32m[20221213 22:34:54 @agent_ppo2.py:185][0m |          -0.0048 |          13.0613 |          10.3994 |
[32m[20221213 22:34:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.81
[32m[20221213 22:34:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.86
[32m[20221213 22:34:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 208.84
[32m[20221213 22:34:54 @agent_ppo2.py:143][0m Total time:      16.68 min
[32m[20221213 22:34:54 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221213 22:34:54 @agent_ppo2.py:121][0m #------------------------ Iteration 790 --------------------------#
[32m[20221213 22:34:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:34:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:54 @agent_ppo2.py:185][0m |           0.0023 |          18.3172 |          10.3764 |
[32m[20221213 22:34:54 @agent_ppo2.py:185][0m |          -0.0028 |          16.5943 |          10.3472 |
[32m[20221213 22:34:54 @agent_ppo2.py:185][0m |          -0.0110 |          15.8993 |          10.3481 |
[32m[20221213 22:34:55 @agent_ppo2.py:185][0m |          -0.0066 |          15.3714 |          10.3477 |
[32m[20221213 22:34:55 @agent_ppo2.py:185][0m |          -0.0087 |          15.1035 |          10.3582 |
[32m[20221213 22:34:55 @agent_ppo2.py:185][0m |          -0.0033 |          14.9908 |          10.3556 |
[32m[20221213 22:34:55 @agent_ppo2.py:185][0m |          -0.0132 |          14.6659 |          10.3447 |
[32m[20221213 22:34:55 @agent_ppo2.py:185][0m |          -0.0120 |          14.4493 |          10.3611 |
[32m[20221213 22:34:55 @agent_ppo2.py:185][0m |          -0.0187 |          14.2690 |          10.3656 |
[32m[20221213 22:34:55 @agent_ppo2.py:185][0m |          -0.0102 |          14.0917 |          10.3623 |
[32m[20221213 22:34:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.00
[32m[20221213 22:34:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.08
[32m[20221213 22:34:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.08
[32m[20221213 22:34:55 @agent_ppo2.py:143][0m Total time:      16.71 min
[32m[20221213 22:34:55 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221213 22:34:55 @agent_ppo2.py:121][0m #------------------------ Iteration 791 --------------------------#
[32m[20221213 22:34:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0001 |          31.4774 |          10.6399 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0049 |          29.7685 |          10.6313 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0057 |          28.9967 |          10.6347 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0073 |          28.6372 |          10.6362 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0094 |          28.0191 |          10.6329 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |           0.0022 |          30.3522 |          10.6387 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0014 |          29.3256 |          10.6410 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0135 |          27.5870 |          10.6365 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0147 |          27.4101 |          10.6441 |
[32m[20221213 22:34:56 @agent_ppo2.py:185][0m |          -0.0086 |          27.1176 |          10.6464 |
[32m[20221213 22:34:56 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:34:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.14
[32m[20221213 22:34:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.67
[32m[20221213 22:34:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.86
[32m[20221213 22:34:56 @agent_ppo2.py:143][0m Total time:      16.73 min
[32m[20221213 22:34:56 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221213 22:34:56 @agent_ppo2.py:121][0m #------------------------ Iteration 792 --------------------------#
[32m[20221213 22:34:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0041 |          21.3666 |          10.5378 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0030 |          18.8352 |          10.5271 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |           0.0065 |          20.3610 |          10.5048 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0103 |          17.5807 |          10.5206 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0086 |          17.3468 |          10.5126 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0045 |          17.7180 |          10.5164 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0013 |          18.3136 |          10.5036 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0120 |          16.7156 |          10.5028 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0113 |          16.5835 |          10.5048 |
[32m[20221213 22:34:57 @agent_ppo2.py:185][0m |          -0.0115 |          16.4205 |          10.5113 |
[32m[20221213 22:34:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:34:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.08
[32m[20221213 22:34:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.77
[32m[20221213 22:34:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.01
[32m[20221213 22:34:58 @agent_ppo2.py:143][0m Total time:      16.75 min
[32m[20221213 22:34:58 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221213 22:34:58 @agent_ppo2.py:121][0m #------------------------ Iteration 793 --------------------------#
[32m[20221213 22:34:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:58 @agent_ppo2.py:185][0m |          -0.0031 |          28.7832 |          10.4446 |
[32m[20221213 22:34:58 @agent_ppo2.py:185][0m |          -0.0049 |          26.5644 |          10.4283 |
[32m[20221213 22:34:58 @agent_ppo2.py:185][0m |           0.0014 |          27.1218 |          10.4168 |
[32m[20221213 22:34:58 @agent_ppo2.py:185][0m |          -0.0069 |          25.6063 |          10.4059 |
[32m[20221213 22:34:58 @agent_ppo2.py:185][0m |          -0.0103 |          25.1645 |          10.4081 |
[32m[20221213 22:34:58 @agent_ppo2.py:185][0m |          -0.0086 |          24.9325 |          10.4004 |
[32m[20221213 22:34:58 @agent_ppo2.py:185][0m |          -0.0128 |          24.8042 |          10.4030 |
[32m[20221213 22:34:59 @agent_ppo2.py:185][0m |          -0.0100 |          24.8409 |          10.4002 |
[32m[20221213 22:34:59 @agent_ppo2.py:185][0m |          -0.0106 |          24.3004 |          10.3951 |
[32m[20221213 22:34:59 @agent_ppo2.py:185][0m |          -0.0096 |          24.4236 |          10.3913 |
[32m[20221213 22:34:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:34:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.18
[32m[20221213 22:34:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.95
[32m[20221213 22:34:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.03
[32m[20221213 22:34:59 @agent_ppo2.py:143][0m Total time:      16.77 min
[32m[20221213 22:34:59 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221213 22:34:59 @agent_ppo2.py:121][0m #------------------------ Iteration 794 --------------------------#
[32m[20221213 22:34:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:34:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:34:59 @agent_ppo2.py:185][0m |          -0.0038 |          33.1024 |          10.5872 |
[32m[20221213 22:34:59 @agent_ppo2.py:185][0m |           0.0055 |          34.1003 |          10.5749 |
[32m[20221213 22:34:59 @agent_ppo2.py:185][0m |          -0.0107 |          30.7904 |          10.5622 |
[32m[20221213 22:34:59 @agent_ppo2.py:185][0m |          -0.0092 |          29.9942 |          10.5726 |
[32m[20221213 22:35:00 @agent_ppo2.py:185][0m |          -0.0092 |          29.7263 |          10.5676 |
[32m[20221213 22:35:00 @agent_ppo2.py:185][0m |          -0.0093 |          29.4065 |          10.5749 |
[32m[20221213 22:35:00 @agent_ppo2.py:185][0m |          -0.0099 |          29.3026 |          10.5821 |
[32m[20221213 22:35:00 @agent_ppo2.py:185][0m |          -0.0129 |          29.1226 |          10.5735 |
[32m[20221213 22:35:00 @agent_ppo2.py:185][0m |          -0.0087 |          29.2966 |          10.5750 |
[32m[20221213 22:35:00 @agent_ppo2.py:185][0m |          -0.0098 |          29.0289 |          10.5788 |
[32m[20221213 22:35:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.41
[32m[20221213 22:35:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.20
[32m[20221213 22:35:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.95
[32m[20221213 22:35:00 @agent_ppo2.py:143][0m Total time:      16.79 min
[32m[20221213 22:35:00 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221213 22:35:00 @agent_ppo2.py:121][0m #------------------------ Iteration 795 --------------------------#
[32m[20221213 22:35:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:00 @agent_ppo2.py:185][0m |          -0.0031 |          25.1336 |          10.7358 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0033 |          23.3890 |          10.7340 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0064 |          22.8070 |          10.7308 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0112 |          22.4098 |          10.7376 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0062 |          22.0426 |          10.7409 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0048 |          21.9615 |          10.7352 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0095 |          21.5711 |          10.7390 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0090 |          21.4491 |          10.7406 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0094 |          21.1943 |          10.7460 |
[32m[20221213 22:35:01 @agent_ppo2.py:185][0m |          -0.0060 |          21.0790 |          10.7356 |
[32m[20221213 22:35:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.36
[32m[20221213 22:35:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.74
[32m[20221213 22:35:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.03
[32m[20221213 22:35:01 @agent_ppo2.py:143][0m Total time:      16.81 min
[32m[20221213 22:35:01 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221213 22:35:01 @agent_ppo2.py:121][0m #------------------------ Iteration 796 --------------------------#
[32m[20221213 22:35:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |           0.0017 |          31.8356 |          10.7295 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0036 |          30.5346 |          10.7230 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0100 |          29.9462 |          10.7196 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0040 |          29.4474 |          10.7169 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0032 |          29.5089 |          10.7198 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0046 |          29.1915 |          10.7158 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0111 |          28.9708 |          10.7175 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0101 |          28.8119 |          10.7209 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0067 |          28.7916 |          10.7220 |
[32m[20221213 22:35:02 @agent_ppo2.py:185][0m |          -0.0082 |          28.6735 |          10.7260 |
[32m[20221213 22:35:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.50
[32m[20221213 22:35:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.18
[32m[20221213 22:35:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.75
[32m[20221213 22:35:03 @agent_ppo2.py:143][0m Total time:      16.83 min
[32m[20221213 22:35:03 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221213 22:35:03 @agent_ppo2.py:121][0m #------------------------ Iteration 797 --------------------------#
[32m[20221213 22:35:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:03 @agent_ppo2.py:185][0m |           0.0054 |          20.7392 |          10.8268 |
[32m[20221213 22:35:03 @agent_ppo2.py:185][0m |          -0.0052 |          18.8174 |          10.8055 |
[32m[20221213 22:35:03 @agent_ppo2.py:185][0m |          -0.0055 |          18.4424 |          10.8081 |
[32m[20221213 22:35:03 @agent_ppo2.py:185][0m |          -0.0080 |          17.9961 |          10.7996 |
[32m[20221213 22:35:03 @agent_ppo2.py:185][0m |          -0.0112 |          18.0236 |          10.7973 |
[32m[20221213 22:35:03 @agent_ppo2.py:185][0m |          -0.0134 |          17.5896 |          10.7862 |
[32m[20221213 22:35:03 @agent_ppo2.py:185][0m |          -0.0147 |          17.3097 |          10.7950 |
[32m[20221213 22:35:04 @agent_ppo2.py:185][0m |          -0.0076 |          17.1215 |          10.7922 |
[32m[20221213 22:35:04 @agent_ppo2.py:185][0m |          -0.0104 |          17.0798 |          10.7865 |
[32m[20221213 22:35:04 @agent_ppo2.py:185][0m |          -0.0117 |          16.9374 |          10.7789 |
[32m[20221213 22:35:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 172.61
[32m[20221213 22:35:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.73
[32m[20221213 22:35:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 185.56
[32m[20221213 22:35:04 @agent_ppo2.py:143][0m Total time:      16.85 min
[32m[20221213 22:35:04 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221213 22:35:04 @agent_ppo2.py:121][0m #------------------------ Iteration 798 --------------------------#
[32m[20221213 22:35:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:04 @agent_ppo2.py:185][0m |           0.0016 |          27.3095 |          10.7096 |
[32m[20221213 22:35:04 @agent_ppo2.py:185][0m |          -0.0024 |          25.8209 |          10.6957 |
[32m[20221213 22:35:04 @agent_ppo2.py:185][0m |          -0.0028 |          24.9026 |          10.7063 |
[32m[20221213 22:35:04 @agent_ppo2.py:185][0m |          -0.0063 |          24.3608 |          10.7044 |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |          -0.0089 |          24.0087 |          10.7009 |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |           0.0028 |          26.5144 |          10.6960 |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |          -0.0066 |          23.4200 |          10.7006 |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |          -0.0091 |          23.1445 |          10.7094 |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |          -0.0064 |          22.8200 |          10.7025 |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |          -0.0068 |          22.7747 |          10.7048 |
[32m[20221213 22:35:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.59
[32m[20221213 22:35:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.21
[32m[20221213 22:35:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.49
[32m[20221213 22:35:05 @agent_ppo2.py:143][0m Total time:      16.87 min
[32m[20221213 22:35:05 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221213 22:35:05 @agent_ppo2.py:121][0m #------------------------ Iteration 799 --------------------------#
[32m[20221213 22:35:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |          -0.0025 |          30.7958 |          10.9542 |
[32m[20221213 22:35:05 @agent_ppo2.py:185][0m |          -0.0067 |          28.9540 |          10.9418 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0177 |          28.5920 |          10.9384 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0014 |          29.0911 |          10.9337 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0073 |          27.8217 |          10.9294 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0065 |          27.8045 |          10.9365 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0006 |          28.9433 |          10.9338 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0152 |          27.1155 |          10.9344 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0110 |          26.9656 |          10.9255 |
[32m[20221213 22:35:06 @agent_ppo2.py:185][0m |          -0.0092 |          27.0126 |          10.9262 |
[32m[20221213 22:35:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.24
[32m[20221213 22:35:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.43
[32m[20221213 22:35:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.72
[32m[20221213 22:35:06 @agent_ppo2.py:143][0m Total time:      16.89 min
[32m[20221213 22:35:06 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221213 22:35:06 @agent_ppo2.py:121][0m #------------------------ Iteration 800 --------------------------#
[32m[20221213 22:35:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:35:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0004 |          32.2652 |          10.7789 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0059 |          31.0712 |          10.7725 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0032 |          30.6309 |          10.7678 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0107 |          30.4326 |          10.7646 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0070 |          30.0820 |          10.7643 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0017 |          30.1106 |          10.7620 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0063 |          29.8979 |          10.7530 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0089 |          29.7182 |          10.7614 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0106 |          29.5002 |          10.7523 |
[32m[20221213 22:35:07 @agent_ppo2.py:185][0m |          -0.0099 |          29.3967 |          10.7570 |
[32m[20221213 22:35:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.79
[32m[20221213 22:35:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 371.19
[32m[20221213 22:35:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.46
[32m[20221213 22:35:08 @agent_ppo2.py:143][0m Total time:      16.91 min
[32m[20221213 22:35:08 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221213 22:35:08 @agent_ppo2.py:121][0m #------------------------ Iteration 801 --------------------------#
[32m[20221213 22:35:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |           0.0031 |          32.8811 |          10.6403 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |          -0.0011 |          31.9274 |          10.6282 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |          -0.0065 |          31.3239 |          10.6164 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |          -0.0036 |          31.0723 |          10.6193 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |           0.0003 |          31.6119 |          10.6131 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |          -0.0047 |          31.4985 |          10.6129 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |          -0.0094 |          30.6378 |          10.6167 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |          -0.0063 |          30.7270 |          10.6218 |
[32m[20221213 22:35:08 @agent_ppo2.py:185][0m |          -0.0098 |          30.4362 |          10.6248 |
[32m[20221213 22:35:09 @agent_ppo2.py:185][0m |          -0.0100 |          30.3596 |          10.6125 |
[32m[20221213 22:35:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.41
[32m[20221213 22:35:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.77
[32m[20221213 22:35:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.62
[32m[20221213 22:35:09 @agent_ppo2.py:143][0m Total time:      16.93 min
[32m[20221213 22:35:09 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221213 22:35:09 @agent_ppo2.py:121][0m #------------------------ Iteration 802 --------------------------#
[32m[20221213 22:35:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:09 @agent_ppo2.py:185][0m |          -0.0019 |          39.1251 |          10.6730 |
[32m[20221213 22:35:09 @agent_ppo2.py:185][0m |          -0.0026 |          37.9979 |          10.6745 |
[32m[20221213 22:35:09 @agent_ppo2.py:185][0m |          -0.0054 |          37.5036 |          10.6705 |
[32m[20221213 22:35:09 @agent_ppo2.py:185][0m |          -0.0057 |          37.2985 |          10.6692 |
[32m[20221213 22:35:09 @agent_ppo2.py:185][0m |          -0.0069 |          37.0092 |          10.6627 |
[32m[20221213 22:35:09 @agent_ppo2.py:185][0m |          -0.0046 |          36.7285 |          10.6590 |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |          -0.0114 |          36.6962 |          10.6564 |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |          -0.0075 |          36.5861 |          10.6508 |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |          -0.0112 |          36.4153 |          10.6509 |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |          -0.0080 |          36.2490 |          10.6545 |
[32m[20221213 22:35:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.04
[32m[20221213 22:35:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.01
[32m[20221213 22:35:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.52
[32m[20221213 22:35:10 @agent_ppo2.py:143][0m Total time:      16.95 min
[32m[20221213 22:35:10 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221213 22:35:10 @agent_ppo2.py:121][0m #------------------------ Iteration 803 --------------------------#
[32m[20221213 22:35:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |           0.0061 |          32.0564 |          10.9887 |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |          -0.0051 |          28.9186 |          10.9669 |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |          -0.0084 |          28.2927 |          10.9516 |
[32m[20221213 22:35:10 @agent_ppo2.py:185][0m |          -0.0092 |          27.8502 |          10.9570 |
[32m[20221213 22:35:11 @agent_ppo2.py:185][0m |          -0.0053 |          27.7092 |          10.9559 |
[32m[20221213 22:35:11 @agent_ppo2.py:185][0m |          -0.0079 |          27.4849 |          10.9543 |
[32m[20221213 22:35:11 @agent_ppo2.py:185][0m |          -0.0092 |          27.2775 |          10.9495 |
[32m[20221213 22:35:11 @agent_ppo2.py:185][0m |          -0.0108 |          27.1002 |          10.9496 |
[32m[20221213 22:35:11 @agent_ppo2.py:185][0m |           0.0031 |          31.4724 |          10.9498 |
[32m[20221213 22:35:11 @agent_ppo2.py:185][0m |          -0.0067 |          27.1812 |          10.9313 |
[32m[20221213 22:35:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.73
[32m[20221213 22:35:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.98
[32m[20221213 22:35:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.30
[32m[20221213 22:35:11 @agent_ppo2.py:143][0m Total time:      16.97 min
[32m[20221213 22:35:11 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221213 22:35:11 @agent_ppo2.py:121][0m #------------------------ Iteration 804 --------------------------#
[32m[20221213 22:35:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:11 @agent_ppo2.py:185][0m |           0.0024 |          38.9194 |          10.8349 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0030 |          36.1410 |          10.8259 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0042 |          35.1681 |          10.8195 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0006 |          34.2781 |          10.8186 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0066 |          33.7123 |          10.8142 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |           0.0091 |          36.6397 |          10.8184 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0089 |          33.8428 |          10.7911 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0145 |          33.3614 |          10.8003 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0151 |          33.2325 |          10.8019 |
[32m[20221213 22:35:12 @agent_ppo2.py:185][0m |          -0.0088 |          32.8015 |          10.8024 |
[32m[20221213 22:35:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.95
[32m[20221213 22:35:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.58
[32m[20221213 22:35:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.94
[32m[20221213 22:35:12 @agent_ppo2.py:143][0m Total time:      16.99 min
[32m[20221213 22:35:12 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221213 22:35:12 @agent_ppo2.py:121][0m #------------------------ Iteration 805 --------------------------#
[32m[20221213 22:35:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0027 |          22.7606 |          10.7942 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0060 |          19.3203 |          10.7763 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0071 |          18.8530 |          10.7670 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0039 |          20.8758 |          10.7542 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0065 |          18.6690 |          10.7612 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0119 |          18.4889 |          10.7569 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0012 |          19.6385 |          10.7518 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0141 |          18.3181 |          10.7461 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0138 |          18.2598 |          10.7471 |
[32m[20221213 22:35:13 @agent_ppo2.py:185][0m |          -0.0040 |          19.9129 |          10.7413 |
[32m[20221213 22:35:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:35:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.14
[32m[20221213 22:35:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.50
[32m[20221213 22:35:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.74
[32m[20221213 22:35:14 @agent_ppo2.py:143][0m Total time:      17.01 min
[32m[20221213 22:35:14 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221213 22:35:14 @agent_ppo2.py:121][0m #------------------------ Iteration 806 --------------------------#
[32m[20221213 22:35:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:14 @agent_ppo2.py:185][0m |          -0.0031 |          26.3370 |          10.5037 |
[32m[20221213 22:35:14 @agent_ppo2.py:185][0m |          -0.0050 |          23.6613 |          10.4972 |
[32m[20221213 22:35:14 @agent_ppo2.py:185][0m |          -0.0086 |          22.7791 |          10.4908 |
[32m[20221213 22:35:14 @agent_ppo2.py:185][0m |          -0.0100 |          22.0602 |          10.4847 |
[32m[20221213 22:35:14 @agent_ppo2.py:185][0m |          -0.0079 |          21.9866 |          10.4842 |
[32m[20221213 22:35:14 @agent_ppo2.py:185][0m |          -0.0091 |          21.3323 |          10.4822 |
[32m[20221213 22:35:14 @agent_ppo2.py:185][0m |          -0.0093 |          21.0640 |          10.4820 |
[32m[20221213 22:35:15 @agent_ppo2.py:185][0m |          -0.0099 |          20.8030 |          10.4756 |
[32m[20221213 22:35:15 @agent_ppo2.py:185][0m |          -0.0136 |          20.5751 |          10.4726 |
[32m[20221213 22:35:15 @agent_ppo2.py:185][0m |          -0.0102 |          20.3332 |          10.4728 |
[32m[20221213 22:35:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 171.68
[32m[20221213 22:35:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.11
[32m[20221213 22:35:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.01
[32m[20221213 22:35:15 @agent_ppo2.py:143][0m Total time:      17.03 min
[32m[20221213 22:35:15 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221213 22:35:15 @agent_ppo2.py:121][0m #------------------------ Iteration 807 --------------------------#
[32m[20221213 22:35:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:15 @agent_ppo2.py:185][0m |          -0.0032 |          28.8666 |          10.6822 |
[32m[20221213 22:35:15 @agent_ppo2.py:185][0m |          -0.0066 |          26.7789 |          10.6727 |
[32m[20221213 22:35:15 @agent_ppo2.py:185][0m |          -0.0078 |          26.2762 |          10.6768 |
[32m[20221213 22:35:15 @agent_ppo2.py:185][0m |          -0.0113 |          25.8324 |          10.6742 |
[32m[20221213 22:35:16 @agent_ppo2.py:185][0m |          -0.0103 |          25.5932 |          10.6761 |
[32m[20221213 22:35:16 @agent_ppo2.py:185][0m |          -0.0081 |          25.4614 |          10.6758 |
[32m[20221213 22:35:16 @agent_ppo2.py:185][0m |          -0.0107 |          25.4911 |          10.6785 |
[32m[20221213 22:35:16 @agent_ppo2.py:185][0m |          -0.0066 |          25.1498 |          10.6699 |
[32m[20221213 22:35:16 @agent_ppo2.py:185][0m |          -0.0105 |          25.0490 |          10.6769 |
[32m[20221213 22:35:16 @agent_ppo2.py:185][0m |          -0.0106 |          24.8574 |          10.6748 |
[32m[20221213 22:35:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.48
[32m[20221213 22:35:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.05
[32m[20221213 22:35:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.04
[32m[20221213 22:35:16 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221213 22:35:16 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221213 22:35:16 @agent_ppo2.py:121][0m #------------------------ Iteration 808 --------------------------#
[32m[20221213 22:35:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:16 @agent_ppo2.py:185][0m |          -0.0032 |          35.1758 |          10.5531 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |           0.0060 |          35.9852 |          10.5501 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0067 |          33.1371 |          10.5323 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0087 |          32.6679 |          10.5336 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0113 |          32.4149 |          10.5325 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0101 |          32.2452 |          10.5283 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0122 |          32.0421 |          10.5314 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0086 |          32.1085 |          10.5231 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0124 |          31.6760 |          10.5257 |
[32m[20221213 22:35:17 @agent_ppo2.py:185][0m |          -0.0049 |          32.2532 |          10.5238 |
[32m[20221213 22:35:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.92
[32m[20221213 22:35:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.46
[32m[20221213 22:35:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.56
[32m[20221213 22:35:17 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221213 22:35:17 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221213 22:35:17 @agent_ppo2.py:121][0m #------------------------ Iteration 809 --------------------------#
[32m[20221213 22:35:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |           0.0021 |          27.0152 |          10.6255 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0035 |          26.1003 |          10.6102 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |           0.0006 |          25.8517 |          10.6031 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0036 |          25.7022 |          10.6044 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0040 |          25.6300 |          10.6021 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0050 |          25.4941 |          10.5975 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0081 |          25.4140 |          10.5981 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0014 |          25.8270 |          10.5998 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0077 |          25.2949 |          10.5937 |
[32m[20221213 22:35:18 @agent_ppo2.py:185][0m |          -0.0083 |          25.2179 |          10.5969 |
[32m[20221213 22:35:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:35:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.18
[32m[20221213 22:35:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.01
[32m[20221213 22:35:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.60
[32m[20221213 22:35:19 @agent_ppo2.py:143][0m Total time:      17.09 min
[32m[20221213 22:35:19 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221213 22:35:19 @agent_ppo2.py:121][0m #------------------------ Iteration 810 --------------------------#
[32m[20221213 22:35:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:35:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |          -0.0024 |          25.3958 |          10.6695 |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |          -0.0105 |          23.5400 |          10.6534 |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |          -0.0033 |          23.2337 |          10.6534 |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |           0.0008 |          23.4874 |          10.6481 |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |          -0.0121 |          22.6463 |          10.6518 |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |          -0.0159 |          22.1360 |          10.6558 |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |          -0.0116 |          22.0966 |          10.6648 |
[32m[20221213 22:35:19 @agent_ppo2.py:185][0m |          -0.0123 |          21.8522 |          10.6582 |
[32m[20221213 22:35:20 @agent_ppo2.py:185][0m |          -0.0152 |          21.6675 |          10.6612 |
[32m[20221213 22:35:20 @agent_ppo2.py:185][0m |          -0.0146 |          21.5577 |          10.6608 |
[32m[20221213 22:35:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:35:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.24
[32m[20221213 22:35:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.62
[32m[20221213 22:35:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.18
[32m[20221213 22:35:20 @agent_ppo2.py:143][0m Total time:      17.11 min
[32m[20221213 22:35:20 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221213 22:35:20 @agent_ppo2.py:121][0m #------------------------ Iteration 811 --------------------------#
[32m[20221213 22:35:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:20 @agent_ppo2.py:185][0m |           0.0005 |          17.3937 |          10.7417 |
[32m[20221213 22:35:20 @agent_ppo2.py:185][0m |          -0.0062 |          14.3283 |          10.7212 |
[32m[20221213 22:35:20 @agent_ppo2.py:185][0m |          -0.0074 |          13.6262 |          10.7154 |
[32m[20221213 22:35:20 @agent_ppo2.py:185][0m |           0.0020 |          14.8849 |          10.7038 |
[32m[20221213 22:35:20 @agent_ppo2.py:185][0m |          -0.0078 |          12.9692 |          10.7081 |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0066 |          12.8778 |          10.7091 |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0005 |          14.6142 |          10.7101 |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0099 |          12.5739 |          10.7033 |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0148 |          12.3828 |          10.7073 |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0120 |          12.2467 |          10.7061 |
[32m[20221213 22:35:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.27
[32m[20221213 22:35:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.39
[32m[20221213 22:35:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.79
[32m[20221213 22:35:21 @agent_ppo2.py:143][0m Total time:      17.13 min
[32m[20221213 22:35:21 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221213 22:35:21 @agent_ppo2.py:121][0m #------------------------ Iteration 812 --------------------------#
[32m[20221213 22:35:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0028 |          23.8206 |          10.7207 |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0018 |          22.5503 |          10.7109 |
[32m[20221213 22:35:21 @agent_ppo2.py:185][0m |          -0.0031 |          22.1694 |          10.7065 |
[32m[20221213 22:35:22 @agent_ppo2.py:185][0m |          -0.0078 |          21.6889 |          10.7018 |
[32m[20221213 22:35:22 @agent_ppo2.py:185][0m |          -0.0136 |          21.4611 |          10.7022 |
[32m[20221213 22:35:22 @agent_ppo2.py:185][0m |          -0.0146 |          21.1374 |          10.7090 |
[32m[20221213 22:35:22 @agent_ppo2.py:185][0m |          -0.0028 |          21.8014 |          10.7022 |
[32m[20221213 22:35:22 @agent_ppo2.py:185][0m |          -0.0095 |          21.2625 |          10.6958 |
[32m[20221213 22:35:22 @agent_ppo2.py:185][0m |          -0.0060 |          21.0215 |          10.7038 |
[32m[20221213 22:35:22 @agent_ppo2.py:185][0m |          -0.0124 |          20.5481 |          10.6883 |
[32m[20221213 22:35:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.65
[32m[20221213 22:35:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.11
[32m[20221213 22:35:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.80
[32m[20221213 22:35:22 @agent_ppo2.py:143][0m Total time:      17.16 min
[32m[20221213 22:35:22 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221213 22:35:22 @agent_ppo2.py:121][0m #------------------------ Iteration 813 --------------------------#
[32m[20221213 22:35:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0033 |          26.8137 |          10.5025 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0031 |          24.6231 |          10.4916 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0005 |          24.3684 |          10.4913 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0033 |          23.5464 |          10.4874 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0106 |          22.7936 |          10.4852 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0138 |          22.4709 |          10.4795 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0059 |          22.2714 |          10.4818 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0099 |          22.0867 |          10.4785 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0117 |          21.9207 |          10.4778 |
[32m[20221213 22:35:23 @agent_ppo2.py:185][0m |          -0.0088 |          21.7365 |          10.4799 |
[32m[20221213 22:35:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:35:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.53
[32m[20221213 22:35:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.00
[32m[20221213 22:35:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 214.40
[32m[20221213 22:35:23 @agent_ppo2.py:143][0m Total time:      17.18 min
[32m[20221213 22:35:23 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221213 22:35:23 @agent_ppo2.py:121][0m #------------------------ Iteration 814 --------------------------#
[32m[20221213 22:35:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0016 |          31.0966 |          10.7051 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0025 |          30.0215 |          10.6914 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |           0.0002 |          29.9747 |          10.6856 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0121 |          29.3623 |          10.6787 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0109 |          29.0159 |          10.6794 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0127 |          28.7864 |          10.6751 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0074 |          28.7086 |          10.6785 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0139 |          28.5153 |          10.6755 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0142 |          28.3061 |          10.6694 |
[32m[20221213 22:35:24 @agent_ppo2.py:185][0m |          -0.0122 |          28.2095 |          10.6711 |
[32m[20221213 22:35:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.95
[32m[20221213 22:35:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.51
[32m[20221213 22:35:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.03
[32m[20221213 22:35:25 @agent_ppo2.py:143][0m Total time:      17.20 min
[32m[20221213 22:35:25 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221213 22:35:25 @agent_ppo2.py:121][0m #------------------------ Iteration 815 --------------------------#
[32m[20221213 22:35:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:25 @agent_ppo2.py:185][0m |          -0.0001 |          31.5031 |          10.7011 |
[32m[20221213 22:35:25 @agent_ppo2.py:185][0m |          -0.0041 |          28.9704 |          10.6866 |
[32m[20221213 22:35:25 @agent_ppo2.py:185][0m |          -0.0021 |          28.3899 |          10.6895 |
[32m[20221213 22:35:25 @agent_ppo2.py:185][0m |          -0.0028 |          28.0569 |          10.6814 |
[32m[20221213 22:35:25 @agent_ppo2.py:185][0m |          -0.0057 |          27.9223 |          10.6694 |
[32m[20221213 22:35:25 @agent_ppo2.py:185][0m |          -0.0024 |          27.7639 |          10.6753 |
[32m[20221213 22:35:25 @agent_ppo2.py:185][0m |          -0.0002 |          27.9815 |          10.6696 |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |          -0.0037 |          27.8363 |          10.6692 |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |          -0.0047 |          27.4464 |          10.6619 |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |          -0.0081 |          27.5177 |          10.6657 |
[32m[20221213 22:35:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.88
[32m[20221213 22:35:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.65
[32m[20221213 22:35:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.36
[32m[20221213 22:35:26 @agent_ppo2.py:143][0m Total time:      17.22 min
[32m[20221213 22:35:26 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221213 22:35:26 @agent_ppo2.py:121][0m #------------------------ Iteration 816 --------------------------#
[32m[20221213 22:35:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |           0.0024 |          31.3718 |          10.7750 |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |          -0.0028 |          27.6980 |          10.7696 |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |          -0.0056 |          26.5177 |          10.7666 |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |          -0.0085 |          25.6616 |          10.7621 |
[32m[20221213 22:35:26 @agent_ppo2.py:185][0m |          -0.0089 |          25.4102 |          10.7599 |
[32m[20221213 22:35:27 @agent_ppo2.py:185][0m |          -0.0053 |          24.9096 |          10.7610 |
[32m[20221213 22:35:27 @agent_ppo2.py:185][0m |          -0.0080 |          24.6855 |          10.7642 |
[32m[20221213 22:35:27 @agent_ppo2.py:185][0m |          -0.0122 |          24.3562 |          10.7614 |
[32m[20221213 22:35:27 @agent_ppo2.py:185][0m |          -0.0066 |          24.5615 |          10.7584 |
[32m[20221213 22:35:27 @agent_ppo2.py:185][0m |          -0.0135 |          23.9039 |          10.7617 |
[32m[20221213 22:35:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.11
[32m[20221213 22:35:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.27
[32m[20221213 22:35:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.03
[32m[20221213 22:35:27 @agent_ppo2.py:143][0m Total time:      17.24 min
[32m[20221213 22:35:27 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221213 22:35:27 @agent_ppo2.py:121][0m #------------------------ Iteration 817 --------------------------#
[32m[20221213 22:35:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:27 @agent_ppo2.py:185][0m |          -0.0020 |          27.8531 |          10.7342 |
[32m[20221213 22:35:27 @agent_ppo2.py:185][0m |           0.0003 |          26.3561 |          10.7337 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |          -0.0087 |          24.8451 |          10.7305 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |          -0.0091 |          24.2071 |          10.7267 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |          -0.0065 |          23.7711 |          10.7328 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |          -0.0092 |          23.4133 |          10.7296 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |           0.0008 |          23.7248 |          10.7287 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |          -0.0017 |          23.6522 |          10.7340 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |          -0.0109 |          22.9674 |          10.7359 |
[32m[20221213 22:35:28 @agent_ppo2.py:185][0m |          -0.0137 |          22.5118 |          10.7293 |
[32m[20221213 22:35:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:35:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.30
[32m[20221213 22:35:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.70
[32m[20221213 22:35:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.35
[32m[20221213 22:35:28 @agent_ppo2.py:143][0m Total time:      17.26 min
[32m[20221213 22:35:28 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221213 22:35:28 @agent_ppo2.py:121][0m #------------------------ Iteration 818 --------------------------#
[32m[20221213 22:35:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0026 |          25.6240 |          10.5697 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0061 |          23.9505 |          10.5558 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0082 |          23.4593 |          10.5667 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0042 |          23.1614 |          10.5606 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0113 |          22.8944 |          10.5627 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0087 |          22.7316 |          10.5739 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0106 |          22.4964 |          10.5731 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0065 |          22.6342 |          10.5697 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0063 |          22.3813 |          10.5722 |
[32m[20221213 22:35:29 @agent_ppo2.py:185][0m |          -0.0104 |          22.1881 |          10.5744 |
[32m[20221213 22:35:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.63
[32m[20221213 22:35:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.74
[32m[20221213 22:35:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.52
[32m[20221213 22:35:29 @agent_ppo2.py:143][0m Total time:      17.28 min
[32m[20221213 22:35:29 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221213 22:35:29 @agent_ppo2.py:121][0m #------------------------ Iteration 819 --------------------------#
[32m[20221213 22:35:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |           0.0001 |          36.9356 |          10.8349 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0040 |          34.3116 |          10.8095 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0006 |          33.7278 |          10.8042 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0049 |          33.1664 |          10.8025 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0071 |          32.6917 |          10.7955 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0131 |          32.6458 |          10.7929 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0032 |          33.6592 |          10.8002 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0093 |          31.9975 |          10.7849 |
[32m[20221213 22:35:30 @agent_ppo2.py:185][0m |          -0.0110 |          31.7672 |          10.7827 |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |          -0.0129 |          31.7384 |          10.7980 |
[32m[20221213 22:35:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:35:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.82
[32m[20221213 22:35:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.97
[32m[20221213 22:35:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 227.30
[32m[20221213 22:35:31 @agent_ppo2.py:143][0m Total time:      17.30 min
[32m[20221213 22:35:31 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221213 22:35:31 @agent_ppo2.py:121][0m #------------------------ Iteration 820 --------------------------#
[32m[20221213 22:35:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:35:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |          -0.0039 |          26.9332 |          10.9093 |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |           0.0010 |          24.8537 |          10.8889 |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |          -0.0069 |          23.5722 |          10.8772 |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |          -0.0036 |          23.0845 |          10.8878 |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |          -0.0101 |          22.6983 |          10.8816 |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |          -0.0088 |          22.3520 |          10.8797 |
[32m[20221213 22:35:31 @agent_ppo2.py:185][0m |          -0.0076 |          22.1077 |          10.8716 |
[32m[20221213 22:35:32 @agent_ppo2.py:185][0m |          -0.0072 |          21.9066 |          10.8706 |
[32m[20221213 22:35:32 @agent_ppo2.py:185][0m |          -0.0118 |          21.7702 |          10.8724 |
[32m[20221213 22:35:32 @agent_ppo2.py:185][0m |          -0.0065 |          22.4939 |          10.8631 |
[32m[20221213 22:35:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.99
[32m[20221213 22:35:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.05
[32m[20221213 22:35:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.06
[32m[20221213 22:35:32 @agent_ppo2.py:143][0m Total time:      17.32 min
[32m[20221213 22:35:32 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221213 22:35:32 @agent_ppo2.py:121][0m #------------------------ Iteration 821 --------------------------#
[32m[20221213 22:35:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:32 @agent_ppo2.py:185][0m |          -0.0006 |          22.1888 |          10.7724 |
[32m[20221213 22:35:32 @agent_ppo2.py:185][0m |          -0.0018 |          20.1936 |          10.7718 |
[32m[20221213 22:35:32 @agent_ppo2.py:185][0m |          -0.0054 |          19.3160 |          10.7692 |
[32m[20221213 22:35:32 @agent_ppo2.py:185][0m |          -0.0011 |          18.9821 |          10.7787 |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |          -0.0102 |          18.4300 |          10.7725 |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |          -0.0057 |          18.2896 |          10.7837 |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |          -0.0040 |          18.1038 |          10.7749 |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |          -0.0018 |          18.1604 |          10.7816 |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |          -0.0138 |          17.5221 |          10.7814 |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |          -0.0050 |          17.4063 |          10.7765 |
[32m[20221213 22:35:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:35:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.02
[32m[20221213 22:35:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.53
[32m[20221213 22:35:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.68
[32m[20221213 22:35:33 @agent_ppo2.py:143][0m Total time:      17.34 min
[32m[20221213 22:35:33 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221213 22:35:33 @agent_ppo2.py:121][0m #------------------------ Iteration 822 --------------------------#
[32m[20221213 22:35:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |           0.0066 |          37.1352 |          10.8716 |
[32m[20221213 22:35:33 @agent_ppo2.py:185][0m |          -0.0024 |          33.9487 |          10.8673 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |          -0.0044 |          32.9853 |          10.8660 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |           0.0144 |          36.9511 |          10.8532 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |          -0.0071 |          32.3057 |          10.8600 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |          -0.0047 |          31.6497 |          10.8649 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |           0.0038 |          35.9105 |          10.8583 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |          -0.0120 |          31.4942 |          10.8587 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |          -0.0089 |          31.3320 |          10.8609 |
[32m[20221213 22:35:34 @agent_ppo2.py:185][0m |          -0.0071 |          30.7411 |          10.8601 |
[32m[20221213 22:35:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:35:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.64
[32m[20221213 22:35:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.85
[32m[20221213 22:35:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.45
[32m[20221213 22:35:34 @agent_ppo2.py:143][0m Total time:      17.36 min
[32m[20221213 22:35:34 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221213 22:35:34 @agent_ppo2.py:121][0m #------------------------ Iteration 823 --------------------------#
[32m[20221213 22:35:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |           0.0018 |          21.7481 |          10.9789 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |           0.0019 |          18.4053 |          10.9666 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |           0.0045 |          18.4893 |          10.9650 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |          -0.0036 |          17.1153 |          10.9584 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |          -0.0040 |          16.7545 |          10.9632 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |           0.0001 |          16.5082 |          10.9569 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |          -0.0027 |          16.1829 |          10.9608 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |          -0.0105 |          16.0079 |          10.9551 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |          -0.0064 |          15.8321 |          10.9508 |
[32m[20221213 22:35:35 @agent_ppo2.py:185][0m |          -0.0091 |          15.6602 |          10.9459 |
[32m[20221213 22:35:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.11
[32m[20221213 22:35:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.51
[32m[20221213 22:35:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 219.56
[32m[20221213 22:35:36 @agent_ppo2.py:143][0m Total time:      17.38 min
[32m[20221213 22:35:36 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221213 22:35:36 @agent_ppo2.py:121][0m #------------------------ Iteration 824 --------------------------#
[32m[20221213 22:35:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |           0.0108 |          27.8133 |          10.8933 |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |          -0.0055 |          23.6199 |          10.8785 |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |          -0.0073 |          22.5668 |          10.8665 |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |          -0.0049 |          22.0115 |          10.8613 |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |          -0.0079 |          21.6637 |          10.8675 |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |           0.0003 |          23.9640 |          10.8485 |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |          -0.0067 |          21.1553 |          10.8147 |
[32m[20221213 22:35:36 @agent_ppo2.py:185][0m |          -0.0072 |          20.9853 |          10.8493 |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0092 |          20.7038 |          10.8520 |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0094 |          20.5618 |          10.8449 |
[32m[20221213 22:35:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.15
[32m[20221213 22:35:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.15
[32m[20221213 22:35:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.36
[32m[20221213 22:35:37 @agent_ppo2.py:143][0m Total time:      17.40 min
[32m[20221213 22:35:37 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221213 22:35:37 @agent_ppo2.py:121][0m #------------------------ Iteration 825 --------------------------#
[32m[20221213 22:35:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0012 |          28.4450 |          10.8761 |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0029 |          27.4775 |          10.8685 |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0025 |          27.7497 |          10.8676 |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0068 |          27.0604 |          10.8590 |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0078 |          27.0850 |          10.8558 |
[32m[20221213 22:35:37 @agent_ppo2.py:185][0m |          -0.0076 |          27.0268 |          10.8481 |
[32m[20221213 22:35:38 @agent_ppo2.py:185][0m |          -0.0078 |          26.7911 |          10.8589 |
[32m[20221213 22:35:38 @agent_ppo2.py:185][0m |          -0.0060 |          26.7020 |          10.8520 |
[32m[20221213 22:35:38 @agent_ppo2.py:185][0m |          -0.0067 |          26.6386 |          10.8584 |
[32m[20221213 22:35:38 @agent_ppo2.py:185][0m |          -0.0062 |          26.5855 |          10.8537 |
[32m[20221213 22:35:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:35:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.74
[32m[20221213 22:35:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.96
[32m[20221213 22:35:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.64
[32m[20221213 22:35:38 @agent_ppo2.py:143][0m Total time:      17.42 min
[32m[20221213 22:35:38 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221213 22:35:38 @agent_ppo2.py:121][0m #------------------------ Iteration 826 --------------------------#
[32m[20221213 22:35:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:38 @agent_ppo2.py:185][0m |           0.0010 |          30.1035 |          10.9091 |
[32m[20221213 22:35:38 @agent_ppo2.py:185][0m |          -0.0068 |          27.9495 |          10.9042 |
[32m[20221213 22:35:38 @agent_ppo2.py:185][0m |          -0.0070 |          27.3921 |          10.8947 |
[32m[20221213 22:35:39 @agent_ppo2.py:185][0m |          -0.0025 |          27.1284 |          10.8914 |
[32m[20221213 22:35:39 @agent_ppo2.py:185][0m |          -0.0100 |          26.9420 |          10.8960 |
[32m[20221213 22:35:39 @agent_ppo2.py:185][0m |          -0.0060 |          26.7582 |          10.8903 |
[32m[20221213 22:35:39 @agent_ppo2.py:185][0m |          -0.0047 |          26.9576 |          10.8862 |
[32m[20221213 22:35:39 @agent_ppo2.py:185][0m |          -0.0115 |          26.5279 |          10.8866 |
[32m[20221213 22:35:39 @agent_ppo2.py:185][0m |          -0.0091 |          26.4673 |          10.8742 |
[32m[20221213 22:35:39 @agent_ppo2.py:185][0m |          -0.0008 |          28.1966 |          10.8869 |
[32m[20221213 22:35:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.39
[32m[20221213 22:35:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.09
[32m[20221213 22:35:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 174.53
[32m[20221213 22:35:39 @agent_ppo2.py:143][0m Total time:      17.44 min
[32m[20221213 22:35:39 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221213 22:35:39 @agent_ppo2.py:121][0m #------------------------ Iteration 827 --------------------------#
[32m[20221213 22:35:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0025 |          31.2736 |          10.6853 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0037 |          29.6357 |          10.6640 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0039 |          29.0194 |          10.6635 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0054 |          29.3891 |          10.6558 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0101 |          28.5789 |          10.6517 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0066 |          28.2832 |          10.6502 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0014 |          29.9114 |          10.6479 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0051 |          28.0963 |          10.6378 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0117 |          27.6566 |          10.6448 |
[32m[20221213 22:35:40 @agent_ppo2.py:185][0m |          -0.0134 |          27.6409 |          10.6493 |
[32m[20221213 22:35:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:35:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.96
[32m[20221213 22:35:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.58
[32m[20221213 22:35:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.10
[32m[20221213 22:35:40 @agent_ppo2.py:143][0m Total time:      17.46 min
[32m[20221213 22:35:40 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221213 22:35:40 @agent_ppo2.py:121][0m #------------------------ Iteration 828 --------------------------#
[32m[20221213 22:35:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |           0.0006 |          29.3073 |          10.6548 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0055 |          26.9454 |          10.6415 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0063 |          26.2727 |          10.6405 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0014 |          26.7782 |          10.6517 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0059 |          25.8502 |          10.6525 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0083 |          25.5055 |          10.6557 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0079 |          25.3332 |          10.6569 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0071 |          25.3629 |          10.6585 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0105 |          25.0009 |          10.6585 |
[32m[20221213 22:35:41 @agent_ppo2.py:185][0m |          -0.0070 |          25.4984 |          10.6642 |
[32m[20221213 22:35:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.75
[32m[20221213 22:35:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.65
[32m[20221213 22:35:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.76
[32m[20221213 22:35:42 @agent_ppo2.py:143][0m Total time:      17.48 min
[32m[20221213 22:35:42 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221213 22:35:42 @agent_ppo2.py:121][0m #------------------------ Iteration 829 --------------------------#
[32m[20221213 22:35:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:42 @agent_ppo2.py:185][0m |           0.0172 |          31.0252 |          10.9774 |
[32m[20221213 22:35:42 @agent_ppo2.py:185][0m |          -0.0067 |          24.2124 |          10.9749 |
[32m[20221213 22:35:42 @agent_ppo2.py:185][0m |          -0.0061 |          22.4804 |          10.9741 |
[32m[20221213 22:35:42 @agent_ppo2.py:185][0m |          -0.0045 |          21.8061 |          10.9753 |
[32m[20221213 22:35:42 @agent_ppo2.py:185][0m |          -0.0020 |          22.2636 |          10.9597 |
[32m[20221213 22:35:42 @agent_ppo2.py:185][0m |          -0.0064 |          21.1813 |          10.9774 |
[32m[20221213 22:35:42 @agent_ppo2.py:185][0m |          -0.0077 |          20.8804 |          10.9713 |
[32m[20221213 22:35:43 @agent_ppo2.py:185][0m |          -0.0070 |          20.9488 |          10.9669 |
[32m[20221213 22:35:43 @agent_ppo2.py:185][0m |          -0.0080 |          20.5523 |          10.9668 |
[32m[20221213 22:35:43 @agent_ppo2.py:185][0m |          -0.0092 |          20.4120 |          10.9719 |
[32m[20221213 22:35:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.21
[32m[20221213 22:35:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.57
[32m[20221213 22:35:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.83
[32m[20221213 22:35:43 @agent_ppo2.py:143][0m Total time:      17.50 min
[32m[20221213 22:35:43 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221213 22:35:43 @agent_ppo2.py:121][0m #------------------------ Iteration 830 --------------------------#
[32m[20221213 22:35:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:35:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:43 @agent_ppo2.py:185][0m |           0.0120 |          33.7888 |          10.9583 |
[32m[20221213 22:35:43 @agent_ppo2.py:185][0m |          -0.0055 |          27.8860 |          10.9423 |
[32m[20221213 22:35:43 @agent_ppo2.py:185][0m |          -0.0078 |          26.1182 |          10.9378 |
[32m[20221213 22:35:43 @agent_ppo2.py:185][0m |          -0.0067 |          25.6056 |          10.9405 |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |          -0.0089 |          25.4349 |          10.9397 |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |          -0.0096 |          25.2089 |          10.9268 |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |          -0.0104 |          25.1409 |          10.9328 |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |          -0.0090 |          25.0697 |          10.9363 |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |          -0.0104 |          24.9076 |          10.9262 |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |          -0.0094 |          24.8997 |          10.9274 |
[32m[20221213 22:35:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.50
[32m[20221213 22:35:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.15
[32m[20221213 22:35:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.74
[32m[20221213 22:35:44 @agent_ppo2.py:143][0m Total time:      17.52 min
[32m[20221213 22:35:44 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221213 22:35:44 @agent_ppo2.py:121][0m #------------------------ Iteration 831 --------------------------#
[32m[20221213 22:35:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |          -0.0005 |          25.6206 |          11.0071 |
[32m[20221213 22:35:44 @agent_ppo2.py:185][0m |           0.0035 |          18.6592 |          10.9970 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0030 |          17.4498 |          10.9873 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0054 |          16.9101 |          10.9766 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0026 |          16.4825 |          10.9815 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0050 |          16.4472 |          10.9803 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0073 |          16.1034 |          10.9767 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0040 |          15.9346 |          10.9793 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0115 |          15.9156 |          10.9834 |
[32m[20221213 22:35:45 @agent_ppo2.py:185][0m |          -0.0100 |          15.7271 |          10.9753 |
[32m[20221213 22:35:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.77
[32m[20221213 22:35:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.46
[32m[20221213 22:35:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.96
[32m[20221213 22:35:45 @agent_ppo2.py:143][0m Total time:      17.54 min
[32m[20221213 22:35:45 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221213 22:35:45 @agent_ppo2.py:121][0m #------------------------ Iteration 832 --------------------------#
[32m[20221213 22:35:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |           0.0007 |          26.9349 |          10.8450 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0122 |          24.5865 |          10.8490 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0000 |          24.4480 |          10.8496 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0090 |          23.0167 |          10.8353 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0065 |          22.6625 |          10.8440 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0053 |          22.5081 |          10.8400 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0073 |          22.2638 |          10.8451 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0081 |          22.0777 |          10.8353 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0073 |          22.0685 |          10.8326 |
[32m[20221213 22:35:46 @agent_ppo2.py:185][0m |          -0.0070 |          21.8633 |          10.8346 |
[32m[20221213 22:35:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.10
[32m[20221213 22:35:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.87
[32m[20221213 22:35:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.80
[32m[20221213 22:35:46 @agent_ppo2.py:143][0m Total time:      17.56 min
[32m[20221213 22:35:46 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221213 22:35:46 @agent_ppo2.py:121][0m #------------------------ Iteration 833 --------------------------#
[32m[20221213 22:35:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |           0.0128 |          30.1734 |          10.8178 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0027 |          24.0426 |          10.7907 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0052 |          23.3082 |          10.7842 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0060 |          22.6976 |          10.7829 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0140 |          22.2423 |          10.7840 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0119 |          22.1232 |          10.7777 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0163 |          21.7047 |          10.7831 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0126 |          21.5050 |          10.7705 |
[32m[20221213 22:35:47 @agent_ppo2.py:185][0m |          -0.0110 |          21.3689 |          10.7718 |
[32m[20221213 22:35:48 @agent_ppo2.py:185][0m |          -0.0088 |          21.1959 |          10.7734 |
[32m[20221213 22:35:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:35:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.02
[32m[20221213 22:35:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.78
[32m[20221213 22:35:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 207.89
[32m[20221213 22:35:48 @agent_ppo2.py:143][0m Total time:      17.58 min
[32m[20221213 22:35:48 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221213 22:35:48 @agent_ppo2.py:121][0m #------------------------ Iteration 834 --------------------------#
[32m[20221213 22:35:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:48 @agent_ppo2.py:185][0m |           0.0001 |          28.3715 |          10.7948 |
[32m[20221213 22:35:48 @agent_ppo2.py:185][0m |           0.0008 |          26.1160 |          10.7952 |
[32m[20221213 22:35:48 @agent_ppo2.py:185][0m |          -0.0060 |          25.2309 |          10.7833 |
[32m[20221213 22:35:48 @agent_ppo2.py:185][0m |          -0.0066 |          24.5764 |          10.7826 |
[32m[20221213 22:35:48 @agent_ppo2.py:185][0m |          -0.0086 |          24.2643 |          10.7840 |
[32m[20221213 22:35:48 @agent_ppo2.py:185][0m |          -0.0098 |          23.9639 |          10.7749 |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0098 |          23.8140 |          10.7797 |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0113 |          23.3978 |          10.7775 |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0091 |          23.5322 |          10.7774 |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0037 |          23.4609 |          10.7745 |
[32m[20221213 22:35:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.19
[32m[20221213 22:35:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.85
[32m[20221213 22:35:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.75
[32m[20221213 22:35:49 @agent_ppo2.py:143][0m Total time:      17.60 min
[32m[20221213 22:35:49 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221213 22:35:49 @agent_ppo2.py:121][0m #------------------------ Iteration 835 --------------------------#
[32m[20221213 22:35:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0007 |          26.5647 |          10.9393 |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0105 |          24.9748 |          10.9283 |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0058 |          24.2429 |          10.9134 |
[32m[20221213 22:35:49 @agent_ppo2.py:185][0m |          -0.0156 |          23.8758 |          10.9126 |
[32m[20221213 22:35:50 @agent_ppo2.py:185][0m |          -0.0044 |          23.7163 |          10.9179 |
[32m[20221213 22:35:50 @agent_ppo2.py:185][0m |          -0.0130 |          23.3160 |          10.9198 |
[32m[20221213 22:35:50 @agent_ppo2.py:185][0m |          -0.0139 |          23.0989 |          10.9094 |
[32m[20221213 22:35:50 @agent_ppo2.py:185][0m |          -0.0144 |          22.9412 |          10.9089 |
[32m[20221213 22:35:50 @agent_ppo2.py:185][0m |          -0.0073 |          24.0961 |          10.9055 |
[32m[20221213 22:35:50 @agent_ppo2.py:185][0m |          -0.0131 |          22.7264 |          10.9058 |
[32m[20221213 22:35:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:35:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.59
[32m[20221213 22:35:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.72
[32m[20221213 22:35:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.56
[32m[20221213 22:35:50 @agent_ppo2.py:143][0m Total time:      17.62 min
[32m[20221213 22:35:50 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221213 22:35:50 @agent_ppo2.py:121][0m #------------------------ Iteration 836 --------------------------#
[32m[20221213 22:35:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:35:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:50 @agent_ppo2.py:185][0m |           0.0036 |          32.0623 |          10.7769 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0077 |          28.5373 |          10.7736 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0022 |          27.6586 |          10.7771 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0070 |          27.1196 |          10.7767 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0094 |          26.7213 |          10.7730 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0124 |          26.4702 |          10.7763 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0059 |          26.1113 |          10.7851 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0112 |          25.8261 |          10.7767 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0123 |          25.6995 |          10.7845 |
[32m[20221213 22:35:51 @agent_ppo2.py:185][0m |          -0.0081 |          25.9509 |          10.7831 |
[32m[20221213 22:35:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.75
[32m[20221213 22:35:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.32
[32m[20221213 22:35:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.97
[32m[20221213 22:35:51 @agent_ppo2.py:143][0m Total time:      17.64 min
[32m[20221213 22:35:51 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221213 22:35:51 @agent_ppo2.py:121][0m #------------------------ Iteration 837 --------------------------#
[32m[20221213 22:35:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0021 |          32.2161 |          10.9839 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0068 |          28.1426 |          10.9902 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0105 |          27.4095 |          10.9823 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0080 |          27.0251 |          10.9760 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |           0.0005 |          28.9504 |          10.9727 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0094 |          26.6213 |          10.9672 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0105 |          26.2847 |          10.9750 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0149 |          26.1591 |          10.9717 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0127 |          25.9730 |          10.9647 |
[32m[20221213 22:35:52 @agent_ppo2.py:185][0m |          -0.0061 |          27.7562 |          10.9756 |
[32m[20221213 22:35:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.50
[32m[20221213 22:35:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.81
[32m[20221213 22:35:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.18
[32m[20221213 22:35:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 418.18
[32m[20221213 22:35:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 418.18
[32m[20221213 22:35:53 @agent_ppo2.py:143][0m Total time:      17.66 min
[32m[20221213 22:35:53 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221213 22:35:53 @agent_ppo2.py:121][0m #------------------------ Iteration 838 --------------------------#
[32m[20221213 22:35:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:53 @agent_ppo2.py:185][0m |           0.0063 |          22.5286 |          11.1565 |
[32m[20221213 22:35:53 @agent_ppo2.py:185][0m |          -0.0063 |          20.2006 |          11.1352 |
[32m[20221213 22:35:53 @agent_ppo2.py:185][0m |          -0.0073 |          19.4426 |          11.1306 |
[32m[20221213 22:35:53 @agent_ppo2.py:185][0m |          -0.0077 |          19.0479 |          11.1336 |
[32m[20221213 22:35:53 @agent_ppo2.py:185][0m |          -0.0115 |          18.8323 |          11.1286 |
[32m[20221213 22:35:53 @agent_ppo2.py:185][0m |          -0.0085 |          18.5001 |          11.1330 |
[32m[20221213 22:35:53 @agent_ppo2.py:185][0m |           0.0088 |          19.5202 |          11.1303 |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |          -0.0119 |          18.1894 |          11.1291 |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |          -0.0082 |          17.9636 |          11.1255 |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |          -0.0143 |          17.8874 |          11.1233 |
[32m[20221213 22:35:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:35:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.96
[32m[20221213 22:35:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 358.42
[32m[20221213 22:35:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.96
[32m[20221213 22:35:54 @agent_ppo2.py:143][0m Total time:      17.68 min
[32m[20221213 22:35:54 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221213 22:35:54 @agent_ppo2.py:121][0m #------------------------ Iteration 839 --------------------------#
[32m[20221213 22:35:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |           0.0013 |          25.8666 |          10.8603 |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |           0.0009 |          23.4017 |          10.8506 |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |          -0.0074 |          21.4991 |          10.8403 |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |          -0.0087 |          20.9401 |          10.8436 |
[32m[20221213 22:35:54 @agent_ppo2.py:185][0m |          -0.0095 |          20.5096 |          10.8332 |
[32m[20221213 22:35:55 @agent_ppo2.py:185][0m |          -0.0095 |          20.2376 |          10.8384 |
[32m[20221213 22:35:55 @agent_ppo2.py:185][0m |           0.0038 |          20.6091 |          10.8359 |
[32m[20221213 22:35:55 @agent_ppo2.py:185][0m |          -0.0158 |          19.8715 |          10.8340 |
[32m[20221213 22:35:55 @agent_ppo2.py:185][0m |          -0.0095 |          19.7415 |          10.8336 |
[32m[20221213 22:35:55 @agent_ppo2.py:185][0m |          -0.0137 |          19.3519 |          10.8328 |
[32m[20221213 22:35:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:35:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.37
[32m[20221213 22:35:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.00
[32m[20221213 22:35:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 1.62
[32m[20221213 22:35:55 @agent_ppo2.py:143][0m Total time:      17.70 min
[32m[20221213 22:35:55 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221213 22:35:55 @agent_ppo2.py:121][0m #------------------------ Iteration 840 --------------------------#
[32m[20221213 22:35:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:35:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:55 @agent_ppo2.py:185][0m |          -0.0014 |          25.6777 |          10.9683 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |          -0.0034 |          24.5776 |          10.9492 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |          -0.0040 |          23.9324 |          10.9575 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |          -0.0061 |          23.3909 |          10.9561 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |          -0.0106 |          23.0623 |          10.9526 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |           0.0028 |          23.2366 |          10.9538 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |          -0.0081 |          22.4619 |          10.9454 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |          -0.0111 |          22.1640 |          10.9527 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |          -0.0061 |          21.9329 |          10.9528 |
[32m[20221213 22:35:56 @agent_ppo2.py:185][0m |           0.0035 |          23.7457 |          10.9487 |
[32m[20221213 22:35:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:35:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.90
[32m[20221213 22:35:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.46
[32m[20221213 22:35:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.39
[32m[20221213 22:35:56 @agent_ppo2.py:143][0m Total time:      17.72 min
[32m[20221213 22:35:56 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221213 22:35:56 @agent_ppo2.py:121][0m #------------------------ Iteration 841 --------------------------#
[32m[20221213 22:35:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0007 |          20.5859 |          10.9670 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |           0.0019 |          18.4190 |          10.9525 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0048 |          17.5782 |          10.9587 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0061 |          16.9564 |          10.9496 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0094 |          16.6583 |          10.9438 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0057 |          16.4719 |          10.9356 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0099 |          16.3880 |          10.9343 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0070 |          16.2400 |          10.9294 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0046 |          16.1168 |          10.9339 |
[32m[20221213 22:35:57 @agent_ppo2.py:185][0m |          -0.0130 |          15.9236 |          10.9265 |
[32m[20221213 22:35:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.03
[32m[20221213 22:35:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.51
[32m[20221213 22:35:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.44
[32m[20221213 22:35:58 @agent_ppo2.py:143][0m Total time:      17.74 min
[32m[20221213 22:35:58 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221213 22:35:58 @agent_ppo2.py:121][0m #------------------------ Iteration 842 --------------------------#
[32m[20221213 22:35:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |          -0.0003 |          15.1040 |          10.9650 |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |          -0.0029 |          13.0789 |          10.9582 |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |           0.0001 |          12.4605 |          10.9560 |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |          -0.0117 |          11.9436 |          10.9514 |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |          -0.0080 |          11.6433 |          10.9606 |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |          -0.0059 |          11.4647 |          10.9538 |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |          -0.0094 |          11.1272 |          10.9550 |
[32m[20221213 22:35:58 @agent_ppo2.py:185][0m |          -0.0155 |          11.0631 |          10.9579 |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |          -0.0032 |          10.9347 |          10.9550 |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |          -0.0069 |          10.8449 |          10.9589 |
[32m[20221213 22:35:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:35:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.71
[32m[20221213 22:35:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.32
[32m[20221213 22:35:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.99
[32m[20221213 22:35:59 @agent_ppo2.py:143][0m Total time:      17.76 min
[32m[20221213 22:35:59 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221213 22:35:59 @agent_ppo2.py:121][0m #------------------------ Iteration 843 --------------------------#
[32m[20221213 22:35:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:35:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |           0.0052 |          25.5583 |          11.1415 |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |          -0.0075 |          23.3925 |          11.1247 |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |          -0.0015 |          22.7878 |          11.1101 |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |          -0.0065 |          22.2108 |          11.1154 |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |          -0.0078 |          21.9746 |          11.1130 |
[32m[20221213 22:35:59 @agent_ppo2.py:185][0m |          -0.0106 |          21.6193 |          11.1102 |
[32m[20221213 22:36:00 @agent_ppo2.py:185][0m |          -0.0145 |          21.4278 |          11.1218 |
[32m[20221213 22:36:00 @agent_ppo2.py:185][0m |          -0.0096 |          21.1769 |          11.1165 |
[32m[20221213 22:36:00 @agent_ppo2.py:185][0m |          -0.0096 |          21.0147 |          11.1192 |
[32m[20221213 22:36:00 @agent_ppo2.py:185][0m |          -0.0080 |          21.0144 |          11.1144 |
[32m[20221213 22:36:00 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.77
[32m[20221213 22:36:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 219.25
[32m[20221213 22:36:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.86
[32m[20221213 22:36:00 @agent_ppo2.py:143][0m Total time:      17.78 min
[32m[20221213 22:36:00 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221213 22:36:00 @agent_ppo2.py:121][0m #------------------------ Iteration 844 --------------------------#
[32m[20221213 22:36:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:00 @agent_ppo2.py:185][0m |           0.0040 |          27.2599 |          10.9227 |
[32m[20221213 22:36:00 @agent_ppo2.py:185][0m |          -0.0005 |          25.5216 |          10.9050 |
[32m[20221213 22:36:00 @agent_ppo2.py:185][0m |          -0.0071 |          24.5369 |          10.9022 |
[32m[20221213 22:36:01 @agent_ppo2.py:185][0m |          -0.0107 |          24.2413 |          10.9062 |
[32m[20221213 22:36:01 @agent_ppo2.py:185][0m |          -0.0087 |          24.3218 |          10.8977 |
[32m[20221213 22:36:01 @agent_ppo2.py:185][0m |          -0.0081 |          23.9935 |          10.9017 |
[32m[20221213 22:36:01 @agent_ppo2.py:185][0m |          -0.0115 |          23.9320 |          10.8898 |
[32m[20221213 22:36:01 @agent_ppo2.py:185][0m |          -0.0115 |          23.8704 |          10.8943 |
[32m[20221213 22:36:01 @agent_ppo2.py:185][0m |          -0.0133 |          23.9086 |          10.8882 |
[32m[20221213 22:36:01 @agent_ppo2.py:185][0m |          -0.0058 |          23.8569 |          10.8908 |
[32m[20221213 22:36:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:36:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.48
[32m[20221213 22:36:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.99
[32m[20221213 22:36:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.31
[32m[20221213 22:36:01 @agent_ppo2.py:143][0m Total time:      17.80 min
[32m[20221213 22:36:01 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221213 22:36:01 @agent_ppo2.py:121][0m #------------------------ Iteration 845 --------------------------#
[32m[20221213 22:36:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |           0.0139 |          27.4402 |          10.9335 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0052 |          21.7782 |          10.8954 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0065 |          20.5404 |          10.9024 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0127 |          19.7501 |          10.8898 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0121 |          19.1288 |          10.8912 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0123 |          18.7236 |          10.8814 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0157 |          18.2990 |          10.8745 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0106 |          18.0530 |          10.8737 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0150 |          17.7596 |          10.8684 |
[32m[20221213 22:36:02 @agent_ppo2.py:185][0m |          -0.0149 |          17.5314 |          10.8671 |
[32m[20221213 22:36:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.56
[32m[20221213 22:36:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.03
[32m[20221213 22:36:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.11
[32m[20221213 22:36:02 @agent_ppo2.py:143][0m Total time:      17.83 min
[32m[20221213 22:36:02 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221213 22:36:02 @agent_ppo2.py:121][0m #------------------------ Iteration 846 --------------------------#
[32m[20221213 22:36:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |           0.0029 |          35.6299 |          10.9803 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0058 |          32.6006 |          10.9605 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0021 |          32.3195 |          10.9539 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0054 |          31.7249 |          10.9502 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0102 |          31.0405 |          10.9473 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0027 |          30.7856 |          10.9441 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0083 |          30.3536 |          10.9442 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0153 |          30.1853 |          10.9458 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0136 |          29.8983 |          10.9363 |
[32m[20221213 22:36:03 @agent_ppo2.py:185][0m |          -0.0089 |          29.9804 |          10.9474 |
[32m[20221213 22:36:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.49
[32m[20221213 22:36:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 307.94
[32m[20221213 22:36:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.53
[32m[20221213 22:36:04 @agent_ppo2.py:143][0m Total time:      17.85 min
[32m[20221213 22:36:04 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221213 22:36:04 @agent_ppo2.py:121][0m #------------------------ Iteration 847 --------------------------#
[32m[20221213 22:36:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:04 @agent_ppo2.py:185][0m |           0.0028 |          32.7986 |          10.9072 |
[32m[20221213 22:36:04 @agent_ppo2.py:185][0m |          -0.0030 |          30.3458 |          10.8914 |
[32m[20221213 22:36:04 @agent_ppo2.py:185][0m |          -0.0087 |          29.8273 |          10.8850 |
[32m[20221213 22:36:04 @agent_ppo2.py:185][0m |          -0.0031 |          29.9743 |          10.8834 |
[32m[20221213 22:36:04 @agent_ppo2.py:185][0m |          -0.0058 |          29.3303 |          10.8855 |
[32m[20221213 22:36:04 @agent_ppo2.py:185][0m |          -0.0082 |          29.1301 |          10.8847 |
[32m[20221213 22:36:05 @agent_ppo2.py:185][0m |          -0.0010 |          29.6918 |          10.8899 |
[32m[20221213 22:36:05 @agent_ppo2.py:185][0m |          -0.0099 |          28.8663 |          10.8904 |
[32m[20221213 22:36:05 @agent_ppo2.py:185][0m |          -0.0054 |          28.9094 |          10.8915 |
[32m[20221213 22:36:05 @agent_ppo2.py:185][0m |          -0.0019 |          28.8235 |          10.8809 |
[32m[20221213 22:36:05 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:36:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.52
[32m[20221213 22:36:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.22
[32m[20221213 22:36:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.18
[32m[20221213 22:36:05 @agent_ppo2.py:143][0m Total time:      17.87 min
[32m[20221213 22:36:05 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221213 22:36:05 @agent_ppo2.py:121][0m #------------------------ Iteration 848 --------------------------#
[32m[20221213 22:36:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:05 @agent_ppo2.py:185][0m |           0.0008 |          20.2905 |          10.6854 |
[32m[20221213 22:36:05 @agent_ppo2.py:185][0m |           0.0004 |          17.9791 |          10.6728 |
[32m[20221213 22:36:05 @agent_ppo2.py:185][0m |          -0.0055 |          17.1637 |          10.6651 |
[32m[20221213 22:36:06 @agent_ppo2.py:185][0m |          -0.0030 |          16.4897 |          10.6724 |
[32m[20221213 22:36:06 @agent_ppo2.py:185][0m |          -0.0117 |          16.0852 |          10.6564 |
[32m[20221213 22:36:06 @agent_ppo2.py:185][0m |          -0.0100 |          15.6834 |          10.6687 |
[32m[20221213 22:36:06 @agent_ppo2.py:185][0m |          -0.0127 |          15.5436 |          10.6569 |
[32m[20221213 22:36:06 @agent_ppo2.py:185][0m |          -0.0094 |          15.5861 |          10.6727 |
[32m[20221213 22:36:06 @agent_ppo2.py:185][0m |          -0.0108 |          15.1359 |          10.6657 |
[32m[20221213 22:36:06 @agent_ppo2.py:185][0m |          -0.0174 |          15.0917 |          10.6660 |
[32m[20221213 22:36:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:36:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.38
[32m[20221213 22:36:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.03
[32m[20221213 22:36:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.75
[32m[20221213 22:36:06 @agent_ppo2.py:143][0m Total time:      17.89 min
[32m[20221213 22:36:06 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221213 22:36:06 @agent_ppo2.py:121][0m #------------------------ Iteration 849 --------------------------#
[32m[20221213 22:36:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0030 |          31.5317 |          10.8435 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0045 |          29.1482 |          10.8426 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0068 |          28.0212 |          10.8361 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0054 |          27.4270 |          10.8379 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0106 |          26.9005 |          10.8451 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0109 |          26.5115 |          10.8443 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0128 |          26.3422 |          10.8430 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0066 |          26.7160 |          10.8553 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0043 |          25.9144 |          10.8465 |
[32m[20221213 22:36:07 @agent_ppo2.py:185][0m |          -0.0157 |          25.7469 |          10.8547 |
[32m[20221213 22:36:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:36:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.91
[32m[20221213 22:36:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.19
[32m[20221213 22:36:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.51
[32m[20221213 22:36:08 @agent_ppo2.py:143][0m Total time:      17.91 min
[32m[20221213 22:36:08 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221213 22:36:08 @agent_ppo2.py:121][0m #------------------------ Iteration 850 --------------------------#
[32m[20221213 22:36:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:36:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:08 @agent_ppo2.py:185][0m |          -0.0006 |          32.6258 |          11.0114 |
[32m[20221213 22:36:08 @agent_ppo2.py:185][0m |          -0.0076 |          31.2883 |          11.0036 |
[32m[20221213 22:36:08 @agent_ppo2.py:185][0m |          -0.0067 |          30.6252 |          10.9960 |
[32m[20221213 22:36:08 @agent_ppo2.py:185][0m |          -0.0072 |          30.2188 |          10.9927 |
[32m[20221213 22:36:08 @agent_ppo2.py:185][0m |          -0.0113 |          29.9583 |          10.9879 |
[32m[20221213 22:36:08 @agent_ppo2.py:185][0m |          -0.0117 |          29.7955 |          10.9901 |
[32m[20221213 22:36:08 @agent_ppo2.py:185][0m |          -0.0116 |          29.6128 |          10.9922 |
[32m[20221213 22:36:09 @agent_ppo2.py:185][0m |          -0.0123 |          29.3363 |          10.9791 |
[32m[20221213 22:36:09 @agent_ppo2.py:185][0m |          -0.0110 |          29.2798 |          10.9827 |
[32m[20221213 22:36:09 @agent_ppo2.py:185][0m |          -0.0103 |          29.0663 |          10.9811 |
[32m[20221213 22:36:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:36:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.09
[32m[20221213 22:36:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.04
[32m[20221213 22:36:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.03
[32m[20221213 22:36:09 @agent_ppo2.py:143][0m Total time:      17.93 min
[32m[20221213 22:36:09 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221213 22:36:09 @agent_ppo2.py:121][0m #------------------------ Iteration 851 --------------------------#
[32m[20221213 22:36:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:09 @agent_ppo2.py:185][0m |           0.0033 |          19.4525 |          10.9386 |
[32m[20221213 22:36:09 @agent_ppo2.py:185][0m |          -0.0037 |          16.8411 |          10.9227 |
[32m[20221213 22:36:09 @agent_ppo2.py:185][0m |          -0.0059 |          16.0285 |          10.9176 |
[32m[20221213 22:36:09 @agent_ppo2.py:185][0m |          -0.0045 |          15.9003 |          10.9199 |
[32m[20221213 22:36:10 @agent_ppo2.py:185][0m |          -0.0115 |          15.2577 |          10.9165 |
[32m[20221213 22:36:10 @agent_ppo2.py:185][0m |          -0.0071 |          15.1759 |          10.9140 |
[32m[20221213 22:36:10 @agent_ppo2.py:185][0m |          -0.0087 |          14.7036 |          10.9166 |
[32m[20221213 22:36:10 @agent_ppo2.py:185][0m |          -0.0118 |          14.4247 |          10.9139 |
[32m[20221213 22:36:10 @agent_ppo2.py:185][0m |          -0.0075 |          14.8203 |          10.9120 |
[32m[20221213 22:36:10 @agent_ppo2.py:185][0m |           0.0009 |          14.1995 |          10.9116 |
[32m[20221213 22:36:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:36:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.99
[32m[20221213 22:36:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.34
[32m[20221213 22:36:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.83
[32m[20221213 22:36:10 @agent_ppo2.py:143][0m Total time:      17.95 min
[32m[20221213 22:36:10 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221213 22:36:10 @agent_ppo2.py:121][0m #------------------------ Iteration 852 --------------------------#
[32m[20221213 22:36:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:36:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:10 @agent_ppo2.py:185][0m |          -0.0033 |          27.6729 |          11.0694 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |           0.0057 |          26.6962 |          11.0609 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0117 |          23.6354 |          11.0538 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0120 |          22.8292 |          11.0547 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0034 |          23.6563 |          11.0577 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0101 |          22.0409 |          11.0485 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0131 |          21.4999 |          11.0514 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0176 |          21.3263 |          11.0526 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0157 |          21.1467 |          11.0538 |
[32m[20221213 22:36:11 @agent_ppo2.py:185][0m |          -0.0084 |          21.3085 |          11.0442 |
[32m[20221213 22:36:11 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:36:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 169.77
[32m[20221213 22:36:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.37
[32m[20221213 22:36:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.07
[32m[20221213 22:36:11 @agent_ppo2.py:143][0m Total time:      17.98 min
[32m[20221213 22:36:11 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221213 22:36:11 @agent_ppo2.py:121][0m #------------------------ Iteration 853 --------------------------#
[32m[20221213 22:36:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:36:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |           0.0004 |           9.2025 |          11.0942 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |          -0.0054 |           6.9379 |          11.0739 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |          -0.0007 |           6.7551 |          11.0885 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |          -0.0073 |           6.6274 |          11.0819 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |          -0.0050 |           6.5985 |          11.0938 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |           0.0017 |           6.5594 |          11.0870 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |          -0.0091 |           6.4873 |          11.0791 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |          -0.0044 |           6.4935 |          11.0920 |
[32m[20221213 22:36:12 @agent_ppo2.py:185][0m |          -0.0057 |           6.4337 |          11.0885 |
[32m[20221213 22:36:13 @agent_ppo2.py:185][0m |          -0.0086 |           6.4237 |          11.0913 |
[32m[20221213 22:36:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:36:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:36:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:36:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 13.54
[32m[20221213 22:36:13 @agent_ppo2.py:143][0m Total time:      18.00 min
[32m[20221213 22:36:13 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221213 22:36:13 @agent_ppo2.py:121][0m #------------------------ Iteration 854 --------------------------#
[32m[20221213 22:36:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:36:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:13 @agent_ppo2.py:185][0m |          -0.0054 |          24.1481 |          11.1143 |
[32m[20221213 22:36:13 @agent_ppo2.py:185][0m |          -0.0004 |          21.9864 |          11.0867 |
[32m[20221213 22:36:13 @agent_ppo2.py:185][0m |           0.0050 |          24.0820 |          11.0942 |
[32m[20221213 22:36:13 @agent_ppo2.py:185][0m |           0.0008 |          20.6769 |          11.0556 |
[32m[20221213 22:36:13 @agent_ppo2.py:185][0m |          -0.0101 |          20.1033 |          11.0837 |
[32m[20221213 22:36:13 @agent_ppo2.py:185][0m |          -0.0077 |          19.8575 |          11.0865 |
[32m[20221213 22:36:14 @agent_ppo2.py:185][0m |          -0.0074 |          19.5316 |          11.0841 |
[32m[20221213 22:36:14 @agent_ppo2.py:185][0m |          -0.0136 |          19.4769 |          11.0886 |
[32m[20221213 22:36:14 @agent_ppo2.py:185][0m |          -0.0127 |          19.3148 |          11.0730 |
[32m[20221213 22:36:14 @agent_ppo2.py:185][0m |          -0.0086 |          19.1492 |          11.0838 |
[32m[20221213 22:36:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:36:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.67
[32m[20221213 22:36:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 318.44
[32m[20221213 22:36:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.56
[32m[20221213 22:36:14 @agent_ppo2.py:143][0m Total time:      18.02 min
[32m[20221213 22:36:14 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221213 22:36:14 @agent_ppo2.py:121][0m #------------------------ Iteration 855 --------------------------#
[32m[20221213 22:36:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:14 @agent_ppo2.py:185][0m |          -0.0020 |          23.9309 |          11.1460 |
[32m[20221213 22:36:14 @agent_ppo2.py:185][0m |          -0.0051 |          21.7576 |          11.1365 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0120 |          20.8682 |          11.1274 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0134 |          20.0344 |          11.1337 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0089 |          19.3981 |          11.1311 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0107 |          18.8714 |          11.1290 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0173 |          18.6113 |          11.1295 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0123 |          18.1783 |          11.1301 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0094 |          18.0710 |          11.1299 |
[32m[20221213 22:36:15 @agent_ppo2.py:185][0m |          -0.0159 |          17.6305 |          11.1267 |
[32m[20221213 22:36:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:36:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.85
[32m[20221213 22:36:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.74
[32m[20221213 22:36:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.37
[32m[20221213 22:36:15 @agent_ppo2.py:143][0m Total time:      18.04 min
[32m[20221213 22:36:15 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221213 22:36:15 @agent_ppo2.py:121][0m #------------------------ Iteration 856 --------------------------#
[32m[20221213 22:36:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:36:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0015 |          28.2257 |          11.0342 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0014 |          26.3111 |          11.0205 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |           0.0000 |          25.5964 |          11.0152 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0065 |          25.4309 |          11.0218 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0071 |          24.7080 |          11.0049 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0046 |          24.4202 |          11.0158 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0078 |          23.9592 |          11.0214 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0091 |          23.7884 |          11.0150 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0135 |          23.6968 |          11.0211 |
[32m[20221213 22:36:16 @agent_ppo2.py:185][0m |          -0.0139 |          23.5647 |          11.0096 |
[32m[20221213 22:36:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:36:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.21
[32m[20221213 22:36:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.55
[32m[20221213 22:36:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.74
[32m[20221213 22:36:17 @agent_ppo2.py:143][0m Total time:      18.06 min
[32m[20221213 22:36:17 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221213 22:36:17 @agent_ppo2.py:121][0m #------------------------ Iteration 857 --------------------------#
[32m[20221213 22:36:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |          -0.0012 |          25.0047 |          11.2826 |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |           0.0079 |          24.5181 |          11.2826 |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |          -0.0047 |          22.0312 |          11.2746 |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |          -0.0073 |          21.4573 |          11.2705 |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |          -0.0065 |          21.0704 |          11.2727 |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |          -0.0039 |          22.0624 |          11.2740 |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |          -0.0070 |          20.4909 |          11.2678 |
[32m[20221213 22:36:17 @agent_ppo2.py:185][0m |          -0.0104 |          20.3868 |          11.2742 |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |          -0.0104 |          19.9570 |          11.2680 |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |          -0.0133 |          19.8832 |          11.2724 |
[32m[20221213 22:36:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:36:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 200.66
[32m[20221213 22:36:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.71
[32m[20221213 22:36:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.14
[32m[20221213 22:36:18 @agent_ppo2.py:143][0m Total time:      18.08 min
[32m[20221213 22:36:18 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221213 22:36:18 @agent_ppo2.py:121][0m #------------------------ Iteration 858 --------------------------#
[32m[20221213 22:36:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |           0.0036 |          26.0945 |          11.2514 |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |          -0.0062 |          23.2463 |          11.2598 |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |          -0.0083 |          22.2647 |          11.2630 |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |          -0.0054 |          21.7805 |          11.2560 |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |          -0.0085 |          21.3254 |          11.2526 |
[32m[20221213 22:36:18 @agent_ppo2.py:185][0m |          -0.0098 |          21.0548 |          11.2497 |
[32m[20221213 22:36:19 @agent_ppo2.py:185][0m |          -0.0080 |          20.6782 |          11.2530 |
[32m[20221213 22:36:19 @agent_ppo2.py:185][0m |          -0.0108 |          20.4449 |          11.2594 |
[32m[20221213 22:36:19 @agent_ppo2.py:185][0m |          -0.0143 |          20.3608 |          11.2512 |
[32m[20221213 22:36:19 @agent_ppo2.py:185][0m |          -0.0092 |          20.0500 |          11.2527 |
[32m[20221213 22:36:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:36:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.40
[32m[20221213 22:36:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.27
[32m[20221213 22:36:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.20
[32m[20221213 22:36:19 @agent_ppo2.py:143][0m Total time:      18.10 min
[32m[20221213 22:36:19 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221213 22:36:19 @agent_ppo2.py:121][0m #------------------------ Iteration 859 --------------------------#
[32m[20221213 22:36:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:19 @agent_ppo2.py:185][0m |          -0.0016 |          22.8322 |          11.2237 |
[32m[20221213 22:36:19 @agent_ppo2.py:185][0m |          -0.0079 |          19.3929 |          11.2073 |
[32m[20221213 22:36:19 @agent_ppo2.py:185][0m |          -0.0126 |          17.4224 |          11.2072 |
[32m[20221213 22:36:20 @agent_ppo2.py:185][0m |          -0.0116 |          16.7238 |          11.2067 |
[32m[20221213 22:36:20 @agent_ppo2.py:185][0m |          -0.0108 |          16.2756 |          11.1988 |
[32m[20221213 22:36:20 @agent_ppo2.py:185][0m |          -0.0168 |          15.8397 |          11.2016 |
[32m[20221213 22:36:20 @agent_ppo2.py:185][0m |          -0.0072 |          16.6359 |          11.2033 |
[32m[20221213 22:36:20 @agent_ppo2.py:185][0m |          -0.0194 |          15.4573 |          11.1949 |
[32m[20221213 22:36:20 @agent_ppo2.py:185][0m |          -0.0118 |          15.2086 |          11.1996 |
[32m[20221213 22:36:20 @agent_ppo2.py:185][0m |          -0.0090 |          15.3084 |          11.2017 |
[32m[20221213 22:36:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:36:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.90
[32m[20221213 22:36:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.13
[32m[20221213 22:36:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.79
[32m[20221213 22:36:20 @agent_ppo2.py:143][0m Total time:      18.12 min
[32m[20221213 22:36:20 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221213 22:36:20 @agent_ppo2.py:121][0m #------------------------ Iteration 860 --------------------------#
[32m[20221213 22:36:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:36:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |           0.0029 |          30.5480 |          11.1997 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0058 |          28.7633 |          11.1957 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0079 |          27.8735 |          11.1947 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0065 |          27.3389 |          11.2001 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0042 |          26.9255 |          11.1941 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0059 |          26.8307 |          11.2003 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0084 |          26.3486 |          11.2001 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0083 |          26.0976 |          11.1959 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0137 |          26.0280 |          11.2051 |
[32m[20221213 22:36:21 @agent_ppo2.py:185][0m |          -0.0067 |          25.8046 |          11.2040 |
[32m[20221213 22:36:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.72
[32m[20221213 22:36:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.04
[32m[20221213 22:36:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.07
[32m[20221213 22:36:21 @agent_ppo2.py:143][0m Total time:      18.14 min
[32m[20221213 22:36:21 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221213 22:36:21 @agent_ppo2.py:121][0m #------------------------ Iteration 861 --------------------------#
[32m[20221213 22:36:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0050 |          37.0684 |          11.1885 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0060 |          36.0697 |          11.1725 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0123 |          34.7849 |          11.1609 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0102 |          34.4012 |          11.1665 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0123 |          33.9123 |          11.1761 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0028 |          35.8736 |          11.1735 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0101 |          33.6068 |          11.1558 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0091 |          33.0452 |          11.1640 |
[32m[20221213 22:36:22 @agent_ppo2.py:185][0m |          -0.0105 |          32.8832 |          11.1704 |
[32m[20221213 22:36:23 @agent_ppo2.py:185][0m |          -0.0061 |          33.8208 |          11.1746 |
[32m[20221213 22:36:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:36:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.10
[32m[20221213 22:36:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.24
[32m[20221213 22:36:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 174.58
[32m[20221213 22:36:23 @agent_ppo2.py:143][0m Total time:      18.16 min
[32m[20221213 22:36:23 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221213 22:36:23 @agent_ppo2.py:121][0m #------------------------ Iteration 862 --------------------------#
[32m[20221213 22:36:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:36:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:23 @agent_ppo2.py:185][0m |           0.0110 |          30.2978 |          11.2065 |
[32m[20221213 22:36:23 @agent_ppo2.py:185][0m |           0.0101 |          26.5859 |          11.1846 |
[32m[20221213 22:36:23 @agent_ppo2.py:185][0m |          -0.0096 |          20.9296 |          11.1788 |
[32m[20221213 22:36:23 @agent_ppo2.py:185][0m |          -0.0094 |          18.9906 |          11.1838 |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0098 |          18.1261 |          11.1945 |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0112 |          17.4727 |          11.1923 |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0059 |          17.0975 |          11.1825 |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0077 |          16.6816 |          11.1979 |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0108 |          16.4322 |          11.1917 |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0119 |          16.1723 |          11.1894 |
[32m[20221213 22:36:24 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:36:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.15
[32m[20221213 22:36:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.53
[32m[20221213 22:36:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.34
[32m[20221213 22:36:24 @agent_ppo2.py:143][0m Total time:      18.19 min
[32m[20221213 22:36:24 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221213 22:36:24 @agent_ppo2.py:121][0m #------------------------ Iteration 863 --------------------------#
[32m[20221213 22:36:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0021 |          19.2662 |          11.1911 |
[32m[20221213 22:36:24 @agent_ppo2.py:185][0m |          -0.0076 |          18.2133 |          11.1916 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |           0.0054 |          19.5856 |          11.1813 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |          -0.0062 |          17.9747 |          11.1595 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |          -0.0077 |          17.8679 |          11.1796 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |          -0.0076 |          17.7164 |          11.1832 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |          -0.0139 |          17.6984 |          11.1922 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |          -0.0084 |          17.6074 |          11.1844 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |          -0.0091 |          17.6855 |          11.1887 |
[32m[20221213 22:36:25 @agent_ppo2.py:185][0m |          -0.0089 |          17.5637 |          11.1815 |
[32m[20221213 22:36:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.94
[32m[20221213 22:36:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.83
[32m[20221213 22:36:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.26
[32m[20221213 22:36:25 @agent_ppo2.py:143][0m Total time:      18.21 min
[32m[20221213 22:36:25 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221213 22:36:25 @agent_ppo2.py:121][0m #------------------------ Iteration 864 --------------------------#
[32m[20221213 22:36:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0074 |          27.1204 |          11.2514 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0067 |          24.6335 |          11.2365 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0095 |          24.0384 |          11.2291 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0098 |          23.8331 |          11.2298 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0048 |          23.7895 |          11.2247 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0108 |          23.5523 |          11.2205 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0093 |          23.4807 |          11.2187 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0112 |          23.2930 |          11.2147 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0118 |          23.2357 |          11.2221 |
[32m[20221213 22:36:26 @agent_ppo2.py:185][0m |          -0.0111 |          23.2242 |          11.2186 |
[32m[20221213 22:36:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:36:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.54
[32m[20221213 22:36:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.92
[32m[20221213 22:36:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.02
[32m[20221213 22:36:27 @agent_ppo2.py:143][0m Total time:      18.23 min
[32m[20221213 22:36:27 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221213 22:36:27 @agent_ppo2.py:121][0m #------------------------ Iteration 865 --------------------------#
[32m[20221213 22:36:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |           0.0027 |          23.6310 |          11.3409 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0026 |          17.3386 |          11.3231 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0060 |          15.9950 |          11.3246 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0068 |          15.3088 |          11.3149 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0059 |          14.8188 |          11.3200 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0074 |          14.5306 |          11.3208 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0120 |          14.2870 |          11.3216 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0040 |          14.8327 |          11.3217 |
[32m[20221213 22:36:27 @agent_ppo2.py:185][0m |          -0.0136 |          13.8195 |          11.3159 |
[32m[20221213 22:36:28 @agent_ppo2.py:185][0m |          -0.0083 |          13.7277 |          11.3169 |
[32m[20221213 22:36:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.72
[32m[20221213 22:36:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.18
[32m[20221213 22:36:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.53
[32m[20221213 22:36:28 @agent_ppo2.py:143][0m Total time:      18.25 min
[32m[20221213 22:36:28 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221213 22:36:28 @agent_ppo2.py:121][0m #------------------------ Iteration 866 --------------------------#
[32m[20221213 22:36:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:28 @agent_ppo2.py:185][0m |          -0.0002 |          26.3226 |          11.3799 |
[32m[20221213 22:36:28 @agent_ppo2.py:185][0m |          -0.0058 |          24.5924 |          11.3731 |
[32m[20221213 22:36:28 @agent_ppo2.py:185][0m |          -0.0090 |          24.2354 |          11.3658 |
[32m[20221213 22:36:28 @agent_ppo2.py:185][0m |          -0.0102 |          24.0227 |          11.3580 |
[32m[20221213 22:36:28 @agent_ppo2.py:185][0m |          -0.0087 |          23.8949 |          11.3517 |
[32m[20221213 22:36:28 @agent_ppo2.py:185][0m |          -0.0080 |          23.7652 |          11.3579 |
[32m[20221213 22:36:29 @agent_ppo2.py:185][0m |          -0.0113 |          23.6355 |          11.3436 |
[32m[20221213 22:36:29 @agent_ppo2.py:185][0m |          -0.0125 |          23.6688 |          11.3414 |
[32m[20221213 22:36:29 @agent_ppo2.py:185][0m |          -0.0120 |          23.5237 |          11.3457 |
[32m[20221213 22:36:29 @agent_ppo2.py:185][0m |          -0.0070 |          23.8628 |          11.3479 |
[32m[20221213 22:36:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.26
[32m[20221213 22:36:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.14
[32m[20221213 22:36:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.32
[32m[20221213 22:36:29 @agent_ppo2.py:143][0m Total time:      18.27 min
[32m[20221213 22:36:29 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221213 22:36:29 @agent_ppo2.py:121][0m #------------------------ Iteration 867 --------------------------#
[32m[20221213 22:36:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:29 @agent_ppo2.py:185][0m |          -0.0040 |          28.3823 |          11.3989 |
[32m[20221213 22:36:29 @agent_ppo2.py:185][0m |          -0.0028 |          26.0488 |          11.3847 |
[32m[20221213 22:36:29 @agent_ppo2.py:185][0m |          -0.0112 |          25.0254 |          11.3848 |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |          -0.0119 |          24.2609 |          11.3872 |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |          -0.0109 |          23.8304 |          11.3851 |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |          -0.0109 |          23.4541 |          11.3844 |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |          -0.0121 |          23.0885 |          11.3883 |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |          -0.0132 |          22.9072 |          11.3940 |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |          -0.0056 |          23.5276 |          11.3906 |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |          -0.0158 |          22.6484 |          11.3892 |
[32m[20221213 22:36:30 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.66
[32m[20221213 22:36:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.75
[32m[20221213 22:36:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.32
[32m[20221213 22:36:30 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 421.32
[32m[20221213 22:36:30 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 421.32
[32m[20221213 22:36:30 @agent_ppo2.py:143][0m Total time:      18.29 min
[32m[20221213 22:36:30 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221213 22:36:30 @agent_ppo2.py:121][0m #------------------------ Iteration 868 --------------------------#
[32m[20221213 22:36:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:30 @agent_ppo2.py:185][0m |           0.0027 |          19.5991 |          11.5490 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |           0.0005 |          17.9707 |          11.5319 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |          -0.0085 |          17.0394 |          11.5186 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |           0.0064 |          18.3198 |          11.5268 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |          -0.0102 |          16.6158 |          11.5190 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |          -0.0085 |          16.2614 |          11.5218 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |          -0.0117 |          16.1765 |          11.5241 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |          -0.0118 |          16.0909 |          11.5261 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |          -0.0051 |          15.9468 |          11.5128 |
[32m[20221213 22:36:31 @agent_ppo2.py:185][0m |          -0.0111 |          15.8250 |          11.5214 |
[32m[20221213 22:36:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.96
[32m[20221213 22:36:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.32
[32m[20221213 22:36:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.98
[32m[20221213 22:36:31 @agent_ppo2.py:143][0m Total time:      18.31 min
[32m[20221213 22:36:31 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221213 22:36:31 @agent_ppo2.py:121][0m #------------------------ Iteration 869 --------------------------#
[32m[20221213 22:36:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0019 |          19.1092 |          11.4699 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0018 |          16.4373 |          11.4490 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0135 |          15.7188 |          11.4546 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0112 |          15.1696 |          11.4538 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0123 |          14.8329 |          11.4483 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0088 |          14.5540 |          11.4541 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0017 |          15.2772 |          11.4539 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0079 |          14.0496 |          11.4427 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0097 |          13.8933 |          11.4552 |
[32m[20221213 22:36:32 @agent_ppo2.py:185][0m |          -0.0152 |          13.7399 |          11.4544 |
[32m[20221213 22:36:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.80
[32m[20221213 22:36:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.96
[32m[20221213 22:36:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.99
[32m[20221213 22:36:33 @agent_ppo2.py:143][0m Total time:      18.33 min
[32m[20221213 22:36:33 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221213 22:36:33 @agent_ppo2.py:121][0m #------------------------ Iteration 870 --------------------------#
[32m[20221213 22:36:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:36:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0048 |          20.2574 |          11.5616 |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0059 |          16.4142 |          11.5516 |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0023 |          15.2255 |          11.5445 |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0099 |          14.4044 |          11.5465 |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0111 |          14.2814 |          11.5382 |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0134 |          13.6248 |          11.5357 |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0195 |          13.3711 |          11.5328 |
[32m[20221213 22:36:33 @agent_ppo2.py:185][0m |          -0.0082 |          13.1104 |          11.5318 |
[32m[20221213 22:36:34 @agent_ppo2.py:185][0m |          -0.0155 |          12.8349 |          11.5304 |
[32m[20221213 22:36:34 @agent_ppo2.py:185][0m |          -0.0157 |          12.7043 |          11.5290 |
[32m[20221213 22:36:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 173.72
[32m[20221213 22:36:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.59
[32m[20221213 22:36:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.65
[32m[20221213 22:36:34 @agent_ppo2.py:143][0m Total time:      18.35 min
[32m[20221213 22:36:34 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221213 22:36:34 @agent_ppo2.py:121][0m #------------------------ Iteration 871 --------------------------#
[32m[20221213 22:36:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:34 @agent_ppo2.py:185][0m |           0.0008 |          31.5063 |          11.5864 |
[32m[20221213 22:36:34 @agent_ppo2.py:185][0m |          -0.0061 |          29.6830 |          11.5695 |
[32m[20221213 22:36:34 @agent_ppo2.py:185][0m |          -0.0065 |          28.8650 |          11.5636 |
[32m[20221213 22:36:34 @agent_ppo2.py:185][0m |          -0.0114 |          28.4147 |          11.5537 |
[32m[20221213 22:36:34 @agent_ppo2.py:185][0m |          -0.0093 |          28.2216 |          11.5483 |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |          -0.0047 |          27.9855 |          11.5561 |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |          -0.0103 |          27.6549 |          11.5445 |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |          -0.0073 |          27.6215 |          11.5439 |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |          -0.0106 |          27.2760 |          11.5462 |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |          -0.0028 |          27.6283 |          11.5428 |
[32m[20221213 22:36:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.19
[32m[20221213 22:36:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.19
[32m[20221213 22:36:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.14
[32m[20221213 22:36:35 @agent_ppo2.py:143][0m Total time:      18.37 min
[32m[20221213 22:36:35 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221213 22:36:35 @agent_ppo2.py:121][0m #------------------------ Iteration 872 --------------------------#
[32m[20221213 22:36:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |           0.0040 |          19.9493 |          11.4193 |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |          -0.0095 |          16.5796 |          11.4089 |
[32m[20221213 22:36:35 @agent_ppo2.py:185][0m |          -0.0022 |          16.0264 |          11.3953 |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0052 |          15.3624 |          11.3847 |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0081 |          14.8370 |          11.3866 |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0126 |          14.6176 |          11.3801 |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0150 |          14.3577 |          11.3851 |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0097 |          14.1127 |          11.3830 |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0142 |          13.8911 |          11.3860 |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0147 |          13.9788 |          11.3821 |
[32m[20221213 22:36:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.49
[32m[20221213 22:36:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.71
[32m[20221213 22:36:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.75
[32m[20221213 22:36:36 @agent_ppo2.py:143][0m Total time:      18.39 min
[32m[20221213 22:36:36 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221213 22:36:36 @agent_ppo2.py:121][0m #------------------------ Iteration 873 --------------------------#
[32m[20221213 22:36:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:36 @agent_ppo2.py:185][0m |          -0.0019 |          25.3811 |          11.3204 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |           0.0045 |          22.5822 |          11.3126 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0086 |          21.0592 |          11.3100 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0105 |          20.0425 |          11.3150 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0100 |          19.7859 |          11.3086 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0089 |          19.2480 |          11.3107 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0126 |          19.0257 |          11.3166 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0069 |          18.7984 |          11.3140 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0139 |          18.6298 |          11.3177 |
[32m[20221213 22:36:37 @agent_ppo2.py:185][0m |          -0.0124 |          18.4080 |          11.3171 |
[32m[20221213 22:36:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:36:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 207.75
[32m[20221213 22:36:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.88
[32m[20221213 22:36:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.01
[32m[20221213 22:36:37 @agent_ppo2.py:143][0m Total time:      18.41 min
[32m[20221213 22:36:37 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221213 22:36:37 @agent_ppo2.py:121][0m #------------------------ Iteration 874 --------------------------#
[32m[20221213 22:36:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |           0.0002 |          18.4027 |          11.5034 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0065 |          14.9182 |          11.4802 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0084 |          13.7952 |          11.4772 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0014 |          12.9212 |          11.4808 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0136 |          12.4255 |          11.4814 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0086 |          11.7818 |          11.4759 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0121 |          11.3638 |          11.4836 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0206 |          10.9798 |          11.4920 |
[32m[20221213 22:36:38 @agent_ppo2.py:185][0m |          -0.0140 |          10.6582 |          11.4869 |
[32m[20221213 22:36:39 @agent_ppo2.py:185][0m |          -0.0120 |          10.6583 |          11.4811 |
[32m[20221213 22:36:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:36:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.31
[32m[20221213 22:36:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.54
[32m[20221213 22:36:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.41
[32m[20221213 22:36:39 @agent_ppo2.py:143][0m Total time:      18.43 min
[32m[20221213 22:36:39 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221213 22:36:39 @agent_ppo2.py:121][0m #------------------------ Iteration 875 --------------------------#
[32m[20221213 22:36:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:39 @agent_ppo2.py:185][0m |           0.0021 |          27.6104 |          11.4354 |
[32m[20221213 22:36:39 @agent_ppo2.py:185][0m |          -0.0009 |          25.2437 |          11.4368 |
[32m[20221213 22:36:39 @agent_ppo2.py:185][0m |          -0.0065 |          24.1928 |          11.4188 |
[32m[20221213 22:36:39 @agent_ppo2.py:185][0m |          -0.0038 |          23.8002 |          11.4208 |
[32m[20221213 22:36:39 @agent_ppo2.py:185][0m |           0.0068 |          25.6398 |          11.4196 |
[32m[20221213 22:36:39 @agent_ppo2.py:185][0m |          -0.0101 |          23.4634 |          11.4007 |
[32m[20221213 22:36:40 @agent_ppo2.py:185][0m |          -0.0089 |          23.1965 |          11.4154 |
[32m[20221213 22:36:40 @agent_ppo2.py:185][0m |          -0.0122 |          23.0714 |          11.4153 |
[32m[20221213 22:36:40 @agent_ppo2.py:185][0m |          -0.0090 |          22.8645 |          11.4144 |
[32m[20221213 22:36:40 @agent_ppo2.py:185][0m |          -0.0121 |          22.7727 |          11.4011 |
[32m[20221213 22:36:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:36:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.29
[32m[20221213 22:36:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.29
[32m[20221213 22:36:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.79
[32m[20221213 22:36:40 @agent_ppo2.py:143][0m Total time:      18.45 min
[32m[20221213 22:36:40 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221213 22:36:40 @agent_ppo2.py:121][0m #------------------------ Iteration 876 --------------------------#
[32m[20221213 22:36:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:36:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:40 @agent_ppo2.py:185][0m |           0.0014 |          26.6698 |          11.6948 |
[32m[20221213 22:36:40 @agent_ppo2.py:185][0m |          -0.0113 |          24.4084 |          11.6902 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |           0.0034 |          23.9621 |          11.7073 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |          -0.0048 |          22.6876 |          11.6892 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |          -0.0133 |          22.3667 |          11.6932 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |          -0.0094 |          21.9356 |          11.6992 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |          -0.0083 |          21.4885 |          11.6963 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |           0.0014 |          22.1521 |          11.6995 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |          -0.0069 |          21.1726 |          11.7012 |
[32m[20221213 22:36:41 @agent_ppo2.py:185][0m |          -0.0145 |          20.9445 |          11.7016 |
[32m[20221213 22:36:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:36:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.77
[32m[20221213 22:36:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.89
[32m[20221213 22:36:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.52
[32m[20221213 22:36:41 @agent_ppo2.py:143][0m Total time:      18.47 min
[32m[20221213 22:36:41 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221213 22:36:41 @agent_ppo2.py:121][0m #------------------------ Iteration 877 --------------------------#
[32m[20221213 22:36:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0003 |          26.6785 |          11.5873 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0074 |          24.3272 |          11.5723 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0071 |          23.7196 |          11.5759 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0062 |          23.0478 |          11.5659 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0136 |          22.7694 |          11.5657 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0039 |          22.3167 |          11.5737 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0147 |          21.9760 |          11.5771 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0135 |          21.9343 |          11.5757 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0117 |          21.4905 |          11.5798 |
[32m[20221213 22:36:42 @agent_ppo2.py:185][0m |          -0.0123 |          21.4195 |          11.5838 |
[32m[20221213 22:36:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:36:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.68
[32m[20221213 22:36:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.49
[32m[20221213 22:36:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.62
[32m[20221213 22:36:43 @agent_ppo2.py:143][0m Total time:      18.49 min
[32m[20221213 22:36:43 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221213 22:36:43 @agent_ppo2.py:121][0m #------------------------ Iteration 878 --------------------------#
[32m[20221213 22:36:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |           0.0014 |          23.5582 |          11.8396 |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |          -0.0049 |          21.1042 |          11.8209 |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |          -0.0098 |          20.4058 |          11.8130 |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |           0.0018 |          21.4015 |          11.8186 |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |           0.0046 |          21.3582 |          11.7992 |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |          -0.0075 |          19.1099 |          11.8007 |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |          -0.0089 |          18.6050 |          11.8155 |
[32m[20221213 22:36:43 @agent_ppo2.py:185][0m |          -0.0101 |          18.3351 |          11.8089 |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |          -0.0071 |          18.1731 |          11.8077 |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |          -0.0095 |          17.9886 |          11.8107 |
[32m[20221213 22:36:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 190.20
[32m[20221213 22:36:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.06
[32m[20221213 22:36:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.66
[32m[20221213 22:36:44 @agent_ppo2.py:143][0m Total time:      18.51 min
[32m[20221213 22:36:44 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221213 22:36:44 @agent_ppo2.py:121][0m #------------------------ Iteration 879 --------------------------#
[32m[20221213 22:36:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |           0.0017 |          26.0063 |          11.8865 |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |           0.0000 |          21.9326 |          11.8770 |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |          -0.0049 |          20.6092 |          11.8766 |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |          -0.0040 |          20.4155 |          11.8710 |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |          -0.0062 |          19.3100 |          11.8797 |
[32m[20221213 22:36:44 @agent_ppo2.py:185][0m |          -0.0017 |          19.3000 |          11.8737 |
[32m[20221213 22:36:45 @agent_ppo2.py:185][0m |          -0.0087 |          18.6850 |          11.8702 |
[32m[20221213 22:36:45 @agent_ppo2.py:185][0m |          -0.0063 |          18.2717 |          11.8674 |
[32m[20221213 22:36:45 @agent_ppo2.py:185][0m |          -0.0101 |          18.2565 |          11.8771 |
[32m[20221213 22:36:45 @agent_ppo2.py:185][0m |          -0.0110 |          17.7820 |          11.8754 |
[32m[20221213 22:36:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.52
[32m[20221213 22:36:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.40
[32m[20221213 22:36:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.74
[32m[20221213 22:36:45 @agent_ppo2.py:143][0m Total time:      18.53 min
[32m[20221213 22:36:45 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221213 22:36:45 @agent_ppo2.py:121][0m #------------------------ Iteration 880 --------------------------#
[32m[20221213 22:36:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:36:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:45 @agent_ppo2.py:185][0m |          -0.0020 |          27.2458 |          11.6474 |
[32m[20221213 22:36:45 @agent_ppo2.py:185][0m |          -0.0051 |          23.8013 |          11.6266 |
[32m[20221213 22:36:45 @agent_ppo2.py:185][0m |          -0.0059 |          22.8053 |          11.6094 |
[32m[20221213 22:36:46 @agent_ppo2.py:185][0m |          -0.0121 |          22.2646 |          11.6183 |
[32m[20221213 22:36:46 @agent_ppo2.py:185][0m |          -0.0110 |          22.1701 |          11.5989 |
[32m[20221213 22:36:46 @agent_ppo2.py:185][0m |          -0.0014 |          23.5331 |          11.6179 |
[32m[20221213 22:36:46 @agent_ppo2.py:185][0m |          -0.0027 |          22.1326 |          11.5911 |
[32m[20221213 22:36:46 @agent_ppo2.py:185][0m |          -0.0106 |          21.4250 |          11.5961 |
[32m[20221213 22:36:46 @agent_ppo2.py:185][0m |          -0.0139 |          21.2804 |          11.5949 |
[32m[20221213 22:36:46 @agent_ppo2.py:185][0m |          -0.0160 |          21.2823 |          11.5913 |
[32m[20221213 22:36:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.58
[32m[20221213 22:36:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.38
[32m[20221213 22:36:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.96
[32m[20221213 22:36:46 @agent_ppo2.py:143][0m Total time:      18.55 min
[32m[20221213 22:36:46 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221213 22:36:46 @agent_ppo2.py:121][0m #------------------------ Iteration 881 --------------------------#
[32m[20221213 22:36:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0022 |          34.0027 |          11.8635 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0080 |          31.1628 |          11.8366 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0086 |          30.0486 |          11.8416 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0107 |          28.7932 |          11.8388 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0071 |          28.4687 |          11.8428 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0096 |          27.8833 |          11.8351 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0096 |          27.5915 |          11.8432 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0106 |          27.2744 |          11.8366 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |           0.0017 |          28.8140 |          11.8423 |
[32m[20221213 22:36:47 @agent_ppo2.py:185][0m |          -0.0139 |          27.5923 |          11.8427 |
[32m[20221213 22:36:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.45
[32m[20221213 22:36:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.10
[32m[20221213 22:36:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.70
[32m[20221213 22:36:47 @agent_ppo2.py:143][0m Total time:      18.57 min
[32m[20221213 22:36:47 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221213 22:36:47 @agent_ppo2.py:121][0m #------------------------ Iteration 882 --------------------------#
[32m[20221213 22:36:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |           0.0032 |          33.8892 |          11.7929 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0080 |          31.0138 |          11.7786 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0029 |          30.1663 |          11.7571 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0046 |          29.8627 |          11.7632 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0097 |          29.2921 |          11.7628 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0107 |          28.8931 |          11.7465 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0111 |          28.7651 |          11.7459 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0029 |          31.3926 |          11.7419 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0109 |          28.4908 |          11.7190 |
[32m[20221213 22:36:48 @agent_ppo2.py:185][0m |          -0.0050 |          28.2388 |          11.7387 |
[32m[20221213 22:36:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.79
[32m[20221213 22:36:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.87
[32m[20221213 22:36:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.48
[32m[20221213 22:36:49 @agent_ppo2.py:143][0m Total time:      18.59 min
[32m[20221213 22:36:49 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221213 22:36:49 @agent_ppo2.py:121][0m #------------------------ Iteration 883 --------------------------#
[32m[20221213 22:36:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |          -0.0055 |          30.7311 |          11.9959 |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |           0.0046 |          30.1537 |          11.9681 |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |          -0.0019 |          27.5978 |          11.9541 |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |          -0.0110 |          26.8684 |          11.9560 |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |          -0.0112 |          26.5922 |          11.9562 |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |          -0.0085 |          26.4133 |          11.9530 |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |          -0.0125 |          25.7979 |          11.9535 |
[32m[20221213 22:36:49 @agent_ppo2.py:185][0m |          -0.0134 |          25.6581 |          11.9539 |
[32m[20221213 22:36:50 @agent_ppo2.py:185][0m |          -0.0157 |          25.3132 |          11.9552 |
[32m[20221213 22:36:50 @agent_ppo2.py:185][0m |          -0.0122 |          25.1657 |          11.9540 |
[32m[20221213 22:36:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:36:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.37
[32m[20221213 22:36:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.00
[32m[20221213 22:36:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.32
[32m[20221213 22:36:50 @agent_ppo2.py:143][0m Total time:      18.62 min
[32m[20221213 22:36:50 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221213 22:36:50 @agent_ppo2.py:121][0m #------------------------ Iteration 884 --------------------------#
[32m[20221213 22:36:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:50 @agent_ppo2.py:185][0m |           0.0007 |          23.5474 |          11.6008 |
[32m[20221213 22:36:50 @agent_ppo2.py:185][0m |           0.0030 |          24.5663 |          11.5735 |
[32m[20221213 22:36:50 @agent_ppo2.py:185][0m |          -0.0042 |          21.7638 |          11.5666 |
[32m[20221213 22:36:50 @agent_ppo2.py:185][0m |          -0.0110 |          21.1429 |          11.5608 |
[32m[20221213 22:36:50 @agent_ppo2.py:185][0m |          -0.0139 |          20.8880 |          11.5631 |
[32m[20221213 22:36:51 @agent_ppo2.py:185][0m |          -0.0127 |          20.4436 |          11.5617 |
[32m[20221213 22:36:51 @agent_ppo2.py:185][0m |          -0.0143 |          20.3530 |          11.5563 |
[32m[20221213 22:36:51 @agent_ppo2.py:185][0m |          -0.0110 |          20.0203 |          11.5569 |
[32m[20221213 22:36:51 @agent_ppo2.py:185][0m |          -0.0111 |          19.7534 |          11.5553 |
[32m[20221213 22:36:51 @agent_ppo2.py:185][0m |          -0.0043 |          21.2632 |          11.5588 |
[32m[20221213 22:36:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.05
[32m[20221213 22:36:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.02
[32m[20221213 22:36:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.89
[32m[20221213 22:36:51 @agent_ppo2.py:143][0m Total time:      18.64 min
[32m[20221213 22:36:51 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221213 22:36:51 @agent_ppo2.py:121][0m #------------------------ Iteration 885 --------------------------#
[32m[20221213 22:36:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:51 @agent_ppo2.py:185][0m |           0.0007 |          22.8936 |          11.6631 |
[32m[20221213 22:36:51 @agent_ppo2.py:185][0m |          -0.0055 |          19.9911 |          11.6548 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0069 |          18.9587 |          11.6627 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0146 |          18.4255 |          11.6521 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0086 |          18.1543 |          11.6603 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0090 |          17.6484 |          11.6564 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0145 |          17.4082 |          11.6570 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0091 |          17.3513 |          11.6598 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0127 |          17.0299 |          11.6505 |
[32m[20221213 22:36:52 @agent_ppo2.py:185][0m |          -0.0123 |          16.8679 |          11.6535 |
[32m[20221213 22:36:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:36:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.40
[32m[20221213 22:36:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.21
[32m[20221213 22:36:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.64
[32m[20221213 22:36:52 @agent_ppo2.py:143][0m Total time:      18.66 min
[32m[20221213 22:36:52 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221213 22:36:52 @agent_ppo2.py:121][0m #------------------------ Iteration 886 --------------------------#
[32m[20221213 22:36:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |           0.0025 |          27.5680 |          11.6964 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0055 |          24.4585 |          11.6839 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0051 |          23.6098 |          11.6805 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0061 |          23.2705 |          11.6811 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0100 |          22.5889 |          11.6870 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0094 |          22.2585 |          11.6769 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0118 |          21.9206 |          11.6745 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0123 |          21.6714 |          11.6793 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0130 |          21.4207 |          11.6667 |
[32m[20221213 22:36:53 @agent_ppo2.py:185][0m |          -0.0081 |          21.2511 |          11.6661 |
[32m[20221213 22:36:53 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.94
[32m[20221213 22:36:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.91
[32m[20221213 22:36:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.20
[32m[20221213 22:36:53 @agent_ppo2.py:143][0m Total time:      18.68 min
[32m[20221213 22:36:53 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221213 22:36:53 @agent_ppo2.py:121][0m #------------------------ Iteration 887 --------------------------#
[32m[20221213 22:36:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |           0.0083 |          35.3968 |          11.6911 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0040 |          31.3296 |          11.6666 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0024 |          30.4799 |          11.6727 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0091 |          29.9465 |          11.6689 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0061 |          29.4843 |          11.6706 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0094 |          29.0476 |          11.6651 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0099 |          28.8226 |          11.6711 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0083 |          28.6875 |          11.6737 |
[32m[20221213 22:36:54 @agent_ppo2.py:185][0m |          -0.0059 |          28.4025 |          11.6745 |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |          -0.0120 |          28.2234 |          11.6645 |
[32m[20221213 22:36:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.70
[32m[20221213 22:36:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.89
[32m[20221213 22:36:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.10
[32m[20221213 22:36:55 @agent_ppo2.py:143][0m Total time:      18.70 min
[32m[20221213 22:36:55 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221213 22:36:55 @agent_ppo2.py:121][0m #------------------------ Iteration 888 --------------------------#
[32m[20221213 22:36:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |           0.0010 |          31.0047 |          11.9052 |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |          -0.0010 |          27.4491 |          11.8902 |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |          -0.0022 |          26.8242 |          11.8795 |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |          -0.0067 |          26.0774 |          11.8918 |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |          -0.0035 |          26.0369 |          11.8754 |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |          -0.0085 |          25.2002 |          11.8948 |
[32m[20221213 22:36:55 @agent_ppo2.py:185][0m |          -0.0046 |          25.0188 |          11.8776 |
[32m[20221213 22:36:56 @agent_ppo2.py:185][0m |          -0.0142 |          24.7412 |          11.8767 |
[32m[20221213 22:36:56 @agent_ppo2.py:185][0m |          -0.0120 |          24.6020 |          11.8792 |
[32m[20221213 22:36:56 @agent_ppo2.py:185][0m |          -0.0109 |          24.3839 |          11.8880 |
[32m[20221213 22:36:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.52
[32m[20221213 22:36:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.34
[32m[20221213 22:36:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.18
[32m[20221213 22:36:56 @agent_ppo2.py:143][0m Total time:      18.72 min
[32m[20221213 22:36:56 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221213 22:36:56 @agent_ppo2.py:121][0m #------------------------ Iteration 889 --------------------------#
[32m[20221213 22:36:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:56 @agent_ppo2.py:185][0m |           0.0040 |          27.8298 |          11.8183 |
[32m[20221213 22:36:56 @agent_ppo2.py:185][0m |          -0.0056 |          26.4656 |          11.8104 |
[32m[20221213 22:36:56 @agent_ppo2.py:185][0m |          -0.0075 |          25.9948 |          11.8039 |
[32m[20221213 22:36:56 @agent_ppo2.py:185][0m |          -0.0081 |          25.5472 |          11.8032 |
[32m[20221213 22:36:57 @agent_ppo2.py:185][0m |          -0.0087 |          25.2634 |          11.8055 |
[32m[20221213 22:36:57 @agent_ppo2.py:185][0m |          -0.0075 |          24.9365 |          11.8006 |
[32m[20221213 22:36:57 @agent_ppo2.py:185][0m |          -0.0096 |          24.6625 |          11.7930 |
[32m[20221213 22:36:57 @agent_ppo2.py:185][0m |          -0.0053 |          25.0440 |          11.7962 |
[32m[20221213 22:36:57 @agent_ppo2.py:185][0m |          -0.0096 |          24.6246 |          11.8002 |
[32m[20221213 22:36:57 @agent_ppo2.py:185][0m |          -0.0098 |          24.4450 |          11.7983 |
[32m[20221213 22:36:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:36:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.54
[32m[20221213 22:36:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.85
[32m[20221213 22:36:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.22
[32m[20221213 22:36:57 @agent_ppo2.py:143][0m Total time:      18.74 min
[32m[20221213 22:36:57 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221213 22:36:57 @agent_ppo2.py:121][0m #------------------------ Iteration 890 --------------------------#
[32m[20221213 22:36:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:36:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:57 @agent_ppo2.py:185][0m |           0.0003 |          28.6575 |          11.7900 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0037 |          26.6833 |          11.7745 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0079 |          23.8998 |          11.7817 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0098 |          23.1539 |          11.7871 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0068 |          22.7654 |          11.7870 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0105 |          22.4885 |          11.7860 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0143 |          22.2354 |          11.7837 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0033 |          22.4833 |          11.7827 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0162 |          21.7695 |          11.7831 |
[32m[20221213 22:36:58 @agent_ppo2.py:185][0m |          -0.0090 |          21.4995 |          11.7822 |
[32m[20221213 22:36:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:36:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.11
[32m[20221213 22:36:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.77
[32m[20221213 22:36:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.24
[32m[20221213 22:36:58 @agent_ppo2.py:143][0m Total time:      18.76 min
[32m[20221213 22:36:58 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221213 22:36:58 @agent_ppo2.py:121][0m #------------------------ Iteration 891 --------------------------#
[32m[20221213 22:36:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:36:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0013 |          36.2126 |          12.0307 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0077 |          33.5132 |          12.0003 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0074 |          32.5632 |          12.0064 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0057 |          32.1377 |          12.0046 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0088 |          31.9181 |          12.0047 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0065 |          31.9148 |          12.0084 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0023 |          32.7922 |          12.0029 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0101 |          31.3118 |          12.0059 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0090 |          31.0588 |          12.0032 |
[32m[20221213 22:36:59 @agent_ppo2.py:185][0m |          -0.0120 |          30.8831 |          12.0032 |
[32m[20221213 22:36:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.86
[32m[20221213 22:37:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.05
[32m[20221213 22:37:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 206.25
[32m[20221213 22:37:00 @agent_ppo2.py:143][0m Total time:      18.78 min
[32m[20221213 22:37:00 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221213 22:37:00 @agent_ppo2.py:121][0m #------------------------ Iteration 892 --------------------------#
[32m[20221213 22:37:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |           0.0015 |          35.4705 |          12.1451 |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |          -0.0025 |          32.3035 |          12.1359 |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |          -0.0044 |          31.2457 |          12.1432 |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |          -0.0053 |          30.6873 |          12.1415 |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |          -0.0060 |          30.1286 |          12.1333 |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |          -0.0082 |          29.8750 |          12.1429 |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |          -0.0089 |          29.4886 |          12.1375 |
[32m[20221213 22:37:00 @agent_ppo2.py:185][0m |          -0.0089 |          29.2702 |          12.1407 |
[32m[20221213 22:37:01 @agent_ppo2.py:185][0m |          -0.0115 |          29.0461 |          12.1442 |
[32m[20221213 22:37:01 @agent_ppo2.py:185][0m |          -0.0089 |          29.0137 |          12.1402 |
[32m[20221213 22:37:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:37:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.59
[32m[20221213 22:37:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.46
[32m[20221213 22:37:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.79
[32m[20221213 22:37:01 @agent_ppo2.py:143][0m Total time:      18.80 min
[32m[20221213 22:37:01 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221213 22:37:01 @agent_ppo2.py:121][0m #------------------------ Iteration 893 --------------------------#
[32m[20221213 22:37:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:37:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:01 @agent_ppo2.py:185][0m |          -0.0026 |          36.1864 |          12.0931 |
[32m[20221213 22:37:01 @agent_ppo2.py:185][0m |          -0.0037 |          33.7319 |          12.0859 |
[32m[20221213 22:37:01 @agent_ppo2.py:185][0m |          -0.0046 |          33.1512 |          12.0689 |
[32m[20221213 22:37:01 @agent_ppo2.py:185][0m |          -0.0110 |          32.5918 |          12.0748 |
[32m[20221213 22:37:01 @agent_ppo2.py:185][0m |          -0.0082 |          32.2517 |          12.0757 |
[32m[20221213 22:37:02 @agent_ppo2.py:185][0m |          -0.0096 |          32.0031 |          12.0732 |
[32m[20221213 22:37:02 @agent_ppo2.py:185][0m |          -0.0058 |          32.1559 |          12.0725 |
[32m[20221213 22:37:02 @agent_ppo2.py:185][0m |          -0.0046 |          32.6570 |          12.0603 |
[32m[20221213 22:37:02 @agent_ppo2.py:185][0m |          -0.0133 |          31.6914 |          12.0510 |
[32m[20221213 22:37:02 @agent_ppo2.py:185][0m |          -0.0126 |          31.5335 |          12.0571 |
[32m[20221213 22:37:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:37:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.56
[32m[20221213 22:37:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.97
[32m[20221213 22:37:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.89
[32m[20221213 22:37:02 @agent_ppo2.py:143][0m Total time:      18.82 min
[32m[20221213 22:37:02 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221213 22:37:02 @agent_ppo2.py:121][0m #------------------------ Iteration 894 --------------------------#
[32m[20221213 22:37:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:37:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:02 @agent_ppo2.py:185][0m |          -0.0050 |          31.6692 |          11.9381 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0059 |          30.2123 |          11.9297 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0061 |          29.7212 |          11.9167 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0096 |          29.4677 |          11.9256 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0090 |          29.4787 |          11.9124 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0096 |          28.9945 |          11.9099 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0103 |          28.9583 |          11.9154 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0034 |          29.6636 |          11.9128 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0114 |          28.9010 |          11.9148 |
[32m[20221213 22:37:03 @agent_ppo2.py:185][0m |          -0.0117 |          28.5809 |          11.9202 |
[32m[20221213 22:37:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:37:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.03
[32m[20221213 22:37:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.62
[32m[20221213 22:37:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.00
[32m[20221213 22:37:03 @agent_ppo2.py:143][0m Total time:      18.84 min
[32m[20221213 22:37:03 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221213 22:37:03 @agent_ppo2.py:121][0m #------------------------ Iteration 895 --------------------------#
[32m[20221213 22:37:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0011 |          38.2991 |          11.9160 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0095 |          34.8738 |          11.9020 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0088 |          33.6915 |          11.8953 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0098 |          32.9638 |          11.8935 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0096 |          32.4657 |          11.8990 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0108 |          32.0853 |          11.8908 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0120 |          31.8042 |          11.8907 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0113 |          31.5882 |          11.8915 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0008 |          33.0219 |          11.8896 |
[32m[20221213 22:37:04 @agent_ppo2.py:185][0m |          -0.0132 |          31.2956 |          11.8939 |
[32m[20221213 22:37:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.24
[32m[20221213 22:37:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.62
[32m[20221213 22:37:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 208.91
[32m[20221213 22:37:05 @agent_ppo2.py:143][0m Total time:      18.86 min
[32m[20221213 22:37:05 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221213 22:37:05 @agent_ppo2.py:121][0m #------------------------ Iteration 896 --------------------------#
[32m[20221213 22:37:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:05 @agent_ppo2.py:185][0m |          -0.0010 |          28.8865 |          11.8329 |
[32m[20221213 22:37:05 @agent_ppo2.py:185][0m |           0.0072 |          27.8492 |          11.8224 |
[32m[20221213 22:37:05 @agent_ppo2.py:185][0m |          -0.0036 |          24.7381 |          11.8131 |
[32m[20221213 22:37:05 @agent_ppo2.py:185][0m |          -0.0059 |          24.5919 |          11.8243 |
[32m[20221213 22:37:05 @agent_ppo2.py:185][0m |          -0.0044 |          24.1987 |          11.8202 |
[32m[20221213 22:37:05 @agent_ppo2.py:185][0m |          -0.0091 |          23.7831 |          11.8213 |
[32m[20221213 22:37:05 @agent_ppo2.py:185][0m |          -0.0098 |          23.5804 |          11.8124 |
[32m[20221213 22:37:06 @agent_ppo2.py:185][0m |          -0.0020 |          23.5338 |          11.8212 |
[32m[20221213 22:37:06 @agent_ppo2.py:185][0m |          -0.0117 |          23.2049 |          11.8173 |
[32m[20221213 22:37:06 @agent_ppo2.py:185][0m |          -0.0080 |          23.0993 |          11.8187 |
[32m[20221213 22:37:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:37:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.99
[32m[20221213 22:37:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.92
[32m[20221213 22:37:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.00
[32m[20221213 22:37:06 @agent_ppo2.py:143][0m Total time:      18.88 min
[32m[20221213 22:37:06 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221213 22:37:06 @agent_ppo2.py:121][0m #------------------------ Iteration 897 --------------------------#
[32m[20221213 22:37:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:06 @agent_ppo2.py:185][0m |          -0.0030 |          30.3987 |          12.2230 |
[32m[20221213 22:37:06 @agent_ppo2.py:185][0m |          -0.0038 |          28.2754 |          12.2084 |
[32m[20221213 22:37:06 @agent_ppo2.py:185][0m |          -0.0063 |          27.4335 |          12.1925 |
[32m[20221213 22:37:06 @agent_ppo2.py:185][0m |           0.0011 |          31.4909 |          12.1914 |
[32m[20221213 22:37:07 @agent_ppo2.py:185][0m |          -0.0075 |          26.7393 |          12.1832 |
[32m[20221213 22:37:07 @agent_ppo2.py:185][0m |          -0.0023 |          26.7712 |          12.1855 |
[32m[20221213 22:37:07 @agent_ppo2.py:185][0m |          -0.0119 |          25.9407 |          12.1955 |
[32m[20221213 22:37:07 @agent_ppo2.py:185][0m |          -0.0118 |          25.9155 |          12.1877 |
[32m[20221213 22:37:07 @agent_ppo2.py:185][0m |          -0.0057 |          25.9548 |          12.1865 |
[32m[20221213 22:37:07 @agent_ppo2.py:185][0m |          -0.0049 |          25.5336 |          12.1875 |
[32m[20221213 22:37:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.60
[32m[20221213 22:37:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 357.39
[32m[20221213 22:37:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.32
[32m[20221213 22:37:07 @agent_ppo2.py:143][0m Total time:      18.90 min
[32m[20221213 22:37:07 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221213 22:37:07 @agent_ppo2.py:121][0m #------------------------ Iteration 898 --------------------------#
[32m[20221213 22:37:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:07 @agent_ppo2.py:185][0m |           0.0036 |          29.1613 |          12.0850 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0028 |          26.6793 |          12.0686 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0063 |          25.7568 |          12.0653 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0064 |          25.1985 |          12.0629 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0035 |          24.7077 |          12.0590 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0094 |          24.3364 |          12.0580 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0091 |          23.9623 |          12.0552 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0044 |          24.8749 |          12.0637 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0102 |          23.6918 |          12.0627 |
[32m[20221213 22:37:08 @agent_ppo2.py:185][0m |          -0.0077 |          23.6155 |          12.0670 |
[32m[20221213 22:37:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:37:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.53
[32m[20221213 22:37:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.14
[32m[20221213 22:37:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.27
[32m[20221213 22:37:08 @agent_ppo2.py:143][0m Total time:      18.92 min
[32m[20221213 22:37:08 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221213 22:37:08 @agent_ppo2.py:121][0m #------------------------ Iteration 899 --------------------------#
[32m[20221213 22:37:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |           0.0110 |          36.3547 |          12.0577 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0027 |          30.9717 |          12.0396 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0135 |          29.1540 |          12.0286 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0053 |          28.8378 |          12.0256 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0116 |          27.5942 |          12.0114 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0122 |          27.2446 |          12.0162 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0176 |          27.0627 |          12.0245 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0118 |          26.7683 |          12.0268 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0147 |          26.5844 |          12.0163 |
[32m[20221213 22:37:09 @agent_ppo2.py:185][0m |          -0.0174 |          26.4494 |          12.0225 |
[32m[20221213 22:37:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.78
[32m[20221213 22:37:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.60
[32m[20221213 22:37:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.50
[32m[20221213 22:37:10 @agent_ppo2.py:143][0m Total time:      18.94 min
[32m[20221213 22:37:10 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221213 22:37:10 @agent_ppo2.py:121][0m #------------------------ Iteration 900 --------------------------#
[32m[20221213 22:37:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:37:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0016 |          35.5584 |          12.0796 |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0054 |          33.7010 |          12.0697 |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0074 |          33.2307 |          12.0635 |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0083 |          33.0647 |          12.0611 |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0069 |          32.9254 |          12.0587 |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0083 |          32.9085 |          12.0554 |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0120 |          32.7799 |          12.0557 |
[32m[20221213 22:37:10 @agent_ppo2.py:185][0m |          -0.0111 |          32.7706 |          12.0443 |
[32m[20221213 22:37:11 @agent_ppo2.py:185][0m |          -0.0083 |          32.6548 |          12.0525 |
[32m[20221213 22:37:11 @agent_ppo2.py:185][0m |          -0.0089 |          32.5367 |          12.0539 |
[32m[20221213 22:37:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.07
[32m[20221213 22:37:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.62
[32m[20221213 22:37:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.32
[32m[20221213 22:37:11 @agent_ppo2.py:143][0m Total time:      18.96 min
[32m[20221213 22:37:11 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221213 22:37:11 @agent_ppo2.py:121][0m #------------------------ Iteration 901 --------------------------#
[32m[20221213 22:37:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:11 @agent_ppo2.py:185][0m |          -0.0048 |          32.9030 |          12.2071 |
[32m[20221213 22:37:11 @agent_ppo2.py:185][0m |          -0.0128 |          30.0956 |          12.1941 |
[32m[20221213 22:37:11 @agent_ppo2.py:185][0m |          -0.0080 |          28.7892 |          12.2005 |
[32m[20221213 22:37:11 @agent_ppo2.py:185][0m |          -0.0128 |          28.1809 |          12.1872 |
[32m[20221213 22:37:11 @agent_ppo2.py:185][0m |          -0.0124 |          27.6288 |          12.1922 |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |          -0.0115 |          27.2879 |          12.1900 |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |          -0.0105 |          26.9269 |          12.1872 |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |          -0.0131 |          26.7421 |          12.1908 |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |          -0.0141 |          26.4429 |          12.2037 |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |          -0.0148 |          26.3176 |          12.1987 |
[32m[20221213 22:37:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.05
[32m[20221213 22:37:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.20
[32m[20221213 22:37:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.90
[32m[20221213 22:37:12 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221213 22:37:12 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221213 22:37:12 @agent_ppo2.py:121][0m #------------------------ Iteration 902 --------------------------#
[32m[20221213 22:37:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |           0.0019 |          31.7747 |          12.1999 |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |          -0.0082 |          29.1982 |          12.1873 |
[32m[20221213 22:37:12 @agent_ppo2.py:185][0m |          -0.0068 |          27.7822 |          12.1919 |
[32m[20221213 22:37:13 @agent_ppo2.py:185][0m |          -0.0090 |          27.0710 |          12.1981 |
[32m[20221213 22:37:13 @agent_ppo2.py:185][0m |          -0.0029 |          27.2989 |          12.1863 |
[32m[20221213 22:37:13 @agent_ppo2.py:185][0m |          -0.0130 |          26.0506 |          12.1848 |
[32m[20221213 22:37:13 @agent_ppo2.py:185][0m |          -0.0083 |          25.8630 |          12.1908 |
[32m[20221213 22:37:13 @agent_ppo2.py:185][0m |          -0.0116 |          25.4417 |          12.1935 |
[32m[20221213 22:37:13 @agent_ppo2.py:185][0m |          -0.0122 |          25.1014 |          12.1984 |
[32m[20221213 22:37:13 @agent_ppo2.py:185][0m |          -0.0126 |          24.8198 |          12.1954 |
[32m[20221213 22:37:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.05
[32m[20221213 22:37:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.46
[32m[20221213 22:37:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.74
[32m[20221213 22:37:13 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221213 22:37:13 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221213 22:37:13 @agent_ppo2.py:121][0m #------------------------ Iteration 903 --------------------------#
[32m[20221213 22:37:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0017 |          25.3244 |          12.3659 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0029 |          19.6487 |          12.3546 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0006 |          18.6580 |          12.3326 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |           0.0063 |          19.5500 |          12.3220 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0037 |          17.7280 |          12.3202 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0045 |          18.3985 |          12.3243 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0073 |          17.1152 |          12.3159 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0102 |          16.9803 |          12.3178 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0121 |          16.8694 |          12.3189 |
[32m[20221213 22:37:14 @agent_ppo2.py:185][0m |          -0.0064 |          16.5668 |          12.3153 |
[32m[20221213 22:37:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:37:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.51
[32m[20221213 22:37:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.64
[32m[20221213 22:37:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.84
[32m[20221213 22:37:14 @agent_ppo2.py:143][0m Total time:      19.03 min
[32m[20221213 22:37:14 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221213 22:37:14 @agent_ppo2.py:121][0m #------------------------ Iteration 904 --------------------------#
[32m[20221213 22:37:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:37:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |           0.0029 |          21.5716 |          12.1895 |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |          -0.0075 |          19.3220 |          12.1697 |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |          -0.0122 |          18.6039 |          12.1606 |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |          -0.0103 |          18.1329 |          12.1474 |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |          -0.0067 |          17.7998 |          12.1550 |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |           0.0004 |          18.2591 |          12.1364 |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |          -0.0169 |          17.0205 |          12.1327 |
[32m[20221213 22:37:15 @agent_ppo2.py:185][0m |          -0.0134 |          16.7941 |          12.1357 |
[32m[20221213 22:37:16 @agent_ppo2.py:185][0m |          -0.0156 |          16.5653 |          12.1386 |
[32m[20221213 22:37:16 @agent_ppo2.py:185][0m |          -0.0115 |          16.4161 |          12.1300 |
[32m[20221213 22:37:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:37:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.95
[32m[20221213 22:37:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.44
[32m[20221213 22:37:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.43
[32m[20221213 22:37:16 @agent_ppo2.py:143][0m Total time:      19.05 min
[32m[20221213 22:37:16 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221213 22:37:16 @agent_ppo2.py:121][0m #------------------------ Iteration 905 --------------------------#
[32m[20221213 22:37:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:16 @agent_ppo2.py:185][0m |           0.0001 |          33.7767 |          12.2349 |
[32m[20221213 22:37:16 @agent_ppo2.py:185][0m |          -0.0044 |          31.5628 |          12.2015 |
[32m[20221213 22:37:16 @agent_ppo2.py:185][0m |          -0.0064 |          30.7505 |          12.2007 |
[32m[20221213 22:37:16 @agent_ppo2.py:185][0m |          -0.0090 |          30.2031 |          12.1930 |
[32m[20221213 22:37:16 @agent_ppo2.py:185][0m |          -0.0088 |          29.9044 |          12.1931 |
[32m[20221213 22:37:17 @agent_ppo2.py:185][0m |          -0.0102 |          29.6267 |          12.1822 |
[32m[20221213 22:37:17 @agent_ppo2.py:185][0m |          -0.0069 |          29.4940 |          12.1790 |
[32m[20221213 22:37:17 @agent_ppo2.py:185][0m |          -0.0077 |          29.4416 |          12.1789 |
[32m[20221213 22:37:17 @agent_ppo2.py:185][0m |          -0.0070 |          29.6756 |          12.1757 |
[32m[20221213 22:37:17 @agent_ppo2.py:185][0m |          -0.0088 |          29.2657 |          12.1757 |
[32m[20221213 22:37:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:37:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.90
[32m[20221213 22:37:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.19
[32m[20221213 22:37:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.78
[32m[20221213 22:37:17 @agent_ppo2.py:143][0m Total time:      19.07 min
[32m[20221213 22:37:17 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221213 22:37:17 @agent_ppo2.py:121][0m #------------------------ Iteration 906 --------------------------#
[32m[20221213 22:37:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:37:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:17 @agent_ppo2.py:185][0m |           0.0005 |          14.7694 |          12.2268 |
[32m[20221213 22:37:17 @agent_ppo2.py:185][0m |          -0.0091 |          12.4851 |          12.2186 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0031 |          11.7310 |          12.2081 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0111 |          11.1433 |          12.2127 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0087 |          10.8105 |          12.2079 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0118 |          10.3850 |          12.2028 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0099 |          10.1304 |          12.2059 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0159 |           9.7947 |          12.2019 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0176 |           9.5966 |          12.2100 |
[32m[20221213 22:37:18 @agent_ppo2.py:185][0m |          -0.0173 |           9.3261 |          12.1994 |
[32m[20221213 22:37:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:37:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.10
[32m[20221213 22:37:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.33
[32m[20221213 22:37:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.29
[32m[20221213 22:37:18 @agent_ppo2.py:143][0m Total time:      19.09 min
[32m[20221213 22:37:18 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221213 22:37:18 @agent_ppo2.py:121][0m #------------------------ Iteration 907 --------------------------#
[32m[20221213 22:37:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0001 |          35.0482 |          12.1832 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0027 |          34.0722 |          12.1664 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0030 |          33.3993 |          12.1564 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0067 |          33.0918 |          12.1573 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0088 |          32.9991 |          12.1583 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0126 |          32.8069 |          12.1550 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0104 |          32.6608 |          12.1550 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0088 |          32.5884 |          12.1610 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0081 |          32.5747 |          12.1543 |
[32m[20221213 22:37:19 @agent_ppo2.py:185][0m |          -0.0117 |          32.5379 |          12.1577 |
[32m[20221213 22:37:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.06
[32m[20221213 22:37:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.42
[32m[20221213 22:37:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.78
[32m[20221213 22:37:20 @agent_ppo2.py:143][0m Total time:      19.11 min
[32m[20221213 22:37:20 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221213 22:37:20 @agent_ppo2.py:121][0m #------------------------ Iteration 908 --------------------------#
[32m[20221213 22:37:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0055 |          30.7860 |          12.1407 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0106 |          28.1615 |          12.1232 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0053 |          27.4344 |          12.1192 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0160 |          26.8933 |          12.1080 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0134 |          26.4392 |          12.1011 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0135 |          26.1233 |          12.1008 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0070 |          26.0633 |          12.0958 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0106 |          25.6842 |          12.1039 |
[32m[20221213 22:37:20 @agent_ppo2.py:185][0m |          -0.0158 |          25.3851 |          12.0923 |
[32m[20221213 22:37:21 @agent_ppo2.py:185][0m |           0.0104 |          30.4444 |          12.0983 |
[32m[20221213 22:37:21 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.55
[32m[20221213 22:37:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.49
[32m[20221213 22:37:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.70
[32m[20221213 22:37:21 @agent_ppo2.py:143][0m Total time:      19.13 min
[32m[20221213 22:37:21 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221213 22:37:21 @agent_ppo2.py:121][0m #------------------------ Iteration 909 --------------------------#
[32m[20221213 22:37:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:21 @agent_ppo2.py:185][0m |           0.0060 |          34.4227 |          12.3976 |
[32m[20221213 22:37:21 @agent_ppo2.py:185][0m |          -0.0032 |          31.7783 |          12.3799 |
[32m[20221213 22:37:21 @agent_ppo2.py:185][0m |          -0.0069 |          31.0008 |          12.3687 |
[32m[20221213 22:37:21 @agent_ppo2.py:185][0m |          -0.0045 |          30.6387 |          12.3632 |
[32m[20221213 22:37:21 @agent_ppo2.py:185][0m |          -0.0019 |          30.3352 |          12.3622 |
[32m[20221213 22:37:21 @agent_ppo2.py:185][0m |          -0.0086 |          29.6411 |          12.3618 |
[32m[20221213 22:37:22 @agent_ppo2.py:185][0m |          -0.0066 |          29.7112 |          12.3717 |
[32m[20221213 22:37:22 @agent_ppo2.py:185][0m |          -0.0098 |          29.1548 |          12.3711 |
[32m[20221213 22:37:22 @agent_ppo2.py:185][0m |          -0.0012 |          30.2033 |          12.3667 |
[32m[20221213 22:37:22 @agent_ppo2.py:185][0m |          -0.0049 |          28.8380 |          12.3605 |
[32m[20221213 22:37:22 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.75
[32m[20221213 22:37:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.95
[32m[20221213 22:37:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.17
[32m[20221213 22:37:22 @agent_ppo2.py:143][0m Total time:      19.15 min
[32m[20221213 22:37:22 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221213 22:37:22 @agent_ppo2.py:121][0m #------------------------ Iteration 910 --------------------------#
[32m[20221213 22:37:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:37:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:22 @agent_ppo2.py:185][0m |          -0.0003 |          24.5908 |          12.3684 |
[32m[20221213 22:37:22 @agent_ppo2.py:185][0m |          -0.0012 |          22.1006 |          12.3623 |
[32m[20221213 22:37:22 @agent_ppo2.py:185][0m |          -0.0075 |          21.2529 |          12.3557 |
[32m[20221213 22:37:23 @agent_ppo2.py:185][0m |          -0.0062 |          20.5963 |          12.3621 |
[32m[20221213 22:37:23 @agent_ppo2.py:185][0m |          -0.0082 |          20.2335 |          12.3492 |
[32m[20221213 22:37:23 @agent_ppo2.py:185][0m |          -0.0118 |          20.0087 |          12.3456 |
[32m[20221213 22:37:23 @agent_ppo2.py:185][0m |          -0.0142 |          19.7369 |          12.3468 |
[32m[20221213 22:37:23 @agent_ppo2.py:185][0m |          -0.0099 |          19.5454 |          12.3456 |
[32m[20221213 22:37:23 @agent_ppo2.py:185][0m |          -0.0117 |          19.3371 |          12.3467 |
[32m[20221213 22:37:23 @agent_ppo2.py:185][0m |          -0.0136 |          19.2178 |          12.3470 |
[32m[20221213 22:37:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:37:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.22
[32m[20221213 22:37:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.53
[32m[20221213 22:37:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.75
[32m[20221213 22:37:23 @agent_ppo2.py:143][0m Total time:      19.17 min
[32m[20221213 22:37:23 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221213 22:37:23 @agent_ppo2.py:121][0m #------------------------ Iteration 911 --------------------------#
[32m[20221213 22:37:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0002 |          21.5478 |          12.2362 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0045 |          18.3464 |          12.2292 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0000 |          17.4901 |          12.2294 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0109 |          16.7661 |          12.2220 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0086 |          16.3907 |          12.2193 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0151 |          16.0604 |          12.2309 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0252 |          15.8277 |          12.2261 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0125 |          15.3790 |          12.2241 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0138 |          15.1675 |          12.2240 |
[32m[20221213 22:37:24 @agent_ppo2.py:185][0m |          -0.0121 |          14.8711 |          12.2254 |
[32m[20221213 22:37:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.29
[32m[20221213 22:37:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.86
[32m[20221213 22:37:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.76
[32m[20221213 22:37:24 @agent_ppo2.py:143][0m Total time:      19.19 min
[32m[20221213 22:37:24 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221213 22:37:24 @agent_ppo2.py:121][0m #------------------------ Iteration 912 --------------------------#
[32m[20221213 22:37:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0010 |          25.2285 |          12.3836 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0036 |          23.9066 |          12.3668 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |           0.0090 |          24.5565 |          12.3602 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0104 |          22.8669 |          12.3538 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0088 |          22.4957 |          12.3555 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0085 |          22.3307 |          12.3439 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0125 |          22.1415 |          12.3464 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0099 |          22.0002 |          12.3464 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0106 |          21.8527 |          12.3407 |
[32m[20221213 22:37:25 @agent_ppo2.py:185][0m |          -0.0106 |          21.8099 |          12.3354 |
[32m[20221213 22:37:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.03
[32m[20221213 22:37:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.94
[32m[20221213 22:37:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.34
[32m[20221213 22:37:26 @agent_ppo2.py:143][0m Total time:      19.21 min
[32m[20221213 22:37:26 @agent_ppo2.py:145][0m 1869824 total steps have happened
[32m[20221213 22:37:26 @agent_ppo2.py:121][0m #------------------------ Iteration 913 --------------------------#
[32m[20221213 22:37:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0013 |          30.8621 |          12.2361 |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0011 |          29.8688 |          12.2226 |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0093 |          27.8912 |          12.2083 |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0100 |          27.2548 |          12.2320 |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0119 |          26.8250 |          12.2231 |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0148 |          26.6589 |          12.2217 |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0142 |          26.0750 |          12.2237 |
[32m[20221213 22:37:26 @agent_ppo2.py:185][0m |          -0.0136 |          25.8658 |          12.2210 |
[32m[20221213 22:37:27 @agent_ppo2.py:185][0m |          -0.0073 |          26.0229 |          12.2257 |
[32m[20221213 22:37:27 @agent_ppo2.py:185][0m |          -0.0126 |          25.4212 |          12.2144 |
[32m[20221213 22:37:27 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.44
[32m[20221213 22:37:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 342.99
[32m[20221213 22:37:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.89
[32m[20221213 22:37:27 @agent_ppo2.py:143][0m Total time:      19.23 min
[32m[20221213 22:37:27 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221213 22:37:27 @agent_ppo2.py:121][0m #------------------------ Iteration 914 --------------------------#
[32m[20221213 22:37:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:27 @agent_ppo2.py:185][0m |          -0.0033 |          33.0016 |          12.2984 |
[32m[20221213 22:37:27 @agent_ppo2.py:185][0m |          -0.0081 |          29.0819 |          12.2904 |
[32m[20221213 22:37:27 @agent_ppo2.py:185][0m |          -0.0083 |          27.5273 |          12.2945 |
[32m[20221213 22:37:27 @agent_ppo2.py:185][0m |          -0.0054 |          26.8787 |          12.2924 |
[32m[20221213 22:37:27 @agent_ppo2.py:185][0m |          -0.0087 |          25.7281 |          12.2888 |
[32m[20221213 22:37:28 @agent_ppo2.py:185][0m |          -0.0115 |          25.1002 |          12.3012 |
[32m[20221213 22:37:28 @agent_ppo2.py:185][0m |          -0.0073 |          25.1530 |          12.2961 |
[32m[20221213 22:37:28 @agent_ppo2.py:185][0m |          -0.0069 |          25.0779 |          12.2932 |
[32m[20221213 22:37:28 @agent_ppo2.py:185][0m |          -0.0128 |          23.9438 |          12.2924 |
[32m[20221213 22:37:28 @agent_ppo2.py:185][0m |          -0.0144 |          23.6415 |          12.2978 |
[32m[20221213 22:37:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 140.17
[32m[20221213 22:37:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.53
[32m[20221213 22:37:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.96
[32m[20221213 22:37:28 @agent_ppo2.py:143][0m Total time:      19.25 min
[32m[20221213 22:37:28 @agent_ppo2.py:145][0m 1873920 total steps have happened
[32m[20221213 22:37:28 @agent_ppo2.py:121][0m #------------------------ Iteration 915 --------------------------#
[32m[20221213 22:37:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:28 @agent_ppo2.py:185][0m |           0.0059 |          36.9442 |          12.3168 |
[32m[20221213 22:37:28 @agent_ppo2.py:185][0m |          -0.0060 |          31.1043 |          12.2962 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |          -0.0089 |          30.0132 |          12.2965 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |           0.0019 |          29.4783 |          12.3060 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |          -0.0075 |          28.8021 |          12.2965 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |          -0.0129 |          28.3495 |          12.2973 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |          -0.0116 |          28.0329 |          12.2959 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |          -0.0123 |          27.7924 |          12.2879 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |          -0.0139 |          27.6718 |          12.2982 |
[32m[20221213 22:37:29 @agent_ppo2.py:185][0m |          -0.0144 |          27.3377 |          12.2945 |
[32m[20221213 22:37:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.35
[32m[20221213 22:37:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.21
[32m[20221213 22:37:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.31
[32m[20221213 22:37:29 @agent_ppo2.py:143][0m Total time:      19.27 min
[32m[20221213 22:37:29 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221213 22:37:29 @agent_ppo2.py:121][0m #------------------------ Iteration 916 --------------------------#
[32m[20221213 22:37:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |           0.0020 |          26.5534 |          12.3865 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |           0.0007 |          23.3438 |          12.3640 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0055 |          22.3288 |          12.3660 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0086 |          21.8172 |          12.3499 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0054 |          21.3897 |          12.3536 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0077 |          21.2673 |          12.3481 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0018 |          21.4885 |          12.3478 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0063 |          20.5929 |          12.3493 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0097 |          20.2736 |          12.3534 |
[32m[20221213 22:37:30 @agent_ppo2.py:185][0m |          -0.0110 |          20.3061 |          12.3395 |
[32m[20221213 22:37:30 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.72
[32m[20221213 22:37:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.39
[32m[20221213 22:37:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.61
[32m[20221213 22:37:30 @agent_ppo2.py:143][0m Total time:      19.29 min
[32m[20221213 22:37:30 @agent_ppo2.py:145][0m 1878016 total steps have happened
[32m[20221213 22:37:30 @agent_ppo2.py:121][0m #------------------------ Iteration 917 --------------------------#
[32m[20221213 22:37:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0012 |          28.2078 |          12.5316 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0007 |          23.0476 |          12.5213 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0042 |          21.5362 |          12.5203 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0057 |          20.9227 |          12.5220 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0091 |          19.7934 |          12.5246 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0053 |          19.1760 |          12.5214 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0105 |          18.6046 |          12.5191 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |           0.0019 |          18.7434 |          12.5246 |
[32m[20221213 22:37:31 @agent_ppo2.py:185][0m |          -0.0082 |          17.8758 |          12.5121 |
[32m[20221213 22:37:32 @agent_ppo2.py:185][0m |           0.0043 |          19.0110 |          12.5133 |
[32m[20221213 22:37:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.98
[32m[20221213 22:37:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.60
[32m[20221213 22:37:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.91
[32m[20221213 22:37:32 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 430.91
[32m[20221213 22:37:32 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 430.91
[32m[20221213 22:37:32 @agent_ppo2.py:143][0m Total time:      19.31 min
[32m[20221213 22:37:32 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221213 22:37:32 @agent_ppo2.py:121][0m #------------------------ Iteration 918 --------------------------#
[32m[20221213 22:37:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:32 @agent_ppo2.py:185][0m |          -0.0029 |          25.1917 |          12.4185 |
[32m[20221213 22:37:32 @agent_ppo2.py:185][0m |          -0.0100 |          20.0981 |          12.3920 |
[32m[20221213 22:37:32 @agent_ppo2.py:185][0m |          -0.0129 |          19.0318 |          12.3981 |
[32m[20221213 22:37:32 @agent_ppo2.py:185][0m |          -0.0141 |          18.3102 |          12.4015 |
[32m[20221213 22:37:32 @agent_ppo2.py:185][0m |          -0.0145 |          17.6707 |          12.3996 |
[32m[20221213 22:37:32 @agent_ppo2.py:185][0m |          -0.0046 |          17.7254 |          12.3996 |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0137 |          17.0018 |          12.3851 |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0181 |          16.6416 |          12.3853 |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0178 |          16.2452 |          12.3940 |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0137 |          16.1307 |          12.3837 |
[32m[20221213 22:37:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:37:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 201.99
[32m[20221213 22:37:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.23
[32m[20221213 22:37:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.88
[32m[20221213 22:37:33 @agent_ppo2.py:143][0m Total time:      19.33 min
[32m[20221213 22:37:33 @agent_ppo2.py:145][0m 1882112 total steps have happened
[32m[20221213 22:37:33 @agent_ppo2.py:121][0m #------------------------ Iteration 919 --------------------------#
[32m[20221213 22:37:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0026 |          21.6766 |          12.4705 |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0025 |          18.6788 |          12.4585 |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0066 |          17.3903 |          12.4460 |
[32m[20221213 22:37:33 @agent_ppo2.py:185][0m |          -0.0100 |          16.4911 |          12.4480 |
[32m[20221213 22:37:34 @agent_ppo2.py:185][0m |          -0.0113 |          15.9566 |          12.4429 |
[32m[20221213 22:37:34 @agent_ppo2.py:185][0m |          -0.0109 |          15.5061 |          12.4381 |
[32m[20221213 22:37:34 @agent_ppo2.py:185][0m |          -0.0055 |          15.0727 |          12.4341 |
[32m[20221213 22:37:34 @agent_ppo2.py:185][0m |          -0.0103 |          15.1798 |          12.4434 |
[32m[20221213 22:37:34 @agent_ppo2.py:185][0m |          -0.0199 |          14.7255 |          12.4368 |
[32m[20221213 22:37:34 @agent_ppo2.py:185][0m |          -0.0155 |          14.1546 |          12.4404 |
[32m[20221213 22:37:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.10
[32m[20221213 22:37:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.05
[32m[20221213 22:37:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.59
[32m[20221213 22:37:34 @agent_ppo2.py:143][0m Total time:      19.35 min
[32m[20221213 22:37:34 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221213 22:37:34 @agent_ppo2.py:121][0m #------------------------ Iteration 920 --------------------------#
[32m[20221213 22:37:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:37:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:34 @agent_ppo2.py:185][0m |          -0.0008 |          30.5377 |          12.4472 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |          -0.0052 |          28.2582 |          12.4368 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |          -0.0045 |          27.2542 |          12.4384 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |          -0.0104 |          26.6697 |          12.4371 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |           0.0024 |          28.1405 |          12.4422 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |          -0.0048 |          25.8438 |          12.4352 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |          -0.0070 |          24.9668 |          12.4376 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |          -0.0114 |          24.6050 |          12.4449 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |          -0.0035 |          25.3953 |          12.4310 |
[32m[20221213 22:37:35 @agent_ppo2.py:185][0m |           0.0002 |          25.5579 |          12.4378 |
[32m[20221213 22:37:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.22
[32m[20221213 22:37:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.14
[32m[20221213 22:37:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.06
[32m[20221213 22:37:35 @agent_ppo2.py:143][0m Total time:      19.37 min
[32m[20221213 22:37:35 @agent_ppo2.py:145][0m 1886208 total steps have happened
[32m[20221213 22:37:35 @agent_ppo2.py:121][0m #------------------------ Iteration 921 --------------------------#
[32m[20221213 22:37:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |           0.0187 |          34.4326 |          12.5912 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0042 |          29.0143 |          12.5772 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |           0.0052 |          29.2098 |          12.5751 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0094 |          27.1497 |          12.5798 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0070 |          26.3992 |          12.5737 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0105 |          25.8977 |          12.5726 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0077 |          25.5532 |          12.5783 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0103 |          25.1872 |          12.5755 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0119 |          24.8337 |          12.5757 |
[32m[20221213 22:37:36 @agent_ppo2.py:185][0m |          -0.0101 |          24.6336 |          12.5613 |
[32m[20221213 22:37:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.08
[32m[20221213 22:37:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.83
[32m[20221213 22:37:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.52
[32m[20221213 22:37:37 @agent_ppo2.py:143][0m Total time:      19.39 min
[32m[20221213 22:37:37 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221213 22:37:37 @agent_ppo2.py:121][0m #------------------------ Iteration 922 --------------------------#
[32m[20221213 22:37:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |          -0.0020 |          29.0135 |          12.5099 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |          -0.0033 |          25.2777 |          12.5073 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |          -0.0057 |          23.9471 |          12.5035 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |           0.0005 |          25.3399 |          12.5048 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |          -0.0092 |          23.0145 |          12.5058 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |          -0.0131 |          22.2655 |          12.4965 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |           0.0012 |          24.5921 |          12.4956 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |          -0.0076 |          21.5893 |          12.5009 |
[32m[20221213 22:37:37 @agent_ppo2.py:185][0m |          -0.0031 |          21.3168 |          12.5091 |
[32m[20221213 22:37:38 @agent_ppo2.py:185][0m |          -0.0108 |          21.1095 |          12.5067 |
[32m[20221213 22:37:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.57
[32m[20221213 22:37:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.87
[32m[20221213 22:37:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.40
[32m[20221213 22:37:38 @agent_ppo2.py:143][0m Total time:      19.41 min
[32m[20221213 22:37:38 @agent_ppo2.py:145][0m 1890304 total steps have happened
[32m[20221213 22:37:38 @agent_ppo2.py:121][0m #------------------------ Iteration 923 --------------------------#
[32m[20221213 22:37:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:38 @agent_ppo2.py:185][0m |           0.0089 |          30.2417 |          12.5577 |
[32m[20221213 22:37:38 @agent_ppo2.py:185][0m |          -0.0086 |          23.3081 |          12.5265 |
[32m[20221213 22:37:38 @agent_ppo2.py:185][0m |          -0.0069 |          22.1054 |          12.5304 |
[32m[20221213 22:37:38 @agent_ppo2.py:185][0m |          -0.0102 |          21.5340 |          12.5231 |
[32m[20221213 22:37:38 @agent_ppo2.py:185][0m |          -0.0103 |          21.0673 |          12.5271 |
[32m[20221213 22:37:38 @agent_ppo2.py:185][0m |          -0.0097 |          20.7119 |          12.5236 |
[32m[20221213 22:37:39 @agent_ppo2.py:185][0m |          -0.0148 |          20.2344 |          12.5268 |
[32m[20221213 22:37:39 @agent_ppo2.py:185][0m |          -0.0126 |          20.0568 |          12.5287 |
[32m[20221213 22:37:39 @agent_ppo2.py:185][0m |          -0.0165 |          19.9660 |          12.5280 |
[32m[20221213 22:37:39 @agent_ppo2.py:185][0m |          -0.0142 |          19.6755 |          12.5262 |
[32m[20221213 22:37:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.00
[32m[20221213 22:37:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.62
[32m[20221213 22:37:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.93
[32m[20221213 22:37:39 @agent_ppo2.py:143][0m Total time:      19.43 min
[32m[20221213 22:37:39 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221213 22:37:39 @agent_ppo2.py:121][0m #------------------------ Iteration 924 --------------------------#
[32m[20221213 22:37:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:39 @agent_ppo2.py:185][0m |           0.0003 |          22.7687 |          12.4961 |
[32m[20221213 22:37:39 @agent_ppo2.py:185][0m |          -0.0086 |          20.9236 |          12.4810 |
[32m[20221213 22:37:39 @agent_ppo2.py:185][0m |          -0.0003 |          20.4535 |          12.4904 |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |          -0.0043 |          20.1493 |          12.4779 |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |          -0.0068 |          19.9882 |          12.4660 |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |          -0.0078 |          19.8437 |          12.4696 |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |          -0.0095 |          19.4051 |          12.4510 |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |          -0.0113 |          19.2305 |          12.4681 |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |          -0.0069 |          19.0535 |          12.4576 |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |          -0.0134 |          19.0715 |          12.4515 |
[32m[20221213 22:37:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.46
[32m[20221213 22:37:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.89
[32m[20221213 22:37:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.47
[32m[20221213 22:37:40 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 440.47
[32m[20221213 22:37:40 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 440.47
[32m[20221213 22:37:40 @agent_ppo2.py:143][0m Total time:      19.45 min
[32m[20221213 22:37:40 @agent_ppo2.py:145][0m 1894400 total steps have happened
[32m[20221213 22:37:40 @agent_ppo2.py:121][0m #------------------------ Iteration 925 --------------------------#
[32m[20221213 22:37:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:40 @agent_ppo2.py:185][0m |           0.0036 |          36.2426 |          12.5003 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0025 |          33.6902 |          12.4770 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0035 |          32.4590 |          12.4901 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0050 |          31.7669 |          12.4802 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0112 |          31.0624 |          12.4824 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0099 |          30.7933 |          12.4798 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0062 |          30.3629 |          12.4798 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0048 |          30.2927 |          12.4801 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0092 |          29.9859 |          12.4827 |
[32m[20221213 22:37:41 @agent_ppo2.py:185][0m |          -0.0098 |          29.7763 |          12.4689 |
[32m[20221213 22:37:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.46
[32m[20221213 22:37:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.81
[32m[20221213 22:37:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.62
[32m[20221213 22:37:41 @agent_ppo2.py:143][0m Total time:      19.47 min
[32m[20221213 22:37:41 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221213 22:37:41 @agent_ppo2.py:121][0m #------------------------ Iteration 926 --------------------------#
[32m[20221213 22:37:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0044 |          20.3963 |          12.5319 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0083 |          17.4527 |          12.5081 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |           0.0007 |          17.0400 |          12.5094 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0096 |          15.9704 |          12.4959 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0100 |          15.4062 |          12.5076 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0134 |          15.0608 |          12.5060 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0127 |          14.7658 |          12.5003 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0148 |          14.5899 |          12.5021 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0168 |          14.3166 |          12.4997 |
[32m[20221213 22:37:42 @agent_ppo2.py:185][0m |          -0.0138 |          14.2712 |          12.4978 |
[32m[20221213 22:37:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 207.90
[32m[20221213 22:37:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.95
[32m[20221213 22:37:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.60
[32m[20221213 22:37:43 @agent_ppo2.py:143][0m Total time:      19.49 min
[32m[20221213 22:37:43 @agent_ppo2.py:145][0m 1898496 total steps have happened
[32m[20221213 22:37:43 @agent_ppo2.py:121][0m #------------------------ Iteration 927 --------------------------#
[32m[20221213 22:37:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0010 |          26.3873 |          12.6409 |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0035 |          19.4132 |          12.6247 |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0068 |          17.1063 |          12.6230 |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0107 |          15.7635 |          12.6179 |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0142 |          15.0524 |          12.6103 |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0092 |          14.3523 |          12.6100 |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0112 |          13.8729 |          12.6055 |
[32m[20221213 22:37:43 @agent_ppo2.py:185][0m |          -0.0060 |          13.5838 |          12.6024 |
[32m[20221213 22:37:44 @agent_ppo2.py:185][0m |          -0.0139 |          13.1593 |          12.6140 |
[32m[20221213 22:37:44 @agent_ppo2.py:185][0m |          -0.0095 |          13.0380 |          12.6022 |
[32m[20221213 22:37:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 308.84
[32m[20221213 22:37:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.32
[32m[20221213 22:37:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 226.58
[32m[20221213 22:37:44 @agent_ppo2.py:143][0m Total time:      19.51 min
[32m[20221213 22:37:44 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221213 22:37:44 @agent_ppo2.py:121][0m #------------------------ Iteration 928 --------------------------#
[32m[20221213 22:37:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:44 @agent_ppo2.py:185][0m |           0.0034 |          30.2038 |          12.4171 |
[32m[20221213 22:37:44 @agent_ppo2.py:185][0m |           0.0033 |          20.7262 |          12.4037 |
[32m[20221213 22:37:44 @agent_ppo2.py:185][0m |           0.0018 |          19.4892 |          12.3995 |
[32m[20221213 22:37:44 @agent_ppo2.py:185][0m |          -0.0069 |          18.5066 |          12.3967 |
[32m[20221213 22:37:44 @agent_ppo2.py:185][0m |          -0.0069 |          18.0637 |          12.3925 |
[32m[20221213 22:37:45 @agent_ppo2.py:185][0m |          -0.0065 |          17.6260 |          12.3996 |
[32m[20221213 22:37:45 @agent_ppo2.py:185][0m |          -0.0113 |          17.5182 |          12.3999 |
[32m[20221213 22:37:45 @agent_ppo2.py:185][0m |          -0.0040 |          17.2639 |          12.3882 |
[32m[20221213 22:37:45 @agent_ppo2.py:185][0m |          -0.0012 |          17.1623 |          12.3822 |
[32m[20221213 22:37:45 @agent_ppo2.py:185][0m |          -0.0082 |          16.8646 |          12.3922 |
[32m[20221213 22:37:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.28
[32m[20221213 22:37:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.89
[32m[20221213 22:37:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.15
[32m[20221213 22:37:45 @agent_ppo2.py:143][0m Total time:      19.54 min
[32m[20221213 22:37:45 @agent_ppo2.py:145][0m 1902592 total steps have happened
[32m[20221213 22:37:45 @agent_ppo2.py:121][0m #------------------------ Iteration 929 --------------------------#
[32m[20221213 22:37:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:45 @agent_ppo2.py:185][0m |           0.0026 |          27.6287 |          12.5953 |
[32m[20221213 22:37:45 @agent_ppo2.py:185][0m |          -0.0099 |          24.6674 |          12.5872 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0062 |          23.9485 |          12.5864 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0108 |          23.5979 |          12.5860 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0120 |          23.1897 |          12.5782 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0116 |          23.1568 |          12.5869 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0120 |          23.0630 |          12.5910 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0129 |          22.7609 |          12.5874 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0116 |          22.7046 |          12.5897 |
[32m[20221213 22:37:46 @agent_ppo2.py:185][0m |          -0.0123 |          22.6282 |          12.5913 |
[32m[20221213 22:37:46 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:37:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.15
[32m[20221213 22:37:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.46
[32m[20221213 22:37:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.54
[32m[20221213 22:37:46 @agent_ppo2.py:143][0m Total time:      19.56 min
[32m[20221213 22:37:46 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221213 22:37:46 @agent_ppo2.py:121][0m #------------------------ Iteration 930 --------------------------#
[32m[20221213 22:37:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:37:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |           0.0053 |          21.2479 |          12.8980 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0096 |          16.9134 |          12.8890 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0080 |          15.9723 |          12.9040 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0076 |          15.5105 |          12.8889 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0092 |          15.1078 |          12.8966 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0147 |          14.9163 |          12.8894 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0139 |          14.6865 |          12.8789 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0130 |          14.4790 |          12.8908 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0134 |          14.3295 |          12.8825 |
[32m[20221213 22:37:47 @agent_ppo2.py:185][0m |          -0.0117 |          14.2098 |          12.8864 |
[32m[20221213 22:37:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 195.79
[32m[20221213 22:37:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.88
[32m[20221213 22:37:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.07
[32m[20221213 22:37:47 @agent_ppo2.py:143][0m Total time:      19.58 min
[32m[20221213 22:37:47 @agent_ppo2.py:145][0m 1906688 total steps have happened
[32m[20221213 22:37:47 @agent_ppo2.py:121][0m #------------------------ Iteration 931 --------------------------#
[32m[20221213 22:37:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |           0.0054 |          27.3010 |          12.6787 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0150 |          25.2620 |          12.6679 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0079 |          24.5063 |          12.6553 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0104 |          24.0864 |          12.6547 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0089 |          23.8012 |          12.6562 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0124 |          23.5432 |          12.6513 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0137 |          23.3596 |          12.6495 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0114 |          23.1126 |          12.6466 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0151 |          23.1160 |          12.6423 |
[32m[20221213 22:37:48 @agent_ppo2.py:185][0m |          -0.0120 |          23.0363 |          12.6476 |
[32m[20221213 22:37:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.23
[32m[20221213 22:37:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.29
[32m[20221213 22:37:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.70
[32m[20221213 22:37:49 @agent_ppo2.py:143][0m Total time:      19.60 min
[32m[20221213 22:37:49 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221213 22:37:49 @agent_ppo2.py:121][0m #------------------------ Iteration 932 --------------------------#
[32m[20221213 22:37:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:49 @agent_ppo2.py:185][0m |          -0.0002 |          28.1601 |          12.4972 |
[32m[20221213 22:37:49 @agent_ppo2.py:185][0m |           0.0011 |          25.5314 |          12.4745 |
[32m[20221213 22:37:49 @agent_ppo2.py:185][0m |          -0.0055 |          24.6450 |          12.4881 |
[32m[20221213 22:37:49 @agent_ppo2.py:185][0m |          -0.0070 |          23.8335 |          12.4857 |
[32m[20221213 22:37:49 @agent_ppo2.py:185][0m |          -0.0098 |          23.4702 |          12.4802 |
[32m[20221213 22:37:49 @agent_ppo2.py:185][0m |          -0.0101 |          23.0205 |          12.4881 |
[32m[20221213 22:37:49 @agent_ppo2.py:185][0m |          -0.0107 |          22.7824 |          12.4804 |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |          -0.0039 |          22.7326 |          12.4832 |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |           0.0026 |          23.8100 |          12.4830 |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |           0.0030 |          25.1957 |          12.4734 |
[32m[20221213 22:37:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.43
[32m[20221213 22:37:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.21
[32m[20221213 22:37:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.18
[32m[20221213 22:37:50 @agent_ppo2.py:143][0m Total time:      19.62 min
[32m[20221213 22:37:50 @agent_ppo2.py:145][0m 1910784 total steps have happened
[32m[20221213 22:37:50 @agent_ppo2.py:121][0m #------------------------ Iteration 933 --------------------------#
[32m[20221213 22:37:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |          -0.0030 |          14.6446 |          12.5653 |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |          -0.0006 |           9.4034 |          12.5544 |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |           0.0016 |           8.7177 |          12.5425 |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |           0.0004 |           7.8127 |          12.5446 |
[32m[20221213 22:37:50 @agent_ppo2.py:185][0m |          -0.0078 |           7.3904 |          12.5437 |
[32m[20221213 22:37:51 @agent_ppo2.py:185][0m |          -0.0062 |           7.1883 |          12.5300 |
[32m[20221213 22:37:51 @agent_ppo2.py:185][0m |          -0.0056 |           7.1388 |          12.5273 |
[32m[20221213 22:37:51 @agent_ppo2.py:185][0m |          -0.0065 |           6.8519 |          12.5363 |
[32m[20221213 22:37:51 @agent_ppo2.py:185][0m |          -0.0143 |           6.6999 |          12.5222 |
[32m[20221213 22:37:51 @agent_ppo2.py:185][0m |          -0.0065 |           6.5515 |          12.5205 |
[32m[20221213 22:37:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:37:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.00
[32m[20221213 22:37:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.18
[32m[20221213 22:37:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.48
[32m[20221213 22:37:51 @agent_ppo2.py:143][0m Total time:      19.64 min
[32m[20221213 22:37:51 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221213 22:37:51 @agent_ppo2.py:121][0m #------------------------ Iteration 934 --------------------------#
[32m[20221213 22:37:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:51 @agent_ppo2.py:185][0m |          -0.0008 |          22.6832 |          12.6127 |
[32m[20221213 22:37:51 @agent_ppo2.py:185][0m |          -0.0057 |          20.6087 |          12.6041 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0068 |          19.6566 |          12.5962 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0077 |          19.1932 |          12.5945 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0105 |          18.6837 |          12.5835 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0135 |          18.4664 |          12.5854 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0132 |          18.2698 |          12.5739 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0128 |          18.1898 |          12.5820 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0129 |          18.0681 |          12.5806 |
[32m[20221213 22:37:52 @agent_ppo2.py:185][0m |          -0.0127 |          18.0933 |          12.5754 |
[32m[20221213 22:37:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.54
[32m[20221213 22:37:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.48
[32m[20221213 22:37:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.42
[32m[20221213 22:37:52 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221213 22:37:52 @agent_ppo2.py:145][0m 1914880 total steps have happened
[32m[20221213 22:37:52 @agent_ppo2.py:121][0m #------------------------ Iteration 935 --------------------------#
[32m[20221213 22:37:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |           0.0031 |          17.8533 |          12.5184 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |           0.0058 |          15.8859 |          12.5027 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0004 |          15.1713 |          12.5020 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0097 |          14.7451 |          12.4922 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0077 |          14.2492 |          12.4971 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0070 |          13.8218 |          12.4951 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0149 |          13.7406 |          12.4944 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0071 |          13.5464 |          12.4966 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0127 |          13.3179 |          12.4946 |
[32m[20221213 22:37:53 @agent_ppo2.py:185][0m |          -0.0033 |          15.5781 |          12.4975 |
[32m[20221213 22:37:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.05
[32m[20221213 22:37:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.78
[32m[20221213 22:37:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.67
[32m[20221213 22:37:53 @agent_ppo2.py:143][0m Total time:      19.68 min
[32m[20221213 22:37:53 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221213 22:37:53 @agent_ppo2.py:121][0m #------------------------ Iteration 936 --------------------------#
[32m[20221213 22:37:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0008 |          34.7624 |          12.7964 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0051 |          30.7705 |          12.7858 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0049 |          29.3200 |          12.7774 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |           0.0071 |          32.2523 |          12.7722 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0060 |          28.3654 |          12.7759 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0110 |          27.2455 |          12.7765 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0106 |          26.7436 |          12.7873 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0124 |          26.0989 |          12.7831 |
[32m[20221213 22:37:54 @agent_ppo2.py:185][0m |          -0.0121 |          26.0988 |          12.7831 |
[32m[20221213 22:37:55 @agent_ppo2.py:185][0m |          -0.0144 |          25.9072 |          12.7885 |
[32m[20221213 22:37:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:37:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.63
[32m[20221213 22:37:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 318.52
[32m[20221213 22:37:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.86
[32m[20221213 22:37:55 @agent_ppo2.py:143][0m Total time:      19.70 min
[32m[20221213 22:37:55 @agent_ppo2.py:145][0m 1918976 total steps have happened
[32m[20221213 22:37:55 @agent_ppo2.py:121][0m #------------------------ Iteration 937 --------------------------#
[32m[20221213 22:37:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:55 @agent_ppo2.py:185][0m |           0.0019 |          24.0361 |          12.6701 |
[32m[20221213 22:37:55 @agent_ppo2.py:185][0m |          -0.0057 |          21.0761 |          12.6569 |
[32m[20221213 22:37:55 @agent_ppo2.py:185][0m |          -0.0037 |          20.1745 |          12.6544 |
[32m[20221213 22:37:55 @agent_ppo2.py:185][0m |          -0.0133 |          19.4294 |          12.6485 |
[32m[20221213 22:37:55 @agent_ppo2.py:185][0m |           0.0014 |          20.6091 |          12.6560 |
[32m[20221213 22:37:55 @agent_ppo2.py:185][0m |          -0.0130 |          18.7106 |          12.6566 |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0102 |          18.1109 |          12.6526 |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0128 |          17.7939 |          12.6547 |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0135 |          17.5714 |          12.6518 |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0131 |          17.3988 |          12.6637 |
[32m[20221213 22:37:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.94
[32m[20221213 22:37:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.54
[32m[20221213 22:37:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.75
[32m[20221213 22:37:56 @agent_ppo2.py:143][0m Total time:      19.72 min
[32m[20221213 22:37:56 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221213 22:37:56 @agent_ppo2.py:121][0m #------------------------ Iteration 938 --------------------------#
[32m[20221213 22:37:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0024 |          29.7158 |          12.8227 |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0055 |          28.0719 |          12.8171 |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0039 |          27.1579 |          12.8100 |
[32m[20221213 22:37:56 @agent_ppo2.py:185][0m |          -0.0067 |          26.6482 |          12.7970 |
[32m[20221213 22:37:57 @agent_ppo2.py:185][0m |          -0.0080 |          26.3791 |          12.8017 |
[32m[20221213 22:37:57 @agent_ppo2.py:185][0m |          -0.0060 |          25.8951 |          12.7891 |
[32m[20221213 22:37:57 @agent_ppo2.py:185][0m |          -0.0082 |          25.6194 |          12.7973 |
[32m[20221213 22:37:57 @agent_ppo2.py:185][0m |          -0.0101 |          25.4055 |          12.7878 |
[32m[20221213 22:37:57 @agent_ppo2.py:185][0m |          -0.0147 |          25.3554 |          12.7815 |
[32m[20221213 22:37:57 @agent_ppo2.py:185][0m |          -0.0060 |          25.1478 |          12.7904 |
[32m[20221213 22:37:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:37:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.02
[32m[20221213 22:37:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.99
[32m[20221213 22:37:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.05
[32m[20221213 22:37:57 @agent_ppo2.py:143][0m Total time:      19.74 min
[32m[20221213 22:37:57 @agent_ppo2.py:145][0m 1923072 total steps have happened
[32m[20221213 22:37:57 @agent_ppo2.py:121][0m #------------------------ Iteration 939 --------------------------#
[32m[20221213 22:37:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:37:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:57 @agent_ppo2.py:185][0m |          -0.0007 |          20.5306 |          12.6550 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0044 |          18.8347 |          12.6428 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0048 |          18.1088 |          12.6488 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0049 |          17.8078 |          12.6490 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0085 |          17.3089 |          12.6442 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0078 |          16.9931 |          12.6438 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0072 |          16.7458 |          12.6510 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0126 |          16.7181 |          12.6443 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0106 |          16.4362 |          12.6525 |
[32m[20221213 22:37:58 @agent_ppo2.py:185][0m |          -0.0108 |          16.3040 |          12.6436 |
[32m[20221213 22:37:58 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:37:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.06
[32m[20221213 22:37:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.29
[32m[20221213 22:37:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.85
[32m[20221213 22:37:58 @agent_ppo2.py:143][0m Total time:      19.76 min
[32m[20221213 22:37:58 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221213 22:37:58 @agent_ppo2.py:121][0m #------------------------ Iteration 940 --------------------------#
[32m[20221213 22:37:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:37:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0029 |          22.9717 |          12.7562 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0059 |          21.5035 |          12.7404 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0062 |          20.8584 |          12.7391 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0126 |          20.6113 |          12.7378 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0077 |          20.2729 |          12.7346 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0118 |          20.0967 |          12.7205 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0131 |          19.9526 |          12.7339 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0099 |          19.8149 |          12.7290 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0056 |          19.7971 |          12.7281 |
[32m[20221213 22:37:59 @agent_ppo2.py:185][0m |          -0.0138 |          19.5691 |          12.7318 |
[32m[20221213 22:37:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.38
[32m[20221213 22:38:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.45
[32m[20221213 22:38:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.44
[32m[20221213 22:38:00 @agent_ppo2.py:143][0m Total time:      19.78 min
[32m[20221213 22:38:00 @agent_ppo2.py:145][0m 1927168 total steps have happened
[32m[20221213 22:38:00 @agent_ppo2.py:121][0m #------------------------ Iteration 941 --------------------------#
[32m[20221213 22:38:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |           0.0012 |          23.0463 |          12.6880 |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |          -0.0083 |          19.6361 |          12.6631 |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |           0.0112 |          20.8970 |          12.6617 |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |          -0.0027 |          17.8030 |          12.6593 |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |          -0.0146 |          17.3444 |          12.6582 |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |          -0.0040 |          16.8506 |          12.6587 |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |          -0.0113 |          16.4295 |          12.6544 |
[32m[20221213 22:38:00 @agent_ppo2.py:185][0m |          -0.0072 |          15.9920 |          12.6574 |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |          -0.0137 |          15.7309 |          12.6525 |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |          -0.0096 |          15.5020 |          12.6591 |
[32m[20221213 22:38:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 175.12
[32m[20221213 22:38:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.99
[32m[20221213 22:38:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.34
[32m[20221213 22:38:01 @agent_ppo2.py:143][0m Total time:      19.80 min
[32m[20221213 22:38:01 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221213 22:38:01 @agent_ppo2.py:121][0m #------------------------ Iteration 942 --------------------------#
[32m[20221213 22:38:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |           0.0001 |          30.1602 |          12.8694 |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |          -0.0123 |          28.0319 |          12.8504 |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |          -0.0052 |          27.0083 |          12.8470 |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |          -0.0019 |          26.9384 |          12.8484 |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |          -0.0100 |          25.7353 |          12.8406 |
[32m[20221213 22:38:01 @agent_ppo2.py:185][0m |          -0.0042 |          25.6119 |          12.8326 |
[32m[20221213 22:38:02 @agent_ppo2.py:185][0m |          -0.0115 |          24.9545 |          12.8316 |
[32m[20221213 22:38:02 @agent_ppo2.py:185][0m |          -0.0106 |          24.7014 |          12.8387 |
[32m[20221213 22:38:02 @agent_ppo2.py:185][0m |          -0.0122 |          24.4320 |          12.8351 |
[32m[20221213 22:38:02 @agent_ppo2.py:185][0m |          -0.0151 |          24.6217 |          12.8277 |
[32m[20221213 22:38:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 171.57
[32m[20221213 22:38:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.61
[32m[20221213 22:38:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.36
[32m[20221213 22:38:02 @agent_ppo2.py:143][0m Total time:      19.82 min
[32m[20221213 22:38:02 @agent_ppo2.py:145][0m 1931264 total steps have happened
[32m[20221213 22:38:02 @agent_ppo2.py:121][0m #------------------------ Iteration 943 --------------------------#
[32m[20221213 22:38:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:02 @agent_ppo2.py:185][0m |           0.0021 |          27.0634 |          12.7249 |
[32m[20221213 22:38:02 @agent_ppo2.py:185][0m |          -0.0013 |          23.6112 |          12.7166 |
[32m[20221213 22:38:02 @agent_ppo2.py:185][0m |          -0.0096 |          22.4052 |          12.7128 |
[32m[20221213 22:38:03 @agent_ppo2.py:185][0m |          -0.0106 |          21.6175 |          12.7056 |
[32m[20221213 22:38:03 @agent_ppo2.py:185][0m |          -0.0116 |          20.9980 |          12.7056 |
[32m[20221213 22:38:03 @agent_ppo2.py:185][0m |          -0.0173 |          20.6264 |          12.6961 |
[32m[20221213 22:38:03 @agent_ppo2.py:185][0m |          -0.0170 |          20.2097 |          12.7024 |
[32m[20221213 22:38:03 @agent_ppo2.py:185][0m |          -0.0139 |          19.7844 |          12.7082 |
[32m[20221213 22:38:03 @agent_ppo2.py:185][0m |          -0.0104 |          19.7456 |          12.6987 |
[32m[20221213 22:38:03 @agent_ppo2.py:185][0m |          -0.0137 |          19.4049 |          12.7027 |
[32m[20221213 22:38:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:38:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.32
[32m[20221213 22:38:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.36
[32m[20221213 22:38:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.90
[32m[20221213 22:38:03 @agent_ppo2.py:143][0m Total time:      19.84 min
[32m[20221213 22:38:03 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221213 22:38:03 @agent_ppo2.py:121][0m #------------------------ Iteration 944 --------------------------#
[32m[20221213 22:38:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |           0.0015 |          31.0077 |          12.6442 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0008 |          27.2733 |          12.6410 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |           0.0006 |          25.8789 |          12.6452 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0036 |          24.7385 |          12.6423 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0079 |          24.0424 |          12.6449 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0060 |          23.5630 |          12.6391 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0068 |          23.6568 |          12.6232 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0029 |          22.8799 |          12.6367 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0100 |          22.4060 |          12.6299 |
[32m[20221213 22:38:04 @agent_ppo2.py:185][0m |          -0.0037 |          22.3330 |          12.6361 |
[32m[20221213 22:38:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.59
[32m[20221213 22:38:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.08
[32m[20221213 22:38:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.19
[32m[20221213 22:38:04 @agent_ppo2.py:143][0m Total time:      19.86 min
[32m[20221213 22:38:04 @agent_ppo2.py:145][0m 1935360 total steps have happened
[32m[20221213 22:38:04 @agent_ppo2.py:121][0m #------------------------ Iteration 945 --------------------------#
[32m[20221213 22:38:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |           0.0018 |          27.0286 |          12.8475 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |           0.0003 |          24.4669 |          12.8331 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0057 |          23.4277 |          12.8238 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0083 |          22.6616 |          12.8238 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0091 |          22.1418 |          12.8206 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0079 |          21.6286 |          12.8219 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0052 |          21.3909 |          12.8148 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0079 |          21.0178 |          12.8214 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0089 |          20.7395 |          12.8106 |
[32m[20221213 22:38:05 @agent_ppo2.py:185][0m |          -0.0091 |          20.5903 |          12.8190 |
[32m[20221213 22:38:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.90
[32m[20221213 22:38:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.68
[32m[20221213 22:38:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.27
[32m[20221213 22:38:06 @agent_ppo2.py:143][0m Total time:      19.88 min
[32m[20221213 22:38:06 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221213 22:38:06 @agent_ppo2.py:121][0m #------------------------ Iteration 946 --------------------------#
[32m[20221213 22:38:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |          -0.0012 |          28.4919 |          12.7732 |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |           0.0029 |          28.3254 |          12.7576 |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |          -0.0062 |          26.0359 |          12.7611 |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |          -0.0058 |          25.6201 |          12.7611 |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |          -0.0067 |          25.2651 |          12.7590 |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |          -0.0108 |          25.0317 |          12.7693 |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |          -0.0082 |          24.8923 |          12.7603 |
[32m[20221213 22:38:06 @agent_ppo2.py:185][0m |          -0.0085 |          24.6642 |          12.7700 |
[32m[20221213 22:38:07 @agent_ppo2.py:185][0m |          -0.0057 |          24.5831 |          12.7733 |
[32m[20221213 22:38:07 @agent_ppo2.py:185][0m |          -0.0079 |          24.4148 |          12.7623 |
[32m[20221213 22:38:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.46
[32m[20221213 22:38:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.05
[32m[20221213 22:38:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.30
[32m[20221213 22:38:07 @agent_ppo2.py:143][0m Total time:      19.90 min
[32m[20221213 22:38:07 @agent_ppo2.py:145][0m 1939456 total steps have happened
[32m[20221213 22:38:07 @agent_ppo2.py:121][0m #------------------------ Iteration 947 --------------------------#
[32m[20221213 22:38:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:07 @agent_ppo2.py:185][0m |           0.0082 |          28.3288 |          12.9871 |
[32m[20221213 22:38:07 @agent_ppo2.py:185][0m |          -0.0088 |          24.5598 |          12.9779 |
[32m[20221213 22:38:07 @agent_ppo2.py:185][0m |          -0.0066 |          23.3752 |          12.9732 |
[32m[20221213 22:38:07 @agent_ppo2.py:185][0m |          -0.0082 |          22.6764 |          12.9740 |
[32m[20221213 22:38:07 @agent_ppo2.py:185][0m |          -0.0057 |          22.1785 |          12.9642 |
[32m[20221213 22:38:08 @agent_ppo2.py:185][0m |          -0.0111 |          21.7499 |          12.9615 |
[32m[20221213 22:38:08 @agent_ppo2.py:185][0m |          -0.0020 |          22.0589 |          12.9616 |
[32m[20221213 22:38:08 @agent_ppo2.py:185][0m |          -0.0143 |          21.0722 |          12.9577 |
[32m[20221213 22:38:08 @agent_ppo2.py:185][0m |          -0.0116 |          20.7243 |          12.9433 |
[32m[20221213 22:38:08 @agent_ppo2.py:185][0m |          -0.0129 |          20.5337 |          12.9520 |
[32m[20221213 22:38:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.54
[32m[20221213 22:38:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.69
[32m[20221213 22:38:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.51
[32m[20221213 22:38:08 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221213 22:38:08 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221213 22:38:08 @agent_ppo2.py:121][0m #------------------------ Iteration 948 --------------------------#
[32m[20221213 22:38:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:08 @agent_ppo2.py:185][0m |          -0.0013 |          35.7551 |          12.9394 |
[32m[20221213 22:38:08 @agent_ppo2.py:185][0m |          -0.0075 |          31.9587 |          12.9182 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0073 |          30.1649 |          12.9217 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0022 |          30.3831 |          12.9270 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0078 |          28.3279 |          12.9086 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0132 |          27.6336 |          12.9205 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0151 |          27.0497 |          12.9354 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0115 |          26.5864 |          12.9258 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0153 |          26.0374 |          12.9249 |
[32m[20221213 22:38:09 @agent_ppo2.py:185][0m |          -0.0132 |          25.8873 |          12.9283 |
[32m[20221213 22:38:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:38:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.53
[32m[20221213 22:38:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.18
[32m[20221213 22:38:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.43
[32m[20221213 22:38:09 @agent_ppo2.py:143][0m Total time:      19.94 min
[32m[20221213 22:38:09 @agent_ppo2.py:145][0m 1943552 total steps have happened
[32m[20221213 22:38:09 @agent_ppo2.py:121][0m #------------------------ Iteration 949 --------------------------#
[32m[20221213 22:38:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |           0.0036 |          24.6450 |          12.7829 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0092 |          21.6923 |          12.7603 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0147 |          20.3508 |          12.7740 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0106 |          19.5469 |          12.7592 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0130 |          19.0120 |          12.7610 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0149 |          18.8594 |          12.7575 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0102 |          18.3587 |          12.7581 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0051 |          18.5885 |          12.7579 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0083 |          17.8274 |          12.7430 |
[32m[20221213 22:38:10 @agent_ppo2.py:185][0m |          -0.0022 |          18.0856 |          12.7594 |
[32m[20221213 22:38:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.97
[32m[20221213 22:38:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.91
[32m[20221213 22:38:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.85
[32m[20221213 22:38:10 @agent_ppo2.py:143][0m Total time:      19.96 min
[32m[20221213 22:38:10 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221213 22:38:10 @agent_ppo2.py:121][0m #------------------------ Iteration 950 --------------------------#
[32m[20221213 22:38:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:38:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0021 |          28.2144 |          12.8246 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0030 |          24.3629 |          12.7995 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0053 |          23.6896 |          12.7914 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0077 |          22.9422 |          12.7803 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0116 |          22.7074 |          12.7884 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0012 |          22.9265 |          12.7825 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0116 |          22.0283 |          12.7768 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0005 |          22.0362 |          12.7914 |
[32m[20221213 22:38:11 @agent_ppo2.py:185][0m |          -0.0101 |          21.7877 |          12.7817 |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |          -0.0133 |          21.6100 |          12.7710 |
[32m[20221213 22:38:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.61
[32m[20221213 22:38:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.89
[32m[20221213 22:38:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.66
[32m[20221213 22:38:12 @agent_ppo2.py:143][0m Total time:      19.98 min
[32m[20221213 22:38:12 @agent_ppo2.py:145][0m 1947648 total steps have happened
[32m[20221213 22:38:12 @agent_ppo2.py:121][0m #------------------------ Iteration 951 --------------------------#
[32m[20221213 22:38:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |           0.0061 |          33.8031 |          12.7622 |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |          -0.0037 |          30.6595 |          12.7357 |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |          -0.0036 |          29.4621 |          12.7342 |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |          -0.0129 |          29.3126 |          12.7353 |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |          -0.0064 |          28.1706 |          12.7268 |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |          -0.0084 |          27.6607 |          12.7209 |
[32m[20221213 22:38:12 @agent_ppo2.py:185][0m |          -0.0121 |          27.2137 |          12.7233 |
[32m[20221213 22:38:13 @agent_ppo2.py:185][0m |          -0.0132 |          27.0648 |          12.7193 |
[32m[20221213 22:38:13 @agent_ppo2.py:185][0m |          -0.0125 |          26.5801 |          12.7171 |
[32m[20221213 22:38:13 @agent_ppo2.py:185][0m |          -0.0106 |          26.3192 |          12.7119 |
[32m[20221213 22:38:13 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:38:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.10
[32m[20221213 22:38:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.46
[32m[20221213 22:38:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.38
[32m[20221213 22:38:13 @agent_ppo2.py:143][0m Total time:      20.00 min
[32m[20221213 22:38:13 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221213 22:38:13 @agent_ppo2.py:121][0m #------------------------ Iteration 952 --------------------------#
[32m[20221213 22:38:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:13 @agent_ppo2.py:185][0m |           0.0017 |          29.0113 |          12.9659 |
[32m[20221213 22:38:13 @agent_ppo2.py:185][0m |          -0.0029 |          24.6450 |          12.9473 |
[32m[20221213 22:38:13 @agent_ppo2.py:185][0m |          -0.0066 |          23.2689 |          12.9450 |
[32m[20221213 22:38:13 @agent_ppo2.py:185][0m |          -0.0039 |          22.4164 |          12.9442 |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0066 |          21.5696 |          12.9378 |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0111 |          20.8334 |          12.9366 |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0085 |          20.4048 |          12.9251 |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0097 |          20.0607 |          12.9428 |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0077 |          19.7245 |          12.9341 |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0123 |          19.5572 |          12.9394 |
[32m[20221213 22:38:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.33
[32m[20221213 22:38:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.41
[32m[20221213 22:38:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.11
[32m[20221213 22:38:14 @agent_ppo2.py:143][0m Total time:      20.02 min
[32m[20221213 22:38:14 @agent_ppo2.py:145][0m 1951744 total steps have happened
[32m[20221213 22:38:14 @agent_ppo2.py:121][0m #------------------------ Iteration 953 --------------------------#
[32m[20221213 22:38:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0001 |          28.0292 |          12.8506 |
[32m[20221213 22:38:14 @agent_ppo2.py:185][0m |          -0.0027 |          25.1515 |          12.8462 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0001 |          24.1491 |          12.8424 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0055 |          22.8792 |          12.8424 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0058 |          22.0439 |          12.8432 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0106 |          22.2300 |          12.8446 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0109 |          20.9700 |          12.8483 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0088 |          20.6301 |          12.8503 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0114 |          20.5018 |          12.8475 |
[32m[20221213 22:38:15 @agent_ppo2.py:185][0m |          -0.0103 |          19.9889 |          12.8519 |
[32m[20221213 22:38:15 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:38:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.43
[32m[20221213 22:38:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.95
[32m[20221213 22:38:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.69
[32m[20221213 22:38:15 @agent_ppo2.py:143][0m Total time:      20.04 min
[32m[20221213 22:38:15 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221213 22:38:15 @agent_ppo2.py:121][0m #------------------------ Iteration 954 --------------------------#
[32m[20221213 22:38:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0050 |          24.9446 |          12.8915 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0047 |          22.1404 |          12.8670 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0095 |          21.4240 |          12.8696 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0086 |          21.0356 |          12.8557 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0146 |          20.6530 |          12.8675 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0133 |          20.5563 |          12.8617 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0021 |          21.7925 |          12.8633 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0104 |          20.3164 |          12.8589 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0109 |          20.1321 |          12.8630 |
[32m[20221213 22:38:16 @agent_ppo2.py:185][0m |          -0.0105 |          20.0212 |          12.8417 |
[32m[20221213 22:38:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.47
[32m[20221213 22:38:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.91
[32m[20221213 22:38:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.61
[32m[20221213 22:38:17 @agent_ppo2.py:143][0m Total time:      20.06 min
[32m[20221213 22:38:17 @agent_ppo2.py:145][0m 1955840 total steps have happened
[32m[20221213 22:38:17 @agent_ppo2.py:121][0m #------------------------ Iteration 955 --------------------------#
[32m[20221213 22:38:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |           0.0005 |          30.5487 |          12.9534 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0021 |          29.0073 |          12.9240 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0038 |          28.3625 |          12.9196 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0070 |          28.0500 |          12.9184 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0065 |          27.7643 |          12.9224 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0099 |          27.5371 |          12.9216 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0081 |          27.4828 |          12.9201 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0105 |          27.4283 |          12.9209 |
[32m[20221213 22:38:17 @agent_ppo2.py:185][0m |          -0.0069 |          27.3095 |          12.9169 |
[32m[20221213 22:38:18 @agent_ppo2.py:185][0m |          -0.0112 |          27.1449 |          12.9216 |
[32m[20221213 22:38:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.71
[32m[20221213 22:38:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.94
[32m[20221213 22:38:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.88
[32m[20221213 22:38:18 @agent_ppo2.py:143][0m Total time:      20.08 min
[32m[20221213 22:38:18 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221213 22:38:18 @agent_ppo2.py:121][0m #------------------------ Iteration 956 --------------------------#
[32m[20221213 22:38:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:18 @agent_ppo2.py:185][0m |           0.0027 |          26.5841 |          12.9000 |
[32m[20221213 22:38:18 @agent_ppo2.py:185][0m |          -0.0069 |          23.1458 |          12.8891 |
[32m[20221213 22:38:18 @agent_ppo2.py:185][0m |          -0.0046 |          21.7335 |          12.8943 |
[32m[20221213 22:38:18 @agent_ppo2.py:185][0m |          -0.0094 |          21.3487 |          12.8826 |
[32m[20221213 22:38:18 @agent_ppo2.py:185][0m |          -0.0031 |          20.4593 |          12.8835 |
[32m[20221213 22:38:18 @agent_ppo2.py:185][0m |          -0.0118 |          19.9413 |          12.8969 |
[32m[20221213 22:38:19 @agent_ppo2.py:185][0m |          -0.0094 |          19.4668 |          12.8936 |
[32m[20221213 22:38:19 @agent_ppo2.py:185][0m |          -0.0125 |          19.1269 |          12.8895 |
[32m[20221213 22:38:19 @agent_ppo2.py:185][0m |          -0.0124 |          18.9957 |          12.8970 |
[32m[20221213 22:38:19 @agent_ppo2.py:185][0m |          -0.0102 |          18.6009 |          12.8953 |
[32m[20221213 22:38:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.08
[32m[20221213 22:38:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.55
[32m[20221213 22:38:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.59
[32m[20221213 22:38:19 @agent_ppo2.py:143][0m Total time:      20.10 min
[32m[20221213 22:38:19 @agent_ppo2.py:145][0m 1959936 total steps have happened
[32m[20221213 22:38:19 @agent_ppo2.py:121][0m #------------------------ Iteration 957 --------------------------#
[32m[20221213 22:38:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:19 @agent_ppo2.py:185][0m |          -0.0025 |          25.0625 |          13.0102 |
[32m[20221213 22:38:19 @agent_ppo2.py:185][0m |          -0.0058 |          22.4704 |          13.0003 |
[32m[20221213 22:38:19 @agent_ppo2.py:185][0m |          -0.0015 |          21.7106 |          12.9915 |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |          -0.0024 |          22.1047 |          12.9845 |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |          -0.0045 |          20.0490 |          12.9842 |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |          -0.0153 |          19.6107 |          12.9904 |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |          -0.0149 |          19.3229 |          12.9797 |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |          -0.0106 |          18.9257 |          12.9801 |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |          -0.0155 |          18.8020 |          12.9856 |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |          -0.0213 |          18.4349 |          12.9743 |
[32m[20221213 22:38:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.18
[32m[20221213 22:38:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.01
[32m[20221213 22:38:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.07
[32m[20221213 22:38:20 @agent_ppo2.py:143][0m Total time:      20.12 min
[32m[20221213 22:38:20 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221213 22:38:20 @agent_ppo2.py:121][0m #------------------------ Iteration 958 --------------------------#
[32m[20221213 22:38:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:20 @agent_ppo2.py:185][0m |           0.0072 |          34.2918 |          12.8044 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0080 |          28.0474 |          12.7890 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0102 |          26.5335 |          12.7864 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0098 |          25.7064 |          12.7855 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0095 |          24.7488 |          12.7891 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |           0.0039 |          25.8888 |          12.7929 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0136 |          23.5837 |          12.7841 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0105 |          23.1771 |          12.7884 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0083 |          22.9523 |          12.7916 |
[32m[20221213 22:38:21 @agent_ppo2.py:185][0m |          -0.0090 |          22.4602 |          12.7863 |
[32m[20221213 22:38:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:38:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.07
[32m[20221213 22:38:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.86
[32m[20221213 22:38:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.54
[32m[20221213 22:38:21 @agent_ppo2.py:143][0m Total time:      20.14 min
[32m[20221213 22:38:21 @agent_ppo2.py:145][0m 1964032 total steps have happened
[32m[20221213 22:38:21 @agent_ppo2.py:121][0m #------------------------ Iteration 959 --------------------------#
[32m[20221213 22:38:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0030 |          47.2411 |          13.0356 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0049 |          44.1774 |          13.0279 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0068 |          42.9025 |          13.0180 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0110 |          42.2800 |          13.0247 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0111 |          42.1340 |          13.0030 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0078 |          41.9343 |          13.0077 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0110 |          41.7953 |          13.0063 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0129 |          41.5683 |          13.0104 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0166 |          41.4897 |          13.0095 |
[32m[20221213 22:38:22 @agent_ppo2.py:185][0m |          -0.0121 |          41.4656 |          13.0030 |
[32m[20221213 22:38:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.48
[32m[20221213 22:38:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.47
[32m[20221213 22:38:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.90
[32m[20221213 22:38:23 @agent_ppo2.py:143][0m Total time:      20.16 min
[32m[20221213 22:38:23 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221213 22:38:23 @agent_ppo2.py:121][0m #------------------------ Iteration 960 --------------------------#
[32m[20221213 22:38:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:38:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |           0.0022 |          39.3360 |          13.1655 |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |          -0.0054 |          36.1118 |          13.1566 |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |          -0.0070 |          34.8770 |          13.1553 |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |          -0.0029 |          34.8328 |          13.1425 |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |          -0.0089 |          33.4615 |          13.1358 |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |          -0.0089 |          33.1676 |          13.1375 |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |          -0.0136 |          32.8952 |          13.1353 |
[32m[20221213 22:38:23 @agent_ppo2.py:185][0m |          -0.0131 |          32.5521 |          13.1407 |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |          -0.0100 |          32.0954 |          13.1381 |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |          -0.0134 |          32.1894 |          13.1400 |
[32m[20221213 22:38:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.76
[32m[20221213 22:38:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.14
[32m[20221213 22:38:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.74
[32m[20221213 22:38:24 @agent_ppo2.py:143][0m Total time:      20.18 min
[32m[20221213 22:38:24 @agent_ppo2.py:145][0m 1968128 total steps have happened
[32m[20221213 22:38:24 @agent_ppo2.py:121][0m #------------------------ Iteration 961 --------------------------#
[32m[20221213 22:38:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |           0.0007 |          32.5122 |          12.9623 |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |          -0.0026 |          29.9003 |          12.9285 |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |          -0.0082 |          28.9850 |          12.9319 |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |          -0.0060 |          28.3769 |          12.9399 |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |          -0.0044 |          27.8192 |          12.9250 |
[32m[20221213 22:38:24 @agent_ppo2.py:185][0m |          -0.0064 |          27.5116 |          12.9203 |
[32m[20221213 22:38:25 @agent_ppo2.py:185][0m |          -0.0056 |          26.9752 |          12.9246 |
[32m[20221213 22:38:25 @agent_ppo2.py:185][0m |          -0.0076 |          26.6911 |          12.9127 |
[32m[20221213 22:38:25 @agent_ppo2.py:185][0m |          -0.0086 |          26.4212 |          12.9168 |
[32m[20221213 22:38:25 @agent_ppo2.py:185][0m |          -0.0107 |          26.1727 |          12.9143 |
[32m[20221213 22:38:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.80
[32m[20221213 22:38:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.22
[32m[20221213 22:38:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.50
[32m[20221213 22:38:25 @agent_ppo2.py:143][0m Total time:      20.20 min
[32m[20221213 22:38:25 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221213 22:38:25 @agent_ppo2.py:121][0m #------------------------ Iteration 962 --------------------------#
[32m[20221213 22:38:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:25 @agent_ppo2.py:185][0m |          -0.0063 |          25.4443 |          13.0985 |
[32m[20221213 22:38:25 @agent_ppo2.py:185][0m |          -0.0062 |          23.0900 |          13.0882 |
[32m[20221213 22:38:25 @agent_ppo2.py:185][0m |          -0.0053 |          22.7556 |          13.0897 |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |          -0.0093 |          22.4581 |          13.0882 |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |          -0.0064 |          22.2955 |          13.0952 |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |          -0.0090 |          22.1446 |          13.0920 |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |          -0.0108 |          21.9892 |          13.0895 |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |          -0.0113 |          21.8465 |          13.0870 |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |          -0.0041 |          21.9183 |          13.0863 |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |          -0.0104 |          21.7012 |          13.0832 |
[32m[20221213 22:38:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.35
[32m[20221213 22:38:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.85
[32m[20221213 22:38:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.76
[32m[20221213 22:38:26 @agent_ppo2.py:143][0m Total time:      20.22 min
[32m[20221213 22:38:26 @agent_ppo2.py:145][0m 1972224 total steps have happened
[32m[20221213 22:38:26 @agent_ppo2.py:121][0m #------------------------ Iteration 963 --------------------------#
[32m[20221213 22:38:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:26 @agent_ppo2.py:185][0m |           0.0036 |          23.2062 |          13.0867 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0059 |          21.3812 |          13.0707 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0055 |          21.0019 |          13.0517 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0074 |          20.6616 |          13.0542 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0144 |          20.3851 |          13.0477 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0133 |          20.1745 |          13.0403 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0083 |          20.0349 |          13.0465 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0144 |          19.8886 |          13.0354 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0133 |          19.8082 |          13.0423 |
[32m[20221213 22:38:27 @agent_ppo2.py:185][0m |          -0.0114 |          19.6163 |          13.0334 |
[32m[20221213 22:38:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:38:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.66
[32m[20221213 22:38:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.04
[32m[20221213 22:38:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.88
[32m[20221213 22:38:27 @agent_ppo2.py:143][0m Total time:      20.24 min
[32m[20221213 22:38:27 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221213 22:38:27 @agent_ppo2.py:121][0m #------------------------ Iteration 964 --------------------------#
[32m[20221213 22:38:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |          -0.0004 |          26.1684 |          13.1785 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |          -0.0001 |          23.4248 |          13.1362 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |           0.0119 |          24.5714 |          13.1348 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |          -0.0062 |          21.3008 |          13.1266 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |           0.0059 |          21.6257 |          13.1279 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |           0.0077 |          22.1507 |          13.1504 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |          -0.0097 |          19.8420 |          13.1255 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |          -0.0103 |          19.3952 |          13.1387 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |          -0.0138 |          19.1424 |          13.1385 |
[32m[20221213 22:38:28 @agent_ppo2.py:185][0m |          -0.0004 |          20.8340 |          13.1306 |
[32m[20221213 22:38:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.68
[32m[20221213 22:38:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.97
[32m[20221213 22:38:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.73
[32m[20221213 22:38:29 @agent_ppo2.py:143][0m Total time:      20.26 min
[32m[20221213 22:38:29 @agent_ppo2.py:145][0m 1976320 total steps have happened
[32m[20221213 22:38:29 @agent_ppo2.py:121][0m #------------------------ Iteration 965 --------------------------#
[32m[20221213 22:38:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:29 @agent_ppo2.py:185][0m |           0.0007 |          31.4218 |          13.0830 |
[32m[20221213 22:38:29 @agent_ppo2.py:185][0m |           0.0084 |          32.4559 |          13.0619 |
[32m[20221213 22:38:29 @agent_ppo2.py:185][0m |          -0.0062 |          27.1566 |          13.0508 |
[32m[20221213 22:38:29 @agent_ppo2.py:185][0m |          -0.0020 |          26.5559 |          13.0552 |
[32m[20221213 22:38:29 @agent_ppo2.py:185][0m |          -0.0072 |          26.0753 |          13.0383 |
[32m[20221213 22:38:29 @agent_ppo2.py:185][0m |          -0.0051 |          25.9282 |          13.0441 |
[32m[20221213 22:38:29 @agent_ppo2.py:185][0m |          -0.0104 |          25.6178 |          13.0479 |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0090 |          25.5168 |          13.0424 |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0107 |          25.3774 |          13.0311 |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0125 |          25.3124 |          13.0299 |
[32m[20221213 22:38:30 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.71
[32m[20221213 22:38:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.07
[32m[20221213 22:38:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.67
[32m[20221213 22:38:30 @agent_ppo2.py:143][0m Total time:      20.28 min
[32m[20221213 22:38:30 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221213 22:38:30 @agent_ppo2.py:121][0m #------------------------ Iteration 966 --------------------------#
[32m[20221213 22:38:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0005 |          40.5210 |          13.0081 |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0036 |          32.7128 |          12.9836 |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0039 |          31.6730 |          12.9888 |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0074 |          30.8489 |          12.9871 |
[32m[20221213 22:38:30 @agent_ppo2.py:185][0m |          -0.0099 |          30.3758 |          12.9932 |
[32m[20221213 22:38:31 @agent_ppo2.py:185][0m |          -0.0099 |          29.8832 |          12.9848 |
[32m[20221213 22:38:31 @agent_ppo2.py:185][0m |          -0.0016 |          30.8021 |          12.9859 |
[32m[20221213 22:38:31 @agent_ppo2.py:185][0m |          -0.0103 |          29.5456 |          12.9753 |
[32m[20221213 22:38:31 @agent_ppo2.py:185][0m |          -0.0083 |          29.2879 |          12.9885 |
[32m[20221213 22:38:31 @agent_ppo2.py:185][0m |          -0.0094 |          28.9403 |          12.9784 |
[32m[20221213 22:38:31 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.41
[32m[20221213 22:38:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 332.42
[32m[20221213 22:38:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.41
[32m[20221213 22:38:31 @agent_ppo2.py:143][0m Total time:      20.30 min
[32m[20221213 22:38:31 @agent_ppo2.py:145][0m 1980416 total steps have happened
[32m[20221213 22:38:31 @agent_ppo2.py:121][0m #------------------------ Iteration 967 --------------------------#
[32m[20221213 22:38:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:31 @agent_ppo2.py:185][0m |          -0.0007 |          24.2949 |          13.1413 |
[32m[20221213 22:38:31 @agent_ppo2.py:185][0m |          -0.0036 |          20.9652 |          13.1269 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0023 |          20.2241 |          13.1237 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0043 |          19.7147 |          13.1190 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0058 |          19.4492 |          13.1183 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0001 |          20.8875 |          13.1204 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0035 |          18.9114 |          13.0830 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0139 |          18.6829 |          13.1136 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0079 |          18.5484 |          13.1141 |
[32m[20221213 22:38:32 @agent_ppo2.py:185][0m |          -0.0075 |          18.4151 |          13.1160 |
[32m[20221213 22:38:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.62
[32m[20221213 22:38:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.98
[32m[20221213 22:38:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.91
[32m[20221213 22:38:32 @agent_ppo2.py:143][0m Total time:      20.32 min
[32m[20221213 22:38:32 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221213 22:38:32 @agent_ppo2.py:121][0m #------------------------ Iteration 968 --------------------------#
[32m[20221213 22:38:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0015 |          31.0895 |          13.3074 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0020 |          28.8163 |          13.3000 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0011 |          28.5990 |          13.3045 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0080 |          28.0330 |          13.3060 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0089 |          27.7217 |          13.3049 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0086 |          27.7257 |          13.3013 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0075 |          27.4389 |          13.2947 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0071 |          27.1833 |          13.2972 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0119 |          27.0595 |          13.3007 |
[32m[20221213 22:38:33 @agent_ppo2.py:185][0m |          -0.0120 |          27.0943 |          13.2936 |
[32m[20221213 22:38:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:38:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.99
[32m[20221213 22:38:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.23
[32m[20221213 22:38:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.70
[32m[20221213 22:38:33 @agent_ppo2.py:143][0m Total time:      20.34 min
[32m[20221213 22:38:33 @agent_ppo2.py:145][0m 1984512 total steps have happened
[32m[20221213 22:38:33 @agent_ppo2.py:121][0m #------------------------ Iteration 969 --------------------------#
[32m[20221213 22:38:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |           0.0035 |          29.6406 |          13.1020 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0017 |          25.3677 |          13.0881 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0083 |          23.5055 |          13.0893 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0100 |          22.6228 |          13.0885 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0116 |          22.1073 |          13.0831 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0144 |          21.5727 |          13.0843 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0115 |          20.9988 |          13.0873 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0129 |          20.4720 |          13.0890 |
[32m[20221213 22:38:34 @agent_ppo2.py:185][0m |          -0.0154 |          20.4186 |          13.0790 |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |          -0.0182 |          19.9453 |          13.0874 |
[32m[20221213 22:38:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.67
[32m[20221213 22:38:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.33
[32m[20221213 22:38:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.17
[32m[20221213 22:38:35 @agent_ppo2.py:143][0m Total time:      20.36 min
[32m[20221213 22:38:35 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221213 22:38:35 @agent_ppo2.py:121][0m #------------------------ Iteration 970 --------------------------#
[32m[20221213 22:38:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:38:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |           0.0010 |          36.0349 |          13.1500 |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |          -0.0052 |          33.4213 |          13.1429 |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |           0.0057 |          36.2974 |          13.1339 |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |          -0.0072 |          31.9582 |          13.1217 |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |          -0.0121 |          31.1313 |          13.1354 |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |          -0.0117 |          30.7987 |          13.1212 |
[32m[20221213 22:38:35 @agent_ppo2.py:185][0m |          -0.0100 |          30.3970 |          13.1264 |
[32m[20221213 22:38:36 @agent_ppo2.py:185][0m |          -0.0110 |          30.1690 |          13.1331 |
[32m[20221213 22:38:36 @agent_ppo2.py:185][0m |          -0.0134 |          30.2562 |          13.1193 |
[32m[20221213 22:38:36 @agent_ppo2.py:185][0m |          -0.0088 |          29.9463 |          13.1268 |
[32m[20221213 22:38:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.40
[32m[20221213 22:38:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.87
[32m[20221213 22:38:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.34
[32m[20221213 22:38:36 @agent_ppo2.py:143][0m Total time:      20.38 min
[32m[20221213 22:38:36 @agent_ppo2.py:145][0m 1988608 total steps have happened
[32m[20221213 22:38:36 @agent_ppo2.py:121][0m #------------------------ Iteration 971 --------------------------#
[32m[20221213 22:38:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:36 @agent_ppo2.py:185][0m |           0.0076 |          22.4184 |          13.0574 |
[32m[20221213 22:38:36 @agent_ppo2.py:185][0m |          -0.0051 |          18.8735 |          13.0366 |
[32m[20221213 22:38:36 @agent_ppo2.py:185][0m |          -0.0041 |          18.2231 |          13.0380 |
[32m[20221213 22:38:36 @agent_ppo2.py:185][0m |          -0.0086 |          17.8749 |          13.0365 |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |          -0.0028 |          17.9617 |          13.0361 |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |          -0.0057 |          17.4547 |          13.0236 |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |          -0.0078 |          17.3682 |          13.0231 |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |          -0.0085 |          17.0896 |          13.0352 |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |          -0.0093 |          16.9380 |          13.0301 |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |          -0.0103 |          16.9252 |          13.0237 |
[32m[20221213 22:38:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.21
[32m[20221213 22:38:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.39
[32m[20221213 22:38:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.17
[32m[20221213 22:38:37 @agent_ppo2.py:143][0m Total time:      20.40 min
[32m[20221213 22:38:37 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221213 22:38:37 @agent_ppo2.py:121][0m #------------------------ Iteration 972 --------------------------#
[32m[20221213 22:38:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |           0.0038 |          31.9751 |          13.1648 |
[32m[20221213 22:38:37 @agent_ppo2.py:185][0m |          -0.0049 |          30.1918 |          13.1368 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0078 |          29.6955 |          13.1408 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0095 |          29.4144 |          13.1343 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0110 |          29.3025 |          13.1353 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0084 |          29.1849 |          13.1235 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0118 |          28.9330 |          13.1313 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0106 |          28.9811 |          13.1227 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0094 |          28.7944 |          13.1265 |
[32m[20221213 22:38:38 @agent_ppo2.py:185][0m |          -0.0122 |          28.5999 |          13.1191 |
[32m[20221213 22:38:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.05
[32m[20221213 22:38:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.85
[32m[20221213 22:38:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.80
[32m[20221213 22:38:38 @agent_ppo2.py:143][0m Total time:      20.42 min
[32m[20221213 22:38:38 @agent_ppo2.py:145][0m 1992704 total steps have happened
[32m[20221213 22:38:38 @agent_ppo2.py:121][0m #------------------------ Iteration 973 --------------------------#
[32m[20221213 22:38:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |           0.0000 |          28.4693 |          13.0831 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0010 |          23.9730 |          13.0709 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0096 |          22.6698 |          13.0715 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0060 |          22.1355 |          13.0724 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0014 |          21.9407 |          13.0674 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0087 |          21.3117 |          13.0619 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0045 |          21.1239 |          13.0612 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0093 |          20.9948 |          13.0647 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |           0.0031 |          24.6272 |          13.0671 |
[32m[20221213 22:38:39 @agent_ppo2.py:185][0m |          -0.0112 |          20.4559 |          13.0515 |
[32m[20221213 22:38:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:38:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.17
[32m[20221213 22:38:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.21
[32m[20221213 22:38:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.36
[32m[20221213 22:38:40 @agent_ppo2.py:143][0m Total time:      20.44 min
[32m[20221213 22:38:40 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221213 22:38:40 @agent_ppo2.py:121][0m #------------------------ Iteration 974 --------------------------#
[32m[20221213 22:38:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:38:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |           0.0009 |          31.7203 |          13.0647 |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |          -0.0059 |          30.9967 |          13.0529 |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |          -0.0070 |          30.8026 |          13.0413 |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |          -0.0097 |          30.5542 |          13.0396 |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |          -0.0076 |          30.4797 |          13.0282 |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |          -0.0099 |          30.3068 |          13.0212 |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |          -0.0076 |          30.8811 |          13.0236 |
[32m[20221213 22:38:40 @agent_ppo2.py:185][0m |          -0.0097 |          30.3547 |          13.0041 |
[32m[20221213 22:38:41 @agent_ppo2.py:185][0m |          -0.0105 |          29.9939 |          13.0237 |
[32m[20221213 22:38:41 @agent_ppo2.py:185][0m |          -0.0126 |          29.9724 |          13.0231 |
[32m[20221213 22:38:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:38:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.64
[32m[20221213 22:38:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.95
[32m[20221213 22:38:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.47
[32m[20221213 22:38:41 @agent_ppo2.py:143][0m Total time:      20.46 min
[32m[20221213 22:38:41 @agent_ppo2.py:145][0m 1996800 total steps have happened
[32m[20221213 22:38:41 @agent_ppo2.py:121][0m #------------------------ Iteration 975 --------------------------#
[32m[20221213 22:38:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:38:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:41 @agent_ppo2.py:185][0m |           0.0087 |          35.4501 |          13.3298 |
[32m[20221213 22:38:41 @agent_ppo2.py:185][0m |          -0.0080 |          30.4590 |          13.3135 |
[32m[20221213 22:38:41 @agent_ppo2.py:185][0m |          -0.0067 |          29.0160 |          13.2963 |
[32m[20221213 22:38:41 @agent_ppo2.py:185][0m |          -0.0065 |          27.6262 |          13.2969 |
[32m[20221213 22:38:42 @agent_ppo2.py:185][0m |          -0.0043 |          26.3911 |          13.2988 |
[32m[20221213 22:38:42 @agent_ppo2.py:185][0m |          -0.0134 |          25.2973 |          13.2929 |
[32m[20221213 22:38:42 @agent_ppo2.py:185][0m |          -0.0088 |          24.7245 |          13.2952 |
[32m[20221213 22:38:42 @agent_ppo2.py:185][0m |          -0.0148 |          24.2252 |          13.2935 |
[32m[20221213 22:38:42 @agent_ppo2.py:185][0m |          -0.0135 |          23.8220 |          13.2900 |
[32m[20221213 22:38:42 @agent_ppo2.py:185][0m |          -0.0122 |          23.5884 |          13.2919 |
[32m[20221213 22:38:42 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 22:38:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.49
[32m[20221213 22:38:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.20
[32m[20221213 22:38:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.67
[32m[20221213 22:38:42 @agent_ppo2.py:143][0m Total time:      20.49 min
[32m[20221213 22:38:42 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221213 22:38:42 @agent_ppo2.py:121][0m #------------------------ Iteration 976 --------------------------#
[32m[20221213 22:38:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0027 |          36.9958 |          13.1861 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0100 |          27.5883 |          13.1633 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0048 |          26.4863 |          13.1524 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0146 |          25.6731 |          13.1442 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0155 |          24.9806 |          13.1469 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0098 |          24.6588 |          13.1516 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0159 |          24.3686 |          13.1428 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0124 |          24.1022 |          13.1578 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0127 |          23.8652 |          13.1464 |
[32m[20221213 22:38:43 @agent_ppo2.py:185][0m |          -0.0104 |          23.7066 |          13.1600 |
[32m[20221213 22:38:43 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 22:38:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.31
[32m[20221213 22:38:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.38
[32m[20221213 22:38:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 34.72
[32m[20221213 22:38:44 @agent_ppo2.py:143][0m Total time:      20.51 min
[32m[20221213 22:38:44 @agent_ppo2.py:145][0m 2000896 total steps have happened
[32m[20221213 22:38:44 @agent_ppo2.py:121][0m #------------------------ Iteration 977 --------------------------#
[32m[20221213 22:38:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:38:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:44 @agent_ppo2.py:185][0m |          -0.0004 |          37.6562 |          13.2296 |
[32m[20221213 22:38:44 @agent_ppo2.py:185][0m |          -0.0042 |          33.6174 |          13.2295 |
[32m[20221213 22:38:44 @agent_ppo2.py:185][0m |          -0.0054 |          32.7062 |          13.2390 |
[32m[20221213 22:38:44 @agent_ppo2.py:185][0m |          -0.0044 |          32.1342 |          13.2350 |
[32m[20221213 22:38:44 @agent_ppo2.py:185][0m |          -0.0073 |          31.6177 |          13.2315 |
[32m[20221213 22:38:44 @agent_ppo2.py:185][0m |          -0.0075 |          31.1847 |          13.2220 |
[32m[20221213 22:38:44 @agent_ppo2.py:185][0m |          -0.0092 |          31.0896 |          13.2300 |
[32m[20221213 22:38:45 @agent_ppo2.py:185][0m |          -0.0079 |          30.6163 |          13.2270 |
[32m[20221213 22:38:45 @agent_ppo2.py:185][0m |          -0.0055 |          30.4085 |          13.2316 |
[32m[20221213 22:38:45 @agent_ppo2.py:185][0m |          -0.0022 |          30.5063 |          13.2317 |
[32m[20221213 22:38:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:38:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.60
[32m[20221213 22:38:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.95
[32m[20221213 22:38:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.86
[32m[20221213 22:38:45 @agent_ppo2.py:143][0m Total time:      20.53 min
[32m[20221213 22:38:45 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221213 22:38:45 @agent_ppo2.py:121][0m #------------------------ Iteration 978 --------------------------#
[32m[20221213 22:38:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:45 @agent_ppo2.py:185][0m |          -0.0016 |          35.0495 |          13.2556 |
[32m[20221213 22:38:45 @agent_ppo2.py:185][0m |          -0.0042 |          33.2521 |          13.2376 |
[32m[20221213 22:38:45 @agent_ppo2.py:185][0m |          -0.0063 |          32.7145 |          13.2341 |
[32m[20221213 22:38:45 @agent_ppo2.py:185][0m |          -0.0029 |          33.2822 |          13.2326 |
[32m[20221213 22:38:46 @agent_ppo2.py:185][0m |          -0.0034 |          32.1955 |          13.2260 |
[32m[20221213 22:38:46 @agent_ppo2.py:185][0m |          -0.0015 |          32.7534 |          13.2312 |
[32m[20221213 22:38:46 @agent_ppo2.py:185][0m |          -0.0097 |          31.5521 |          13.2315 |
[32m[20221213 22:38:46 @agent_ppo2.py:185][0m |          -0.0064 |          31.3008 |          13.2176 |
[32m[20221213 22:38:46 @agent_ppo2.py:185][0m |          -0.0084 |          31.1561 |          13.2230 |
[32m[20221213 22:38:46 @agent_ppo2.py:185][0m |          -0.0120 |          31.0344 |          13.2258 |
[32m[20221213 22:38:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:38:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.95
[32m[20221213 22:38:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.07
[32m[20221213 22:38:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.52
[32m[20221213 22:38:46 @agent_ppo2.py:143][0m Total time:      20.55 min
[32m[20221213 22:38:46 @agent_ppo2.py:145][0m 2004992 total steps have happened
[32m[20221213 22:38:46 @agent_ppo2.py:121][0m #------------------------ Iteration 979 --------------------------#
[32m[20221213 22:38:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:46 @agent_ppo2.py:185][0m |          -0.0011 |          35.0005 |          13.2820 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0081 |          32.8659 |          13.2698 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0064 |          32.3378 |          13.2662 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0135 |          31.9126 |          13.2632 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0099 |          31.3723 |          13.2510 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0143 |          31.2211 |          13.2539 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0113 |          31.0348 |          13.2515 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0002 |          32.8073 |          13.2415 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0113 |          30.6874 |          13.2407 |
[32m[20221213 22:38:47 @agent_ppo2.py:185][0m |          -0.0140 |          30.5270 |          13.2472 |
[32m[20221213 22:38:47 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.10
[32m[20221213 22:38:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.75
[32m[20221213 22:38:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.99
[32m[20221213 22:38:47 @agent_ppo2.py:143][0m Total time:      20.57 min
[32m[20221213 22:38:47 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221213 22:38:47 @agent_ppo2.py:121][0m #------------------------ Iteration 980 --------------------------#
[32m[20221213 22:38:48 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:38:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0041 |          31.2485 |          13.2660 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0094 |          29.1722 |          13.2456 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0076 |          28.4505 |          13.2477 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0106 |          28.1753 |          13.2488 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0106 |          27.7772 |          13.2412 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0110 |          27.7671 |          13.2484 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0149 |          27.2593 |          13.2446 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0165 |          27.2447 |          13.2316 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0120 |          26.9578 |          13.2374 |
[32m[20221213 22:38:48 @agent_ppo2.py:185][0m |          -0.0126 |          26.8958 |          13.2434 |
[32m[20221213 22:38:48 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:38:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.82
[32m[20221213 22:38:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.92
[32m[20221213 22:38:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.75
[32m[20221213 22:38:49 @agent_ppo2.py:143][0m Total time:      20.59 min
[32m[20221213 22:38:49 @agent_ppo2.py:145][0m 2009088 total steps have happened
[32m[20221213 22:38:49 @agent_ppo2.py:121][0m #------------------------ Iteration 981 --------------------------#
[32m[20221213 22:38:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |           0.0011 |          29.4231 |          13.2414 |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |          -0.0044 |          27.5347 |          13.2410 |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |          -0.0048 |          27.0240 |          13.2304 |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |          -0.0082 |          26.8403 |          13.2295 |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |          -0.0048 |          26.5055 |          13.2193 |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |          -0.0070 |          26.4369 |          13.2315 |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |          -0.0092 |          26.2675 |          13.2254 |
[32m[20221213 22:38:49 @agent_ppo2.py:185][0m |          -0.0093 |          26.1213 |          13.2140 |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |          -0.0039 |          25.9085 |          13.2160 |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |          -0.0058 |          26.0344 |          13.2164 |
[32m[20221213 22:38:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.65
[32m[20221213 22:38:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.55
[32m[20221213 22:38:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.84
[32m[20221213 22:38:50 @agent_ppo2.py:143][0m Total time:      20.61 min
[32m[20221213 22:38:50 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221213 22:38:50 @agent_ppo2.py:121][0m #------------------------ Iteration 982 --------------------------#
[32m[20221213 22:38:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |           0.0015 |          23.2274 |          13.0699 |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |          -0.0042 |          20.1619 |          13.0498 |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |          -0.0056 |          19.5154 |          13.0501 |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |          -0.0074 |          19.2454 |          13.0579 |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |          -0.0072 |          18.9365 |          13.0441 |
[32m[20221213 22:38:50 @agent_ppo2.py:185][0m |          -0.0097 |          18.6456 |          13.0452 |
[32m[20221213 22:38:51 @agent_ppo2.py:185][0m |          -0.0092 |          18.6361 |          13.0533 |
[32m[20221213 22:38:51 @agent_ppo2.py:185][0m |          -0.0129 |          18.3295 |          13.0460 |
[32m[20221213 22:38:51 @agent_ppo2.py:185][0m |          -0.0118 |          18.1053 |          13.0439 |
[32m[20221213 22:38:51 @agent_ppo2.py:185][0m |          -0.0049 |          18.5142 |          13.0464 |
[32m[20221213 22:38:51 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:38:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.57
[32m[20221213 22:38:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.33
[32m[20221213 22:38:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.56
[32m[20221213 22:38:51 @agent_ppo2.py:143][0m Total time:      20.63 min
[32m[20221213 22:38:51 @agent_ppo2.py:145][0m 2013184 total steps have happened
[32m[20221213 22:38:51 @agent_ppo2.py:121][0m #------------------------ Iteration 983 --------------------------#
[32m[20221213 22:38:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:38:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:51 @agent_ppo2.py:185][0m |          -0.0012 |          21.5709 |          13.1643 |
[32m[20221213 22:38:51 @agent_ppo2.py:185][0m |          -0.0034 |          19.1509 |          13.1442 |
[32m[20221213 22:38:51 @agent_ppo2.py:185][0m |          -0.0045 |          18.4803 |          13.1438 |
[32m[20221213 22:38:52 @agent_ppo2.py:185][0m |          -0.0052 |          18.1182 |          13.1423 |
[32m[20221213 22:38:52 @agent_ppo2.py:185][0m |          -0.0035 |          17.6915 |          13.1414 |
[32m[20221213 22:38:52 @agent_ppo2.py:185][0m |          -0.0091 |          17.4309 |          13.1430 |
[32m[20221213 22:38:52 @agent_ppo2.py:185][0m |          -0.0085 |          17.2612 |          13.1397 |
[32m[20221213 22:38:52 @agent_ppo2.py:185][0m |          -0.0104 |          17.0774 |          13.1317 |
[32m[20221213 22:38:52 @agent_ppo2.py:185][0m |          -0.0075 |          16.9115 |          13.1352 |
[32m[20221213 22:38:52 @agent_ppo2.py:185][0m |          -0.0107 |          16.8232 |          13.1404 |
[32m[20221213 22:38:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:38:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.20
[32m[20221213 22:38:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.72
[32m[20221213 22:38:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.71
[32m[20221213 22:38:52 @agent_ppo2.py:143][0m Total time:      20.66 min
[32m[20221213 22:38:52 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221213 22:38:52 @agent_ppo2.py:121][0m #------------------------ Iteration 984 --------------------------#
[32m[20221213 22:38:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:38:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0028 |          32.5044 |          13.3077 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0082 |          29.5186 |          13.2793 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0076 |          28.7900 |          13.2767 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0050 |          28.3127 |          13.2706 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0085 |          27.8964 |          13.2723 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0074 |          28.0988 |          13.2724 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0084 |          27.5978 |          13.2758 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0056 |          27.6479 |          13.2668 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0107 |          27.1612 |          13.2724 |
[32m[20221213 22:38:53 @agent_ppo2.py:185][0m |          -0.0104 |          26.9488 |          13.2733 |
[32m[20221213 22:38:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:38:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.94
[32m[20221213 22:38:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.29
[32m[20221213 22:38:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.10
[32m[20221213 22:38:53 @agent_ppo2.py:143][0m Total time:      20.68 min
[32m[20221213 22:38:53 @agent_ppo2.py:145][0m 2017280 total steps have happened
[32m[20221213 22:38:53 @agent_ppo2.py:121][0m #------------------------ Iteration 985 --------------------------#
[32m[20221213 22:38:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:38:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |           0.0005 |          32.1029 |          13.4359 |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |          -0.0067 |          29.4246 |          13.4272 |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |          -0.0074 |          28.6587 |          13.4170 |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |          -0.0054 |          28.3379 |          13.4118 |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |          -0.0071 |          28.0996 |          13.4091 |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |          -0.0090 |          27.9536 |          13.4061 |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |          -0.0049 |          27.9974 |          13.4206 |
[32m[20221213 22:38:54 @agent_ppo2.py:185][0m |          -0.0100 |          27.6014 |          13.4176 |
[32m[20221213 22:38:55 @agent_ppo2.py:185][0m |          -0.0086 |          27.5691 |          13.4178 |
[32m[20221213 22:38:55 @agent_ppo2.py:185][0m |          -0.0040 |          28.2162 |          13.4079 |
[32m[20221213 22:38:55 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:38:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.81
[32m[20221213 22:38:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.89
[32m[20221213 22:38:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.79
[32m[20221213 22:38:55 @agent_ppo2.py:143][0m Total time:      20.70 min
[32m[20221213 22:38:55 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221213 22:38:55 @agent_ppo2.py:121][0m #------------------------ Iteration 986 --------------------------#
[32m[20221213 22:38:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:38:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:55 @agent_ppo2.py:185][0m |           0.0047 |          29.3351 |          13.4269 |
[32m[20221213 22:38:55 @agent_ppo2.py:185][0m |          -0.0049 |          24.7750 |          13.4032 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |          -0.0081 |          23.9186 |          13.4022 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |          -0.0058 |          23.5059 |          13.3867 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |           0.0075 |          25.8605 |          13.3993 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |          -0.0061 |          23.2022 |          13.3871 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |          -0.0061 |          22.8227 |          13.4004 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |          -0.0065 |          22.7005 |          13.3929 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |          -0.0086 |          22.5201 |          13.3983 |
[32m[20221213 22:38:56 @agent_ppo2.py:185][0m |          -0.0103 |          22.4749 |          13.3887 |
[32m[20221213 22:38:56 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:38:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.37
[32m[20221213 22:38:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.64
[32m[20221213 22:38:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.04
[32m[20221213 22:38:56 @agent_ppo2.py:143][0m Total time:      20.73 min
[32m[20221213 22:38:56 @agent_ppo2.py:145][0m 2021376 total steps have happened
[32m[20221213 22:38:56 @agent_ppo2.py:121][0m #------------------------ Iteration 987 --------------------------#
[32m[20221213 22:38:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0044 |          26.9007 |          13.2939 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0065 |          24.9575 |          13.2680 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0083 |          24.3550 |          13.2700 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0053 |          24.3920 |          13.2657 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0093 |          23.8355 |          13.2569 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0074 |          23.6779 |          13.2592 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0121 |          23.4638 |          13.2530 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0117 |          23.1400 |          13.2552 |
[32m[20221213 22:38:57 @agent_ppo2.py:185][0m |          -0.0143 |          23.0191 |          13.2502 |
[32m[20221213 22:38:58 @agent_ppo2.py:185][0m |          -0.0126 |          22.9470 |          13.2461 |
[32m[20221213 22:38:58 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:38:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.67
[32m[20221213 22:38:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.16
[32m[20221213 22:38:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.60
[32m[20221213 22:38:58 @agent_ppo2.py:143][0m Total time:      20.75 min
[32m[20221213 22:38:58 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221213 22:38:58 @agent_ppo2.py:121][0m #------------------------ Iteration 988 --------------------------#
[32m[20221213 22:38:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:38:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:58 @agent_ppo2.py:185][0m |           0.0023 |          31.2027 |          13.2186 |
[32m[20221213 22:38:58 @agent_ppo2.py:185][0m |           0.0013 |          28.9894 |          13.2120 |
[32m[20221213 22:38:58 @agent_ppo2.py:185][0m |          -0.0006 |          26.8900 |          13.1963 |
[32m[20221213 22:38:58 @agent_ppo2.py:185][0m |          -0.0086 |          26.1925 |          13.1961 |
[32m[20221213 22:38:58 @agent_ppo2.py:185][0m |          -0.0126 |          25.4859 |          13.1977 |
[32m[20221213 22:38:58 @agent_ppo2.py:185][0m |          -0.0112 |          25.2294 |          13.1983 |
[32m[20221213 22:38:59 @agent_ppo2.py:185][0m |          -0.0124 |          24.7407 |          13.1964 |
[32m[20221213 22:38:59 @agent_ppo2.py:185][0m |          -0.0146 |          24.5136 |          13.2014 |
[32m[20221213 22:38:59 @agent_ppo2.py:185][0m |          -0.0131 |          24.3018 |          13.1968 |
[32m[20221213 22:38:59 @agent_ppo2.py:185][0m |          -0.0142 |          23.8794 |          13.1897 |
[32m[20221213 22:38:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:38:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 177.91
[32m[20221213 22:38:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.07
[32m[20221213 22:38:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.49
[32m[20221213 22:38:59 @agent_ppo2.py:143][0m Total time:      20.77 min
[32m[20221213 22:38:59 @agent_ppo2.py:145][0m 2025472 total steps have happened
[32m[20221213 22:38:59 @agent_ppo2.py:121][0m #------------------------ Iteration 989 --------------------------#
[32m[20221213 22:38:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:38:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:38:59 @agent_ppo2.py:185][0m |           0.0009 |          35.4084 |          13.3138 |
[32m[20221213 22:38:59 @agent_ppo2.py:185][0m |          -0.0064 |          31.5600 |          13.2989 |
[32m[20221213 22:38:59 @agent_ppo2.py:185][0m |          -0.0072 |          30.6545 |          13.2966 |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0090 |          30.0842 |          13.2949 |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0057 |          29.6369 |          13.2933 |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0076 |          29.2215 |          13.2936 |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0101 |          28.8644 |          13.2935 |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0130 |          28.4613 |          13.2966 |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0080 |          28.2728 |          13.2891 |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0155 |          27.9497 |          13.2935 |
[32m[20221213 22:39:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:39:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.99
[32m[20221213 22:39:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.40
[32m[20221213 22:39:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.15
[32m[20221213 22:39:00 @agent_ppo2.py:143][0m Total time:      20.79 min
[32m[20221213 22:39:00 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221213 22:39:00 @agent_ppo2.py:121][0m #------------------------ Iteration 990 --------------------------#
[32m[20221213 22:39:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:00 @agent_ppo2.py:185][0m |          -0.0044 |          29.3056 |          13.3701 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0002 |          27.8852 |          13.3553 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0029 |          27.6413 |          13.3436 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0031 |          27.4084 |          13.3584 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0037 |          27.2095 |          13.3458 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0023 |          27.1040 |          13.3498 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |           0.0104 |          29.8417 |          13.3368 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0013 |          26.9419 |          13.3536 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0049 |          26.8170 |          13.3487 |
[32m[20221213 22:39:01 @agent_ppo2.py:185][0m |          -0.0058 |          26.7817 |          13.3483 |
[32m[20221213 22:39:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:39:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.62
[32m[20221213 22:39:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.20
[32m[20221213 22:39:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.26
[32m[20221213 22:39:01 @agent_ppo2.py:143][0m Total time:      20.81 min
[32m[20221213 22:39:01 @agent_ppo2.py:145][0m 2029568 total steps have happened
[32m[20221213 22:39:01 @agent_ppo2.py:121][0m #------------------------ Iteration 991 --------------------------#
[32m[20221213 22:39:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |           0.0156 |          30.4228 |          13.3020 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0071 |          25.7880 |          13.2776 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |           0.0009 |          27.7207 |          13.2746 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0120 |          24.4049 |          13.2669 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0137 |          23.9521 |          13.2628 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0128 |          23.5780 |          13.2571 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0125 |          23.1475 |          13.2487 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0140 |          22.9094 |          13.2554 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0170 |          22.8536 |          13.2446 |
[32m[20221213 22:39:02 @agent_ppo2.py:185][0m |          -0.0121 |          22.5091 |          13.2495 |
[32m[20221213 22:39:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:39:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.94
[32m[20221213 22:39:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.73
[32m[20221213 22:39:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.12
[32m[20221213 22:39:03 @agent_ppo2.py:143][0m Total time:      20.83 min
[32m[20221213 22:39:03 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221213 22:39:03 @agent_ppo2.py:121][0m #------------------------ Iteration 992 --------------------------#
[32m[20221213 22:39:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:03 @agent_ppo2.py:185][0m |           0.0016 |          20.9462 |          13.2720 |
[32m[20221213 22:39:03 @agent_ppo2.py:185][0m |           0.0106 |          21.0230 |          13.2459 |
[32m[20221213 22:39:03 @agent_ppo2.py:185][0m |          -0.0077 |          16.6288 |          13.2505 |
[32m[20221213 22:39:03 @agent_ppo2.py:185][0m |          -0.0129 |          15.8536 |          13.2509 |
[32m[20221213 22:39:03 @agent_ppo2.py:185][0m |          -0.0124 |          15.3987 |          13.2479 |
[32m[20221213 22:39:03 @agent_ppo2.py:185][0m |          -0.0102 |          15.0744 |          13.2376 |
[32m[20221213 22:39:03 @agent_ppo2.py:185][0m |          -0.0096 |          14.8058 |          13.2429 |
[32m[20221213 22:39:04 @agent_ppo2.py:185][0m |          -0.0166 |          14.6639 |          13.2412 |
[32m[20221213 22:39:04 @agent_ppo2.py:185][0m |          -0.0056 |          14.4688 |          13.2313 |
[32m[20221213 22:39:04 @agent_ppo2.py:185][0m |          -0.0090 |          14.2019 |          13.2359 |
[32m[20221213 22:39:04 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:39:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.30
[32m[20221213 22:39:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.20
[32m[20221213 22:39:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.06
[32m[20221213 22:39:04 @agent_ppo2.py:143][0m Total time:      20.85 min
[32m[20221213 22:39:04 @agent_ppo2.py:145][0m 2033664 total steps have happened
[32m[20221213 22:39:04 @agent_ppo2.py:121][0m #------------------------ Iteration 993 --------------------------#
[32m[20221213 22:39:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:04 @agent_ppo2.py:185][0m |           0.0009 |          34.6845 |          13.1889 |
[32m[20221213 22:39:04 @agent_ppo2.py:185][0m |           0.0018 |          33.4547 |          13.1756 |
[32m[20221213 22:39:04 @agent_ppo2.py:185][0m |          -0.0096 |          32.5999 |          13.1657 |
[32m[20221213 22:39:04 @agent_ppo2.py:185][0m |           0.0071 |          35.9823 |          13.1655 |
[32m[20221213 22:39:05 @agent_ppo2.py:185][0m |          -0.0071 |          32.2442 |          13.1700 |
[32m[20221213 22:39:05 @agent_ppo2.py:185][0m |          -0.0067 |          31.7801 |          13.1715 |
[32m[20221213 22:39:05 @agent_ppo2.py:185][0m |          -0.0099 |          31.7737 |          13.1733 |
[32m[20221213 22:39:05 @agent_ppo2.py:185][0m |          -0.0074 |          31.6600 |          13.1711 |
[32m[20221213 22:39:05 @agent_ppo2.py:185][0m |          -0.0065 |          31.5600 |          13.1699 |
[32m[20221213 22:39:05 @agent_ppo2.py:185][0m |          -0.0096 |          31.4948 |          13.1747 |
[32m[20221213 22:39:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:39:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.06
[32m[20221213 22:39:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.98
[32m[20221213 22:39:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.93
[32m[20221213 22:39:05 @agent_ppo2.py:143][0m Total time:      20.87 min
[32m[20221213 22:39:05 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221213 22:39:05 @agent_ppo2.py:121][0m #------------------------ Iteration 994 --------------------------#
[32m[20221213 22:39:05 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 22:39:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0035 |          33.9466 |          13.2608 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0057 |          31.1455 |          13.2380 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0069 |          29.7153 |          13.2260 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0107 |          28.9163 |          13.2190 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0108 |          28.4642 |          13.2159 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0105 |          28.2108 |          13.2207 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0146 |          27.8949 |          13.2264 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0130 |          27.5699 |          13.2183 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0106 |          27.4035 |          13.2236 |
[32m[20221213 22:39:06 @agent_ppo2.py:185][0m |          -0.0169 |          27.2829 |          13.2135 |
[32m[20221213 22:39:06 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:39:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.72
[32m[20221213 22:39:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.14
[32m[20221213 22:39:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.76
[32m[20221213 22:39:07 @agent_ppo2.py:143][0m Total time:      20.89 min
[32m[20221213 22:39:07 @agent_ppo2.py:145][0m 2037760 total steps have happened
[32m[20221213 22:39:07 @agent_ppo2.py:121][0m #------------------------ Iteration 995 --------------------------#
[32m[20221213 22:39:07 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 22:39:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:07 @agent_ppo2.py:185][0m |          -0.0077 |          18.9490 |          13.1675 |
[32m[20221213 22:39:07 @agent_ppo2.py:185][0m |           0.0021 |          16.8438 |          13.1597 |
[32m[20221213 22:39:07 @agent_ppo2.py:185][0m |          -0.0046 |          16.0722 |          13.1647 |
[32m[20221213 22:39:07 @agent_ppo2.py:185][0m |          -0.0054 |          15.6501 |          13.1626 |
[32m[20221213 22:39:07 @agent_ppo2.py:185][0m |          -0.0070 |          15.4993 |          13.1612 |
[32m[20221213 22:39:08 @agent_ppo2.py:185][0m |          -0.0063 |          15.2982 |          13.1557 |
[32m[20221213 22:39:08 @agent_ppo2.py:185][0m |          -0.0043 |          15.2958 |          13.1506 |
[32m[20221213 22:39:08 @agent_ppo2.py:185][0m |          -0.0153 |          15.0493 |          13.1624 |
[32m[20221213 22:39:08 @agent_ppo2.py:185][0m |          -0.0080 |          14.9780 |          13.1563 |
[32m[20221213 22:39:08 @agent_ppo2.py:185][0m |          -0.0084 |          14.8421 |          13.1503 |
[32m[20221213 22:39:08 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 22:39:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.29
[32m[20221213 22:39:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.38
[32m[20221213 22:39:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.15
[32m[20221213 22:39:08 @agent_ppo2.py:143][0m Total time:      20.92 min
[32m[20221213 22:39:08 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221213 22:39:08 @agent_ppo2.py:121][0m #------------------------ Iteration 996 --------------------------#
[32m[20221213 22:39:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0030 |          27.2001 |          13.5141 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0025 |          25.0269 |          13.4938 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0087 |          24.1763 |          13.4818 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |           0.0003 |          25.5615 |          13.4759 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0117 |          23.5945 |          13.4897 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0127 |          23.2930 |          13.4771 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0130 |          23.1522 |          13.4797 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0099 |          23.0960 |          13.4646 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0129 |          22.9380 |          13.4683 |
[32m[20221213 22:39:09 @agent_ppo2.py:185][0m |          -0.0150 |          22.7500 |          13.4713 |
[32m[20221213 22:39:09 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 22:39:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.29
[32m[20221213 22:39:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.28
[32m[20221213 22:39:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.18
[32m[20221213 22:39:10 @agent_ppo2.py:143][0m Total time:      20.95 min
[32m[20221213 22:39:10 @agent_ppo2.py:145][0m 2041856 total steps have happened
[32m[20221213 22:39:10 @agent_ppo2.py:121][0m #------------------------ Iteration 997 --------------------------#
[32m[20221213 22:39:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:10 @agent_ppo2.py:185][0m |          -0.0006 |          31.4546 |          13.5401 |
[32m[20221213 22:39:10 @agent_ppo2.py:185][0m |          -0.0065 |          27.3812 |          13.5128 |
[32m[20221213 22:39:10 @agent_ppo2.py:185][0m |          -0.0136 |          26.3047 |          13.5205 |
[32m[20221213 22:39:10 @agent_ppo2.py:185][0m |          -0.0055 |          25.7135 |          13.5151 |
[32m[20221213 22:39:10 @agent_ppo2.py:185][0m |          -0.0080 |          25.4074 |          13.5060 |
[32m[20221213 22:39:11 @agent_ppo2.py:185][0m |          -0.0084 |          25.0699 |          13.5077 |
[32m[20221213 22:39:11 @agent_ppo2.py:185][0m |          -0.0093 |          24.9286 |          13.5060 |
[32m[20221213 22:39:11 @agent_ppo2.py:185][0m |          -0.0103 |          24.6418 |          13.5051 |
[32m[20221213 22:39:11 @agent_ppo2.py:185][0m |          -0.0124 |          24.5379 |          13.5085 |
[32m[20221213 22:39:11 @agent_ppo2.py:185][0m |          -0.0168 |          24.3461 |          13.5027 |
[32m[20221213 22:39:11 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 22:39:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.76
[32m[20221213 22:39:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.89
[32m[20221213 22:39:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.49
[32m[20221213 22:39:11 @agent_ppo2.py:143][0m Total time:      20.97 min
[32m[20221213 22:39:11 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221213 22:39:11 @agent_ppo2.py:121][0m #------------------------ Iteration 998 --------------------------#
[32m[20221213 22:39:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:39:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:11 @agent_ppo2.py:185][0m |           0.0023 |          25.0947 |          13.4348 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0033 |          24.1607 |          13.4320 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0058 |          23.6955 |          13.4182 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0091 |          23.4593 |          13.4213 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0001 |          23.9197 |          13.4122 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0098 |          23.2986 |          13.4069 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0109 |          23.2266 |          13.4138 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0091 |          23.1506 |          13.4056 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0072 |          23.0901 |          13.4028 |
[32m[20221213 22:39:12 @agent_ppo2.py:185][0m |          -0.0123 |          22.9884 |          13.3985 |
[32m[20221213 22:39:12 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 22:39:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.59
[32m[20221213 22:39:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.29
[32m[20221213 22:39:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.59
[32m[20221213 22:39:13 @agent_ppo2.py:143][0m Total time:      20.99 min
[32m[20221213 22:39:13 @agent_ppo2.py:145][0m 2045952 total steps have happened
[32m[20221213 22:39:13 @agent_ppo2.py:121][0m #------------------------ Iteration 999 --------------------------#
[32m[20221213 22:39:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:13 @agent_ppo2.py:185][0m |          -0.0018 |          32.0046 |          13.3981 |
[32m[20221213 22:39:13 @agent_ppo2.py:185][0m |          -0.0053 |          29.3982 |          13.3791 |
[32m[20221213 22:39:13 @agent_ppo2.py:185][0m |          -0.0030 |          28.5569 |          13.3772 |
[32m[20221213 22:39:13 @agent_ppo2.py:185][0m |          -0.0087 |          28.0899 |          13.3759 |
[32m[20221213 22:39:13 @agent_ppo2.py:185][0m |          -0.0080 |          27.6252 |          13.3735 |
[32m[20221213 22:39:13 @agent_ppo2.py:185][0m |          -0.0095 |          27.3596 |          13.3819 |
[32m[20221213 22:39:14 @agent_ppo2.py:185][0m |          -0.0057 |          27.3538 |          13.3655 |
[32m[20221213 22:39:14 @agent_ppo2.py:185][0m |          -0.0134 |          26.9359 |          13.3656 |
[32m[20221213 22:39:14 @agent_ppo2.py:185][0m |          -0.0093 |          26.7854 |          13.3671 |
[32m[20221213 22:39:14 @agent_ppo2.py:185][0m |          -0.0102 |          26.5949 |          13.3665 |
[32m[20221213 22:39:14 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 22:39:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.35
[32m[20221213 22:39:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.43
[32m[20221213 22:39:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.04
[32m[20221213 22:39:14 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 440.47
[32m[20221213 22:39:14 @agent_ppo2.py:143][0m Total time:      21.02 min
[32m[20221213 22:39:14 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221213 22:39:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221213 22:39:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:39:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:14 @agent_ppo2.py:185][0m |           0.0016 |          30.6634 |          13.5933 |
[32m[20221213 22:39:14 @agent_ppo2.py:185][0m |           0.0041 |          29.2033 |          13.5660 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0072 |          27.3503 |          13.5589 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0096 |          26.5375 |          13.5556 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0103 |          26.1514 |          13.5524 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0103 |          26.1050 |          13.5430 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0147 |          25.5983 |          13.5520 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0159 |          25.5029 |          13.5536 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0126 |          25.3857 |          13.5447 |
[32m[20221213 22:39:15 @agent_ppo2.py:185][0m |          -0.0126 |          25.0131 |          13.5443 |
[32m[20221213 22:39:15 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:39:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.49
[32m[20221213 22:39:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.79
[32m[20221213 22:39:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.08
[32m[20221213 22:39:15 @agent_ppo2.py:143][0m Total time:      21.04 min
[32m[20221213 22:39:15 @agent_ppo2.py:145][0m 2050048 total steps have happened
[32m[20221213 22:39:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221213 22:39:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |           0.0004 |          34.6409 |          13.5298 |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |          -0.0063 |          32.7228 |          13.5273 |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |          -0.0052 |          32.0253 |          13.5193 |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |           0.0000 |          33.1375 |          13.5021 |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |          -0.0076 |          31.2951 |          13.5057 |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |          -0.0084 |          30.9362 |          13.5056 |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |          -0.0023 |          32.0398 |          13.4995 |
[32m[20221213 22:39:16 @agent_ppo2.py:185][0m |          -0.0066 |          30.5143 |          13.5003 |
[32m[20221213 22:39:17 @agent_ppo2.py:185][0m |          -0.0134 |          30.4010 |          13.4930 |
[32m[20221213 22:39:17 @agent_ppo2.py:185][0m |          -0.0106 |          30.2311 |          13.4967 |
[32m[20221213 22:39:17 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.33
[32m[20221213 22:39:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.08
[32m[20221213 22:39:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.70
[32m[20221213 22:39:17 @agent_ppo2.py:143][0m Total time:      21.06 min
[32m[20221213 22:39:17 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221213 22:39:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221213 22:39:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:17 @agent_ppo2.py:185][0m |          -0.0017 |          38.4523 |          13.4629 |
[32m[20221213 22:39:17 @agent_ppo2.py:185][0m |          -0.0081 |          34.0605 |          13.4571 |
[32m[20221213 22:39:17 @agent_ppo2.py:185][0m |          -0.0068 |          33.1759 |          13.4357 |
[32m[20221213 22:39:17 @agent_ppo2.py:185][0m |          -0.0117 |          32.7335 |          13.4335 |
[32m[20221213 22:39:17 @agent_ppo2.py:185][0m |          -0.0061 |          32.8734 |          13.4291 |
[32m[20221213 22:39:18 @agent_ppo2.py:185][0m |          -0.0101 |          32.2279 |          13.4259 |
[32m[20221213 22:39:18 @agent_ppo2.py:185][0m |          -0.0016 |          33.8573 |          13.4073 |
[32m[20221213 22:39:18 @agent_ppo2.py:185][0m |          -0.0141 |          31.6339 |          13.4164 |
[32m[20221213 22:39:18 @agent_ppo2.py:185][0m |          -0.0109 |          31.5729 |          13.4184 |
[32m[20221213 22:39:18 @agent_ppo2.py:185][0m |          -0.0166 |          31.2388 |          13.4047 |
[32m[20221213 22:39:18 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:39:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.79
[32m[20221213 22:39:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.36
[32m[20221213 22:39:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.69
[32m[20221213 22:39:18 @agent_ppo2.py:143][0m Total time:      21.09 min
[32m[20221213 22:39:18 @agent_ppo2.py:145][0m 2054144 total steps have happened
[32m[20221213 22:39:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221213 22:39:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:18 @agent_ppo2.py:185][0m |           0.0020 |          29.7233 |          13.6488 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0025 |          27.5358 |          13.6414 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0074 |          26.8179 |          13.6421 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0077 |          26.3379 |          13.6363 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0046 |          26.0129 |          13.6351 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0059 |          25.7603 |          13.6260 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0088 |          25.5133 |          13.6260 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0081 |          25.4012 |          13.6239 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0072 |          25.1020 |          13.6154 |
[32m[20221213 22:39:19 @agent_ppo2.py:185][0m |          -0.0101 |          24.9946 |          13.6219 |
[32m[20221213 22:39:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.93
[32m[20221213 22:39:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.80
[32m[20221213 22:39:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.36
[32m[20221213 22:39:20 @agent_ppo2.py:143][0m Total time:      21.11 min
[32m[20221213 22:39:20 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221213 22:39:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221213 22:39:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:20 @agent_ppo2.py:185][0m |          -0.0011 |          27.1248 |          13.3988 |
[32m[20221213 22:39:20 @agent_ppo2.py:185][0m |          -0.0047 |          23.1348 |          13.3867 |
[32m[20221213 22:39:20 @agent_ppo2.py:185][0m |          -0.0093 |          22.0578 |          13.3812 |
[32m[20221213 22:39:20 @agent_ppo2.py:185][0m |          -0.0071 |          21.5641 |          13.3800 |
[32m[20221213 22:39:20 @agent_ppo2.py:185][0m |          -0.0097 |          21.2779 |          13.3711 |
[32m[20221213 22:39:20 @agent_ppo2.py:185][0m |          -0.0034 |          20.9241 |          13.3683 |
[32m[20221213 22:39:20 @agent_ppo2.py:185][0m |          -0.0095 |          20.6424 |          13.3628 |
[32m[20221213 22:39:21 @agent_ppo2.py:185][0m |          -0.0096 |          20.4424 |          13.3663 |
[32m[20221213 22:39:21 @agent_ppo2.py:185][0m |          -0.0120 |          20.2512 |          13.3683 |
[32m[20221213 22:39:21 @agent_ppo2.py:185][0m |          -0.0109 |          20.1883 |          13.3566 |
[32m[20221213 22:39:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:39:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.04
[32m[20221213 22:39:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.70
[32m[20221213 22:39:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.63
[32m[20221213 22:39:21 @agent_ppo2.py:143][0m Total time:      21.13 min
[32m[20221213 22:39:21 @agent_ppo2.py:145][0m 2058240 total steps have happened
[32m[20221213 22:39:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221213 22:39:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:21 @agent_ppo2.py:185][0m |           0.0005 |          29.8673 |          13.5639 |
[32m[20221213 22:39:21 @agent_ppo2.py:185][0m |          -0.0017 |          26.6278 |          13.5429 |
[32m[20221213 22:39:21 @agent_ppo2.py:185][0m |          -0.0048 |          25.5966 |          13.5444 |
[32m[20221213 22:39:22 @agent_ppo2.py:185][0m |          -0.0069 |          25.0962 |          13.5380 |
[32m[20221213 22:39:22 @agent_ppo2.py:185][0m |          -0.0067 |          24.6494 |          13.5449 |
[32m[20221213 22:39:22 @agent_ppo2.py:185][0m |          -0.0050 |          24.4229 |          13.5268 |
[32m[20221213 22:39:22 @agent_ppo2.py:185][0m |          -0.0169 |          24.0641 |          13.5402 |
[32m[20221213 22:39:22 @agent_ppo2.py:185][0m |          -0.0140 |          23.7669 |          13.5351 |
[32m[20221213 22:39:22 @agent_ppo2.py:185][0m |          -0.0148 |          23.5797 |          13.5359 |
[32m[20221213 22:39:22 @agent_ppo2.py:185][0m |          -0.0129 |          23.2072 |          13.5332 |
[32m[20221213 22:39:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.93
[32m[20221213 22:39:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.30
[32m[20221213 22:39:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.81
[32m[20221213 22:39:22 @agent_ppo2.py:143][0m Total time:      21.16 min
[32m[20221213 22:39:22 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221213 22:39:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221213 22:39:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |           0.0053 |          36.8088 |          13.4875 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0022 |          34.3384 |          13.4756 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0065 |          33.2279 |          13.4565 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0082 |          32.6613 |          13.4705 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0100 |          32.0673 |          13.4535 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0098 |          31.7748 |          13.4567 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0110 |          31.3765 |          13.4532 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0109 |          31.0750 |          13.4547 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0109 |          30.7961 |          13.4486 |
[32m[20221213 22:39:23 @agent_ppo2.py:185][0m |          -0.0122 |          30.5961 |          13.4489 |
[32m[20221213 22:39:23 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.18
[32m[20221213 22:39:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.14
[32m[20221213 22:39:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.81
[32m[20221213 22:39:24 @agent_ppo2.py:143][0m Total time:      21.18 min
[32m[20221213 22:39:24 @agent_ppo2.py:145][0m 2062336 total steps have happened
[32m[20221213 22:39:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221213 22:39:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:24 @agent_ppo2.py:185][0m |           0.0009 |          33.2684 |          13.5575 |
[32m[20221213 22:39:24 @agent_ppo2.py:185][0m |           0.0004 |          32.1393 |          13.5319 |
[32m[20221213 22:39:24 @agent_ppo2.py:185][0m |          -0.0087 |          30.4014 |          13.5390 |
[32m[20221213 22:39:24 @agent_ppo2.py:185][0m |          -0.0174 |          29.9175 |          13.5377 |
[32m[20221213 22:39:24 @agent_ppo2.py:185][0m |          -0.0016 |          29.8244 |          13.5159 |
[32m[20221213 22:39:24 @agent_ppo2.py:185][0m |          -0.0106 |          29.4007 |          13.5246 |
[32m[20221213 22:39:25 @agent_ppo2.py:185][0m |          -0.0046 |          30.0156 |          13.5201 |
[32m[20221213 22:39:25 @agent_ppo2.py:185][0m |          -0.0101 |          28.8233 |          13.5169 |
[32m[20221213 22:39:25 @agent_ppo2.py:185][0m |          -0.0099 |          28.7827 |          13.5219 |
[32m[20221213 22:39:25 @agent_ppo2.py:185][0m |          -0.0134 |          28.4547 |          13.5239 |
[32m[20221213 22:39:25 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.38
[32m[20221213 22:39:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.54
[32m[20221213 22:39:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.05
[32m[20221213 22:39:25 @agent_ppo2.py:143][0m Total time:      21.20 min
[32m[20221213 22:39:25 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221213 22:39:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221213 22:39:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:25 @agent_ppo2.py:185][0m |          -0.0022 |          44.6798 |          13.6506 |
[32m[20221213 22:39:25 @agent_ppo2.py:185][0m |          -0.0010 |          41.4996 |          13.6360 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0041 |          41.2145 |          13.6163 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0094 |          39.7668 |          13.6297 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0100 |          39.4688 |          13.6130 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0100 |          39.3272 |          13.6254 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0118 |          38.8932 |          13.6161 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0082 |          38.9366 |          13.6158 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0124 |          38.5936 |          13.6093 |
[32m[20221213 22:39:26 @agent_ppo2.py:185][0m |          -0.0071 |          39.2524 |          13.6026 |
[32m[20221213 22:39:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:39:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.16
[32m[20221213 22:39:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.51
[32m[20221213 22:39:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.94
[32m[20221213 22:39:26 @agent_ppo2.py:143][0m Total time:      21.22 min
[32m[20221213 22:39:26 @agent_ppo2.py:145][0m 2066432 total steps have happened
[32m[20221213 22:39:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221213 22:39:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |           0.0048 |          36.0386 |          13.5661 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0104 |          32.8270 |          13.5445 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0057 |          31.6665 |          13.5405 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0114 |          30.7838 |          13.5273 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0113 |          30.3285 |          13.5347 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0144 |          29.9248 |          13.5280 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0147 |          29.4621 |          13.5274 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0116 |          29.2314 |          13.5235 |
[32m[20221213 22:39:27 @agent_ppo2.py:185][0m |          -0.0179 |          28.8592 |          13.5178 |
[32m[20221213 22:39:28 @agent_ppo2.py:185][0m |          -0.0130 |          28.6924 |          13.5201 |
[32m[20221213 22:39:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.78
[32m[20221213 22:39:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.13
[32m[20221213 22:39:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.07
[32m[20221213 22:39:28 @agent_ppo2.py:143][0m Total time:      21.25 min
[32m[20221213 22:39:28 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221213 22:39:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221213 22:39:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:39:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:28 @agent_ppo2.py:185][0m |          -0.0015 |          28.5040 |          13.7305 |
[32m[20221213 22:39:28 @agent_ppo2.py:185][0m |          -0.0023 |          25.1730 |          13.7130 |
[32m[20221213 22:39:28 @agent_ppo2.py:185][0m |          -0.0059 |          24.1009 |          13.7024 |
[32m[20221213 22:39:28 @agent_ppo2.py:185][0m |          -0.0069 |          23.5268 |          13.7024 |
[32m[20221213 22:39:28 @agent_ppo2.py:185][0m |          -0.0014 |          23.1753 |          13.6999 |
[32m[20221213 22:39:29 @agent_ppo2.py:185][0m |          -0.0065 |          22.8032 |          13.7017 |
[32m[20221213 22:39:29 @agent_ppo2.py:185][0m |          -0.0021 |          23.4850 |          13.6935 |
[32m[20221213 22:39:29 @agent_ppo2.py:185][0m |          -0.0112 |          22.1806 |          13.6947 |
[32m[20221213 22:39:29 @agent_ppo2.py:185][0m |          -0.0132 |          21.8820 |          13.7008 |
[32m[20221213 22:39:29 @agent_ppo2.py:185][0m |          -0.0148 |          21.6621 |          13.6965 |
[32m[20221213 22:39:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:39:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.08
[32m[20221213 22:39:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 357.13
[32m[20221213 22:39:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.95
[32m[20221213 22:39:29 @agent_ppo2.py:143][0m Total time:      21.27 min
[32m[20221213 22:39:29 @agent_ppo2.py:145][0m 2070528 total steps have happened
[32m[20221213 22:39:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221213 22:39:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:29 @agent_ppo2.py:185][0m |           0.0026 |          34.1461 |          13.4817 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0029 |          30.1388 |          13.4726 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0055 |          28.6196 |          13.4586 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0062 |          27.8193 |          13.4666 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0101 |          27.2388 |          13.4632 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0104 |          27.0113 |          13.4586 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0122 |          26.5683 |          13.4644 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0113 |          26.2365 |          13.4613 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0082 |          26.1445 |          13.4649 |
[32m[20221213 22:39:30 @agent_ppo2.py:185][0m |          -0.0113 |          25.7197 |          13.4629 |
[32m[20221213 22:39:30 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.42
[32m[20221213 22:39:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.67
[32m[20221213 22:39:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.63
[32m[20221213 22:39:30 @agent_ppo2.py:143][0m Total time:      21.29 min
[32m[20221213 22:39:30 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221213 22:39:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221213 22:39:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0003 |          39.0345 |          13.5923 |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0022 |          37.0506 |          13.5851 |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0027 |          36.4894 |          13.5856 |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0074 |          36.3415 |          13.5698 |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0054 |          36.2948 |          13.5697 |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0119 |          35.1549 |          13.5671 |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0090 |          34.8563 |          13.5630 |
[32m[20221213 22:39:31 @agent_ppo2.py:185][0m |          -0.0114 |          34.5519 |          13.5638 |
[32m[20221213 22:39:32 @agent_ppo2.py:185][0m |          -0.0106 |          34.1831 |          13.5639 |
[32m[20221213 22:39:32 @agent_ppo2.py:185][0m |          -0.0136 |          34.2910 |          13.5615 |
[32m[20221213 22:39:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.64
[32m[20221213 22:39:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.08
[32m[20221213 22:39:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.94
[32m[20221213 22:39:32 @agent_ppo2.py:143][0m Total time:      21.31 min
[32m[20221213 22:39:32 @agent_ppo2.py:145][0m 2074624 total steps have happened
[32m[20221213 22:39:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221213 22:39:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:32 @agent_ppo2.py:185][0m |           0.0003 |          31.1352 |          13.5495 |
[32m[20221213 22:39:32 @agent_ppo2.py:185][0m |          -0.0058 |          29.0995 |          13.5361 |
[32m[20221213 22:39:32 @agent_ppo2.py:185][0m |          -0.0085 |          28.2561 |          13.5327 |
[32m[20221213 22:39:32 @agent_ppo2.py:185][0m |          -0.0042 |          28.4777 |          13.5076 |
[32m[20221213 22:39:33 @agent_ppo2.py:185][0m |          -0.0073 |          27.5320 |          13.5130 |
[32m[20221213 22:39:33 @agent_ppo2.py:185][0m |          -0.0070 |          27.0133 |          13.5288 |
[32m[20221213 22:39:33 @agent_ppo2.py:185][0m |          -0.0057 |          27.1097 |          13.5239 |
[32m[20221213 22:39:33 @agent_ppo2.py:185][0m |          -0.0144 |          26.5928 |          13.5242 |
[32m[20221213 22:39:33 @agent_ppo2.py:185][0m |          -0.0109 |          26.1523 |          13.5235 |
[32m[20221213 22:39:33 @agent_ppo2.py:185][0m |          -0.0155 |          25.9978 |          13.5166 |
[32m[20221213 22:39:33 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:39:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.61
[32m[20221213 22:39:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.14
[32m[20221213 22:39:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.41
[32m[20221213 22:39:33 @agent_ppo2.py:143][0m Total time:      21.34 min
[32m[20221213 22:39:33 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221213 22:39:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221213 22:39:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0017 |          33.7727 |          13.4232 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0108 |          31.0811 |          13.4119 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0052 |          30.2406 |          13.3955 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0049 |          29.6983 |          13.3942 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0059 |          29.5605 |          13.3928 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0085 |          29.1959 |          13.3971 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0065 |          29.3120 |          13.3907 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0082 |          28.5780 |          13.3833 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0063 |          28.4563 |          13.3784 |
[32m[20221213 22:39:34 @agent_ppo2.py:185][0m |          -0.0090 |          28.3634 |          13.3847 |
[32m[20221213 22:39:34 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:39:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.27
[32m[20221213 22:39:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.83
[32m[20221213 22:39:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.87
[32m[20221213 22:39:35 @agent_ppo2.py:143][0m Total time:      21.36 min
[32m[20221213 22:39:35 @agent_ppo2.py:145][0m 2078720 total steps have happened
[32m[20221213 22:39:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221213 22:39:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:35 @agent_ppo2.py:185][0m |          -0.0031 |          33.5747 |          13.4589 |
[32m[20221213 22:39:35 @agent_ppo2.py:185][0m |          -0.0058 |          31.4800 |          13.4478 |
[32m[20221213 22:39:35 @agent_ppo2.py:185][0m |          -0.0068 |          30.9330 |          13.4510 |
[32m[20221213 22:39:35 @agent_ppo2.py:185][0m |          -0.0036 |          30.9403 |          13.4439 |
[32m[20221213 22:39:35 @agent_ppo2.py:185][0m |          -0.0122 |          30.2508 |          13.4472 |
[32m[20221213 22:39:35 @agent_ppo2.py:185][0m |          -0.0105 |          29.8563 |          13.4355 |
[32m[20221213 22:39:35 @agent_ppo2.py:185][0m |          -0.0103 |          29.6780 |          13.4369 |
[32m[20221213 22:39:36 @agent_ppo2.py:185][0m |          -0.0127 |          29.4618 |          13.4333 |
[32m[20221213 22:39:36 @agent_ppo2.py:185][0m |          -0.0119 |          29.4687 |          13.4410 |
[32m[20221213 22:39:36 @agent_ppo2.py:185][0m |          -0.0128 |          29.1804 |          13.4408 |
[32m[20221213 22:39:36 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:39:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.68
[32m[20221213 22:39:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.57
[32m[20221213 22:39:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.21
[32m[20221213 22:39:36 @agent_ppo2.py:143][0m Total time:      21.38 min
[32m[20221213 22:39:36 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221213 22:39:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221213 22:39:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:36 @agent_ppo2.py:185][0m |           0.0008 |          31.7777 |          13.7697 |
[32m[20221213 22:39:36 @agent_ppo2.py:185][0m |          -0.0063 |          29.2730 |          13.7586 |
[32m[20221213 22:39:36 @agent_ppo2.py:185][0m |          -0.0073 |          28.2819 |          13.7475 |
[32m[20221213 22:39:37 @agent_ppo2.py:185][0m |          -0.0065 |          27.6543 |          13.7422 |
[32m[20221213 22:39:37 @agent_ppo2.py:185][0m |          -0.0086 |          27.1168 |          13.7329 |
[32m[20221213 22:39:37 @agent_ppo2.py:185][0m |          -0.0095 |          26.8181 |          13.7380 |
[32m[20221213 22:39:37 @agent_ppo2.py:185][0m |          -0.0018 |          28.2286 |          13.7313 |
[32m[20221213 22:39:37 @agent_ppo2.py:185][0m |          -0.0117 |          26.4923 |          13.7203 |
[32m[20221213 22:39:37 @agent_ppo2.py:185][0m |          -0.0081 |          26.2080 |          13.7229 |
[32m[20221213 22:39:37 @agent_ppo2.py:185][0m |          -0.0127 |          26.0916 |          13.7235 |
[32m[20221213 22:39:37 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 22:39:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.99
[32m[20221213 22:39:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.13
[32m[20221213 22:39:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.93
[32m[20221213 22:39:37 @agent_ppo2.py:143][0m Total time:      21.41 min
[32m[20221213 22:39:37 @agent_ppo2.py:145][0m 2082816 total steps have happened
[32m[20221213 22:39:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221213 22:39:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0012 |          29.7578 |          13.2945 |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0044 |          27.3247 |          13.2868 |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0069 |          26.3869 |          13.2898 |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0129 |          26.0722 |          13.2837 |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0095 |          25.8718 |          13.2754 |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0125 |          25.3642 |          13.2793 |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0100 |          25.2301 |          13.2803 |
[32m[20221213 22:39:38 @agent_ppo2.py:185][0m |          -0.0109 |          25.0640 |          13.2762 |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0076 |          25.2028 |          13.2754 |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0084 |          24.8572 |          13.2673 |
[32m[20221213 22:39:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:39:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.87
[32m[20221213 22:39:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.18
[32m[20221213 22:39:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.08
[32m[20221213 22:39:39 @agent_ppo2.py:143][0m Total time:      21.43 min
[32m[20221213 22:39:39 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221213 22:39:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221213 22:39:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0023 |          33.7340 |          13.3326 |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0078 |          30.7536 |          13.3199 |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0105 |          29.8784 |          13.3120 |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0090 |          29.3223 |          13.3137 |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0077 |          29.1236 |          13.3049 |
[32m[20221213 22:39:39 @agent_ppo2.py:185][0m |          -0.0068 |          28.6077 |          13.3017 |
[32m[20221213 22:39:40 @agent_ppo2.py:185][0m |          -0.0114 |          28.4262 |          13.3078 |
[32m[20221213 22:39:40 @agent_ppo2.py:185][0m |          -0.0126 |          28.2767 |          13.2945 |
[32m[20221213 22:39:40 @agent_ppo2.py:185][0m |          -0.0075 |          27.9600 |          13.3080 |
[32m[20221213 22:39:40 @agent_ppo2.py:185][0m |          -0.0132 |          27.8045 |          13.2905 |
[32m[20221213 22:39:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:39:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.48
[32m[20221213 22:39:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.28
[32m[20221213 22:39:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.95
[32m[20221213 22:39:40 @agent_ppo2.py:143][0m Total time:      21.45 min
[32m[20221213 22:39:40 @agent_ppo2.py:145][0m 2086912 total steps have happened
[32m[20221213 22:39:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221213 22:39:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:40 @agent_ppo2.py:185][0m |          -0.0022 |          30.6925 |          13.3838 |
[32m[20221213 22:39:40 @agent_ppo2.py:185][0m |          -0.0110 |          28.8712 |          13.3833 |
[32m[20221213 22:39:40 @agent_ppo2.py:185][0m |           0.0020 |          29.2280 |          13.3780 |
[32m[20221213 22:39:41 @agent_ppo2.py:185][0m |          -0.0111 |          27.7681 |          13.3748 |
[32m[20221213 22:39:41 @agent_ppo2.py:185][0m |          -0.0128 |          27.5589 |          13.3738 |
[32m[20221213 22:39:41 @agent_ppo2.py:185][0m |          -0.0148 |          27.4044 |          13.3704 |
[32m[20221213 22:39:41 @agent_ppo2.py:185][0m |          -0.0134 |          27.0668 |          13.3713 |
[32m[20221213 22:39:41 @agent_ppo2.py:185][0m |          -0.0107 |          27.9944 |          13.3716 |
[32m[20221213 22:39:41 @agent_ppo2.py:185][0m |          -0.0113 |          26.8393 |          13.3578 |
[32m[20221213 22:39:41 @agent_ppo2.py:185][0m |          -0.0096 |          26.8051 |          13.3591 |
[32m[20221213 22:39:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:39:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.77
[32m[20221213 22:39:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.25
[32m[20221213 22:39:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.44
[32m[20221213 22:39:41 @agent_ppo2.py:143][0m Total time:      21.47 min
[32m[20221213 22:39:41 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221213 22:39:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221213 22:39:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:39:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |           0.0146 |          39.2981 |          13.4440 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |           0.0095 |          36.1459 |          13.4078 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |           0.0060 |          33.0064 |          13.3863 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |          -0.0049 |          31.6765 |          13.4183 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |          -0.0068 |          31.3992 |          13.4210 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |          -0.0098 |          31.1164 |          13.4223 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |          -0.0092 |          31.0409 |          13.4253 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |          -0.0100 |          30.9122 |          13.4195 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |          -0.0091 |          30.7865 |          13.4190 |
[32m[20221213 22:39:42 @agent_ppo2.py:185][0m |          -0.0072 |          30.7713 |          13.4207 |
[32m[20221213 22:39:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:39:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.24
[32m[20221213 22:39:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.02
[32m[20221213 22:39:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.18
[32m[20221213 22:39:42 @agent_ppo2.py:143][0m Total time:      21.49 min
[32m[20221213 22:39:42 @agent_ppo2.py:145][0m 2091008 total steps have happened
[32m[20221213 22:39:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221213 22:39:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |           0.0095 |          42.2775 |          13.4305 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0051 |          36.1139 |          13.4110 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0083 |          34.6156 |          13.4137 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0110 |          33.7672 |          13.4022 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0107 |          33.3109 |          13.4071 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0149 |          33.1049 |          13.4081 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0130 |          32.7137 |          13.3997 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0153 |          32.6081 |          13.3981 |
[32m[20221213 22:39:43 @agent_ppo2.py:185][0m |          -0.0097 |          32.1374 |          13.4018 |
[32m[20221213 22:39:44 @agent_ppo2.py:185][0m |          -0.0167 |          31.9876 |          13.4003 |
[32m[20221213 22:39:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:39:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.81
[32m[20221213 22:39:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.36
[32m[20221213 22:39:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.63
[32m[20221213 22:39:44 @agent_ppo2.py:143][0m Total time:      21.51 min
[32m[20221213 22:39:44 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221213 22:39:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221213 22:39:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:44 @agent_ppo2.py:185][0m |          -0.0022 |          20.2942 |          13.5733 |
[32m[20221213 22:39:44 @agent_ppo2.py:185][0m |          -0.0074 |          16.5564 |          13.5503 |
[32m[20221213 22:39:44 @agent_ppo2.py:185][0m |          -0.0119 |          16.0177 |          13.5579 |
[32m[20221213 22:39:44 @agent_ppo2.py:185][0m |          -0.0004 |          16.3873 |          13.5453 |
[32m[20221213 22:39:44 @agent_ppo2.py:185][0m |          -0.0141 |          15.0775 |          13.5342 |
[32m[20221213 22:39:44 @agent_ppo2.py:185][0m |          -0.0119 |          14.8922 |          13.5369 |
[32m[20221213 22:39:45 @agent_ppo2.py:185][0m |          -0.0077 |          14.6648 |          13.5358 |
[32m[20221213 22:39:45 @agent_ppo2.py:185][0m |          -0.0128 |          14.4020 |          13.5361 |
[32m[20221213 22:39:45 @agent_ppo2.py:185][0m |          -0.0125 |          14.3406 |          13.5438 |
[32m[20221213 22:39:45 @agent_ppo2.py:185][0m |          -0.0162 |          14.1433 |          13.5347 |
[32m[20221213 22:39:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:39:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.14
[32m[20221213 22:39:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.87
[32m[20221213 22:39:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.34
[32m[20221213 22:39:45 @agent_ppo2.py:143][0m Total time:      21.53 min
[32m[20221213 22:39:45 @agent_ppo2.py:145][0m 2095104 total steps have happened
[32m[20221213 22:39:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221213 22:39:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:45 @agent_ppo2.py:185][0m |           0.0056 |          30.1242 |          13.6278 |
[32m[20221213 22:39:45 @agent_ppo2.py:185][0m |          -0.0079 |          26.1871 |          13.6044 |
[32m[20221213 22:39:45 @agent_ppo2.py:185][0m |          -0.0072 |          25.7404 |          13.6143 |
[32m[20221213 22:39:46 @agent_ppo2.py:185][0m |          -0.0128 |          25.1783 |          13.6141 |
[32m[20221213 22:39:46 @agent_ppo2.py:185][0m |          -0.0129 |          24.8629 |          13.5996 |
[32m[20221213 22:39:46 @agent_ppo2.py:185][0m |          -0.0071 |          24.6252 |          13.6067 |
[32m[20221213 22:39:46 @agent_ppo2.py:185][0m |          -0.0057 |          24.6155 |          13.5967 |
[32m[20221213 22:39:46 @agent_ppo2.py:185][0m |          -0.0045 |          25.3254 |          13.6042 |
[32m[20221213 22:39:46 @agent_ppo2.py:185][0m |          -0.0086 |          24.5975 |          13.6042 |
[32m[20221213 22:39:46 @agent_ppo2.py:185][0m |          -0.0096 |          24.1486 |          13.5989 |
[32m[20221213 22:39:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:39:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.36
[32m[20221213 22:39:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.20
[32m[20221213 22:39:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.77
[32m[20221213 22:39:46 @agent_ppo2.py:143][0m Total time:      21.56 min
[32m[20221213 22:39:46 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221213 22:39:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221213 22:39:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0019 |          22.9533 |          13.4299 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0068 |          20.6392 |          13.4032 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |           0.0029 |          20.5979 |          13.4070 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0070 |          19.2862 |          13.3849 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0048 |          18.9563 |          13.3950 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0138 |          18.6395 |          13.3844 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0072 |          18.6397 |          13.3902 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0106 |          18.3564 |          13.3873 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0126 |          18.1719 |          13.3806 |
[32m[20221213 22:39:47 @agent_ppo2.py:185][0m |          -0.0020 |          19.0817 |          13.3770 |
[32m[20221213 22:39:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:39:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.36
[32m[20221213 22:39:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.21
[32m[20221213 22:39:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.27
[32m[20221213 22:39:47 @agent_ppo2.py:143][0m Total time:      21.58 min
[32m[20221213 22:39:47 @agent_ppo2.py:145][0m 2099200 total steps have happened
[32m[20221213 22:39:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221213 22:39:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0020 |          30.0069 |          13.4939 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0042 |          27.4466 |          13.4716 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0066 |          26.6984 |          13.4682 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0046 |          26.5351 |          13.4594 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0109 |          25.9789 |          13.4556 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0108 |          25.8893 |          13.4671 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0072 |          25.7910 |          13.4591 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0102 |          25.5803 |          13.4540 |
[32m[20221213 22:39:48 @agent_ppo2.py:185][0m |          -0.0073 |          25.3834 |          13.4569 |
[32m[20221213 22:39:49 @agent_ppo2.py:185][0m |          -0.0083 |          25.2667 |          13.4598 |
[32m[20221213 22:39:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:39:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.86
[32m[20221213 22:39:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.42
[32m[20221213 22:39:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.06
[32m[20221213 22:39:49 @agent_ppo2.py:143][0m Total time:      21.60 min
[32m[20221213 22:39:49 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221213 22:39:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221213 22:39:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:49 @agent_ppo2.py:185][0m |          -0.0007 |          36.1116 |          13.6057 |
[32m[20221213 22:39:49 @agent_ppo2.py:185][0m |           0.0041 |          36.7730 |          13.5839 |
[32m[20221213 22:39:49 @agent_ppo2.py:185][0m |           0.0010 |          34.8894 |          13.5783 |
[32m[20221213 22:39:49 @agent_ppo2.py:185][0m |          -0.0086 |          33.8096 |          13.5628 |
[32m[20221213 22:39:49 @agent_ppo2.py:185][0m |          -0.0078 |          33.6602 |          13.5696 |
[32m[20221213 22:39:49 @agent_ppo2.py:185][0m |          -0.0111 |          33.4148 |          13.5636 |
[32m[20221213 22:39:50 @agent_ppo2.py:185][0m |          -0.0104 |          33.1241 |          13.5562 |
[32m[20221213 22:39:50 @agent_ppo2.py:185][0m |          -0.0071 |          33.0747 |          13.5552 |
[32m[20221213 22:39:50 @agent_ppo2.py:185][0m |          -0.0106 |          32.6894 |          13.5596 |
[32m[20221213 22:39:50 @agent_ppo2.py:185][0m |          -0.0146 |          32.5051 |          13.5598 |
[32m[20221213 22:39:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:39:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.01
[32m[20221213 22:39:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.06
[32m[20221213 22:39:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.02
[32m[20221213 22:39:50 @agent_ppo2.py:143][0m Total time:      21.62 min
[32m[20221213 22:39:50 @agent_ppo2.py:145][0m 2103296 total steps have happened
[32m[20221213 22:39:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221213 22:39:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:50 @agent_ppo2.py:185][0m |          -0.0006 |          26.4672 |          13.5033 |
[32m[20221213 22:39:50 @agent_ppo2.py:185][0m |          -0.0068 |          25.2071 |          13.5085 |
[32m[20221213 22:39:50 @agent_ppo2.py:185][0m |          -0.0067 |          24.8358 |          13.4965 |
[32m[20221213 22:39:51 @agent_ppo2.py:185][0m |          -0.0068 |          24.7662 |          13.4962 |
[32m[20221213 22:39:51 @agent_ppo2.py:185][0m |          -0.0094 |          24.6690 |          13.4941 |
[32m[20221213 22:39:51 @agent_ppo2.py:185][0m |          -0.0104 |          24.5809 |          13.4852 |
[32m[20221213 22:39:51 @agent_ppo2.py:185][0m |          -0.0068 |          24.4835 |          13.5018 |
[32m[20221213 22:39:51 @agent_ppo2.py:185][0m |          -0.0111 |          24.3320 |          13.4951 |
[32m[20221213 22:39:51 @agent_ppo2.py:185][0m |          -0.0117 |          24.3162 |          13.4881 |
[32m[20221213 22:39:51 @agent_ppo2.py:185][0m |          -0.0094 |          24.2371 |          13.5038 |
[32m[20221213 22:39:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:39:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.41
[32m[20221213 22:39:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.20
[32m[20221213 22:39:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.60
[32m[20221213 22:39:51 @agent_ppo2.py:143][0m Total time:      21.64 min
[32m[20221213 22:39:51 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221213 22:39:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221213 22:39:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0012 |          26.9494 |          13.4789 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0036 |          24.7972 |          13.4590 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0078 |          24.0637 |          13.4631 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0023 |          23.6633 |          13.4628 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0076 |          23.2014 |          13.4502 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0107 |          22.9460 |          13.4559 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0055 |          22.7675 |          13.4496 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0097 |          22.5259 |          13.4504 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0123 |          22.3528 |          13.4517 |
[32m[20221213 22:39:52 @agent_ppo2.py:185][0m |          -0.0147 |          22.1817 |          13.4506 |
[32m[20221213 22:39:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:39:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.75
[32m[20221213 22:39:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.37
[32m[20221213 22:39:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.06
[32m[20221213 22:39:52 @agent_ppo2.py:143][0m Total time:      21.66 min
[32m[20221213 22:39:52 @agent_ppo2.py:145][0m 2107392 total steps have happened
[32m[20221213 22:39:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221213 22:39:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:39:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |          -0.0009 |          25.1150 |          13.4272 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |           0.0009 |          21.0181 |          13.4005 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |           0.0014 |          20.1270 |          13.4031 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |          -0.0072 |          19.6008 |          13.4041 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |          -0.0090 |          19.3299 |          13.3945 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |          -0.0035 |          18.9786 |          13.3991 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |          -0.0079 |          18.8538 |          13.3798 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |          -0.0115 |          18.5429 |          13.3995 |
[32m[20221213 22:39:53 @agent_ppo2.py:185][0m |          -0.0103 |          18.4851 |          13.3884 |
[32m[20221213 22:39:54 @agent_ppo2.py:185][0m |          -0.0071 |          18.2414 |          13.3926 |
[32m[20221213 22:39:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:39:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.84
[32m[20221213 22:39:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.01
[32m[20221213 22:39:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 171.06
[32m[20221213 22:39:54 @agent_ppo2.py:143][0m Total time:      21.68 min
[32m[20221213 22:39:54 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221213 22:39:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221213 22:39:54 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:39:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:54 @agent_ppo2.py:185][0m |           0.0001 |          35.1600 |          13.5417 |
[32m[20221213 22:39:54 @agent_ppo2.py:185][0m |          -0.0063 |          32.3600 |          13.5131 |
[32m[20221213 22:39:54 @agent_ppo2.py:185][0m |          -0.0033 |          31.3449 |          13.5211 |
[32m[20221213 22:39:54 @agent_ppo2.py:185][0m |          -0.0072 |          31.0935 |          13.5195 |
[32m[20221213 22:39:54 @agent_ppo2.py:185][0m |          -0.0070 |          30.6133 |          13.5081 |
[32m[20221213 22:39:54 @agent_ppo2.py:185][0m |          -0.0050 |          30.4835 |          13.5127 |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0120 |          30.2610 |          13.5043 |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0072 |          30.1243 |          13.5073 |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0091 |          30.5226 |          13.5091 |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0090 |          29.9373 |          13.4976 |
[32m[20221213 22:39:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:39:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.34
[32m[20221213 22:39:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.50
[32m[20221213 22:39:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.21
[32m[20221213 22:39:55 @agent_ppo2.py:143][0m Total time:      21.70 min
[32m[20221213 22:39:55 @agent_ppo2.py:145][0m 2111488 total steps have happened
[32m[20221213 22:39:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221213 22:39:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0031 |          28.0846 |          13.6648 |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0033 |          25.0126 |          13.6673 |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0093 |          24.0508 |          13.6568 |
[32m[20221213 22:39:55 @agent_ppo2.py:185][0m |          -0.0115 |          23.4191 |          13.6489 |
[32m[20221213 22:39:56 @agent_ppo2.py:185][0m |          -0.0046 |          23.2241 |          13.6603 |
[32m[20221213 22:39:56 @agent_ppo2.py:185][0m |          -0.0052 |          22.8739 |          13.6505 |
[32m[20221213 22:39:56 @agent_ppo2.py:185][0m |          -0.0069 |          22.7591 |          13.6558 |
[32m[20221213 22:39:56 @agent_ppo2.py:185][0m |          -0.0079 |          22.6777 |          13.6447 |
[32m[20221213 22:39:56 @agent_ppo2.py:185][0m |          -0.0060 |          22.3288 |          13.6489 |
[32m[20221213 22:39:56 @agent_ppo2.py:185][0m |          -0.0092 |          22.5238 |          13.6536 |
[32m[20221213 22:39:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:39:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.94
[32m[20221213 22:39:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.71
[32m[20221213 22:39:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.97
[32m[20221213 22:39:56 @agent_ppo2.py:143][0m Total time:      21.72 min
[32m[20221213 22:39:56 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221213 22:39:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221213 22:39:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:56 @agent_ppo2.py:185][0m |           0.0133 |          28.4807 |          13.6093 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0018 |          24.2980 |          13.5871 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0043 |          23.1029 |          13.5841 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0062 |          22.2974 |          13.5847 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0072 |          21.7080 |          13.5837 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0058 |          21.2687 |          13.5821 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0046 |          21.2272 |          13.5842 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0092 |          20.4996 |          13.5822 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0115 |          20.0279 |          13.5742 |
[32m[20221213 22:39:57 @agent_ppo2.py:185][0m |          -0.0116 |          19.7107 |          13.5807 |
[32m[20221213 22:39:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:39:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.45
[32m[20221213 22:39:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.40
[32m[20221213 22:39:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.32
[32m[20221213 22:39:57 @agent_ppo2.py:143][0m Total time:      21.74 min
[32m[20221213 22:39:57 @agent_ppo2.py:145][0m 2115584 total steps have happened
[32m[20221213 22:39:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221213 22:39:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |           0.0023 |          27.8946 |          13.5851 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0020 |          25.3486 |          13.5814 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0091 |          23.9234 |          13.5826 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0065 |          23.2866 |          13.5781 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0132 |          22.8043 |          13.5845 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0122 |          22.5925 |          13.5776 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0104 |          22.3994 |          13.5788 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0099 |          22.1109 |          13.5768 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0123 |          21.8671 |          13.5748 |
[32m[20221213 22:39:58 @agent_ppo2.py:185][0m |          -0.0161 |          21.7298 |          13.5797 |
[32m[20221213 22:39:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:39:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.25
[32m[20221213 22:39:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 318.49
[32m[20221213 22:39:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.66
[32m[20221213 22:39:59 @agent_ppo2.py:143][0m Total time:      21.76 min
[32m[20221213 22:39:59 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221213 22:39:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221213 22:39:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:39:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |           0.0004 |          39.1034 |          13.4680 |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |          -0.0027 |          37.6402 |          13.4639 |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |          -0.0086 |          37.1176 |          13.4567 |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |          -0.0050 |          36.5999 |          13.4524 |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |          -0.0087 |          36.3215 |          13.4511 |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |          -0.0071 |          36.0959 |          13.4452 |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |          -0.0054 |          35.9865 |          13.4330 |
[32m[20221213 22:39:59 @agent_ppo2.py:185][0m |          -0.0075 |          35.6691 |          13.4371 |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |          -0.0104 |          35.3704 |          13.4391 |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |          -0.0150 |          35.3769 |          13.4361 |
[32m[20221213 22:40:00 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:40:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.60
[32m[20221213 22:40:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.60
[32m[20221213 22:40:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.75
[32m[20221213 22:40:00 @agent_ppo2.py:143][0m Total time:      21.78 min
[32m[20221213 22:40:00 @agent_ppo2.py:145][0m 2119680 total steps have happened
[32m[20221213 22:40:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221213 22:40:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |           0.0024 |          29.6631 |          13.5273 |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |          -0.0018 |          28.3617 |          13.5241 |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |          -0.0070 |          27.9513 |          13.5235 |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |          -0.0055 |          27.6853 |          13.5143 |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |          -0.0079 |          27.4356 |          13.5180 |
[32m[20221213 22:40:00 @agent_ppo2.py:185][0m |          -0.0110 |          27.3925 |          13.5158 |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0130 |          27.3209 |          13.5189 |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0129 |          27.1558 |          13.5157 |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0106 |          27.3365 |          13.5178 |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0077 |          26.9452 |          13.5148 |
[32m[20221213 22:40:01 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:40:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.82
[32m[20221213 22:40:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.35
[32m[20221213 22:40:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.68
[32m[20221213 22:40:01 @agent_ppo2.py:143][0m Total time:      21.80 min
[32m[20221213 22:40:01 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221213 22:40:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221213 22:40:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0001 |          33.7218 |          13.7518 |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0091 |          28.4345 |          13.7294 |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0094 |          27.0962 |          13.7236 |
[32m[20221213 22:40:01 @agent_ppo2.py:185][0m |          -0.0073 |          26.1661 |          13.7150 |
[32m[20221213 22:40:02 @agent_ppo2.py:185][0m |          -0.0170 |          25.5346 |          13.7297 |
[32m[20221213 22:40:02 @agent_ppo2.py:185][0m |          -0.0118 |          25.1352 |          13.7263 |
[32m[20221213 22:40:02 @agent_ppo2.py:185][0m |          -0.0134 |          24.8027 |          13.7182 |
[32m[20221213 22:40:02 @agent_ppo2.py:185][0m |          -0.0123 |          24.7539 |          13.7259 |
[32m[20221213 22:40:02 @agent_ppo2.py:185][0m |          -0.0135 |          24.2255 |          13.7104 |
[32m[20221213 22:40:02 @agent_ppo2.py:185][0m |          -0.0152 |          24.0043 |          13.7078 |
[32m[20221213 22:40:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.07
[32m[20221213 22:40:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.03
[32m[20221213 22:40:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.56
[32m[20221213 22:40:02 @agent_ppo2.py:143][0m Total time:      21.82 min
[32m[20221213 22:40:02 @agent_ppo2.py:145][0m 2123776 total steps have happened
[32m[20221213 22:40:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221213 22:40:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:02 @agent_ppo2.py:185][0m |          -0.0032 |          33.5193 |          13.5925 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0077 |          27.6575 |          13.5775 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0103 |          25.6895 |          13.5687 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |           0.0029 |          25.4415 |          13.5619 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0102 |          24.0496 |          13.5590 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0092 |          23.3507 |          13.5605 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0121 |          22.6770 |          13.5494 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0073 |          22.1824 |          13.5560 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0111 |          21.8658 |          13.5524 |
[32m[20221213 22:40:03 @agent_ppo2.py:185][0m |          -0.0170 |          21.7982 |          13.5548 |
[32m[20221213 22:40:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:40:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.74
[32m[20221213 22:40:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.62
[32m[20221213 22:40:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.37
[32m[20221213 22:40:03 @agent_ppo2.py:143][0m Total time:      21.84 min
[32m[20221213 22:40:03 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221213 22:40:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1038 --------------------------#
[32m[20221213 22:40:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0033 |          34.9029 |          13.6673 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0051 |          32.0797 |          13.6641 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0086 |          31.2241 |          13.6709 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0085 |          30.2227 |          13.6611 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0094 |          29.8998 |          13.6585 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0086 |          29.7379 |          13.6456 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0069 |          29.4800 |          13.6610 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0096 |          29.4568 |          13.6621 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0070 |          28.8709 |          13.6590 |
[32m[20221213 22:40:04 @agent_ppo2.py:185][0m |          -0.0127 |          28.5856 |          13.6481 |
[32m[20221213 22:40:04 @agent_ppo2.py:130][0m Policy update time: 0.89 s
[32m[20221213 22:40:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.07
[32m[20221213 22:40:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.59
[32m[20221213 22:40:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.36
[32m[20221213 22:40:05 @agent_ppo2.py:143][0m Total time:      21.86 min
[32m[20221213 22:40:05 @agent_ppo2.py:145][0m 2127872 total steps have happened
[32m[20221213 22:40:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1039 --------------------------#
[32m[20221213 22:40:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |           0.0127 |          43.6405 |          13.7542 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0054 |          34.9542 |          13.7312 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0125 |          33.8240 |          13.7322 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0026 |          33.5699 |          13.7244 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0076 |          32.6046 |          13.7164 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0087 |          32.3319 |          13.7249 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0112 |          32.1103 |          13.7205 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0107 |          31.8090 |          13.7057 |
[32m[20221213 22:40:05 @agent_ppo2.py:185][0m |          -0.0121 |          31.6099 |          13.7079 |
[32m[20221213 22:40:06 @agent_ppo2.py:185][0m |          -0.0149 |          31.3656 |          13.7032 |
[32m[20221213 22:40:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.43
[32m[20221213 22:40:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.10
[32m[20221213 22:40:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.50
[32m[20221213 22:40:06 @agent_ppo2.py:143][0m Total time:      21.88 min
[32m[20221213 22:40:06 @agent_ppo2.py:145][0m 2129920 total steps have happened
[32m[20221213 22:40:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1040 --------------------------#
[32m[20221213 22:40:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:40:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:06 @agent_ppo2.py:185][0m |          -0.0056 |          34.7077 |          13.7545 |
[32m[20221213 22:40:06 @agent_ppo2.py:185][0m |          -0.0010 |          31.7773 |          13.7411 |
[32m[20221213 22:40:06 @agent_ppo2.py:185][0m |          -0.0088 |          30.9887 |          13.7374 |
[32m[20221213 22:40:06 @agent_ppo2.py:185][0m |          -0.0067 |          30.0418 |          13.7294 |
[32m[20221213 22:40:06 @agent_ppo2.py:185][0m |          -0.0072 |          30.0785 |          13.7377 |
[32m[20221213 22:40:06 @agent_ppo2.py:185][0m |           0.0018 |          33.5802 |          13.7204 |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |          -0.0084 |          29.2383 |          13.7110 |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |          -0.0090 |          28.7883 |          13.7182 |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |          -0.0112 |          28.7815 |          13.7154 |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |          -0.0122 |          28.5657 |          13.7239 |
[32m[20221213 22:40:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.84
[32m[20221213 22:40:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.33
[32m[20221213 22:40:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.73
[32m[20221213 22:40:07 @agent_ppo2.py:143][0m Total time:      21.90 min
[32m[20221213 22:40:07 @agent_ppo2.py:145][0m 2131968 total steps have happened
[32m[20221213 22:40:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1041 --------------------------#
[32m[20221213 22:40:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |           0.0031 |          36.9667 |          13.6044 |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |          -0.0062 |          34.0158 |          13.5856 |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |          -0.0076 |          33.4430 |          13.5775 |
[32m[20221213 22:40:07 @agent_ppo2.py:185][0m |          -0.0079 |          33.0938 |          13.5823 |
[32m[20221213 22:40:08 @agent_ppo2.py:185][0m |          -0.0066 |          32.8724 |          13.5813 |
[32m[20221213 22:40:08 @agent_ppo2.py:185][0m |          -0.0130 |          32.6288 |          13.5845 |
[32m[20221213 22:40:08 @agent_ppo2.py:185][0m |          -0.0092 |          32.1990 |          13.5811 |
[32m[20221213 22:40:08 @agent_ppo2.py:185][0m |          -0.0088 |          32.1019 |          13.5833 |
[32m[20221213 22:40:08 @agent_ppo2.py:185][0m |          -0.0147 |          31.8482 |          13.5756 |
[32m[20221213 22:40:08 @agent_ppo2.py:185][0m |          -0.0078 |          35.3442 |          13.5743 |
[32m[20221213 22:40:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.94
[32m[20221213 22:40:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.60
[32m[20221213 22:40:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.32
[32m[20221213 22:40:08 @agent_ppo2.py:143][0m Total time:      21.92 min
[32m[20221213 22:40:08 @agent_ppo2.py:145][0m 2134016 total steps have happened
[32m[20221213 22:40:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1042 --------------------------#
[32m[20221213 22:40:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:08 @agent_ppo2.py:185][0m |          -0.0014 |          36.8677 |          13.5732 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0078 |          35.5354 |          13.5623 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0054 |          34.9207 |          13.5549 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0075 |          34.6471 |          13.5470 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0105 |          34.6522 |          13.5405 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0109 |          34.4257 |          13.5472 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0123 |          34.2377 |          13.5437 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0089 |          34.0034 |          13.5393 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0079 |          34.3669 |          13.5422 |
[32m[20221213 22:40:09 @agent_ppo2.py:185][0m |          -0.0127 |          33.8902 |          13.5359 |
[32m[20221213 22:40:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:40:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.40
[32m[20221213 22:40:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.82
[32m[20221213 22:40:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.05
[32m[20221213 22:40:09 @agent_ppo2.py:143][0m Total time:      21.94 min
[32m[20221213 22:40:09 @agent_ppo2.py:145][0m 2136064 total steps have happened
[32m[20221213 22:40:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1043 --------------------------#
[32m[20221213 22:40:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0029 |          32.4286 |          13.6937 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0085 |          29.1284 |          13.6729 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0068 |          28.3185 |          13.6700 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0059 |          28.6640 |          13.6604 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0084 |          27.6413 |          13.6590 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0094 |          27.4141 |          13.6573 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0113 |          27.1749 |          13.6516 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0111 |          27.0027 |          13.6465 |
[32m[20221213 22:40:10 @agent_ppo2.py:185][0m |          -0.0118 |          26.9227 |          13.6535 |
[32m[20221213 22:40:11 @agent_ppo2.py:185][0m |          -0.0128 |          26.9053 |          13.6493 |
[32m[20221213 22:40:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:40:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.56
[32m[20221213 22:40:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.27
[32m[20221213 22:40:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.39
[32m[20221213 22:40:11 @agent_ppo2.py:143][0m Total time:      21.96 min
[32m[20221213 22:40:11 @agent_ppo2.py:145][0m 2138112 total steps have happened
[32m[20221213 22:40:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1044 --------------------------#
[32m[20221213 22:40:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:11 @agent_ppo2.py:185][0m |           0.0006 |          30.7774 |          13.6484 |
[32m[20221213 22:40:11 @agent_ppo2.py:185][0m |           0.0046 |          30.8242 |          13.6317 |
[32m[20221213 22:40:11 @agent_ppo2.py:185][0m |          -0.0074 |          27.5424 |          13.6317 |
[32m[20221213 22:40:11 @agent_ppo2.py:185][0m |          -0.0113 |          27.0338 |          13.6361 |
[32m[20221213 22:40:11 @agent_ppo2.py:185][0m |          -0.0137 |          26.7019 |          13.6277 |
[32m[20221213 22:40:11 @agent_ppo2.py:185][0m |          -0.0135 |          26.5203 |          13.6169 |
[32m[20221213 22:40:12 @agent_ppo2.py:185][0m |          -0.0137 |          26.3432 |          13.6222 |
[32m[20221213 22:40:12 @agent_ppo2.py:185][0m |          -0.0085 |          26.6345 |          13.6218 |
[32m[20221213 22:40:12 @agent_ppo2.py:185][0m |          -0.0129 |          26.0181 |          13.6053 |
[32m[20221213 22:40:12 @agent_ppo2.py:185][0m |          -0.0125 |          25.9077 |          13.6060 |
[32m[20221213 22:40:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:40:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.17
[32m[20221213 22:40:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.42
[32m[20221213 22:40:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.12
[32m[20221213 22:40:12 @agent_ppo2.py:143][0m Total time:      21.98 min
[32m[20221213 22:40:12 @agent_ppo2.py:145][0m 2140160 total steps have happened
[32m[20221213 22:40:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1045 --------------------------#
[32m[20221213 22:40:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:12 @agent_ppo2.py:185][0m |           0.0073 |          30.3673 |          13.5203 |
[32m[20221213 22:40:12 @agent_ppo2.py:185][0m |          -0.0001 |          23.8225 |          13.4772 |
[32m[20221213 22:40:12 @agent_ppo2.py:185][0m |          -0.0079 |          22.4751 |          13.5097 |
[32m[20221213 22:40:13 @agent_ppo2.py:185][0m |          -0.0045 |          21.9299 |          13.4920 |
[32m[20221213 22:40:13 @agent_ppo2.py:185][0m |          -0.0117 |          21.3465 |          13.5025 |
[32m[20221213 22:40:13 @agent_ppo2.py:185][0m |          -0.0151 |          21.0931 |          13.4894 |
[32m[20221213 22:40:13 @agent_ppo2.py:185][0m |          -0.0102 |          20.6920 |          13.4903 |
[32m[20221213 22:40:13 @agent_ppo2.py:185][0m |          -0.0090 |          20.4903 |          13.4830 |
[32m[20221213 22:40:13 @agent_ppo2.py:185][0m |          -0.0140 |          20.2001 |          13.4921 |
[32m[20221213 22:40:13 @agent_ppo2.py:185][0m |          -0.0072 |          20.3416 |          13.4929 |
[32m[20221213 22:40:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:40:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.06
[32m[20221213 22:40:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.79
[32m[20221213 22:40:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.00
[32m[20221213 22:40:13 @agent_ppo2.py:143][0m Total time:      22.01 min
[32m[20221213 22:40:13 @agent_ppo2.py:145][0m 2142208 total steps have happened
[32m[20221213 22:40:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1046 --------------------------#
[32m[20221213 22:40:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0007 |          34.0995 |          13.5581 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0067 |          31.3700 |          13.5452 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0105 |          30.3116 |          13.5427 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0097 |          29.6535 |          13.5386 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0051 |          29.1939 |          13.5402 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0127 |          28.6158 |          13.5311 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0110 |          28.0696 |          13.5231 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |           0.0059 |          32.3300 |          13.5276 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0097 |          27.9787 |          13.5158 |
[32m[20221213 22:40:14 @agent_ppo2.py:185][0m |          -0.0131 |          27.3608 |          13.5153 |
[32m[20221213 22:40:14 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:40:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 299.90
[32m[20221213 22:40:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 357.28
[32m[20221213 22:40:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.54
[32m[20221213 22:40:15 @agent_ppo2.py:143][0m Total time:      22.03 min
[32m[20221213 22:40:15 @agent_ppo2.py:145][0m 2144256 total steps have happened
[32m[20221213 22:40:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1047 --------------------------#
[32m[20221213 22:40:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0034 |          33.5464 |          13.4452 |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0049 |          31.3803 |          13.4354 |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0053 |          30.4717 |          13.4140 |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0070 |          29.9122 |          13.4083 |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0090 |          29.5130 |          13.4167 |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0071 |          29.2369 |          13.4051 |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0121 |          29.0140 |          13.4164 |
[32m[20221213 22:40:15 @agent_ppo2.py:185][0m |          -0.0103 |          28.7816 |          13.4181 |
[32m[20221213 22:40:16 @agent_ppo2.py:185][0m |          -0.0079 |          28.5342 |          13.4067 |
[32m[20221213 22:40:16 @agent_ppo2.py:185][0m |          -0.0082 |          28.3761 |          13.4027 |
[32m[20221213 22:40:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:40:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.02
[32m[20221213 22:40:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.28
[32m[20221213 22:40:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.37
[32m[20221213 22:40:16 @agent_ppo2.py:143][0m Total time:      22.05 min
[32m[20221213 22:40:16 @agent_ppo2.py:145][0m 2146304 total steps have happened
[32m[20221213 22:40:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1048 --------------------------#
[32m[20221213 22:40:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:16 @agent_ppo2.py:185][0m |          -0.0009 |          39.0736 |          13.5077 |
[32m[20221213 22:40:16 @agent_ppo2.py:185][0m |          -0.0045 |          35.3915 |          13.5020 |
[32m[20221213 22:40:16 @agent_ppo2.py:185][0m |          -0.0022 |          34.3798 |          13.5039 |
[32m[20221213 22:40:16 @agent_ppo2.py:185][0m |           0.0025 |          34.6177 |          13.4999 |
[32m[20221213 22:40:16 @agent_ppo2.py:185][0m |          -0.0087 |          33.6272 |          13.4952 |
[32m[20221213 22:40:17 @agent_ppo2.py:185][0m |          -0.0083 |          33.4148 |          13.4973 |
[32m[20221213 22:40:17 @agent_ppo2.py:185][0m |          -0.0081 |          33.0390 |          13.4947 |
[32m[20221213 22:40:17 @agent_ppo2.py:185][0m |          -0.0125 |          32.8087 |          13.4995 |
[32m[20221213 22:40:17 @agent_ppo2.py:185][0m |          -0.0088 |          32.6148 |          13.4924 |
[32m[20221213 22:40:17 @agent_ppo2.py:185][0m |          -0.0104 |          32.3900 |          13.4949 |
[32m[20221213 22:40:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:40:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.33
[32m[20221213 22:40:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.09
[32m[20221213 22:40:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 213.59
[32m[20221213 22:40:17 @agent_ppo2.py:143][0m Total time:      22.07 min
[32m[20221213 22:40:17 @agent_ppo2.py:145][0m 2148352 total steps have happened
[32m[20221213 22:40:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1049 --------------------------#
[32m[20221213 22:40:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:17 @agent_ppo2.py:185][0m |           0.0041 |          33.1992 |          13.7640 |
[32m[20221213 22:40:17 @agent_ppo2.py:185][0m |          -0.0085 |          29.7689 |          13.7428 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0082 |          29.0455 |          13.7454 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0083 |          27.6822 |          13.7349 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0091 |          27.3240 |          13.7430 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0113 |          26.7910 |          13.7328 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0025 |          27.8933 |          13.7328 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0189 |          26.3844 |          13.7279 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0053 |          28.7348 |          13.7361 |
[32m[20221213 22:40:18 @agent_ppo2.py:185][0m |          -0.0086 |          27.1702 |          13.7285 |
[32m[20221213 22:40:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:40:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.68
[32m[20221213 22:40:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.81
[32m[20221213 22:40:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.13
[32m[20221213 22:40:18 @agent_ppo2.py:143][0m Total time:      22.09 min
[32m[20221213 22:40:18 @agent_ppo2.py:145][0m 2150400 total steps have happened
[32m[20221213 22:40:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1050 --------------------------#
[32m[20221213 22:40:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:40:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0008 |          25.9021 |          13.5849 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0029 |          22.3388 |          13.5680 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0042 |          21.1938 |          13.5745 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |           0.0027 |          20.8329 |          13.5803 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0048 |          20.0409 |          13.5713 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0075 |          19.6297 |          13.5618 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0064 |          19.3309 |          13.5643 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |           0.0031 |          21.2072 |          13.5759 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0076 |          18.9617 |          13.5778 |
[32m[20221213 22:40:19 @agent_ppo2.py:185][0m |          -0.0094 |          18.7118 |          13.5773 |
[32m[20221213 22:40:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:40:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.01
[32m[20221213 22:40:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.38
[32m[20221213 22:40:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.71
[32m[20221213 22:40:20 @agent_ppo2.py:143][0m Total time:      22.11 min
[32m[20221213 22:40:20 @agent_ppo2.py:145][0m 2152448 total steps have happened
[32m[20221213 22:40:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1051 --------------------------#
[32m[20221213 22:40:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:20 @agent_ppo2.py:185][0m |           0.0022 |          28.0554 |          13.6579 |
[32m[20221213 22:40:20 @agent_ppo2.py:185][0m |          -0.0006 |          18.7903 |          13.6428 |
[32m[20221213 22:40:20 @agent_ppo2.py:185][0m |          -0.0038 |          16.9629 |          13.6325 |
[32m[20221213 22:40:20 @agent_ppo2.py:185][0m |          -0.0064 |          16.3762 |          13.6365 |
[32m[20221213 22:40:20 @agent_ppo2.py:185][0m |          -0.0087 |          16.0750 |          13.6332 |
[32m[20221213 22:40:20 @agent_ppo2.py:185][0m |          -0.0044 |          15.1544 |          13.6420 |
[32m[20221213 22:40:21 @agent_ppo2.py:185][0m |          -0.0098 |          14.7944 |          13.6297 |
[32m[20221213 22:40:21 @agent_ppo2.py:185][0m |          -0.0023 |          14.3348 |          13.6309 |
[32m[20221213 22:40:21 @agent_ppo2.py:185][0m |          -0.0124 |          13.9574 |          13.6332 |
[32m[20221213 22:40:21 @agent_ppo2.py:185][0m |          -0.0007 |          14.7141 |          13.6427 |
[32m[20221213 22:40:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:40:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.84
[32m[20221213 22:40:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.13
[32m[20221213 22:40:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.19
[32m[20221213 22:40:21 @agent_ppo2.py:143][0m Total time:      22.13 min
[32m[20221213 22:40:21 @agent_ppo2.py:145][0m 2154496 total steps have happened
[32m[20221213 22:40:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1052 --------------------------#
[32m[20221213 22:40:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:21 @agent_ppo2.py:185][0m |          -0.0019 |          33.1444 |          13.6854 |
[32m[20221213 22:40:21 @agent_ppo2.py:185][0m |           0.0002 |          31.7094 |          13.6678 |
[32m[20221213 22:40:21 @agent_ppo2.py:185][0m |          -0.0062 |          27.9558 |          13.6601 |
[32m[20221213 22:40:22 @agent_ppo2.py:185][0m |          -0.0078 |          27.3188 |          13.6621 |
[32m[20221213 22:40:22 @agent_ppo2.py:185][0m |          -0.0098 |          26.8075 |          13.6623 |
[32m[20221213 22:40:22 @agent_ppo2.py:185][0m |          -0.0079 |          26.4150 |          13.6552 |
[32m[20221213 22:40:22 @agent_ppo2.py:185][0m |          -0.0077 |          26.1580 |          13.6557 |
[32m[20221213 22:40:22 @agent_ppo2.py:185][0m |          -0.0087 |          26.2474 |          13.6523 |
[32m[20221213 22:40:22 @agent_ppo2.py:185][0m |          -0.0057 |          27.0706 |          13.6497 |
[32m[20221213 22:40:22 @agent_ppo2.py:185][0m |          -0.0155 |          25.5717 |          13.6417 |
[32m[20221213 22:40:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:40:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.33
[32m[20221213 22:40:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.42
[32m[20221213 22:40:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.85
[32m[20221213 22:40:22 @agent_ppo2.py:143][0m Total time:      22.16 min
[32m[20221213 22:40:22 @agent_ppo2.py:145][0m 2156544 total steps have happened
[32m[20221213 22:40:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1053 --------------------------#
[32m[20221213 22:40:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0006 |          26.3809 |          13.6462 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0014 |          23.8425 |          13.6273 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |           0.0124 |          25.9519 |          13.6291 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0054 |          22.9039 |          13.6217 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0056 |          22.5262 |          13.6095 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0091 |          22.4436 |          13.6173 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0138 |          22.0858 |          13.6043 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0154 |          21.9159 |          13.6075 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0085 |          21.8150 |          13.6109 |
[32m[20221213 22:40:23 @agent_ppo2.py:185][0m |          -0.0166 |          21.7467 |          13.6011 |
[32m[20221213 22:40:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:40:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.13
[32m[20221213 22:40:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.40
[32m[20221213 22:40:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.83
[32m[20221213 22:40:23 @agent_ppo2.py:143][0m Total time:      22.18 min
[32m[20221213 22:40:23 @agent_ppo2.py:145][0m 2158592 total steps have happened
[32m[20221213 22:40:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1054 --------------------------#
[32m[20221213 22:40:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0001 |          23.6306 |          13.6668 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0084 |          22.0350 |          13.6746 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0060 |          21.4867 |          13.6679 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0048 |          21.1436 |          13.6622 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0109 |          20.7584 |          13.6552 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0009 |          22.0889 |          13.6548 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0103 |          20.4229 |          13.6617 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0097 |          20.0476 |          13.6652 |
[32m[20221213 22:40:24 @agent_ppo2.py:185][0m |          -0.0151 |          19.9891 |          13.6614 |
[32m[20221213 22:40:25 @agent_ppo2.py:185][0m |          -0.0113 |          19.7873 |          13.6533 |
[32m[20221213 22:40:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:40:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.39
[32m[20221213 22:40:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.35
[32m[20221213 22:40:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.06
[32m[20221213 22:40:25 @agent_ppo2.py:143][0m Total time:      22.20 min
[32m[20221213 22:40:25 @agent_ppo2.py:145][0m 2160640 total steps have happened
[32m[20221213 22:40:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1055 --------------------------#
[32m[20221213 22:40:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:25 @agent_ppo2.py:185][0m |          -0.0029 |          28.5043 |          13.7362 |
[32m[20221213 22:40:25 @agent_ppo2.py:185][0m |           0.0049 |          26.8388 |          13.7219 |
[32m[20221213 22:40:25 @agent_ppo2.py:185][0m |          -0.0044 |          24.3458 |          13.7121 |
[32m[20221213 22:40:25 @agent_ppo2.py:185][0m |          -0.0098 |          23.6684 |          13.7134 |
[32m[20221213 22:40:26 @agent_ppo2.py:185][0m |          -0.0077 |          23.2422 |          13.7069 |
[32m[20221213 22:40:26 @agent_ppo2.py:185][0m |          -0.0151 |          22.6827 |          13.7026 |
[32m[20221213 22:40:26 @agent_ppo2.py:185][0m |          -0.0155 |          22.3486 |          13.7061 |
[32m[20221213 22:40:26 @agent_ppo2.py:185][0m |          -0.0139 |          22.0434 |          13.7033 |
[32m[20221213 22:40:26 @agent_ppo2.py:185][0m |          -0.0061 |          22.7355 |          13.7036 |
[32m[20221213 22:40:26 @agent_ppo2.py:185][0m |          -0.0126 |          21.7051 |          13.7061 |
[32m[20221213 22:40:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:40:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.04
[32m[20221213 22:40:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.07
[32m[20221213 22:40:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.39
[32m[20221213 22:40:26 @agent_ppo2.py:143][0m Total time:      22.22 min
[32m[20221213 22:40:26 @agent_ppo2.py:145][0m 2162688 total steps have happened
[32m[20221213 22:40:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1056 --------------------------#
[32m[20221213 22:40:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:26 @agent_ppo2.py:185][0m |           0.0162 |          20.9293 |          13.7396 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |           0.0002 |          17.1516 |          13.7129 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0110 |          16.4166 |          13.7232 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0107 |          15.9693 |          13.7198 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0054 |          15.7030 |          13.7243 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0086 |          15.5797 |          13.7221 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0013 |          15.7475 |          13.7258 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0108 |          15.2588 |          13.7163 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0111 |          15.1334 |          13.7212 |
[32m[20221213 22:40:27 @agent_ppo2.py:185][0m |          -0.0130 |          15.0482 |          13.7212 |
[32m[20221213 22:40:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:40:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.70
[32m[20221213 22:40:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.23
[32m[20221213 22:40:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.41
[32m[20221213 22:40:27 @agent_ppo2.py:143][0m Total time:      22.24 min
[32m[20221213 22:40:27 @agent_ppo2.py:145][0m 2164736 total steps have happened
[32m[20221213 22:40:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1057 --------------------------#
[32m[20221213 22:40:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |           0.0025 |          32.0666 |          13.9810 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0021 |          29.0744 |          13.9786 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0029 |          27.9649 |          13.9719 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0083 |          27.6740 |          13.9642 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0095 |          27.3081 |          13.9538 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0073 |          26.9826 |          13.9619 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0088 |          26.8828 |          13.9656 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0040 |          26.6737 |          13.9485 |
[32m[20221213 22:40:28 @agent_ppo2.py:185][0m |          -0.0110 |          26.6603 |          13.9491 |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0078 |          26.3792 |          13.9589 |
[32m[20221213 22:40:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:40:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.68
[32m[20221213 22:40:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.30
[32m[20221213 22:40:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.53
[32m[20221213 22:40:29 @agent_ppo2.py:143][0m Total time:      22.26 min
[32m[20221213 22:40:29 @agent_ppo2.py:145][0m 2166784 total steps have happened
[32m[20221213 22:40:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1058 --------------------------#
[32m[20221213 22:40:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0007 |          28.7112 |          13.8768 |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0009 |          27.4146 |          13.8431 |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0050 |          27.2910 |          13.8525 |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0084 |          27.1941 |          13.8526 |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0063 |          27.1059 |          13.8511 |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0071 |          27.0005 |          13.8464 |
[32m[20221213 22:40:29 @agent_ppo2.py:185][0m |          -0.0077 |          27.0241 |          13.8437 |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |          -0.0122 |          26.9378 |          13.8432 |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |          -0.0060 |          26.9179 |          13.8424 |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |           0.0043 |          28.7266 |          13.8460 |
[32m[20221213 22:40:30 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.91
[32m[20221213 22:40:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.83
[32m[20221213 22:40:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.43
[32m[20221213 22:40:30 @agent_ppo2.py:143][0m Total time:      22.28 min
[32m[20221213 22:40:30 @agent_ppo2.py:145][0m 2168832 total steps have happened
[32m[20221213 22:40:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1059 --------------------------#
[32m[20221213 22:40:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |          -0.0009 |          28.4725 |          13.8163 |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |          -0.0063 |          27.3946 |          13.8069 |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |          -0.0017 |          27.5889 |          13.7870 |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |          -0.0096 |          26.8828 |          13.7776 |
[32m[20221213 22:40:30 @agent_ppo2.py:185][0m |          -0.0114 |          26.7124 |          13.7888 |
[32m[20221213 22:40:31 @agent_ppo2.py:185][0m |          -0.0104 |          26.5896 |          13.7737 |
[32m[20221213 22:40:31 @agent_ppo2.py:185][0m |          -0.0023 |          27.6073 |          13.7777 |
[32m[20221213 22:40:31 @agent_ppo2.py:185][0m |          -0.0121 |          26.4716 |          13.7725 |
[32m[20221213 22:40:31 @agent_ppo2.py:185][0m |          -0.0129 |          26.4576 |          13.7669 |
[32m[20221213 22:40:31 @agent_ppo2.py:185][0m |          -0.0119 |          26.2596 |          13.7704 |
[32m[20221213 22:40:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.43
[32m[20221213 22:40:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.23
[32m[20221213 22:40:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.11
[32m[20221213 22:40:31 @agent_ppo2.py:143][0m Total time:      22.30 min
[32m[20221213 22:40:31 @agent_ppo2.py:145][0m 2170880 total steps have happened
[32m[20221213 22:40:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1060 --------------------------#
[32m[20221213 22:40:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:40:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:31 @agent_ppo2.py:185][0m |          -0.0013 |          33.8194 |          13.9130 |
[32m[20221213 22:40:31 @agent_ppo2.py:185][0m |          -0.0055 |          30.9797 |          13.9121 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |          -0.0089 |          29.9180 |          13.8834 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |          -0.0118 |          29.3429 |          13.8847 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |          -0.0127 |          28.8038 |          13.8810 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |           0.0020 |          32.9477 |          13.8732 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |          -0.0143 |          28.3451 |          13.8639 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |          -0.0150 |          28.2539 |          13.8843 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |          -0.0156 |          28.0148 |          13.8681 |
[32m[20221213 22:40:32 @agent_ppo2.py:185][0m |          -0.0116 |          28.0728 |          13.8699 |
[32m[20221213 22:40:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:40:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.88
[32m[20221213 22:40:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.63
[32m[20221213 22:40:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.26
[32m[20221213 22:40:32 @agent_ppo2.py:143][0m Total time:      22.32 min
[32m[20221213 22:40:32 @agent_ppo2.py:145][0m 2172928 total steps have happened
[32m[20221213 22:40:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1061 --------------------------#
[32m[20221213 22:40:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |           0.0019 |          28.1459 |          13.6827 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0012 |          22.3440 |          13.6688 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0084 |          21.7339 |          13.6666 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0131 |          21.4208 |          13.6585 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0111 |          21.2286 |          13.6559 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0084 |          21.0187 |          13.6540 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0097 |          20.9295 |          13.6476 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0036 |          21.0210 |          13.6487 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0092 |          20.7868 |          13.6524 |
[32m[20221213 22:40:33 @agent_ppo2.py:185][0m |          -0.0145 |          20.7040 |          13.6480 |
[32m[20221213 22:40:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:40:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.80
[32m[20221213 22:40:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.96
[32m[20221213 22:40:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.75
[32m[20221213 22:40:33 @agent_ppo2.py:143][0m Total time:      22.34 min
[32m[20221213 22:40:33 @agent_ppo2.py:145][0m 2174976 total steps have happened
[32m[20221213 22:40:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1062 --------------------------#
[32m[20221213 22:40:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0024 |          19.4747 |          13.6960 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0035 |          15.2897 |          13.6874 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0064 |          14.2751 |          13.6810 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |           0.0039 |          13.8052 |          13.6724 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0043 |          13.2932 |          13.6699 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0007 |          13.7353 |          13.6656 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0160 |          12.6127 |          13.6664 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0099 |          12.2883 |          13.6554 |
[32m[20221213 22:40:34 @agent_ppo2.py:185][0m |          -0.0095 |          12.1760 |          13.6645 |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |          -0.0153 |          11.8886 |          13.6684 |
[32m[20221213 22:40:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.17
[32m[20221213 22:40:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.85
[32m[20221213 22:40:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.53
[32m[20221213 22:40:35 @agent_ppo2.py:143][0m Total time:      22.36 min
[32m[20221213 22:40:35 @agent_ppo2.py:145][0m 2177024 total steps have happened
[32m[20221213 22:40:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1063 --------------------------#
[32m[20221213 22:40:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |           0.0008 |          17.8351 |          13.8584 |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |          -0.0051 |          14.8445 |          13.8456 |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |          -0.0062 |          14.2727 |          13.8380 |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |          -0.0045 |          15.3222 |          13.8348 |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |          -0.0070 |          13.8083 |          13.8227 |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |          -0.0123 |          13.2111 |          13.8238 |
[32m[20221213 22:40:35 @agent_ppo2.py:185][0m |          -0.0080 |          13.0616 |          13.8252 |
[32m[20221213 22:40:36 @agent_ppo2.py:185][0m |          -0.0135 |          12.8878 |          13.8244 |
[32m[20221213 22:40:36 @agent_ppo2.py:185][0m |          -0.0144 |          12.7884 |          13.8156 |
[32m[20221213 22:40:36 @agent_ppo2.py:185][0m |          -0.0151 |          12.5223 |          13.8209 |
[32m[20221213 22:40:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:40:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.05
[32m[20221213 22:40:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.99
[32m[20221213 22:40:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.86
[32m[20221213 22:40:36 @agent_ppo2.py:143][0m Total time:      22.38 min
[32m[20221213 22:40:36 @agent_ppo2.py:145][0m 2179072 total steps have happened
[32m[20221213 22:40:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1064 --------------------------#
[32m[20221213 22:40:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:36 @agent_ppo2.py:185][0m |          -0.0018 |          32.1682 |          13.6276 |
[32m[20221213 22:40:36 @agent_ppo2.py:185][0m |          -0.0078 |          29.1869 |          13.6037 |
[32m[20221213 22:40:36 @agent_ppo2.py:185][0m |          -0.0044 |          28.4155 |          13.6203 |
[32m[20221213 22:40:36 @agent_ppo2.py:185][0m |          -0.0080 |          27.7728 |          13.5978 |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |          -0.0088 |          27.4151 |          13.6075 |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |          -0.0108 |          27.1465 |          13.6020 |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |          -0.0089 |          26.8825 |          13.6107 |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |          -0.0161 |          26.8786 |          13.6157 |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |          -0.0155 |          26.6422 |          13.5911 |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |          -0.0144 |          26.4584 |          13.6096 |
[32m[20221213 22:40:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.69
[32m[20221213 22:40:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.15
[32m[20221213 22:40:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.86
[32m[20221213 22:40:37 @agent_ppo2.py:143][0m Total time:      22.40 min
[32m[20221213 22:40:37 @agent_ppo2.py:145][0m 2181120 total steps have happened
[32m[20221213 22:40:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1065 --------------------------#
[32m[20221213 22:40:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |           0.0009 |          24.1952 |          13.5805 |
[32m[20221213 22:40:37 @agent_ppo2.py:185][0m |          -0.0078 |          21.5201 |          13.5613 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0045 |          20.5337 |          13.5508 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0133 |          20.0865 |          13.5417 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0056 |          19.8761 |          13.5458 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0112 |          19.5345 |          13.5372 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0142 |          19.3732 |          13.5332 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0122 |          19.1220 |          13.5260 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0146 |          18.9253 |          13.5303 |
[32m[20221213 22:40:38 @agent_ppo2.py:185][0m |          -0.0100 |          18.7878 |          13.5255 |
[32m[20221213 22:40:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:40:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.81
[32m[20221213 22:40:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 346.10
[32m[20221213 22:40:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.99
[32m[20221213 22:40:38 @agent_ppo2.py:143][0m Total time:      22.42 min
[32m[20221213 22:40:38 @agent_ppo2.py:145][0m 2183168 total steps have happened
[32m[20221213 22:40:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1066 --------------------------#
[32m[20221213 22:40:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |           0.0036 |          31.8056 |          13.7680 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0048 |          30.7904 |          13.7603 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |           0.0002 |          30.9026 |          13.7477 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0039 |          30.8879 |          13.7449 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0092 |          30.4902 |          13.7459 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0083 |          30.3221 |          13.7293 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0075 |          30.2613 |          13.7452 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0079 |          30.2621 |          13.7401 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0014 |          30.7557 |          13.7268 |
[32m[20221213 22:40:39 @agent_ppo2.py:185][0m |          -0.0092 |          30.1393 |          13.7435 |
[32m[20221213 22:40:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:40:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.97
[32m[20221213 22:40:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.28
[32m[20221213 22:40:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.44
[32m[20221213 22:40:40 @agent_ppo2.py:143][0m Total time:      22.44 min
[32m[20221213 22:40:40 @agent_ppo2.py:145][0m 2185216 total steps have happened
[32m[20221213 22:40:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1067 --------------------------#
[32m[20221213 22:40:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0014 |          27.9765 |          13.6922 |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0035 |          27.3516 |          13.6828 |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0052 |          27.1348 |          13.6986 |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0070 |          27.0397 |          13.6845 |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0016 |          27.0260 |          13.6820 |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0060 |          26.8577 |          13.6885 |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0070 |          26.7978 |          13.6711 |
[32m[20221213 22:40:40 @agent_ppo2.py:185][0m |          -0.0046 |          26.9951 |          13.6852 |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |          -0.0128 |          26.7521 |          13.6719 |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |          -0.0051 |          26.7456 |          13.6799 |
[32m[20221213 22:40:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:40:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.93
[32m[20221213 22:40:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.16
[32m[20221213 22:40:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.80
[32m[20221213 22:40:41 @agent_ppo2.py:143][0m Total time:      22.46 min
[32m[20221213 22:40:41 @agent_ppo2.py:145][0m 2187264 total steps have happened
[32m[20221213 22:40:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1068 --------------------------#
[32m[20221213 22:40:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |          -0.0007 |          24.2847 |          13.7763 |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |          -0.0105 |          22.8370 |          13.7617 |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |          -0.0073 |          22.2806 |          13.7538 |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |          -0.0079 |          22.2453 |          13.7576 |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |           0.0005 |          22.6378 |          13.7457 |
[32m[20221213 22:40:41 @agent_ppo2.py:185][0m |          -0.0069 |          22.0705 |          13.7352 |
[32m[20221213 22:40:42 @agent_ppo2.py:185][0m |          -0.0104 |          21.4496 |          13.7486 |
[32m[20221213 22:40:42 @agent_ppo2.py:185][0m |          -0.0048 |          24.3542 |          13.7426 |
[32m[20221213 22:40:42 @agent_ppo2.py:185][0m |          -0.0186 |          21.3707 |          13.7299 |
[32m[20221213 22:40:42 @agent_ppo2.py:185][0m |          -0.0091 |          21.4425 |          13.7436 |
[32m[20221213 22:40:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:40:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.96
[32m[20221213 22:40:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.74
[32m[20221213 22:40:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.61
[32m[20221213 22:40:42 @agent_ppo2.py:143][0m Total time:      22.48 min
[32m[20221213 22:40:42 @agent_ppo2.py:145][0m 2189312 total steps have happened
[32m[20221213 22:40:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1069 --------------------------#
[32m[20221213 22:40:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:42 @agent_ppo2.py:185][0m |          -0.0029 |          24.2734 |          13.8881 |
[32m[20221213 22:40:42 @agent_ppo2.py:185][0m |           0.0103 |          27.6360 |          13.8669 |
[32m[20221213 22:40:42 @agent_ppo2.py:185][0m |          -0.0012 |          23.6788 |          13.8516 |
[32m[20221213 22:40:43 @agent_ppo2.py:185][0m |          -0.0092 |          23.0016 |          13.8552 |
[32m[20221213 22:40:43 @agent_ppo2.py:185][0m |           0.0010 |          23.9294 |          13.8572 |
[32m[20221213 22:40:43 @agent_ppo2.py:185][0m |          -0.0088 |          22.7581 |          13.8410 |
[32m[20221213 22:40:43 @agent_ppo2.py:185][0m |          -0.0058 |          22.7514 |          13.8558 |
[32m[20221213 22:40:43 @agent_ppo2.py:185][0m |          -0.0093 |          22.6695 |          13.8567 |
[32m[20221213 22:40:43 @agent_ppo2.py:185][0m |          -0.0100 |          22.6205 |          13.8472 |
[32m[20221213 22:40:43 @agent_ppo2.py:185][0m |          -0.0110 |          22.5546 |          13.8580 |
[32m[20221213 22:40:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:40:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.22
[32m[20221213 22:40:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.30
[32m[20221213 22:40:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.59
[32m[20221213 22:40:43 @agent_ppo2.py:143][0m Total time:      22.51 min
[32m[20221213 22:40:43 @agent_ppo2.py:145][0m 2191360 total steps have happened
[32m[20221213 22:40:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1070 --------------------------#
[32m[20221213 22:40:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:40:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0021 |          31.1917 |          13.8900 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |           0.0059 |          32.8238 |          13.8661 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0070 |          30.1101 |          13.8440 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0105 |          29.7535 |          13.8508 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0102 |          29.6053 |          13.8646 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0090 |          29.4105 |          13.8604 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0107 |          29.3259 |          13.8556 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0113 |          29.2426 |          13.8607 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0040 |          30.9181 |          13.8381 |
[32m[20221213 22:40:44 @agent_ppo2.py:185][0m |          -0.0098 |          29.3082 |          13.8369 |
[32m[20221213 22:40:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:40:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.77
[32m[20221213 22:40:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.95
[32m[20221213 22:40:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.01
[32m[20221213 22:40:45 @agent_ppo2.py:143][0m Total time:      22.53 min
[32m[20221213 22:40:45 @agent_ppo2.py:145][0m 2193408 total steps have happened
[32m[20221213 22:40:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1071 --------------------------#
[32m[20221213 22:40:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |           0.0064 |          34.7379 |          13.8192 |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |          -0.0066 |          33.2895 |          13.7897 |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |          -0.0007 |          36.1687 |          13.7934 |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |          -0.0002 |          35.2120 |          13.7830 |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |          -0.0070 |          32.6712 |          13.7685 |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |          -0.0076 |          32.4969 |          13.7796 |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |          -0.0050 |          32.5053 |          13.7831 |
[32m[20221213 22:40:45 @agent_ppo2.py:185][0m |          -0.0105 |          32.4328 |          13.7809 |
[32m[20221213 22:40:46 @agent_ppo2.py:185][0m |          -0.0005 |          32.9699 |          13.7765 |
[32m[20221213 22:40:46 @agent_ppo2.py:185][0m |          -0.0086 |          32.3653 |          13.7803 |
[32m[20221213 22:40:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:40:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.06
[32m[20221213 22:40:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.30
[32m[20221213 22:40:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.08
[32m[20221213 22:40:46 @agent_ppo2.py:143][0m Total time:      22.55 min
[32m[20221213 22:40:46 @agent_ppo2.py:145][0m 2195456 total steps have happened
[32m[20221213 22:40:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1072 --------------------------#
[32m[20221213 22:40:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:46 @agent_ppo2.py:185][0m |           0.0016 |          32.6696 |          13.9663 |
[32m[20221213 22:40:46 @agent_ppo2.py:185][0m |          -0.0066 |          30.0932 |          13.9482 |
[32m[20221213 22:40:46 @agent_ppo2.py:185][0m |          -0.0119 |          29.1618 |          13.9280 |
[32m[20221213 22:40:46 @agent_ppo2.py:185][0m |          -0.0124 |          28.9534 |          13.9355 |
[32m[20221213 22:40:46 @agent_ppo2.py:185][0m |          -0.0109 |          28.3515 |          13.9328 |
[32m[20221213 22:40:47 @agent_ppo2.py:185][0m |          -0.0152 |          28.2426 |          13.9166 |
[32m[20221213 22:40:47 @agent_ppo2.py:185][0m |          -0.0151 |          28.0670 |          13.9279 |
[32m[20221213 22:40:47 @agent_ppo2.py:185][0m |          -0.0132 |          27.8006 |          13.9245 |
[32m[20221213 22:40:47 @agent_ppo2.py:185][0m |          -0.0133 |          27.4917 |          13.9233 |
[32m[20221213 22:40:47 @agent_ppo2.py:185][0m |          -0.0159 |          27.5125 |          13.9198 |
[32m[20221213 22:40:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:40:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.91
[32m[20221213 22:40:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.48
[32m[20221213 22:40:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.97
[32m[20221213 22:40:47 @agent_ppo2.py:143][0m Total time:      22.57 min
[32m[20221213 22:40:47 @agent_ppo2.py:145][0m 2197504 total steps have happened
[32m[20221213 22:40:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1073 --------------------------#
[32m[20221213 22:40:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:47 @agent_ppo2.py:185][0m |          -0.0013 |          36.5943 |          13.9031 |
[32m[20221213 22:40:47 @agent_ppo2.py:185][0m |          -0.0020 |          33.6506 |          13.8823 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0068 |          32.3914 |          13.8919 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0057 |          31.5015 |          13.8851 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0085 |          31.2252 |          13.8836 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0101 |          30.8685 |          13.8769 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0096 |          30.4332 |          13.8822 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0099 |          29.9409 |          13.8888 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0117 |          29.6849 |          13.8771 |
[32m[20221213 22:40:48 @agent_ppo2.py:185][0m |          -0.0114 |          29.4167 |          13.8743 |
[32m[20221213 22:40:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:40:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.43
[32m[20221213 22:40:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.57
[32m[20221213 22:40:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.30
[32m[20221213 22:40:48 @agent_ppo2.py:143][0m Total time:      22.59 min
[32m[20221213 22:40:48 @agent_ppo2.py:145][0m 2199552 total steps have happened
[32m[20221213 22:40:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1074 --------------------------#
[32m[20221213 22:40:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |           0.0048 |          36.8008 |          13.8516 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0057 |          34.1628 |          13.8265 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0060 |          33.7363 |          13.8338 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0042 |          33.6305 |          13.8164 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0100 |          33.4204 |          13.8207 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0094 |          33.2331 |          13.8187 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0098 |          33.2551 |          13.8189 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0105 |          33.1210 |          13.8106 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0119 |          33.0219 |          13.8114 |
[32m[20221213 22:40:49 @agent_ppo2.py:185][0m |          -0.0127 |          32.9457 |          13.8075 |
[32m[20221213 22:40:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:40:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.99
[32m[20221213 22:40:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.13
[32m[20221213 22:40:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.91
[32m[20221213 22:40:50 @agent_ppo2.py:143][0m Total time:      22.61 min
[32m[20221213 22:40:50 @agent_ppo2.py:145][0m 2201600 total steps have happened
[32m[20221213 22:40:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1075 --------------------------#
[32m[20221213 22:40:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:50 @agent_ppo2.py:185][0m |           0.0007 |          31.9957 |          13.8396 |
[32m[20221213 22:40:50 @agent_ppo2.py:185][0m |          -0.0033 |          29.2070 |          13.8170 |
[32m[20221213 22:40:50 @agent_ppo2.py:185][0m |           0.0082 |          32.3828 |          13.8252 |
[32m[20221213 22:40:50 @agent_ppo2.py:185][0m |          -0.0039 |          28.5563 |          13.8272 |
[32m[20221213 22:40:50 @agent_ppo2.py:185][0m |          -0.0058 |          28.4262 |          13.8127 |
[32m[20221213 22:40:50 @agent_ppo2.py:185][0m |          -0.0059 |          28.2194 |          13.8229 |
[32m[20221213 22:40:50 @agent_ppo2.py:185][0m |          -0.0129 |          28.1114 |          13.8090 |
[32m[20221213 22:40:51 @agent_ppo2.py:185][0m |           0.0021 |          29.9245 |          13.8212 |
[32m[20221213 22:40:51 @agent_ppo2.py:185][0m |          -0.0086 |          27.9973 |          13.8063 |
[32m[20221213 22:40:51 @agent_ppo2.py:185][0m |          -0.0099 |          27.9598 |          13.8159 |
[32m[20221213 22:40:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:40:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.19
[32m[20221213 22:40:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.47
[32m[20221213 22:40:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.19
[32m[20221213 22:40:51 @agent_ppo2.py:143][0m Total time:      22.63 min
[32m[20221213 22:40:51 @agent_ppo2.py:145][0m 2203648 total steps have happened
[32m[20221213 22:40:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1076 --------------------------#
[32m[20221213 22:40:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:51 @agent_ppo2.py:185][0m |           0.0013 |          36.5176 |          13.7198 |
[32m[20221213 22:40:51 @agent_ppo2.py:185][0m |          -0.0020 |          33.5389 |          13.7037 |
[32m[20221213 22:40:51 @agent_ppo2.py:185][0m |          -0.0103 |          32.3908 |          13.6927 |
[32m[20221213 22:40:51 @agent_ppo2.py:185][0m |          -0.0073 |          31.6921 |          13.6925 |
[32m[20221213 22:40:52 @agent_ppo2.py:185][0m |          -0.0006 |          32.1116 |          13.6868 |
[32m[20221213 22:40:52 @agent_ppo2.py:185][0m |          -0.0113 |          30.8102 |          13.6831 |
[32m[20221213 22:40:52 @agent_ppo2.py:185][0m |          -0.0095 |          30.6480 |          13.6767 |
[32m[20221213 22:40:52 @agent_ppo2.py:185][0m |          -0.0123 |          30.3880 |          13.6885 |
[32m[20221213 22:40:52 @agent_ppo2.py:185][0m |          -0.0135 |          30.0275 |          13.6704 |
[32m[20221213 22:40:52 @agent_ppo2.py:185][0m |          -0.0142 |          29.8764 |          13.6783 |
[32m[20221213 22:40:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:40:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.89
[32m[20221213 22:40:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.17
[32m[20221213 22:40:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.30
[32m[20221213 22:40:52 @agent_ppo2.py:143][0m Total time:      22.65 min
[32m[20221213 22:40:52 @agent_ppo2.py:145][0m 2205696 total steps have happened
[32m[20221213 22:40:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1077 --------------------------#
[32m[20221213 22:40:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:52 @agent_ppo2.py:185][0m |          -0.0002 |          33.9480 |          13.8329 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0008 |          33.4322 |          13.8047 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0032 |          33.1699 |          13.8062 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0066 |          32.9800 |          13.8004 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0042 |          33.0049 |          13.8140 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0059 |          32.8595 |          13.8014 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0070 |          32.7353 |          13.8054 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0015 |          33.7873 |          13.8038 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0079 |          32.6572 |          13.7972 |
[32m[20221213 22:40:53 @agent_ppo2.py:185][0m |          -0.0093 |          32.6227 |          13.8003 |
[32m[20221213 22:40:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:40:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.63
[32m[20221213 22:40:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 362.24
[32m[20221213 22:40:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.48
[32m[20221213 22:40:53 @agent_ppo2.py:143][0m Total time:      22.68 min
[32m[20221213 22:40:53 @agent_ppo2.py:145][0m 2207744 total steps have happened
[32m[20221213 22:40:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1078 --------------------------#
[32m[20221213 22:40:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |           0.0048 |          33.3819 |          13.9134 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0040 |          32.6240 |          13.8886 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0054 |          32.3706 |          13.8850 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0070 |          32.1875 |          13.8864 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0050 |          32.1642 |          13.9030 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0070 |          31.9068 |          13.8973 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0081 |          31.7854 |          13.8900 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0069 |          31.7709 |          13.8817 |
[32m[20221213 22:40:54 @agent_ppo2.py:185][0m |          -0.0096 |          31.6001 |          13.9047 |
[32m[20221213 22:40:55 @agent_ppo2.py:185][0m |          -0.0085 |          31.6179 |          13.8779 |
[32m[20221213 22:40:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:40:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.10
[32m[20221213 22:40:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.36
[32m[20221213 22:40:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 177.38
[32m[20221213 22:40:55 @agent_ppo2.py:143][0m Total time:      22.70 min
[32m[20221213 22:40:55 @agent_ppo2.py:145][0m 2209792 total steps have happened
[32m[20221213 22:40:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1079 --------------------------#
[32m[20221213 22:40:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:40:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:55 @agent_ppo2.py:185][0m |          -0.0011 |          30.5544 |          13.7083 |
[32m[20221213 22:40:55 @agent_ppo2.py:185][0m |          -0.0069 |          27.5263 |          13.6699 |
[32m[20221213 22:40:55 @agent_ppo2.py:185][0m |          -0.0085 |          26.1289 |          13.6699 |
[32m[20221213 22:40:55 @agent_ppo2.py:185][0m |          -0.0003 |          27.4489 |          13.6743 |
[32m[20221213 22:40:55 @agent_ppo2.py:185][0m |          -0.0019 |          25.4020 |          13.6676 |
[32m[20221213 22:40:55 @agent_ppo2.py:185][0m |          -0.0132 |          24.2419 |          13.6544 |
[32m[20221213 22:40:56 @agent_ppo2.py:185][0m |          -0.0145 |          23.9493 |          13.6618 |
[32m[20221213 22:40:56 @agent_ppo2.py:185][0m |          -0.0134 |          23.6343 |          13.6747 |
[32m[20221213 22:40:56 @agent_ppo2.py:185][0m |          -0.0149 |          23.2000 |          13.6636 |
[32m[20221213 22:40:56 @agent_ppo2.py:185][0m |          -0.0127 |          22.9574 |          13.6658 |
[32m[20221213 22:40:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:40:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.17
[32m[20221213 22:40:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.71
[32m[20221213 22:40:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.33
[32m[20221213 22:40:56 @agent_ppo2.py:143][0m Total time:      22.72 min
[32m[20221213 22:40:56 @agent_ppo2.py:145][0m 2211840 total steps have happened
[32m[20221213 22:40:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1080 --------------------------#
[32m[20221213 22:40:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:40:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:56 @agent_ppo2.py:185][0m |          -0.0022 |          33.5049 |          14.0548 |
[32m[20221213 22:40:56 @agent_ppo2.py:185][0m |          -0.0050 |          31.5644 |          14.0563 |
[32m[20221213 22:40:56 @agent_ppo2.py:185][0m |          -0.0046 |          30.9359 |          14.0419 |
[32m[20221213 22:40:57 @agent_ppo2.py:185][0m |          -0.0108 |          30.3394 |          14.0438 |
[32m[20221213 22:40:57 @agent_ppo2.py:185][0m |          -0.0040 |          30.7911 |          14.0415 |
[32m[20221213 22:40:57 @agent_ppo2.py:185][0m |          -0.0123 |          29.8343 |          14.0422 |
[32m[20221213 22:40:57 @agent_ppo2.py:185][0m |          -0.0096 |          29.5996 |          14.0392 |
[32m[20221213 22:40:57 @agent_ppo2.py:185][0m |          -0.0107 |          29.4672 |          14.0330 |
[32m[20221213 22:40:57 @agent_ppo2.py:185][0m |          -0.0118 |          29.2967 |          14.0276 |
[32m[20221213 22:40:57 @agent_ppo2.py:185][0m |          -0.0045 |          30.9318 |          14.0375 |
[32m[20221213 22:40:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:40:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.36
[32m[20221213 22:40:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.18
[32m[20221213 22:40:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.32
[32m[20221213 22:40:57 @agent_ppo2.py:143][0m Total time:      22.74 min
[32m[20221213 22:40:57 @agent_ppo2.py:145][0m 2213888 total steps have happened
[32m[20221213 22:40:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1081 --------------------------#
[32m[20221213 22:40:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0032 |          34.7400 |          13.9690 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0081 |          31.3323 |          13.9402 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0088 |          30.1538 |          13.9405 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0064 |          29.4134 |          13.9401 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0095 |          28.8664 |          13.9372 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0095 |          28.5783 |          13.9433 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0152 |          28.2691 |          13.9433 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0120 |          27.9249 |          13.9415 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0150 |          27.7339 |          13.9443 |
[32m[20221213 22:40:58 @agent_ppo2.py:185][0m |          -0.0127 |          27.4067 |          13.9436 |
[32m[20221213 22:40:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:40:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.78
[32m[20221213 22:40:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 354.68
[32m[20221213 22:40:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.93
[32m[20221213 22:40:58 @agent_ppo2.py:143][0m Total time:      22.76 min
[32m[20221213 22:40:58 @agent_ppo2.py:145][0m 2215936 total steps have happened
[32m[20221213 22:40:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1082 --------------------------#
[32m[20221213 22:40:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:40:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |           0.0033 |          39.4386 |          14.0070 |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |          -0.0005 |          35.6269 |          14.0011 |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |          -0.0046 |          34.0728 |          14.0039 |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |          -0.0059 |          33.1983 |          14.0034 |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |          -0.0045 |          32.6960 |          13.9990 |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |          -0.0092 |          32.0016 |          13.9975 |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |          -0.0082 |          31.5087 |          13.9884 |
[32m[20221213 22:40:59 @agent_ppo2.py:185][0m |          -0.0062 |          31.4956 |          13.9873 |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |          -0.0070 |          31.4922 |          13.9927 |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |          -0.0122 |          30.7470 |          13.9855 |
[32m[20221213 22:41:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:41:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.55
[32m[20221213 22:41:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.27
[32m[20221213 22:41:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.02
[32m[20221213 22:41:00 @agent_ppo2.py:143][0m Total time:      22.78 min
[32m[20221213 22:41:00 @agent_ppo2.py:145][0m 2217984 total steps have happened
[32m[20221213 22:41:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1083 --------------------------#
[32m[20221213 22:41:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |           0.0057 |          39.6383 |          13.9824 |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |          -0.0083 |          35.0501 |          13.9522 |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |          -0.0101 |          33.9871 |          13.9598 |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |          -0.0106 |          33.2606 |          13.9588 |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |          -0.0125 |          33.1563 |          13.9500 |
[32m[20221213 22:41:00 @agent_ppo2.py:185][0m |          -0.0131 |          32.7089 |          13.9406 |
[32m[20221213 22:41:01 @agent_ppo2.py:185][0m |          -0.0167 |          32.4722 |          13.9508 |
[32m[20221213 22:41:01 @agent_ppo2.py:185][0m |          -0.0128 |          32.0651 |          13.9571 |
[32m[20221213 22:41:01 @agent_ppo2.py:185][0m |          -0.0087 |          32.0513 |          13.9492 |
[32m[20221213 22:41:01 @agent_ppo2.py:185][0m |          -0.0155 |          31.6557 |          13.9379 |
[32m[20221213 22:41:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.33
[32m[20221213 22:41:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 316.51
[32m[20221213 22:41:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.43
[32m[20221213 22:41:01 @agent_ppo2.py:143][0m Total time:      22.80 min
[32m[20221213 22:41:01 @agent_ppo2.py:145][0m 2220032 total steps have happened
[32m[20221213 22:41:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1084 --------------------------#
[32m[20221213 22:41:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:01 @agent_ppo2.py:185][0m |           0.0014 |          29.5087 |          13.9248 |
[32m[20221213 22:41:01 @agent_ppo2.py:185][0m |          -0.0054 |          27.4790 |          13.9113 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0010 |          27.6790 |          13.9088 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0005 |          28.7353 |          13.8904 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0010 |          28.0241 |          13.8841 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0079 |          27.0282 |          13.8783 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0046 |          27.0215 |          13.8863 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0068 |          26.8600 |          13.8720 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0117 |          26.8688 |          13.8802 |
[32m[20221213 22:41:02 @agent_ppo2.py:185][0m |          -0.0084 |          26.7723 |          13.8762 |
[32m[20221213 22:41:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.91
[32m[20221213 22:41:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.34
[32m[20221213 22:41:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.06
[32m[20221213 22:41:02 @agent_ppo2.py:143][0m Total time:      22.82 min
[32m[20221213 22:41:02 @agent_ppo2.py:145][0m 2222080 total steps have happened
[32m[20221213 22:41:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1085 --------------------------#
[32m[20221213 22:41:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:41:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0027 |          37.0666 |          13.9048 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0116 |          34.5194 |          13.8825 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0094 |          33.7400 |          13.8726 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0128 |          33.1102 |          13.8754 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0115 |          32.7640 |          13.8653 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0116 |          32.7736 |          13.8756 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0113 |          32.6419 |          13.8738 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0143 |          32.1892 |          13.8666 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0144 |          31.9570 |          13.8714 |
[32m[20221213 22:41:03 @agent_ppo2.py:185][0m |          -0.0153 |          31.8803 |          13.8691 |
[32m[20221213 22:41:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.60
[32m[20221213 22:41:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.08
[32m[20221213 22:41:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.28
[32m[20221213 22:41:04 @agent_ppo2.py:143][0m Total time:      22.84 min
[32m[20221213 22:41:04 @agent_ppo2.py:145][0m 2224128 total steps have happened
[32m[20221213 22:41:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1086 --------------------------#
[32m[20221213 22:41:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |           0.0008 |          25.3684 |          14.0791 |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |          -0.0025 |          24.1622 |          14.0820 |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |          -0.0032 |          23.9552 |          14.0856 |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |          -0.0028 |          24.3731 |          14.0752 |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |          -0.0055 |          23.7359 |          14.0811 |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |          -0.0062 |          23.6882 |          14.0672 |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |          -0.0064 |          23.5976 |          14.0753 |
[32m[20221213 22:41:04 @agent_ppo2.py:185][0m |          -0.0071 |          23.8488 |          14.0833 |
[32m[20221213 22:41:05 @agent_ppo2.py:185][0m |          -0.0073 |          23.4757 |          14.0723 |
[32m[20221213 22:41:05 @agent_ppo2.py:185][0m |          -0.0083 |          23.4709 |          14.0773 |
[32m[20221213 22:41:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.04
[32m[20221213 22:41:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.44
[32m[20221213 22:41:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.17
[32m[20221213 22:41:05 @agent_ppo2.py:143][0m Total time:      22.87 min
[32m[20221213 22:41:05 @agent_ppo2.py:145][0m 2226176 total steps have happened
[32m[20221213 22:41:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1087 --------------------------#
[32m[20221213 22:41:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:05 @agent_ppo2.py:185][0m |          -0.0022 |          32.9259 |          13.8955 |
[32m[20221213 22:41:05 @agent_ppo2.py:185][0m |          -0.0066 |          32.1761 |          13.8780 |
[32m[20221213 22:41:05 @agent_ppo2.py:185][0m |          -0.0067 |          31.6646 |          13.8680 |
[32m[20221213 22:41:05 @agent_ppo2.py:185][0m |          -0.0092 |          31.4236 |          13.8620 |
[32m[20221213 22:41:05 @agent_ppo2.py:185][0m |          -0.0071 |          31.1238 |          13.8552 |
[32m[20221213 22:41:06 @agent_ppo2.py:185][0m |          -0.0044 |          31.9247 |          13.8604 |
[32m[20221213 22:41:06 @agent_ppo2.py:185][0m |          -0.0097 |          31.2139 |          13.8519 |
[32m[20221213 22:41:06 @agent_ppo2.py:185][0m |          -0.0106 |          30.9162 |          13.8566 |
[32m[20221213 22:41:06 @agent_ppo2.py:185][0m |          -0.0108 |          30.9842 |          13.8599 |
[32m[20221213 22:41:06 @agent_ppo2.py:185][0m |          -0.0119 |          30.8060 |          13.8457 |
[32m[20221213 22:41:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:41:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.84
[32m[20221213 22:41:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.67
[32m[20221213 22:41:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.97
[32m[20221213 22:41:06 @agent_ppo2.py:143][0m Total time:      22.89 min
[32m[20221213 22:41:06 @agent_ppo2.py:145][0m 2228224 total steps have happened
[32m[20221213 22:41:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1088 --------------------------#
[32m[20221213 22:41:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:06 @agent_ppo2.py:185][0m |          -0.0005 |          37.9407 |          13.8831 |
[32m[20221213 22:41:06 @agent_ppo2.py:185][0m |          -0.0044 |          36.5821 |          13.8660 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0054 |          36.3680 |          13.8750 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0065 |          35.9531 |          13.8659 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0059 |          35.8450 |          13.8617 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0085 |          35.7550 |          13.8483 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0087 |          35.6010 |          13.8647 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0098 |          35.4245 |          13.8553 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0098 |          35.5502 |          13.8542 |
[32m[20221213 22:41:07 @agent_ppo2.py:185][0m |          -0.0085 |          35.3471 |          13.8694 |
[32m[20221213 22:41:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.71
[32m[20221213 22:41:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.07
[32m[20221213 22:41:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.58
[32m[20221213 22:41:07 @agent_ppo2.py:143][0m Total time:      22.91 min
[32m[20221213 22:41:07 @agent_ppo2.py:145][0m 2230272 total steps have happened
[32m[20221213 22:41:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1089 --------------------------#
[32m[20221213 22:41:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0012 |          32.5566 |          14.0444 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0099 |          29.0325 |          14.0274 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0056 |          27.7888 |          14.0085 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0045 |          27.1049 |          14.0129 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0058 |          26.3561 |          14.0032 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0106 |          26.0255 |          13.9956 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0073 |          25.8705 |          14.0028 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0135 |          25.4177 |          13.9926 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0148 |          25.3017 |          14.0033 |
[32m[20221213 22:41:08 @agent_ppo2.py:185][0m |          -0.0120 |          25.0104 |          13.9882 |
[32m[20221213 22:41:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:41:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.06
[32m[20221213 22:41:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.80
[32m[20221213 22:41:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.91
[32m[20221213 22:41:09 @agent_ppo2.py:143][0m Total time:      22.93 min
[32m[20221213 22:41:09 @agent_ppo2.py:145][0m 2232320 total steps have happened
[32m[20221213 22:41:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1090 --------------------------#
[32m[20221213 22:41:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:41:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:09 @agent_ppo2.py:185][0m |          -0.0020 |          37.0645 |          13.8196 |
[32m[20221213 22:41:09 @agent_ppo2.py:185][0m |          -0.0041 |          34.7159 |          13.8211 |
[32m[20221213 22:41:09 @agent_ppo2.py:185][0m |          -0.0048 |          33.7182 |          13.8044 |
[32m[20221213 22:41:09 @agent_ppo2.py:185][0m |          -0.0082 |          33.0559 |          13.8151 |
[32m[20221213 22:41:09 @agent_ppo2.py:185][0m |          -0.0083 |          32.4277 |          13.7843 |
[32m[20221213 22:41:09 @agent_ppo2.py:185][0m |          -0.0070 |          31.9435 |          13.8039 |
[32m[20221213 22:41:09 @agent_ppo2.py:185][0m |          -0.0109 |          31.6489 |          13.7957 |
[32m[20221213 22:41:10 @agent_ppo2.py:185][0m |          -0.0098 |          31.4309 |          13.7942 |
[32m[20221213 22:41:10 @agent_ppo2.py:185][0m |          -0.0109 |          31.1663 |          13.7881 |
[32m[20221213 22:41:10 @agent_ppo2.py:185][0m |          -0.0121 |          30.9193 |          13.7885 |
[32m[20221213 22:41:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.71
[32m[20221213 22:41:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.09
[32m[20221213 22:41:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 165.57
[32m[20221213 22:41:10 @agent_ppo2.py:143][0m Total time:      22.95 min
[32m[20221213 22:41:10 @agent_ppo2.py:145][0m 2234368 total steps have happened
[32m[20221213 22:41:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1091 --------------------------#
[32m[20221213 22:41:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:10 @agent_ppo2.py:185][0m |          -0.0001 |          36.6588 |          13.7532 |
[32m[20221213 22:41:10 @agent_ppo2.py:185][0m |           0.0046 |          37.0033 |          13.7561 |
[32m[20221213 22:41:10 @agent_ppo2.py:185][0m |          -0.0052 |          35.6579 |          13.7506 |
[32m[20221213 22:41:10 @agent_ppo2.py:185][0m |          -0.0063 |          35.3887 |          13.7550 |
[32m[20221213 22:41:11 @agent_ppo2.py:185][0m |          -0.0076 |          35.1894 |          13.7442 |
[32m[20221213 22:41:11 @agent_ppo2.py:185][0m |          -0.0089 |          35.1623 |          13.7461 |
[32m[20221213 22:41:11 @agent_ppo2.py:185][0m |          -0.0025 |          35.5201 |          13.7449 |
[32m[20221213 22:41:11 @agent_ppo2.py:185][0m |          -0.0074 |          34.9095 |          13.7479 |
[32m[20221213 22:41:11 @agent_ppo2.py:185][0m |          -0.0080 |          34.8691 |          13.7437 |
[32m[20221213 22:41:11 @agent_ppo2.py:185][0m |          -0.0086 |          34.7255 |          13.7457 |
[32m[20221213 22:41:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.32
[32m[20221213 22:41:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.34
[32m[20221213 22:41:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.07
[32m[20221213 22:41:11 @agent_ppo2.py:143][0m Total time:      22.97 min
[32m[20221213 22:41:11 @agent_ppo2.py:145][0m 2236416 total steps have happened
[32m[20221213 22:41:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1092 --------------------------#
[32m[20221213 22:41:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:11 @agent_ppo2.py:185][0m |           0.0021 |          35.1162 |          13.9577 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0077 |          27.0701 |          13.9400 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |           0.0008 |          26.5906 |          13.9394 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0083 |          25.4402 |          13.9345 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0034 |          25.1622 |          13.9337 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0078 |          24.9429 |          13.9298 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0066 |          26.0186 |          13.9321 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0070 |          24.7943 |          13.9238 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0031 |          24.6294 |          13.9213 |
[32m[20221213 22:41:12 @agent_ppo2.py:185][0m |          -0.0135 |          24.3891 |          13.9133 |
[32m[20221213 22:41:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:41:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.78
[32m[20221213 22:41:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.89
[32m[20221213 22:41:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.18
[32m[20221213 22:41:12 @agent_ppo2.py:143][0m Total time:      22.99 min
[32m[20221213 22:41:12 @agent_ppo2.py:145][0m 2238464 total steps have happened
[32m[20221213 22:41:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1093 --------------------------#
[32m[20221213 22:41:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |           0.0096 |          30.0808 |          13.9191 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0007 |          27.7754 |          13.8897 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0059 |          27.3869 |          13.9137 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0041 |          27.1261 |          13.9101 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0076 |          27.0120 |          13.9142 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0099 |          26.7761 |          13.9008 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0079 |          26.7022 |          13.8979 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0048 |          26.7163 |          13.9077 |
[32m[20221213 22:41:13 @agent_ppo2.py:185][0m |          -0.0053 |          26.5071 |          13.9052 |
[32m[20221213 22:41:14 @agent_ppo2.py:185][0m |          -0.0060 |          26.4554 |          13.9058 |
[32m[20221213 22:41:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.75
[32m[20221213 22:41:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.63
[32m[20221213 22:41:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.70
[32m[20221213 22:41:14 @agent_ppo2.py:143][0m Total time:      23.01 min
[32m[20221213 22:41:14 @agent_ppo2.py:145][0m 2240512 total steps have happened
[32m[20221213 22:41:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1094 --------------------------#
[32m[20221213 22:41:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:41:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:14 @agent_ppo2.py:185][0m |          -0.0042 |          33.9707 |          13.8272 |
[32m[20221213 22:41:14 @agent_ppo2.py:185][0m |          -0.0021 |          33.3448 |          13.8060 |
[32m[20221213 22:41:14 @agent_ppo2.py:185][0m |          -0.0074 |          32.7097 |          13.8019 |
[32m[20221213 22:41:14 @agent_ppo2.py:185][0m |          -0.0092 |          32.3935 |          13.7838 |
[32m[20221213 22:41:14 @agent_ppo2.py:185][0m |           0.0053 |          36.0614 |          13.7886 |
[32m[20221213 22:41:14 @agent_ppo2.py:185][0m |          -0.0065 |          32.2682 |          13.7775 |
[32m[20221213 22:41:15 @agent_ppo2.py:185][0m |          -0.0016 |          33.6762 |          13.7870 |
[32m[20221213 22:41:15 @agent_ppo2.py:185][0m |          -0.0017 |          32.8039 |          13.7921 |
[32m[20221213 22:41:15 @agent_ppo2.py:185][0m |          -0.0048 |          31.9887 |          13.7822 |
[32m[20221213 22:41:15 @agent_ppo2.py:185][0m |          -0.0045 |          32.0772 |          13.7952 |
[32m[20221213 22:41:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:41:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.71
[32m[20221213 22:41:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.23
[32m[20221213 22:41:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.72
[32m[20221213 22:41:15 @agent_ppo2.py:143][0m Total time:      23.03 min
[32m[20221213 22:41:15 @agent_ppo2.py:145][0m 2242560 total steps have happened
[32m[20221213 22:41:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1095 --------------------------#
[32m[20221213 22:41:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:15 @agent_ppo2.py:185][0m |           0.0002 |          32.3872 |          13.8256 |
[32m[20221213 22:41:15 @agent_ppo2.py:185][0m |          -0.0006 |          31.6845 |          13.8265 |
[32m[20221213 22:41:15 @agent_ppo2.py:185][0m |          -0.0041 |          31.3483 |          13.8188 |
[32m[20221213 22:41:16 @agent_ppo2.py:185][0m |          -0.0028 |          31.0616 |          13.8176 |
[32m[20221213 22:41:16 @agent_ppo2.py:185][0m |          -0.0071 |          30.8127 |          13.8134 |
[32m[20221213 22:41:16 @agent_ppo2.py:185][0m |          -0.0052 |          30.7384 |          13.8113 |
[32m[20221213 22:41:16 @agent_ppo2.py:185][0m |          -0.0070 |          30.5768 |          13.8105 |
[32m[20221213 22:41:16 @agent_ppo2.py:185][0m |          -0.0070 |          30.5045 |          13.8128 |
[32m[20221213 22:41:16 @agent_ppo2.py:185][0m |          -0.0079 |          30.4609 |          13.8138 |
[32m[20221213 22:41:16 @agent_ppo2.py:185][0m |          -0.0077 |          30.3119 |          13.8017 |
[32m[20221213 22:41:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.84
[32m[20221213 22:41:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.82
[32m[20221213 22:41:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.78
[32m[20221213 22:41:16 @agent_ppo2.py:143][0m Total time:      23.06 min
[32m[20221213 22:41:16 @agent_ppo2.py:145][0m 2244608 total steps have happened
[32m[20221213 22:41:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1096 --------------------------#
[32m[20221213 22:41:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |           0.0040 |          24.0775 |          13.6896 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |           0.0026 |          22.8052 |          13.6867 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |           0.0126 |          25.0353 |          13.6687 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |          -0.0054 |          22.4488 |          13.6676 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |          -0.0030 |          21.9222 |          13.6712 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |          -0.0030 |          21.9345 |          13.6767 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |          -0.0073 |          21.7079 |          13.6665 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |          -0.0106 |          21.4212 |          13.6688 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |          -0.0110 |          21.3703 |          13.6600 |
[32m[20221213 22:41:17 @agent_ppo2.py:185][0m |          -0.0077 |          21.2995 |          13.6503 |
[32m[20221213 22:41:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.28
[32m[20221213 22:41:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.53
[32m[20221213 22:41:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.28
[32m[20221213 22:41:18 @agent_ppo2.py:143][0m Total time:      23.08 min
[32m[20221213 22:41:18 @agent_ppo2.py:145][0m 2246656 total steps have happened
[32m[20221213 22:41:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1097 --------------------------#
[32m[20221213 22:41:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0019 |          35.2305 |          13.6378 |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0049 |          33.8440 |          13.6295 |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0055 |          33.1670 |          13.6107 |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0049 |          32.8013 |          13.6219 |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0042 |          32.6965 |          13.6106 |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0040 |          32.2610 |          13.6201 |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0070 |          32.0906 |          13.6126 |
[32m[20221213 22:41:18 @agent_ppo2.py:185][0m |          -0.0106 |          31.9835 |          13.6193 |
[32m[20221213 22:41:19 @agent_ppo2.py:185][0m |          -0.0069 |          31.8320 |          13.6139 |
[32m[20221213 22:41:19 @agent_ppo2.py:185][0m |          -0.0097 |          31.6427 |          13.6126 |
[32m[20221213 22:41:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.50
[32m[20221213 22:41:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.52
[32m[20221213 22:41:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.38
[32m[20221213 22:41:19 @agent_ppo2.py:143][0m Total time:      23.10 min
[32m[20221213 22:41:19 @agent_ppo2.py:145][0m 2248704 total steps have happened
[32m[20221213 22:41:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1098 --------------------------#
[32m[20221213 22:41:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:19 @agent_ppo2.py:185][0m |           0.0019 |          32.4886 |          13.8511 |
[32m[20221213 22:41:19 @agent_ppo2.py:185][0m |          -0.0025 |          31.6451 |          13.8682 |
[32m[20221213 22:41:19 @agent_ppo2.py:185][0m |          -0.0028 |          31.4897 |          13.8521 |
[32m[20221213 22:41:19 @agent_ppo2.py:185][0m |          -0.0047 |          31.2932 |          13.8602 |
[32m[20221213 22:41:19 @agent_ppo2.py:185][0m |          -0.0047 |          31.1520 |          13.8518 |
[32m[20221213 22:41:20 @agent_ppo2.py:185][0m |          -0.0062 |          30.9316 |          13.8564 |
[32m[20221213 22:41:20 @agent_ppo2.py:185][0m |          -0.0078 |          30.8699 |          13.8517 |
[32m[20221213 22:41:20 @agent_ppo2.py:185][0m |          -0.0065 |          30.9439 |          13.8406 |
[32m[20221213 22:41:20 @agent_ppo2.py:185][0m |           0.0006 |          31.3100 |          13.8560 |
[32m[20221213 22:41:20 @agent_ppo2.py:185][0m |          -0.0032 |          31.0806 |          13.8537 |
[32m[20221213 22:41:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.87
[32m[20221213 22:41:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.24
[32m[20221213 22:41:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.30
[32m[20221213 22:41:20 @agent_ppo2.py:143][0m Total time:      23.12 min
[32m[20221213 22:41:20 @agent_ppo2.py:145][0m 2250752 total steps have happened
[32m[20221213 22:41:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1099 --------------------------#
[32m[20221213 22:41:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:41:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:20 @agent_ppo2.py:185][0m |          -0.0048 |          30.0407 |          13.8471 |
[32m[20221213 22:41:20 @agent_ppo2.py:185][0m |          -0.0058 |          28.3377 |          13.8323 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0025 |          27.8186 |          13.8181 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0137 |          27.6587 |          13.8299 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0067 |          27.4105 |          13.8183 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0087 |          27.2327 |          13.8112 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0092 |          27.0808 |          13.8101 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0072 |          27.0639 |          13.8153 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0113 |          26.8813 |          13.8130 |
[32m[20221213 22:41:21 @agent_ppo2.py:185][0m |          -0.0095 |          26.8264 |          13.8100 |
[32m[20221213 22:41:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 299.22
[32m[20221213 22:41:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 354.43
[32m[20221213 22:41:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.92
[32m[20221213 22:41:21 @agent_ppo2.py:143][0m Total time:      23.14 min
[32m[20221213 22:41:21 @agent_ppo2.py:145][0m 2252800 total steps have happened
[32m[20221213 22:41:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1100 --------------------------#
[32m[20221213 22:41:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:41:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |           0.0093 |          27.2452 |          14.0012 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0035 |          23.6979 |          13.9887 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0052 |          23.1746 |          13.9826 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0055 |          22.8886 |          13.9812 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0084 |          22.5186 |          13.9766 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0086 |          22.3628 |          13.9858 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0059 |          22.2230 |          13.9646 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0098 |          22.1355 |          13.9721 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0039 |          22.2086 |          13.9692 |
[32m[20221213 22:41:22 @agent_ppo2.py:185][0m |          -0.0074 |          21.8822 |          13.9664 |
[32m[20221213 22:41:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.71
[32m[20221213 22:41:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.52
[32m[20221213 22:41:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.85
[32m[20221213 22:41:23 @agent_ppo2.py:143][0m Total time:      23.16 min
[32m[20221213 22:41:23 @agent_ppo2.py:145][0m 2254848 total steps have happened
[32m[20221213 22:41:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1101 --------------------------#
[32m[20221213 22:41:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:23 @agent_ppo2.py:185][0m |          -0.0021 |          36.6345 |          13.9591 |
[32m[20221213 22:41:23 @agent_ppo2.py:185][0m |           0.0003 |          36.1616 |          13.9643 |
[32m[20221213 22:41:23 @agent_ppo2.py:185][0m |          -0.0002 |          35.2427 |          13.9598 |
[32m[20221213 22:41:23 @agent_ppo2.py:185][0m |          -0.0046 |          34.3340 |          13.9674 |
[32m[20221213 22:41:23 @agent_ppo2.py:185][0m |          -0.0076 |          34.3708 |          13.9661 |
[32m[20221213 22:41:23 @agent_ppo2.py:185][0m |          -0.0063 |          34.0371 |          13.9736 |
[32m[20221213 22:41:23 @agent_ppo2.py:185][0m |          -0.0099 |          34.0339 |          13.9642 |
[32m[20221213 22:41:24 @agent_ppo2.py:185][0m |          -0.0033 |          33.9416 |          13.9592 |
[32m[20221213 22:41:24 @agent_ppo2.py:185][0m |          -0.0081 |          33.6936 |          13.9687 |
[32m[20221213 22:41:24 @agent_ppo2.py:185][0m |          -0.0104 |          33.6543 |          13.9556 |
[32m[20221213 22:41:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.06
[32m[20221213 22:41:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.38
[32m[20221213 22:41:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.38
[32m[20221213 22:41:24 @agent_ppo2.py:143][0m Total time:      23.18 min
[32m[20221213 22:41:24 @agent_ppo2.py:145][0m 2256896 total steps have happened
[32m[20221213 22:41:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1102 --------------------------#
[32m[20221213 22:41:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:24 @agent_ppo2.py:185][0m |           0.0042 |          34.9386 |          13.9571 |
[32m[20221213 22:41:24 @agent_ppo2.py:185][0m |          -0.0061 |          32.5804 |          13.9189 |
[32m[20221213 22:41:24 @agent_ppo2.py:185][0m |          -0.0076 |          32.0254 |          13.9240 |
[32m[20221213 22:41:24 @agent_ppo2.py:185][0m |          -0.0074 |          31.4816 |          13.9278 |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0075 |          31.5318 |          13.9118 |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0114 |          31.0503 |          13.9202 |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0025 |          33.1816 |          13.9186 |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0050 |          32.3290 |          13.9175 |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0145 |          30.5793 |          13.9179 |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0145 |          30.4265 |          13.9125 |
[32m[20221213 22:41:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.57
[32m[20221213 22:41:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.76
[32m[20221213 22:41:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.97
[32m[20221213 22:41:25 @agent_ppo2.py:143][0m Total time:      23.20 min
[32m[20221213 22:41:25 @agent_ppo2.py:145][0m 2258944 total steps have happened
[32m[20221213 22:41:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1103 --------------------------#
[32m[20221213 22:41:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0018 |          36.0424 |          14.0293 |
[32m[20221213 22:41:25 @agent_ppo2.py:185][0m |          -0.0078 |          34.8660 |          14.0356 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |          -0.0072 |          34.1916 |          14.0224 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |          -0.0055 |          33.8973 |          14.0243 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |          -0.0105 |          33.2631 |          14.0187 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |          -0.0116 |          33.0144 |          14.0074 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |           0.0019 |          35.8235 |          14.0056 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |          -0.0078 |          32.7051 |          14.0190 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |          -0.0122 |          32.4062 |          13.9980 |
[32m[20221213 22:41:26 @agent_ppo2.py:185][0m |          -0.0127 |          32.2742 |          14.0088 |
[32m[20221213 22:41:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.11
[32m[20221213 22:41:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.06
[32m[20221213 22:41:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.00
[32m[20221213 22:41:26 @agent_ppo2.py:143][0m Total time:      23.22 min
[32m[20221213 22:41:26 @agent_ppo2.py:145][0m 2260992 total steps have happened
[32m[20221213 22:41:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1104 --------------------------#
[32m[20221213 22:41:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |           0.0021 |          36.1145 |          13.9836 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0051 |          32.4817 |          13.9732 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0042 |          31.5108 |          13.9680 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0081 |          31.1341 |          13.9570 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0094 |          30.6212 |          13.9420 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0129 |          30.3884 |          13.9616 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0125 |          30.2040 |          13.9474 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0104 |          30.1371 |          13.9465 |
[32m[20221213 22:41:27 @agent_ppo2.py:185][0m |          -0.0149 |          29.8596 |          13.9458 |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0129 |          29.7073 |          13.9528 |
[32m[20221213 22:41:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:41:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.68
[32m[20221213 22:41:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.78
[32m[20221213 22:41:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.88
[32m[20221213 22:41:28 @agent_ppo2.py:143][0m Total time:      23.25 min
[32m[20221213 22:41:28 @agent_ppo2.py:145][0m 2263040 total steps have happened
[32m[20221213 22:41:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1105 --------------------------#
[32m[20221213 22:41:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0061 |          31.4234 |          13.9551 |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0121 |          28.2727 |          13.9297 |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0006 |          27.0132 |          13.9073 |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0102 |          25.8465 |          13.9018 |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0025 |          27.7184 |          13.8969 |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0060 |          24.3427 |          13.9092 |
[32m[20221213 22:41:28 @agent_ppo2.py:185][0m |          -0.0170 |          23.9768 |          13.9070 |
[32m[20221213 22:41:29 @agent_ppo2.py:185][0m |          -0.0163 |          23.4592 |          13.9123 |
[32m[20221213 22:41:29 @agent_ppo2.py:185][0m |          -0.0130 |          23.2693 |          13.9006 |
[32m[20221213 22:41:29 @agent_ppo2.py:185][0m |          -0.0064 |          25.8862 |          13.9014 |
[32m[20221213 22:41:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.42
[32m[20221213 22:41:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.16
[32m[20221213 22:41:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.74
[32m[20221213 22:41:29 @agent_ppo2.py:143][0m Total time:      23.27 min
[32m[20221213 22:41:29 @agent_ppo2.py:145][0m 2265088 total steps have happened
[32m[20221213 22:41:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1106 --------------------------#
[32m[20221213 22:41:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:29 @agent_ppo2.py:185][0m |          -0.0018 |          31.0709 |          14.0783 |
[32m[20221213 22:41:29 @agent_ppo2.py:185][0m |          -0.0069 |          29.2003 |          14.0519 |
[32m[20221213 22:41:29 @agent_ppo2.py:185][0m |           0.0082 |          31.8363 |          14.0520 |
[32m[20221213 22:41:29 @agent_ppo2.py:185][0m |          -0.0072 |          28.1895 |          14.0259 |
[32m[20221213 22:41:30 @agent_ppo2.py:185][0m |          -0.0083 |          27.8596 |          14.0471 |
[32m[20221213 22:41:30 @agent_ppo2.py:185][0m |          -0.0087 |          27.7121 |          14.0494 |
[32m[20221213 22:41:30 @agent_ppo2.py:185][0m |          -0.0065 |          27.5377 |          14.0447 |
[32m[20221213 22:41:30 @agent_ppo2.py:185][0m |           0.0006 |          29.7647 |          14.0477 |
[32m[20221213 22:41:30 @agent_ppo2.py:185][0m |           0.0025 |          32.3114 |          14.0251 |
[32m[20221213 22:41:30 @agent_ppo2.py:185][0m |          -0.0069 |          27.3332 |          14.0158 |
[32m[20221213 22:41:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:41:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.35
[32m[20221213 22:41:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.38
[32m[20221213 22:41:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.96
[32m[20221213 22:41:30 @agent_ppo2.py:143][0m Total time:      23.29 min
[32m[20221213 22:41:30 @agent_ppo2.py:145][0m 2267136 total steps have happened
[32m[20221213 22:41:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1107 --------------------------#
[32m[20221213 22:41:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |           0.0006 |          33.6871 |          14.2031 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0060 |          29.6258 |          14.1784 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0070 |          28.3385 |          14.1821 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0059 |          27.6402 |          14.1840 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0124 |          27.4925 |          14.1862 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0102 |          26.8490 |          14.1894 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0136 |          26.5256 |          14.1868 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0122 |          26.3153 |          14.1808 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0184 |          26.1507 |          14.1825 |
[32m[20221213 22:41:31 @agent_ppo2.py:185][0m |          -0.0138 |          25.9062 |          14.1887 |
[32m[20221213 22:41:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.84
[32m[20221213 22:41:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.01
[32m[20221213 22:41:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.46
[32m[20221213 22:41:31 @agent_ppo2.py:143][0m Total time:      23.31 min
[32m[20221213 22:41:31 @agent_ppo2.py:145][0m 2269184 total steps have happened
[32m[20221213 22:41:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1108 --------------------------#
[32m[20221213 22:41:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:41:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |          -0.0053 |          38.1795 |          14.0808 |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |          -0.0048 |          34.9010 |          14.0550 |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |           0.0019 |          33.8724 |          14.0515 |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |          -0.0029 |          33.3854 |          14.0554 |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |          -0.0042 |          32.2925 |          14.0411 |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |          -0.0012 |          33.9786 |          14.0448 |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |          -0.0118 |          31.2564 |          14.0303 |
[32m[20221213 22:41:32 @agent_ppo2.py:185][0m |          -0.0108 |          30.8902 |          14.0425 |
[32m[20221213 22:41:33 @agent_ppo2.py:185][0m |          -0.0145 |          30.5754 |          14.0344 |
[32m[20221213 22:41:33 @agent_ppo2.py:185][0m |          -0.0073 |          30.4364 |          14.0319 |
[32m[20221213 22:41:33 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:41:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.15
[32m[20221213 22:41:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.64
[32m[20221213 22:41:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.77
[32m[20221213 22:41:33 @agent_ppo2.py:143][0m Total time:      23.33 min
[32m[20221213 22:41:33 @agent_ppo2.py:145][0m 2271232 total steps have happened
[32m[20221213 22:41:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1109 --------------------------#
[32m[20221213 22:41:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:41:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:33 @agent_ppo2.py:185][0m |          -0.0012 |          38.5727 |          14.0246 |
[32m[20221213 22:41:33 @agent_ppo2.py:185][0m |          -0.0048 |          35.0943 |          13.9981 |
[32m[20221213 22:41:33 @agent_ppo2.py:185][0m |          -0.0045 |          34.4153 |          14.0078 |
[32m[20221213 22:41:33 @agent_ppo2.py:185][0m |          -0.0010 |          33.7671 |          14.0123 |
[32m[20221213 22:41:34 @agent_ppo2.py:185][0m |          -0.0049 |          32.6593 |          14.0149 |
[32m[20221213 22:41:34 @agent_ppo2.py:185][0m |          -0.0070 |          32.4678 |          14.0125 |
[32m[20221213 22:41:34 @agent_ppo2.py:185][0m |          -0.0097 |          32.0802 |          13.9974 |
[32m[20221213 22:41:34 @agent_ppo2.py:185][0m |          -0.0125 |          31.7435 |          14.0087 |
[32m[20221213 22:41:34 @agent_ppo2.py:185][0m |          -0.0045 |          34.5502 |          13.9998 |
[32m[20221213 22:41:34 @agent_ppo2.py:185][0m |          -0.0094 |          31.6092 |          14.0003 |
[32m[20221213 22:41:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:41:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.64
[32m[20221213 22:41:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.74
[32m[20221213 22:41:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.69
[32m[20221213 22:41:34 @agent_ppo2.py:143][0m Total time:      23.35 min
[32m[20221213 22:41:34 @agent_ppo2.py:145][0m 2273280 total steps have happened
[32m[20221213 22:41:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1110 --------------------------#
[32m[20221213 22:41:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:41:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |           0.0001 |          30.5614 |          14.0487 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |           0.0019 |          30.9081 |          14.0245 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0063 |          29.4339 |          14.0210 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0089 |          29.0356 |          14.0298 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0111 |          28.9527 |          14.0392 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0108 |          28.7580 |          14.0242 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0091 |          28.5614 |          14.0223 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0079 |          28.5203 |          14.0192 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0122 |          28.4068 |          14.0211 |
[32m[20221213 22:41:35 @agent_ppo2.py:185][0m |          -0.0119 |          28.2766 |          14.0250 |
[32m[20221213 22:41:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:41:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.11
[32m[20221213 22:41:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.31
[32m[20221213 22:41:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.59
[32m[20221213 22:41:35 @agent_ppo2.py:143][0m Total time:      23.38 min
[32m[20221213 22:41:35 @agent_ppo2.py:145][0m 2275328 total steps have happened
[32m[20221213 22:41:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1111 --------------------------#
[32m[20221213 22:41:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:41:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |           0.0148 |          32.7926 |          14.1589 |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |          -0.0108 |          25.3162 |          14.1394 |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |          -0.0136 |          24.8648 |          14.1328 |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |          -0.0014 |          25.1243 |          14.1313 |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |          -0.0085 |          24.5874 |          14.1388 |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |          -0.0021 |          24.9342 |          14.1253 |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |          -0.0092 |          24.3639 |          14.1263 |
[32m[20221213 22:41:36 @agent_ppo2.py:185][0m |          -0.0046 |          24.3593 |          14.1265 |
[32m[20221213 22:41:37 @agent_ppo2.py:185][0m |          -0.0043 |          24.3864 |          14.1195 |
[32m[20221213 22:41:37 @agent_ppo2.py:185][0m |          -0.0086 |          24.2338 |          14.1184 |
[32m[20221213 22:41:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:41:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.18
[32m[20221213 22:41:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.80
[32m[20221213 22:41:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 205.30
[32m[20221213 22:41:37 @agent_ppo2.py:143][0m Total time:      23.40 min
[32m[20221213 22:41:37 @agent_ppo2.py:145][0m 2277376 total steps have happened
[32m[20221213 22:41:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1112 --------------------------#
[32m[20221213 22:41:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:37 @agent_ppo2.py:185][0m |           0.0122 |          31.5197 |          14.1254 |
[32m[20221213 22:41:37 @agent_ppo2.py:185][0m |          -0.0088 |          24.4996 |          14.1078 |
[32m[20221213 22:41:37 @agent_ppo2.py:185][0m |          -0.0058 |          23.2467 |          14.1009 |
[32m[20221213 22:41:37 @agent_ppo2.py:185][0m |          -0.0043 |          22.6511 |          14.0973 |
[32m[20221213 22:41:37 @agent_ppo2.py:185][0m |          -0.0133 |          21.8286 |          14.0959 |
[32m[20221213 22:41:38 @agent_ppo2.py:185][0m |          -0.0117 |          21.3944 |          14.0909 |
[32m[20221213 22:41:38 @agent_ppo2.py:185][0m |          -0.0095 |          20.9093 |          14.0848 |
[32m[20221213 22:41:38 @agent_ppo2.py:185][0m |          -0.0106 |          20.5618 |          14.0832 |
[32m[20221213 22:41:38 @agent_ppo2.py:185][0m |          -0.0096 |          20.3126 |          14.0879 |
[32m[20221213 22:41:38 @agent_ppo2.py:185][0m |          -0.0168 |          20.0353 |          14.0708 |
[32m[20221213 22:41:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.76
[32m[20221213 22:41:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.24
[32m[20221213 22:41:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.50
[32m[20221213 22:41:38 @agent_ppo2.py:143][0m Total time:      23.42 min
[32m[20221213 22:41:38 @agent_ppo2.py:145][0m 2279424 total steps have happened
[32m[20221213 22:41:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1113 --------------------------#
[32m[20221213 22:41:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:38 @agent_ppo2.py:185][0m |          -0.0074 |          27.9959 |          14.1451 |
[32m[20221213 22:41:38 @agent_ppo2.py:185][0m |          -0.0039 |          25.2923 |          14.1279 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0092 |          24.3672 |          14.1223 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0037 |          23.7041 |          14.1251 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0095 |          23.2666 |          14.1095 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0035 |          23.4263 |          14.1149 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0059 |          22.6993 |          14.1110 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0139 |          22.2837 |          14.1069 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0137 |          22.1775 |          14.1091 |
[32m[20221213 22:41:39 @agent_ppo2.py:185][0m |          -0.0115 |          21.9583 |          14.1042 |
[32m[20221213 22:41:39 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:41:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.12
[32m[20221213 22:41:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.11
[32m[20221213 22:41:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.71
[32m[20221213 22:41:39 @agent_ppo2.py:143][0m Total time:      23.44 min
[32m[20221213 22:41:39 @agent_ppo2.py:145][0m 2281472 total steps have happened
[32m[20221213 22:41:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1114 --------------------------#
[32m[20221213 22:41:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |           0.0036 |          43.3353 |          14.3014 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |           0.0001 |          40.3407 |          14.2562 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |          -0.0095 |          37.5173 |          14.2741 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |          -0.0076 |          36.9628 |          14.2722 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |          -0.0085 |          36.5320 |          14.2706 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |          -0.0079 |          36.2656 |          14.2752 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |          -0.0070 |          36.4950 |          14.2795 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |           0.0019 |          38.9955 |          14.2636 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |          -0.0118 |          35.8604 |          14.2688 |
[32m[20221213 22:41:40 @agent_ppo2.py:185][0m |          -0.0122 |          35.6602 |          14.2827 |
[32m[20221213 22:41:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.82
[32m[20221213 22:41:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.87
[32m[20221213 22:41:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.25
[32m[20221213 22:41:41 @agent_ppo2.py:143][0m Total time:      23.46 min
[32m[20221213 22:41:41 @agent_ppo2.py:145][0m 2283520 total steps have happened
[32m[20221213 22:41:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1115 --------------------------#
[32m[20221213 22:41:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:41 @agent_ppo2.py:185][0m |          -0.0039 |          33.1202 |          14.0926 |
[32m[20221213 22:41:41 @agent_ppo2.py:185][0m |          -0.0124 |          27.9958 |          14.0703 |
[32m[20221213 22:41:41 @agent_ppo2.py:185][0m |          -0.0050 |          26.8247 |          14.0656 |
[32m[20221213 22:41:41 @agent_ppo2.py:185][0m |          -0.0064 |          26.2228 |          14.0695 |
[32m[20221213 22:41:41 @agent_ppo2.py:185][0m |          -0.0086 |          25.8379 |          14.0614 |
[32m[20221213 22:41:41 @agent_ppo2.py:185][0m |          -0.0089 |          25.5938 |          14.0518 |
[32m[20221213 22:41:41 @agent_ppo2.py:185][0m |          -0.0082 |          25.3220 |          14.0676 |
[32m[20221213 22:41:42 @agent_ppo2.py:185][0m |          -0.0131 |          25.1774 |          14.0520 |
[32m[20221213 22:41:42 @agent_ppo2.py:185][0m |          -0.0117 |          24.7939 |          14.0552 |
[32m[20221213 22:41:42 @agent_ppo2.py:185][0m |          -0.0113 |          24.7072 |          14.0461 |
[32m[20221213 22:41:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:41:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.35
[32m[20221213 22:41:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.77
[32m[20221213 22:41:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.97
[32m[20221213 22:41:42 @agent_ppo2.py:143][0m Total time:      23.48 min
[32m[20221213 22:41:42 @agent_ppo2.py:145][0m 2285568 total steps have happened
[32m[20221213 22:41:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1116 --------------------------#
[32m[20221213 22:41:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:42 @agent_ppo2.py:185][0m |          -0.0044 |          31.6526 |          14.1098 |
[32m[20221213 22:41:42 @agent_ppo2.py:185][0m |          -0.0063 |          28.0957 |          14.0950 |
[32m[20221213 22:41:42 @agent_ppo2.py:185][0m |          -0.0089 |          26.9188 |          14.0965 |
[32m[20221213 22:41:42 @agent_ppo2.py:185][0m |          -0.0078 |          26.2600 |          14.0737 |
[32m[20221213 22:41:43 @agent_ppo2.py:185][0m |          -0.0001 |          27.6404 |          14.0885 |
[32m[20221213 22:41:43 @agent_ppo2.py:185][0m |          -0.0119 |          25.6757 |          14.0831 |
[32m[20221213 22:41:43 @agent_ppo2.py:185][0m |          -0.0058 |          27.2081 |          14.0872 |
[32m[20221213 22:41:43 @agent_ppo2.py:185][0m |          -0.0107 |          25.3379 |          14.0880 |
[32m[20221213 22:41:43 @agent_ppo2.py:185][0m |          -0.0058 |          27.1905 |          14.0886 |
[32m[20221213 22:41:43 @agent_ppo2.py:185][0m |          -0.0017 |          26.6928 |          14.0557 |
[32m[20221213 22:41:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.58
[32m[20221213 22:41:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.69
[32m[20221213 22:41:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.28
[32m[20221213 22:41:43 @agent_ppo2.py:143][0m Total time:      23.50 min
[32m[20221213 22:41:43 @agent_ppo2.py:145][0m 2287616 total steps have happened
[32m[20221213 22:41:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1117 --------------------------#
[32m[20221213 22:41:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:43 @agent_ppo2.py:185][0m |           0.0012 |          30.6439 |          14.1459 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0044 |          28.6953 |          14.1170 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0111 |          28.0708 |          14.1067 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0065 |          27.7106 |          14.1187 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0063 |          27.5630 |          14.1093 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0137 |          27.1230 |          14.1111 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |           0.0000 |          28.1591 |          14.1144 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0096 |          27.0688 |          14.0960 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0102 |          26.6498 |          14.1168 |
[32m[20221213 22:41:44 @agent_ppo2.py:185][0m |          -0.0120 |          26.5243 |          14.1121 |
[32m[20221213 22:41:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.19
[32m[20221213 22:41:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.27
[32m[20221213 22:41:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.40
[32m[20221213 22:41:44 @agent_ppo2.py:143][0m Total time:      23.52 min
[32m[20221213 22:41:44 @agent_ppo2.py:145][0m 2289664 total steps have happened
[32m[20221213 22:41:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1118 --------------------------#
[32m[20221213 22:41:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0045 |          29.1164 |          14.2284 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0046 |          26.8779 |          14.2166 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0113 |          26.1117 |          14.2039 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0056 |          25.6795 |          14.2066 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0097 |          25.3308 |          14.1961 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0090 |          25.3158 |          14.2026 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0116 |          24.9666 |          14.1855 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0135 |          24.6527 |          14.1922 |
[32m[20221213 22:41:45 @agent_ppo2.py:185][0m |          -0.0078 |          24.6888 |          14.1910 |
[32m[20221213 22:41:46 @agent_ppo2.py:185][0m |          -0.0047 |          25.0155 |          14.1928 |
[32m[20221213 22:41:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:41:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.98
[32m[20221213 22:41:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.22
[32m[20221213 22:41:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.16
[32m[20221213 22:41:46 @agent_ppo2.py:143][0m Total time:      23.55 min
[32m[20221213 22:41:46 @agent_ppo2.py:145][0m 2291712 total steps have happened
[32m[20221213 22:41:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1119 --------------------------#
[32m[20221213 22:41:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:46 @agent_ppo2.py:185][0m |           0.0012 |          29.7581 |          13.9623 |
[32m[20221213 22:41:46 @agent_ppo2.py:185][0m |          -0.0008 |          27.2720 |          13.9590 |
[32m[20221213 22:41:46 @agent_ppo2.py:185][0m |          -0.0069 |          26.3277 |          13.9613 |
[32m[20221213 22:41:46 @agent_ppo2.py:185][0m |          -0.0087 |          25.6284 |          13.9656 |
[32m[20221213 22:41:46 @agent_ppo2.py:185][0m |          -0.0077 |          25.1721 |          13.9507 |
[32m[20221213 22:41:46 @agent_ppo2.py:185][0m |          -0.0096 |          24.8448 |          13.9419 |
[32m[20221213 22:41:47 @agent_ppo2.py:185][0m |          -0.0124 |          24.5582 |          13.9439 |
[32m[20221213 22:41:47 @agent_ppo2.py:185][0m |          -0.0119 |          24.3760 |          13.9486 |
[32m[20221213 22:41:47 @agent_ppo2.py:185][0m |          -0.0141 |          24.2057 |          13.9440 |
[32m[20221213 22:41:47 @agent_ppo2.py:185][0m |          -0.0137 |          24.1029 |          13.9487 |
[32m[20221213 22:41:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.27
[32m[20221213 22:41:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.26
[32m[20221213 22:41:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.13
[32m[20221213 22:41:47 @agent_ppo2.py:143][0m Total time:      23.57 min
[32m[20221213 22:41:47 @agent_ppo2.py:145][0m 2293760 total steps have happened
[32m[20221213 22:41:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1120 --------------------------#
[32m[20221213 22:41:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:41:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:47 @agent_ppo2.py:185][0m |          -0.0055 |          30.1563 |          14.1710 |
[32m[20221213 22:41:47 @agent_ppo2.py:185][0m |           0.0025 |          27.0661 |          14.1474 |
[32m[20221213 22:41:47 @agent_ppo2.py:185][0m |          -0.0050 |          25.3185 |          14.1447 |
[32m[20221213 22:41:48 @agent_ppo2.py:185][0m |          -0.0084 |          24.6245 |          14.1373 |
[32m[20221213 22:41:48 @agent_ppo2.py:185][0m |          -0.0047 |          24.3203 |          14.1334 |
[32m[20221213 22:41:48 @agent_ppo2.py:185][0m |          -0.0136 |          23.9202 |          14.1337 |
[32m[20221213 22:41:48 @agent_ppo2.py:185][0m |          -0.0049 |          23.7690 |          14.1378 |
[32m[20221213 22:41:48 @agent_ppo2.py:185][0m |          -0.0119 |          23.5705 |          14.1323 |
[32m[20221213 22:41:48 @agent_ppo2.py:185][0m |          -0.0131 |          23.3869 |          14.1280 |
[32m[20221213 22:41:48 @agent_ppo2.py:185][0m |          -0.0102 |          23.2223 |          14.1269 |
[32m[20221213 22:41:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:41:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.84
[32m[20221213 22:41:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.07
[32m[20221213 22:41:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.02
[32m[20221213 22:41:48 @agent_ppo2.py:143][0m Total time:      23.59 min
[32m[20221213 22:41:48 @agent_ppo2.py:145][0m 2295808 total steps have happened
[32m[20221213 22:41:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1121 --------------------------#
[32m[20221213 22:41:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |           0.0072 |          26.6880 |          14.2881 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |           0.0016 |          22.6656 |          14.2701 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0042 |          21.8459 |          14.2746 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0039 |          21.3618 |          14.2705 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0029 |          20.9968 |          14.2670 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0056 |          20.7398 |          14.2633 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0019 |          20.5471 |          14.2689 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0060 |          20.5039 |          14.2587 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0110 |          20.2051 |          14.2517 |
[32m[20221213 22:41:49 @agent_ppo2.py:185][0m |          -0.0049 |          19.9416 |          14.2539 |
[32m[20221213 22:41:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.36
[32m[20221213 22:41:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 357.75
[32m[20221213 22:41:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.86
[32m[20221213 22:41:49 @agent_ppo2.py:143][0m Total time:      23.61 min
[32m[20221213 22:41:49 @agent_ppo2.py:145][0m 2297856 total steps have happened
[32m[20221213 22:41:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1122 --------------------------#
[32m[20221213 22:41:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |           0.0029 |          25.1748 |          14.4572 |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |          -0.0041 |          22.3574 |          14.4300 |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |          -0.0089 |          21.6179 |          14.4286 |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |          -0.0083 |          21.1647 |          14.4156 |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |          -0.0087 |          20.8418 |          14.4174 |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |           0.0002 |          21.6076 |          14.4191 |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |          -0.0115 |          20.4377 |          14.4149 |
[32m[20221213 22:41:50 @agent_ppo2.py:185][0m |          -0.0133 |          20.3216 |          14.4169 |
[32m[20221213 22:41:51 @agent_ppo2.py:185][0m |          -0.0147 |          20.0549 |          14.4194 |
[32m[20221213 22:41:51 @agent_ppo2.py:185][0m |          -0.0119 |          20.0178 |          14.4278 |
[32m[20221213 22:41:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:41:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.36
[32m[20221213 22:41:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.92
[32m[20221213 22:41:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.91
[32m[20221213 22:41:51 @agent_ppo2.py:143][0m Total time:      23.63 min
[32m[20221213 22:41:51 @agent_ppo2.py:145][0m 2299904 total steps have happened
[32m[20221213 22:41:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1123 --------------------------#
[32m[20221213 22:41:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:41:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:51 @agent_ppo2.py:185][0m |          -0.0042 |          19.6824 |          14.2017 |
[32m[20221213 22:41:51 @agent_ppo2.py:185][0m |           0.0042 |          17.7047 |          14.2035 |
[32m[20221213 22:41:51 @agent_ppo2.py:185][0m |          -0.0032 |          16.0975 |          14.1958 |
[32m[20221213 22:41:51 @agent_ppo2.py:185][0m |          -0.0089 |          15.6903 |          14.1919 |
[32m[20221213 22:41:51 @agent_ppo2.py:185][0m |          -0.0016 |          15.4396 |          14.1835 |
[32m[20221213 22:41:52 @agent_ppo2.py:185][0m |          -0.0072 |          14.9960 |          14.1867 |
[32m[20221213 22:41:52 @agent_ppo2.py:185][0m |          -0.0034 |          15.0107 |          14.1851 |
[32m[20221213 22:41:52 @agent_ppo2.py:185][0m |          -0.0050 |          15.0076 |          14.1874 |
[32m[20221213 22:41:52 @agent_ppo2.py:185][0m |          -0.0135 |          14.5230 |          14.1882 |
[32m[20221213 22:41:52 @agent_ppo2.py:185][0m |          -0.0135 |          14.2686 |          14.1918 |
[32m[20221213 22:41:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.41
[32m[20221213 22:41:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.43
[32m[20221213 22:41:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.67
[32m[20221213 22:41:52 @agent_ppo2.py:143][0m Total time:      23.65 min
[32m[20221213 22:41:52 @agent_ppo2.py:145][0m 2301952 total steps have happened
[32m[20221213 22:41:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1124 --------------------------#
[32m[20221213 22:41:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:52 @agent_ppo2.py:185][0m |           0.0030 |          28.0262 |          14.5009 |
[32m[20221213 22:41:52 @agent_ppo2.py:185][0m |          -0.0045 |          22.9176 |          14.4902 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0013 |          22.0071 |          14.4819 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0004 |          21.9695 |          14.4691 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0054 |          21.0399 |          14.4726 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0106 |          20.8185 |          14.4622 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0134 |          20.4997 |          14.4636 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0118 |          20.3341 |          14.4671 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0147 |          20.1139 |          14.4548 |
[32m[20221213 22:41:53 @agent_ppo2.py:185][0m |          -0.0139 |          20.0770 |          14.4653 |
[32m[20221213 22:41:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:41:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.04
[32m[20221213 22:41:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 314.17
[32m[20221213 22:41:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.27
[32m[20221213 22:41:53 @agent_ppo2.py:143][0m Total time:      23.67 min
[32m[20221213 22:41:53 @agent_ppo2.py:145][0m 2304000 total steps have happened
[32m[20221213 22:41:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1125 --------------------------#
[32m[20221213 22:41:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |           0.0010 |          26.4202 |          14.4215 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0089 |          23.6468 |          14.4133 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0073 |          22.6326 |          14.4077 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0061 |          21.9205 |          14.4019 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0097 |          21.2872 |          14.3949 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0043 |          21.2072 |          14.3924 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0087 |          20.4242 |          14.3921 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0115 |          20.4430 |          14.3969 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0080 |          19.8323 |          14.3805 |
[32m[20221213 22:41:54 @agent_ppo2.py:185][0m |          -0.0151 |          19.7185 |          14.3894 |
[32m[20221213 22:41:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:41:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.13
[32m[20221213 22:41:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.58
[32m[20221213 22:41:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.35
[32m[20221213 22:41:55 @agent_ppo2.py:143][0m Total time:      23.69 min
[32m[20221213 22:41:55 @agent_ppo2.py:145][0m 2306048 total steps have happened
[32m[20221213 22:41:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1126 --------------------------#
[32m[20221213 22:41:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0016 |          25.8055 |          14.2548 |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0049 |          22.8639 |          14.2090 |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0066 |          21.9749 |          14.2071 |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0038 |          21.0769 |          14.2116 |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0107 |          20.4066 |          14.2030 |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0109 |          19.8508 |          14.2002 |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0035 |          21.2658 |          14.2036 |
[32m[20221213 22:41:55 @agent_ppo2.py:185][0m |          -0.0092 |          19.2352 |          14.1889 |
[32m[20221213 22:41:56 @agent_ppo2.py:185][0m |          -0.0135 |          18.8773 |          14.1947 |
[32m[20221213 22:41:56 @agent_ppo2.py:185][0m |          -0.0082 |          18.8525 |          14.1917 |
[32m[20221213 22:41:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 308.85
[32m[20221213 22:41:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.84
[32m[20221213 22:41:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.24
[32m[20221213 22:41:56 @agent_ppo2.py:143][0m Total time:      23.72 min
[32m[20221213 22:41:56 @agent_ppo2.py:145][0m 2308096 total steps have happened
[32m[20221213 22:41:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1127 --------------------------#
[32m[20221213 22:41:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:56 @agent_ppo2.py:185][0m |          -0.0004 |          29.4860 |          14.2475 |
[32m[20221213 22:41:56 @agent_ppo2.py:185][0m |          -0.0038 |          27.3632 |          14.2231 |
[32m[20221213 22:41:56 @agent_ppo2.py:185][0m |          -0.0031 |          27.0212 |          14.2164 |
[32m[20221213 22:41:56 @agent_ppo2.py:185][0m |          -0.0092 |          26.5781 |          14.2130 |
[32m[20221213 22:41:57 @agent_ppo2.py:185][0m |          -0.0083 |          26.2096 |          14.1990 |
[32m[20221213 22:41:57 @agent_ppo2.py:185][0m |          -0.0081 |          26.1657 |          14.2015 |
[32m[20221213 22:41:57 @agent_ppo2.py:185][0m |          -0.0121 |          26.0261 |          14.2022 |
[32m[20221213 22:41:57 @agent_ppo2.py:185][0m |          -0.0108 |          25.7405 |          14.2001 |
[32m[20221213 22:41:57 @agent_ppo2.py:185][0m |          -0.0067 |          25.8555 |          14.1972 |
[32m[20221213 22:41:57 @agent_ppo2.py:185][0m |          -0.0109 |          25.6199 |          14.1988 |
[32m[20221213 22:41:57 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:41:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.96
[32m[20221213 22:41:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.21
[32m[20221213 22:41:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.20
[32m[20221213 22:41:57 @agent_ppo2.py:143][0m Total time:      23.74 min
[32m[20221213 22:41:57 @agent_ppo2.py:145][0m 2310144 total steps have happened
[32m[20221213 22:41:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1128 --------------------------#
[32m[20221213 22:41:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:41:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:57 @agent_ppo2.py:185][0m |          -0.0027 |          32.0621 |          14.3781 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0074 |          29.4487 |          14.3580 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0086 |          28.9505 |          14.3535 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0095 |          28.4989 |          14.3430 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0114 |          28.2205 |          14.3382 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0148 |          28.0542 |          14.3314 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0112 |          27.9219 |          14.3505 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0112 |          27.8296 |          14.3330 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0122 |          27.6314 |          14.3378 |
[32m[20221213 22:41:58 @agent_ppo2.py:185][0m |          -0.0026 |          30.5529 |          14.3292 |
[32m[20221213 22:41:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:41:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.49
[32m[20221213 22:41:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.21
[32m[20221213 22:41:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.77
[32m[20221213 22:41:58 @agent_ppo2.py:143][0m Total time:      23.76 min
[32m[20221213 22:41:58 @agent_ppo2.py:145][0m 2312192 total steps have happened
[32m[20221213 22:41:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1129 --------------------------#
[32m[20221213 22:41:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:41:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |           0.0056 |          38.3957 |          14.4575 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |           0.0004 |          37.2779 |          14.4621 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |          -0.0048 |          36.2141 |          14.4613 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |          -0.0082 |          35.8905 |          14.4534 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |          -0.0056 |          35.7004 |          14.4504 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |          -0.0050 |          36.0729 |          14.4494 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |          -0.0016 |          35.9916 |          14.4556 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |          -0.0094 |          35.2504 |          14.4589 |
[32m[20221213 22:41:59 @agent_ppo2.py:185][0m |          -0.0034 |          35.7551 |          14.4397 |
[32m[20221213 22:42:00 @agent_ppo2.py:185][0m |          -0.0071 |          35.1854 |          14.4548 |
[32m[20221213 22:42:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:42:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.11
[32m[20221213 22:42:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.13
[32m[20221213 22:42:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.95
[32m[20221213 22:42:00 @agent_ppo2.py:143][0m Total time:      23.78 min
[32m[20221213 22:42:00 @agent_ppo2.py:145][0m 2314240 total steps have happened
[32m[20221213 22:42:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1130 --------------------------#
[32m[20221213 22:42:00 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:42:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:00 @agent_ppo2.py:185][0m |          -0.0006 |          36.5979 |          14.5358 |
[32m[20221213 22:42:00 @agent_ppo2.py:185][0m |          -0.0001 |          34.3855 |          14.5141 |
[32m[20221213 22:42:00 @agent_ppo2.py:185][0m |          -0.0058 |          33.4478 |          14.5111 |
[32m[20221213 22:42:00 @agent_ppo2.py:185][0m |          -0.0052 |          33.1640 |          14.5069 |
[32m[20221213 22:42:00 @agent_ppo2.py:185][0m |          -0.0091 |          32.7038 |          14.5016 |
[32m[20221213 22:42:00 @agent_ppo2.py:185][0m |          -0.0110 |          32.3927 |          14.5007 |
[32m[20221213 22:42:01 @agent_ppo2.py:185][0m |          -0.0057 |          32.4627 |          14.5071 |
[32m[20221213 22:42:01 @agent_ppo2.py:185][0m |          -0.0040 |          32.1472 |          14.5042 |
[32m[20221213 22:42:01 @agent_ppo2.py:185][0m |          -0.0117 |          31.9038 |          14.5010 |
[32m[20221213 22:42:01 @agent_ppo2.py:185][0m |          -0.0033 |          31.7787 |          14.5102 |
[32m[20221213 22:42:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:42:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.05
[32m[20221213 22:42:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.90
[32m[20221213 22:42:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.61
[32m[20221213 22:42:01 @agent_ppo2.py:143][0m Total time:      23.80 min
[32m[20221213 22:42:01 @agent_ppo2.py:145][0m 2316288 total steps have happened
[32m[20221213 22:42:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1131 --------------------------#
[32m[20221213 22:42:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:01 @agent_ppo2.py:185][0m |           0.0124 |          35.9456 |          14.3643 |
[32m[20221213 22:42:01 @agent_ppo2.py:185][0m |          -0.0002 |          29.3104 |          14.3535 |
[32m[20221213 22:42:01 @agent_ppo2.py:185][0m |          -0.0085 |          27.4914 |          14.3622 |
[32m[20221213 22:42:02 @agent_ppo2.py:185][0m |          -0.0091 |          26.8340 |          14.3545 |
[32m[20221213 22:42:02 @agent_ppo2.py:185][0m |          -0.0103 |          26.3088 |          14.3544 |
[32m[20221213 22:42:02 @agent_ppo2.py:185][0m |          -0.0117 |          26.1393 |          14.3430 |
[32m[20221213 22:42:02 @agent_ppo2.py:185][0m |          -0.0125 |          25.9606 |          14.3448 |
[32m[20221213 22:42:02 @agent_ppo2.py:185][0m |          -0.0056 |          27.9224 |          14.3460 |
[32m[20221213 22:42:02 @agent_ppo2.py:185][0m |          -0.0060 |          25.9128 |          14.3361 |
[32m[20221213 22:42:02 @agent_ppo2.py:185][0m |          -0.0091 |          25.6273 |          14.3393 |
[32m[20221213 22:42:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:42:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.20
[32m[20221213 22:42:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.45
[32m[20221213 22:42:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.47
[32m[20221213 22:42:02 @agent_ppo2.py:143][0m Total time:      23.82 min
[32m[20221213 22:42:02 @agent_ppo2.py:145][0m 2318336 total steps have happened
[32m[20221213 22:42:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1132 --------------------------#
[32m[20221213 22:42:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |           0.0068 |          26.1615 |          14.3157 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0043 |          21.7780 |          14.3013 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0024 |          20.9727 |          14.2882 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0102 |          20.4593 |          14.2914 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0089 |          20.4733 |          14.2931 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0089 |          20.1491 |          14.2810 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0143 |          19.6819 |          14.2776 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0100 |          19.6367 |          14.2700 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0123 |          19.4574 |          14.2780 |
[32m[20221213 22:42:03 @agent_ppo2.py:185][0m |          -0.0131 |          19.3348 |          14.2602 |
[32m[20221213 22:42:03 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:42:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.69
[32m[20221213 22:42:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.64
[32m[20221213 22:42:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.06
[32m[20221213 22:42:04 @agent_ppo2.py:143][0m Total time:      23.84 min
[32m[20221213 22:42:04 @agent_ppo2.py:145][0m 2320384 total steps have happened
[32m[20221213 22:42:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1133 --------------------------#
[32m[20221213 22:42:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0013 |          36.3987 |          14.3987 |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0060 |          33.3357 |          14.3837 |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0081 |          32.8985 |          14.3686 |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0061 |          32.4101 |          14.3598 |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0092 |          32.1996 |          14.3658 |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0080 |          32.0065 |          14.3734 |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0106 |          31.8186 |          14.3609 |
[32m[20221213 22:42:04 @agent_ppo2.py:185][0m |          -0.0082 |          31.5729 |          14.3688 |
[32m[20221213 22:42:05 @agent_ppo2.py:185][0m |          -0.0145 |          31.5554 |          14.3733 |
[32m[20221213 22:42:05 @agent_ppo2.py:185][0m |           0.0070 |          35.6508 |          14.3666 |
[32m[20221213 22:42:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.55
[32m[20221213 22:42:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.93
[32m[20221213 22:42:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.13
[32m[20221213 22:42:05 @agent_ppo2.py:143][0m Total time:      23.86 min
[32m[20221213 22:42:05 @agent_ppo2.py:145][0m 2322432 total steps have happened
[32m[20221213 22:42:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1134 --------------------------#
[32m[20221213 22:42:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:05 @agent_ppo2.py:185][0m |           0.0119 |          41.5727 |          14.4936 |
[32m[20221213 22:42:05 @agent_ppo2.py:185][0m |          -0.0062 |          36.8168 |          14.4704 |
[32m[20221213 22:42:05 @agent_ppo2.py:185][0m |          -0.0072 |          35.7671 |          14.4783 |
[32m[20221213 22:42:05 @agent_ppo2.py:185][0m |          -0.0096 |          35.3219 |          14.4775 |
[32m[20221213 22:42:05 @agent_ppo2.py:185][0m |          -0.0115 |          35.0937 |          14.4821 |
[32m[20221213 22:42:06 @agent_ppo2.py:185][0m |          -0.0078 |          34.7606 |          14.4717 |
[32m[20221213 22:42:06 @agent_ppo2.py:185][0m |          -0.0082 |          34.5348 |          14.4755 |
[32m[20221213 22:42:06 @agent_ppo2.py:185][0m |          -0.0104 |          34.2222 |          14.4882 |
[32m[20221213 22:42:06 @agent_ppo2.py:185][0m |           0.0018 |          39.9447 |          14.4844 |
[32m[20221213 22:42:06 @agent_ppo2.py:185][0m |          -0.0124 |          34.0698 |          14.4791 |
[32m[20221213 22:42:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:42:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.76
[32m[20221213 22:42:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.10
[32m[20221213 22:42:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.14
[32m[20221213 22:42:06 @agent_ppo2.py:143][0m Total time:      23.89 min
[32m[20221213 22:42:06 @agent_ppo2.py:145][0m 2324480 total steps have happened
[32m[20221213 22:42:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1135 --------------------------#
[32m[20221213 22:42:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:42:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:06 @agent_ppo2.py:185][0m |          -0.0026 |          11.3581 |          14.3714 |
[32m[20221213 22:42:06 @agent_ppo2.py:185][0m |          -0.0031 |          10.0245 |          14.3419 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |          -0.0076 |           9.8682 |          14.3586 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |          -0.0079 |           9.8216 |          14.3443 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |          -0.0065 |           9.9109 |          14.3445 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |          -0.0083 |           9.7966 |          14.3518 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |          -0.0079 |           9.7794 |          14.3303 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |          -0.0080 |           9.7535 |          14.3459 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |          -0.0101 |           9.7473 |          14.3373 |
[32m[20221213 22:42:07 @agent_ppo2.py:185][0m |           0.0033 |          11.1225 |          14.3397 |
[32m[20221213 22:42:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:42:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:42:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.50
[32m[20221213 22:42:07 @agent_ppo2.py:143][0m Total time:      23.91 min
[32m[20221213 22:42:07 @agent_ppo2.py:145][0m 2326528 total steps have happened
[32m[20221213 22:42:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1136 --------------------------#
[32m[20221213 22:42:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |           0.0059 |          39.1336 |          14.6641 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0038 |          33.9828 |          14.6505 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0066 |          32.9546 |          14.6520 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0080 |          32.1997 |          14.6474 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0010 |          32.8889 |          14.6537 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0118 |          31.3045 |          14.6541 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0117 |          30.8460 |          14.6519 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0099 |          30.5106 |          14.6470 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0122 |          30.4165 |          14.6553 |
[32m[20221213 22:42:08 @agent_ppo2.py:185][0m |          -0.0149 |          30.0143 |          14.6429 |
[32m[20221213 22:42:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:42:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.74
[32m[20221213 22:42:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.30
[32m[20221213 22:42:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.92
[32m[20221213 22:42:09 @agent_ppo2.py:143][0m Total time:      23.93 min
[32m[20221213 22:42:09 @agent_ppo2.py:145][0m 2328576 total steps have happened
[32m[20221213 22:42:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1137 --------------------------#
[32m[20221213 22:42:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:42:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:09 @agent_ppo2.py:185][0m |          -0.0019 |          32.2844 |          14.4928 |
[32m[20221213 22:42:09 @agent_ppo2.py:185][0m |          -0.0044 |          29.4391 |          14.4919 |
[32m[20221213 22:42:09 @agent_ppo2.py:185][0m |          -0.0099 |          28.3797 |          14.4758 |
[32m[20221213 22:42:09 @agent_ppo2.py:185][0m |          -0.0097 |          27.4579 |          14.4807 |
[32m[20221213 22:42:09 @agent_ppo2.py:185][0m |          -0.0120 |          27.0401 |          14.4817 |
[32m[20221213 22:42:10 @agent_ppo2.py:185][0m |          -0.0126 |          26.6291 |          14.4721 |
[32m[20221213 22:42:10 @agent_ppo2.py:185][0m |          -0.0103 |          26.2067 |          14.4665 |
[32m[20221213 22:42:10 @agent_ppo2.py:185][0m |          -0.0021 |          29.1895 |          14.4747 |
[32m[20221213 22:42:10 @agent_ppo2.py:185][0m |          -0.0154 |          25.9783 |          14.4707 |
[32m[20221213 22:42:10 @agent_ppo2.py:185][0m |          -0.0121 |          25.6301 |          14.4687 |
[32m[20221213 22:42:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:42:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.58
[32m[20221213 22:42:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.87
[32m[20221213 22:42:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.83
[32m[20221213 22:42:10 @agent_ppo2.py:143][0m Total time:      23.95 min
[32m[20221213 22:42:10 @agent_ppo2.py:145][0m 2330624 total steps have happened
[32m[20221213 22:42:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1138 --------------------------#
[32m[20221213 22:42:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:10 @agent_ppo2.py:185][0m |          -0.0017 |          43.8359 |          14.3015 |
[32m[20221213 22:42:10 @agent_ppo2.py:185][0m |          -0.0091 |          37.7729 |          14.2998 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |           0.0007 |          38.5463 |          14.2869 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |          -0.0037 |          35.4196 |          14.2776 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |          -0.0139 |          33.7005 |          14.2772 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |          -0.0080 |          34.3031 |          14.2699 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |          -0.0130 |          32.7005 |          14.2693 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |          -0.0079 |          33.0881 |          14.2679 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |          -0.0064 |          32.9040 |          14.2678 |
[32m[20221213 22:42:11 @agent_ppo2.py:185][0m |          -0.0133 |          31.6190 |          14.2590 |
[32m[20221213 22:42:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:42:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.70
[32m[20221213 22:42:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.97
[32m[20221213 22:42:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.49
[32m[20221213 22:42:11 @agent_ppo2.py:143][0m Total time:      23.97 min
[32m[20221213 22:42:11 @agent_ppo2.py:145][0m 2332672 total steps have happened
[32m[20221213 22:42:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1139 --------------------------#
[32m[20221213 22:42:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:42:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0028 |          29.0291 |          14.6513 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0026 |          25.0144 |          14.6255 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0076 |          24.1329 |          14.6329 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0065 |          23.5220 |          14.6201 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0053 |          23.1038 |          14.6121 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0029 |          24.1272 |          14.6111 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0139 |          22.5677 |          14.6062 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0127 |          22.4063 |          14.6117 |
[32m[20221213 22:42:12 @agent_ppo2.py:185][0m |          -0.0093 |          22.1286 |          14.6137 |
[32m[20221213 22:42:13 @agent_ppo2.py:185][0m |          -0.0174 |          22.2534 |          14.6107 |
[32m[20221213 22:42:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:42:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.29
[32m[20221213 22:42:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.88
[32m[20221213 22:42:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.87
[32m[20221213 22:42:13 @agent_ppo2.py:143][0m Total time:      24.00 min
[32m[20221213 22:42:13 @agent_ppo2.py:145][0m 2334720 total steps have happened
[32m[20221213 22:42:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1140 --------------------------#
[32m[20221213 22:42:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:42:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:13 @agent_ppo2.py:185][0m |          -0.0012 |          31.8159 |          14.3672 |
[32m[20221213 22:42:13 @agent_ppo2.py:185][0m |          -0.0074 |          29.2267 |          14.3620 |
[32m[20221213 22:42:13 @agent_ppo2.py:185][0m |          -0.0100 |          28.7163 |          14.3696 |
[32m[20221213 22:42:13 @agent_ppo2.py:185][0m |          -0.0119 |          28.2060 |          14.3540 |
[32m[20221213 22:42:13 @agent_ppo2.py:185][0m |          -0.0042 |          27.8739 |          14.3449 |
[32m[20221213 22:42:13 @agent_ppo2.py:185][0m |          -0.0124 |          27.7804 |          14.3497 |
[32m[20221213 22:42:14 @agent_ppo2.py:185][0m |          -0.0139 |          27.6028 |          14.3484 |
[32m[20221213 22:42:14 @agent_ppo2.py:185][0m |          -0.0118 |          27.2639 |          14.3523 |
[32m[20221213 22:42:14 @agent_ppo2.py:185][0m |          -0.0129 |          27.0736 |          14.3520 |
[32m[20221213 22:42:14 @agent_ppo2.py:185][0m |          -0.0096 |          26.9911 |          14.3472 |
[32m[20221213 22:42:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:42:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.52
[32m[20221213 22:42:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.27
[32m[20221213 22:42:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.95
[32m[20221213 22:42:14 @agent_ppo2.py:143][0m Total time:      24.02 min
[32m[20221213 22:42:14 @agent_ppo2.py:145][0m 2336768 total steps have happened
[32m[20221213 22:42:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1141 --------------------------#
[32m[20221213 22:42:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:14 @agent_ppo2.py:185][0m |           0.0107 |          31.0402 |          14.4922 |
[32m[20221213 22:42:14 @agent_ppo2.py:185][0m |          -0.0018 |          27.9701 |          14.4624 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |          -0.0080 |          27.3869 |          14.4629 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |           0.0017 |          28.5454 |          14.4466 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |          -0.0083 |          26.8888 |          14.4321 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |          -0.0056 |          26.6636 |          14.4530 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |          -0.0123 |          26.5458 |          14.4450 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |          -0.0070 |          26.7872 |          14.4575 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |          -0.0112 |          26.3431 |          14.4561 |
[32m[20221213 22:42:15 @agent_ppo2.py:185][0m |          -0.0131 |          26.2745 |          14.4520 |
[32m[20221213 22:42:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:42:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.61
[32m[20221213 22:42:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.58
[32m[20221213 22:42:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 181.40
[32m[20221213 22:42:15 @agent_ppo2.py:143][0m Total time:      24.04 min
[32m[20221213 22:42:15 @agent_ppo2.py:145][0m 2338816 total steps have happened
[32m[20221213 22:42:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1142 --------------------------#
[32m[20221213 22:42:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |           0.0010 |          27.3710 |          14.4233 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0022 |          24.0994 |          14.3996 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0122 |          22.6026 |          14.3944 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0084 |          21.7942 |          14.3928 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0116 |          21.2722 |          14.3902 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0138 |          20.7468 |          14.3862 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0126 |          20.5294 |          14.3801 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0140 |          20.0845 |          14.3816 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0137 |          19.6914 |          14.3779 |
[32m[20221213 22:42:16 @agent_ppo2.py:185][0m |          -0.0171 |          19.4853 |          14.3849 |
[32m[20221213 22:42:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:42:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.96
[32m[20221213 22:42:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.52
[32m[20221213 22:42:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.70
[32m[20221213 22:42:17 @agent_ppo2.py:143][0m Total time:      24.06 min
[32m[20221213 22:42:17 @agent_ppo2.py:145][0m 2340864 total steps have happened
[32m[20221213 22:42:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1143 --------------------------#
[32m[20221213 22:42:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0000 |          30.6677 |          14.2934 |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0046 |          29.2828 |          14.2612 |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0029 |          28.6977 |          14.2549 |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0121 |          28.3218 |          14.2464 |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0070 |          28.2203 |          14.2487 |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0134 |          27.7924 |          14.2551 |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0043 |          28.6514 |          14.2398 |
[32m[20221213 22:42:17 @agent_ppo2.py:185][0m |          -0.0108 |          27.6755 |          14.2455 |
[32m[20221213 22:42:18 @agent_ppo2.py:185][0m |          -0.0123 |          27.4926 |          14.2388 |
[32m[20221213 22:42:18 @agent_ppo2.py:185][0m |          -0.0108 |          27.3444 |          14.2438 |
[32m[20221213 22:42:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:42:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.38
[32m[20221213 22:42:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.14
[32m[20221213 22:42:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.07
[32m[20221213 22:42:18 @agent_ppo2.py:143][0m Total time:      24.08 min
[32m[20221213 22:42:18 @agent_ppo2.py:145][0m 2342912 total steps have happened
[32m[20221213 22:42:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1144 --------------------------#
[32m[20221213 22:42:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:18 @agent_ppo2.py:185][0m |           0.0041 |          27.8245 |          14.3702 |
[32m[20221213 22:42:18 @agent_ppo2.py:185][0m |          -0.0065 |          22.9884 |          14.3460 |
[32m[20221213 22:42:18 @agent_ppo2.py:185][0m |          -0.0126 |          21.5423 |          14.3453 |
[32m[20221213 22:42:18 @agent_ppo2.py:185][0m |          -0.0124 |          20.5174 |          14.3405 |
[32m[20221213 22:42:18 @agent_ppo2.py:185][0m |          -0.0128 |          19.8980 |          14.3349 |
[32m[20221213 22:42:19 @agent_ppo2.py:185][0m |          -0.0131 |          19.3274 |          14.3355 |
[32m[20221213 22:42:19 @agent_ppo2.py:185][0m |          -0.0162 |          19.0373 |          14.3286 |
[32m[20221213 22:42:19 @agent_ppo2.py:185][0m |          -0.0139 |          18.5837 |          14.3289 |
[32m[20221213 22:42:19 @agent_ppo2.py:185][0m |          -0.0185 |          18.2363 |          14.3252 |
[32m[20221213 22:42:19 @agent_ppo2.py:185][0m |          -0.0190 |          18.1980 |          14.3252 |
[32m[20221213 22:42:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:42:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.69
[32m[20221213 22:42:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 305.08
[32m[20221213 22:42:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.46
[32m[20221213 22:42:19 @agent_ppo2.py:143][0m Total time:      24.10 min
[32m[20221213 22:42:19 @agent_ppo2.py:145][0m 2344960 total steps have happened
[32m[20221213 22:42:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1145 --------------------------#
[32m[20221213 22:42:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:19 @agent_ppo2.py:185][0m |           0.0003 |          26.4934 |          14.5208 |
[32m[20221213 22:42:19 @agent_ppo2.py:185][0m |          -0.0057 |          23.5044 |          14.5049 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0076 |          22.7681 |          14.4983 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0151 |          22.1868 |          14.4951 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0119 |          21.8799 |          14.4820 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0134 |          21.5749 |          14.4862 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0080 |          21.2430 |          14.4810 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0075 |          22.0537 |          14.4822 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0156 |          20.9529 |          14.4777 |
[32m[20221213 22:42:20 @agent_ppo2.py:185][0m |          -0.0140 |          20.7767 |          14.4796 |
[32m[20221213 22:42:20 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:42:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.37
[32m[20221213 22:42:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.39
[32m[20221213 22:42:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.02
[32m[20221213 22:42:20 @agent_ppo2.py:143][0m Total time:      24.12 min
[32m[20221213 22:42:20 @agent_ppo2.py:145][0m 2347008 total steps have happened
[32m[20221213 22:42:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1146 --------------------------#
[32m[20221213 22:42:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |           0.0083 |          30.6742 |          14.3909 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0046 |          25.6634 |          14.3651 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0132 |          24.5760 |          14.3670 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0050 |          24.0514 |          14.3683 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0115 |          23.3750 |          14.3646 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0082 |          23.2117 |          14.3594 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0082 |          22.4863 |          14.3492 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0118 |          22.3177 |          14.3693 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0160 |          21.9262 |          14.3597 |
[32m[20221213 22:42:21 @agent_ppo2.py:185][0m |          -0.0122 |          21.6427 |          14.3585 |
[32m[20221213 22:42:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:42:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.87
[32m[20221213 22:42:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 315.20
[32m[20221213 22:42:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.51
[32m[20221213 22:42:22 @agent_ppo2.py:143][0m Total time:      24.14 min
[32m[20221213 22:42:22 @agent_ppo2.py:145][0m 2349056 total steps have happened
[32m[20221213 22:42:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1147 --------------------------#
[32m[20221213 22:42:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |           0.0009 |          40.5942 |          14.4943 |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |          -0.0052 |          38.0510 |          14.4632 |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |          -0.0045 |          37.0534 |          14.4649 |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |          -0.0084 |          36.6131 |          14.4646 |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |          -0.0062 |          36.1580 |          14.4513 |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |          -0.0099 |          35.7891 |          14.4553 |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |          -0.0109 |          35.6186 |          14.4357 |
[32m[20221213 22:42:22 @agent_ppo2.py:185][0m |          -0.0094 |          35.4723 |          14.4522 |
[32m[20221213 22:42:23 @agent_ppo2.py:185][0m |          -0.0103 |          34.8903 |          14.4509 |
[32m[20221213 22:42:23 @agent_ppo2.py:185][0m |          -0.0105 |          34.8013 |          14.4492 |
[32m[20221213 22:42:23 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:42:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.98
[32m[20221213 22:42:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.63
[32m[20221213 22:42:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.24
[32m[20221213 22:42:23 @agent_ppo2.py:143][0m Total time:      24.16 min
[32m[20221213 22:42:23 @agent_ppo2.py:145][0m 2351104 total steps have happened
[32m[20221213 22:42:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1148 --------------------------#
[32m[20221213 22:42:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:42:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:23 @agent_ppo2.py:185][0m |          -0.0040 |          34.0042 |          14.6027 |
[32m[20221213 22:42:23 @agent_ppo2.py:185][0m |          -0.0049 |          31.6266 |          14.5759 |
[32m[20221213 22:42:23 @agent_ppo2.py:185][0m |          -0.0090 |          30.6354 |          14.5726 |
[32m[20221213 22:42:23 @agent_ppo2.py:185][0m |          -0.0127 |          29.7886 |          14.5715 |
[32m[20221213 22:42:23 @agent_ppo2.py:185][0m |          -0.0132 |          29.3774 |          14.5620 |
[32m[20221213 22:42:24 @agent_ppo2.py:185][0m |          -0.0121 |          28.9595 |          14.5720 |
[32m[20221213 22:42:24 @agent_ppo2.py:185][0m |          -0.0120 |          28.5424 |          14.5647 |
[32m[20221213 22:42:24 @agent_ppo2.py:185][0m |          -0.0045 |          29.0238 |          14.5616 |
[32m[20221213 22:42:24 @agent_ppo2.py:185][0m |          -0.0118 |          28.0550 |          14.5629 |
[32m[20221213 22:42:24 @agent_ppo2.py:185][0m |          -0.0133 |          28.1196 |          14.5589 |
[32m[20221213 22:42:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:42:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.12
[32m[20221213 22:42:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.11
[32m[20221213 22:42:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.74
[32m[20221213 22:42:24 @agent_ppo2.py:143][0m Total time:      24.19 min
[32m[20221213 22:42:24 @agent_ppo2.py:145][0m 2353152 total steps have happened
[32m[20221213 22:42:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1149 --------------------------#
[32m[20221213 22:42:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:24 @agent_ppo2.py:185][0m |           0.0033 |          26.2421 |          14.4144 |
[32m[20221213 22:42:24 @agent_ppo2.py:185][0m |           0.0003 |          23.4427 |          14.4024 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0066 |          22.2768 |          14.4063 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0044 |          21.6240 |          14.4060 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0069 |          21.2084 |          14.3981 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0100 |          20.7695 |          14.3959 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0107 |          20.5593 |          14.4034 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0066 |          20.3610 |          14.3913 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0127 |          20.1549 |          14.3970 |
[32m[20221213 22:42:25 @agent_ppo2.py:185][0m |          -0.0111 |          19.9930 |          14.3881 |
[32m[20221213 22:42:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:42:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.71
[32m[20221213 22:42:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.60
[32m[20221213 22:42:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.00
[32m[20221213 22:42:25 @agent_ppo2.py:143][0m Total time:      24.21 min
[32m[20221213 22:42:25 @agent_ppo2.py:145][0m 2355200 total steps have happened
[32m[20221213 22:42:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1150 --------------------------#
[32m[20221213 22:42:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:42:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0047 |          36.2833 |          14.4360 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0039 |          31.6182 |          14.4241 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0055 |          30.4110 |          14.4155 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0067 |          29.8003 |          14.4174 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0087 |          29.0055 |          14.4048 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0073 |          28.6579 |          14.4056 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0077 |          28.3431 |          14.4019 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0086 |          27.9884 |          14.3971 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0052 |          29.2665 |          14.4024 |
[32m[20221213 22:42:26 @agent_ppo2.py:185][0m |          -0.0133 |          27.4405 |          14.3848 |
[32m[20221213 22:42:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:42:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.39
[32m[20221213 22:42:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.89
[32m[20221213 22:42:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.91
[32m[20221213 22:42:27 @agent_ppo2.py:143][0m Total time:      24.23 min
[32m[20221213 22:42:27 @agent_ppo2.py:145][0m 2357248 total steps have happened
[32m[20221213 22:42:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1151 --------------------------#
[32m[20221213 22:42:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |          -0.0061 |          30.6511 |          14.5469 |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |           0.0008 |          26.9376 |          14.5288 |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |          -0.0100 |          25.8435 |          14.5214 |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |          -0.0098 |          25.2378 |          14.5202 |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |           0.0042 |          26.9215 |          14.5140 |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |          -0.0089 |          24.5195 |          14.5024 |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |          -0.0148 |          24.1122 |          14.5115 |
[32m[20221213 22:42:27 @agent_ppo2.py:185][0m |          -0.0094 |          23.8870 |          14.5253 |
[32m[20221213 22:42:28 @agent_ppo2.py:185][0m |          -0.0171 |          23.6501 |          14.5069 |
[32m[20221213 22:42:28 @agent_ppo2.py:185][0m |          -0.0170 |          23.4198 |          14.5096 |
[32m[20221213 22:42:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:42:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.65
[32m[20221213 22:42:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.52
[32m[20221213 22:42:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.88
[32m[20221213 22:42:28 @agent_ppo2.py:143][0m Total time:      24.25 min
[32m[20221213 22:42:28 @agent_ppo2.py:145][0m 2359296 total steps have happened
[32m[20221213 22:42:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1152 --------------------------#
[32m[20221213 22:42:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:28 @agent_ppo2.py:185][0m |          -0.0054 |          34.2750 |          14.5965 |
[32m[20221213 22:42:28 @agent_ppo2.py:185][0m |          -0.0058 |          31.2805 |          14.5830 |
[32m[20221213 22:42:28 @agent_ppo2.py:185][0m |          -0.0091 |          29.8024 |          14.5716 |
[32m[20221213 22:42:28 @agent_ppo2.py:185][0m |          -0.0083 |          28.9399 |          14.5805 |
[32m[20221213 22:42:28 @agent_ppo2.py:185][0m |          -0.0143 |          28.3494 |          14.5706 |
[32m[20221213 22:42:29 @agent_ppo2.py:185][0m |          -0.0096 |          27.8693 |          14.5663 |
[32m[20221213 22:42:29 @agent_ppo2.py:185][0m |          -0.0123 |          27.4343 |          14.5672 |
[32m[20221213 22:42:29 @agent_ppo2.py:185][0m |          -0.0133 |          27.1572 |          14.5595 |
[32m[20221213 22:42:29 @agent_ppo2.py:185][0m |          -0.0161 |          26.7938 |          14.5648 |
[32m[20221213 22:42:29 @agent_ppo2.py:185][0m |          -0.0072 |          26.6852 |          14.5663 |
[32m[20221213 22:42:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.05
[32m[20221213 22:42:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.64
[32m[20221213 22:42:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.06
[32m[20221213 22:42:29 @agent_ppo2.py:143][0m Total time:      24.27 min
[32m[20221213 22:42:29 @agent_ppo2.py:145][0m 2361344 total steps have happened
[32m[20221213 22:42:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1153 --------------------------#
[32m[20221213 22:42:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:29 @agent_ppo2.py:185][0m |           0.0028 |          34.3209 |          14.6565 |
[32m[20221213 22:42:29 @agent_ppo2.py:185][0m |          -0.0068 |          32.6000 |          14.6412 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0067 |          31.9194 |          14.6235 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0100 |          31.6011 |          14.6193 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0109 |          31.1890 |          14.6263 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0099 |          30.9403 |          14.6166 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0132 |          30.7886 |          14.6274 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0126 |          30.7048 |          14.6325 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0002 |          34.1258 |          14.6215 |
[32m[20221213 22:42:30 @agent_ppo2.py:185][0m |          -0.0131 |          30.4226 |          14.6107 |
[32m[20221213 22:42:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.19
[32m[20221213 22:42:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.99
[32m[20221213 22:42:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.87
[32m[20221213 22:42:30 @agent_ppo2.py:143][0m Total time:      24.29 min
[32m[20221213 22:42:30 @agent_ppo2.py:145][0m 2363392 total steps have happened
[32m[20221213 22:42:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1154 --------------------------#
[32m[20221213 22:42:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |           0.0005 |          22.5964 |          14.6089 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0052 |          18.4049 |          14.5941 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0066 |          17.4224 |          14.5869 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0072 |          16.7756 |          14.5940 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0091 |          16.4235 |          14.5848 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0070 |          17.0148 |          14.5876 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0165 |          15.7175 |          14.5726 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0078 |          15.5127 |          14.5751 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0046 |          17.3019 |          14.5869 |
[32m[20221213 22:42:31 @agent_ppo2.py:185][0m |          -0.0116 |          15.1546 |          14.5784 |
[32m[20221213 22:42:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.49
[32m[20221213 22:42:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.48
[32m[20221213 22:42:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.33
[32m[20221213 22:42:32 @agent_ppo2.py:143][0m Total time:      24.31 min
[32m[20221213 22:42:32 @agent_ppo2.py:145][0m 2365440 total steps have happened
[32m[20221213 22:42:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1155 --------------------------#
[32m[20221213 22:42:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |           0.0018 |          33.1342 |          14.7000 |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |          -0.0062 |          30.6957 |          14.6891 |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |          -0.0002 |          30.4721 |          14.6882 |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |          -0.0056 |          29.1807 |          14.6737 |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |          -0.0089 |          28.4146 |          14.6771 |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |          -0.0068 |          30.0254 |          14.6762 |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |          -0.0102 |          27.7061 |          14.6762 |
[32m[20221213 22:42:32 @agent_ppo2.py:185][0m |          -0.0141 |          27.2935 |          14.6781 |
[32m[20221213 22:42:33 @agent_ppo2.py:185][0m |          -0.0121 |          27.1412 |          14.6767 |
[32m[20221213 22:42:33 @agent_ppo2.py:185][0m |          -0.0092 |          26.9026 |          14.6797 |
[32m[20221213 22:42:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:42:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.56
[32m[20221213 22:42:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.85
[32m[20221213 22:42:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.15
[32m[20221213 22:42:33 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 450.15
[32m[20221213 22:42:33 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 450.15
[32m[20221213 22:42:33 @agent_ppo2.py:143][0m Total time:      24.33 min
[32m[20221213 22:42:33 @agent_ppo2.py:145][0m 2367488 total steps have happened
[32m[20221213 22:42:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1156 --------------------------#
[32m[20221213 22:42:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:42:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:33 @agent_ppo2.py:185][0m |          -0.0040 |          26.0921 |          14.5950 |
[32m[20221213 22:42:33 @agent_ppo2.py:185][0m |          -0.0099 |          22.9613 |          14.5857 |
[32m[20221213 22:42:33 @agent_ppo2.py:185][0m |          -0.0060 |          21.9958 |          14.5852 |
[32m[20221213 22:42:33 @agent_ppo2.py:185][0m |          -0.0136 |          21.4014 |          14.5775 |
[32m[20221213 22:42:34 @agent_ppo2.py:185][0m |          -0.0135 |          20.9186 |          14.5748 |
[32m[20221213 22:42:34 @agent_ppo2.py:185][0m |          -0.0105 |          20.8190 |          14.5688 |
[32m[20221213 22:42:34 @agent_ppo2.py:185][0m |          -0.0160 |          20.5903 |          14.5619 |
[32m[20221213 22:42:34 @agent_ppo2.py:185][0m |          -0.0142 |          20.0287 |          14.5689 |
[32m[20221213 22:42:34 @agent_ppo2.py:185][0m |          -0.0170 |          19.8410 |          14.5608 |
[32m[20221213 22:42:34 @agent_ppo2.py:185][0m |          -0.0136 |          19.6812 |          14.5627 |
[32m[20221213 22:42:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:42:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.94
[32m[20221213 22:42:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.47
[32m[20221213 22:42:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.90
[32m[20221213 22:42:34 @agent_ppo2.py:143][0m Total time:      24.35 min
[32m[20221213 22:42:34 @agent_ppo2.py:145][0m 2369536 total steps have happened
[32m[20221213 22:42:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1157 --------------------------#
[32m[20221213 22:42:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:34 @agent_ppo2.py:185][0m |           0.0020 |          35.5783 |          14.5673 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0093 |          31.9460 |          14.5374 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0010 |          31.0646 |          14.5341 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0115 |          29.7583 |          14.5216 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0149 |          28.7156 |          14.5261 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0182 |          28.0878 |          14.5127 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0192 |          27.6753 |          14.5131 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0172 |          27.1991 |          14.5202 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0181 |          26.9252 |          14.5098 |
[32m[20221213 22:42:35 @agent_ppo2.py:185][0m |          -0.0180 |          26.5908 |          14.5137 |
[32m[20221213 22:42:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:42:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.82
[32m[20221213 22:42:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.93
[32m[20221213 22:42:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.94
[32m[20221213 22:42:35 @agent_ppo2.py:143][0m Total time:      24.37 min
[32m[20221213 22:42:35 @agent_ppo2.py:145][0m 2371584 total steps have happened
[32m[20221213 22:42:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1158 --------------------------#
[32m[20221213 22:42:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0009 |          24.1342 |          14.5171 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0057 |          21.0321 |          14.5117 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0061 |          20.3788 |          14.5130 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0129 |          19.7125 |          14.5084 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0175 |          19.4777 |          14.5049 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0061 |          20.0263 |          14.5116 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0142 |          19.0677 |          14.5076 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0136 |          18.8149 |          14.5103 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0153 |          18.5877 |          14.5125 |
[32m[20221213 22:42:36 @agent_ppo2.py:185][0m |          -0.0131 |          18.4706 |          14.5016 |
[32m[20221213 22:42:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.12
[32m[20221213 22:42:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.41
[32m[20221213 22:42:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.82
[32m[20221213 22:42:37 @agent_ppo2.py:143][0m Total time:      24.39 min
[32m[20221213 22:42:37 @agent_ppo2.py:145][0m 2373632 total steps have happened
[32m[20221213 22:42:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1159 --------------------------#
[32m[20221213 22:42:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:37 @agent_ppo2.py:185][0m |          -0.0005 |          30.6018 |          14.5863 |
[32m[20221213 22:42:37 @agent_ppo2.py:185][0m |          -0.0028 |          27.7803 |          14.5717 |
[32m[20221213 22:42:37 @agent_ppo2.py:185][0m |          -0.0010 |          27.0949 |          14.5613 |
[32m[20221213 22:42:37 @agent_ppo2.py:185][0m |          -0.0057 |          26.4016 |          14.5562 |
[32m[20221213 22:42:37 @agent_ppo2.py:185][0m |          -0.0042 |          26.0273 |          14.5490 |
[32m[20221213 22:42:37 @agent_ppo2.py:185][0m |          -0.0074 |          25.7309 |          14.5683 |
[32m[20221213 22:42:37 @agent_ppo2.py:185][0m |          -0.0082 |          25.5835 |          14.5508 |
[32m[20221213 22:42:38 @agent_ppo2.py:185][0m |          -0.0095 |          25.2554 |          14.5538 |
[32m[20221213 22:42:38 @agent_ppo2.py:185][0m |          -0.0083 |          25.1531 |          14.5554 |
[32m[20221213 22:42:38 @agent_ppo2.py:185][0m |          -0.0084 |          24.9913 |          14.5646 |
[32m[20221213 22:42:38 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 22:42:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.74
[32m[20221213 22:42:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.28
[32m[20221213 22:42:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.15
[32m[20221213 22:42:38 @agent_ppo2.py:143][0m Total time:      24.42 min
[32m[20221213 22:42:38 @agent_ppo2.py:145][0m 2375680 total steps have happened
[32m[20221213 22:42:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1160 --------------------------#
[32m[20221213 22:42:38 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 22:42:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:38 @agent_ppo2.py:185][0m |           0.0001 |          37.2174 |          14.7375 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0040 |          31.8044 |          14.7248 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0111 |          30.4325 |          14.7148 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0134 |          29.5439 |          14.7120 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0091 |          28.9944 |          14.7081 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0085 |          28.4709 |          14.7083 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0103 |          28.1232 |          14.7121 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0155 |          27.8314 |          14.7090 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0140 |          27.2627 |          14.7057 |
[32m[20221213 22:42:39 @agent_ppo2.py:185][0m |          -0.0109 |          27.0362 |          14.6956 |
[32m[20221213 22:42:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.00
[32m[20221213 22:42:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.86
[32m[20221213 22:42:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.20
[32m[20221213 22:42:39 @agent_ppo2.py:143][0m Total time:      24.44 min
[32m[20221213 22:42:39 @agent_ppo2.py:145][0m 2377728 total steps have happened
[32m[20221213 22:42:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1161 --------------------------#
[32m[20221213 22:42:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:42:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0016 |          32.6152 |          14.4753 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0094 |          29.7462 |          14.4593 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0083 |          29.0315 |          14.4508 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0104 |          28.3922 |          14.4552 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0106 |          27.7921 |          14.4423 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0124 |          27.4605 |          14.4410 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0137 |          27.2909 |          14.4439 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |           0.0022 |          31.6281 |          14.4389 |
[32m[20221213 22:42:40 @agent_ppo2.py:185][0m |          -0.0133 |          27.1348 |          14.4110 |
[32m[20221213 22:42:41 @agent_ppo2.py:185][0m |          -0.0141 |          26.3428 |          14.4418 |
[32m[20221213 22:42:41 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:42:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.72
[32m[20221213 22:42:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.67
[32m[20221213 22:42:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.68
[32m[20221213 22:42:41 @agent_ppo2.py:143][0m Total time:      24.46 min
[32m[20221213 22:42:41 @agent_ppo2.py:145][0m 2379776 total steps have happened
[32m[20221213 22:42:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1162 --------------------------#
[32m[20221213 22:42:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:42:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:41 @agent_ppo2.py:185][0m |          -0.0044 |          35.7317 |          14.5129 |
[32m[20221213 22:42:41 @agent_ppo2.py:185][0m |           0.0032 |          36.6920 |          14.4890 |
[32m[20221213 22:42:41 @agent_ppo2.py:185][0m |          -0.0058 |          31.9368 |          14.4710 |
[32m[20221213 22:42:41 @agent_ppo2.py:185][0m |           0.0090 |          35.0926 |          14.4779 |
[32m[20221213 22:42:41 @agent_ppo2.py:185][0m |          -0.0086 |          30.8653 |          14.4548 |
[32m[20221213 22:42:42 @agent_ppo2.py:185][0m |          -0.0089 |          30.3652 |          14.4737 |
[32m[20221213 22:42:42 @agent_ppo2.py:185][0m |          -0.0045 |          32.4780 |          14.4641 |
[32m[20221213 22:42:42 @agent_ppo2.py:185][0m |          -0.0088 |          30.3748 |          14.4754 |
[32m[20221213 22:42:42 @agent_ppo2.py:185][0m |          -0.0129 |          29.5160 |          14.4718 |
[32m[20221213 22:42:42 @agent_ppo2.py:185][0m |          -0.0159 |          29.2349 |          14.4681 |
[32m[20221213 22:42:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:42:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.66
[32m[20221213 22:42:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.33
[32m[20221213 22:42:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.88
[32m[20221213 22:42:42 @agent_ppo2.py:143][0m Total time:      24.49 min
[32m[20221213 22:42:42 @agent_ppo2.py:145][0m 2381824 total steps have happened
[32m[20221213 22:42:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1163 --------------------------#
[32m[20221213 22:42:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:42 @agent_ppo2.py:185][0m |          -0.0053 |          31.3785 |          14.5025 |
[32m[20221213 22:42:42 @agent_ppo2.py:185][0m |           0.0027 |          29.0662 |          14.4913 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0052 |          27.3976 |          14.4795 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0091 |          26.8724 |          14.4831 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0106 |          26.7418 |          14.4756 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0036 |          26.6175 |          14.4723 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0102 |          26.3257 |          14.4791 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0101 |          26.2365 |          14.4770 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0088 |          27.4997 |          14.4705 |
[32m[20221213 22:42:43 @agent_ppo2.py:185][0m |          -0.0028 |          28.3561 |          14.4717 |
[32m[20221213 22:42:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:42:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.18
[32m[20221213 22:42:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.45
[32m[20221213 22:42:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.50
[32m[20221213 22:42:43 @agent_ppo2.py:143][0m Total time:      24.51 min
[32m[20221213 22:42:43 @agent_ppo2.py:145][0m 2383872 total steps have happened
[32m[20221213 22:42:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1164 --------------------------#
[32m[20221213 22:42:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:42:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |           0.0010 |           6.7486 |          14.5741 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |          -0.0040 |           4.9948 |          14.5619 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |          -0.0053 |           4.9092 |          14.5689 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |           0.0048 |           5.0039 |          14.5703 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |          -0.0071 |           4.8165 |          14.5576 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |           0.0025 |           5.0734 |          14.5702 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |          -0.0072 |           4.7924 |          14.5586 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |          -0.0059 |           4.7692 |          14.5713 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |          -0.0083 |           4.7594 |          14.5605 |
[32m[20221213 22:42:44 @agent_ppo2.py:185][0m |          -0.0041 |           4.7639 |          14.5551 |
[32m[20221213 22:42:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:42:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:42:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.03
[32m[20221213 22:42:45 @agent_ppo2.py:143][0m Total time:      24.53 min
[32m[20221213 22:42:45 @agent_ppo2.py:145][0m 2385920 total steps have happened
[32m[20221213 22:42:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1165 --------------------------#
[32m[20221213 22:42:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:42:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0006 |          31.4026 |          14.6295 |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0038 |          29.0611 |          14.6081 |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0077 |          28.4054 |          14.5990 |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0084 |          27.9030 |          14.5947 |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0088 |          27.6100 |          14.5995 |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0156 |          27.3145 |          14.5809 |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0096 |          27.2216 |          14.6012 |
[32m[20221213 22:42:45 @agent_ppo2.py:185][0m |          -0.0130 |          27.0133 |          14.5879 |
[32m[20221213 22:42:46 @agent_ppo2.py:185][0m |          -0.0083 |          26.8830 |          14.5982 |
[32m[20221213 22:42:46 @agent_ppo2.py:185][0m |          -0.0129 |          26.6862 |          14.6010 |
[32m[20221213 22:42:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:42:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.29
[32m[20221213 22:42:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.56
[32m[20221213 22:42:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.61
[32m[20221213 22:42:46 @agent_ppo2.py:143][0m Total time:      24.55 min
[32m[20221213 22:42:46 @agent_ppo2.py:145][0m 2387968 total steps have happened
[32m[20221213 22:42:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1166 --------------------------#
[32m[20221213 22:42:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:46 @agent_ppo2.py:185][0m |           0.0023 |          26.5440 |          14.5491 |
[32m[20221213 22:42:46 @agent_ppo2.py:185][0m |          -0.0045 |          23.2322 |          14.5343 |
[32m[20221213 22:42:46 @agent_ppo2.py:185][0m |          -0.0030 |          22.3661 |          14.5319 |
[32m[20221213 22:42:46 @agent_ppo2.py:185][0m |          -0.0053 |          21.5712 |          14.5196 |
[32m[20221213 22:42:46 @agent_ppo2.py:185][0m |          -0.0122 |          21.1916 |          14.5201 |
[32m[20221213 22:42:47 @agent_ppo2.py:185][0m |          -0.0111 |          20.6375 |          14.5192 |
[32m[20221213 22:42:47 @agent_ppo2.py:185][0m |           0.0005 |          23.5097 |          14.5148 |
[32m[20221213 22:42:47 @agent_ppo2.py:185][0m |          -0.0137 |          20.2123 |          14.5031 |
[32m[20221213 22:42:47 @agent_ppo2.py:185][0m |          -0.0173 |          19.9177 |          14.5060 |
[32m[20221213 22:42:47 @agent_ppo2.py:185][0m |          -0.0105 |          20.0676 |          14.5121 |
[32m[20221213 22:42:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:42:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.15
[32m[20221213 22:42:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.24
[32m[20221213 22:42:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.11
[32m[20221213 22:42:47 @agent_ppo2.py:143][0m Total time:      24.57 min
[32m[20221213 22:42:47 @agent_ppo2.py:145][0m 2390016 total steps have happened
[32m[20221213 22:42:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1167 --------------------------#
[32m[20221213 22:42:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:47 @agent_ppo2.py:185][0m |           0.0017 |          31.6969 |          14.6988 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0038 |          29.8129 |          14.7014 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0049 |          29.1201 |          14.7034 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0061 |          28.7320 |          14.6913 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0078 |          28.3449 |          14.6951 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0121 |          28.1693 |          14.6898 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0106 |          27.9468 |          14.6890 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0119 |          27.8052 |          14.6738 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0098 |          27.5741 |          14.6856 |
[32m[20221213 22:42:48 @agent_ppo2.py:185][0m |          -0.0188 |          27.6327 |          14.6853 |
[32m[20221213 22:42:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:42:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.78
[32m[20221213 22:42:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.24
[32m[20221213 22:42:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.79
[32m[20221213 22:42:48 @agent_ppo2.py:143][0m Total time:      24.59 min
[32m[20221213 22:42:48 @agent_ppo2.py:145][0m 2392064 total steps have happened
[32m[20221213 22:42:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1168 --------------------------#
[32m[20221213 22:42:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |           0.0016 |          37.7040 |          14.5679 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0057 |          34.0029 |          14.5604 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0070 |          32.9897 |          14.5495 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0096 |          31.9126 |          14.5599 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0049 |          31.2100 |          14.5528 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0098 |          30.6311 |          14.5525 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0107 |          30.1617 |          14.5582 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0134 |          29.8477 |          14.5558 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0089 |          29.5365 |          14.5584 |
[32m[20221213 22:42:49 @agent_ppo2.py:185][0m |          -0.0105 |          29.6999 |          14.5560 |
[32m[20221213 22:42:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:42:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.96
[32m[20221213 22:42:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.06
[32m[20221213 22:42:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.00
[32m[20221213 22:42:50 @agent_ppo2.py:143][0m Total time:      24.61 min
[32m[20221213 22:42:50 @agent_ppo2.py:145][0m 2394112 total steps have happened
[32m[20221213 22:42:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1169 --------------------------#
[32m[20221213 22:42:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |           0.0065 |          29.4708 |          14.5714 |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |           0.0027 |          24.8504 |          14.5602 |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |          -0.0061 |          23.7765 |          14.5647 |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |          -0.0087 |          23.0675 |          14.5505 |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |          -0.0064 |          22.7666 |          14.5513 |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |          -0.0121 |          22.6957 |          14.5479 |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |          -0.0163 |          22.4944 |          14.5520 |
[32m[20221213 22:42:50 @agent_ppo2.py:185][0m |          -0.0080 |          22.2984 |          14.5460 |
[32m[20221213 22:42:51 @agent_ppo2.py:185][0m |          -0.0125 |          22.1496 |          14.5440 |
[32m[20221213 22:42:51 @agent_ppo2.py:185][0m |          -0.0068 |          21.9024 |          14.5498 |
[32m[20221213 22:42:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:42:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.69
[32m[20221213 22:42:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 343.80
[32m[20221213 22:42:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.38
[32m[20221213 22:42:51 @agent_ppo2.py:143][0m Total time:      24.63 min
[32m[20221213 22:42:51 @agent_ppo2.py:145][0m 2396160 total steps have happened
[32m[20221213 22:42:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1170 --------------------------#
[32m[20221213 22:42:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 22:42:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:51 @agent_ppo2.py:185][0m |          -0.0036 |          35.0670 |          14.5286 |
[32m[20221213 22:42:51 @agent_ppo2.py:185][0m |          -0.0063 |          33.8330 |          14.5087 |
[32m[20221213 22:42:51 @agent_ppo2.py:185][0m |          -0.0076 |          33.1941 |          14.5124 |
[32m[20221213 22:42:52 @agent_ppo2.py:185][0m |          -0.0072 |          32.8913 |          14.5142 |
[32m[20221213 22:42:52 @agent_ppo2.py:185][0m |          -0.0111 |          32.6903 |          14.4849 |
[32m[20221213 22:42:52 @agent_ppo2.py:185][0m |          -0.0097 |          32.3879 |          14.4985 |
[32m[20221213 22:42:52 @agent_ppo2.py:185][0m |          -0.0082 |          32.2403 |          14.5048 |
[32m[20221213 22:42:52 @agent_ppo2.py:185][0m |          -0.0088 |          32.1572 |          14.4881 |
[32m[20221213 22:42:52 @agent_ppo2.py:185][0m |          -0.0109 |          31.9950 |          14.4986 |
[32m[20221213 22:42:52 @agent_ppo2.py:185][0m |          -0.0057 |          32.3448 |          14.4977 |
[32m[20221213 22:42:52 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:42:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.52
[32m[20221213 22:42:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.05
[32m[20221213 22:42:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:42:52 @agent_ppo2.py:143][0m Total time:      24.65 min
[32m[20221213 22:42:52 @agent_ppo2.py:145][0m 2398208 total steps have happened
[32m[20221213 22:42:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1171 --------------------------#
[32m[20221213 22:42:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |           0.0004 |          27.1851 |          14.6954 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0065 |          24.9251 |          14.6811 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0089 |          24.3578 |          14.6681 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0019 |          25.0931 |          14.6613 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0149 |          23.6528 |          14.6662 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0094 |          23.2633 |          14.6678 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0129 |          23.0940 |          14.6587 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0077 |          22.8336 |          14.6598 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0131 |          22.6986 |          14.6587 |
[32m[20221213 22:42:53 @agent_ppo2.py:185][0m |          -0.0070 |          22.7438 |          14.6600 |
[32m[20221213 22:42:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:42:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.73
[32m[20221213 22:42:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.05
[32m[20221213 22:42:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.34
[32m[20221213 22:42:53 @agent_ppo2.py:143][0m Total time:      24.68 min
[32m[20221213 22:42:53 @agent_ppo2.py:145][0m 2400256 total steps have happened
[32m[20221213 22:42:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1172 --------------------------#
[32m[20221213 22:42:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0076 |          24.8742 |          14.5832 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0058 |          22.8474 |          14.5564 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0065 |          22.7895 |          14.5566 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0095 |          22.3133 |          14.5591 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0109 |          21.9513 |          14.5560 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0054 |          22.7217 |          14.5501 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0165 |          21.6761 |          14.5510 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0141 |          21.4182 |          14.5481 |
[32m[20221213 22:42:54 @agent_ppo2.py:185][0m |          -0.0142 |          21.4079 |          14.5527 |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |          -0.0190 |          21.2444 |          14.5424 |
[32m[20221213 22:42:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:42:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.38
[32m[20221213 22:42:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.21
[32m[20221213 22:42:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.14
[32m[20221213 22:42:55 @agent_ppo2.py:143][0m Total time:      24.70 min
[32m[20221213 22:42:55 @agent_ppo2.py:145][0m 2402304 total steps have happened
[32m[20221213 22:42:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1173 --------------------------#
[32m[20221213 22:42:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |           0.0029 |          30.0644 |          14.7157 |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |          -0.0051 |          27.9626 |          14.6909 |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |           0.0070 |          29.3167 |          14.6862 |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |          -0.0046 |          26.6076 |          14.6602 |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |          -0.0077 |          26.2250 |          14.6820 |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |          -0.0079 |          25.9172 |          14.6704 |
[32m[20221213 22:42:55 @agent_ppo2.py:185][0m |          -0.0083 |          25.6639 |          14.6738 |
[32m[20221213 22:42:56 @agent_ppo2.py:185][0m |          -0.0129 |          25.5336 |          14.6726 |
[32m[20221213 22:42:56 @agent_ppo2.py:185][0m |          -0.0107 |          25.3717 |          14.6716 |
[32m[20221213 22:42:56 @agent_ppo2.py:185][0m |          -0.0112 |          25.2166 |          14.6681 |
[32m[20221213 22:42:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:42:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.96
[32m[20221213 22:42:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.44
[32m[20221213 22:42:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.10
[32m[20221213 22:42:56 @agent_ppo2.py:143][0m Total time:      24.72 min
[32m[20221213 22:42:56 @agent_ppo2.py:145][0m 2404352 total steps have happened
[32m[20221213 22:42:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1174 --------------------------#
[32m[20221213 22:42:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:56 @agent_ppo2.py:185][0m |          -0.0002 |          31.2652 |          14.6548 |
[32m[20221213 22:42:56 @agent_ppo2.py:185][0m |          -0.0063 |          29.2927 |          14.6232 |
[32m[20221213 22:42:56 @agent_ppo2.py:185][0m |          -0.0060 |          28.6157 |          14.6098 |
[32m[20221213 22:42:56 @agent_ppo2.py:185][0m |          -0.0082 |          27.7754 |          14.6092 |
[32m[20221213 22:42:57 @agent_ppo2.py:185][0m |          -0.0082 |          27.2502 |          14.6095 |
[32m[20221213 22:42:57 @agent_ppo2.py:185][0m |          -0.0123 |          26.6662 |          14.6132 |
[32m[20221213 22:42:57 @agent_ppo2.py:185][0m |          -0.0094 |          26.2926 |          14.6155 |
[32m[20221213 22:42:57 @agent_ppo2.py:185][0m |          -0.0144 |          25.9813 |          14.6103 |
[32m[20221213 22:42:57 @agent_ppo2.py:185][0m |          -0.0164 |          25.7935 |          14.6140 |
[32m[20221213 22:42:57 @agent_ppo2.py:185][0m |          -0.0022 |          28.6057 |          14.6025 |
[32m[20221213 22:42:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:42:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.70
[32m[20221213 22:42:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.87
[32m[20221213 22:42:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.91
[32m[20221213 22:42:57 @agent_ppo2.py:143][0m Total time:      24.74 min
[32m[20221213 22:42:57 @agent_ppo2.py:145][0m 2406400 total steps have happened
[32m[20221213 22:42:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1175 --------------------------#
[32m[20221213 22:42:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:57 @agent_ppo2.py:185][0m |           0.0020 |          17.2366 |          14.6628 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0024 |          13.4887 |          14.6433 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0038 |          12.2636 |          14.6369 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0137 |          11.5287 |          14.6256 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0108 |          10.9578 |          14.6242 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0156 |          10.6370 |          14.6296 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0146 |          10.2954 |          14.6232 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0123 |          10.0009 |          14.6226 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0202 |           9.8531 |          14.6297 |
[32m[20221213 22:42:58 @agent_ppo2.py:185][0m |          -0.0174 |           9.6237 |          14.6253 |
[32m[20221213 22:42:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:42:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.46
[32m[20221213 22:42:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.85
[32m[20221213 22:42:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.24
[32m[20221213 22:42:58 @agent_ppo2.py:143][0m Total time:      24.76 min
[32m[20221213 22:42:58 @agent_ppo2.py:145][0m 2408448 total steps have happened
[32m[20221213 22:42:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1176 --------------------------#
[32m[20221213 22:42:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:42:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0038 |          25.3964 |          14.6737 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0033 |          23.6395 |          14.6629 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0018 |          23.1555 |          14.6518 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0091 |          22.5562 |          14.6434 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0107 |          22.2700 |          14.6510 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0112 |          22.1030 |          14.6437 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0143 |          22.2621 |          14.6420 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0137 |          21.6906 |          14.6338 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0084 |          22.6442 |          14.6365 |
[32m[20221213 22:42:59 @agent_ppo2.py:185][0m |          -0.0077 |          21.7417 |          14.6343 |
[32m[20221213 22:42:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.10
[32m[20221213 22:43:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.81
[32m[20221213 22:43:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.14
[32m[20221213 22:43:00 @agent_ppo2.py:143][0m Total time:      24.78 min
[32m[20221213 22:43:00 @agent_ppo2.py:145][0m 2410496 total steps have happened
[32m[20221213 22:43:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1177 --------------------------#
[32m[20221213 22:43:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |           0.0004 |          30.6575 |          14.6494 |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |          -0.0014 |          30.1261 |          14.6238 |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |          -0.0068 |          27.9901 |          14.6106 |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |          -0.0102 |          27.6195 |          14.6143 |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |          -0.0115 |          27.0792 |          14.6054 |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |          -0.0123 |          26.5690 |          14.6022 |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |          -0.0126 |          26.3440 |          14.6123 |
[32m[20221213 22:43:00 @agent_ppo2.py:185][0m |          -0.0118 |          26.0606 |          14.6069 |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |          -0.0053 |          26.5498 |          14.6070 |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |          -0.0158 |          25.7112 |          14.6154 |
[32m[20221213 22:43:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.75
[32m[20221213 22:43:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.15
[32m[20221213 22:43:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.58
[32m[20221213 22:43:01 @agent_ppo2.py:143][0m Total time:      24.80 min
[32m[20221213 22:43:01 @agent_ppo2.py:145][0m 2412544 total steps have happened
[32m[20221213 22:43:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1178 --------------------------#
[32m[20221213 22:43:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |           0.0016 |          35.1234 |          14.5378 |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |          -0.0037 |          32.3321 |          14.5232 |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |          -0.0061 |          31.2541 |          14.5077 |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |           0.0068 |          33.8639 |          14.5067 |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |          -0.0058 |          30.4793 |          14.5125 |
[32m[20221213 22:43:01 @agent_ppo2.py:185][0m |          -0.0008 |          30.9563 |          14.5085 |
[32m[20221213 22:43:02 @agent_ppo2.py:185][0m |           0.0003 |          32.6825 |          14.4946 |
[32m[20221213 22:43:02 @agent_ppo2.py:185][0m |          -0.0099 |          29.8154 |          14.5086 |
[32m[20221213 22:43:02 @agent_ppo2.py:185][0m |          -0.0119 |          29.7002 |          14.5150 |
[32m[20221213 22:43:02 @agent_ppo2.py:185][0m |          -0.0104 |          29.5356 |          14.5008 |
[32m[20221213 22:43:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.25
[32m[20221213 22:43:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.25
[32m[20221213 22:43:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.59
[32m[20221213 22:43:02 @agent_ppo2.py:143][0m Total time:      24.82 min
[32m[20221213 22:43:02 @agent_ppo2.py:145][0m 2414592 total steps have happened
[32m[20221213 22:43:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1179 --------------------------#
[32m[20221213 22:43:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:02 @agent_ppo2.py:185][0m |           0.0053 |          29.9455 |          14.6861 |
[32m[20221213 22:43:02 @agent_ppo2.py:185][0m |          -0.0075 |          28.0010 |          14.6683 |
[32m[20221213 22:43:02 @agent_ppo2.py:185][0m |           0.0111 |          32.4772 |          14.6716 |
[32m[20221213 22:43:03 @agent_ppo2.py:185][0m |          -0.0029 |          28.4171 |          14.6648 |
[32m[20221213 22:43:03 @agent_ppo2.py:185][0m |          -0.0108 |          26.7184 |          14.6571 |
[32m[20221213 22:43:03 @agent_ppo2.py:185][0m |          -0.0133 |          26.5101 |          14.6657 |
[32m[20221213 22:43:03 @agent_ppo2.py:185][0m |          -0.0128 |          26.3092 |          14.6674 |
[32m[20221213 22:43:03 @agent_ppo2.py:185][0m |          -0.0164 |          26.1169 |          14.6621 |
[32m[20221213 22:43:03 @agent_ppo2.py:185][0m |          -0.0097 |          25.9786 |          14.6680 |
[32m[20221213 22:43:03 @agent_ppo2.py:185][0m |          -0.0009 |          30.6781 |          14.6658 |
[32m[20221213 22:43:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:43:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.48
[32m[20221213 22:43:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.96
[32m[20221213 22:43:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 208.03
[32m[20221213 22:43:03 @agent_ppo2.py:143][0m Total time:      24.84 min
[32m[20221213 22:43:03 @agent_ppo2.py:145][0m 2416640 total steps have happened
[32m[20221213 22:43:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1180 --------------------------#
[32m[20221213 22:43:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:43:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0034 |          28.5455 |          14.6128 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0052 |          24.3048 |          14.6062 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0053 |          22.9029 |          14.6155 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0130 |          21.9555 |          14.6071 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0064 |          21.3501 |          14.5999 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0107 |          20.5821 |          14.5993 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0139 |          20.0618 |          14.6004 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0085 |          19.7410 |          14.5943 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0176 |          19.3333 |          14.5933 |
[32m[20221213 22:43:04 @agent_ppo2.py:185][0m |          -0.0134 |          19.0211 |          14.5985 |
[32m[20221213 22:43:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.63
[32m[20221213 22:43:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.66
[32m[20221213 22:43:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.03
[32m[20221213 22:43:04 @agent_ppo2.py:143][0m Total time:      24.86 min
[32m[20221213 22:43:04 @agent_ppo2.py:145][0m 2418688 total steps have happened
[32m[20221213 22:43:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1181 --------------------------#
[32m[20221213 22:43:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |           0.0029 |          15.8340 |          14.5000 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0085 |          12.5962 |          14.4746 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0013 |          12.2094 |          14.4803 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0080 |          11.7225 |          14.4695 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0080 |          11.4106 |          14.4699 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0028 |          11.2853 |          14.4597 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0144 |          11.2939 |          14.4763 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0101 |          10.9572 |          14.4639 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0104 |          10.8201 |          14.4670 |
[32m[20221213 22:43:05 @agent_ppo2.py:185][0m |          -0.0118 |          10.6842 |          14.4651 |
[32m[20221213 22:43:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.62
[32m[20221213 22:43:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.84
[32m[20221213 22:43:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.07
[32m[20221213 22:43:06 @agent_ppo2.py:143][0m Total time:      24.88 min
[32m[20221213 22:43:06 @agent_ppo2.py:145][0m 2420736 total steps have happened
[32m[20221213 22:43:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1182 --------------------------#
[32m[20221213 22:43:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |           0.0016 |          29.4966 |          14.4499 |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |          -0.0085 |          26.3432 |          14.4314 |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |          -0.0126 |          24.9071 |          14.4310 |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |          -0.0088 |          24.2157 |          14.4283 |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |          -0.0077 |          23.6172 |          14.4119 |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |          -0.0106 |          23.2604 |          14.4171 |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |          -0.0068 |          22.9813 |          14.4064 |
[32m[20221213 22:43:06 @agent_ppo2.py:185][0m |          -0.0064 |          22.5834 |          14.4113 |
[32m[20221213 22:43:07 @agent_ppo2.py:185][0m |          -0.0119 |          22.2777 |          14.4092 |
[32m[20221213 22:43:07 @agent_ppo2.py:185][0m |          -0.0137 |          22.0396 |          14.4093 |
[32m[20221213 22:43:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.37
[32m[20221213 22:43:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.85
[32m[20221213 22:43:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.06
[32m[20221213 22:43:07 @agent_ppo2.py:143][0m Total time:      24.90 min
[32m[20221213 22:43:07 @agent_ppo2.py:145][0m 2422784 total steps have happened
[32m[20221213 22:43:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1183 --------------------------#
[32m[20221213 22:43:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:07 @agent_ppo2.py:185][0m |          -0.0025 |          29.8206 |          14.6299 |
[32m[20221213 22:43:07 @agent_ppo2.py:185][0m |          -0.0023 |          26.1592 |          14.6189 |
[32m[20221213 22:43:07 @agent_ppo2.py:185][0m |          -0.0078 |          24.4972 |          14.6045 |
[32m[20221213 22:43:07 @agent_ppo2.py:185][0m |          -0.0096 |          24.1093 |          14.5969 |
[32m[20221213 22:43:07 @agent_ppo2.py:185][0m |          -0.0098 |          23.6941 |          14.5898 |
[32m[20221213 22:43:08 @agent_ppo2.py:185][0m |          -0.0122 |          23.1392 |          14.5905 |
[32m[20221213 22:43:08 @agent_ppo2.py:185][0m |          -0.0102 |          22.8890 |          14.5903 |
[32m[20221213 22:43:08 @agent_ppo2.py:185][0m |          -0.0121 |          22.6098 |          14.5925 |
[32m[20221213 22:43:08 @agent_ppo2.py:185][0m |          -0.0168 |          22.5144 |          14.5893 |
[32m[20221213 22:43:08 @agent_ppo2.py:185][0m |          -0.0127 |          22.3383 |          14.5851 |
[32m[20221213 22:43:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.98
[32m[20221213 22:43:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.47
[32m[20221213 22:43:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.86
[32m[20221213 22:43:08 @agent_ppo2.py:143][0m Total time:      24.92 min
[32m[20221213 22:43:08 @agent_ppo2.py:145][0m 2424832 total steps have happened
[32m[20221213 22:43:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1184 --------------------------#
[32m[20221213 22:43:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:08 @agent_ppo2.py:185][0m |          -0.0002 |          28.1325 |          14.6232 |
[32m[20221213 22:43:08 @agent_ppo2.py:185][0m |           0.0006 |          25.1488 |          14.6178 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0054 |          23.6823 |          14.6119 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0073 |          22.8477 |          14.6009 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0062 |          22.0665 |          14.5937 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0093 |          21.5841 |          14.6039 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0118 |          21.2291 |          14.5945 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0110 |          20.9586 |          14.6039 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0064 |          20.6037 |          14.5879 |
[32m[20221213 22:43:09 @agent_ppo2.py:185][0m |          -0.0160 |          20.3385 |          14.5930 |
[32m[20221213 22:43:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.43
[32m[20221213 22:43:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.95
[32m[20221213 22:43:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.19
[32m[20221213 22:43:09 @agent_ppo2.py:143][0m Total time:      24.94 min
[32m[20221213 22:43:09 @agent_ppo2.py:145][0m 2426880 total steps have happened
[32m[20221213 22:43:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1185 --------------------------#
[32m[20221213 22:43:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0075 |          27.8682 |          14.6454 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0022 |          20.7549 |          14.6199 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0071 |          19.4663 |          14.6197 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0092 |          18.7493 |          14.6278 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0081 |          18.3424 |          14.6069 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0056 |          18.2207 |          14.6204 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0125 |          17.8697 |          14.6199 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0101 |          17.7421 |          14.6099 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0107 |          18.0069 |          14.6165 |
[32m[20221213 22:43:10 @agent_ppo2.py:185][0m |          -0.0132 |          17.3737 |          14.6077 |
[32m[20221213 22:43:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.80
[32m[20221213 22:43:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.19
[32m[20221213 22:43:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.43
[32m[20221213 22:43:10 @agent_ppo2.py:143][0m Total time:      24.96 min
[32m[20221213 22:43:10 @agent_ppo2.py:145][0m 2428928 total steps have happened
[32m[20221213 22:43:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1186 --------------------------#
[32m[20221213 22:43:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |           0.0094 |          28.9809 |          14.8186 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0034 |          23.3124 |          14.7847 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |           0.0020 |          23.9427 |          14.7777 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0021 |          24.3470 |          14.7717 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0069 |          21.9048 |          14.7610 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0139 |          21.6508 |          14.7696 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0092 |          21.3848 |          14.7598 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0115 |          21.2206 |          14.7707 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0094 |          21.1517 |          14.7655 |
[32m[20221213 22:43:11 @agent_ppo2.py:185][0m |          -0.0111 |          20.9522 |          14.7598 |
[32m[20221213 22:43:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.73
[32m[20221213 22:43:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.52
[32m[20221213 22:43:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.39
[32m[20221213 22:43:12 @agent_ppo2.py:143][0m Total time:      24.98 min
[32m[20221213 22:43:12 @agent_ppo2.py:145][0m 2430976 total steps have happened
[32m[20221213 22:43:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1187 --------------------------#
[32m[20221213 22:43:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:12 @agent_ppo2.py:185][0m |           0.0015 |          22.9566 |          14.7129 |
[32m[20221213 22:43:12 @agent_ppo2.py:185][0m |          -0.0082 |          21.3728 |          14.6957 |
[32m[20221213 22:43:12 @agent_ppo2.py:185][0m |          -0.0041 |          20.6614 |          14.6872 |
[32m[20221213 22:43:12 @agent_ppo2.py:185][0m |           0.0036 |          21.8444 |          14.6834 |
[32m[20221213 22:43:12 @agent_ppo2.py:185][0m |          -0.0087 |          19.8535 |          14.6654 |
[32m[20221213 22:43:12 @agent_ppo2.py:185][0m |          -0.0112 |          19.5144 |          14.6773 |
[32m[20221213 22:43:12 @agent_ppo2.py:185][0m |          -0.0147 |          19.2408 |          14.6771 |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0142 |          19.0549 |          14.6713 |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0125 |          18.9005 |          14.6776 |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0144 |          18.7690 |          14.6642 |
[32m[20221213 22:43:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.21
[32m[20221213 22:43:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.05
[32m[20221213 22:43:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.96
[32m[20221213 22:43:13 @agent_ppo2.py:143][0m Total time:      25.00 min
[32m[20221213 22:43:13 @agent_ppo2.py:145][0m 2433024 total steps have happened
[32m[20221213 22:43:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1188 --------------------------#
[32m[20221213 22:43:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0059 |          26.9715 |          14.4148 |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0023 |          25.5087 |          14.4185 |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0071 |          24.5477 |          14.4053 |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0013 |          24.6982 |          14.3913 |
[32m[20221213 22:43:13 @agent_ppo2.py:185][0m |          -0.0007 |          25.1865 |          14.3879 |
[32m[20221213 22:43:14 @agent_ppo2.py:185][0m |          -0.0023 |          25.5081 |          14.3979 |
[32m[20221213 22:43:14 @agent_ppo2.py:185][0m |          -0.0169 |          23.8230 |          14.3833 |
[32m[20221213 22:43:14 @agent_ppo2.py:185][0m |          -0.0109 |          23.7184 |          14.3928 |
[32m[20221213 22:43:14 @agent_ppo2.py:185][0m |          -0.0116 |          23.6480 |          14.3931 |
[32m[20221213 22:43:14 @agent_ppo2.py:185][0m |          -0.0123 |          23.4297 |          14.3948 |
[32m[20221213 22:43:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.02
[32m[20221213 22:43:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.01
[32m[20221213 22:43:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.04
[32m[20221213 22:43:14 @agent_ppo2.py:143][0m Total time:      25.02 min
[32m[20221213 22:43:14 @agent_ppo2.py:145][0m 2435072 total steps have happened
[32m[20221213 22:43:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1189 --------------------------#
[32m[20221213 22:43:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:14 @agent_ppo2.py:185][0m |          -0.0026 |          22.1203 |          14.7426 |
[32m[20221213 22:43:14 @agent_ppo2.py:185][0m |          -0.0062 |          19.0218 |          14.7181 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |          -0.0076 |          18.2221 |          14.7131 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |           0.0025 |          19.8538 |          14.7182 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |          -0.0081 |          17.3945 |          14.6979 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |          -0.0105 |          17.0913 |          14.7064 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |          -0.0085 |          16.9281 |          14.7003 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |          -0.0103 |          16.6630 |          14.7017 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |          -0.0133 |          16.6220 |          14.7036 |
[32m[20221213 22:43:15 @agent_ppo2.py:185][0m |          -0.0136 |          16.3025 |          14.6916 |
[32m[20221213 22:43:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:43:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.75
[32m[20221213 22:43:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.91
[32m[20221213 22:43:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.22
[32m[20221213 22:43:15 @agent_ppo2.py:143][0m Total time:      25.04 min
[32m[20221213 22:43:15 @agent_ppo2.py:145][0m 2437120 total steps have happened
[32m[20221213 22:43:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1190 --------------------------#
[32m[20221213 22:43:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:43:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0030 |          34.5760 |          14.8486 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |           0.0071 |          32.1635 |          14.8518 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0054 |          29.1232 |          14.8324 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0094 |          28.1982 |          14.8254 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0110 |          27.7050 |          14.8301 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0095 |          27.4863 |          14.8307 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0107 |          26.8471 |          14.8258 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0118 |          26.5818 |          14.8269 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0067 |          27.0108 |          14.8236 |
[32m[20221213 22:43:16 @agent_ppo2.py:185][0m |          -0.0126 |          26.2036 |          14.8227 |
[32m[20221213 22:43:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:43:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.37
[32m[20221213 22:43:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.61
[32m[20221213 22:43:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.53
[32m[20221213 22:43:17 @agent_ppo2.py:143][0m Total time:      25.06 min
[32m[20221213 22:43:17 @agent_ppo2.py:145][0m 2439168 total steps have happened
[32m[20221213 22:43:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1191 --------------------------#
[32m[20221213 22:43:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0040 |          24.1864 |          14.7253 |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0100 |          20.8666 |          14.7052 |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0101 |          20.4983 |          14.7013 |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0031 |          20.0845 |          14.6991 |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0076 |          19.7852 |          14.6881 |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0100 |          19.4542 |          14.6831 |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0069 |          19.3079 |          14.6859 |
[32m[20221213 22:43:17 @agent_ppo2.py:185][0m |          -0.0094 |          19.2361 |          14.6860 |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0100 |          19.0306 |          14.6841 |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0147 |          18.8273 |          14.6788 |
[32m[20221213 22:43:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.23
[32m[20221213 22:43:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.41
[32m[20221213 22:43:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.91
[32m[20221213 22:43:18 @agent_ppo2.py:143][0m Total time:      25.08 min
[32m[20221213 22:43:18 @agent_ppo2.py:145][0m 2441216 total steps have happened
[32m[20221213 22:43:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1192 --------------------------#
[32m[20221213 22:43:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0020 |          26.5433 |          14.7625 |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0028 |          24.4046 |          14.7376 |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0055 |          23.2207 |          14.7377 |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0128 |          22.8888 |          14.7343 |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0138 |          22.4431 |          14.7370 |
[32m[20221213 22:43:18 @agent_ppo2.py:185][0m |          -0.0155 |          22.2225 |          14.7293 |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |          -0.0109 |          22.2634 |          14.7250 |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |          -0.0152 |          21.9549 |          14.7170 |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |          -0.0101 |          22.3734 |          14.7170 |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |          -0.0089 |          22.7245 |          14.7192 |
[32m[20221213 22:43:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.54
[32m[20221213 22:43:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.60
[32m[20221213 22:43:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.27
[32m[20221213 22:43:19 @agent_ppo2.py:143][0m Total time:      25.10 min
[32m[20221213 22:43:19 @agent_ppo2.py:145][0m 2443264 total steps have happened
[32m[20221213 22:43:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1193 --------------------------#
[32m[20221213 22:43:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |           0.0006 |          20.6862 |          14.9413 |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |          -0.0013 |          17.6809 |          14.9304 |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |          -0.0048 |          16.8916 |          14.9232 |
[32m[20221213 22:43:19 @agent_ppo2.py:185][0m |          -0.0061 |          16.6729 |          14.9244 |
[32m[20221213 22:43:20 @agent_ppo2.py:185][0m |          -0.0064 |          16.2542 |          14.9363 |
[32m[20221213 22:43:20 @agent_ppo2.py:185][0m |          -0.0077 |          16.0133 |          14.9307 |
[32m[20221213 22:43:20 @agent_ppo2.py:185][0m |          -0.0108 |          15.9468 |          14.9285 |
[32m[20221213 22:43:20 @agent_ppo2.py:185][0m |          -0.0125 |          15.8001 |          14.9201 |
[32m[20221213 22:43:20 @agent_ppo2.py:185][0m |          -0.0130 |          15.6292 |          14.9261 |
[32m[20221213 22:43:20 @agent_ppo2.py:185][0m |          -0.0132 |          15.6069 |          14.9271 |
[32m[20221213 22:43:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.87
[32m[20221213 22:43:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.12
[32m[20221213 22:43:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 210.63
[32m[20221213 22:43:20 @agent_ppo2.py:143][0m Total time:      25.12 min
[32m[20221213 22:43:20 @agent_ppo2.py:145][0m 2445312 total steps have happened
[32m[20221213 22:43:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1194 --------------------------#
[32m[20221213 22:43:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:20 @agent_ppo2.py:185][0m |          -0.0008 |          23.3248 |          14.8258 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |           0.0027 |          23.1973 |          14.8254 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0070 |          21.1529 |          14.7955 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0075 |          20.7225 |          14.8068 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0044 |          20.4478 |          14.8059 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0026 |          20.4407 |          14.7980 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0111 |          20.0973 |          14.7946 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0156 |          19.8545 |          14.7988 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0124 |          19.6853 |          14.7956 |
[32m[20221213 22:43:21 @agent_ppo2.py:185][0m |          -0.0140 |          19.5045 |          14.7988 |
[32m[20221213 22:43:21 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.40
[32m[20221213 22:43:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.99
[32m[20221213 22:43:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.52
[32m[20221213 22:43:21 @agent_ppo2.py:143][0m Total time:      25.14 min
[32m[20221213 22:43:21 @agent_ppo2.py:145][0m 2447360 total steps have happened
[32m[20221213 22:43:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1195 --------------------------#
[32m[20221213 22:43:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0046 |          36.2443 |          14.7918 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0047 |          35.0027 |          14.7617 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0098 |          34.2348 |          14.7647 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0102 |          34.0065 |          14.7483 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0090 |          33.7315 |          14.7528 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0092 |          33.5170 |          14.7488 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0121 |          33.2893 |          14.7495 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0109 |          33.2214 |          14.7424 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0020 |          35.1069 |          14.7374 |
[32m[20221213 22:43:22 @agent_ppo2.py:185][0m |          -0.0111 |          32.9226 |          14.7357 |
[32m[20221213 22:43:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.75
[32m[20221213 22:43:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.22
[32m[20221213 22:43:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.27
[32m[20221213 22:43:23 @agent_ppo2.py:143][0m Total time:      25.16 min
[32m[20221213 22:43:23 @agent_ppo2.py:145][0m 2449408 total steps have happened
[32m[20221213 22:43:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1196 --------------------------#
[32m[20221213 22:43:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |           0.0001 |          25.2385 |          14.8496 |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |           0.0016 |          22.1531 |          14.8319 |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |          -0.0084 |          20.4743 |          14.8216 |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |          -0.0132 |          19.4097 |          14.8085 |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |          -0.0073 |          18.7560 |          14.8143 |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |          -0.0132 |          18.1786 |          14.8119 |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |          -0.0123 |          17.7502 |          14.8088 |
[32m[20221213 22:43:23 @agent_ppo2.py:185][0m |          -0.0021 |          17.9920 |          14.8115 |
[32m[20221213 22:43:24 @agent_ppo2.py:185][0m |          -0.0139 |          17.3303 |          14.7994 |
[32m[20221213 22:43:24 @agent_ppo2.py:185][0m |          -0.0090 |          17.0963 |          14.8081 |
[32m[20221213 22:43:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.12
[32m[20221213 22:43:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.38
[32m[20221213 22:43:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.26
[32m[20221213 22:43:24 @agent_ppo2.py:143][0m Total time:      25.18 min
[32m[20221213 22:43:24 @agent_ppo2.py:145][0m 2451456 total steps have happened
[32m[20221213 22:43:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1197 --------------------------#
[32m[20221213 22:43:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:24 @agent_ppo2.py:185][0m |          -0.0016 |          31.3974 |          14.9274 |
[32m[20221213 22:43:24 @agent_ppo2.py:185][0m |          -0.0052 |          28.1694 |          14.9183 |
[32m[20221213 22:43:24 @agent_ppo2.py:185][0m |           0.0019 |          29.1367 |          14.9194 |
[32m[20221213 22:43:24 @agent_ppo2.py:185][0m |          -0.0077 |          26.9390 |          14.9047 |
[32m[20221213 22:43:24 @agent_ppo2.py:185][0m |          -0.0110 |          26.6045 |          14.9017 |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |          -0.0070 |          26.2774 |          14.9045 |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |          -0.0096 |          26.1100 |          14.9163 |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |          -0.0087 |          26.2350 |          14.9141 |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |           0.0024 |          29.0154 |          14.9009 |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |          -0.0132 |          25.6830 |          14.8972 |
[32m[20221213 22:43:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.02
[32m[20221213 22:43:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.22
[32m[20221213 22:43:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.05
[32m[20221213 22:43:25 @agent_ppo2.py:143][0m Total time:      25.20 min
[32m[20221213 22:43:25 @agent_ppo2.py:145][0m 2453504 total steps have happened
[32m[20221213 22:43:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1198 --------------------------#
[32m[20221213 22:43:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |           0.0009 |          32.4115 |          15.1404 |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |          -0.0042 |          30.5995 |          15.1453 |
[32m[20221213 22:43:25 @agent_ppo2.py:185][0m |          -0.0025 |          30.0797 |          15.1368 |
[32m[20221213 22:43:26 @agent_ppo2.py:185][0m |          -0.0049 |          29.7867 |          15.1368 |
[32m[20221213 22:43:26 @agent_ppo2.py:185][0m |          -0.0073 |          29.5990 |          15.1206 |
[32m[20221213 22:43:26 @agent_ppo2.py:185][0m |          -0.0058 |          29.3393 |          15.1285 |
[32m[20221213 22:43:26 @agent_ppo2.py:185][0m |          -0.0049 |          29.2561 |          15.1338 |
[32m[20221213 22:43:26 @agent_ppo2.py:185][0m |          -0.0126 |          29.0128 |          15.1376 |
[32m[20221213 22:43:26 @agent_ppo2.py:185][0m |          -0.0074 |          29.0889 |          15.1339 |
[32m[20221213 22:43:26 @agent_ppo2.py:185][0m |          -0.0002 |          29.7748 |          15.1288 |
[32m[20221213 22:43:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.99
[32m[20221213 22:43:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.24
[32m[20221213 22:43:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.93
[32m[20221213 22:43:26 @agent_ppo2.py:143][0m Total time:      25.22 min
[32m[20221213 22:43:26 @agent_ppo2.py:145][0m 2455552 total steps have happened
[32m[20221213 22:43:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1199 --------------------------#
[32m[20221213 22:43:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |           0.0003 |          30.3090 |          15.0748 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0068 |          26.7178 |          15.0745 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0121 |          25.5226 |          15.0770 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0111 |          24.8278 |          15.0737 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0147 |          24.2889 |          15.0734 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0116 |          23.8914 |          15.0682 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0138 |          23.8057 |          15.0743 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0182 |          23.3216 |          15.0772 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0106 |          23.2705 |          15.0685 |
[32m[20221213 22:43:27 @agent_ppo2.py:185][0m |          -0.0118 |          22.9369 |          15.0683 |
[32m[20221213 22:43:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:43:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.52
[32m[20221213 22:43:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.19
[32m[20221213 22:43:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.05
[32m[20221213 22:43:27 @agent_ppo2.py:143][0m Total time:      25.24 min
[32m[20221213 22:43:27 @agent_ppo2.py:145][0m 2457600 total steps have happened
[32m[20221213 22:43:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1200 --------------------------#
[32m[20221213 22:43:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:43:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0031 |          33.4444 |          15.1724 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0064 |          31.0548 |          15.1633 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0057 |          30.2714 |          15.1486 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0081 |          29.6945 |          15.1567 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0067 |          29.4192 |          15.1391 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0066 |          29.0348 |          15.1470 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0128 |          28.9433 |          15.1437 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0115 |          28.5647 |          15.1397 |
[32m[20221213 22:43:28 @agent_ppo2.py:185][0m |          -0.0165 |          28.6227 |          15.1410 |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |          -0.0129 |          28.3412 |          15.1352 |
[32m[20221213 22:43:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.52
[32m[20221213 22:43:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 346.54
[32m[20221213 22:43:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.00
[32m[20221213 22:43:29 @agent_ppo2.py:143][0m Total time:      25.26 min
[32m[20221213 22:43:29 @agent_ppo2.py:145][0m 2459648 total steps have happened
[32m[20221213 22:43:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1201 --------------------------#
[32m[20221213 22:43:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |           0.0018 |          29.9688 |          15.0325 |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |          -0.0027 |          28.4925 |          15.0153 |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |          -0.0106 |          28.2751 |          15.0067 |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |          -0.0094 |          27.8417 |          15.0062 |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |           0.0032 |          31.0281 |          15.0104 |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |          -0.0073 |          27.8082 |          15.0033 |
[32m[20221213 22:43:29 @agent_ppo2.py:185][0m |          -0.0081 |          27.5477 |          15.0059 |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |          -0.0165 |          27.5543 |          15.0070 |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |          -0.0107 |          27.9071 |          14.9993 |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |          -0.0123 |          27.3034 |          15.0116 |
[32m[20221213 22:43:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.98
[32m[20221213 22:43:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.23
[32m[20221213 22:43:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.98
[32m[20221213 22:43:30 @agent_ppo2.py:143][0m Total time:      25.28 min
[32m[20221213 22:43:30 @agent_ppo2.py:145][0m 2461696 total steps have happened
[32m[20221213 22:43:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1202 --------------------------#
[32m[20221213 22:43:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |           0.0033 |          28.9380 |          15.1773 |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |          -0.0042 |          26.5438 |          15.1761 |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |          -0.0079 |          25.8592 |          15.1778 |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |          -0.0004 |          26.2281 |          15.1684 |
[32m[20221213 22:43:30 @agent_ppo2.py:185][0m |          -0.0029 |          25.2414 |          15.1600 |
[32m[20221213 22:43:31 @agent_ppo2.py:185][0m |          -0.0122 |          24.9098 |          15.1671 |
[32m[20221213 22:43:31 @agent_ppo2.py:185][0m |          -0.0052 |          24.8465 |          15.1516 |
[32m[20221213 22:43:31 @agent_ppo2.py:185][0m |          -0.0093 |          24.5150 |          15.1634 |
[32m[20221213 22:43:31 @agent_ppo2.py:185][0m |          -0.0083 |          24.4040 |          15.1638 |
[32m[20221213 22:43:31 @agent_ppo2.py:185][0m |          -0.0097 |          24.2413 |          15.1583 |
[32m[20221213 22:43:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.68
[32m[20221213 22:43:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.73
[32m[20221213 22:43:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 205.83
[32m[20221213 22:43:31 @agent_ppo2.py:143][0m Total time:      25.30 min
[32m[20221213 22:43:31 @agent_ppo2.py:145][0m 2463744 total steps have happened
[32m[20221213 22:43:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1203 --------------------------#
[32m[20221213 22:43:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:31 @agent_ppo2.py:185][0m |          -0.0008 |          28.7711 |          14.8286 |
[32m[20221213 22:43:31 @agent_ppo2.py:185][0m |          -0.0035 |          27.4183 |          14.8040 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0086 |          26.9325 |          14.8091 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0124 |          26.4769 |          14.7984 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0077 |          26.3171 |          14.7992 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0114 |          26.1661 |          14.7883 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0099 |          26.1011 |          14.7941 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0117 |          25.8714 |          14.7952 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0127 |          25.8144 |          14.7934 |
[32m[20221213 22:43:32 @agent_ppo2.py:185][0m |          -0.0053 |          25.9002 |          14.7840 |
[32m[20221213 22:43:32 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.38
[32m[20221213 22:43:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.76
[32m[20221213 22:43:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.95
[32m[20221213 22:43:32 @agent_ppo2.py:143][0m Total time:      25.32 min
[32m[20221213 22:43:32 @agent_ppo2.py:145][0m 2465792 total steps have happened
[32m[20221213 22:43:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1204 --------------------------#
[32m[20221213 22:43:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |           0.0050 |          29.8700 |          14.9231 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0096 |          26.5037 |          14.8958 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0077 |          25.5578 |          14.9023 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |           0.0028 |          28.7116 |          14.9056 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0121 |          24.3072 |          14.8891 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0136 |          23.7884 |          14.8956 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0141 |          23.4128 |          14.8906 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0145 |          23.0963 |          14.8912 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0162 |          22.9076 |          14.8844 |
[32m[20221213 22:43:33 @agent_ppo2.py:185][0m |          -0.0109 |          23.2343 |          14.8923 |
[32m[20221213 22:43:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.19
[32m[20221213 22:43:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.54
[32m[20221213 22:43:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.65
[32m[20221213 22:43:33 @agent_ppo2.py:143][0m Total time:      25.34 min
[32m[20221213 22:43:33 @agent_ppo2.py:145][0m 2467840 total steps have happened
[32m[20221213 22:43:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1205 --------------------------#
[32m[20221213 22:43:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |           0.0014 |          24.7879 |          14.9467 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0001 |          21.1545 |          14.9240 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0134 |          20.5336 |          14.9136 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0078 |          20.0437 |          14.9114 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0112 |          19.9673 |          14.9146 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0119 |          19.7394 |          14.8983 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0193 |          19.5142 |          14.8999 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0122 |          19.2384 |          14.8940 |
[32m[20221213 22:43:34 @agent_ppo2.py:185][0m |          -0.0033 |          19.3579 |          14.8982 |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |          -0.0115 |          19.0964 |          14.8892 |
[32m[20221213 22:43:35 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.06
[32m[20221213 22:43:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.82
[32m[20221213 22:43:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.10
[32m[20221213 22:43:35 @agent_ppo2.py:143][0m Total time:      25.36 min
[32m[20221213 22:43:35 @agent_ppo2.py:145][0m 2469888 total steps have happened
[32m[20221213 22:43:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1206 --------------------------#
[32m[20221213 22:43:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |           0.0017 |          29.4956 |          14.8845 |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |          -0.0080 |          27.2484 |          14.8880 |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |          -0.0049 |          26.4104 |          14.8640 |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |          -0.0076 |          25.7481 |          14.8676 |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |          -0.0096 |          25.3153 |          14.8593 |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |          -0.0097 |          25.1856 |          14.8542 |
[32m[20221213 22:43:35 @agent_ppo2.py:185][0m |          -0.0102 |          24.9529 |          14.8555 |
[32m[20221213 22:43:36 @agent_ppo2.py:185][0m |          -0.0095 |          24.7808 |          14.8559 |
[32m[20221213 22:43:36 @agent_ppo2.py:185][0m |          -0.0062 |          24.6213 |          14.8456 |
[32m[20221213 22:43:36 @agent_ppo2.py:185][0m |          -0.0093 |          24.4133 |          14.8499 |
[32m[20221213 22:43:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.32
[32m[20221213 22:43:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 354.83
[32m[20221213 22:43:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 219.71
[32m[20221213 22:43:36 @agent_ppo2.py:143][0m Total time:      25.38 min
[32m[20221213 22:43:36 @agent_ppo2.py:145][0m 2471936 total steps have happened
[32m[20221213 22:43:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1207 --------------------------#
[32m[20221213 22:43:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:36 @agent_ppo2.py:185][0m |           0.0044 |          19.6210 |          14.8859 |
[32m[20221213 22:43:36 @agent_ppo2.py:185][0m |          -0.0050 |          16.9432 |          14.8708 |
[32m[20221213 22:43:36 @agent_ppo2.py:185][0m |          -0.0093 |          16.2686 |          14.8710 |
[32m[20221213 22:43:36 @agent_ppo2.py:185][0m |          -0.0083 |          15.9093 |          14.8627 |
[32m[20221213 22:43:37 @agent_ppo2.py:185][0m |          -0.0053 |          15.5228 |          14.8617 |
[32m[20221213 22:43:37 @agent_ppo2.py:185][0m |          -0.0094 |          15.2510 |          14.8649 |
[32m[20221213 22:43:37 @agent_ppo2.py:185][0m |          -0.0126 |          15.0582 |          14.8578 |
[32m[20221213 22:43:37 @agent_ppo2.py:185][0m |          -0.0121 |          14.8023 |          14.8565 |
[32m[20221213 22:43:37 @agent_ppo2.py:185][0m |          -0.0129 |          14.6522 |          14.8528 |
[32m[20221213 22:43:37 @agent_ppo2.py:185][0m |          -0.0136 |          14.5176 |          14.8520 |
[32m[20221213 22:43:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 156.82
[32m[20221213 22:43:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.72
[32m[20221213 22:43:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.28
[32m[20221213 22:43:37 @agent_ppo2.py:143][0m Total time:      25.40 min
[32m[20221213 22:43:37 @agent_ppo2.py:145][0m 2473984 total steps have happened
[32m[20221213 22:43:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1208 --------------------------#
[32m[20221213 22:43:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:37 @agent_ppo2.py:185][0m |           0.0020 |          28.0436 |          14.8817 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |           0.0006 |          26.4815 |          14.8653 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |           0.0001 |          25.5127 |          14.8511 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |          -0.0069 |          25.0160 |          14.8463 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |          -0.0002 |          24.6794 |          14.8498 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |          -0.0070 |          24.2973 |          14.8466 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |          -0.0072 |          24.1008 |          14.8474 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |          -0.0110 |          23.9651 |          14.8524 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |          -0.0092 |          23.5453 |          14.8461 |
[32m[20221213 22:43:38 @agent_ppo2.py:185][0m |          -0.0096 |          23.4616 |          14.8424 |
[32m[20221213 22:43:38 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.12
[32m[20221213 22:43:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.94
[32m[20221213 22:43:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.82
[32m[20221213 22:43:38 @agent_ppo2.py:143][0m Total time:      25.42 min
[32m[20221213 22:43:38 @agent_ppo2.py:145][0m 2476032 total steps have happened
[32m[20221213 22:43:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1209 --------------------------#
[32m[20221213 22:43:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |           0.0040 |          30.4427 |          15.0267 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0076 |          28.5635 |          14.9998 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0017 |          28.2353 |          15.0014 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0066 |          27.9021 |          14.9843 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0066 |          27.7673 |          14.9885 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0090 |          27.6139 |          14.9878 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0080 |          27.4786 |          14.9672 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0100 |          27.4250 |          14.9691 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0081 |          27.3363 |          14.9828 |
[32m[20221213 22:43:39 @agent_ppo2.py:185][0m |          -0.0108 |          27.2368 |          14.9681 |
[32m[20221213 22:43:39 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.32
[32m[20221213 22:43:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.45
[32m[20221213 22:43:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.31
[32m[20221213 22:43:40 @agent_ppo2.py:143][0m Total time:      25.44 min
[32m[20221213 22:43:40 @agent_ppo2.py:145][0m 2478080 total steps have happened
[32m[20221213 22:43:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1210 --------------------------#
[32m[20221213 22:43:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:43:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0055 |          22.5036 |          15.0911 |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0031 |          18.8033 |          15.0605 |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0065 |          17.3876 |          15.0628 |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0114 |          16.5134 |          15.0641 |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0110 |          15.9444 |          15.0623 |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0126 |          15.2479 |          15.0604 |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0149 |          14.9286 |          15.0527 |
[32m[20221213 22:43:40 @agent_ppo2.py:185][0m |          -0.0162 |          14.3932 |          15.0494 |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |          -0.0088 |          14.1673 |          15.0505 |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |          -0.0094 |          13.8326 |          15.0468 |
[32m[20221213 22:43:41 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.41
[32m[20221213 22:43:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.66
[32m[20221213 22:43:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.53
[32m[20221213 22:43:41 @agent_ppo2.py:143][0m Total time:      25.46 min
[32m[20221213 22:43:41 @agent_ppo2.py:145][0m 2480128 total steps have happened
[32m[20221213 22:43:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1211 --------------------------#
[32m[20221213 22:43:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |           0.0027 |          32.2239 |          14.9089 |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |          -0.0029 |          25.6858 |          14.8856 |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |          -0.0073 |          24.4139 |          14.8814 |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |          -0.0088 |          23.7517 |          14.8813 |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |          -0.0081 |          23.1333 |          14.8767 |
[32m[20221213 22:43:41 @agent_ppo2.py:185][0m |          -0.0070 |          22.7961 |          14.8688 |
[32m[20221213 22:43:42 @agent_ppo2.py:185][0m |          -0.0054 |          23.0642 |          14.8694 |
[32m[20221213 22:43:42 @agent_ppo2.py:185][0m |          -0.0099 |          22.1585 |          14.8701 |
[32m[20221213 22:43:42 @agent_ppo2.py:185][0m |          -0.0150 |          21.8930 |          14.8637 |
[32m[20221213 22:43:42 @agent_ppo2.py:185][0m |          -0.0118 |          21.6777 |          14.8640 |
[32m[20221213 22:43:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.39
[32m[20221213 22:43:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.76
[32m[20221213 22:43:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.98
[32m[20221213 22:43:42 @agent_ppo2.py:143][0m Total time:      25.48 min
[32m[20221213 22:43:42 @agent_ppo2.py:145][0m 2482176 total steps have happened
[32m[20221213 22:43:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1212 --------------------------#
[32m[20221213 22:43:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:42 @agent_ppo2.py:185][0m |           0.0034 |          25.0226 |          14.9240 |
[32m[20221213 22:43:42 @agent_ppo2.py:185][0m |          -0.0072 |          21.3368 |          14.8933 |
[32m[20221213 22:43:42 @agent_ppo2.py:185][0m |          -0.0093 |          20.5413 |          14.8869 |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |          -0.0133 |          20.0539 |          14.8900 |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |          -0.0178 |          19.7850 |          14.8716 |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |          -0.0173 |          19.4566 |          14.8839 |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |          -0.0087 |          19.2760 |          14.8763 |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |          -0.0122 |          19.1448 |          14.8743 |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |          -0.0067 |          18.9715 |          14.8768 |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |          -0.0125 |          18.8438 |          14.8835 |
[32m[20221213 22:43:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.87
[32m[20221213 22:43:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.49
[32m[20221213 22:43:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.96
[32m[20221213 22:43:43 @agent_ppo2.py:143][0m Total time:      25.50 min
[32m[20221213 22:43:43 @agent_ppo2.py:145][0m 2484224 total steps have happened
[32m[20221213 22:43:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1213 --------------------------#
[32m[20221213 22:43:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:43 @agent_ppo2.py:185][0m |           0.0018 |          31.5871 |          15.1463 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |           0.0046 |          31.0212 |          15.1318 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0052 |          28.0641 |          15.1308 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0097 |          27.6101 |          15.1296 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0124 |          27.2221 |          15.1333 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0080 |          26.9023 |          15.1217 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0067 |          26.9376 |          15.1243 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0123 |          26.4715 |          15.1223 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0098 |          26.4806 |          15.1230 |
[32m[20221213 22:43:44 @agent_ppo2.py:185][0m |          -0.0141 |          26.2129 |          15.1158 |
[32m[20221213 22:43:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.70
[32m[20221213 22:43:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.14
[32m[20221213 22:43:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.67
[32m[20221213 22:43:44 @agent_ppo2.py:143][0m Total time:      25.52 min
[32m[20221213 22:43:44 @agent_ppo2.py:145][0m 2486272 total steps have happened
[32m[20221213 22:43:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1214 --------------------------#
[32m[20221213 22:43:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |           0.0022 |          42.7634 |          15.1384 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |          -0.0035 |          40.1449 |          15.1160 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |          -0.0044 |          38.6753 |          15.1172 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |          -0.0045 |          38.3876 |          15.1047 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |          -0.0057 |          38.0434 |          15.1044 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |           0.0128 |          46.4218 |          15.1088 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |           0.0042 |          39.6562 |          15.0890 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |          -0.0094 |          37.6602 |          15.1253 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |          -0.0100 |          37.4270 |          15.1081 |
[32m[20221213 22:43:45 @agent_ppo2.py:185][0m |          -0.0083 |          37.1468 |          15.1227 |
[32m[20221213 22:43:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:43:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.61
[32m[20221213 22:43:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.49
[32m[20221213 22:43:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.14
[32m[20221213 22:43:46 @agent_ppo2.py:143][0m Total time:      25.55 min
[32m[20221213 22:43:46 @agent_ppo2.py:145][0m 2488320 total steps have happened
[32m[20221213 22:43:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1215 --------------------------#
[32m[20221213 22:43:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:46 @agent_ppo2.py:185][0m |          -0.0021 |          32.6699 |          15.2099 |
[32m[20221213 22:43:46 @agent_ppo2.py:185][0m |          -0.0049 |          31.4492 |          15.1952 |
[32m[20221213 22:43:46 @agent_ppo2.py:185][0m |          -0.0037 |          30.9040 |          15.1879 |
[32m[20221213 22:43:46 @agent_ppo2.py:185][0m |          -0.0019 |          31.5656 |          15.1878 |
[32m[20221213 22:43:46 @agent_ppo2.py:185][0m |          -0.0100 |          30.3733 |          15.1691 |
[32m[20221213 22:43:46 @agent_ppo2.py:185][0m |          -0.0102 |          30.0364 |          15.1840 |
[32m[20221213 22:43:46 @agent_ppo2.py:185][0m |          -0.0126 |          29.8363 |          15.1880 |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |           0.0064 |          33.6762 |          15.1889 |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |           0.0049 |          33.9315 |          15.1555 |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |          -0.0065 |          29.6585 |          15.1745 |
[32m[20221213 22:43:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.18
[32m[20221213 22:43:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.04
[32m[20221213 22:43:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.05
[32m[20221213 22:43:47 @agent_ppo2.py:143][0m Total time:      25.57 min
[32m[20221213 22:43:47 @agent_ppo2.py:145][0m 2490368 total steps have happened
[32m[20221213 22:43:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1216 --------------------------#
[32m[20221213 22:43:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |          -0.0008 |          43.2650 |          14.9487 |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |          -0.0036 |          41.8553 |          14.9353 |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |          -0.0045 |          41.0696 |          14.9243 |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |          -0.0074 |          40.8475 |          14.9133 |
[32m[20221213 22:43:47 @agent_ppo2.py:185][0m |          -0.0104 |          40.5346 |          14.9083 |
[32m[20221213 22:43:48 @agent_ppo2.py:185][0m |          -0.0092 |          40.3281 |          14.9064 |
[32m[20221213 22:43:48 @agent_ppo2.py:185][0m |          -0.0088 |          40.1981 |          14.9035 |
[32m[20221213 22:43:48 @agent_ppo2.py:185][0m |          -0.0074 |          40.6878 |          14.8952 |
[32m[20221213 22:43:48 @agent_ppo2.py:185][0m |          -0.0081 |          40.0758 |          14.9137 |
[32m[20221213 22:43:48 @agent_ppo2.py:185][0m |          -0.0093 |          40.0128 |          14.9021 |
[32m[20221213 22:43:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.31
[32m[20221213 22:43:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.76
[32m[20221213 22:43:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.74
[32m[20221213 22:43:48 @agent_ppo2.py:143][0m Total time:      25.59 min
[32m[20221213 22:43:48 @agent_ppo2.py:145][0m 2492416 total steps have happened
[32m[20221213 22:43:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1217 --------------------------#
[32m[20221213 22:43:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:48 @agent_ppo2.py:185][0m |           0.0005 |          37.5497 |          15.1402 |
[32m[20221213 22:43:48 @agent_ppo2.py:185][0m |          -0.0021 |          36.3088 |          15.1252 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |          -0.0051 |          35.8954 |          15.1350 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |           0.0108 |          38.8726 |          15.1240 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |          -0.0096 |          35.4458 |          15.1189 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |          -0.0066 |          35.0858 |          15.1285 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |          -0.0063 |          34.9365 |          15.1173 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |           0.0016 |          36.2032 |          15.1230 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |          -0.0047 |          34.8383 |          15.1213 |
[32m[20221213 22:43:49 @agent_ppo2.py:185][0m |          -0.0065 |          34.7166 |          15.1065 |
[32m[20221213 22:43:49 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.92
[32m[20221213 22:43:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.05
[32m[20221213 22:43:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.21
[32m[20221213 22:43:49 @agent_ppo2.py:143][0m Total time:      25.61 min
[32m[20221213 22:43:49 @agent_ppo2.py:145][0m 2494464 total steps have happened
[32m[20221213 22:43:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1218 --------------------------#
[32m[20221213 22:43:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |           0.0003 |          29.2128 |          15.1322 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0035 |          25.6875 |          15.1197 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0037 |          25.4359 |          15.1237 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0077 |          24.3808 |          15.1255 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0083 |          23.9734 |          15.1239 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0067 |          23.7338 |          15.1191 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0078 |          23.3996 |          15.1156 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0124 |          23.2029 |          15.1161 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0130 |          22.8818 |          15.1126 |
[32m[20221213 22:43:50 @agent_ppo2.py:185][0m |          -0.0123 |          22.5791 |          15.1160 |
[32m[20221213 22:43:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 160.15
[32m[20221213 22:43:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.69
[32m[20221213 22:43:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.95
[32m[20221213 22:43:50 @agent_ppo2.py:143][0m Total time:      25.63 min
[32m[20221213 22:43:50 @agent_ppo2.py:145][0m 2496512 total steps have happened
[32m[20221213 22:43:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1219 --------------------------#
[32m[20221213 22:43:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0040 |          25.3236 |          15.0174 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0074 |          23.2263 |          15.0124 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0091 |          22.8175 |          14.9934 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0095 |          22.6088 |          15.0011 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0093 |          22.4642 |          15.0067 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0126 |          22.3508 |          14.9992 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0119 |          22.2865 |          14.9950 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0082 |          22.2308 |          15.0052 |
[32m[20221213 22:43:51 @agent_ppo2.py:185][0m |          -0.0098 |          22.1935 |          14.9939 |
[32m[20221213 22:43:52 @agent_ppo2.py:185][0m |          -0.0118 |          22.1886 |          14.9919 |
[32m[20221213 22:43:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:43:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.29
[32m[20221213 22:43:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.90
[32m[20221213 22:43:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.21
[32m[20221213 22:43:52 @agent_ppo2.py:143][0m Total time:      25.65 min
[32m[20221213 22:43:52 @agent_ppo2.py:145][0m 2498560 total steps have happened
[32m[20221213 22:43:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1220 --------------------------#
[32m[20221213 22:43:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:43:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:52 @agent_ppo2.py:185][0m |          -0.0007 |          32.9680 |          14.9817 |
[32m[20221213 22:43:52 @agent_ppo2.py:185][0m |          -0.0080 |          27.4122 |          14.9569 |
[32m[20221213 22:43:52 @agent_ppo2.py:185][0m |          -0.0080 |          25.8651 |          14.9657 |
[32m[20221213 22:43:52 @agent_ppo2.py:185][0m |          -0.0093 |          25.4035 |          14.9569 |
[32m[20221213 22:43:52 @agent_ppo2.py:185][0m |           0.0022 |          25.5292 |          14.9617 |
[32m[20221213 22:43:52 @agent_ppo2.py:185][0m |          -0.0070 |          24.4542 |          14.9519 |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0092 |          24.0522 |          14.9596 |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0108 |          23.6415 |          14.9569 |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0170 |          23.8388 |          14.9525 |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0153 |          23.3310 |          14.9599 |
[32m[20221213 22:43:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.98
[32m[20221213 22:43:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.15
[32m[20221213 22:43:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 213.50
[32m[20221213 22:43:53 @agent_ppo2.py:143][0m Total time:      25.67 min
[32m[20221213 22:43:53 @agent_ppo2.py:145][0m 2500608 total steps have happened
[32m[20221213 22:43:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1221 --------------------------#
[32m[20221213 22:43:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0060 |          36.6304 |          15.0362 |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0051 |          34.2359 |          15.0271 |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0085 |          33.7468 |          15.0279 |
[32m[20221213 22:43:53 @agent_ppo2.py:185][0m |          -0.0102 |          33.1955 |          14.9991 |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0081 |          32.9079 |          15.0134 |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0101 |          32.6668 |          15.0213 |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0116 |          32.4904 |          15.0124 |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0148 |          32.3196 |          15.0159 |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0114 |          32.1429 |          15.0170 |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0110 |          31.9947 |          15.0163 |
[32m[20221213 22:43:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:43:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.64
[32m[20221213 22:43:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.06
[32m[20221213 22:43:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.43
[32m[20221213 22:43:54 @agent_ppo2.py:143][0m Total time:      25.69 min
[32m[20221213 22:43:54 @agent_ppo2.py:145][0m 2502656 total steps have happened
[32m[20221213 22:43:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1222 --------------------------#
[32m[20221213 22:43:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0007 |          23.1592 |          15.0850 |
[32m[20221213 22:43:54 @agent_ppo2.py:185][0m |          -0.0017 |          19.3769 |          15.0813 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |          -0.0069 |          18.0860 |          15.0652 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |           0.0096 |          22.6540 |          15.0594 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |          -0.0046 |          17.2754 |          15.0593 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |          -0.0001 |          17.4652 |          15.0538 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |          -0.0088 |          16.1728 |          15.0552 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |          -0.0088 |          15.8440 |          15.0601 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |          -0.0065 |          15.6249 |          15.0559 |
[32m[20221213 22:43:55 @agent_ppo2.py:185][0m |          -0.0115 |          15.5223 |          15.0622 |
[32m[20221213 22:43:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:43:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 169.28
[32m[20221213 22:43:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.43
[32m[20221213 22:43:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.82
[32m[20221213 22:43:55 @agent_ppo2.py:143][0m Total time:      25.71 min
[32m[20221213 22:43:55 @agent_ppo2.py:145][0m 2504704 total steps have happened
[32m[20221213 22:43:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1223 --------------------------#
[32m[20221213 22:43:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |           0.0101 |          38.3668 |          15.2256 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |           0.0002 |          34.8818 |          15.1674 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0062 |          34.4202 |          15.1803 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0053 |          34.2485 |          15.1725 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0110 |          34.1954 |          15.1740 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0082 |          34.0988 |          15.1691 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0120 |          33.9682 |          15.1647 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0029 |          35.1256 |          15.1637 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0105 |          33.8612 |          15.1616 |
[32m[20221213 22:43:56 @agent_ppo2.py:185][0m |          -0.0071 |          33.7614 |          15.1645 |
[32m[20221213 22:43:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.07
[32m[20221213 22:43:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.88
[32m[20221213 22:43:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.82
[32m[20221213 22:43:57 @agent_ppo2.py:143][0m Total time:      25.73 min
[32m[20221213 22:43:57 @agent_ppo2.py:145][0m 2506752 total steps have happened
[32m[20221213 22:43:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1224 --------------------------#
[32m[20221213 22:43:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |          -0.0039 |          33.5228 |          14.9720 |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |           0.0049 |          33.2817 |          14.9499 |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |          -0.0066 |          32.4666 |          14.9395 |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |          -0.0090 |          32.3727 |          14.9348 |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |          -0.0071 |          32.1892 |          14.9527 |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |          -0.0093 |          31.9831 |          14.9305 |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |          -0.0059 |          31.9126 |          14.9517 |
[32m[20221213 22:43:57 @agent_ppo2.py:185][0m |          -0.0126 |          31.8324 |          14.9344 |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |          -0.0108 |          31.7213 |          14.9497 |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |          -0.0106 |          31.6504 |          14.9484 |
[32m[20221213 22:43:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:43:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.01
[32m[20221213 22:43:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.74
[32m[20221213 22:43:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.08
[32m[20221213 22:43:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 462.08
[32m[20221213 22:43:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 462.08
[32m[20221213 22:43:58 @agent_ppo2.py:143][0m Total time:      25.75 min
[32m[20221213 22:43:58 @agent_ppo2.py:145][0m 2508800 total steps have happened
[32m[20221213 22:43:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1225 --------------------------#
[32m[20221213 22:43:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |           0.0011 |          36.1237 |          14.9478 |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |          -0.0045 |          34.3059 |          14.9210 |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |          -0.0036 |          33.3740 |          14.9384 |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |          -0.0048 |          33.1510 |          14.9239 |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |          -0.0096 |          32.2351 |          14.9238 |
[32m[20221213 22:43:58 @agent_ppo2.py:185][0m |          -0.0098 |          31.8615 |          14.9308 |
[32m[20221213 22:43:59 @agent_ppo2.py:185][0m |          -0.0097 |          31.6401 |          14.9246 |
[32m[20221213 22:43:59 @agent_ppo2.py:185][0m |          -0.0126 |          31.3630 |          14.9246 |
[32m[20221213 22:43:59 @agent_ppo2.py:185][0m |          -0.0094 |          31.0709 |          14.9271 |
[32m[20221213 22:43:59 @agent_ppo2.py:185][0m |          -0.0143 |          30.9951 |          14.9364 |
[32m[20221213 22:43:59 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:43:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.68
[32m[20221213 22:43:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.59
[32m[20221213 22:43:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.79
[32m[20221213 22:43:59 @agent_ppo2.py:143][0m Total time:      25.77 min
[32m[20221213 22:43:59 @agent_ppo2.py:145][0m 2510848 total steps have happened
[32m[20221213 22:43:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1226 --------------------------#
[32m[20221213 22:43:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:43:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:43:59 @agent_ppo2.py:185][0m |           0.0055 |          32.5115 |          15.1622 |
[32m[20221213 22:43:59 @agent_ppo2.py:185][0m |          -0.0032 |          29.4312 |          15.1355 |
[32m[20221213 22:43:59 @agent_ppo2.py:185][0m |          -0.0005 |          28.2407 |          15.1519 |
[32m[20221213 22:44:00 @agent_ppo2.py:185][0m |          -0.0039 |          27.2720 |          15.1465 |
[32m[20221213 22:44:00 @agent_ppo2.py:185][0m |          -0.0064 |          26.7462 |          15.1387 |
[32m[20221213 22:44:00 @agent_ppo2.py:185][0m |          -0.0145 |          26.1313 |          15.1363 |
[32m[20221213 22:44:00 @agent_ppo2.py:185][0m |          -0.0132 |          25.9753 |          15.1401 |
[32m[20221213 22:44:00 @agent_ppo2.py:185][0m |          -0.0112 |          25.5251 |          15.1284 |
[32m[20221213 22:44:00 @agent_ppo2.py:185][0m |          -0.0143 |          25.4417 |          15.1345 |
[32m[20221213 22:44:00 @agent_ppo2.py:185][0m |          -0.0104 |          25.0424 |          15.1285 |
[32m[20221213 22:44:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:44:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.09
[32m[20221213 22:44:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.19
[32m[20221213 22:44:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.12
[32m[20221213 22:44:00 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 480.12
[32m[20221213 22:44:00 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 480.12
[32m[20221213 22:44:00 @agent_ppo2.py:143][0m Total time:      25.79 min
[32m[20221213 22:44:00 @agent_ppo2.py:145][0m 2512896 total steps have happened
[32m[20221213 22:44:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1227 --------------------------#
[32m[20221213 22:44:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0003 |          25.6540 |          14.9829 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0148 |          21.9335 |          14.9669 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |           0.0001 |          21.0827 |          14.9627 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0110 |          19.7666 |          14.9608 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0077 |          19.2858 |          14.9620 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0148 |          19.1087 |          14.9626 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0086 |          18.9510 |          14.9600 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0091 |          18.5018 |          14.9721 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0034 |          20.0875 |          14.9678 |
[32m[20221213 22:44:01 @agent_ppo2.py:185][0m |          -0.0128 |          18.3023 |          14.9603 |
[32m[20221213 22:44:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.56
[32m[20221213 22:44:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.72
[32m[20221213 22:44:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.44
[32m[20221213 22:44:01 @agent_ppo2.py:143][0m Total time:      25.81 min
[32m[20221213 22:44:01 @agent_ppo2.py:145][0m 2514944 total steps have happened
[32m[20221213 22:44:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1228 --------------------------#
[32m[20221213 22:44:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |           0.0025 |          33.7201 |          14.8984 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |          -0.0026 |          31.4675 |          14.8869 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |          -0.0080 |          30.6189 |          14.8860 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |           0.0064 |          31.5733 |          14.8949 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |          -0.0026 |          29.9287 |          14.8742 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |          -0.0044 |          29.4331 |          14.8915 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |          -0.0133 |          29.1249 |          14.8871 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |           0.0013 |          30.8096 |          14.8862 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |          -0.0082 |          28.7651 |          14.8845 |
[32m[20221213 22:44:02 @agent_ppo2.py:185][0m |          -0.0100 |          28.6551 |          14.8891 |
[32m[20221213 22:44:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.58
[32m[20221213 22:44:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.44
[32m[20221213 22:44:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:44:03 @agent_ppo2.py:143][0m Total time:      25.83 min
[32m[20221213 22:44:03 @agent_ppo2.py:145][0m 2516992 total steps have happened
[32m[20221213 22:44:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1229 --------------------------#
[32m[20221213 22:44:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:03 @agent_ppo2.py:185][0m |           0.0008 |          37.2040 |          14.9220 |
[32m[20221213 22:44:03 @agent_ppo2.py:185][0m |          -0.0064 |          35.0550 |          14.9003 |
[32m[20221213 22:44:03 @agent_ppo2.py:185][0m |          -0.0071 |          34.5201 |          14.8905 |
[32m[20221213 22:44:03 @agent_ppo2.py:185][0m |          -0.0050 |          34.1479 |          14.9026 |
[32m[20221213 22:44:03 @agent_ppo2.py:185][0m |          -0.0070 |          34.0293 |          14.8759 |
[32m[20221213 22:44:03 @agent_ppo2.py:185][0m |          -0.0022 |          34.1610 |          14.8831 |
[32m[20221213 22:44:03 @agent_ppo2.py:185][0m |          -0.0062 |          33.6145 |          14.8721 |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |          -0.0009 |          33.8107 |          14.8697 |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |          -0.0083 |          33.2311 |          14.8798 |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |          -0.0070 |          33.1992 |          14.8720 |
[32m[20221213 22:44:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:44:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.74
[32m[20221213 22:44:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.47
[32m[20221213 22:44:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.52
[32m[20221213 22:44:04 @agent_ppo2.py:143][0m Total time:      25.85 min
[32m[20221213 22:44:04 @agent_ppo2.py:145][0m 2519040 total steps have happened
[32m[20221213 22:44:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1230 --------------------------#
[32m[20221213 22:44:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:44:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |           0.0050 |          42.0790 |          14.9461 |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |          -0.0042 |          38.3827 |          14.9292 |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |          -0.0014 |          37.8716 |          14.9138 |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |          -0.0089 |          36.0557 |          14.9144 |
[32m[20221213 22:44:04 @agent_ppo2.py:185][0m |          -0.0090 |          35.3368 |          14.9187 |
[32m[20221213 22:44:05 @agent_ppo2.py:185][0m |          -0.0078 |          34.3527 |          14.9277 |
[32m[20221213 22:44:05 @agent_ppo2.py:185][0m |           0.0004 |          36.6888 |          14.9242 |
[32m[20221213 22:44:05 @agent_ppo2.py:185][0m |          -0.0098 |          33.2982 |          14.9166 |
[32m[20221213 22:44:05 @agent_ppo2.py:185][0m |          -0.0093 |          32.6947 |          14.9162 |
[32m[20221213 22:44:05 @agent_ppo2.py:185][0m |          -0.0113 |          32.3889 |          14.9243 |
[32m[20221213 22:44:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.22
[32m[20221213 22:44:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.89
[32m[20221213 22:44:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.01
[32m[20221213 22:44:05 @agent_ppo2.py:143][0m Total time:      25.87 min
[32m[20221213 22:44:05 @agent_ppo2.py:145][0m 2521088 total steps have happened
[32m[20221213 22:44:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1231 --------------------------#
[32m[20221213 22:44:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:05 @agent_ppo2.py:185][0m |          -0.0011 |          28.8421 |          14.9834 |
[32m[20221213 22:44:05 @agent_ppo2.py:185][0m |          -0.0028 |          26.1188 |          14.9688 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |          -0.0037 |          25.6069 |          14.9628 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |          -0.0107 |          25.3166 |          14.9578 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |           0.0032 |          27.4485 |          14.9578 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |          -0.0089 |          25.0691 |          14.9447 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |          -0.0174 |          24.8797 |          14.9516 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |           0.0042 |          26.7325 |          14.9447 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |          -0.0069 |          24.7774 |          14.9592 |
[32m[20221213 22:44:06 @agent_ppo2.py:185][0m |          -0.0075 |          24.8205 |          14.9556 |
[32m[20221213 22:44:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.82
[32m[20221213 22:44:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.76
[32m[20221213 22:44:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.49
[32m[20221213 22:44:06 @agent_ppo2.py:143][0m Total time:      25.89 min
[32m[20221213 22:44:06 @agent_ppo2.py:145][0m 2523136 total steps have happened
[32m[20221213 22:44:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1232 --------------------------#
[32m[20221213 22:44:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0023 |          36.4600 |          15.0158 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0080 |          35.1230 |          14.9914 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0087 |          34.6744 |          14.9814 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0109 |          34.4710 |          14.9896 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0096 |          34.1689 |          14.9905 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0119 |          34.0292 |          14.9794 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0097 |          33.8940 |          14.9812 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |           0.0003 |          35.2581 |          14.9832 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0084 |          33.8223 |          14.9750 |
[32m[20221213 22:44:07 @agent_ppo2.py:185][0m |          -0.0097 |          33.6829 |          14.9748 |
[32m[20221213 22:44:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.45
[32m[20221213 22:44:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.83
[32m[20221213 22:44:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.65
[32m[20221213 22:44:07 @agent_ppo2.py:143][0m Total time:      25.91 min
[32m[20221213 22:44:07 @agent_ppo2.py:145][0m 2525184 total steps have happened
[32m[20221213 22:44:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1233 --------------------------#
[32m[20221213 22:44:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |           0.0014 |          33.2721 |          15.0240 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0057 |          32.5576 |          14.9982 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0065 |          32.4172 |          15.0054 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0067 |          32.1528 |          15.0089 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0106 |          32.0582 |          15.0025 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0074 |          31.9886 |          15.0074 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0070 |          31.7653 |          14.9786 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0101 |          31.7341 |          15.0005 |
[32m[20221213 22:44:08 @agent_ppo2.py:185][0m |          -0.0045 |          32.4325 |          14.9948 |
[32m[20221213 22:44:09 @agent_ppo2.py:185][0m |          -0.0083 |          31.6355 |          14.9900 |
[32m[20221213 22:44:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.97
[32m[20221213 22:44:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.67
[32m[20221213 22:44:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.69
[32m[20221213 22:44:09 @agent_ppo2.py:143][0m Total time:      25.93 min
[32m[20221213 22:44:09 @agent_ppo2.py:145][0m 2527232 total steps have happened
[32m[20221213 22:44:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1234 --------------------------#
[32m[20221213 22:44:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:09 @agent_ppo2.py:185][0m |           0.0009 |          41.6044 |          15.0700 |
[32m[20221213 22:44:09 @agent_ppo2.py:185][0m |           0.0093 |          42.7030 |          15.0652 |
[32m[20221213 22:44:09 @agent_ppo2.py:185][0m |          -0.0047 |          39.8301 |          15.0617 |
[32m[20221213 22:44:09 @agent_ppo2.py:185][0m |          -0.0082 |          39.0127 |          15.0623 |
[32m[20221213 22:44:09 @agent_ppo2.py:185][0m |          -0.0079 |          39.0396 |          15.0664 |
[32m[20221213 22:44:09 @agent_ppo2.py:185][0m |          -0.0099 |          38.7443 |          15.0658 |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0098 |          38.6265 |          15.0517 |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0021 |          40.7084 |          15.0607 |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0106 |          38.3558 |          15.0397 |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0138 |          38.2114 |          15.0491 |
[32m[20221213 22:44:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:44:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.89
[32m[20221213 22:44:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.23
[32m[20221213 22:44:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.08
[32m[20221213 22:44:10 @agent_ppo2.py:143][0m Total time:      25.95 min
[32m[20221213 22:44:10 @agent_ppo2.py:145][0m 2529280 total steps have happened
[32m[20221213 22:44:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1235 --------------------------#
[32m[20221213 22:44:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0022 |          34.1887 |          15.1880 |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0057 |          32.5017 |          15.1748 |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0068 |          32.0180 |          15.1661 |
[32m[20221213 22:44:10 @agent_ppo2.py:185][0m |          -0.0086 |          31.8252 |          15.1695 |
[32m[20221213 22:44:11 @agent_ppo2.py:185][0m |          -0.0041 |          31.9539 |          15.1615 |
[32m[20221213 22:44:11 @agent_ppo2.py:185][0m |          -0.0093 |          31.6715 |          15.1528 |
[32m[20221213 22:44:11 @agent_ppo2.py:185][0m |          -0.0095 |          31.2670 |          15.1596 |
[32m[20221213 22:44:11 @agent_ppo2.py:185][0m |          -0.0086 |          31.2076 |          15.1594 |
[32m[20221213 22:44:11 @agent_ppo2.py:185][0m |          -0.0172 |          31.1474 |          15.1581 |
[32m[20221213 22:44:11 @agent_ppo2.py:185][0m |          -0.0135 |          31.7456 |          15.1617 |
[32m[20221213 22:44:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.05
[32m[20221213 22:44:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.49
[32m[20221213 22:44:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.24
[32m[20221213 22:44:11 @agent_ppo2.py:143][0m Total time:      25.97 min
[32m[20221213 22:44:11 @agent_ppo2.py:145][0m 2531328 total steps have happened
[32m[20221213 22:44:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1236 --------------------------#
[32m[20221213 22:44:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:11 @agent_ppo2.py:185][0m |           0.0094 |          42.0084 |          14.8982 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0046 |          35.9920 |          14.8593 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0057 |          35.2054 |          14.8644 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0117 |          34.8138 |          14.8480 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0092 |          34.4333 |          14.8700 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0073 |          34.8150 |          14.8634 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0111 |          34.2671 |          14.8515 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0088 |          34.4631 |          14.8587 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0140 |          33.9764 |          14.8494 |
[32m[20221213 22:44:12 @agent_ppo2.py:185][0m |          -0.0059 |          34.9881 |          14.8447 |
[32m[20221213 22:44:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.11
[32m[20221213 22:44:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.92
[32m[20221213 22:44:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.84
[32m[20221213 22:44:12 @agent_ppo2.py:143][0m Total time:      25.99 min
[32m[20221213 22:44:12 @agent_ppo2.py:145][0m 2533376 total steps have happened
[32m[20221213 22:44:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1237 --------------------------#
[32m[20221213 22:44:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0031 |          35.8940 |          14.9167 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0092 |          32.6199 |          14.8840 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0023 |          32.0992 |          14.8979 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0088 |          31.0761 |          14.8835 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0096 |          30.6077 |          14.8840 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0120 |          30.1940 |          14.8770 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0144 |          29.9021 |          14.8718 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0141 |          29.6880 |          14.8717 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0174 |          29.4011 |          14.8693 |
[32m[20221213 22:44:13 @agent_ppo2.py:185][0m |          -0.0150 |          29.1811 |          14.8715 |
[32m[20221213 22:44:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.67
[32m[20221213 22:44:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.02
[32m[20221213 22:44:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.69
[32m[20221213 22:44:14 @agent_ppo2.py:143][0m Total time:      26.01 min
[32m[20221213 22:44:14 @agent_ppo2.py:145][0m 2535424 total steps have happened
[32m[20221213 22:44:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1238 --------------------------#
[32m[20221213 22:44:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0021 |          32.9250 |          14.8998 |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0056 |          30.7166 |          14.8787 |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0081 |          30.1803 |          14.8949 |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0079 |          29.8392 |          14.8828 |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0094 |          29.5880 |          14.8707 |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0090 |          29.3724 |          14.8671 |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0105 |          29.1657 |          14.8738 |
[32m[20221213 22:44:14 @agent_ppo2.py:185][0m |          -0.0101 |          29.1169 |          14.8691 |
[32m[20221213 22:44:15 @agent_ppo2.py:185][0m |          -0.0193 |          29.0577 |          14.8735 |
[32m[20221213 22:44:15 @agent_ppo2.py:185][0m |          -0.0140 |          28.8135 |          14.8717 |
[32m[20221213 22:44:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:44:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.31
[32m[20221213 22:44:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.27
[32m[20221213 22:44:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.02
[32m[20221213 22:44:15 @agent_ppo2.py:143][0m Total time:      26.03 min
[32m[20221213 22:44:15 @agent_ppo2.py:145][0m 2537472 total steps have happened
[32m[20221213 22:44:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1239 --------------------------#
[32m[20221213 22:44:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:15 @agent_ppo2.py:185][0m |          -0.0001 |          34.9203 |          14.9903 |
[32m[20221213 22:44:15 @agent_ppo2.py:185][0m |          -0.0080 |          31.9265 |          14.9750 |
[32m[20221213 22:44:15 @agent_ppo2.py:185][0m |          -0.0090 |          31.2688 |          14.9715 |
[32m[20221213 22:44:15 @agent_ppo2.py:185][0m |          -0.0128 |          30.8275 |          14.9706 |
[32m[20221213 22:44:15 @agent_ppo2.py:185][0m |          -0.0117 |          30.4309 |          14.9746 |
[32m[20221213 22:44:16 @agent_ppo2.py:185][0m |          -0.0043 |          31.9585 |          14.9699 |
[32m[20221213 22:44:16 @agent_ppo2.py:185][0m |          -0.0145 |          29.9721 |          14.9664 |
[32m[20221213 22:44:16 @agent_ppo2.py:185][0m |          -0.0160 |          29.8480 |          14.9690 |
[32m[20221213 22:44:16 @agent_ppo2.py:185][0m |          -0.0176 |          29.6729 |          14.9651 |
[32m[20221213 22:44:16 @agent_ppo2.py:185][0m |          -0.0125 |          29.3573 |          14.9634 |
[32m[20221213 22:44:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:44:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.49
[32m[20221213 22:44:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.16
[32m[20221213 22:44:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.18
[32m[20221213 22:44:16 @agent_ppo2.py:143][0m Total time:      26.05 min
[32m[20221213 22:44:16 @agent_ppo2.py:145][0m 2539520 total steps have happened
[32m[20221213 22:44:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1240 --------------------------#
[32m[20221213 22:44:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:44:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:16 @agent_ppo2.py:185][0m |           0.0006 |          35.3927 |          15.0990 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0052 |          33.1679 |          15.0884 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0085 |          32.8661 |          15.0817 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |           0.0048 |          36.9906 |          15.0775 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0072 |          31.8953 |          15.0860 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0077 |          31.5899 |          15.0793 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0102 |          31.3999 |          15.0726 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0099 |          31.4895 |          15.0867 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0094 |          31.1438 |          15.0803 |
[32m[20221213 22:44:17 @agent_ppo2.py:185][0m |          -0.0115 |          31.0063 |          15.0860 |
[32m[20221213 22:44:17 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:44:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.34
[32m[20221213 22:44:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.41
[32m[20221213 22:44:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.93
[32m[20221213 22:44:17 @agent_ppo2.py:143][0m Total time:      26.07 min
[32m[20221213 22:44:17 @agent_ppo2.py:145][0m 2541568 total steps have happened
[32m[20221213 22:44:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1241 --------------------------#
[32m[20221213 22:44:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:44:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0028 |          35.3689 |          15.0315 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0033 |          32.4439 |          15.0186 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0047 |          31.1402 |          15.0166 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0105 |          30.5368 |          14.9944 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0118 |          30.0411 |          15.0031 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0106 |          29.5549 |          14.9946 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0107 |          29.3008 |          14.9996 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0111 |          28.8790 |          15.0006 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0141 |          28.6841 |          14.9929 |
[32m[20221213 22:44:18 @agent_ppo2.py:185][0m |          -0.0154 |          28.3793 |          14.9894 |
[32m[20221213 22:44:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:44:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.15
[32m[20221213 22:44:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.90
[32m[20221213 22:44:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 28.87
[32m[20221213 22:44:19 @agent_ppo2.py:143][0m Total time:      26.10 min
[32m[20221213 22:44:19 @agent_ppo2.py:145][0m 2543616 total steps have happened
[32m[20221213 22:44:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1242 --------------------------#
[32m[20221213 22:44:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:19 @agent_ppo2.py:185][0m |          -0.0025 |          37.5602 |          15.1308 |
[32m[20221213 22:44:19 @agent_ppo2.py:185][0m |          -0.0056 |          35.0475 |          15.1285 |
[32m[20221213 22:44:19 @agent_ppo2.py:185][0m |          -0.0088 |          34.2155 |          15.1172 |
[32m[20221213 22:44:19 @agent_ppo2.py:185][0m |          -0.0084 |          33.7010 |          15.1172 |
[32m[20221213 22:44:19 @agent_ppo2.py:185][0m |          -0.0118 |          33.3311 |          15.1200 |
[32m[20221213 22:44:19 @agent_ppo2.py:185][0m |          -0.0124 |          33.2712 |          15.1224 |
[32m[20221213 22:44:19 @agent_ppo2.py:185][0m |          -0.0113 |          32.8043 |          15.1230 |
[32m[20221213 22:44:20 @agent_ppo2.py:185][0m |          -0.0088 |          32.8421 |          15.1170 |
[32m[20221213 22:44:20 @agent_ppo2.py:185][0m |          -0.0125 |          32.4891 |          15.1181 |
[32m[20221213 22:44:20 @agent_ppo2.py:185][0m |          -0.0132 |          32.2551 |          15.1249 |
[32m[20221213 22:44:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:44:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.13
[32m[20221213 22:44:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.56
[32m[20221213 22:44:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 122.97
[32m[20221213 22:44:20 @agent_ppo2.py:143][0m Total time:      26.12 min
[32m[20221213 22:44:20 @agent_ppo2.py:145][0m 2545664 total steps have happened
[32m[20221213 22:44:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1243 --------------------------#
[32m[20221213 22:44:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:20 @agent_ppo2.py:185][0m |          -0.0017 |          31.0733 |          15.1103 |
[32m[20221213 22:44:20 @agent_ppo2.py:185][0m |          -0.0072 |          28.6914 |          15.1004 |
[32m[20221213 22:44:20 @agent_ppo2.py:185][0m |          -0.0102 |          27.8791 |          15.0843 |
[32m[20221213 22:44:20 @agent_ppo2.py:185][0m |          -0.0081 |          27.4264 |          15.0881 |
[32m[20221213 22:44:21 @agent_ppo2.py:185][0m |           0.0215 |          32.2054 |          15.0836 |
[32m[20221213 22:44:21 @agent_ppo2.py:185][0m |          -0.0099 |          26.9493 |          15.0672 |
[32m[20221213 22:44:21 @agent_ppo2.py:185][0m |          -0.0133 |          26.5190 |          15.0752 |
[32m[20221213 22:44:21 @agent_ppo2.py:185][0m |          -0.0132 |          26.3825 |          15.0740 |
[32m[20221213 22:44:21 @agent_ppo2.py:185][0m |          -0.0146 |          26.2194 |          15.0848 |
[32m[20221213 22:44:21 @agent_ppo2.py:185][0m |          -0.0124 |          26.0504 |          15.0719 |
[32m[20221213 22:44:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.70
[32m[20221213 22:44:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.41
[32m[20221213 22:44:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.48
[32m[20221213 22:44:21 @agent_ppo2.py:143][0m Total time:      26.14 min
[32m[20221213 22:44:21 @agent_ppo2.py:145][0m 2547712 total steps have happened
[32m[20221213 22:44:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1244 --------------------------#
[32m[20221213 22:44:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:21 @agent_ppo2.py:185][0m |          -0.0041 |          34.4582 |          15.0302 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0048 |          31.5481 |          15.0319 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0115 |          30.4733 |          15.0256 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0072 |          29.7729 |          15.0266 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0120 |          29.2069 |          15.0174 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0106 |          29.0539 |          15.0174 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0075 |          30.4851 |          15.0045 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0110 |          28.5549 |          15.0114 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0088 |          28.4175 |          15.0075 |
[32m[20221213 22:44:22 @agent_ppo2.py:185][0m |          -0.0086 |          28.2505 |          15.0003 |
[32m[20221213 22:44:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:44:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.41
[32m[20221213 22:44:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.28
[32m[20221213 22:44:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.41
[32m[20221213 22:44:22 @agent_ppo2.py:143][0m Total time:      26.16 min
[32m[20221213 22:44:22 @agent_ppo2.py:145][0m 2549760 total steps have happened
[32m[20221213 22:44:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1245 --------------------------#
[32m[20221213 22:44:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |           0.0079 |          31.3494 |          15.0054 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0020 |          25.6195 |          14.9861 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0077 |          24.6792 |          14.9921 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0063 |          23.8447 |          14.9935 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0065 |          23.5175 |          14.9981 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0063 |          23.4336 |          14.9851 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0152 |          22.9106 |          14.9851 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0164 |          22.4957 |          14.9766 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0120 |          22.4544 |          14.9799 |
[32m[20221213 22:44:23 @agent_ppo2.py:185][0m |          -0.0131 |          22.1701 |          14.9802 |
[32m[20221213 22:44:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.55
[32m[20221213 22:44:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.33
[32m[20221213 22:44:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.72
[32m[20221213 22:44:24 @agent_ppo2.py:143][0m Total time:      26.18 min
[32m[20221213 22:44:24 @agent_ppo2.py:145][0m 2551808 total steps have happened
[32m[20221213 22:44:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1246 --------------------------#
[32m[20221213 22:44:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0020 |          20.1008 |          15.1840 |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0019 |          17.5044 |          15.1782 |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0087 |          16.4123 |          15.1651 |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0108 |          15.6290 |          15.1477 |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0090 |          15.1495 |          15.1585 |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0021 |          16.2583 |          15.1484 |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0122 |          14.5704 |          15.1528 |
[32m[20221213 22:44:24 @agent_ppo2.py:185][0m |          -0.0096 |          14.1507 |          15.1462 |
[32m[20221213 22:44:25 @agent_ppo2.py:185][0m |          -0.0150 |          13.9150 |          15.1539 |
[32m[20221213 22:44:25 @agent_ppo2.py:185][0m |          -0.0156 |          13.6508 |          15.1412 |
[32m[20221213 22:44:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.48
[32m[20221213 22:44:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.08
[32m[20221213 22:44:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.77
[32m[20221213 22:44:25 @agent_ppo2.py:143][0m Total time:      26.20 min
[32m[20221213 22:44:25 @agent_ppo2.py:145][0m 2553856 total steps have happened
[32m[20221213 22:44:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1247 --------------------------#
[32m[20221213 22:44:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:25 @agent_ppo2.py:185][0m |           0.0016 |          31.7571 |          14.9898 |
[32m[20221213 22:44:25 @agent_ppo2.py:185][0m |          -0.0084 |          29.2067 |          14.9703 |
[32m[20221213 22:44:25 @agent_ppo2.py:185][0m |          -0.0039 |          28.1076 |          14.9718 |
[32m[20221213 22:44:25 @agent_ppo2.py:185][0m |          -0.0112 |          27.4057 |          14.9563 |
[32m[20221213 22:44:25 @agent_ppo2.py:185][0m |          -0.0132 |          27.0215 |          14.9632 |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |          -0.0115 |          26.7837 |          14.9612 |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |          -0.0159 |          26.5944 |          14.9602 |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |          -0.0106 |          26.3692 |          14.9556 |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |          -0.0154 |          26.0855 |          14.9511 |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |          -0.0111 |          26.0281 |          14.9552 |
[32m[20221213 22:44:26 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:44:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.99
[32m[20221213 22:44:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.63
[32m[20221213 22:44:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.36
[32m[20221213 22:44:26 @agent_ppo2.py:143][0m Total time:      26.22 min
[32m[20221213 22:44:26 @agent_ppo2.py:145][0m 2555904 total steps have happened
[32m[20221213 22:44:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1248 --------------------------#
[32m[20221213 22:44:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |           0.0018 |          27.8007 |          15.0889 |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |          -0.0033 |          26.0191 |          15.0559 |
[32m[20221213 22:44:26 @agent_ppo2.py:185][0m |          -0.0089 |          25.5022 |          15.0548 |
[32m[20221213 22:44:27 @agent_ppo2.py:185][0m |          -0.0075 |          25.0908 |          15.0616 |
[32m[20221213 22:44:27 @agent_ppo2.py:185][0m |          -0.0093 |          24.6907 |          15.0581 |
[32m[20221213 22:44:27 @agent_ppo2.py:185][0m |          -0.0077 |          24.5529 |          15.0496 |
[32m[20221213 22:44:27 @agent_ppo2.py:185][0m |          -0.0135 |          24.2924 |          15.0572 |
[32m[20221213 22:44:27 @agent_ppo2.py:185][0m |          -0.0061 |          24.2457 |          15.0506 |
[32m[20221213 22:44:27 @agent_ppo2.py:185][0m |          -0.0120 |          24.0476 |          15.0456 |
[32m[20221213 22:44:27 @agent_ppo2.py:185][0m |          -0.0086 |          23.9595 |          15.0434 |
[32m[20221213 22:44:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 168.33
[32m[20221213 22:44:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.05
[32m[20221213 22:44:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.24
[32m[20221213 22:44:27 @agent_ppo2.py:143][0m Total time:      26.24 min
[32m[20221213 22:44:27 @agent_ppo2.py:145][0m 2557952 total steps have happened
[32m[20221213 22:44:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1249 --------------------------#
[32m[20221213 22:44:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0007 |          23.9311 |          15.0432 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0043 |          22.5774 |          15.0051 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0052 |          22.2605 |          15.0182 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0099 |          21.9970 |          14.9998 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0113 |          21.9033 |          15.0047 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0096 |          21.6801 |          14.9975 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0111 |          21.5751 |          15.0067 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0109 |          21.5848 |          15.0005 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0119 |          21.4720 |          14.9905 |
[32m[20221213 22:44:28 @agent_ppo2.py:185][0m |          -0.0092 |          21.3274 |          14.9880 |
[32m[20221213 22:44:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:44:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.13
[32m[20221213 22:44:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.08
[32m[20221213 22:44:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.62
[32m[20221213 22:44:28 @agent_ppo2.py:143][0m Total time:      26.26 min
[32m[20221213 22:44:28 @agent_ppo2.py:145][0m 2560000 total steps have happened
[32m[20221213 22:44:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1250 --------------------------#
[32m[20221213 22:44:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:44:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0011 |          31.6948 |          15.2325 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0076 |          29.4989 |          15.2104 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0046 |          28.8265 |          15.2042 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0042 |          28.4802 |          15.1901 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0087 |          28.1708 |          15.1847 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0088 |          28.1708 |          15.1975 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0095 |          27.9545 |          15.1822 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0091 |          27.7905 |          15.1911 |
[32m[20221213 22:44:29 @agent_ppo2.py:185][0m |          -0.0094 |          27.6731 |          15.1860 |
[32m[20221213 22:44:30 @agent_ppo2.py:185][0m |          -0.0122 |          27.5687 |          15.1848 |
[32m[20221213 22:44:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:44:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.05
[32m[20221213 22:44:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.64
[32m[20221213 22:44:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.73
[32m[20221213 22:44:30 @agent_ppo2.py:143][0m Total time:      26.28 min
[32m[20221213 22:44:30 @agent_ppo2.py:145][0m 2562048 total steps have happened
[32m[20221213 22:44:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1251 --------------------------#
[32m[20221213 22:44:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:44:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:30 @agent_ppo2.py:185][0m |          -0.0016 |          24.6313 |          15.2235 |
[32m[20221213 22:44:30 @agent_ppo2.py:185][0m |          -0.0046 |          22.8325 |          15.2076 |
[32m[20221213 22:44:30 @agent_ppo2.py:185][0m |          -0.0066 |          22.0618 |          15.1973 |
[32m[20221213 22:44:30 @agent_ppo2.py:185][0m |          -0.0063 |          21.5874 |          15.2030 |
[32m[20221213 22:44:30 @agent_ppo2.py:185][0m |          -0.0071 |          21.1584 |          15.1964 |
[32m[20221213 22:44:31 @agent_ppo2.py:185][0m |          -0.0068 |          20.8588 |          15.1909 |
[32m[20221213 22:44:31 @agent_ppo2.py:185][0m |          -0.0105 |          20.7062 |          15.1931 |
[32m[20221213 22:44:31 @agent_ppo2.py:185][0m |          -0.0086 |          20.5196 |          15.1838 |
[32m[20221213 22:44:31 @agent_ppo2.py:185][0m |          -0.0033 |          20.7434 |          15.1782 |
[32m[20221213 22:44:31 @agent_ppo2.py:185][0m |          -0.0079 |          20.1798 |          15.1865 |
[32m[20221213 22:44:31 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:44:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.68
[32m[20221213 22:44:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.33
[32m[20221213 22:44:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.47
[32m[20221213 22:44:31 @agent_ppo2.py:143][0m Total time:      26.30 min
[32m[20221213 22:44:31 @agent_ppo2.py:145][0m 2564096 total steps have happened
[32m[20221213 22:44:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1252 --------------------------#
[32m[20221213 22:44:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:31 @agent_ppo2.py:185][0m |           0.0011 |          27.5266 |          15.2634 |
[32m[20221213 22:44:31 @agent_ppo2.py:185][0m |          -0.0051 |          25.3018 |          15.2315 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |          -0.0048 |          24.7418 |          15.2273 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |          -0.0069 |          24.5084 |          15.2193 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |          -0.0042 |          24.0487 |          15.2195 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |           0.0010 |          25.5169 |          15.2175 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |          -0.0102 |          23.8514 |          15.2165 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |          -0.0095 |          23.5939 |          15.2229 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |          -0.0090 |          23.5399 |          15.2204 |
[32m[20221213 22:44:32 @agent_ppo2.py:185][0m |          -0.0138 |          23.3382 |          15.2141 |
[32m[20221213 22:44:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:44:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.96
[32m[20221213 22:44:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.48
[32m[20221213 22:44:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 121.28
[32m[20221213 22:44:32 @agent_ppo2.py:143][0m Total time:      26.32 min
[32m[20221213 22:44:32 @agent_ppo2.py:145][0m 2566144 total steps have happened
[32m[20221213 22:44:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1253 --------------------------#
[32m[20221213 22:44:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:44:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0015 |          39.7237 |          15.3807 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |           0.0034 |          40.8878 |          15.3612 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0086 |          35.7626 |          15.3244 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0059 |          34.6298 |          15.3456 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0008 |          35.8960 |          15.3416 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0126 |          33.7841 |          15.3450 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0135 |          33.6324 |          15.3360 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0125 |          33.1333 |          15.3501 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0018 |          34.7296 |          15.3415 |
[32m[20221213 22:44:33 @agent_ppo2.py:185][0m |          -0.0050 |          33.8318 |          15.3430 |
[32m[20221213 22:44:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:44:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.86
[32m[20221213 22:44:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.96
[32m[20221213 22:44:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 205.67
[32m[20221213 22:44:34 @agent_ppo2.py:143][0m Total time:      26.35 min
[32m[20221213 22:44:34 @agent_ppo2.py:145][0m 2568192 total steps have happened
[32m[20221213 22:44:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1254 --------------------------#
[32m[20221213 22:44:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:34 @agent_ppo2.py:185][0m |          -0.0010 |          40.0377 |          15.0835 |
[32m[20221213 22:44:34 @agent_ppo2.py:185][0m |           0.0035 |          36.4424 |          15.0591 |
[32m[20221213 22:44:34 @agent_ppo2.py:185][0m |          -0.0116 |          35.3621 |          15.0497 |
[32m[20221213 22:44:34 @agent_ppo2.py:185][0m |          -0.0076 |          34.9235 |          15.0438 |
[32m[20221213 22:44:34 @agent_ppo2.py:185][0m |          -0.0092 |          34.4224 |          15.0362 |
[32m[20221213 22:44:34 @agent_ppo2.py:185][0m |          -0.0084 |          34.0149 |          15.0452 |
[32m[20221213 22:44:34 @agent_ppo2.py:185][0m |          -0.0128 |          33.6964 |          15.0382 |
[32m[20221213 22:44:35 @agent_ppo2.py:185][0m |          -0.0044 |          33.8623 |          15.0419 |
[32m[20221213 22:44:35 @agent_ppo2.py:185][0m |          -0.0120 |          33.1693 |          15.0337 |
[32m[20221213 22:44:35 @agent_ppo2.py:185][0m |          -0.0114 |          32.9961 |          15.0263 |
[32m[20221213 22:44:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:44:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.65
[32m[20221213 22:44:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.51
[32m[20221213 22:44:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.50
[32m[20221213 22:44:35 @agent_ppo2.py:143][0m Total time:      26.37 min
[32m[20221213 22:44:35 @agent_ppo2.py:145][0m 2570240 total steps have happened
[32m[20221213 22:44:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1255 --------------------------#
[32m[20221213 22:44:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:35 @agent_ppo2.py:185][0m |           0.0041 |          32.2172 |          15.0097 |
[32m[20221213 22:44:35 @agent_ppo2.py:185][0m |          -0.0051 |          29.9825 |          14.9920 |
[32m[20221213 22:44:35 @agent_ppo2.py:185][0m |           0.0145 |          32.7634 |          14.9816 |
[32m[20221213 22:44:35 @agent_ppo2.py:185][0m |           0.0012 |          30.5184 |          14.9793 |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0055 |          28.4641 |          14.9928 |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0111 |          28.0175 |          14.9889 |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0078 |          27.8374 |          14.9833 |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0024 |          28.3244 |          14.9852 |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0102 |          27.4906 |          14.9800 |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0093 |          27.1282 |          14.9939 |
[32m[20221213 22:44:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.49
[32m[20221213 22:44:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.95
[32m[20221213 22:44:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.37
[32m[20221213 22:44:36 @agent_ppo2.py:143][0m Total time:      26.39 min
[32m[20221213 22:44:36 @agent_ppo2.py:145][0m 2572288 total steps have happened
[32m[20221213 22:44:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1256 --------------------------#
[32m[20221213 22:44:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0001 |          27.4300 |          15.0871 |
[32m[20221213 22:44:36 @agent_ppo2.py:185][0m |          -0.0114 |          24.0186 |          15.0917 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0033 |          22.8568 |          15.0738 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0088 |          22.1594 |          15.0675 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0159 |          21.6469 |          15.0672 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0098 |          21.4006 |          15.0651 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0151 |          20.8742 |          15.0582 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0124 |          20.7150 |          15.0701 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0246 |          20.5551 |          15.0750 |
[32m[20221213 22:44:37 @agent_ppo2.py:185][0m |          -0.0134 |          20.2523 |          15.0625 |
[32m[20221213 22:44:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:44:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.95
[32m[20221213 22:44:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.79
[32m[20221213 22:44:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.85
[32m[20221213 22:44:37 @agent_ppo2.py:143][0m Total time:      26.41 min
[32m[20221213 22:44:37 @agent_ppo2.py:145][0m 2574336 total steps have happened
[32m[20221213 22:44:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1257 --------------------------#
[32m[20221213 22:44:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:44:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0007 |          28.5091 |          15.2447 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0054 |          25.7677 |          15.2200 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0065 |          24.6300 |          15.2117 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0115 |          23.7366 |          15.2140 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0085 |          23.5044 |          15.2081 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0144 |          22.7959 |          15.2056 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0082 |          22.3503 |          15.2011 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0083 |          22.4256 |          15.2110 |
[32m[20221213 22:44:38 @agent_ppo2.py:185][0m |          -0.0096 |          21.8373 |          15.2010 |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |           0.0060 |          24.5180 |          15.2012 |
[32m[20221213 22:44:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:44:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 193.95
[32m[20221213 22:44:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 333.91
[32m[20221213 22:44:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.87
[32m[20221213 22:44:39 @agent_ppo2.py:143][0m Total time:      26.43 min
[32m[20221213 22:44:39 @agent_ppo2.py:145][0m 2576384 total steps have happened
[32m[20221213 22:44:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1258 --------------------------#
[32m[20221213 22:44:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |          -0.0017 |          31.9324 |          15.1888 |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |          -0.0040 |          29.1472 |          15.1673 |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |          -0.0102 |          27.9814 |          15.1593 |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |          -0.0094 |          27.1671 |          15.1607 |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |          -0.0108 |          26.5901 |          15.1613 |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |          -0.0136 |          26.0256 |          15.1476 |
[32m[20221213 22:44:39 @agent_ppo2.py:185][0m |          -0.0022 |          28.2462 |          15.1504 |
[32m[20221213 22:44:40 @agent_ppo2.py:185][0m |          -0.0093 |          25.4492 |          15.1530 |
[32m[20221213 22:44:40 @agent_ppo2.py:185][0m |          -0.0142 |          24.9689 |          15.1531 |
[32m[20221213 22:44:40 @agent_ppo2.py:185][0m |          -0.0183 |          24.8482 |          15.1530 |
[32m[20221213 22:44:40 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:44:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.06
[32m[20221213 22:44:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.28
[32m[20221213 22:44:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.62
[32m[20221213 22:44:40 @agent_ppo2.py:143][0m Total time:      26.45 min
[32m[20221213 22:44:40 @agent_ppo2.py:145][0m 2578432 total steps have happened
[32m[20221213 22:44:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1259 --------------------------#
[32m[20221213 22:44:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:40 @agent_ppo2.py:185][0m |           0.0069 |          30.3548 |          15.2885 |
[32m[20221213 22:44:40 @agent_ppo2.py:185][0m |          -0.0096 |          27.4050 |          15.2675 |
[32m[20221213 22:44:40 @agent_ppo2.py:185][0m |          -0.0100 |          26.1025 |          15.2585 |
[32m[20221213 22:44:40 @agent_ppo2.py:185][0m |          -0.0145 |          25.4495 |          15.2624 |
[32m[20221213 22:44:41 @agent_ppo2.py:185][0m |          -0.0109 |          24.7759 |          15.2564 |
[32m[20221213 22:44:41 @agent_ppo2.py:185][0m |          -0.0162 |          24.3484 |          15.2545 |
[32m[20221213 22:44:41 @agent_ppo2.py:185][0m |          -0.0071 |          24.2681 |          15.2630 |
[32m[20221213 22:44:41 @agent_ppo2.py:185][0m |          -0.0120 |          24.0669 |          15.2466 |
[32m[20221213 22:44:41 @agent_ppo2.py:185][0m |          -0.0159 |          23.4305 |          15.2547 |
[32m[20221213 22:44:41 @agent_ppo2.py:185][0m |          -0.0143 |          23.3149 |          15.2494 |
[32m[20221213 22:44:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.88
[32m[20221213 22:44:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.63
[32m[20221213 22:44:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.09
[32m[20221213 22:44:41 @agent_ppo2.py:143][0m Total time:      26.47 min
[32m[20221213 22:44:41 @agent_ppo2.py:145][0m 2580480 total steps have happened
[32m[20221213 22:44:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1260 --------------------------#
[32m[20221213 22:44:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:44:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:41 @agent_ppo2.py:185][0m |           0.0027 |          31.2418 |          15.0954 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0066 |          27.9313 |          15.0984 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0070 |          26.5123 |          15.0794 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0012 |          27.2592 |          15.0767 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0067 |          25.2340 |          15.0757 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0125 |          24.8188 |          15.0767 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0085 |          24.9066 |          15.0775 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0142 |          24.1371 |          15.0843 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0069 |          24.2745 |          15.0699 |
[32m[20221213 22:44:42 @agent_ppo2.py:185][0m |          -0.0175 |          23.7447 |          15.0702 |
[32m[20221213 22:44:42 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.75
[32m[20221213 22:44:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.38
[32m[20221213 22:44:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.58
[32m[20221213 22:44:42 @agent_ppo2.py:143][0m Total time:      26.49 min
[32m[20221213 22:44:42 @agent_ppo2.py:145][0m 2582528 total steps have happened
[32m[20221213 22:44:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1261 --------------------------#
[32m[20221213 22:44:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |           0.0029 |          19.6109 |          15.1058 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0018 |          16.9559 |          15.0997 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0088 |          16.0758 |          15.1006 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0050 |          15.6033 |          15.0842 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0110 |          15.1593 |          15.0946 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0144 |          15.0620 |          15.0915 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0093 |          14.8541 |          15.0835 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0127 |          14.4944 |          15.0893 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0111 |          14.2762 |          15.0875 |
[32m[20221213 22:44:43 @agent_ppo2.py:185][0m |          -0.0159 |          14.1754 |          15.0940 |
[32m[20221213 22:44:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:44:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.65
[32m[20221213 22:44:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 360.85
[32m[20221213 22:44:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.53
[32m[20221213 22:44:44 @agent_ppo2.py:143][0m Total time:      26.51 min
[32m[20221213 22:44:44 @agent_ppo2.py:145][0m 2584576 total steps have happened
[32m[20221213 22:44:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1262 --------------------------#
[32m[20221213 22:44:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |           0.0026 |          24.0889 |          15.1084 |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |           0.0017 |          21.0878 |          15.1007 |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |          -0.0063 |          20.0126 |          15.0928 |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |          -0.0057 |          19.3453 |          15.0859 |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |          -0.0063 |          18.6772 |          15.0894 |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |          -0.0097 |          18.4062 |          15.0887 |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |          -0.0081 |          18.0065 |          15.0889 |
[32m[20221213 22:44:44 @agent_ppo2.py:185][0m |          -0.0122 |          17.6277 |          15.0845 |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0109 |          17.4720 |          15.0801 |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0131 |          17.2333 |          15.0847 |
[32m[20221213 22:44:45 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.21
[32m[20221213 22:44:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.81
[32m[20221213 22:44:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.09
[32m[20221213 22:44:45 @agent_ppo2.py:143][0m Total time:      26.53 min
[32m[20221213 22:44:45 @agent_ppo2.py:145][0m 2586624 total steps have happened
[32m[20221213 22:44:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1263 --------------------------#
[32m[20221213 22:44:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0006 |          33.1628 |          15.3367 |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0026 |          31.6991 |          15.3316 |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0073 |          31.2085 |          15.3333 |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0042 |          31.1221 |          15.3272 |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0050 |          30.7479 |          15.3163 |
[32m[20221213 22:44:45 @agent_ppo2.py:185][0m |          -0.0094 |          30.7269 |          15.3197 |
[32m[20221213 22:44:46 @agent_ppo2.py:185][0m |          -0.0077 |          30.5513 |          15.3194 |
[32m[20221213 22:44:46 @agent_ppo2.py:185][0m |          -0.0093 |          30.4095 |          15.3198 |
[32m[20221213 22:44:46 @agent_ppo2.py:185][0m |          -0.0060 |          30.5860 |          15.3119 |
[32m[20221213 22:44:46 @agent_ppo2.py:185][0m |          -0.0100 |          30.3785 |          15.3106 |
[32m[20221213 22:44:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:44:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.13
[32m[20221213 22:44:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.70
[32m[20221213 22:44:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.63
[32m[20221213 22:44:46 @agent_ppo2.py:143][0m Total time:      26.55 min
[32m[20221213 22:44:46 @agent_ppo2.py:145][0m 2588672 total steps have happened
[32m[20221213 22:44:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1264 --------------------------#
[32m[20221213 22:44:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:46 @agent_ppo2.py:185][0m |          -0.0048 |          38.1817 |          15.2367 |
[32m[20221213 22:44:46 @agent_ppo2.py:185][0m |          -0.0030 |          35.9270 |          15.2205 |
[32m[20221213 22:44:46 @agent_ppo2.py:185][0m |          -0.0035 |          35.0635 |          15.2165 |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |          -0.0043 |          34.6002 |          15.2172 |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |           0.0034 |          37.1488 |          15.2320 |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |          -0.0087 |          33.9886 |          15.2256 |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |          -0.0099 |          33.6568 |          15.2190 |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |          -0.0070 |          33.6217 |          15.2298 |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |          -0.0138 |          33.3007 |          15.2277 |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |          -0.0089 |          33.1006 |          15.2344 |
[32m[20221213 22:44:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.24
[32m[20221213 22:44:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.58
[32m[20221213 22:44:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.44
[32m[20221213 22:44:47 @agent_ppo2.py:143][0m Total time:      26.57 min
[32m[20221213 22:44:47 @agent_ppo2.py:145][0m 2590720 total steps have happened
[32m[20221213 22:44:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1265 --------------------------#
[32m[20221213 22:44:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:47 @agent_ppo2.py:185][0m |           0.0025 |          29.1604 |          15.2175 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |           0.0047 |          25.3783 |          15.2064 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |           0.0082 |          27.6562 |          15.2199 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |          -0.0067 |          24.2568 |          15.1962 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |          -0.0012 |          24.0301 |          15.1987 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |           0.0131 |          28.8167 |          15.1960 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |          -0.0026 |          23.6154 |          15.1742 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |          -0.0058 |          23.2632 |          15.1909 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |          -0.0128 |          23.2367 |          15.1985 |
[32m[20221213 22:44:48 @agent_ppo2.py:185][0m |          -0.0089 |          23.1869 |          15.1977 |
[32m[20221213 22:44:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:44:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.18
[32m[20221213 22:44:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.63
[32m[20221213 22:44:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.24
[32m[20221213 22:44:48 @agent_ppo2.py:143][0m Total time:      26.59 min
[32m[20221213 22:44:48 @agent_ppo2.py:145][0m 2592768 total steps have happened
[32m[20221213 22:44:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1266 --------------------------#
[32m[20221213 22:44:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0013 |          31.1753 |          15.1788 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0043 |          29.6450 |          15.1639 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0030 |          29.1202 |          15.1634 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0021 |          29.4367 |          15.1591 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0116 |          28.5379 |          15.1602 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0002 |          31.0342 |          15.1618 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0111 |          28.3958 |          15.1533 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0127 |          27.9679 |          15.1557 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |           0.0060 |          31.0133 |          15.1526 |
[32m[20221213 22:44:49 @agent_ppo2.py:185][0m |          -0.0118 |          28.0056 |          15.1510 |
[32m[20221213 22:44:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.86
[32m[20221213 22:44:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.41
[32m[20221213 22:44:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.62
[32m[20221213 22:44:50 @agent_ppo2.py:143][0m Total time:      26.61 min
[32m[20221213 22:44:50 @agent_ppo2.py:145][0m 2594816 total steps have happened
[32m[20221213 22:44:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1267 --------------------------#
[32m[20221213 22:44:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |          -0.0035 |          35.1335 |          15.3235 |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |          -0.0048 |          31.9795 |          15.3097 |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |           0.0017 |          31.5220 |          15.3179 |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |          -0.0048 |          30.5377 |          15.2993 |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |          -0.0130 |          30.4384 |          15.3043 |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |          -0.0151 |          29.9757 |          15.2963 |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |          -0.0102 |          29.8668 |          15.2988 |
[32m[20221213 22:44:50 @agent_ppo2.py:185][0m |          -0.0107 |          29.2265 |          15.3105 |
[32m[20221213 22:44:51 @agent_ppo2.py:185][0m |          -0.0151 |          28.9322 |          15.3035 |
[32m[20221213 22:44:51 @agent_ppo2.py:185][0m |          -0.0137 |          28.8553 |          15.3001 |
[32m[20221213 22:44:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:44:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.64
[32m[20221213 22:44:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.80
[32m[20221213 22:44:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.94
[32m[20221213 22:44:51 @agent_ppo2.py:143][0m Total time:      26.63 min
[32m[20221213 22:44:51 @agent_ppo2.py:145][0m 2596864 total steps have happened
[32m[20221213 22:44:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1268 --------------------------#
[32m[20221213 22:44:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:51 @agent_ppo2.py:185][0m |          -0.0036 |          34.2822 |          15.0720 |
[32m[20221213 22:44:51 @agent_ppo2.py:185][0m |          -0.0067 |          31.2967 |          15.0591 |
[32m[20221213 22:44:51 @agent_ppo2.py:185][0m |          -0.0080 |          30.5007 |          15.0436 |
[32m[20221213 22:44:51 @agent_ppo2.py:185][0m |          -0.0054 |          30.0191 |          15.0519 |
[32m[20221213 22:44:51 @agent_ppo2.py:185][0m |          -0.0108 |          29.7959 |          15.0413 |
[32m[20221213 22:44:52 @agent_ppo2.py:185][0m |          -0.0096 |          29.5368 |          15.0400 |
[32m[20221213 22:44:52 @agent_ppo2.py:185][0m |          -0.0106 |          29.2475 |          15.0434 |
[32m[20221213 22:44:52 @agent_ppo2.py:185][0m |          -0.0154 |          29.1214 |          15.0337 |
[32m[20221213 22:44:52 @agent_ppo2.py:185][0m |          -0.0133 |          29.0110 |          15.0394 |
[32m[20221213 22:44:52 @agent_ppo2.py:185][0m |          -0.0110 |          28.8615 |          15.0480 |
[32m[20221213 22:44:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:44:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.83
[32m[20221213 22:44:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 305.45
[32m[20221213 22:44:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.12
[32m[20221213 22:44:52 @agent_ppo2.py:143][0m Total time:      26.65 min
[32m[20221213 22:44:52 @agent_ppo2.py:145][0m 2598912 total steps have happened
[32m[20221213 22:44:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1269 --------------------------#
[32m[20221213 22:44:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:52 @agent_ppo2.py:185][0m |          -0.0028 |          22.4829 |          15.1405 |
[32m[20221213 22:44:52 @agent_ppo2.py:185][0m |          -0.0073 |          20.7625 |          15.1140 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0040 |          20.2310 |          15.1050 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0095 |          19.8688 |          15.0845 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0076 |          19.4490 |          15.1075 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0081 |          19.1097 |          15.0885 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0040 |          19.4039 |          15.1005 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0078 |          18.8027 |          15.0955 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0165 |          18.5526 |          15.0902 |
[32m[20221213 22:44:53 @agent_ppo2.py:185][0m |          -0.0115 |          18.4967 |          15.0968 |
[32m[20221213 22:44:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 177.19
[32m[20221213 22:44:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 322.55
[32m[20221213 22:44:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.32
[32m[20221213 22:44:53 @agent_ppo2.py:143][0m Total time:      26.67 min
[32m[20221213 22:44:53 @agent_ppo2.py:145][0m 2600960 total steps have happened
[32m[20221213 22:44:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1270 --------------------------#
[32m[20221213 22:44:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:44:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0017 |          38.6423 |          15.1725 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0046 |          36.4181 |          15.1730 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0071 |          35.1787 |          15.1672 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0067 |          34.1960 |          15.1629 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0081 |          33.7038 |          15.1609 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0048 |          35.2749 |          15.1634 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |           0.0063 |          35.2459 |          15.1500 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0094 |          33.0390 |          15.1535 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0102 |          32.3757 |          15.1545 |
[32m[20221213 22:44:54 @agent_ppo2.py:185][0m |          -0.0121 |          31.9674 |          15.1630 |
[32m[20221213 22:44:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.82
[32m[20221213 22:44:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.53
[32m[20221213 22:44:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.67
[32m[20221213 22:44:54 @agent_ppo2.py:143][0m Total time:      26.69 min
[32m[20221213 22:44:54 @agent_ppo2.py:145][0m 2603008 total steps have happened
[32m[20221213 22:44:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1271 --------------------------#
[32m[20221213 22:44:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0018 |          35.0246 |          15.0915 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0055 |          33.0234 |          15.0497 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0063 |          31.9685 |          15.0662 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0044 |          31.2113 |          15.0613 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0093 |          30.7096 |          15.0518 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0114 |          30.2726 |          15.0629 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0112 |          29.9130 |          15.0577 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0125 |          29.7537 |          15.0573 |
[32m[20221213 22:44:55 @agent_ppo2.py:185][0m |          -0.0111 |          29.4966 |          15.0618 |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |          -0.0137 |          29.3593 |          15.0613 |
[32m[20221213 22:44:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.90
[32m[20221213 22:44:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.06
[32m[20221213 22:44:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.05
[32m[20221213 22:44:56 @agent_ppo2.py:143][0m Total time:      26.71 min
[32m[20221213 22:44:56 @agent_ppo2.py:145][0m 2605056 total steps have happened
[32m[20221213 22:44:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1272 --------------------------#
[32m[20221213 22:44:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |          -0.0026 |          21.7809 |          15.2451 |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |           0.0019 |          19.3829 |          15.2237 |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |          -0.0035 |          18.2718 |          15.2142 |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |          -0.0116 |          17.3924 |          15.1936 |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |          -0.0115 |          16.8279 |          15.2203 |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |          -0.0106 |          16.5342 |          15.2122 |
[32m[20221213 22:44:56 @agent_ppo2.py:185][0m |          -0.0117 |          16.1642 |          15.2180 |
[32m[20221213 22:44:57 @agent_ppo2.py:185][0m |          -0.0132 |          15.9746 |          15.2130 |
[32m[20221213 22:44:57 @agent_ppo2.py:185][0m |          -0.0125 |          15.7986 |          15.2113 |
[32m[20221213 22:44:57 @agent_ppo2.py:185][0m |          -0.0150 |          15.6585 |          15.2089 |
[32m[20221213 22:44:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.50
[32m[20221213 22:44:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.97
[32m[20221213 22:44:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.76
[32m[20221213 22:44:57 @agent_ppo2.py:143][0m Total time:      26.73 min
[32m[20221213 22:44:57 @agent_ppo2.py:145][0m 2607104 total steps have happened
[32m[20221213 22:44:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1273 --------------------------#
[32m[20221213 22:44:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:57 @agent_ppo2.py:185][0m |           0.0035 |          27.8250 |          15.2804 |
[32m[20221213 22:44:57 @agent_ppo2.py:185][0m |          -0.0024 |          24.2144 |          15.2975 |
[32m[20221213 22:44:57 @agent_ppo2.py:185][0m |          -0.0070 |          23.6001 |          15.2823 |
[32m[20221213 22:44:57 @agent_ppo2.py:185][0m |          -0.0079 |          22.8383 |          15.2877 |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0070 |          22.5322 |          15.2861 |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0026 |          23.3340 |          15.2903 |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0051 |          21.9720 |          15.2879 |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0123 |          21.9994 |          15.2832 |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0099 |          21.5022 |          15.2874 |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0135 |          21.4307 |          15.2744 |
[32m[20221213 22:44:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:44:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.27
[32m[20221213 22:44:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.26
[32m[20221213 22:44:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.49
[32m[20221213 22:44:58 @agent_ppo2.py:143][0m Total time:      26.75 min
[32m[20221213 22:44:58 @agent_ppo2.py:145][0m 2609152 total steps have happened
[32m[20221213 22:44:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1274 --------------------------#
[32m[20221213 22:44:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:44:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0009 |          37.6881 |          15.2868 |
[32m[20221213 22:44:58 @agent_ppo2.py:185][0m |          -0.0102 |          34.1692 |          15.2750 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0013 |          34.2816 |          15.2647 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0084 |          33.3015 |          15.2703 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0070 |          33.1978 |          15.2705 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0118 |          32.9720 |          15.2764 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0110 |          32.6715 |          15.2702 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0094 |          32.5314 |          15.2800 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0122 |          32.4135 |          15.2734 |
[32m[20221213 22:44:59 @agent_ppo2.py:185][0m |          -0.0110 |          32.4181 |          15.2776 |
[32m[20221213 22:44:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:44:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.19
[32m[20221213 22:44:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.22
[32m[20221213 22:44:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.68
[32m[20221213 22:44:59 @agent_ppo2.py:143][0m Total time:      26.77 min
[32m[20221213 22:44:59 @agent_ppo2.py:145][0m 2611200 total steps have happened
[32m[20221213 22:44:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1275 --------------------------#
[32m[20221213 22:44:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |           0.0043 |          35.1941 |          15.1007 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0034 |          33.0629 |          15.0836 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0019 |          32.1194 |          15.0852 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0068 |          31.2392 |          15.0727 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0036 |          30.8036 |          15.0631 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0053 |          30.5539 |          15.0744 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0067 |          30.1267 |          15.0737 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0094 |          29.9344 |          15.0755 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0095 |          29.6694 |          15.0769 |
[32m[20221213 22:45:00 @agent_ppo2.py:185][0m |          -0.0122 |          29.4349 |          15.0678 |
[32m[20221213 22:45:00 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.93
[32m[20221213 22:45:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.95
[32m[20221213 22:45:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.54
[32m[20221213 22:45:01 @agent_ppo2.py:143][0m Total time:      26.79 min
[32m[20221213 22:45:01 @agent_ppo2.py:145][0m 2613248 total steps have happened
[32m[20221213 22:45:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1276 --------------------------#
[32m[20221213 22:45:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |          -0.0056 |          28.2257 |          15.1557 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |           0.0044 |          23.1746 |          15.1363 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |          -0.0074 |          22.4558 |          15.1464 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |          -0.0034 |          21.7709 |          15.1428 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |          -0.0104 |          21.3399 |          15.1395 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |          -0.0037 |          20.9777 |          15.1421 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |           0.0033 |          20.8374 |          15.1394 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |          -0.0071 |          20.3595 |          15.1439 |
[32m[20221213 22:45:01 @agent_ppo2.py:185][0m |           0.0032 |          22.2644 |          15.1399 |
[32m[20221213 22:45:02 @agent_ppo2.py:185][0m |          -0.0085 |          20.0648 |          15.1276 |
[32m[20221213 22:45:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 216.90
[32m[20221213 22:45:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.23
[32m[20221213 22:45:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.50
[32m[20221213 22:45:02 @agent_ppo2.py:143][0m Total time:      26.81 min
[32m[20221213 22:45:02 @agent_ppo2.py:145][0m 2615296 total steps have happened
[32m[20221213 22:45:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1277 --------------------------#
[32m[20221213 22:45:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:02 @agent_ppo2.py:185][0m |           0.0188 |          48.3042 |          15.0642 |
[32m[20221213 22:45:02 @agent_ppo2.py:185][0m |          -0.0031 |          42.3082 |          15.0255 |
[32m[20221213 22:45:02 @agent_ppo2.py:185][0m |          -0.0061 |          42.1039 |          15.0450 |
[32m[20221213 22:45:02 @agent_ppo2.py:185][0m |           0.0208 |          49.9866 |          15.0561 |
[32m[20221213 22:45:02 @agent_ppo2.py:185][0m |          -0.0084 |          41.5369 |          15.0217 |
[32m[20221213 22:45:02 @agent_ppo2.py:185][0m |          -0.0116 |          40.9562 |          15.0435 |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |          -0.0091 |          40.8047 |          15.0341 |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |          -0.0073 |          40.8745 |          15.0432 |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |          -0.0097 |          40.6756 |          15.0452 |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |          -0.0117 |          40.3777 |          15.0303 |
[32m[20221213 22:45:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.12
[32m[20221213 22:45:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.35
[32m[20221213 22:45:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.96
[32m[20221213 22:45:03 @agent_ppo2.py:143][0m Total time:      26.83 min
[32m[20221213 22:45:03 @agent_ppo2.py:145][0m 2617344 total steps have happened
[32m[20221213 22:45:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1278 --------------------------#
[32m[20221213 22:45:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |           0.0037 |          33.5946 |          14.9899 |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |           0.0002 |          30.8619 |          14.9821 |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |           0.0073 |          31.3594 |          14.9676 |
[32m[20221213 22:45:03 @agent_ppo2.py:185][0m |          -0.0131 |          29.0879 |          14.9744 |
[32m[20221213 22:45:04 @agent_ppo2.py:185][0m |          -0.0109 |          28.2965 |          14.9657 |
[32m[20221213 22:45:04 @agent_ppo2.py:185][0m |          -0.0120 |          27.9451 |          14.9712 |
[32m[20221213 22:45:04 @agent_ppo2.py:185][0m |           0.0026 |          30.6649 |          14.9759 |
[32m[20221213 22:45:04 @agent_ppo2.py:185][0m |          -0.0120 |          27.5927 |          14.9605 |
[32m[20221213 22:45:04 @agent_ppo2.py:185][0m |          -0.0174 |          27.3548 |          14.9701 |
[32m[20221213 22:45:04 @agent_ppo2.py:185][0m |          -0.0180 |          27.1171 |          14.9582 |
[32m[20221213 22:45:04 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:45:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.67
[32m[20221213 22:45:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.83
[32m[20221213 22:45:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.46
[32m[20221213 22:45:04 @agent_ppo2.py:143][0m Total time:      26.85 min
[32m[20221213 22:45:04 @agent_ppo2.py:145][0m 2619392 total steps have happened
[32m[20221213 22:45:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1279 --------------------------#
[32m[20221213 22:45:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:04 @agent_ppo2.py:185][0m |           0.0041 |          37.1117 |          15.0510 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |           0.0002 |          35.4338 |          15.0421 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0090 |          34.0946 |          15.0108 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0046 |          34.3271 |          15.0517 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0097 |          33.2882 |          15.0296 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0132 |          32.7121 |          15.0310 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0085 |          32.3258 |          15.0412 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0077 |          32.4853 |          15.0356 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0126 |          31.9327 |          15.0271 |
[32m[20221213 22:45:05 @agent_ppo2.py:185][0m |          -0.0144 |          31.8480 |          15.0366 |
[32m[20221213 22:45:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.85
[32m[20221213 22:45:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.83
[32m[20221213 22:45:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.99
[32m[20221213 22:45:05 @agent_ppo2.py:143][0m Total time:      26.87 min
[32m[20221213 22:45:05 @agent_ppo2.py:145][0m 2621440 total steps have happened
[32m[20221213 22:45:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1280 --------------------------#
[32m[20221213 22:45:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:45:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |           0.0008 |          34.4600 |          15.1146 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0042 |          30.7766 |          15.0911 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0106 |          29.3946 |          15.0783 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |           0.0017 |          30.1200 |          15.0814 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0085 |          28.4043 |          15.0708 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0128 |          27.4449 |          15.0793 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0146 |          27.1723 |          15.0654 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0042 |          26.9211 |          15.0722 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0121 |          26.1672 |          15.0712 |
[32m[20221213 22:45:06 @agent_ppo2.py:185][0m |          -0.0146 |          25.8272 |          15.0596 |
[32m[20221213 22:45:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.51
[32m[20221213 22:45:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.60
[32m[20221213 22:45:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.22
[32m[20221213 22:45:07 @agent_ppo2.py:143][0m Total time:      26.89 min
[32m[20221213 22:45:07 @agent_ppo2.py:145][0m 2623488 total steps have happened
[32m[20221213 22:45:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1281 --------------------------#
[32m[20221213 22:45:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0005 |          39.2559 |          15.0662 |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0098 |          36.1414 |          15.0347 |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0036 |          34.9466 |          15.0242 |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0126 |          34.5017 |          15.0279 |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0046 |          33.7192 |          15.0273 |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0040 |          33.4434 |          15.0062 |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0128 |          33.1191 |          15.0276 |
[32m[20221213 22:45:07 @agent_ppo2.py:185][0m |          -0.0144 |          32.8081 |          15.0140 |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0112 |          32.5820 |          15.0274 |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0152 |          32.6526 |          15.0182 |
[32m[20221213 22:45:08 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.04
[32m[20221213 22:45:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.97
[32m[20221213 22:45:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.63
[32m[20221213 22:45:08 @agent_ppo2.py:143][0m Total time:      26.91 min
[32m[20221213 22:45:08 @agent_ppo2.py:145][0m 2625536 total steps have happened
[32m[20221213 22:45:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1282 --------------------------#
[32m[20221213 22:45:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0018 |          20.2039 |          14.9620 |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0054 |          16.1082 |          14.9433 |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0076 |          15.5003 |          14.9339 |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0165 |          15.1888 |          14.9202 |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0067 |          14.8469 |          14.9307 |
[32m[20221213 22:45:08 @agent_ppo2.py:185][0m |          -0.0099 |          14.6435 |          14.9231 |
[32m[20221213 22:45:09 @agent_ppo2.py:185][0m |          -0.0126 |          14.3945 |          14.9182 |
[32m[20221213 22:45:09 @agent_ppo2.py:185][0m |          -0.0142 |          14.2867 |          14.9102 |
[32m[20221213 22:45:09 @agent_ppo2.py:185][0m |          -0.0206 |          14.1684 |          14.9230 |
[32m[20221213 22:45:09 @agent_ppo2.py:185][0m |          -0.0147 |          14.0569 |          14.8907 |
[32m[20221213 22:45:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.39
[32m[20221213 22:45:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 330.18
[32m[20221213 22:45:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:45:09 @agent_ppo2.py:143][0m Total time:      26.93 min
[32m[20221213 22:45:09 @agent_ppo2.py:145][0m 2627584 total steps have happened
[32m[20221213 22:45:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1283 --------------------------#
[32m[20221213 22:45:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:09 @agent_ppo2.py:185][0m |          -0.0035 |          19.2260 |          15.2290 |
[32m[20221213 22:45:09 @agent_ppo2.py:185][0m |          -0.0045 |          17.0001 |          15.2200 |
[32m[20221213 22:45:09 @agent_ppo2.py:185][0m |          -0.0073 |          16.5503 |          15.2173 |
[32m[20221213 22:45:10 @agent_ppo2.py:185][0m |          -0.0015 |          17.6975 |          15.2245 |
[32m[20221213 22:45:10 @agent_ppo2.py:185][0m |          -0.0086 |          16.1467 |          15.2141 |
[32m[20221213 22:45:10 @agent_ppo2.py:185][0m |          -0.0100 |          15.8789 |          15.2216 |
[32m[20221213 22:45:10 @agent_ppo2.py:185][0m |          -0.0133 |          15.7467 |          15.2267 |
[32m[20221213 22:45:10 @agent_ppo2.py:185][0m |          -0.0087 |          15.5113 |          15.2194 |
[32m[20221213 22:45:10 @agent_ppo2.py:185][0m |          -0.0068 |          15.3897 |          15.2226 |
[32m[20221213 22:45:10 @agent_ppo2.py:185][0m |          -0.0123 |          15.3048 |          15.2116 |
[32m[20221213 22:45:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:45:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.88
[32m[20221213 22:45:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.17
[32m[20221213 22:45:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.54
[32m[20221213 22:45:10 @agent_ppo2.py:143][0m Total time:      26.95 min
[32m[20221213 22:45:10 @agent_ppo2.py:145][0m 2629632 total steps have happened
[32m[20221213 22:45:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1284 --------------------------#
[32m[20221213 22:45:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0016 |          27.0482 |          15.2929 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0054 |          23.9825 |          15.2749 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0075 |          23.3468 |          15.2800 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0024 |          22.8879 |          15.2750 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0161 |          22.2670 |          15.2767 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0184 |          21.7969 |          15.2834 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0125 |          21.4978 |          15.2680 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0163 |          21.3680 |          15.2779 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0187 |          21.1855 |          15.2678 |
[32m[20221213 22:45:11 @agent_ppo2.py:185][0m |          -0.0228 |          20.9489 |          15.2705 |
[32m[20221213 22:45:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.65
[32m[20221213 22:45:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.86
[32m[20221213 22:45:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 74.52
[32m[20221213 22:45:11 @agent_ppo2.py:143][0m Total time:      26.97 min
[32m[20221213 22:45:11 @agent_ppo2.py:145][0m 2631680 total steps have happened
[32m[20221213 22:45:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1285 --------------------------#
[32m[20221213 22:45:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |           0.0002 |          16.3964 |          15.1702 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |           0.0034 |          13.8283 |          15.1589 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0051 |          12.8753 |          15.1581 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0047 |          12.1300 |          15.1421 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0161 |          11.7774 |          15.1429 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0126 |          11.3735 |          15.1366 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0208 |          11.1361 |          15.1381 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0166 |          10.9812 |          15.1402 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0103 |          10.6983 |          15.1317 |
[32m[20221213 22:45:12 @agent_ppo2.py:185][0m |          -0.0170 |          10.5391 |          15.1304 |
[32m[20221213 22:45:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 151.88
[32m[20221213 22:45:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.45
[32m[20221213 22:45:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.33
[32m[20221213 22:45:13 @agent_ppo2.py:143][0m Total time:      27.00 min
[32m[20221213 22:45:13 @agent_ppo2.py:145][0m 2633728 total steps have happened
[32m[20221213 22:45:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1286 --------------------------#
[32m[20221213 22:45:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0012 |          26.5047 |          15.2663 |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0029 |          25.2525 |          15.2516 |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0053 |          24.8695 |          15.2581 |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0036 |          24.6011 |          15.2340 |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0075 |          24.2276 |          15.2400 |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0032 |          23.9786 |          15.2460 |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0122 |          23.9454 |          15.2426 |
[32m[20221213 22:45:13 @agent_ppo2.py:185][0m |          -0.0071 |          23.7612 |          15.2387 |
[32m[20221213 22:45:14 @agent_ppo2.py:185][0m |          -0.0112 |          23.7041 |          15.2447 |
[32m[20221213 22:45:14 @agent_ppo2.py:185][0m |          -0.0067 |          23.5188 |          15.2339 |
[32m[20221213 22:45:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.81
[32m[20221213 22:45:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.90
[32m[20221213 22:45:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.83
[32m[20221213 22:45:14 @agent_ppo2.py:143][0m Total time:      27.02 min
[32m[20221213 22:45:14 @agent_ppo2.py:145][0m 2635776 total steps have happened
[32m[20221213 22:45:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1287 --------------------------#
[32m[20221213 22:45:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:14 @agent_ppo2.py:185][0m |          -0.0045 |          34.9326 |          15.3184 |
[32m[20221213 22:45:14 @agent_ppo2.py:185][0m |          -0.0013 |          30.9702 |          15.3168 |
[32m[20221213 22:45:14 @agent_ppo2.py:185][0m |          -0.0054 |          29.1468 |          15.3008 |
[32m[20221213 22:45:14 @agent_ppo2.py:185][0m |          -0.0088 |          28.1316 |          15.3066 |
[32m[20221213 22:45:14 @agent_ppo2.py:185][0m |          -0.0103 |          27.3519 |          15.3164 |
[32m[20221213 22:45:15 @agent_ppo2.py:185][0m |           0.0008 |          29.6033 |          15.3038 |
[32m[20221213 22:45:15 @agent_ppo2.py:185][0m |          -0.0104 |          26.2654 |          15.3083 |
[32m[20221213 22:45:15 @agent_ppo2.py:185][0m |          -0.0114 |          25.7013 |          15.3154 |
[32m[20221213 22:45:15 @agent_ppo2.py:185][0m |          -0.0056 |          26.9191 |          15.3154 |
[32m[20221213 22:45:15 @agent_ppo2.py:185][0m |          -0.0146 |          25.0031 |          15.3180 |
[32m[20221213 22:45:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.78
[32m[20221213 22:45:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.52
[32m[20221213 22:45:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.83
[32m[20221213 22:45:15 @agent_ppo2.py:143][0m Total time:      27.04 min
[32m[20221213 22:45:15 @agent_ppo2.py:145][0m 2637824 total steps have happened
[32m[20221213 22:45:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1288 --------------------------#
[32m[20221213 22:45:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:15 @agent_ppo2.py:185][0m |           0.0007 |          18.0239 |          15.4918 |
[32m[20221213 22:45:15 @agent_ppo2.py:185][0m |           0.0005 |          15.1396 |          15.4868 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0058 |          14.3084 |          15.4862 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0124 |          13.6878 |          15.4821 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0037 |          13.5441 |          15.4718 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0000 |          13.3625 |          15.4646 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0097 |          12.6883 |          15.4703 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0057 |          12.9108 |          15.4690 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0128 |          12.4645 |          15.4527 |
[32m[20221213 22:45:16 @agent_ppo2.py:185][0m |          -0.0056 |          12.1254 |          15.4659 |
[32m[20221213 22:45:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:45:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.25
[32m[20221213 22:45:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.88
[32m[20221213 22:45:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.62
[32m[20221213 22:45:16 @agent_ppo2.py:143][0m Total time:      27.06 min
[32m[20221213 22:45:16 @agent_ppo2.py:145][0m 2639872 total steps have happened
[32m[20221213 22:45:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1289 --------------------------#
[32m[20221213 22:45:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |           0.0015 |          24.1490 |          15.3670 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0029 |          21.1962 |          15.3635 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |           0.0005 |          22.1921 |          15.3566 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0085 |          19.7665 |          15.3462 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0084 |          19.1466 |          15.3488 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0109 |          19.0121 |          15.3394 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0122 |          18.5984 |          15.3388 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0126 |          18.4844 |          15.3361 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0124 |          18.2413 |          15.3370 |
[32m[20221213 22:45:17 @agent_ppo2.py:185][0m |          -0.0148 |          18.2263 |          15.3311 |
[32m[20221213 22:45:17 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.24
[32m[20221213 22:45:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.84
[32m[20221213 22:45:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.49
[32m[20221213 22:45:17 @agent_ppo2.py:143][0m Total time:      27.08 min
[32m[20221213 22:45:17 @agent_ppo2.py:145][0m 2641920 total steps have happened
[32m[20221213 22:45:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1290 --------------------------#
[32m[20221213 22:45:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:45:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |           0.0004 |          24.1720 |          15.3165 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0013 |          21.3643 |          15.3031 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0043 |          20.2405 |          15.3038 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0058 |          19.6356 |          15.2941 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0101 |          19.0788 |          15.2999 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0018 |          19.1140 |          15.2944 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0123 |          18.3999 |          15.2882 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0043 |          18.3350 |          15.2946 |
[32m[20221213 22:45:18 @agent_ppo2.py:185][0m |          -0.0163 |          17.9213 |          15.2854 |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |           0.0007 |          18.2519 |          15.2868 |
[32m[20221213 22:45:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.89
[32m[20221213 22:45:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.93
[32m[20221213 22:45:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.44
[32m[20221213 22:45:19 @agent_ppo2.py:143][0m Total time:      27.10 min
[32m[20221213 22:45:19 @agent_ppo2.py:145][0m 2643968 total steps have happened
[32m[20221213 22:45:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1291 --------------------------#
[32m[20221213 22:45:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |          -0.0012 |          22.2485 |          15.3451 |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |           0.0061 |          20.2174 |          15.3453 |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |           0.0043 |          20.0007 |          15.3266 |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |          -0.0044 |          17.7508 |          15.3251 |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |          -0.0075 |          17.3224 |          15.3257 |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |          -0.0092 |          16.8932 |          15.3220 |
[32m[20221213 22:45:19 @agent_ppo2.py:185][0m |          -0.0099 |          16.7165 |          15.3177 |
[32m[20221213 22:45:20 @agent_ppo2.py:185][0m |          -0.0064 |          16.5575 |          15.3242 |
[32m[20221213 22:45:20 @agent_ppo2.py:185][0m |          -0.0118 |          16.2555 |          15.3255 |
[32m[20221213 22:45:20 @agent_ppo2.py:185][0m |          -0.0093 |          16.0574 |          15.3196 |
[32m[20221213 22:45:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.92
[32m[20221213 22:45:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.17
[32m[20221213 22:45:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.02
[32m[20221213 22:45:20 @agent_ppo2.py:143][0m Total time:      27.12 min
[32m[20221213 22:45:20 @agent_ppo2.py:145][0m 2646016 total steps have happened
[32m[20221213 22:45:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1292 --------------------------#
[32m[20221213 22:45:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:20 @agent_ppo2.py:185][0m |           0.0075 |          28.8476 |          15.2352 |
[32m[20221213 22:45:20 @agent_ppo2.py:185][0m |          -0.0049 |          26.0382 |          15.2075 |
[32m[20221213 22:45:20 @agent_ppo2.py:185][0m |          -0.0044 |          24.9987 |          15.2074 |
[32m[20221213 22:45:20 @agent_ppo2.py:185][0m |          -0.0072 |          24.1971 |          15.2063 |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0092 |          23.6696 |          15.2107 |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0092 |          23.3136 |          15.2072 |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0078 |          23.0878 |          15.1990 |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0094 |          22.7256 |          15.2057 |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0154 |          22.5557 |          15.2089 |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0074 |          22.2797 |          15.2037 |
[32m[20221213 22:45:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.47
[32m[20221213 22:45:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 323.85
[32m[20221213 22:45:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.26
[32m[20221213 22:45:21 @agent_ppo2.py:143][0m Total time:      27.14 min
[32m[20221213 22:45:21 @agent_ppo2.py:145][0m 2648064 total steps have happened
[32m[20221213 22:45:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1293 --------------------------#
[32m[20221213 22:45:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0018 |          30.5766 |          15.4207 |
[32m[20221213 22:45:21 @agent_ppo2.py:185][0m |          -0.0062 |          29.0597 |          15.4163 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |          -0.0093 |          28.4711 |          15.4093 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |           0.0052 |          29.7815 |          15.4073 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |           0.0033 |          28.3455 |          15.4112 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |          -0.0082 |          27.2487 |          15.4054 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |          -0.0107 |          26.6983 |          15.4044 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |          -0.0104 |          26.6172 |          15.4106 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |          -0.0112 |          26.4429 |          15.4156 |
[32m[20221213 22:45:22 @agent_ppo2.py:185][0m |          -0.0093 |          26.1495 |          15.4044 |
[32m[20221213 22:45:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:45:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.60
[32m[20221213 22:45:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.99
[32m[20221213 22:45:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.04
[32m[20221213 22:45:22 @agent_ppo2.py:143][0m Total time:      27.16 min
[32m[20221213 22:45:22 @agent_ppo2.py:145][0m 2650112 total steps have happened
[32m[20221213 22:45:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1294 --------------------------#
[32m[20221213 22:45:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0015 |          24.6752 |          15.3965 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0041 |          22.3679 |          15.3693 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0074 |          21.6294 |          15.3899 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0100 |          21.1214 |          15.3865 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0059 |          20.7549 |          15.3952 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0139 |          20.3598 |          15.3844 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0176 |          20.1740 |          15.3851 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0096 |          20.1591 |          15.3921 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0182 |          19.6971 |          15.3829 |
[32m[20221213 22:45:23 @agent_ppo2.py:185][0m |          -0.0102 |          19.4030 |          15.3804 |
[32m[20221213 22:45:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 159.54
[32m[20221213 22:45:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.90
[32m[20221213 22:45:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.32
[32m[20221213 22:45:24 @agent_ppo2.py:143][0m Total time:      27.18 min
[32m[20221213 22:45:24 @agent_ppo2.py:145][0m 2652160 total steps have happened
[32m[20221213 22:45:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1295 --------------------------#
[32m[20221213 22:45:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0010 |          18.7438 |          15.3879 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0050 |          16.0720 |          15.3749 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0054 |          15.0720 |          15.3713 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |           0.0056 |          17.0293 |          15.3672 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0065 |          13.9753 |          15.3565 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0119 |          13.5093 |          15.3551 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0106 |          13.1462 |          15.3689 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0065 |          13.0287 |          15.3555 |
[32m[20221213 22:45:24 @agent_ppo2.py:185][0m |          -0.0136 |          12.7431 |          15.3581 |
[32m[20221213 22:45:25 @agent_ppo2.py:185][0m |          -0.0108 |          12.4899 |          15.3592 |
[32m[20221213 22:45:25 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.21
[32m[20221213 22:45:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 307.72
[32m[20221213 22:45:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.89
[32m[20221213 22:45:25 @agent_ppo2.py:143][0m Total time:      27.20 min
[32m[20221213 22:45:25 @agent_ppo2.py:145][0m 2654208 total steps have happened
[32m[20221213 22:45:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1296 --------------------------#
[32m[20221213 22:45:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:25 @agent_ppo2.py:185][0m |          -0.0037 |          35.7751 |          15.3826 |
[32m[20221213 22:45:25 @agent_ppo2.py:185][0m |          -0.0030 |          33.0524 |          15.3830 |
[32m[20221213 22:45:25 @agent_ppo2.py:185][0m |          -0.0051 |          31.9649 |          15.3843 |
[32m[20221213 22:45:25 @agent_ppo2.py:185][0m |          -0.0081 |          31.2728 |          15.3874 |
[32m[20221213 22:45:25 @agent_ppo2.py:185][0m |          -0.0088 |          30.4152 |          15.3701 |
[32m[20221213 22:45:25 @agent_ppo2.py:185][0m |          -0.0076 |          30.0859 |          15.3792 |
[32m[20221213 22:45:26 @agent_ppo2.py:185][0m |          -0.0094 |          29.6999 |          15.3749 |
[32m[20221213 22:45:26 @agent_ppo2.py:185][0m |          -0.0062 |          29.1571 |          15.3742 |
[32m[20221213 22:45:26 @agent_ppo2.py:185][0m |          -0.0113 |          29.1492 |          15.3734 |
[32m[20221213 22:45:26 @agent_ppo2.py:185][0m |          -0.0145 |          28.7798 |          15.3704 |
[32m[20221213 22:45:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.49
[32m[20221213 22:45:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.35
[32m[20221213 22:45:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.52
[32m[20221213 22:45:26 @agent_ppo2.py:143][0m Total time:      27.22 min
[32m[20221213 22:45:26 @agent_ppo2.py:145][0m 2656256 total steps have happened
[32m[20221213 22:45:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1297 --------------------------#
[32m[20221213 22:45:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:26 @agent_ppo2.py:185][0m |           0.0002 |          36.2182 |          15.3281 |
[32m[20221213 22:45:26 @agent_ppo2.py:185][0m |          -0.0069 |          33.5563 |          15.3125 |
[32m[20221213 22:45:26 @agent_ppo2.py:185][0m |          -0.0089 |          32.4055 |          15.3171 |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0082 |          31.5234 |          15.3130 |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0113 |          31.1464 |          15.3068 |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0112 |          30.5845 |          15.3007 |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0124 |          30.3002 |          15.2995 |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0098 |          30.2110 |          15.3065 |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0091 |          29.9213 |          15.3059 |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0123 |          29.6063 |          15.2940 |
[32m[20221213 22:45:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.82
[32m[20221213 22:45:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.18
[32m[20221213 22:45:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.64
[32m[20221213 22:45:27 @agent_ppo2.py:143][0m Total time:      27.24 min
[32m[20221213 22:45:27 @agent_ppo2.py:145][0m 2658304 total steps have happened
[32m[20221213 22:45:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1298 --------------------------#
[32m[20221213 22:45:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:27 @agent_ppo2.py:185][0m |          -0.0020 |          26.6960 |          15.2965 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0038 |          24.0976 |          15.2694 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0083 |          23.4466 |          15.2780 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0092 |          23.0999 |          15.2714 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0068 |          22.7470 |          15.2740 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0147 |          23.0593 |          15.2547 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0125 |          22.3533 |          15.2539 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0083 |          22.1938 |          15.2518 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0110 |          22.0879 |          15.2564 |
[32m[20221213 22:45:28 @agent_ppo2.py:185][0m |          -0.0101 |          22.0342 |          15.2533 |
[32m[20221213 22:45:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:45:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.25
[32m[20221213 22:45:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.31
[32m[20221213 22:45:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.25
[32m[20221213 22:45:28 @agent_ppo2.py:143][0m Total time:      27.26 min
[32m[20221213 22:45:28 @agent_ppo2.py:145][0m 2660352 total steps have happened
[32m[20221213 22:45:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1299 --------------------------#
[32m[20221213 22:45:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |           0.0015 |          36.2849 |          15.2905 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0000 |          31.4085 |          15.2970 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0077 |          30.0069 |          15.2877 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0079 |          29.2286 |          15.2918 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0054 |          28.9034 |          15.2672 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0089 |          28.3595 |          15.2928 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0094 |          27.9519 |          15.2779 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0101 |          27.8606 |          15.2902 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0103 |          27.5032 |          15.2784 |
[32m[20221213 22:45:29 @agent_ppo2.py:185][0m |          -0.0115 |          27.2598 |          15.2701 |
[32m[20221213 22:45:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.31
[32m[20221213 22:45:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.18
[32m[20221213 22:45:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.53
[32m[20221213 22:45:30 @agent_ppo2.py:143][0m Total time:      27.28 min
[32m[20221213 22:45:30 @agent_ppo2.py:145][0m 2662400 total steps have happened
[32m[20221213 22:45:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1300 --------------------------#
[32m[20221213 22:45:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:45:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |          -0.0008 |          26.5024 |          15.3350 |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |           0.0021 |          24.5025 |          15.3163 |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |          -0.0077 |          23.5778 |          15.3074 |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |          -0.0057 |          23.1290 |          15.3206 |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |           0.0075 |          24.9118 |          15.3082 |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |          -0.0100 |          22.5317 |          15.3008 |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |          -0.0115 |          22.4832 |          15.2991 |
[32m[20221213 22:45:30 @agent_ppo2.py:185][0m |          -0.0060 |          22.3406 |          15.3028 |
[32m[20221213 22:45:31 @agent_ppo2.py:185][0m |          -0.0033 |          21.9781 |          15.3079 |
[32m[20221213 22:45:31 @agent_ppo2.py:185][0m |          -0.0117 |          22.0269 |          15.3084 |
[32m[20221213 22:45:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.44
[32m[20221213 22:45:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.03
[32m[20221213 22:45:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.65
[32m[20221213 22:45:31 @agent_ppo2.py:143][0m Total time:      27.30 min
[32m[20221213 22:45:31 @agent_ppo2.py:145][0m 2664448 total steps have happened
[32m[20221213 22:45:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1301 --------------------------#
[32m[20221213 22:45:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:31 @agent_ppo2.py:185][0m |          -0.0023 |          19.2859 |          15.5057 |
[32m[20221213 22:45:31 @agent_ppo2.py:185][0m |          -0.0076 |          16.2809 |          15.4948 |
[32m[20221213 22:45:31 @agent_ppo2.py:185][0m |          -0.0059 |          15.0333 |          15.4923 |
[32m[20221213 22:45:31 @agent_ppo2.py:185][0m |          -0.0085 |          14.3889 |          15.4855 |
[32m[20221213 22:45:31 @agent_ppo2.py:185][0m |          -0.0062 |          13.7535 |          15.4957 |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0131 |          13.2199 |          15.4969 |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0178 |          12.8331 |          15.4913 |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0171 |          12.4547 |          15.4859 |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0175 |          12.1531 |          15.4927 |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0179 |          11.8982 |          15.4869 |
[32m[20221213 22:45:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.59
[32m[20221213 22:45:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.62
[32m[20221213 22:45:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.55
[32m[20221213 22:45:32 @agent_ppo2.py:143][0m Total time:      27.32 min
[32m[20221213 22:45:32 @agent_ppo2.py:145][0m 2666496 total steps have happened
[32m[20221213 22:45:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1302 --------------------------#
[32m[20221213 22:45:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0023 |          46.4080 |          15.3226 |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0078 |          44.1148 |          15.3164 |
[32m[20221213 22:45:32 @agent_ppo2.py:185][0m |          -0.0054 |          43.3627 |          15.2887 |
[32m[20221213 22:45:33 @agent_ppo2.py:185][0m |          -0.0052 |          42.9034 |          15.2937 |
[32m[20221213 22:45:33 @agent_ppo2.py:185][0m |          -0.0112 |          42.7607 |          15.2966 |
[32m[20221213 22:45:33 @agent_ppo2.py:185][0m |          -0.0009 |          44.5679 |          15.2958 |
[32m[20221213 22:45:33 @agent_ppo2.py:185][0m |          -0.0049 |          42.7073 |          15.2909 |
[32m[20221213 22:45:33 @agent_ppo2.py:185][0m |          -0.0086 |          42.0377 |          15.2841 |
[32m[20221213 22:45:33 @agent_ppo2.py:185][0m |          -0.0139 |          41.8378 |          15.2830 |
[32m[20221213 22:45:33 @agent_ppo2.py:185][0m |          -0.0136 |          41.7100 |          15.2785 |
[32m[20221213 22:45:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.97
[32m[20221213 22:45:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.19
[32m[20221213 22:45:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.48
[32m[20221213 22:45:33 @agent_ppo2.py:143][0m Total time:      27.34 min
[32m[20221213 22:45:33 @agent_ppo2.py:145][0m 2668544 total steps have happened
[32m[20221213 22:45:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1303 --------------------------#
[32m[20221213 22:45:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |           0.0000 |          23.6286 |          15.3727 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0071 |          20.1380 |          15.3748 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0072 |          18.8758 |          15.3755 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0095 |          18.3123 |          15.3651 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0101 |          17.9598 |          15.3726 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0101 |          17.7311 |          15.3666 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0071 |          17.3539 |          15.3670 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0110 |          17.3705 |          15.3644 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0118 |          16.8505 |          15.3595 |
[32m[20221213 22:45:34 @agent_ppo2.py:185][0m |          -0.0180 |          16.5312 |          15.3671 |
[32m[20221213 22:45:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:45:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 140.78
[32m[20221213 22:45:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.78
[32m[20221213 22:45:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.58
[32m[20221213 22:45:34 @agent_ppo2.py:143][0m Total time:      27.36 min
[32m[20221213 22:45:34 @agent_ppo2.py:145][0m 2670592 total steps have happened
[32m[20221213 22:45:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1304 --------------------------#
[32m[20221213 22:45:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0020 |          38.8192 |          15.2871 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0057 |          35.6239 |          15.2832 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0053 |          34.3827 |          15.2833 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0045 |          34.2052 |          15.2756 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0088 |          33.0358 |          15.2784 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0025 |          33.2084 |          15.2741 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0079 |          32.1450 |          15.2746 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0126 |          31.8399 |          15.2778 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0056 |          33.5493 |          15.2722 |
[32m[20221213 22:45:35 @agent_ppo2.py:185][0m |          -0.0120 |          31.3916 |          15.2596 |
[32m[20221213 22:45:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.97
[32m[20221213 22:45:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.08
[32m[20221213 22:45:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.66
[32m[20221213 22:45:36 @agent_ppo2.py:143][0m Total time:      27.38 min
[32m[20221213 22:45:36 @agent_ppo2.py:145][0m 2672640 total steps have happened
[32m[20221213 22:45:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1305 --------------------------#
[32m[20221213 22:45:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:36 @agent_ppo2.py:185][0m |           0.0059 |          29.3031 |          15.4411 |
[32m[20221213 22:45:36 @agent_ppo2.py:185][0m |          -0.0012 |          25.4018 |          15.4013 |
[32m[20221213 22:45:36 @agent_ppo2.py:185][0m |          -0.0007 |          24.7447 |          15.4110 |
[32m[20221213 22:45:36 @agent_ppo2.py:185][0m |          -0.0099 |          23.7918 |          15.4008 |
[32m[20221213 22:45:36 @agent_ppo2.py:185][0m |          -0.0097 |          23.2181 |          15.4033 |
[32m[20221213 22:45:36 @agent_ppo2.py:185][0m |          -0.0100 |          22.9046 |          15.3956 |
[32m[20221213 22:45:36 @agent_ppo2.py:185][0m |          -0.0142 |          22.6246 |          15.3984 |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0117 |          22.2211 |          15.3895 |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0045 |          22.1261 |          15.3881 |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0145 |          21.8479 |          15.3910 |
[32m[20221213 22:45:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.71
[32m[20221213 22:45:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.63
[32m[20221213 22:45:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.83
[32m[20221213 22:45:37 @agent_ppo2.py:143][0m Total time:      27.40 min
[32m[20221213 22:45:37 @agent_ppo2.py:145][0m 2674688 total steps have happened
[32m[20221213 22:45:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1306 --------------------------#
[32m[20221213 22:45:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0012 |          39.3408 |          15.5154 |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0060 |          36.4863 |          15.5155 |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0094 |          35.3332 |          15.5091 |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0090 |          34.7645 |          15.5050 |
[32m[20221213 22:45:37 @agent_ppo2.py:185][0m |          -0.0083 |          34.1484 |          15.5037 |
[32m[20221213 22:45:38 @agent_ppo2.py:185][0m |          -0.0058 |          33.9104 |          15.4955 |
[32m[20221213 22:45:38 @agent_ppo2.py:185][0m |          -0.0122 |          33.4710 |          15.4974 |
[32m[20221213 22:45:38 @agent_ppo2.py:185][0m |          -0.0038 |          33.6658 |          15.4966 |
[32m[20221213 22:45:38 @agent_ppo2.py:185][0m |          -0.0085 |          33.5581 |          15.4944 |
[32m[20221213 22:45:38 @agent_ppo2.py:185][0m |          -0.0137 |          32.6288 |          15.4923 |
[32m[20221213 22:45:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.56
[32m[20221213 22:45:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.99
[32m[20221213 22:45:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.55
[32m[20221213 22:45:38 @agent_ppo2.py:143][0m Total time:      27.42 min
[32m[20221213 22:45:38 @agent_ppo2.py:145][0m 2676736 total steps have happened
[32m[20221213 22:45:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1307 --------------------------#
[32m[20221213 22:45:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:38 @agent_ppo2.py:185][0m |           0.0013 |          30.3664 |          15.3824 |
[32m[20221213 22:45:38 @agent_ppo2.py:185][0m |          -0.0019 |          28.7396 |          15.3764 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0076 |          27.8478 |          15.3652 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0038 |          27.7958 |          15.3741 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0077 |          27.3269 |          15.3632 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0088 |          27.2279 |          15.3727 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0039 |          28.1080 |          15.3683 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0032 |          26.9413 |          15.3610 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0082 |          26.6019 |          15.3654 |
[32m[20221213 22:45:39 @agent_ppo2.py:185][0m |          -0.0163 |          26.6020 |          15.3626 |
[32m[20221213 22:45:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.18
[32m[20221213 22:45:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.98
[32m[20221213 22:45:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.53
[32m[20221213 22:45:39 @agent_ppo2.py:143][0m Total time:      27.44 min
[32m[20221213 22:45:39 @agent_ppo2.py:145][0m 2678784 total steps have happened
[32m[20221213 22:45:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1308 --------------------------#
[32m[20221213 22:45:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |           0.0015 |          30.0227 |          15.3678 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0113 |          26.6248 |          15.3664 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0014 |          27.7476 |          15.3575 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0120 |          24.9207 |          15.3504 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0109 |          24.2328 |          15.3455 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0183 |          23.9693 |          15.3486 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0188 |          23.5493 |          15.3400 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0080 |          23.7132 |          15.3367 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0188 |          23.0169 |          15.3411 |
[32m[20221213 22:45:40 @agent_ppo2.py:185][0m |          -0.0068 |          24.0874 |          15.3319 |
[32m[20221213 22:45:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:45:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.28
[32m[20221213 22:45:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.91
[32m[20221213 22:45:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.39
[32m[20221213 22:45:40 @agent_ppo2.py:143][0m Total time:      27.46 min
[32m[20221213 22:45:40 @agent_ppo2.py:145][0m 2680832 total steps have happened
[32m[20221213 22:45:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1309 --------------------------#
[32m[20221213 22:45:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |          -0.0013 |          29.5383 |          15.3566 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |           0.0015 |          24.9108 |          15.3577 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |          -0.0055 |          23.0942 |          15.3434 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |          -0.0083 |          22.2815 |          15.3395 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |          -0.0131 |          21.7197 |          15.3270 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |          -0.0088 |          21.3494 |          15.3338 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |           0.0011 |          21.6616 |          15.3169 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |          -0.0062 |          22.1990 |          15.3143 |
[32m[20221213 22:45:41 @agent_ppo2.py:185][0m |          -0.0082 |          22.3997 |          15.3203 |
[32m[20221213 22:45:42 @agent_ppo2.py:185][0m |          -0.0191 |          20.0851 |          15.3059 |
[32m[20221213 22:45:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.91
[32m[20221213 22:45:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.00
[32m[20221213 22:45:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.37
[32m[20221213 22:45:42 @agent_ppo2.py:143][0m Total time:      27.48 min
[32m[20221213 22:45:42 @agent_ppo2.py:145][0m 2682880 total steps have happened
[32m[20221213 22:45:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1310 --------------------------#
[32m[20221213 22:45:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:45:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:42 @agent_ppo2.py:185][0m |          -0.0005 |          37.6980 |          15.5711 |
[32m[20221213 22:45:42 @agent_ppo2.py:185][0m |          -0.0037 |          34.4368 |          15.5488 |
[32m[20221213 22:45:42 @agent_ppo2.py:185][0m |           0.0019 |          35.0415 |          15.5481 |
[32m[20221213 22:45:42 @agent_ppo2.py:185][0m |          -0.0084 |          32.4191 |          15.5437 |
[32m[20221213 22:45:42 @agent_ppo2.py:185][0m |          -0.0095 |          31.5374 |          15.5479 |
[32m[20221213 22:45:42 @agent_ppo2.py:185][0m |           0.0011 |          33.3180 |          15.5405 |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0073 |          30.7371 |          15.5292 |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0099 |          30.4421 |          15.5329 |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0097 |          30.1639 |          15.5354 |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0171 |          30.0550 |          15.5282 |
[32m[20221213 22:45:43 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.70
[32m[20221213 22:45:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.76
[32m[20221213 22:45:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.97
[32m[20221213 22:45:43 @agent_ppo2.py:143][0m Total time:      27.50 min
[32m[20221213 22:45:43 @agent_ppo2.py:145][0m 2684928 total steps have happened
[32m[20221213 22:45:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1311 --------------------------#
[32m[20221213 22:45:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0027 |          37.2317 |          15.6061 |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0054 |          33.8780 |          15.5920 |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0022 |          33.1326 |          15.5903 |
[32m[20221213 22:45:43 @agent_ppo2.py:185][0m |          -0.0029 |          31.9750 |          15.5723 |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |          -0.0071 |          31.5959 |          15.5738 |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |          -0.0116 |          31.0441 |          15.5736 |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |           0.0020 |          34.9615 |          15.5700 |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |          -0.0093 |          30.5547 |          15.5664 |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |          -0.0148 |          30.2020 |          15.5624 |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |          -0.0124 |          30.0142 |          15.5601 |
[32m[20221213 22:45:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.99
[32m[20221213 22:45:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.02
[32m[20221213 22:45:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.84
[32m[20221213 22:45:44 @agent_ppo2.py:143][0m Total time:      27.52 min
[32m[20221213 22:45:44 @agent_ppo2.py:145][0m 2686976 total steps have happened
[32m[20221213 22:45:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1312 --------------------------#
[32m[20221213 22:45:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |           0.0014 |          33.5897 |          15.5838 |
[32m[20221213 22:45:44 @agent_ppo2.py:185][0m |          -0.0000 |          31.6038 |          15.5719 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0087 |          30.5474 |          15.5687 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0053 |          29.9933 |          15.5626 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0082 |          29.4492 |          15.5590 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0065 |          29.4420 |          15.5600 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0077 |          28.6730 |          15.5487 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0101 |          28.2605 |          15.5548 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0032 |          28.1360 |          15.5535 |
[32m[20221213 22:45:45 @agent_ppo2.py:185][0m |          -0.0129 |          27.8577 |          15.5542 |
[32m[20221213 22:45:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.75
[32m[20221213 22:45:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.79
[32m[20221213 22:45:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.16
[32m[20221213 22:45:45 @agent_ppo2.py:143][0m Total time:      27.54 min
[32m[20221213 22:45:45 @agent_ppo2.py:145][0m 2689024 total steps have happened
[32m[20221213 22:45:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1313 --------------------------#
[32m[20221213 22:45:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |           0.0144 |          36.8614 |          15.5841 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0052 |          29.6844 |          15.5540 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0092 |          27.1006 |          15.5624 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0126 |          25.7343 |          15.5564 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0119 |          24.7371 |          15.5648 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0076 |          23.9498 |          15.5558 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0132 |          23.4088 |          15.5459 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0175 |          23.1198 |          15.5486 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0176 |          22.7644 |          15.5487 |
[32m[20221213 22:45:46 @agent_ppo2.py:185][0m |          -0.0157 |          22.1249 |          15.5448 |
[32m[20221213 22:45:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:45:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.38
[32m[20221213 22:45:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.80
[32m[20221213 22:45:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.36
[32m[20221213 22:45:47 @agent_ppo2.py:143][0m Total time:      27.56 min
[32m[20221213 22:45:47 @agent_ppo2.py:145][0m 2691072 total steps have happened
[32m[20221213 22:45:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1314 --------------------------#
[32m[20221213 22:45:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |           0.0029 |          30.9844 |          15.4381 |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |          -0.0040 |          26.0663 |          15.4213 |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |          -0.0081 |          24.9858 |          15.4234 |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |          -0.0053 |          23.8282 |          15.4183 |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |          -0.0031 |          23.4604 |          15.4215 |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |          -0.0099 |          22.8026 |          15.4189 |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |          -0.0127 |          22.5321 |          15.4139 |
[32m[20221213 22:45:47 @agent_ppo2.py:185][0m |          -0.0128 |          22.3760 |          15.4220 |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0136 |          21.9272 |          15.4223 |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0111 |          21.7549 |          15.4221 |
[32m[20221213 22:45:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.80
[32m[20221213 22:45:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.20
[32m[20221213 22:45:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.64
[32m[20221213 22:45:48 @agent_ppo2.py:143][0m Total time:      27.58 min
[32m[20221213 22:45:48 @agent_ppo2.py:145][0m 2693120 total steps have happened
[32m[20221213 22:45:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1315 --------------------------#
[32m[20221213 22:45:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0010 |          21.4840 |          15.6094 |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0073 |          19.4700 |          15.5962 |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0060 |          18.7148 |          15.5835 |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0073 |          18.1663 |          15.5875 |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0083 |          17.8012 |          15.5796 |
[32m[20221213 22:45:48 @agent_ppo2.py:185][0m |          -0.0100 |          17.5418 |          15.5739 |
[32m[20221213 22:45:49 @agent_ppo2.py:185][0m |          -0.0001 |          17.8001 |          15.5759 |
[32m[20221213 22:45:49 @agent_ppo2.py:185][0m |          -0.0097 |          17.1012 |          15.5630 |
[32m[20221213 22:45:49 @agent_ppo2.py:185][0m |          -0.0096 |          16.9913 |          15.5710 |
[32m[20221213 22:45:49 @agent_ppo2.py:185][0m |          -0.0158 |          16.7743 |          15.5625 |
[32m[20221213 22:45:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.69
[32m[20221213 22:45:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.27
[32m[20221213 22:45:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.31
[32m[20221213 22:45:49 @agent_ppo2.py:143][0m Total time:      27.60 min
[32m[20221213 22:45:49 @agent_ppo2.py:145][0m 2695168 total steps have happened
[32m[20221213 22:45:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1316 --------------------------#
[32m[20221213 22:45:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:49 @agent_ppo2.py:185][0m |          -0.0005 |          27.6143 |          15.5762 |
[32m[20221213 22:45:49 @agent_ppo2.py:185][0m |          -0.0097 |          23.9928 |          15.5638 |
[32m[20221213 22:45:49 @agent_ppo2.py:185][0m |          -0.0056 |          23.4014 |          15.5473 |
[32m[20221213 22:45:50 @agent_ppo2.py:185][0m |          -0.0100 |          22.4668 |          15.5533 |
[32m[20221213 22:45:50 @agent_ppo2.py:185][0m |          -0.0034 |          22.2453 |          15.5346 |
[32m[20221213 22:45:50 @agent_ppo2.py:185][0m |          -0.0131 |          22.0575 |          15.5180 |
[32m[20221213 22:45:50 @agent_ppo2.py:185][0m |          -0.0124 |          21.7733 |          15.5342 |
[32m[20221213 22:45:50 @agent_ppo2.py:185][0m |          -0.0125 |          21.6443 |          15.5085 |
[32m[20221213 22:45:50 @agent_ppo2.py:185][0m |          -0.0145 |          21.5524 |          15.5189 |
[32m[20221213 22:45:50 @agent_ppo2.py:185][0m |          -0.0085 |          21.4651 |          15.5375 |
[32m[20221213 22:45:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:45:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.41
[32m[20221213 22:45:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.80
[32m[20221213 22:45:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.72
[32m[20221213 22:45:50 @agent_ppo2.py:143][0m Total time:      27.62 min
[32m[20221213 22:45:50 @agent_ppo2.py:145][0m 2697216 total steps have happened
[32m[20221213 22:45:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1317 --------------------------#
[32m[20221213 22:45:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |           0.0020 |          41.2953 |          15.6832 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0064 |          37.8765 |          15.6703 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0032 |          36.5715 |          15.6706 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0042 |          35.8055 |          15.6691 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0090 |          35.3888 |          15.6645 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0107 |          35.1671 |          15.6725 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0087 |          34.8729 |          15.6719 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0070 |          34.9178 |          15.6659 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0086 |          34.6042 |          15.6586 |
[32m[20221213 22:45:51 @agent_ppo2.py:185][0m |          -0.0070 |          34.4200 |          15.6584 |
[32m[20221213 22:45:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.43
[32m[20221213 22:45:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.13
[32m[20221213 22:45:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.75
[32m[20221213 22:45:51 @agent_ppo2.py:143][0m Total time:      27.64 min
[32m[20221213 22:45:51 @agent_ppo2.py:145][0m 2699264 total steps have happened
[32m[20221213 22:45:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1318 --------------------------#
[32m[20221213 22:45:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |           0.0100 |          28.1714 |          15.5953 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |           0.0019 |          22.5617 |          15.5690 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0085 |          21.8485 |          15.5517 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0018 |          21.4973 |          15.5578 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0066 |          21.1696 |          15.5389 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0102 |          20.9641 |          15.5429 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0081 |          20.8017 |          15.5513 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0110 |          20.7159 |          15.5318 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0114 |          20.5466 |          15.5263 |
[32m[20221213 22:45:52 @agent_ppo2.py:185][0m |          -0.0083 |          20.4971 |          15.5259 |
[32m[20221213 22:45:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:45:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.47
[32m[20221213 22:45:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.12
[32m[20221213 22:45:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.70
[32m[20221213 22:45:53 @agent_ppo2.py:143][0m Total time:      27.66 min
[32m[20221213 22:45:53 @agent_ppo2.py:145][0m 2701312 total steps have happened
[32m[20221213 22:45:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1319 --------------------------#
[32m[20221213 22:45:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:53 @agent_ppo2.py:185][0m |          -0.0001 |          28.1326 |          15.5921 |
[32m[20221213 22:45:53 @agent_ppo2.py:185][0m |          -0.0054 |          25.4514 |          15.5770 |
[32m[20221213 22:45:53 @agent_ppo2.py:185][0m |          -0.0082 |          24.9037 |          15.5734 |
[32m[20221213 22:45:53 @agent_ppo2.py:185][0m |          -0.0082 |          24.5402 |          15.5682 |
[32m[20221213 22:45:53 @agent_ppo2.py:185][0m |          -0.0028 |          24.7548 |          15.5580 |
[32m[20221213 22:45:53 @agent_ppo2.py:185][0m |          -0.0087 |          24.0728 |          15.5601 |
[32m[20221213 22:45:53 @agent_ppo2.py:185][0m |          -0.0066 |          24.2613 |          15.5612 |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |          -0.0086 |          23.8558 |          15.5440 |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |          -0.0070 |          23.8595 |          15.5408 |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |          -0.0087 |          23.4128 |          15.5646 |
[32m[20221213 22:45:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:45:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.98
[32m[20221213 22:45:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.36
[32m[20221213 22:45:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.43
[32m[20221213 22:45:54 @agent_ppo2.py:143][0m Total time:      27.68 min
[32m[20221213 22:45:54 @agent_ppo2.py:145][0m 2703360 total steps have happened
[32m[20221213 22:45:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1320 --------------------------#
[32m[20221213 22:45:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:45:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |           0.0005 |          34.2183 |          15.5819 |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |          -0.0052 |          31.1786 |          15.5746 |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |          -0.0082 |          30.0944 |          15.5777 |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |          -0.0091 |          29.3907 |          15.5773 |
[32m[20221213 22:45:54 @agent_ppo2.py:185][0m |          -0.0009 |          29.9428 |          15.5672 |
[32m[20221213 22:45:55 @agent_ppo2.py:185][0m |          -0.0177 |          28.7248 |          15.5743 |
[32m[20221213 22:45:55 @agent_ppo2.py:185][0m |          -0.0145 |          28.2862 |          15.5675 |
[32m[20221213 22:45:55 @agent_ppo2.py:185][0m |          -0.0181 |          28.1772 |          15.5662 |
[32m[20221213 22:45:55 @agent_ppo2.py:185][0m |          -0.0116 |          27.9113 |          15.5693 |
[32m[20221213 22:45:55 @agent_ppo2.py:185][0m |          -0.0144 |          27.7123 |          15.5714 |
[32m[20221213 22:45:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:45:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.15
[32m[20221213 22:45:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 371.93
[32m[20221213 22:45:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.50
[32m[20221213 22:45:55 @agent_ppo2.py:143][0m Total time:      27.70 min
[32m[20221213 22:45:55 @agent_ppo2.py:145][0m 2705408 total steps have happened
[32m[20221213 22:45:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1321 --------------------------#
[32m[20221213 22:45:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:55 @agent_ppo2.py:185][0m |          -0.0033 |          30.7396 |          15.7735 |
[32m[20221213 22:45:55 @agent_ppo2.py:185][0m |           0.0001 |          29.6290 |          15.7444 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0062 |          29.1566 |          15.7421 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0074 |          28.7647 |          15.7518 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0078 |          28.6557 |          15.7343 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0121 |          28.4924 |          15.7385 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0096 |          28.1667 |          15.7301 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0092 |          28.0305 |          15.7368 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0107 |          27.9876 |          15.7268 |
[32m[20221213 22:45:56 @agent_ppo2.py:185][0m |          -0.0075 |          27.7873 |          15.7447 |
[32m[20221213 22:45:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:45:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.53
[32m[20221213 22:45:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.75
[32m[20221213 22:45:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.22
[32m[20221213 22:45:56 @agent_ppo2.py:143][0m Total time:      27.72 min
[32m[20221213 22:45:56 @agent_ppo2.py:145][0m 2707456 total steps have happened
[32m[20221213 22:45:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1322 --------------------------#
[32m[20221213 22:45:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:45:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0021 |          26.2054 |          15.5931 |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0042 |          23.0871 |          15.5761 |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0080 |          21.9868 |          15.5717 |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0126 |          21.3560 |          15.5727 |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0134 |          20.8528 |          15.5682 |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0090 |          20.4856 |          15.5703 |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0094 |          20.1050 |          15.5708 |
[32m[20221213 22:45:57 @agent_ppo2.py:185][0m |          -0.0145 |          19.9607 |          15.5697 |
[32m[20221213 22:45:58 @agent_ppo2.py:185][0m |          -0.0068 |          19.9757 |          15.5590 |
[32m[20221213 22:45:58 @agent_ppo2.py:185][0m |          -0.0146 |          19.6201 |          15.5660 |
[32m[20221213 22:45:58 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221213 22:45:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.18
[32m[20221213 22:45:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.11
[32m[20221213 22:45:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.01
[32m[20221213 22:45:58 @agent_ppo2.py:143][0m Total time:      27.75 min
[32m[20221213 22:45:58 @agent_ppo2.py:145][0m 2709504 total steps have happened
[32m[20221213 22:45:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1323 --------------------------#
[32m[20221213 22:45:58 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 22:45:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:45:58 @agent_ppo2.py:185][0m |           0.0070 |          33.3763 |          15.6922 |
[32m[20221213 22:45:58 @agent_ppo2.py:185][0m |          -0.0033 |          30.6651 |          15.6736 |
[32m[20221213 22:45:58 @agent_ppo2.py:185][0m |          -0.0003 |          31.2653 |          15.6677 |
[32m[20221213 22:45:59 @agent_ppo2.py:185][0m |          -0.0058 |          29.5391 |          15.6608 |
[32m[20221213 22:45:59 @agent_ppo2.py:185][0m |          -0.0040 |          29.5409 |          15.6610 |
[32m[20221213 22:45:59 @agent_ppo2.py:185][0m |          -0.0072 |          28.9491 |          15.6631 |
[32m[20221213 22:45:59 @agent_ppo2.py:185][0m |          -0.0032 |          30.6850 |          15.6607 |
[32m[20221213 22:45:59 @agent_ppo2.py:185][0m |           0.0061 |          32.9975 |          15.6562 |
[32m[20221213 22:45:59 @agent_ppo2.py:185][0m |          -0.0129 |          28.6204 |          15.6551 |
[32m[20221213 22:45:59 @agent_ppo2.py:185][0m |          -0.0131 |          28.3098 |          15.6429 |
[32m[20221213 22:45:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:45:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.62
[32m[20221213 22:45:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.28
[32m[20221213 22:45:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.78
[32m[20221213 22:45:59 @agent_ppo2.py:143][0m Total time:      27.77 min
[32m[20221213 22:45:59 @agent_ppo2.py:145][0m 2711552 total steps have happened
[32m[20221213 22:45:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1324 --------------------------#
[32m[20221213 22:45:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:45:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |           0.0002 |          34.5201 |          15.5178 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0061 |          31.0054 |          15.5124 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0078 |          29.8002 |          15.5106 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0061 |          28.9284 |          15.5067 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0105 |          28.3560 |          15.5087 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0106 |          28.0583 |          15.5049 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0108 |          27.5508 |          15.5043 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0050 |          27.2168 |          15.5092 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0099 |          26.8614 |          15.4900 |
[32m[20221213 22:46:00 @agent_ppo2.py:185][0m |          -0.0141 |          26.7756 |          15.5113 |
[32m[20221213 22:46:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.69
[32m[20221213 22:46:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.45
[32m[20221213 22:46:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.99
[32m[20221213 22:46:00 @agent_ppo2.py:143][0m Total time:      27.79 min
[32m[20221213 22:46:00 @agent_ppo2.py:145][0m 2713600 total steps have happened
[32m[20221213 22:46:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1325 --------------------------#
[32m[20221213 22:46:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |           0.0004 |          32.0849 |          15.7994 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0031 |          27.8047 |          15.7754 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0080 |          26.5124 |          15.7762 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0144 |          25.9344 |          15.7809 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0123 |          25.5068 |          15.7751 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0063 |          26.7464 |          15.7756 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0099 |          24.7409 |          15.7702 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0151 |          24.5207 |          15.7726 |
[32m[20221213 22:46:01 @agent_ppo2.py:185][0m |          -0.0184 |          24.4364 |          15.7685 |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |          -0.0042 |          24.1511 |          15.7660 |
[32m[20221213 22:46:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.67
[32m[20221213 22:46:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.17
[32m[20221213 22:46:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.36
[32m[20221213 22:46:02 @agent_ppo2.py:143][0m Total time:      27.81 min
[32m[20221213 22:46:02 @agent_ppo2.py:145][0m 2715648 total steps have happened
[32m[20221213 22:46:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1326 --------------------------#
[32m[20221213 22:46:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |           0.0001 |          40.1338 |          15.6627 |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |          -0.0043 |          36.6052 |          15.6654 |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |          -0.0088 |          35.4914 |          15.6489 |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |          -0.0108 |          34.5879 |          15.6460 |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |          -0.0094 |          34.0536 |          15.6440 |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |          -0.0060 |          33.6832 |          15.6459 |
[32m[20221213 22:46:02 @agent_ppo2.py:185][0m |          -0.0110 |          33.3386 |          15.6418 |
[32m[20221213 22:46:03 @agent_ppo2.py:185][0m |          -0.0126 |          33.0629 |          15.6447 |
[32m[20221213 22:46:03 @agent_ppo2.py:185][0m |          -0.0056 |          35.1778 |          15.6347 |
[32m[20221213 22:46:03 @agent_ppo2.py:185][0m |          -0.0113 |          32.5922 |          15.6382 |
[32m[20221213 22:46:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.01
[32m[20221213 22:46:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.31
[32m[20221213 22:46:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.16
[32m[20221213 22:46:03 @agent_ppo2.py:143][0m Total time:      27.83 min
[32m[20221213 22:46:03 @agent_ppo2.py:145][0m 2717696 total steps have happened
[32m[20221213 22:46:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1327 --------------------------#
[32m[20221213 22:46:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:03 @agent_ppo2.py:185][0m |           0.0078 |          33.5321 |          15.5218 |
[32m[20221213 22:46:03 @agent_ppo2.py:185][0m |          -0.0047 |          31.2750 |          15.5014 |
[32m[20221213 22:46:03 @agent_ppo2.py:185][0m |          -0.0065 |          30.6660 |          15.4792 |
[32m[20221213 22:46:03 @agent_ppo2.py:185][0m |           0.0033 |          32.0742 |          15.4710 |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |          -0.0090 |          30.0073 |          15.4702 |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |          -0.0115 |          29.5785 |          15.4589 |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |          -0.0068 |          29.4920 |          15.4489 |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |          -0.0072 |          29.3425 |          15.4511 |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |          -0.0074 |          29.3833 |          15.4427 |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |          -0.0126 |          28.9730 |          15.4351 |
[32m[20221213 22:46:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.99
[32m[20221213 22:46:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.93
[32m[20221213 22:46:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.57
[32m[20221213 22:46:04 @agent_ppo2.py:143][0m Total time:      27.85 min
[32m[20221213 22:46:04 @agent_ppo2.py:145][0m 2719744 total steps have happened
[32m[20221213 22:46:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1328 --------------------------#
[32m[20221213 22:46:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |           0.0010 |          30.8721 |          15.6041 |
[32m[20221213 22:46:04 @agent_ppo2.py:185][0m |          -0.0044 |          26.6773 |          15.6049 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |          -0.0095 |          25.6162 |          15.6136 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |          -0.0066 |          24.8653 |          15.6063 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |          -0.0056 |          24.3470 |          15.6154 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |          -0.0083 |          24.2246 |          15.6013 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |          -0.0083 |          23.9007 |          15.6001 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |           0.0024 |          24.9821 |          15.6127 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |          -0.0128 |          23.2860 |          15.5861 |
[32m[20221213 22:46:05 @agent_ppo2.py:185][0m |          -0.0090 |          23.0247 |          15.5975 |
[32m[20221213 22:46:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.56
[32m[20221213 22:46:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.48
[32m[20221213 22:46:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.01
[32m[20221213 22:46:05 @agent_ppo2.py:143][0m Total time:      27.87 min
[32m[20221213 22:46:05 @agent_ppo2.py:145][0m 2721792 total steps have happened
[32m[20221213 22:46:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1329 --------------------------#
[32m[20221213 22:46:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0026 |          47.8522 |          15.7595 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0065 |          45.4093 |          15.7537 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0051 |          44.4133 |          15.7441 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0094 |          43.6912 |          15.7453 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0050 |          43.4790 |          15.7468 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0108 |          42.6613 |          15.7403 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0114 |          42.3665 |          15.7389 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0118 |          42.0785 |          15.7424 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |           0.0013 |          47.1282 |          15.7400 |
[32m[20221213 22:46:06 @agent_ppo2.py:185][0m |          -0.0139 |          41.4832 |          15.7275 |
[32m[20221213 22:46:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.84
[32m[20221213 22:46:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.64
[32m[20221213 22:46:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.77
[32m[20221213 22:46:06 @agent_ppo2.py:143][0m Total time:      27.89 min
[32m[20221213 22:46:06 @agent_ppo2.py:145][0m 2723840 total steps have happened
[32m[20221213 22:46:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1330 --------------------------#
[32m[20221213 22:46:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:46:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |           0.0015 |          29.9751 |          15.6336 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0073 |          27.0596 |          15.6174 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0019 |          26.6262 |          15.6212 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0110 |          25.8889 |          15.6197 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0011 |          25.6961 |          15.6240 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0062 |          25.3291 |          15.6148 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0102 |          25.1922 |          15.6053 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0073 |          24.9525 |          15.6224 |
[32m[20221213 22:46:07 @agent_ppo2.py:185][0m |          -0.0037 |          26.0253 |          15.6079 |
[32m[20221213 22:46:08 @agent_ppo2.py:185][0m |          -0.0091 |          24.7362 |          15.6082 |
[32m[20221213 22:46:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.13
[32m[20221213 22:46:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.27
[32m[20221213 22:46:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 459.87
[32m[20221213 22:46:08 @agent_ppo2.py:143][0m Total time:      27.91 min
[32m[20221213 22:46:08 @agent_ppo2.py:145][0m 2725888 total steps have happened
[32m[20221213 22:46:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1331 --------------------------#
[32m[20221213 22:46:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:08 @agent_ppo2.py:185][0m |           0.0011 |          44.3080 |          15.5918 |
[32m[20221213 22:46:08 @agent_ppo2.py:185][0m |          -0.0010 |          42.7091 |          15.5829 |
[32m[20221213 22:46:08 @agent_ppo2.py:185][0m |          -0.0030 |          39.6823 |          15.5786 |
[32m[20221213 22:46:08 @agent_ppo2.py:185][0m |          -0.0065 |          37.9519 |          15.5874 |
[32m[20221213 22:46:08 @agent_ppo2.py:185][0m |          -0.0071 |          37.4799 |          15.5913 |
[32m[20221213 22:46:08 @agent_ppo2.py:185][0m |          -0.0074 |          36.7592 |          15.5945 |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |          -0.0051 |          37.0773 |          15.5974 |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |          -0.0091 |          36.0077 |          15.5863 |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |          -0.0114 |          35.7820 |          15.5942 |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |          -0.0109 |          35.5989 |          15.5848 |
[32m[20221213 22:46:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.64
[32m[20221213 22:46:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.75
[32m[20221213 22:46:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.15
[32m[20221213 22:46:09 @agent_ppo2.py:143][0m Total time:      27.93 min
[32m[20221213 22:46:09 @agent_ppo2.py:145][0m 2727936 total steps have happened
[32m[20221213 22:46:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1332 --------------------------#
[32m[20221213 22:46:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |           0.0021 |          45.3980 |          15.6192 |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |          -0.0031 |          41.6031 |          15.5912 |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |          -0.0062 |          39.9448 |          15.5975 |
[32m[20221213 22:46:09 @agent_ppo2.py:185][0m |          -0.0009 |          39.2583 |          15.5891 |
[32m[20221213 22:46:10 @agent_ppo2.py:185][0m |          -0.0084 |          38.3929 |          15.5853 |
[32m[20221213 22:46:10 @agent_ppo2.py:185][0m |          -0.0100 |          37.4021 |          15.5826 |
[32m[20221213 22:46:10 @agent_ppo2.py:185][0m |          -0.0107 |          36.9236 |          15.5775 |
[32m[20221213 22:46:10 @agent_ppo2.py:185][0m |          -0.0120 |          36.4571 |          15.5687 |
[32m[20221213 22:46:10 @agent_ppo2.py:185][0m |          -0.0042 |          36.5551 |          15.5624 |
[32m[20221213 22:46:10 @agent_ppo2.py:185][0m |          -0.0093 |          35.5164 |          15.5693 |
[32m[20221213 22:46:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.56
[32m[20221213 22:46:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.34
[32m[20221213 22:46:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.14
[32m[20221213 22:46:10 @agent_ppo2.py:143][0m Total time:      27.95 min
[32m[20221213 22:46:10 @agent_ppo2.py:145][0m 2729984 total steps have happened
[32m[20221213 22:46:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1333 --------------------------#
[32m[20221213 22:46:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:10 @agent_ppo2.py:185][0m |          -0.0016 |          26.6225 |          15.6435 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0019 |          21.9284 |          15.6435 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0062 |          20.4625 |          15.6328 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0001 |          19.6972 |          15.6263 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0061 |          18.7009 |          15.6228 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0012 |          18.3332 |          15.6085 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0023 |          17.8845 |          15.6113 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0077 |          17.4574 |          15.6143 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0102 |          17.1278 |          15.6027 |
[32m[20221213 22:46:11 @agent_ppo2.py:185][0m |          -0.0133 |          16.7416 |          15.6005 |
[32m[20221213 22:46:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.18
[32m[20221213 22:46:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.01
[32m[20221213 22:46:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.29
[32m[20221213 22:46:11 @agent_ppo2.py:143][0m Total time:      27.97 min
[32m[20221213 22:46:11 @agent_ppo2.py:145][0m 2732032 total steps have happened
[32m[20221213 22:46:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1334 --------------------------#
[32m[20221213 22:46:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |           0.0036 |          28.9482 |          15.8005 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0052 |          26.2653 |          15.8016 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0054 |          25.4920 |          15.7797 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0050 |          24.8756 |          15.7751 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0040 |          24.6601 |          15.7908 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |           0.0018 |          24.8607 |          15.7955 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0061 |          24.2293 |          15.7801 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0078 |          23.9890 |          15.7867 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0061 |          23.9210 |          15.7870 |
[32m[20221213 22:46:12 @agent_ppo2.py:185][0m |          -0.0050 |          23.9023 |          15.7829 |
[32m[20221213 22:46:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.25
[32m[20221213 22:46:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.23
[32m[20221213 22:46:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.25
[32m[20221213 22:46:13 @agent_ppo2.py:143][0m Total time:      27.99 min
[32m[20221213 22:46:13 @agent_ppo2.py:145][0m 2734080 total steps have happened
[32m[20221213 22:46:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1335 --------------------------#
[32m[20221213 22:46:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0033 |          33.5688 |          15.7820 |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0087 |          29.7945 |          15.7722 |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0081 |          29.0823 |          15.7732 |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0113 |          28.2460 |          15.7691 |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0065 |          28.1479 |          15.7508 |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0129 |          27.4961 |          15.7522 |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0141 |          27.3168 |          15.7515 |
[32m[20221213 22:46:13 @agent_ppo2.py:185][0m |          -0.0143 |          27.0309 |          15.7533 |
[32m[20221213 22:46:14 @agent_ppo2.py:185][0m |          -0.0181 |          26.9261 |          15.7431 |
[32m[20221213 22:46:14 @agent_ppo2.py:185][0m |          -0.0182 |          26.5651 |          15.7294 |
[32m[20221213 22:46:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.17
[32m[20221213 22:46:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.06
[32m[20221213 22:46:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.14
[32m[20221213 22:46:14 @agent_ppo2.py:143][0m Total time:      28.01 min
[32m[20221213 22:46:14 @agent_ppo2.py:145][0m 2736128 total steps have happened
[32m[20221213 22:46:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1336 --------------------------#
[32m[20221213 22:46:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:14 @agent_ppo2.py:185][0m |           0.0076 |          48.5538 |          15.5953 |
[32m[20221213 22:46:14 @agent_ppo2.py:185][0m |          -0.0019 |          44.6302 |          15.5956 |
[32m[20221213 22:46:14 @agent_ppo2.py:185][0m |          -0.0072 |          43.2847 |          15.5730 |
[32m[20221213 22:46:14 @agent_ppo2.py:185][0m |          -0.0118 |          42.9954 |          15.5785 |
[32m[20221213 22:46:14 @agent_ppo2.py:185][0m |          -0.0023 |          45.6047 |          15.5814 |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |          -0.0132 |          42.5675 |          15.5711 |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |          -0.0113 |          42.2312 |          15.5784 |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |          -0.0025 |          44.6292 |          15.5760 |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |          -0.0096 |          42.0861 |          15.5715 |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |          -0.0123 |          41.9105 |          15.5679 |
[32m[20221213 22:46:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.09
[32m[20221213 22:46:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.20
[32m[20221213 22:46:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.13
[32m[20221213 22:46:15 @agent_ppo2.py:143][0m Total time:      28.03 min
[32m[20221213 22:46:15 @agent_ppo2.py:145][0m 2738176 total steps have happened
[32m[20221213 22:46:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1337 --------------------------#
[32m[20221213 22:46:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |           0.0007 |          32.9993 |          15.7869 |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |          -0.0010 |          28.0312 |          15.7750 |
[32m[20221213 22:46:15 @agent_ppo2.py:185][0m |          -0.0021 |          27.0581 |          15.7658 |
[32m[20221213 22:46:16 @agent_ppo2.py:185][0m |          -0.0075 |          25.9519 |          15.7652 |
[32m[20221213 22:46:16 @agent_ppo2.py:185][0m |          -0.0116 |          25.5130 |          15.7537 |
[32m[20221213 22:46:16 @agent_ppo2.py:185][0m |          -0.0109 |          24.8624 |          15.7555 |
[32m[20221213 22:46:16 @agent_ppo2.py:185][0m |          -0.0113 |          24.6177 |          15.7570 |
[32m[20221213 22:46:16 @agent_ppo2.py:185][0m |          -0.0103 |          24.2535 |          15.7535 |
[32m[20221213 22:46:16 @agent_ppo2.py:185][0m |          -0.0133 |          24.0130 |          15.7529 |
[32m[20221213 22:46:16 @agent_ppo2.py:185][0m |          -0.0108 |          23.7775 |          15.7477 |
[32m[20221213 22:46:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:46:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.87
[32m[20221213 22:46:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 334.03
[32m[20221213 22:46:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.45
[32m[20221213 22:46:16 @agent_ppo2.py:143][0m Total time:      28.06 min
[32m[20221213 22:46:16 @agent_ppo2.py:145][0m 2740224 total steps have happened
[32m[20221213 22:46:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1338 --------------------------#
[32m[20221213 22:46:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:46:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |           0.0017 |          44.7145 |          15.6437 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0032 |          43.0177 |          15.6478 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0064 |          42.7044 |          15.6179 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0043 |          42.5060 |          15.6113 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0066 |          42.3239 |          15.6277 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0083 |          42.2181 |          15.6156 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0102 |          42.0972 |          15.6248 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0091 |          41.9330 |          15.6090 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0094 |          41.9018 |          15.6169 |
[32m[20221213 22:46:17 @agent_ppo2.py:185][0m |          -0.0096 |          41.7880 |          15.6312 |
[32m[20221213 22:46:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:46:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.45
[32m[20221213 22:46:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.91
[32m[20221213 22:46:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.65
[32m[20221213 22:46:17 @agent_ppo2.py:143][0m Total time:      28.08 min
[32m[20221213 22:46:17 @agent_ppo2.py:145][0m 2742272 total steps have happened
[32m[20221213 22:46:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1339 --------------------------#
[32m[20221213 22:46:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |           0.0020 |          27.0624 |          15.6544 |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |          -0.0059 |          23.9418 |          15.6359 |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |          -0.0058 |          23.1431 |          15.6080 |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |          -0.0118 |          22.3681 |          15.5982 |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |          -0.0137 |          22.0426 |          15.6077 |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |          -0.0103 |          21.6843 |          15.6013 |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |          -0.0105 |          21.6954 |          15.6034 |
[32m[20221213 22:46:18 @agent_ppo2.py:185][0m |          -0.0096 |          21.6442 |          15.5987 |
[32m[20221213 22:46:19 @agent_ppo2.py:185][0m |          -0.0152 |          21.1446 |          15.5937 |
[32m[20221213 22:46:19 @agent_ppo2.py:185][0m |          -0.0117 |          20.8776 |          15.5878 |
[32m[20221213 22:46:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:46:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.78
[32m[20221213 22:46:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.36
[32m[20221213 22:46:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.18
[32m[20221213 22:46:19 @agent_ppo2.py:143][0m Total time:      28.10 min
[32m[20221213 22:46:19 @agent_ppo2.py:145][0m 2744320 total steps have happened
[32m[20221213 22:46:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1340 --------------------------#
[32m[20221213 22:46:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:46:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:19 @agent_ppo2.py:185][0m |          -0.0007 |          27.6174 |          15.7263 |
[32m[20221213 22:46:19 @agent_ppo2.py:185][0m |          -0.0042 |          24.4091 |          15.7152 |
[32m[20221213 22:46:19 @agent_ppo2.py:185][0m |          -0.0079 |          23.3064 |          15.7124 |
[32m[20221213 22:46:19 @agent_ppo2.py:185][0m |          -0.0026 |          22.4040 |          15.7082 |
[32m[20221213 22:46:19 @agent_ppo2.py:185][0m |          -0.0107 |          21.9120 |          15.7072 |
[32m[20221213 22:46:20 @agent_ppo2.py:185][0m |          -0.0021 |          21.2733 |          15.6985 |
[32m[20221213 22:46:20 @agent_ppo2.py:185][0m |          -0.0110 |          20.8669 |          15.6976 |
[32m[20221213 22:46:20 @agent_ppo2.py:185][0m |          -0.0162 |          20.7308 |          15.6947 |
[32m[20221213 22:46:20 @agent_ppo2.py:185][0m |          -0.0099 |          20.3197 |          15.6863 |
[32m[20221213 22:46:20 @agent_ppo2.py:185][0m |          -0.0132 |          20.0518 |          15.6875 |
[32m[20221213 22:46:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:46:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 166.02
[32m[20221213 22:46:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 362.95
[32m[20221213 22:46:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.33
[32m[20221213 22:46:20 @agent_ppo2.py:143][0m Total time:      28.12 min
[32m[20221213 22:46:20 @agent_ppo2.py:145][0m 2746368 total steps have happened
[32m[20221213 22:46:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1341 --------------------------#
[32m[20221213 22:46:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:20 @agent_ppo2.py:185][0m |          -0.0024 |          44.0295 |          15.7211 |
[32m[20221213 22:46:20 @agent_ppo2.py:185][0m |          -0.0016 |          42.3551 |          15.7307 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0047 |          42.0324 |          15.7151 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0086 |          42.0760 |          15.6927 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0029 |          41.8264 |          15.6931 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0051 |          41.7224 |          15.6945 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0071 |          41.5671 |          15.6831 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0059 |          41.4732 |          15.6780 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0031 |          42.8751 |          15.6735 |
[32m[20221213 22:46:21 @agent_ppo2.py:185][0m |          -0.0015 |          42.1579 |          15.6721 |
[32m[20221213 22:46:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:46:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.85
[32m[20221213 22:46:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.71
[32m[20221213 22:46:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.99
[32m[20221213 22:46:21 @agent_ppo2.py:143][0m Total time:      28.14 min
[32m[20221213 22:46:21 @agent_ppo2.py:145][0m 2748416 total steps have happened
[32m[20221213 22:46:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1342 --------------------------#
[32m[20221213 22:46:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0048 |          28.2694 |          15.7732 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0094 |          24.9074 |          15.7619 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0056 |          24.1821 |          15.7644 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0087 |          24.0091 |          15.7502 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0121 |          23.3572 |          15.7576 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0091 |          23.1026 |          15.7463 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0027 |          23.8888 |          15.7551 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0075 |          22.7853 |          15.7575 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0126 |          22.5789 |          15.7337 |
[32m[20221213 22:46:22 @agent_ppo2.py:185][0m |          -0.0149 |          22.7683 |          15.7504 |
[32m[20221213 22:46:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:46:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.94
[32m[20221213 22:46:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.40
[32m[20221213 22:46:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.23
[32m[20221213 22:46:23 @agent_ppo2.py:143][0m Total time:      28.16 min
[32m[20221213 22:46:23 @agent_ppo2.py:145][0m 2750464 total steps have happened
[32m[20221213 22:46:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1343 --------------------------#
[32m[20221213 22:46:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |           0.0056 |          38.4535 |          15.7513 |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |          -0.0032 |          34.3149 |          15.7466 |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |          -0.0054 |          33.4231 |          15.7409 |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |          -0.0040 |          33.3106 |          15.7313 |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |          -0.0071 |          32.7316 |          15.7301 |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |          -0.0027 |          32.4634 |          15.7291 |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |          -0.0025 |          33.7678 |          15.7369 |
[32m[20221213 22:46:23 @agent_ppo2.py:185][0m |          -0.0081 |          32.2499 |          15.7120 |
[32m[20221213 22:46:24 @agent_ppo2.py:185][0m |          -0.0119 |          32.1003 |          15.7336 |
[32m[20221213 22:46:24 @agent_ppo2.py:185][0m |          -0.0106 |          32.0748 |          15.7260 |
[32m[20221213 22:46:24 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.18
[32m[20221213 22:46:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.62
[32m[20221213 22:46:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.64
[32m[20221213 22:46:24 @agent_ppo2.py:143][0m Total time:      28.18 min
[32m[20221213 22:46:24 @agent_ppo2.py:145][0m 2752512 total steps have happened
[32m[20221213 22:46:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1344 --------------------------#
[32m[20221213 22:46:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:24 @agent_ppo2.py:185][0m |          -0.0041 |          31.3688 |          15.7741 |
[32m[20221213 22:46:24 @agent_ppo2.py:185][0m |          -0.0134 |          23.9165 |          15.7583 |
[32m[20221213 22:46:24 @agent_ppo2.py:185][0m |          -0.0118 |          23.0888 |          15.7619 |
[32m[20221213 22:46:24 @agent_ppo2.py:185][0m |          -0.0131 |          22.6142 |          15.7432 |
[32m[20221213 22:46:24 @agent_ppo2.py:185][0m |          -0.0119 |          22.1878 |          15.7597 |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |          -0.0149 |          22.0468 |          15.7677 |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |          -0.0167 |          21.6731 |          15.7510 |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |          -0.0079 |          21.6032 |          15.7418 |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |          -0.0171 |          21.4630 |          15.7492 |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |          -0.0164 |          21.4663 |          15.7447 |
[32m[20221213 22:46:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.39
[32m[20221213 22:46:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.02
[32m[20221213 22:46:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.11
[32m[20221213 22:46:25 @agent_ppo2.py:143][0m Total time:      28.20 min
[32m[20221213 22:46:25 @agent_ppo2.py:145][0m 2754560 total steps have happened
[32m[20221213 22:46:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1345 --------------------------#
[32m[20221213 22:46:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |           0.0019 |          25.9125 |          15.7013 |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |          -0.0045 |          22.6642 |          15.6881 |
[32m[20221213 22:46:25 @agent_ppo2.py:185][0m |          -0.0084 |          22.1661 |          15.6877 |
[32m[20221213 22:46:26 @agent_ppo2.py:185][0m |          -0.0073 |          21.7635 |          15.6895 |
[32m[20221213 22:46:26 @agent_ppo2.py:185][0m |          -0.0049 |          21.3947 |          15.6849 |
[32m[20221213 22:46:26 @agent_ppo2.py:185][0m |          -0.0115 |          21.1572 |          15.6891 |
[32m[20221213 22:46:26 @agent_ppo2.py:185][0m |          -0.0110 |          20.9967 |          15.6800 |
[32m[20221213 22:46:26 @agent_ppo2.py:185][0m |          -0.0085 |          20.7406 |          15.6823 |
[32m[20221213 22:46:26 @agent_ppo2.py:185][0m |          -0.0131 |          20.7084 |          15.6880 |
[32m[20221213 22:46:26 @agent_ppo2.py:185][0m |          -0.0158 |          20.5037 |          15.6876 |
[32m[20221213 22:46:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.14
[32m[20221213 22:46:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.85
[32m[20221213 22:46:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.10
[32m[20221213 22:46:26 @agent_ppo2.py:143][0m Total time:      28.22 min
[32m[20221213 22:46:26 @agent_ppo2.py:145][0m 2756608 total steps have happened
[32m[20221213 22:46:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1346 --------------------------#
[32m[20221213 22:46:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0002 |          21.4473 |          15.7670 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0039 |          18.7271 |          15.7459 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0076 |          17.6896 |          15.7331 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0083 |          17.1906 |          15.7247 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0032 |          16.8336 |          15.7082 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0145 |          16.5335 |          15.7232 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0111 |          16.4584 |          15.7047 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0098 |          16.2755 |          15.7121 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0126 |          16.0652 |          15.7145 |
[32m[20221213 22:46:27 @agent_ppo2.py:185][0m |          -0.0057 |          15.9085 |          15.6921 |
[32m[20221213 22:46:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.40
[32m[20221213 22:46:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.06
[32m[20221213 22:46:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.20
[32m[20221213 22:46:27 @agent_ppo2.py:143][0m Total time:      28.24 min
[32m[20221213 22:46:27 @agent_ppo2.py:145][0m 2758656 total steps have happened
[32m[20221213 22:46:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1347 --------------------------#
[32m[20221213 22:46:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0030 |          26.2973 |          15.5294 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0062 |          23.4859 |          15.5136 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0050 |          23.1807 |          15.5057 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0045 |          22.4784 |          15.5095 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0121 |          22.0215 |          15.5176 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0111 |          21.9651 |          15.5021 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0117 |          21.6099 |          15.5172 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0083 |          21.7445 |          15.4954 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0111 |          21.2819 |          15.5124 |
[32m[20221213 22:46:28 @agent_ppo2.py:185][0m |          -0.0125 |          21.2976 |          15.5071 |
[32m[20221213 22:46:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.35
[32m[20221213 22:46:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 364.12
[32m[20221213 22:46:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.07
[32m[20221213 22:46:29 @agent_ppo2.py:143][0m Total time:      28.26 min
[32m[20221213 22:46:29 @agent_ppo2.py:145][0m 2760704 total steps have happened
[32m[20221213 22:46:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1348 --------------------------#
[32m[20221213 22:46:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:29 @agent_ppo2.py:185][0m |          -0.0000 |          26.1243 |          15.7181 |
[32m[20221213 22:46:29 @agent_ppo2.py:185][0m |          -0.0049 |          23.6759 |          15.6952 |
[32m[20221213 22:46:29 @agent_ppo2.py:185][0m |           0.0029 |          24.1112 |          15.7048 |
[32m[20221213 22:46:29 @agent_ppo2.py:185][0m |          -0.0107 |          22.4993 |          15.7055 |
[32m[20221213 22:46:29 @agent_ppo2.py:185][0m |          -0.0026 |          22.1825 |          15.7115 |
[32m[20221213 22:46:29 @agent_ppo2.py:185][0m |          -0.0094 |          21.6593 |          15.7077 |
[32m[20221213 22:46:29 @agent_ppo2.py:185][0m |          -0.0087 |          21.5375 |          15.7256 |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |          -0.0071 |          21.0540 |          15.7079 |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |          -0.0091 |          20.9075 |          15.7194 |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |          -0.0068 |          20.7979 |          15.7178 |
[32m[20221213 22:46:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.30
[32m[20221213 22:46:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.13
[32m[20221213 22:46:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.08
[32m[20221213 22:46:30 @agent_ppo2.py:143][0m Total time:      28.28 min
[32m[20221213 22:46:30 @agent_ppo2.py:145][0m 2762752 total steps have happened
[32m[20221213 22:46:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1349 --------------------------#
[32m[20221213 22:46:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |           0.0009 |          24.5212 |          15.6716 |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |          -0.0043 |          22.4214 |          15.6541 |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |          -0.0086 |          21.8367 |          15.6454 |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |          -0.0126 |          21.4252 |          15.6409 |
[32m[20221213 22:46:30 @agent_ppo2.py:185][0m |          -0.0041 |          21.3213 |          15.6274 |
[32m[20221213 22:46:31 @agent_ppo2.py:185][0m |          -0.0094 |          20.9238 |          15.6381 |
[32m[20221213 22:46:31 @agent_ppo2.py:185][0m |          -0.0080 |          20.7987 |          15.6343 |
[32m[20221213 22:46:31 @agent_ppo2.py:185][0m |          -0.0081 |          20.6918 |          15.6257 |
[32m[20221213 22:46:31 @agent_ppo2.py:185][0m |          -0.0159 |          20.7002 |          15.6210 |
[32m[20221213 22:46:31 @agent_ppo2.py:185][0m |          -0.0136 |          20.4293 |          15.6262 |
[32m[20221213 22:46:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.87
[32m[20221213 22:46:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.40
[32m[20221213 22:46:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.00
[32m[20221213 22:46:31 @agent_ppo2.py:143][0m Total time:      28.30 min
[32m[20221213 22:46:31 @agent_ppo2.py:145][0m 2764800 total steps have happened
[32m[20221213 22:46:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1350 --------------------------#
[32m[20221213 22:46:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:46:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:31 @agent_ppo2.py:185][0m |          -0.0028 |          36.4590 |          15.7971 |
[32m[20221213 22:46:31 @agent_ppo2.py:185][0m |          -0.0036 |          34.8517 |          15.7841 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0108 |          34.4035 |          15.7898 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0094 |          34.2613 |          15.7872 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0101 |          34.1765 |          15.7960 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0134 |          33.9953 |          15.7926 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0130 |          34.0621 |          15.7849 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0139 |          33.8346 |          15.7920 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0125 |          33.8093 |          15.7820 |
[32m[20221213 22:46:32 @agent_ppo2.py:185][0m |          -0.0080 |          33.7867 |          15.7861 |
[32m[20221213 22:46:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.76
[32m[20221213 22:46:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.62
[32m[20221213 22:46:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.53
[32m[20221213 22:46:32 @agent_ppo2.py:143][0m Total time:      28.32 min
[32m[20221213 22:46:32 @agent_ppo2.py:145][0m 2766848 total steps have happened
[32m[20221213 22:46:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1351 --------------------------#
[32m[20221213 22:46:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0003 |          21.8526 |          15.7270 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0069 |          19.3593 |          15.7137 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0113 |          18.3683 |          15.7030 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0083 |          17.7765 |          15.7126 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |           0.0015 |          19.8235 |          15.6995 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0058 |          17.2879 |          15.6994 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0101 |          16.9413 |          15.7039 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0124 |          16.6293 |          15.7018 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0136 |          16.4008 |          15.7012 |
[32m[20221213 22:46:33 @agent_ppo2.py:185][0m |          -0.0175 |          16.3548 |          15.7019 |
[32m[20221213 22:46:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.50
[32m[20221213 22:46:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.63
[32m[20221213 22:46:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.94
[32m[20221213 22:46:33 @agent_ppo2.py:143][0m Total time:      28.34 min
[32m[20221213 22:46:33 @agent_ppo2.py:145][0m 2768896 total steps have happened
[32m[20221213 22:46:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1352 --------------------------#
[32m[20221213 22:46:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |           0.0003 |          23.6009 |          15.6820 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0007 |          21.5932 |          15.6647 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0065 |          20.5602 |          15.6679 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0127 |          20.1337 |          15.6433 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0124 |          19.5728 |          15.6477 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0154 |          19.2567 |          15.6435 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0165 |          19.0189 |          15.6293 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0067 |          19.0845 |          15.6209 |
[32m[20221213 22:46:34 @agent_ppo2.py:185][0m |          -0.0129 |          18.4837 |          15.6196 |
[32m[20221213 22:46:35 @agent_ppo2.py:185][0m |          -0.0134 |          18.2623 |          15.6315 |
[32m[20221213 22:46:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.73
[32m[20221213 22:46:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.11
[32m[20221213 22:46:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.47
[32m[20221213 22:46:35 @agent_ppo2.py:143][0m Total time:      28.36 min
[32m[20221213 22:46:35 @agent_ppo2.py:145][0m 2770944 total steps have happened
[32m[20221213 22:46:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1353 --------------------------#
[32m[20221213 22:46:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:35 @agent_ppo2.py:185][0m |          -0.0023 |          37.9041 |          15.8429 |
[32m[20221213 22:46:35 @agent_ppo2.py:185][0m |          -0.0039 |          35.2484 |          15.8177 |
[32m[20221213 22:46:35 @agent_ppo2.py:185][0m |          -0.0094 |          35.0548 |          15.8242 |
[32m[20221213 22:46:35 @agent_ppo2.py:185][0m |          -0.0065 |          34.0020 |          15.8134 |
[32m[20221213 22:46:35 @agent_ppo2.py:185][0m |          -0.0053 |          33.7237 |          15.8185 |
[32m[20221213 22:46:35 @agent_ppo2.py:185][0m |          -0.0128 |          33.2407 |          15.8153 |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |          -0.0101 |          32.9331 |          15.7990 |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |          -0.0050 |          33.4422 |          15.7954 |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |          -0.0119 |          32.7063 |          15.8294 |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |          -0.0095 |          32.6264 |          15.7940 |
[32m[20221213 22:46:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.85
[32m[20221213 22:46:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.69
[32m[20221213 22:46:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.05
[32m[20221213 22:46:36 @agent_ppo2.py:143][0m Total time:      28.38 min
[32m[20221213 22:46:36 @agent_ppo2.py:145][0m 2772992 total steps have happened
[32m[20221213 22:46:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1354 --------------------------#
[32m[20221213 22:46:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |           0.0008 |          33.6184 |          15.7526 |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |          -0.0052 |          31.2102 |          15.7266 |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |          -0.0043 |          30.1767 |          15.7324 |
[32m[20221213 22:46:36 @agent_ppo2.py:185][0m |          -0.0096 |          29.6005 |          15.7277 |
[32m[20221213 22:46:37 @agent_ppo2.py:185][0m |          -0.0065 |          29.0294 |          15.7131 |
[32m[20221213 22:46:37 @agent_ppo2.py:185][0m |          -0.0096 |          28.7710 |          15.7088 |
[32m[20221213 22:46:37 @agent_ppo2.py:185][0m |          -0.0067 |          28.9490 |          15.7008 |
[32m[20221213 22:46:37 @agent_ppo2.py:185][0m |          -0.0076 |          28.2064 |          15.7064 |
[32m[20221213 22:46:37 @agent_ppo2.py:185][0m |          -0.0064 |          28.5715 |          15.7046 |
[32m[20221213 22:46:37 @agent_ppo2.py:185][0m |          -0.0127 |          27.6554 |          15.6934 |
[32m[20221213 22:46:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.50
[32m[20221213 22:46:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.91
[32m[20221213 22:46:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.31
[32m[20221213 22:46:37 @agent_ppo2.py:143][0m Total time:      28.40 min
[32m[20221213 22:46:37 @agent_ppo2.py:145][0m 2775040 total steps have happened
[32m[20221213 22:46:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1355 --------------------------#
[32m[20221213 22:46:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:37 @agent_ppo2.py:185][0m |          -0.0032 |          37.9802 |          15.8167 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0032 |          36.1723 |          15.8079 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0054 |          35.7369 |          15.8164 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0048 |          35.5318 |          15.8156 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0081 |          35.2262 |          15.8044 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0042 |          35.0786 |          15.7930 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0078 |          35.1876 |          15.7965 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0075 |          34.8584 |          15.7809 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0057 |          35.1754 |          15.7941 |
[32m[20221213 22:46:38 @agent_ppo2.py:185][0m |          -0.0088 |          34.5841 |          15.7734 |
[32m[20221213 22:46:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.21
[32m[20221213 22:46:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.54
[32m[20221213 22:46:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.03
[32m[20221213 22:46:38 @agent_ppo2.py:143][0m Total time:      28.42 min
[32m[20221213 22:46:38 @agent_ppo2.py:145][0m 2777088 total steps have happened
[32m[20221213 22:46:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1356 --------------------------#
[32m[20221213 22:46:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |           0.0082 |          44.5161 |          15.7845 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0035 |          37.3360 |          15.7772 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0067 |          36.1330 |          15.7738 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0074 |          35.3075 |          15.7622 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0109 |          34.4508 |          15.7745 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0129 |          33.9008 |          15.7770 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0118 |          33.6256 |          15.7774 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0055 |          33.4330 |          15.7821 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0054 |          33.0748 |          15.7741 |
[32m[20221213 22:46:39 @agent_ppo2.py:185][0m |          -0.0128 |          32.3881 |          15.7709 |
[32m[20221213 22:46:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.28
[32m[20221213 22:46:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.35
[32m[20221213 22:46:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.55
[32m[20221213 22:46:40 @agent_ppo2.py:143][0m Total time:      28.44 min
[32m[20221213 22:46:40 @agent_ppo2.py:145][0m 2779136 total steps have happened
[32m[20221213 22:46:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1357 --------------------------#
[32m[20221213 22:46:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0027 |          43.7731 |          15.8240 |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0061 |          42.5098 |          15.8056 |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0083 |          42.1984 |          15.8092 |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0114 |          42.0013 |          15.7984 |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0079 |          41.7952 |          15.8118 |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0087 |          41.5689 |          15.7965 |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0121 |          41.5085 |          15.8242 |
[32m[20221213 22:46:40 @agent_ppo2.py:185][0m |          -0.0088 |          41.4646 |          15.8024 |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0115 |          41.3488 |          15.8153 |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0114 |          41.1963 |          15.8041 |
[32m[20221213 22:46:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.83
[32m[20221213 22:46:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.15
[32m[20221213 22:46:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.53
[32m[20221213 22:46:41 @agent_ppo2.py:143][0m Total time:      28.46 min
[32m[20221213 22:46:41 @agent_ppo2.py:145][0m 2781184 total steps have happened
[32m[20221213 22:46:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1358 --------------------------#
[32m[20221213 22:46:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0019 |          36.4260 |          15.6281 |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0068 |          31.8231 |          15.6035 |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0101 |          30.7038 |          15.6083 |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0115 |          29.9634 |          15.6041 |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0121 |          29.6590 |          15.6008 |
[32m[20221213 22:46:41 @agent_ppo2.py:185][0m |          -0.0103 |          29.1718 |          15.6040 |
[32m[20221213 22:46:42 @agent_ppo2.py:185][0m |          -0.0117 |          28.8052 |          15.5969 |
[32m[20221213 22:46:42 @agent_ppo2.py:185][0m |          -0.0156 |          28.6353 |          15.5966 |
[32m[20221213 22:46:42 @agent_ppo2.py:185][0m |          -0.0145 |          28.6113 |          15.5969 |
[32m[20221213 22:46:42 @agent_ppo2.py:185][0m |          -0.0166 |          28.3459 |          15.6045 |
[32m[20221213 22:46:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.14
[32m[20221213 22:46:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.35
[32m[20221213 22:46:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.57
[32m[20221213 22:46:42 @agent_ppo2.py:143][0m Total time:      28.48 min
[32m[20221213 22:46:42 @agent_ppo2.py:145][0m 2783232 total steps have happened
[32m[20221213 22:46:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1359 --------------------------#
[32m[20221213 22:46:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:42 @agent_ppo2.py:185][0m |          -0.0001 |          25.0378 |          15.6285 |
[32m[20221213 22:46:42 @agent_ppo2.py:185][0m |          -0.0007 |          21.3680 |          15.6319 |
[32m[20221213 22:46:42 @agent_ppo2.py:185][0m |          -0.0065 |          20.2893 |          15.6194 |
[32m[20221213 22:46:43 @agent_ppo2.py:185][0m |          -0.0053 |          19.7948 |          15.6141 |
[32m[20221213 22:46:43 @agent_ppo2.py:185][0m |          -0.0061 |          19.8469 |          15.6198 |
[32m[20221213 22:46:43 @agent_ppo2.py:185][0m |          -0.0055 |          19.4866 |          15.6230 |
[32m[20221213 22:46:43 @agent_ppo2.py:185][0m |          -0.0105 |          18.9782 |          15.6185 |
[32m[20221213 22:46:43 @agent_ppo2.py:185][0m |          -0.0135 |          18.6211 |          15.6126 |
[32m[20221213 22:46:43 @agent_ppo2.py:185][0m |          -0.0083 |          18.6470 |          15.6097 |
[32m[20221213 22:46:43 @agent_ppo2.py:185][0m |          -0.0069 |          19.7107 |          15.6099 |
[32m[20221213 22:46:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.26
[32m[20221213 22:46:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 339.96
[32m[20221213 22:46:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.65
[32m[20221213 22:46:43 @agent_ppo2.py:143][0m Total time:      28.50 min
[32m[20221213 22:46:43 @agent_ppo2.py:145][0m 2785280 total steps have happened
[32m[20221213 22:46:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1360 --------------------------#
[32m[20221213 22:46:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:46:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0012 |          20.7382 |          15.6450 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0077 |          18.6950 |          15.6344 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0050 |          18.3402 |          15.6328 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0041 |          18.0236 |          15.6371 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0078 |          17.3209 |          15.6235 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0173 |          17.1857 |          15.6282 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0096 |          17.2581 |          15.6162 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0078 |          16.8088 |          15.6110 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0110 |          16.5323 |          15.6204 |
[32m[20221213 22:46:44 @agent_ppo2.py:185][0m |          -0.0129 |          16.5418 |          15.6190 |
[32m[20221213 22:46:44 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.56
[32m[20221213 22:46:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.49
[32m[20221213 22:46:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.01
[32m[20221213 22:46:44 @agent_ppo2.py:143][0m Total time:      28.53 min
[32m[20221213 22:46:44 @agent_ppo2.py:145][0m 2787328 total steps have happened
[32m[20221213 22:46:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1361 --------------------------#
[32m[20221213 22:46:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |           0.0012 |          27.2273 |          15.7390 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0020 |          24.6946 |          15.7271 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0098 |          23.9202 |          15.7185 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0060 |          23.3046 |          15.7120 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0061 |          22.8352 |          15.7160 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0106 |          22.5711 |          15.7071 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0001 |          24.1992 |          15.6994 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0049 |          22.2609 |          15.6919 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0150 |          21.9623 |          15.7033 |
[32m[20221213 22:46:45 @agent_ppo2.py:185][0m |          -0.0100 |          21.9713 |          15.6984 |
[32m[20221213 22:46:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.86
[32m[20221213 22:46:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.45
[32m[20221213 22:46:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.29
[32m[20221213 22:46:46 @agent_ppo2.py:143][0m Total time:      28.55 min
[32m[20221213 22:46:46 @agent_ppo2.py:145][0m 2789376 total steps have happened
[32m[20221213 22:46:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1362 --------------------------#
[32m[20221213 22:46:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:46 @agent_ppo2.py:185][0m |          -0.0009 |          34.0351 |          15.9317 |
[32m[20221213 22:46:46 @agent_ppo2.py:185][0m |          -0.0016 |          29.0940 |          15.9179 |
[32m[20221213 22:46:46 @agent_ppo2.py:185][0m |          -0.0020 |          27.0782 |          15.9045 |
[32m[20221213 22:46:46 @agent_ppo2.py:185][0m |          -0.0079 |          25.8697 |          15.9042 |
[32m[20221213 22:46:46 @agent_ppo2.py:185][0m |          -0.0039 |          25.6428 |          15.8934 |
[32m[20221213 22:46:46 @agent_ppo2.py:185][0m |          -0.0082 |          24.8038 |          15.8945 |
[32m[20221213 22:46:46 @agent_ppo2.py:185][0m |          -0.0042 |          24.7154 |          15.8834 |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |          -0.0055 |          24.1144 |          15.8785 |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |          -0.0130 |          23.7866 |          15.8752 |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |          -0.0085 |          23.7838 |          15.8687 |
[32m[20221213 22:46:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:46:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.30
[32m[20221213 22:46:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.73
[32m[20221213 22:46:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 196.97
[32m[20221213 22:46:47 @agent_ppo2.py:143][0m Total time:      28.57 min
[32m[20221213 22:46:47 @agent_ppo2.py:145][0m 2791424 total steps have happened
[32m[20221213 22:46:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1363 --------------------------#
[32m[20221213 22:46:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |          -0.0009 |          24.4972 |          15.5926 |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |           0.0055 |          19.2145 |          15.5662 |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |          -0.0112 |          18.1890 |          15.5716 |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |          -0.0052 |          17.5422 |          15.5622 |
[32m[20221213 22:46:47 @agent_ppo2.py:185][0m |          -0.0049 |          17.1475 |          15.5411 |
[32m[20221213 22:46:48 @agent_ppo2.py:185][0m |          -0.0083 |          16.8215 |          15.5373 |
[32m[20221213 22:46:48 @agent_ppo2.py:185][0m |          -0.0090 |          16.4886 |          15.5357 |
[32m[20221213 22:46:48 @agent_ppo2.py:185][0m |          -0.0114 |          16.3904 |          15.5276 |
[32m[20221213 22:46:48 @agent_ppo2.py:185][0m |          -0.0144 |          16.0772 |          15.5325 |
[32m[20221213 22:46:48 @agent_ppo2.py:185][0m |          -0.0055 |          17.4678 |          15.5297 |
[32m[20221213 22:46:48 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 185.42
[32m[20221213 22:46:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.41
[32m[20221213 22:46:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.09
[32m[20221213 22:46:48 @agent_ppo2.py:143][0m Total time:      28.59 min
[32m[20221213 22:46:48 @agent_ppo2.py:145][0m 2793472 total steps have happened
[32m[20221213 22:46:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1364 --------------------------#
[32m[20221213 22:46:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:48 @agent_ppo2.py:185][0m |           0.0002 |          26.3691 |          15.7989 |
[32m[20221213 22:46:48 @agent_ppo2.py:185][0m |          -0.0038 |          24.4410 |          15.7881 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |          -0.0039 |          23.8173 |          15.7901 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |          -0.0014 |          23.5686 |          15.7860 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |          -0.0062 |          23.1627 |          15.7811 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |           0.0035 |          25.0828 |          15.7645 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |          -0.0086 |          22.9621 |          15.7707 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |          -0.0073 |          22.6171 |          15.7747 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |          -0.0105 |          22.5357 |          15.7772 |
[32m[20221213 22:46:49 @agent_ppo2.py:185][0m |          -0.0010 |          22.7805 |          15.7656 |
[32m[20221213 22:46:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.11
[32m[20221213 22:46:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 307.35
[32m[20221213 22:46:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.40
[32m[20221213 22:46:49 @agent_ppo2.py:143][0m Total time:      28.61 min
[32m[20221213 22:46:49 @agent_ppo2.py:145][0m 2795520 total steps have happened
[32m[20221213 22:46:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1365 --------------------------#
[32m[20221213 22:46:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |           0.0040 |          36.6690 |          15.8159 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |           0.0019 |          35.2554 |          15.7822 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0058 |          33.4852 |          15.7568 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0133 |          33.3009 |          15.7637 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0132 |          32.8821 |          15.7671 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0116 |          33.4345 |          15.7448 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0103 |          32.4947 |          15.7548 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0145 |          32.1194 |          15.7356 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0133 |          31.9616 |          15.7515 |
[32m[20221213 22:46:50 @agent_ppo2.py:185][0m |          -0.0139 |          31.8164 |          15.7488 |
[32m[20221213 22:46:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.56
[32m[20221213 22:46:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.50
[32m[20221213 22:46:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.43
[32m[20221213 22:46:50 @agent_ppo2.py:143][0m Total time:      28.63 min
[32m[20221213 22:46:50 @agent_ppo2.py:145][0m 2797568 total steps have happened
[32m[20221213 22:46:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1366 --------------------------#
[32m[20221213 22:46:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:46:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |           0.0011 |           5.9409 |          15.7952 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |          -0.0023 |           4.6012 |          15.7667 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |          -0.0018 |           4.4112 |          15.7794 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |          -0.0022 |           4.3206 |          15.7671 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |           0.0005 |           4.3005 |          15.7679 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |          -0.0046 |           4.1472 |          15.7530 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |          -0.0113 |           4.0879 |          15.7710 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |          -0.0056 |           4.0466 |          15.7601 |
[32m[20221213 22:46:51 @agent_ppo2.py:185][0m |          -0.0046 |           4.0481 |          15.7722 |
[32m[20221213 22:46:52 @agent_ppo2.py:185][0m |          -0.0012 |           4.1037 |          15.7509 |
[32m[20221213 22:46:52 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:46:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:46:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:46:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.42
[32m[20221213 22:46:52 @agent_ppo2.py:143][0m Total time:      28.65 min
[32m[20221213 22:46:52 @agent_ppo2.py:145][0m 2799616 total steps have happened
[32m[20221213 22:46:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1367 --------------------------#
[32m[20221213 22:46:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:52 @agent_ppo2.py:185][0m |           0.0000 |          36.7338 |          15.6675 |
[32m[20221213 22:46:52 @agent_ppo2.py:185][0m |          -0.0008 |          35.1496 |          15.6669 |
[32m[20221213 22:46:52 @agent_ppo2.py:185][0m |          -0.0061 |          34.2781 |          15.6646 |
[32m[20221213 22:46:52 @agent_ppo2.py:185][0m |          -0.0076 |          33.8473 |          15.6677 |
[32m[20221213 22:46:52 @agent_ppo2.py:185][0m |          -0.0071 |          33.6625 |          15.6559 |
[32m[20221213 22:46:52 @agent_ppo2.py:185][0m |          -0.0031 |          33.5780 |          15.6648 |
[32m[20221213 22:46:53 @agent_ppo2.py:185][0m |          -0.0095 |          33.3389 |          15.6368 |
[32m[20221213 22:46:53 @agent_ppo2.py:185][0m |          -0.0071 |          33.2400 |          15.6600 |
[32m[20221213 22:46:53 @agent_ppo2.py:185][0m |          -0.0078 |          33.0493 |          15.6564 |
[32m[20221213 22:46:53 @agent_ppo2.py:185][0m |          -0.0002 |          33.5565 |          15.6486 |
[32m[20221213 22:46:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.00
[32m[20221213 22:46:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.16
[32m[20221213 22:46:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.05
[32m[20221213 22:46:53 @agent_ppo2.py:143][0m Total time:      28.67 min
[32m[20221213 22:46:53 @agent_ppo2.py:145][0m 2801664 total steps have happened
[32m[20221213 22:46:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1368 --------------------------#
[32m[20221213 22:46:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:53 @agent_ppo2.py:185][0m |           0.0009 |          24.5872 |          15.5403 |
[32m[20221213 22:46:53 @agent_ppo2.py:185][0m |          -0.0074 |          21.2541 |          15.5419 |
[32m[20221213 22:46:53 @agent_ppo2.py:185][0m |          -0.0071 |          20.7600 |          15.5206 |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |          -0.0028 |          20.7835 |          15.5271 |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |          -0.0132 |          20.4240 |          15.4860 |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |          -0.0097 |          20.1535 |          15.5048 |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |          -0.0036 |          22.5145 |          15.5108 |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |          -0.0097 |          19.9746 |          15.4984 |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |          -0.0105 |          19.8378 |          15.5077 |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |          -0.0042 |          20.7327 |          15.4989 |
[32m[20221213 22:46:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.51
[32m[20221213 22:46:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.60
[32m[20221213 22:46:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.06
[32m[20221213 22:46:54 @agent_ppo2.py:143][0m Total time:      28.69 min
[32m[20221213 22:46:54 @agent_ppo2.py:145][0m 2803712 total steps have happened
[32m[20221213 22:46:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1369 --------------------------#
[32m[20221213 22:46:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:54 @agent_ppo2.py:185][0m |           0.0017 |          24.1005 |          15.6532 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0033 |          20.0779 |          15.6593 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0102 |          18.7549 |          15.6431 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0045 |          17.8263 |          15.6484 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0131 |          17.2910 |          15.6389 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0002 |          18.1860 |          15.6325 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0094 |          16.6681 |          15.6326 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0127 |          16.3740 |          15.6120 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0050 |          16.1789 |          15.6274 |
[32m[20221213 22:46:55 @agent_ppo2.py:185][0m |          -0.0150 |          15.9541 |          15.6250 |
[32m[20221213 22:46:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.95
[32m[20221213 22:46:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.63
[32m[20221213 22:46:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.73
[32m[20221213 22:46:55 @agent_ppo2.py:143][0m Total time:      28.71 min
[32m[20221213 22:46:55 @agent_ppo2.py:145][0m 2805760 total steps have happened
[32m[20221213 22:46:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1370 --------------------------#
[32m[20221213 22:46:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:46:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |           0.0011 |          39.8524 |          15.6473 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0078 |          37.2184 |          15.6500 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0082 |          36.4211 |          15.6446 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0071 |          35.8762 |          15.6461 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0088 |          35.6526 |          15.6384 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0132 |          35.2893 |          15.6306 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0110 |          35.1230 |          15.6486 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0123 |          34.8040 |          15.6400 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0135 |          34.7946 |          15.6533 |
[32m[20221213 22:46:56 @agent_ppo2.py:185][0m |          -0.0154 |          34.7361 |          15.6463 |
[32m[20221213 22:46:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:46:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.11
[32m[20221213 22:46:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.82
[32m[20221213 22:46:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.75
[32m[20221213 22:46:57 @agent_ppo2.py:143][0m Total time:      28.73 min
[32m[20221213 22:46:57 @agent_ppo2.py:145][0m 2807808 total steps have happened
[32m[20221213 22:46:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1371 --------------------------#
[32m[20221213 22:46:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0018 |          29.6459 |          15.6860 |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0078 |          27.3030 |          15.6932 |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0053 |          26.7634 |          15.6836 |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0043 |          27.1234 |          15.6763 |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0081 |          26.5744 |          15.6858 |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0071 |          26.5263 |          15.6892 |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0082 |          26.1872 |          15.6767 |
[32m[20221213 22:46:57 @agent_ppo2.py:185][0m |          -0.0096 |          26.2840 |          15.6751 |
[32m[20221213 22:46:58 @agent_ppo2.py:185][0m |           0.0008 |          28.4798 |          15.6700 |
[32m[20221213 22:46:58 @agent_ppo2.py:185][0m |          -0.0106 |          26.4374 |          15.6632 |
[32m[20221213 22:46:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:46:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.76
[32m[20221213 22:46:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.45
[32m[20221213 22:46:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.70
[32m[20221213 22:46:58 @agent_ppo2.py:143][0m Total time:      28.75 min
[32m[20221213 22:46:58 @agent_ppo2.py:145][0m 2809856 total steps have happened
[32m[20221213 22:46:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1372 --------------------------#
[32m[20221213 22:46:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:58 @agent_ppo2.py:185][0m |           0.0044 |          35.4850 |          15.7614 |
[32m[20221213 22:46:58 @agent_ppo2.py:185][0m |          -0.0059 |          31.2806 |          15.7687 |
[32m[20221213 22:46:58 @agent_ppo2.py:185][0m |          -0.0088 |          30.4723 |          15.7633 |
[32m[20221213 22:46:58 @agent_ppo2.py:185][0m |          -0.0108 |          29.9835 |          15.7589 |
[32m[20221213 22:46:58 @agent_ppo2.py:185][0m |          -0.0041 |          30.1542 |          15.7636 |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |          -0.0095 |          29.2641 |          15.7498 |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |          -0.0117 |          29.0318 |          15.7477 |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |          -0.0094 |          28.8632 |          15.7468 |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |          -0.0106 |          28.5212 |          15.7508 |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |          -0.0115 |          28.4800 |          15.7512 |
[32m[20221213 22:46:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:46:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.06
[32m[20221213 22:46:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.72
[32m[20221213 22:46:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 149.07
[32m[20221213 22:46:59 @agent_ppo2.py:143][0m Total time:      28.77 min
[32m[20221213 22:46:59 @agent_ppo2.py:145][0m 2811904 total steps have happened
[32m[20221213 22:46:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1373 --------------------------#
[32m[20221213 22:46:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:46:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |           0.0033 |          34.0346 |          15.8158 |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |           0.0009 |          30.5267 |          15.8122 |
[32m[20221213 22:46:59 @agent_ppo2.py:185][0m |          -0.0047 |          29.0517 |          15.8109 |
[32m[20221213 22:47:00 @agent_ppo2.py:185][0m |          -0.0127 |          28.5251 |          15.8046 |
[32m[20221213 22:47:00 @agent_ppo2.py:185][0m |          -0.0097 |          28.0650 |          15.7950 |
[32m[20221213 22:47:00 @agent_ppo2.py:185][0m |          -0.0135 |          27.5922 |          15.8017 |
[32m[20221213 22:47:00 @agent_ppo2.py:185][0m |          -0.0147 |          27.3487 |          15.7961 |
[32m[20221213 22:47:00 @agent_ppo2.py:185][0m |          -0.0145 |          27.0538 |          15.7987 |
[32m[20221213 22:47:00 @agent_ppo2.py:185][0m |          -0.0130 |          26.9565 |          15.7879 |
[32m[20221213 22:47:00 @agent_ppo2.py:185][0m |          -0.0103 |          26.7348 |          15.7927 |
[32m[20221213 22:47:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:47:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.32
[32m[20221213 22:47:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.36
[32m[20221213 22:47:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.14
[32m[20221213 22:47:00 @agent_ppo2.py:143][0m Total time:      28.79 min
[32m[20221213 22:47:00 @agent_ppo2.py:145][0m 2813952 total steps have happened
[32m[20221213 22:47:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1374 --------------------------#
[32m[20221213 22:47:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0033 |          32.4840 |          15.8013 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0041 |          29.5930 |          15.8094 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0090 |          28.0241 |          15.8022 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0056 |          27.9129 |          15.8136 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0087 |          26.4556 |          15.7891 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0074 |          25.8431 |          15.8029 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0104 |          25.4214 |          15.8088 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0072 |          25.0720 |          15.8053 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0127 |          24.5248 |          15.8079 |
[32m[20221213 22:47:01 @agent_ppo2.py:185][0m |          -0.0110 |          24.2642 |          15.8072 |
[32m[20221213 22:47:01 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:47:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.74
[32m[20221213 22:47:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.07
[32m[20221213 22:47:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.88
[32m[20221213 22:47:01 @agent_ppo2.py:143][0m Total time:      28.81 min
[32m[20221213 22:47:01 @agent_ppo2.py:145][0m 2816000 total steps have happened
[32m[20221213 22:47:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1375 --------------------------#
[32m[20221213 22:47:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0036 |          27.6154 |          15.5867 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0078 |          23.2485 |          15.5509 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0103 |          22.1911 |          15.5596 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0052 |          21.4056 |          15.5667 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0111 |          21.1117 |          15.5523 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |           0.0032 |          21.3029 |          15.5637 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0075 |          20.5610 |          15.5520 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0099 |          20.2677 |          15.5369 |
[32m[20221213 22:47:02 @agent_ppo2.py:185][0m |          -0.0029 |          20.3322 |          15.5384 |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |          -0.0132 |          19.9368 |          15.5538 |
[32m[20221213 22:47:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.29
[32m[20221213 22:47:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 319.89
[32m[20221213 22:47:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.09
[32m[20221213 22:47:03 @agent_ppo2.py:143][0m Total time:      28.83 min
[32m[20221213 22:47:03 @agent_ppo2.py:145][0m 2818048 total steps have happened
[32m[20221213 22:47:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1376 --------------------------#
[32m[20221213 22:47:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |          -0.0016 |          29.1952 |          15.7846 |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |           0.0210 |          29.4225 |          15.7635 |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |          -0.0026 |          25.7337 |          15.7409 |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |          -0.0083 |          25.2386 |          15.7237 |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |          -0.0012 |          25.0064 |          15.7272 |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |          -0.0086 |          24.7348 |          15.7309 |
[32m[20221213 22:47:03 @agent_ppo2.py:185][0m |          -0.0077 |          24.5693 |          15.7201 |
[32m[20221213 22:47:04 @agent_ppo2.py:185][0m |          -0.0087 |          24.4596 |          15.7344 |
[32m[20221213 22:47:04 @agent_ppo2.py:185][0m |          -0.0115 |          24.2814 |          15.7219 |
[32m[20221213 22:47:04 @agent_ppo2.py:185][0m |          -0.0078 |          24.2272 |          15.7182 |
[32m[20221213 22:47:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.63
[32m[20221213 22:47:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.47
[32m[20221213 22:47:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 181.01
[32m[20221213 22:47:04 @agent_ppo2.py:143][0m Total time:      28.85 min
[32m[20221213 22:47:04 @agent_ppo2.py:145][0m 2820096 total steps have happened
[32m[20221213 22:47:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1377 --------------------------#
[32m[20221213 22:47:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:04 @agent_ppo2.py:185][0m |           0.0047 |          33.8221 |          15.8214 |
[32m[20221213 22:47:04 @agent_ppo2.py:185][0m |          -0.0053 |          31.0035 |          15.8098 |
[32m[20221213 22:47:04 @agent_ppo2.py:185][0m |          -0.0049 |          30.2314 |          15.8071 |
[32m[20221213 22:47:04 @agent_ppo2.py:185][0m |          -0.0060 |          29.8311 |          15.8044 |
[32m[20221213 22:47:05 @agent_ppo2.py:185][0m |          -0.0051 |          29.6890 |          15.8023 |
[32m[20221213 22:47:05 @agent_ppo2.py:185][0m |          -0.0062 |          29.2089 |          15.7957 |
[32m[20221213 22:47:05 @agent_ppo2.py:185][0m |          -0.0018 |          29.8229 |          15.8077 |
[32m[20221213 22:47:05 @agent_ppo2.py:185][0m |          -0.0113 |          28.9494 |          15.7913 |
[32m[20221213 22:47:05 @agent_ppo2.py:185][0m |           0.0236 |          37.4791 |          15.8022 |
[32m[20221213 22:47:05 @agent_ppo2.py:185][0m |          -0.0017 |          32.4821 |          15.8099 |
[32m[20221213 22:47:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:47:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.12
[32m[20221213 22:47:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.95
[32m[20221213 22:47:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.73
[32m[20221213 22:47:05 @agent_ppo2.py:143][0m Total time:      28.87 min
[32m[20221213 22:47:05 @agent_ppo2.py:145][0m 2822144 total steps have happened
[32m[20221213 22:47:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1378 --------------------------#
[32m[20221213 22:47:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:05 @agent_ppo2.py:185][0m |           0.0014 |          26.9921 |          15.7763 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0057 |          24.5143 |          15.7456 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0032 |          23.7447 |          15.7251 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0071 |          23.2063 |          15.7105 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0104 |          22.8944 |          15.7086 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0078 |          22.4907 |          15.7044 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0138 |          22.2675 |          15.6985 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0124 |          22.1449 |          15.6870 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0171 |          22.0092 |          15.6871 |
[32m[20221213 22:47:06 @agent_ppo2.py:185][0m |          -0.0100 |          21.7329 |          15.6784 |
[32m[20221213 22:47:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.64
[32m[20221213 22:47:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.91
[32m[20221213 22:47:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.19
[32m[20221213 22:47:06 @agent_ppo2.py:143][0m Total time:      28.89 min
[32m[20221213 22:47:06 @agent_ppo2.py:145][0m 2824192 total steps have happened
[32m[20221213 22:47:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1379 --------------------------#
[32m[20221213 22:47:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0012 |          33.9919 |          15.7675 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0011 |          31.5824 |          15.7563 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0017 |          30.5851 |          15.7548 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0016 |          30.5815 |          15.7340 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0075 |          29.7630 |          15.7119 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0063 |          29.3171 |          15.7373 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0058 |          29.4443 |          15.7265 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0080 |          29.1810 |          15.7259 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |           0.0124 |          34.6296 |          15.7086 |
[32m[20221213 22:47:07 @agent_ppo2.py:185][0m |          -0.0091 |          29.0481 |          15.7001 |
[32m[20221213 22:47:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:47:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.58
[32m[20221213 22:47:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.18
[32m[20221213 22:47:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.32
[32m[20221213 22:47:08 @agent_ppo2.py:143][0m Total time:      28.91 min
[32m[20221213 22:47:08 @agent_ppo2.py:145][0m 2826240 total steps have happened
[32m[20221213 22:47:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1380 --------------------------#
[32m[20221213 22:47:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:47:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |           0.0070 |          39.3405 |          15.8025 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |          -0.0056 |          37.3989 |          15.7779 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |          -0.0055 |          36.5968 |          15.7642 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |          -0.0080 |          36.1516 |          15.7737 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |          -0.0079 |          35.9564 |          15.7698 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |          -0.0104 |          35.6865 |          15.7614 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |           0.0023 |          38.4552 |          15.7709 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |           0.0043 |          38.3285 |          15.7740 |
[32m[20221213 22:47:08 @agent_ppo2.py:185][0m |          -0.0094 |          35.2133 |          15.7771 |
[32m[20221213 22:47:09 @agent_ppo2.py:185][0m |          -0.0052 |          35.6509 |          15.7825 |
[32m[20221213 22:47:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.77
[32m[20221213 22:47:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.91
[32m[20221213 22:47:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.54
[32m[20221213 22:47:09 @agent_ppo2.py:143][0m Total time:      28.93 min
[32m[20221213 22:47:09 @agent_ppo2.py:145][0m 2828288 total steps have happened
[32m[20221213 22:47:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1381 --------------------------#
[32m[20221213 22:47:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:09 @agent_ppo2.py:185][0m |           0.0011 |          26.6560 |          15.6557 |
[32m[20221213 22:47:09 @agent_ppo2.py:185][0m |          -0.0027 |          24.4671 |          15.6439 |
[32m[20221213 22:47:09 @agent_ppo2.py:185][0m |          -0.0072 |          23.7552 |          15.6251 |
[32m[20221213 22:47:09 @agent_ppo2.py:185][0m |           0.0066 |          24.8057 |          15.6317 |
[32m[20221213 22:47:09 @agent_ppo2.py:185][0m |          -0.0030 |          23.4404 |          15.6365 |
[32m[20221213 22:47:09 @agent_ppo2.py:185][0m |          -0.0131 |          22.9288 |          15.6318 |
[32m[20221213 22:47:10 @agent_ppo2.py:185][0m |          -0.0104 |          22.6671 |          15.6284 |
[32m[20221213 22:47:10 @agent_ppo2.py:185][0m |          -0.0135 |          22.4202 |          15.6214 |
[32m[20221213 22:47:10 @agent_ppo2.py:185][0m |          -0.0144 |          22.2420 |          15.6192 |
[32m[20221213 22:47:10 @agent_ppo2.py:185][0m |          -0.0102 |          22.0713 |          15.6177 |
[32m[20221213 22:47:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.50
[32m[20221213 22:47:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 312.93
[32m[20221213 22:47:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.56
[32m[20221213 22:47:10 @agent_ppo2.py:143][0m Total time:      28.95 min
[32m[20221213 22:47:10 @agent_ppo2.py:145][0m 2830336 total steps have happened
[32m[20221213 22:47:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1382 --------------------------#
[32m[20221213 22:47:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:10 @agent_ppo2.py:185][0m |           0.0041 |          41.0747 |          15.8794 |
[32m[20221213 22:47:10 @agent_ppo2.py:185][0m |          -0.0042 |          37.8684 |          15.8693 |
[32m[20221213 22:47:10 @agent_ppo2.py:185][0m |           0.0021 |          38.3188 |          15.8643 |
[32m[20221213 22:47:11 @agent_ppo2.py:185][0m |          -0.0074 |          36.6931 |          15.8480 |
[32m[20221213 22:47:11 @agent_ppo2.py:185][0m |          -0.0065 |          36.2835 |          15.8573 |
[32m[20221213 22:47:11 @agent_ppo2.py:185][0m |          -0.0099 |          35.9671 |          15.8471 |
[32m[20221213 22:47:11 @agent_ppo2.py:185][0m |          -0.0022 |          36.5645 |          15.8538 |
[32m[20221213 22:47:11 @agent_ppo2.py:185][0m |          -0.0063 |          35.6452 |          15.8307 |
[32m[20221213 22:47:11 @agent_ppo2.py:185][0m |          -0.0092 |          35.5611 |          15.8356 |
[32m[20221213 22:47:11 @agent_ppo2.py:185][0m |          -0.0091 |          35.4861 |          15.8430 |
[32m[20221213 22:47:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.50
[32m[20221213 22:47:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.19
[32m[20221213 22:47:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.80
[32m[20221213 22:47:11 @agent_ppo2.py:143][0m Total time:      28.97 min
[32m[20221213 22:47:11 @agent_ppo2.py:145][0m 2832384 total steps have happened
[32m[20221213 22:47:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1383 --------------------------#
[32m[20221213 22:47:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |           0.0032 |          30.6597 |          15.8259 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0021 |          28.7604 |          15.8231 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0045 |          28.0538 |          15.8250 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0048 |          28.1012 |          15.8240 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0105 |          27.3383 |          15.8192 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0095 |          26.9848 |          15.8294 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0111 |          26.6659 |          15.8298 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0073 |          26.5510 |          15.8249 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0103 |          26.3797 |          15.8326 |
[32m[20221213 22:47:12 @agent_ppo2.py:185][0m |          -0.0135 |          26.2156 |          15.8355 |
[32m[20221213 22:47:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:47:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.19
[32m[20221213 22:47:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.08
[32m[20221213 22:47:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.75
[32m[20221213 22:47:12 @agent_ppo2.py:143][0m Total time:      28.99 min
[32m[20221213 22:47:12 @agent_ppo2.py:145][0m 2834432 total steps have happened
[32m[20221213 22:47:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1384 --------------------------#
[32m[20221213 22:47:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0033 |          39.7164 |          15.7080 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0055 |          36.3198 |          15.6894 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0091 |          35.0953 |          15.6695 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0094 |          33.9495 |          15.6762 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0038 |          33.7547 |          15.6691 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0068 |          32.9072 |          15.6602 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0123 |          32.6654 |          15.6462 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0161 |          32.2475 |          15.6547 |
[32m[20221213 22:47:13 @agent_ppo2.py:185][0m |          -0.0001 |          33.2578 |          15.6524 |
[32m[20221213 22:47:14 @agent_ppo2.py:185][0m |          -0.0152 |          33.3443 |          15.6424 |
[32m[20221213 22:47:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.21
[32m[20221213 22:47:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.23
[32m[20221213 22:47:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.22
[32m[20221213 22:47:14 @agent_ppo2.py:143][0m Total time:      29.01 min
[32m[20221213 22:47:14 @agent_ppo2.py:145][0m 2836480 total steps have happened
[32m[20221213 22:47:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1385 --------------------------#
[32m[20221213 22:47:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:14 @agent_ppo2.py:185][0m |           0.0087 |          32.4824 |          15.9960 |
[32m[20221213 22:47:14 @agent_ppo2.py:185][0m |          -0.0018 |          26.7333 |          15.9965 |
[32m[20221213 22:47:14 @agent_ppo2.py:185][0m |          -0.0055 |          25.7117 |          15.9769 |
[32m[20221213 22:47:14 @agent_ppo2.py:185][0m |          -0.0075 |          24.9871 |          15.9867 |
[32m[20221213 22:47:14 @agent_ppo2.py:185][0m |          -0.0107 |          24.5573 |          15.9782 |
[32m[20221213 22:47:14 @agent_ppo2.py:185][0m |          -0.0082 |          24.3236 |          15.9920 |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |          -0.0111 |          23.8924 |          15.9911 |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |          -0.0056 |          23.6425 |          15.9909 |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |          -0.0084 |          23.5040 |          15.9978 |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |          -0.0126 |          23.3211 |          15.9958 |
[32m[20221213 22:47:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:47:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.41
[32m[20221213 22:47:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.05
[32m[20221213 22:47:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.10
[32m[20221213 22:47:15 @agent_ppo2.py:143][0m Total time:      29.03 min
[32m[20221213 22:47:15 @agent_ppo2.py:145][0m 2838528 total steps have happened
[32m[20221213 22:47:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1386 --------------------------#
[32m[20221213 22:47:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |           0.0035 |          42.6543 |          15.8530 |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |          -0.0019 |          39.5867 |          15.8534 |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |          -0.0065 |          39.0606 |          15.8381 |
[32m[20221213 22:47:15 @agent_ppo2.py:185][0m |          -0.0059 |          38.8263 |          15.8281 |
[32m[20221213 22:47:16 @agent_ppo2.py:185][0m |          -0.0072 |          38.6082 |          15.8272 |
[32m[20221213 22:47:16 @agent_ppo2.py:185][0m |          -0.0065 |          38.3633 |          15.8281 |
[32m[20221213 22:47:16 @agent_ppo2.py:185][0m |          -0.0115 |          38.3884 |          15.8224 |
[32m[20221213 22:47:16 @agent_ppo2.py:185][0m |          -0.0057 |          38.1543 |          15.8341 |
[32m[20221213 22:47:16 @agent_ppo2.py:185][0m |          -0.0113 |          38.5654 |          15.8245 |
[32m[20221213 22:47:16 @agent_ppo2.py:185][0m |          -0.0091 |          37.9195 |          15.8249 |
[32m[20221213 22:47:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.48
[32m[20221213 22:47:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.61
[32m[20221213 22:47:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.86
[32m[20221213 22:47:16 @agent_ppo2.py:143][0m Total time:      29.05 min
[32m[20221213 22:47:16 @agent_ppo2.py:145][0m 2840576 total steps have happened
[32m[20221213 22:47:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1387 --------------------------#
[32m[20221213 22:47:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:16 @agent_ppo2.py:185][0m |          -0.0008 |          32.1370 |          15.6822 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0039 |          29.8691 |          15.6750 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0067 |          29.1642 |          15.6649 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0052 |          28.9318 |          15.6396 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0065 |          28.5199 |          15.6487 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0074 |          28.3668 |          15.6305 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0137 |          28.1048 |          15.6255 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0050 |          28.9874 |          15.6124 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0147 |          27.7905 |          15.6102 |
[32m[20221213 22:47:17 @agent_ppo2.py:185][0m |          -0.0020 |          28.0688 |          15.5966 |
[32m[20221213 22:47:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:47:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.22
[32m[20221213 22:47:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.02
[32m[20221213 22:47:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.77
[32m[20221213 22:47:17 @agent_ppo2.py:143][0m Total time:      29.08 min
[32m[20221213 22:47:17 @agent_ppo2.py:145][0m 2842624 total steps have happened
[32m[20221213 22:47:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1388 --------------------------#
[32m[20221213 22:47:18 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 22:47:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0003 |          35.2494 |          15.7992 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0087 |          32.4920 |          15.7980 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0144 |          31.6847 |          15.8064 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0085 |          31.1586 |          15.7993 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0095 |          30.7724 |          15.8006 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0105 |          30.6061 |          15.8066 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0075 |          30.5490 |          15.8017 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0106 |          30.2253 |          15.8147 |
[32m[20221213 22:47:18 @agent_ppo2.py:185][0m |          -0.0105 |          30.2530 |          15.8099 |
[32m[20221213 22:47:19 @agent_ppo2.py:185][0m |          -0.0021 |          34.3753 |          15.8089 |
[32m[20221213 22:47:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:47:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 203.41
[32m[20221213 22:47:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.10
[32m[20221213 22:47:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.17
[32m[20221213 22:47:19 @agent_ppo2.py:143][0m Total time:      29.10 min
[32m[20221213 22:47:19 @agent_ppo2.py:145][0m 2844672 total steps have happened
[32m[20221213 22:47:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1389 --------------------------#
[32m[20221213 22:47:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:19 @agent_ppo2.py:185][0m |          -0.0001 |          39.8249 |          15.6333 |
[32m[20221213 22:47:19 @agent_ppo2.py:185][0m |          -0.0119 |          37.2804 |          15.6343 |
[32m[20221213 22:47:19 @agent_ppo2.py:185][0m |          -0.0015 |          38.6249 |          15.6367 |
[32m[20221213 22:47:19 @agent_ppo2.py:185][0m |          -0.0055 |          36.0056 |          15.6160 |
[32m[20221213 22:47:19 @agent_ppo2.py:185][0m |          -0.0044 |          35.6959 |          15.6218 |
[32m[20221213 22:47:20 @agent_ppo2.py:185][0m |          -0.0137 |          35.1658 |          15.6339 |
[32m[20221213 22:47:20 @agent_ppo2.py:185][0m |          -0.0093 |          34.9610 |          15.6274 |
[32m[20221213 22:47:20 @agent_ppo2.py:185][0m |          -0.0042 |          38.4171 |          15.6277 |
[32m[20221213 22:47:20 @agent_ppo2.py:185][0m |          -0.0124 |          34.9926 |          15.6255 |
[32m[20221213 22:47:20 @agent_ppo2.py:185][0m |           0.0004 |          37.5274 |          15.6373 |
[32m[20221213 22:47:20 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:47:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.14
[32m[20221213 22:47:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.56
[32m[20221213 22:47:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.52
[32m[20221213 22:47:20 @agent_ppo2.py:143][0m Total time:      29.12 min
[32m[20221213 22:47:20 @agent_ppo2.py:145][0m 2846720 total steps have happened
[32m[20221213 22:47:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1390 --------------------------#
[32m[20221213 22:47:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:47:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:20 @agent_ppo2.py:185][0m |           0.0002 |          40.2230 |          15.7786 |
[32m[20221213 22:47:20 @agent_ppo2.py:185][0m |           0.0053 |          38.7493 |          15.7692 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0001 |          37.2628 |          15.7651 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0107 |          36.1876 |          15.7585 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0103 |          35.7878 |          15.7628 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0129 |          35.6248 |          15.7582 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0126 |          35.2118 |          15.7527 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0180 |          35.2768 |          15.7553 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0103 |          34.8900 |          15.7377 |
[32m[20221213 22:47:21 @agent_ppo2.py:185][0m |          -0.0143 |          34.7696 |          15.7447 |
[32m[20221213 22:47:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:47:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.17
[32m[20221213 22:47:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 380.38
[32m[20221213 22:47:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.63
[32m[20221213 22:47:21 @agent_ppo2.py:143][0m Total time:      29.14 min
[32m[20221213 22:47:21 @agent_ppo2.py:145][0m 2848768 total steps have happened
[32m[20221213 22:47:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1391 --------------------------#
[32m[20221213 22:47:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:47:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |           0.0038 |          32.6791 |          15.8118 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0093 |          29.6090 |          15.7947 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0103 |          28.6777 |          15.7784 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0072 |          27.8633 |          15.7987 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0044 |          27.3112 |          15.7788 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0046 |          27.1850 |          15.7764 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0090 |          26.7456 |          15.7767 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0093 |          26.4955 |          15.7804 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0075 |          26.5333 |          15.7876 |
[32m[20221213 22:47:22 @agent_ppo2.py:185][0m |          -0.0131 |          26.0540 |          15.7862 |
[32m[20221213 22:47:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:47:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.95
[32m[20221213 22:47:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.51
[32m[20221213 22:47:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.77
[32m[20221213 22:47:23 @agent_ppo2.py:143][0m Total time:      29.16 min
[32m[20221213 22:47:23 @agent_ppo2.py:145][0m 2850816 total steps have happened
[32m[20221213 22:47:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1392 --------------------------#
[32m[20221213 22:47:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:23 @agent_ppo2.py:185][0m |           0.0008 |          32.8036 |          15.8102 |
[32m[20221213 22:47:23 @agent_ppo2.py:185][0m |          -0.0045 |          28.5772 |          15.8163 |
[32m[20221213 22:47:23 @agent_ppo2.py:185][0m |          -0.0010 |          28.0528 |          15.8007 |
[32m[20221213 22:47:23 @agent_ppo2.py:185][0m |          -0.0111 |          26.9756 |          15.7830 |
[32m[20221213 22:47:23 @agent_ppo2.py:185][0m |          -0.0017 |          26.5014 |          15.7846 |
[32m[20221213 22:47:23 @agent_ppo2.py:185][0m |          -0.0089 |          26.1713 |          15.7764 |
[32m[20221213 22:47:23 @agent_ppo2.py:185][0m |          -0.0077 |          25.8481 |          15.7741 |
[32m[20221213 22:47:24 @agent_ppo2.py:185][0m |          -0.0056 |          25.7259 |          15.7604 |
[32m[20221213 22:47:24 @agent_ppo2.py:185][0m |          -0.0112 |          25.6109 |          15.7631 |
[32m[20221213 22:47:24 @agent_ppo2.py:185][0m |          -0.0107 |          25.3573 |          15.7593 |
[32m[20221213 22:47:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.27
[32m[20221213 22:47:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.07
[32m[20221213 22:47:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.22
[32m[20221213 22:47:24 @agent_ppo2.py:143][0m Total time:      29.18 min
[32m[20221213 22:47:24 @agent_ppo2.py:145][0m 2852864 total steps have happened
[32m[20221213 22:47:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1393 --------------------------#
[32m[20221213 22:47:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:24 @agent_ppo2.py:185][0m |          -0.0017 |          32.0432 |          15.6718 |
[32m[20221213 22:47:24 @agent_ppo2.py:185][0m |          -0.0034 |          27.8339 |          15.6486 |
[32m[20221213 22:47:24 @agent_ppo2.py:185][0m |          -0.0027 |          27.2350 |          15.6247 |
[32m[20221213 22:47:24 @agent_ppo2.py:185][0m |          -0.0053 |          26.6768 |          15.6302 |
[32m[20221213 22:47:25 @agent_ppo2.py:185][0m |          -0.0081 |          26.4961 |          15.6136 |
[32m[20221213 22:47:25 @agent_ppo2.py:185][0m |          -0.0060 |          26.1426 |          15.6119 |
[32m[20221213 22:47:25 @agent_ppo2.py:185][0m |          -0.0102 |          26.0169 |          15.6149 |
[32m[20221213 22:47:25 @agent_ppo2.py:185][0m |          -0.0085 |          25.8359 |          15.5988 |
[32m[20221213 22:47:25 @agent_ppo2.py:185][0m |          -0.0096 |          25.8394 |          15.6092 |
[32m[20221213 22:47:25 @agent_ppo2.py:185][0m |          -0.0086 |          25.8446 |          15.5885 |
[32m[20221213 22:47:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:47:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.42
[32m[20221213 22:47:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.71
[32m[20221213 22:47:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.45
[32m[20221213 22:47:25 @agent_ppo2.py:143][0m Total time:      29.21 min
[32m[20221213 22:47:25 @agent_ppo2.py:145][0m 2854912 total steps have happened
[32m[20221213 22:47:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1394 --------------------------#
[32m[20221213 22:47:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |           0.0020 |          29.5913 |          15.8339 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0059 |          25.7087 |          15.8193 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0025 |          24.7491 |          15.8079 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0086 |          24.3975 |          15.7873 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0091 |          24.1320 |          15.7798 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0104 |          24.0743 |          15.7703 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0079 |          23.7762 |          15.7701 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0119 |          23.7169 |          15.7709 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0138 |          23.5189 |          15.7613 |
[32m[20221213 22:47:26 @agent_ppo2.py:185][0m |          -0.0171 |          23.4458 |          15.7500 |
[32m[20221213 22:47:26 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:47:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.68
[32m[20221213 22:47:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.76
[32m[20221213 22:47:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.97
[32m[20221213 22:47:27 @agent_ppo2.py:143][0m Total time:      29.23 min
[32m[20221213 22:47:27 @agent_ppo2.py:145][0m 2856960 total steps have happened
[32m[20221213 22:47:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1395 --------------------------#
[32m[20221213 22:47:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:27 @agent_ppo2.py:185][0m |           0.0009 |          38.8932 |          15.7418 |
[32m[20221213 22:47:27 @agent_ppo2.py:185][0m |          -0.0036 |          33.3896 |          15.7430 |
[32m[20221213 22:47:27 @agent_ppo2.py:185][0m |          -0.0013 |          32.3863 |          15.7288 |
[32m[20221213 22:47:27 @agent_ppo2.py:185][0m |          -0.0079 |          31.1616 |          15.7438 |
[32m[20221213 22:47:27 @agent_ppo2.py:185][0m |          -0.0121 |          30.4103 |          15.7283 |
[32m[20221213 22:47:27 @agent_ppo2.py:185][0m |          -0.0046 |          29.9514 |          15.7486 |
[32m[20221213 22:47:27 @agent_ppo2.py:185][0m |          -0.0133 |          29.6310 |          15.7426 |
[32m[20221213 22:47:28 @agent_ppo2.py:185][0m |          -0.0138 |          29.1788 |          15.7435 |
[32m[20221213 22:47:28 @agent_ppo2.py:185][0m |          -0.0057 |          33.1380 |          15.7361 |
[32m[20221213 22:47:28 @agent_ppo2.py:185][0m |          -0.0018 |          29.8731 |          15.7555 |
[32m[20221213 22:47:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:47:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.50
[32m[20221213 22:47:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 205.89
[32m[20221213 22:47:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.98
[32m[20221213 22:47:28 @agent_ppo2.py:143][0m Total time:      29.25 min
[32m[20221213 22:47:28 @agent_ppo2.py:145][0m 2859008 total steps have happened
[32m[20221213 22:47:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1396 --------------------------#
[32m[20221213 22:47:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:28 @agent_ppo2.py:185][0m |           0.0064 |          34.1528 |          15.7481 |
[32m[20221213 22:47:28 @agent_ppo2.py:185][0m |          -0.0055 |          31.4071 |          15.7396 |
[32m[20221213 22:47:28 @agent_ppo2.py:185][0m |          -0.0027 |          30.7681 |          15.7287 |
[32m[20221213 22:47:28 @agent_ppo2.py:185][0m |          -0.0055 |          30.3532 |          15.7221 |
[32m[20221213 22:47:29 @agent_ppo2.py:185][0m |          -0.0074 |          30.0854 |          15.7183 |
[32m[20221213 22:47:29 @agent_ppo2.py:185][0m |          -0.0057 |          29.9543 |          15.7135 |
[32m[20221213 22:47:29 @agent_ppo2.py:185][0m |          -0.0102 |          29.6760 |          15.7044 |
[32m[20221213 22:47:29 @agent_ppo2.py:185][0m |          -0.0075 |          29.6054 |          15.7088 |
[32m[20221213 22:47:29 @agent_ppo2.py:185][0m |          -0.0108 |          29.5171 |          15.7047 |
[32m[20221213 22:47:29 @agent_ppo2.py:185][0m |          -0.0104 |          29.4451 |          15.7077 |
[32m[20221213 22:47:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.83
[32m[20221213 22:47:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.20
[32m[20221213 22:47:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.90
[32m[20221213 22:47:29 @agent_ppo2.py:143][0m Total time:      29.27 min
[32m[20221213 22:47:29 @agent_ppo2.py:145][0m 2861056 total steps have happened
[32m[20221213 22:47:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1397 --------------------------#
[32m[20221213 22:47:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |           0.0006 |          38.0243 |          15.8147 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0044 |          35.1095 |          15.8066 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0110 |          34.2869 |          15.8030 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0081 |          33.6924 |          15.8042 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0001 |          37.5107 |          15.8038 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0012 |          33.5046 |          15.8034 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0120 |          32.7651 |          15.8048 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0094 |          32.7139 |          15.8179 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0118 |          32.5038 |          15.8183 |
[32m[20221213 22:47:30 @agent_ppo2.py:185][0m |          -0.0137 |          32.2921 |          15.8172 |
[32m[20221213 22:47:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:47:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.59
[32m[20221213 22:47:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.94
[32m[20221213 22:47:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.48
[32m[20221213 22:47:30 @agent_ppo2.py:143][0m Total time:      29.29 min
[32m[20221213 22:47:30 @agent_ppo2.py:145][0m 2863104 total steps have happened
[32m[20221213 22:47:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1398 --------------------------#
[32m[20221213 22:47:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0008 |          29.4773 |          15.7836 |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0019 |          26.4751 |          15.7788 |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0028 |          25.4509 |          15.7656 |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0081 |          24.7834 |          15.7749 |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0058 |          24.3178 |          15.7771 |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0027 |          24.5452 |          15.7786 |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0140 |          23.8835 |          15.7623 |
[32m[20221213 22:47:31 @agent_ppo2.py:185][0m |          -0.0153 |          23.5029 |          15.7613 |
[32m[20221213 22:47:32 @agent_ppo2.py:185][0m |          -0.0106 |          23.3311 |          15.7671 |
[32m[20221213 22:47:32 @agent_ppo2.py:185][0m |          -0.0122 |          23.1135 |          15.7825 |
[32m[20221213 22:47:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:47:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.85
[32m[20221213 22:47:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.66
[32m[20221213 22:47:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.74
[32m[20221213 22:47:32 @agent_ppo2.py:143][0m Total time:      29.31 min
[32m[20221213 22:47:32 @agent_ppo2.py:145][0m 2865152 total steps have happened
[32m[20221213 22:47:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1399 --------------------------#
[32m[20221213 22:47:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:32 @agent_ppo2.py:185][0m |           0.0013 |          42.0059 |          15.7500 |
[32m[20221213 22:47:32 @agent_ppo2.py:185][0m |          -0.0078 |          37.4776 |          15.7605 |
[32m[20221213 22:47:32 @agent_ppo2.py:185][0m |          -0.0106 |          35.5178 |          15.7458 |
[32m[20221213 22:47:32 @agent_ppo2.py:185][0m |          -0.0071 |          34.0615 |          15.7488 |
[32m[20221213 22:47:32 @agent_ppo2.py:185][0m |          -0.0128 |          33.1693 |          15.7379 |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |          -0.0110 |          32.4262 |          15.7305 |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |          -0.0143 |          31.6837 |          15.7250 |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |          -0.0118 |          31.1275 |          15.7214 |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |          -0.0145 |          30.6052 |          15.7254 |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |          -0.0141 |          30.3615 |          15.7178 |
[32m[20221213 22:47:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.80
[32m[20221213 22:47:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.70
[32m[20221213 22:47:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.94
[32m[20221213 22:47:33 @agent_ppo2.py:143][0m Total time:      29.33 min
[32m[20221213 22:47:33 @agent_ppo2.py:145][0m 2867200 total steps have happened
[32m[20221213 22:47:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1400 --------------------------#
[32m[20221213 22:47:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:47:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |           0.0051 |          33.9318 |          15.8503 |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |          -0.0048 |          27.7420 |          15.8167 |
[32m[20221213 22:47:33 @agent_ppo2.py:185][0m |          -0.0036 |          26.5598 |          15.8181 |
[32m[20221213 22:47:34 @agent_ppo2.py:185][0m |          -0.0036 |          25.8367 |          15.8183 |
[32m[20221213 22:47:34 @agent_ppo2.py:185][0m |          -0.0065 |          25.3505 |          15.8059 |
[32m[20221213 22:47:34 @agent_ppo2.py:185][0m |          -0.0124 |          24.9736 |          15.8110 |
[32m[20221213 22:47:34 @agent_ppo2.py:185][0m |          -0.0047 |          25.1937 |          15.7879 |
[32m[20221213 22:47:34 @agent_ppo2.py:185][0m |          -0.0026 |          24.9234 |          15.7983 |
[32m[20221213 22:47:34 @agent_ppo2.py:185][0m |          -0.0102 |          24.2489 |          15.7882 |
[32m[20221213 22:47:34 @agent_ppo2.py:185][0m |          -0.0083 |          24.1438 |          15.7744 |
[32m[20221213 22:47:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:47:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.23
[32m[20221213 22:47:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.44
[32m[20221213 22:47:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.51
[32m[20221213 22:47:34 @agent_ppo2.py:143][0m Total time:      29.36 min
[32m[20221213 22:47:34 @agent_ppo2.py:145][0m 2869248 total steps have happened
[32m[20221213 22:47:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1401 --------------------------#
[32m[20221213 22:47:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |           0.0017 |          36.9737 |          15.8946 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0073 |          32.9541 |          15.8894 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0089 |          32.2643 |          15.8906 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0068 |          31.7454 |          15.8706 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0082 |          31.5834 |          15.8702 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0105 |          31.3667 |          15.8595 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |           0.0011 |          34.3091 |          15.8623 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0115 |          31.3958 |          15.8742 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0120 |          30.7066 |          15.8613 |
[32m[20221213 22:47:35 @agent_ppo2.py:185][0m |          -0.0115 |          30.5977 |          15.8483 |
[32m[20221213 22:47:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:47:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.78
[32m[20221213 22:47:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.66
[32m[20221213 22:47:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.84
[32m[20221213 22:47:35 @agent_ppo2.py:143][0m Total time:      29.38 min
[32m[20221213 22:47:35 @agent_ppo2.py:145][0m 2871296 total steps have happened
[32m[20221213 22:47:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1402 --------------------------#
[32m[20221213 22:47:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0035 |          35.5660 |          15.8902 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0012 |          33.8527 |          15.9171 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0091 |          32.6571 |          15.8912 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0063 |          32.2975 |          15.8828 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0012 |          33.4025 |          15.8914 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0090 |          31.5358 |          15.8640 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0128 |          31.1488 |          15.8861 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0104 |          31.0149 |          15.8870 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |          -0.0111 |          30.8184 |          15.8846 |
[32m[20221213 22:47:36 @agent_ppo2.py:185][0m |           0.0010 |          32.3961 |          15.8845 |
[32m[20221213 22:47:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:47:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.87
[32m[20221213 22:47:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.67
[32m[20221213 22:47:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 102.39
[32m[20221213 22:47:37 @agent_ppo2.py:143][0m Total time:      29.40 min
[32m[20221213 22:47:37 @agent_ppo2.py:145][0m 2873344 total steps have happened
[32m[20221213 22:47:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1403 --------------------------#
[32m[20221213 22:47:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:37 @agent_ppo2.py:185][0m |           0.0061 |          36.9578 |          15.8966 |
[32m[20221213 22:47:37 @agent_ppo2.py:185][0m |          -0.0039 |          32.1949 |          15.8689 |
[32m[20221213 22:47:37 @agent_ppo2.py:185][0m |          -0.0060 |          31.0103 |          15.8677 |
[32m[20221213 22:47:37 @agent_ppo2.py:185][0m |          -0.0050 |          30.4385 |          15.8674 |
[32m[20221213 22:47:37 @agent_ppo2.py:185][0m |          -0.0078 |          30.1317 |          15.8725 |
[32m[20221213 22:47:37 @agent_ppo2.py:185][0m |          -0.0083 |          30.0543 |          15.8779 |
[32m[20221213 22:47:37 @agent_ppo2.py:185][0m |          -0.0082 |          29.9096 |          15.8746 |
[32m[20221213 22:47:38 @agent_ppo2.py:185][0m |          -0.0140 |          29.5466 |          15.8688 |
[32m[20221213 22:47:38 @agent_ppo2.py:185][0m |          -0.0129 |          29.2775 |          15.8810 |
[32m[20221213 22:47:38 @agent_ppo2.py:185][0m |          -0.0198 |          29.2948 |          15.8744 |
[32m[20221213 22:47:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:47:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.18
[32m[20221213 22:47:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.25
[32m[20221213 22:47:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.28
[32m[20221213 22:47:38 @agent_ppo2.py:143][0m Total time:      29.42 min
[32m[20221213 22:47:38 @agent_ppo2.py:145][0m 2875392 total steps have happened
[32m[20221213 22:47:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1404 --------------------------#
[32m[20221213 22:47:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:38 @agent_ppo2.py:185][0m |           0.0026 |          35.0270 |          15.8562 |
[32m[20221213 22:47:38 @agent_ppo2.py:185][0m |          -0.0037 |          29.5870 |          15.8279 |
[32m[20221213 22:47:38 @agent_ppo2.py:185][0m |           0.0070 |          29.3636 |          15.8335 |
[32m[20221213 22:47:38 @agent_ppo2.py:185][0m |          -0.0109 |          27.3114 |          15.8185 |
[32m[20221213 22:47:39 @agent_ppo2.py:185][0m |          -0.0105 |          26.5289 |          15.8126 |
[32m[20221213 22:47:39 @agent_ppo2.py:185][0m |          -0.0101 |          26.2801 |          15.8140 |
[32m[20221213 22:47:39 @agent_ppo2.py:185][0m |          -0.0111 |          25.8027 |          15.7975 |
[32m[20221213 22:47:39 @agent_ppo2.py:185][0m |          -0.0133 |          25.5376 |          15.7902 |
[32m[20221213 22:47:39 @agent_ppo2.py:185][0m |          -0.0106 |          25.2609 |          15.7846 |
[32m[20221213 22:47:39 @agent_ppo2.py:185][0m |          -0.0138 |          24.9191 |          15.7751 |
[32m[20221213 22:47:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:47:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.18
[32m[20221213 22:47:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 338.10
[32m[20221213 22:47:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.23
[32m[20221213 22:47:39 @agent_ppo2.py:143][0m Total time:      29.44 min
[32m[20221213 22:47:39 @agent_ppo2.py:145][0m 2877440 total steps have happened
[32m[20221213 22:47:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1405 --------------------------#
[32m[20221213 22:47:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:39 @agent_ppo2.py:185][0m |           0.0010 |          34.7426 |          15.7529 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0040 |          31.3547 |          15.7387 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0094 |          30.5836 |          15.7226 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0017 |          31.4096 |          15.6994 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0064 |          29.8408 |          15.7083 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |           0.0013 |          31.2839 |          15.7129 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0051 |          29.4007 |          15.7109 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0098 |          29.1918 |          15.7140 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0143 |          29.0909 |          15.7025 |
[32m[20221213 22:47:40 @agent_ppo2.py:185][0m |          -0.0126 |          28.9743 |          15.7072 |
[32m[20221213 22:47:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:47:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.17
[32m[20221213 22:47:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.46
[32m[20221213 22:47:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 111.15
[32m[20221213 22:47:40 @agent_ppo2.py:143][0m Total time:      29.46 min
[32m[20221213 22:47:40 @agent_ppo2.py:145][0m 2879488 total steps have happened
[32m[20221213 22:47:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1406 --------------------------#
[32m[20221213 22:47:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0006 |          39.3033 |          15.9346 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0073 |          34.8134 |          15.9052 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0071 |          32.6374 |          15.9128 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0113 |          31.3459 |          15.9051 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0122 |          30.5848 |          15.9058 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0116 |          29.7473 |          15.9034 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0128 |          29.0970 |          15.9000 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0057 |          29.0078 |          15.8874 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0134 |          28.1559 |          15.8797 |
[32m[20221213 22:47:41 @agent_ppo2.py:185][0m |          -0.0012 |          31.0326 |          15.8903 |
[32m[20221213 22:47:41 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.22
[32m[20221213 22:47:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.99
[32m[20221213 22:47:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.32
[32m[20221213 22:47:42 @agent_ppo2.py:143][0m Total time:      29.48 min
[32m[20221213 22:47:42 @agent_ppo2.py:145][0m 2881536 total steps have happened
[32m[20221213 22:47:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1407 --------------------------#
[32m[20221213 22:47:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:42 @agent_ppo2.py:185][0m |          -0.0085 |          43.8747 |          15.7862 |
[32m[20221213 22:47:42 @agent_ppo2.py:185][0m |          -0.0076 |          40.0537 |          15.7702 |
[32m[20221213 22:47:42 @agent_ppo2.py:185][0m |          -0.0099 |          39.1193 |          15.7686 |
[32m[20221213 22:47:42 @agent_ppo2.py:185][0m |          -0.0052 |          38.1354 |          15.7795 |
[32m[20221213 22:47:42 @agent_ppo2.py:185][0m |          -0.0076 |          37.6848 |          15.7781 |
[32m[20221213 22:47:42 @agent_ppo2.py:185][0m |          -0.0089 |          36.9809 |          15.7677 |
[32m[20221213 22:47:42 @agent_ppo2.py:185][0m |          -0.0117 |          36.7586 |          15.7682 |
[32m[20221213 22:47:43 @agent_ppo2.py:185][0m |          -0.0130 |          36.3235 |          15.7761 |
[32m[20221213 22:47:43 @agent_ppo2.py:185][0m |          -0.0086 |          36.7136 |          15.7729 |
[32m[20221213 22:47:43 @agent_ppo2.py:185][0m |          -0.0072 |          35.9090 |          15.7835 |
[32m[20221213 22:47:43 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.98
[32m[20221213 22:47:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 337.22
[32m[20221213 22:47:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.32
[32m[20221213 22:47:43 @agent_ppo2.py:143][0m Total time:      29.50 min
[32m[20221213 22:47:43 @agent_ppo2.py:145][0m 2883584 total steps have happened
[32m[20221213 22:47:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1408 --------------------------#
[32m[20221213 22:47:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:43 @agent_ppo2.py:185][0m |          -0.0004 |          20.7966 |          15.8383 |
[32m[20221213 22:47:43 @agent_ppo2.py:185][0m |          -0.0010 |          17.2818 |          15.8365 |
[32m[20221213 22:47:43 @agent_ppo2.py:185][0m |          -0.0075 |          16.8019 |          15.8318 |
[32m[20221213 22:47:43 @agent_ppo2.py:185][0m |          -0.0086 |          16.5890 |          15.8311 |
[32m[20221213 22:47:44 @agent_ppo2.py:185][0m |          -0.0036 |          17.2120 |          15.8178 |
[32m[20221213 22:47:44 @agent_ppo2.py:185][0m |          -0.0008 |          16.4174 |          15.8016 |
[32m[20221213 22:47:44 @agent_ppo2.py:185][0m |          -0.0089 |          16.0997 |          15.8112 |
[32m[20221213 22:47:44 @agent_ppo2.py:185][0m |          -0.0076 |          16.0148 |          15.8042 |
[32m[20221213 22:47:44 @agent_ppo2.py:185][0m |          -0.0134 |          15.8550 |          15.7988 |
[32m[20221213 22:47:44 @agent_ppo2.py:185][0m |          -0.0069 |          15.8184 |          15.7895 |
[32m[20221213 22:47:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 351.04
[32m[20221213 22:47:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.89
[32m[20221213 22:47:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.95
[32m[20221213 22:47:44 @agent_ppo2.py:143][0m Total time:      29.52 min
[32m[20221213 22:47:44 @agent_ppo2.py:145][0m 2885632 total steps have happened
[32m[20221213 22:47:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1409 --------------------------#
[32m[20221213 22:47:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:44 @agent_ppo2.py:185][0m |           0.0015 |          26.8261 |          15.9527 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0065 |          24.5583 |          15.9356 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0109 |          23.8808 |          15.9251 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0062 |          23.4553 |          15.8899 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0076 |          23.1480 |          15.8998 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0108 |          22.9125 |          15.8808 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0118 |          22.8722 |          15.8731 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0131 |          22.5792 |          15.8726 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0116 |          22.4748 |          15.8626 |
[32m[20221213 22:47:45 @agent_ppo2.py:185][0m |          -0.0092 |          23.0293 |          15.8671 |
[32m[20221213 22:47:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:47:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.02
[32m[20221213 22:47:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.74
[32m[20221213 22:47:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.73
[32m[20221213 22:47:45 @agent_ppo2.py:143][0m Total time:      29.54 min
[32m[20221213 22:47:45 @agent_ppo2.py:145][0m 2887680 total steps have happened
[32m[20221213 22:47:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1410 --------------------------#
[32m[20221213 22:47:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:47:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |           0.0041 |          26.0736 |          15.7455 |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |          -0.0064 |          23.0584 |          15.7169 |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |          -0.0077 |          22.2243 |          15.6984 |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |          -0.0067 |          21.6901 |          15.6948 |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |          -0.0062 |          21.3098 |          15.6882 |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |          -0.0090 |          21.0095 |          15.6928 |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |          -0.0124 |          20.8439 |          15.6740 |
[32m[20221213 22:47:46 @agent_ppo2.py:185][0m |          -0.0062 |          20.6507 |          15.6704 |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |          -0.0119 |          20.4722 |          15.6672 |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |          -0.0134 |          20.3890 |          15.6602 |
[32m[20221213 22:47:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:47:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.71
[32m[20221213 22:47:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.48
[32m[20221213 22:47:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.78
[32m[20221213 22:47:47 @agent_ppo2.py:143][0m Total time:      29.56 min
[32m[20221213 22:47:47 @agent_ppo2.py:145][0m 2889728 total steps have happened
[32m[20221213 22:47:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1411 --------------------------#
[32m[20221213 22:47:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |           0.0009 |          36.6099 |          15.6352 |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |          -0.0033 |          33.7962 |          15.6350 |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |           0.0122 |          36.7345 |          15.6394 |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |          -0.0025 |          32.4668 |          15.6213 |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |          -0.0066 |          31.8597 |          15.6318 |
[32m[20221213 22:47:47 @agent_ppo2.py:185][0m |          -0.0048 |          31.6719 |          15.6332 |
[32m[20221213 22:47:48 @agent_ppo2.py:185][0m |          -0.0047 |          31.4500 |          15.6164 |
[32m[20221213 22:47:48 @agent_ppo2.py:185][0m |          -0.0106 |          31.3401 |          15.6485 |
[32m[20221213 22:47:48 @agent_ppo2.py:185][0m |          -0.0034 |          31.1603 |          15.6213 |
[32m[20221213 22:47:48 @agent_ppo2.py:185][0m |          -0.0050 |          31.0899 |          15.6369 |
[32m[20221213 22:47:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:47:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.75
[32m[20221213 22:47:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.35
[32m[20221213 22:47:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.80
[32m[20221213 22:47:48 @agent_ppo2.py:143][0m Total time:      29.58 min
[32m[20221213 22:47:48 @agent_ppo2.py:145][0m 2891776 total steps have happened
[32m[20221213 22:47:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1412 --------------------------#
[32m[20221213 22:47:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:48 @agent_ppo2.py:185][0m |          -0.0026 |          29.8352 |          15.7844 |
[32m[20221213 22:47:48 @agent_ppo2.py:185][0m |           0.0077 |          30.8688 |          15.7839 |
[32m[20221213 22:47:48 @agent_ppo2.py:185][0m |          -0.0052 |          27.5039 |          15.7562 |
[32m[20221213 22:47:49 @agent_ppo2.py:185][0m |          -0.0072 |          26.9989 |          15.7573 |
[32m[20221213 22:47:49 @agent_ppo2.py:185][0m |          -0.0107 |          26.8292 |          15.7570 |
[32m[20221213 22:47:49 @agent_ppo2.py:185][0m |          -0.0044 |          26.6510 |          15.7460 |
[32m[20221213 22:47:49 @agent_ppo2.py:185][0m |          -0.0077 |          26.4973 |          15.7394 |
[32m[20221213 22:47:49 @agent_ppo2.py:185][0m |          -0.0113 |          26.3773 |          15.7356 |
[32m[20221213 22:47:49 @agent_ppo2.py:185][0m |          -0.0135 |          26.2503 |          15.7235 |
[32m[20221213 22:47:49 @agent_ppo2.py:185][0m |          -0.0129 |          26.2525 |          15.7221 |
[32m[20221213 22:47:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:47:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.39
[32m[20221213 22:47:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.70
[32m[20221213 22:47:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.43
[32m[20221213 22:47:49 @agent_ppo2.py:143][0m Total time:      29.61 min
[32m[20221213 22:47:49 @agent_ppo2.py:145][0m 2893824 total steps have happened
[32m[20221213 22:47:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1413 --------------------------#
[32m[20221213 22:47:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0042 |          31.3028 |          15.6779 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0043 |          29.3390 |          15.6482 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0060 |          28.4272 |          15.6220 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0119 |          27.7904 |          15.6325 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0126 |          27.4963 |          15.6301 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0087 |          27.0227 |          15.6172 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0151 |          26.5292 |          15.6333 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0159 |          26.3209 |          15.6208 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0061 |          29.2338 |          15.6143 |
[32m[20221213 22:47:50 @agent_ppo2.py:185][0m |          -0.0141 |          26.4445 |          15.6029 |
[32m[20221213 22:47:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.56
[32m[20221213 22:47:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 360.29
[32m[20221213 22:47:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.90
[32m[20221213 22:47:50 @agent_ppo2.py:143][0m Total time:      29.63 min
[32m[20221213 22:47:50 @agent_ppo2.py:145][0m 2895872 total steps have happened
[32m[20221213 22:47:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1414 --------------------------#
[32m[20221213 22:47:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |           0.0040 |          40.2460 |          15.8030 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |          -0.0050 |          38.4122 |          15.7761 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |          -0.0033 |          37.9747 |          15.7840 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |          -0.0082 |          37.6714 |          15.7858 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |          -0.0088 |          37.3399 |          15.7821 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |          -0.0091 |          37.1302 |          15.7967 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |           0.0003 |          39.7460 |          15.7933 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |          -0.0110 |          36.7752 |          15.7848 |
[32m[20221213 22:47:51 @agent_ppo2.py:185][0m |          -0.0139 |          36.6046 |          15.7919 |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0108 |          36.5210 |          15.7905 |
[32m[20221213 22:47:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:47:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.23
[32m[20221213 22:47:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.62
[32m[20221213 22:47:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.06
[32m[20221213 22:47:52 @agent_ppo2.py:143][0m Total time:      29.65 min
[32m[20221213 22:47:52 @agent_ppo2.py:145][0m 2897920 total steps have happened
[32m[20221213 22:47:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1415 --------------------------#
[32m[20221213 22:47:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0006 |          32.0409 |          15.8906 |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0102 |          27.5318 |          15.8677 |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0026 |          25.9254 |          15.8541 |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0086 |          25.4714 |          15.8454 |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0183 |          25.3365 |          15.8505 |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0098 |          24.5483 |          15.8456 |
[32m[20221213 22:47:52 @agent_ppo2.py:185][0m |          -0.0123 |          24.3259 |          15.8289 |
[32m[20221213 22:47:53 @agent_ppo2.py:185][0m |          -0.0167 |          24.1747 |          15.8403 |
[32m[20221213 22:47:53 @agent_ppo2.py:185][0m |          -0.0096 |          24.1879 |          15.8392 |
[32m[20221213 22:47:53 @agent_ppo2.py:185][0m |          -0.0121 |          23.6436 |          15.8385 |
[32m[20221213 22:47:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:47:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.49
[32m[20221213 22:47:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.36
[32m[20221213 22:47:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.45
[32m[20221213 22:47:53 @agent_ppo2.py:143][0m Total time:      29.67 min
[32m[20221213 22:47:53 @agent_ppo2.py:145][0m 2899968 total steps have happened
[32m[20221213 22:47:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1416 --------------------------#
[32m[20221213 22:47:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:53 @agent_ppo2.py:185][0m |           0.0013 |          30.9189 |          15.5958 |
[32m[20221213 22:47:53 @agent_ppo2.py:185][0m |           0.0018 |          27.9947 |          15.5789 |
[32m[20221213 22:47:53 @agent_ppo2.py:185][0m |          -0.0044 |          26.9472 |          15.5615 |
[32m[20221213 22:47:53 @agent_ppo2.py:185][0m |           0.0013 |          28.2470 |          15.5754 |
[32m[20221213 22:47:54 @agent_ppo2.py:185][0m |          -0.0006 |          28.0389 |          15.5547 |
[32m[20221213 22:47:54 @agent_ppo2.py:185][0m |          -0.0090 |          25.7193 |          15.5464 |
[32m[20221213 22:47:54 @agent_ppo2.py:185][0m |          -0.0016 |          26.4811 |          15.5493 |
[32m[20221213 22:47:54 @agent_ppo2.py:185][0m |          -0.0145 |          25.2764 |          15.5525 |
[32m[20221213 22:47:54 @agent_ppo2.py:185][0m |          -0.0144 |          25.0116 |          15.5380 |
[32m[20221213 22:47:54 @agent_ppo2.py:185][0m |          -0.0126 |          24.8792 |          15.5385 |
[32m[20221213 22:47:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:47:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.84
[32m[20221213 22:47:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.35
[32m[20221213 22:47:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.62
[32m[20221213 22:47:54 @agent_ppo2.py:143][0m Total time:      29.69 min
[32m[20221213 22:47:54 @agent_ppo2.py:145][0m 2902016 total steps have happened
[32m[20221213 22:47:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1417 --------------------------#
[32m[20221213 22:47:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:54 @agent_ppo2.py:185][0m |           0.0039 |          40.2928 |          15.8275 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0016 |          38.1382 |          15.8318 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0031 |          38.3066 |          15.8293 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0022 |          37.1495 |          15.8327 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0112 |          36.6938 |          15.8373 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0101 |          36.7391 |          15.8265 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0067 |          36.3427 |          15.8397 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0060 |          36.2247 |          15.8390 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0138 |          36.0413 |          15.8474 |
[32m[20221213 22:47:55 @agent_ppo2.py:185][0m |          -0.0118 |          35.8263 |          15.8409 |
[32m[20221213 22:47:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:47:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.69
[32m[20221213 22:47:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.93
[32m[20221213 22:47:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.31
[32m[20221213 22:47:55 @agent_ppo2.py:143][0m Total time:      29.71 min
[32m[20221213 22:47:55 @agent_ppo2.py:145][0m 2904064 total steps have happened
[32m[20221213 22:47:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1418 --------------------------#
[32m[20221213 22:47:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |           0.0021 |          30.0387 |          15.8265 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0035 |          27.5336 |          15.8141 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0015 |          27.7565 |          15.8038 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |           0.0010 |          27.2190 |          15.8147 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0105 |          26.3711 |          15.8245 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0053 |          27.5785 |          15.8349 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0044 |          28.3796 |          15.8324 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0140 |          25.9002 |          15.8133 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0118 |          25.5136 |          15.8455 |
[32m[20221213 22:47:56 @agent_ppo2.py:185][0m |          -0.0131 |          25.4093 |          15.8491 |
[32m[20221213 22:47:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:47:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.88
[32m[20221213 22:47:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.32
[32m[20221213 22:47:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.48
[32m[20221213 22:47:57 @agent_ppo2.py:143][0m Total time:      29.73 min
[32m[20221213 22:47:57 @agent_ppo2.py:145][0m 2906112 total steps have happened
[32m[20221213 22:47:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1419 --------------------------#
[32m[20221213 22:47:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:47:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |           0.0001 |          34.5125 |          15.7921 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0002 |          32.4278 |          15.7739 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0007 |          31.2768 |          15.7588 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0078 |          31.0375 |          15.7664 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0099 |          30.5197 |          15.7569 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0044 |          30.2725 |          15.7718 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0059 |          30.2703 |          15.7792 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0138 |          29.8928 |          15.7755 |
[32m[20221213 22:47:57 @agent_ppo2.py:185][0m |          -0.0103 |          29.7310 |          15.7658 |
[32m[20221213 22:47:58 @agent_ppo2.py:185][0m |          -0.0120 |          29.6369 |          15.7667 |
[32m[20221213 22:47:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:47:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.22
[32m[20221213 22:47:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.65
[32m[20221213 22:47:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.27
[32m[20221213 22:47:58 @agent_ppo2.py:143][0m Total time:      29.75 min
[32m[20221213 22:47:58 @agent_ppo2.py:145][0m 2908160 total steps have happened
[32m[20221213 22:47:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1420 --------------------------#
[32m[20221213 22:47:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:47:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:58 @agent_ppo2.py:185][0m |          -0.0011 |          37.3635 |          15.7947 |
[32m[20221213 22:47:58 @agent_ppo2.py:185][0m |          -0.0062 |          35.6231 |          15.7869 |
[32m[20221213 22:47:58 @agent_ppo2.py:185][0m |          -0.0090 |          35.0096 |          15.7906 |
[32m[20221213 22:47:58 @agent_ppo2.py:185][0m |          -0.0128 |          34.6382 |          15.7951 |
[32m[20221213 22:47:58 @agent_ppo2.py:185][0m |          -0.0117 |          34.3750 |          15.7877 |
[32m[20221213 22:47:58 @agent_ppo2.py:185][0m |          -0.0118 |          34.0515 |          15.7889 |
[32m[20221213 22:47:59 @agent_ppo2.py:185][0m |          -0.0125 |          33.9500 |          15.7797 |
[32m[20221213 22:47:59 @agent_ppo2.py:185][0m |          -0.0021 |          37.0562 |          15.7975 |
[32m[20221213 22:47:59 @agent_ppo2.py:185][0m |          -0.0118 |          33.8605 |          15.7804 |
[32m[20221213 22:47:59 @agent_ppo2.py:185][0m |          -0.0155 |          33.6276 |          15.7654 |
[32m[20221213 22:47:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:47:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.02
[32m[20221213 22:47:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.61
[32m[20221213 22:47:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.91
[32m[20221213 22:47:59 @agent_ppo2.py:143][0m Total time:      29.77 min
[32m[20221213 22:47:59 @agent_ppo2.py:145][0m 2910208 total steps have happened
[32m[20221213 22:47:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1421 --------------------------#
[32m[20221213 22:47:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:47:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:47:59 @agent_ppo2.py:185][0m |          -0.0022 |          33.1246 |          15.8705 |
[32m[20221213 22:47:59 @agent_ppo2.py:185][0m |          -0.0045 |          31.0413 |          15.8451 |
[32m[20221213 22:47:59 @agent_ppo2.py:185][0m |          -0.0064 |          31.4901 |          15.8459 |
[32m[20221213 22:48:00 @agent_ppo2.py:185][0m |          -0.0016 |          30.5559 |          15.8334 |
[32m[20221213 22:48:00 @agent_ppo2.py:185][0m |          -0.0041 |          29.9007 |          15.8212 |
[32m[20221213 22:48:00 @agent_ppo2.py:185][0m |          -0.0082 |          29.6237 |          15.8297 |
[32m[20221213 22:48:00 @agent_ppo2.py:185][0m |          -0.0037 |          29.8950 |          15.8387 |
[32m[20221213 22:48:00 @agent_ppo2.py:185][0m |          -0.0133 |          29.3583 |          15.8307 |
[32m[20221213 22:48:00 @agent_ppo2.py:185][0m |          -0.0135 |          29.1878 |          15.8390 |
[32m[20221213 22:48:00 @agent_ppo2.py:185][0m |          -0.0124 |          29.0130 |          15.8400 |
[32m[20221213 22:48:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:48:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.57
[32m[20221213 22:48:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 328.20
[32m[20221213 22:48:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.29
[32m[20221213 22:48:00 @agent_ppo2.py:143][0m Total time:      29.79 min
[32m[20221213 22:48:00 @agent_ppo2.py:145][0m 2912256 total steps have happened
[32m[20221213 22:48:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1422 --------------------------#
[32m[20221213 22:48:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:48:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0000 |          33.1778 |          15.6919 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0036 |          31.0395 |          15.6966 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0048 |          30.0164 |          15.6859 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0143 |          29.5839 |          15.6917 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0114 |          29.2797 |          15.6940 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0132 |          29.0893 |          15.6910 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0152 |          28.6304 |          15.7028 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0156 |          28.3580 |          15.7005 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0152 |          28.1643 |          15.7109 |
[32m[20221213 22:48:01 @agent_ppo2.py:185][0m |          -0.0113 |          27.9721 |          15.7070 |
[32m[20221213 22:48:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.51
[32m[20221213 22:48:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.09
[32m[20221213 22:48:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.94
[32m[20221213 22:48:02 @agent_ppo2.py:143][0m Total time:      29.81 min
[32m[20221213 22:48:02 @agent_ppo2.py:145][0m 2914304 total steps have happened
[32m[20221213 22:48:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1423 --------------------------#
[32m[20221213 22:48:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |          -0.0016 |          33.6356 |          15.7976 |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |          -0.0035 |          31.6099 |          15.7961 |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |          -0.0032 |          31.3348 |          15.8052 |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |          -0.0055 |          30.9395 |          15.8121 |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |          -0.0076 |          30.7884 |          15.8078 |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |          -0.0104 |          30.8702 |          15.8215 |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |          -0.0106 |          30.7760 |          15.8223 |
[32m[20221213 22:48:02 @agent_ppo2.py:185][0m |           0.0002 |          33.8567 |          15.8189 |
[32m[20221213 22:48:03 @agent_ppo2.py:185][0m |          -0.0054 |          30.5674 |          15.8202 |
[32m[20221213 22:48:03 @agent_ppo2.py:185][0m |          -0.0112 |          30.4539 |          15.8228 |
[32m[20221213 22:48:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:48:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.94
[32m[20221213 22:48:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.07
[32m[20221213 22:48:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.39
[32m[20221213 22:48:03 @agent_ppo2.py:143][0m Total time:      29.83 min
[32m[20221213 22:48:03 @agent_ppo2.py:145][0m 2916352 total steps have happened
[32m[20221213 22:48:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1424 --------------------------#
[32m[20221213 22:48:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:48:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:03 @agent_ppo2.py:185][0m |           0.0011 |          33.7624 |          15.8111 |
[32m[20221213 22:48:03 @agent_ppo2.py:185][0m |          -0.0065 |          30.4998 |          15.8163 |
[32m[20221213 22:48:03 @agent_ppo2.py:185][0m |          -0.0071 |          28.7906 |          15.8019 |
[32m[20221213 22:48:03 @agent_ppo2.py:185][0m |          -0.0053 |          27.8614 |          15.8110 |
[32m[20221213 22:48:04 @agent_ppo2.py:185][0m |           0.0013 |          30.0176 |          15.8050 |
[32m[20221213 22:48:04 @agent_ppo2.py:185][0m |          -0.0115 |          27.1837 |          15.7809 |
[32m[20221213 22:48:04 @agent_ppo2.py:185][0m |          -0.0108 |          26.6542 |          15.8004 |
[32m[20221213 22:48:04 @agent_ppo2.py:185][0m |          -0.0129 |          26.4263 |          15.8077 |
[32m[20221213 22:48:04 @agent_ppo2.py:185][0m |          -0.0102 |          26.1496 |          15.7960 |
[32m[20221213 22:48:04 @agent_ppo2.py:185][0m |          -0.0079 |          25.9160 |          15.7975 |
[32m[20221213 22:48:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:48:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.64
[32m[20221213 22:48:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.48
[32m[20221213 22:48:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.11
[32m[20221213 22:48:04 @agent_ppo2.py:143][0m Total time:      29.85 min
[32m[20221213 22:48:04 @agent_ppo2.py:145][0m 2918400 total steps have happened
[32m[20221213 22:48:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1425 --------------------------#
[32m[20221213 22:48:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:04 @agent_ppo2.py:185][0m |          -0.0056 |          29.2015 |          15.9069 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0094 |          24.9060 |          15.9170 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0090 |          23.9339 |          15.9188 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0142 |          23.1915 |          15.9119 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0081 |          22.6094 |          15.9168 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0081 |          22.1931 |          15.9192 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0128 |          21.8627 |          15.9098 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0126 |          21.8589 |          15.9162 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0180 |          21.5523 |          15.9126 |
[32m[20221213 22:48:05 @agent_ppo2.py:185][0m |          -0.0074 |          21.1682 |          15.9179 |
[32m[20221213 22:48:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:48:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.59
[32m[20221213 22:48:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.38
[32m[20221213 22:48:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.02
[32m[20221213 22:48:05 @agent_ppo2.py:143][0m Total time:      29.87 min
[32m[20221213 22:48:05 @agent_ppo2.py:145][0m 2920448 total steps have happened
[32m[20221213 22:48:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1426 --------------------------#
[32m[20221213 22:48:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:48:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |           0.0012 |          43.0978 |          15.9661 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |           0.0044 |          43.0873 |          15.9545 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |          -0.0022 |          39.5632 |          15.9325 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |          -0.0056 |          38.9560 |          15.9201 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |          -0.0063 |          38.5111 |          15.9233 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |          -0.0124 |          38.3279 |          15.9205 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |          -0.0080 |          37.8784 |          15.9151 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |          -0.0090 |          37.7714 |          15.9132 |
[32m[20221213 22:48:06 @agent_ppo2.py:185][0m |          -0.0109 |          37.6055 |          15.8939 |
[32m[20221213 22:48:07 @agent_ppo2.py:185][0m |          -0.0108 |          37.4059 |          15.9057 |
[32m[20221213 22:48:07 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 22:48:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.50
[32m[20221213 22:48:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 329.83
[32m[20221213 22:48:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.72
[32m[20221213 22:48:07 @agent_ppo2.py:143][0m Total time:      29.90 min
[32m[20221213 22:48:07 @agent_ppo2.py:145][0m 2922496 total steps have happened
[32m[20221213 22:48:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1427 --------------------------#
[32m[20221213 22:48:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 22:48:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:07 @agent_ppo2.py:185][0m |           0.0004 |          31.6982 |          15.8130 |
[32m[20221213 22:48:07 @agent_ppo2.py:185][0m |          -0.0065 |          28.9496 |          15.7913 |
[32m[20221213 22:48:07 @agent_ppo2.py:185][0m |          -0.0068 |          27.8089 |          15.7862 |
[32m[20221213 22:48:08 @agent_ppo2.py:185][0m |          -0.0065 |          27.2081 |          15.7975 |
[32m[20221213 22:48:08 @agent_ppo2.py:185][0m |          -0.0070 |          26.6167 |          15.7992 |
[32m[20221213 22:48:08 @agent_ppo2.py:185][0m |          -0.0142 |          26.3911 |          15.7850 |
[32m[20221213 22:48:08 @agent_ppo2.py:185][0m |          -0.0075 |          25.8610 |          15.7805 |
[32m[20221213 22:48:08 @agent_ppo2.py:185][0m |          -0.0114 |          25.5897 |          15.7894 |
[32m[20221213 22:48:08 @agent_ppo2.py:185][0m |          -0.0052 |          25.8272 |          15.7927 |
[32m[20221213 22:48:08 @agent_ppo2.py:185][0m |          -0.0115 |          25.0715 |          15.7856 |
[32m[20221213 22:48:08 @agent_ppo2.py:130][0m Policy update time: 1.50 s
[32m[20221213 22:48:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.64
[32m[20221213 22:48:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 320.45
[32m[20221213 22:48:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.18
[32m[20221213 22:48:09 @agent_ppo2.py:143][0m Total time:      29.93 min
[32m[20221213 22:48:09 @agent_ppo2.py:145][0m 2924544 total steps have happened
[32m[20221213 22:48:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1428 --------------------------#
[32m[20221213 22:48:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:09 @agent_ppo2.py:185][0m |           0.0014 |          34.0293 |          15.9508 |
[32m[20221213 22:48:09 @agent_ppo2.py:185][0m |           0.0006 |          32.1192 |          15.9420 |
[32m[20221213 22:48:09 @agent_ppo2.py:185][0m |          -0.0024 |          31.1224 |          15.9299 |
[32m[20221213 22:48:09 @agent_ppo2.py:185][0m |          -0.0041 |          30.6029 |          15.9440 |
[32m[20221213 22:48:09 @agent_ppo2.py:185][0m |          -0.0061 |          30.2093 |          15.9351 |
[32m[20221213 22:48:09 @agent_ppo2.py:185][0m |          -0.0059 |          30.0015 |          15.9224 |
[32m[20221213 22:48:10 @agent_ppo2.py:185][0m |          -0.0082 |          29.7383 |          15.9359 |
[32m[20221213 22:48:10 @agent_ppo2.py:185][0m |          -0.0053 |          29.5862 |          15.9225 |
[32m[20221213 22:48:10 @agent_ppo2.py:185][0m |          -0.0123 |          29.4442 |          15.9210 |
[32m[20221213 22:48:10 @agent_ppo2.py:185][0m |          -0.0111 |          29.2296 |          15.9211 |
[32m[20221213 22:48:10 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.57
[32m[20221213 22:48:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 318.83
[32m[20221213 22:48:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.36
[32m[20221213 22:48:10 @agent_ppo2.py:143][0m Total time:      29.95 min
[32m[20221213 22:48:10 @agent_ppo2.py:145][0m 2926592 total steps have happened
[32m[20221213 22:48:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1429 --------------------------#
[32m[20221213 22:48:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:10 @agent_ppo2.py:185][0m |          -0.0037 |          38.1735 |          15.6196 |
[32m[20221213 22:48:10 @agent_ppo2.py:185][0m |          -0.0056 |          34.6220 |          15.6196 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0054 |          33.7762 |          15.6124 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0103 |          33.3845 |          15.6020 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0060 |          33.0424 |          15.5920 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0079 |          33.3065 |          15.6072 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0123 |          32.6012 |          15.5969 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0130 |          33.0956 |          15.6057 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0093 |          32.2212 |          15.6042 |
[32m[20221213 22:48:11 @agent_ppo2.py:185][0m |          -0.0097 |          32.2836 |          15.6091 |
[32m[20221213 22:48:11 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:48:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.07
[32m[20221213 22:48:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.94
[32m[20221213 22:48:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.08
[32m[20221213 22:48:11 @agent_ppo2.py:143][0m Total time:      29.97 min
[32m[20221213 22:48:11 @agent_ppo2.py:145][0m 2928640 total steps have happened
[32m[20221213 22:48:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1430 --------------------------#
[32m[20221213 22:48:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:48:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |           0.0005 |          43.5605 |          15.8122 |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |          -0.0025 |          39.6182 |          15.7938 |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |          -0.0083 |          37.6028 |          15.7778 |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |          -0.0030 |          37.1471 |          15.7799 |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |          -0.0057 |          36.1422 |          15.7923 |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |          -0.0081 |          35.6003 |          15.7919 |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |          -0.0094 |          35.2145 |          15.7667 |
[32m[20221213 22:48:12 @agent_ppo2.py:185][0m |          -0.0130 |          34.9710 |          15.7838 |
[32m[20221213 22:48:13 @agent_ppo2.py:185][0m |          -0.0100 |          34.4782 |          15.7744 |
[32m[20221213 22:48:13 @agent_ppo2.py:185][0m |          -0.0106 |          34.2382 |          15.7726 |
[32m[20221213 22:48:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.83
[32m[20221213 22:48:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.07
[32m[20221213 22:48:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.17
[32m[20221213 22:48:13 @agent_ppo2.py:143][0m Total time:      30.00 min
[32m[20221213 22:48:13 @agent_ppo2.py:145][0m 2930688 total steps have happened
[32m[20221213 22:48:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1431 --------------------------#
[32m[20221213 22:48:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:13 @agent_ppo2.py:185][0m |          -0.0001 |          39.2058 |          15.8253 |
[32m[20221213 22:48:13 @agent_ppo2.py:185][0m |          -0.0038 |          36.7682 |          15.8011 |
[32m[20221213 22:48:13 @agent_ppo2.py:185][0m |          -0.0114 |          35.9983 |          15.7952 |
[32m[20221213 22:48:13 @agent_ppo2.py:185][0m |          -0.0100 |          35.1848 |          15.7911 |
[32m[20221213 22:48:13 @agent_ppo2.py:185][0m |          -0.0014 |          36.0245 |          15.7959 |
[32m[20221213 22:48:14 @agent_ppo2.py:185][0m |          -0.0136 |          34.3281 |          15.7805 |
[32m[20221213 22:48:14 @agent_ppo2.py:185][0m |          -0.0115 |          33.9995 |          15.7916 |
[32m[20221213 22:48:14 @agent_ppo2.py:185][0m |          -0.0116 |          33.7732 |          15.7920 |
[32m[20221213 22:48:14 @agent_ppo2.py:185][0m |          -0.0088 |          34.4447 |          15.7787 |
[32m[20221213 22:48:14 @agent_ppo2.py:185][0m |          -0.0131 |          33.3944 |          15.7799 |
[32m[20221213 22:48:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.31
[32m[20221213 22:48:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.90
[32m[20221213 22:48:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.17
[32m[20221213 22:48:14 @agent_ppo2.py:143][0m Total time:      30.02 min
[32m[20221213 22:48:14 @agent_ppo2.py:145][0m 2932736 total steps have happened
[32m[20221213 22:48:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1432 --------------------------#
[32m[20221213 22:48:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:14 @agent_ppo2.py:185][0m |           0.0003 |          30.5093 |          15.8108 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |           0.0046 |          31.5035 |          15.8005 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0058 |          27.6567 |          15.7982 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0067 |          27.0633 |          15.8096 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0071 |          26.6633 |          15.7653 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0092 |          26.5737 |          15.7712 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0093 |          26.2946 |          15.7748 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0131 |          26.1127 |          15.7564 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0152 |          26.0502 |          15.7707 |
[32m[20221213 22:48:15 @agent_ppo2.py:185][0m |          -0.0169 |          25.9950 |          15.7586 |
[32m[20221213 22:48:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.34
[32m[20221213 22:48:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.03
[32m[20221213 22:48:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.68
[32m[20221213 22:48:15 @agent_ppo2.py:143][0m Total time:      30.04 min
[32m[20221213 22:48:15 @agent_ppo2.py:145][0m 2934784 total steps have happened
[32m[20221213 22:48:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1433 --------------------------#
[32m[20221213 22:48:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |           0.0038 |          36.4440 |          15.7706 |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |          -0.0053 |          34.1847 |          15.7847 |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |          -0.0117 |          33.2525 |          15.7883 |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |          -0.0079 |          32.6439 |          15.7745 |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |          -0.0065 |          32.1185 |          15.7649 |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |          -0.0106 |          31.6630 |          15.7574 |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |          -0.0052 |          31.4236 |          15.7543 |
[32m[20221213 22:48:16 @agent_ppo2.py:185][0m |          -0.0104 |          31.1131 |          15.7559 |
[32m[20221213 22:48:17 @agent_ppo2.py:185][0m |          -0.0140 |          30.9420 |          15.7596 |
[32m[20221213 22:48:17 @agent_ppo2.py:185][0m |          -0.0120 |          30.5987 |          15.7582 |
[32m[20221213 22:48:17 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:48:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.49
[32m[20221213 22:48:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.97
[32m[20221213 22:48:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.11
[32m[20221213 22:48:17 @agent_ppo2.py:143][0m Total time:      30.07 min
[32m[20221213 22:48:17 @agent_ppo2.py:145][0m 2936832 total steps have happened
[32m[20221213 22:48:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1434 --------------------------#
[32m[20221213 22:48:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:17 @agent_ppo2.py:185][0m |           0.0090 |          45.8411 |          15.8710 |
[32m[20221213 22:48:17 @agent_ppo2.py:185][0m |          -0.0022 |          44.4740 |          15.8572 |
[32m[20221213 22:48:17 @agent_ppo2.py:185][0m |          -0.0101 |          43.0209 |          15.8709 |
[32m[20221213 22:48:18 @agent_ppo2.py:185][0m |          -0.0086 |          43.3188 |          15.8601 |
[32m[20221213 22:48:18 @agent_ppo2.py:185][0m |          -0.0130 |          42.5164 |          15.8613 |
[32m[20221213 22:48:18 @agent_ppo2.py:185][0m |          -0.0093 |          41.8328 |          15.8585 |
[32m[20221213 22:48:18 @agent_ppo2.py:185][0m |          -0.0123 |          41.5966 |          15.8567 |
[32m[20221213 22:48:18 @agent_ppo2.py:185][0m |          -0.0131 |          41.4208 |          15.8701 |
[32m[20221213 22:48:18 @agent_ppo2.py:185][0m |          -0.0081 |          43.4868 |          15.8671 |
[32m[20221213 22:48:18 @agent_ppo2.py:185][0m |          -0.0165 |          41.1932 |          15.8777 |
[32m[20221213 22:48:18 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:48:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.18
[32m[20221213 22:48:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.83
[32m[20221213 22:48:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.22
[32m[20221213 22:48:18 @agent_ppo2.py:143][0m Total time:      30.09 min
[32m[20221213 22:48:18 @agent_ppo2.py:145][0m 2938880 total steps have happened
[32m[20221213 22:48:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1435 --------------------------#
[32m[20221213 22:48:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0005 |          35.2493 |          15.8494 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0052 |          32.0511 |          15.8478 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0041 |          31.4584 |          15.8467 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0069 |          30.8240 |          15.8424 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0133 |          30.2113 |          15.8478 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0101 |          29.9892 |          15.8447 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0165 |          29.8370 |          15.8412 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0166 |          29.5724 |          15.8382 |
[32m[20221213 22:48:19 @agent_ppo2.py:185][0m |          -0.0131 |          29.3479 |          15.8331 |
[32m[20221213 22:48:20 @agent_ppo2.py:185][0m |          -0.0137 |          29.2571 |          15.8407 |
[32m[20221213 22:48:20 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:48:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.49
[32m[20221213 22:48:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.04
[32m[20221213 22:48:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.50
[32m[20221213 22:48:20 @agent_ppo2.py:143][0m Total time:      30.11 min
[32m[20221213 22:48:20 @agent_ppo2.py:145][0m 2940928 total steps have happened
[32m[20221213 22:48:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1436 --------------------------#
[32m[20221213 22:48:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:20 @agent_ppo2.py:185][0m |          -0.0026 |          42.0925 |          15.7991 |
[32m[20221213 22:48:20 @agent_ppo2.py:185][0m |          -0.0034 |          39.0833 |          15.7934 |
[32m[20221213 22:48:20 @agent_ppo2.py:185][0m |          -0.0112 |          37.7634 |          15.7871 |
[32m[20221213 22:48:20 @agent_ppo2.py:185][0m |          -0.0050 |          38.1777 |          15.7885 |
[32m[20221213 22:48:20 @agent_ppo2.py:185][0m |           0.0021 |          40.3239 |          15.7772 |
[32m[20221213 22:48:21 @agent_ppo2.py:185][0m |          -0.0022 |          37.8291 |          15.7655 |
[32m[20221213 22:48:21 @agent_ppo2.py:185][0m |          -0.0126 |          35.1099 |          15.7733 |
[32m[20221213 22:48:21 @agent_ppo2.py:185][0m |          -0.0145 |          34.8121 |          15.7735 |
[32m[20221213 22:48:21 @agent_ppo2.py:185][0m |          -0.0189 |          34.5662 |          15.7732 |
[32m[20221213 22:48:21 @agent_ppo2.py:185][0m |          -0.0169 |          34.3093 |          15.7601 |
[32m[20221213 22:48:21 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 22:48:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.95
[32m[20221213 22:48:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.75
[32m[20221213 22:48:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.01
[32m[20221213 22:48:21 @agent_ppo2.py:143][0m Total time:      30.14 min
[32m[20221213 22:48:21 @agent_ppo2.py:145][0m 2942976 total steps have happened
[32m[20221213 22:48:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1437 --------------------------#
[32m[20221213 22:48:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |           0.0005 |          33.2992 |          15.6446 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0126 |          29.0319 |          15.6229 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0081 |          27.1898 |          15.6215 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0048 |          26.1322 |          15.5959 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0110 |          25.2937 |          15.6012 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0146 |          24.8805 |          15.5969 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0171 |          24.4183 |          15.5817 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0138 |          23.9385 |          15.5784 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0167 |          23.3793 |          15.5775 |
[32m[20221213 22:48:22 @agent_ppo2.py:185][0m |          -0.0084 |          23.4053 |          15.5720 |
[32m[20221213 22:48:22 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:48:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.78
[32m[20221213 22:48:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 371.81
[32m[20221213 22:48:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.80
[32m[20221213 22:48:23 @agent_ppo2.py:143][0m Total time:      30.16 min
[32m[20221213 22:48:23 @agent_ppo2.py:145][0m 2945024 total steps have happened
[32m[20221213 22:48:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1438 --------------------------#
[32m[20221213 22:48:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:23 @agent_ppo2.py:185][0m |          -0.0010 |          32.4941 |          16.0565 |
[32m[20221213 22:48:23 @agent_ppo2.py:185][0m |          -0.0080 |          29.3013 |          16.0548 |
[32m[20221213 22:48:23 @agent_ppo2.py:185][0m |          -0.0104 |          27.6192 |          16.0329 |
[32m[20221213 22:48:23 @agent_ppo2.py:185][0m |          -0.0106 |          26.7035 |          16.0328 |
[32m[20221213 22:48:23 @agent_ppo2.py:185][0m |          -0.0100 |          26.1079 |          16.0151 |
[32m[20221213 22:48:23 @agent_ppo2.py:185][0m |          -0.0160 |          25.6902 |          16.0100 |
[32m[20221213 22:48:24 @agent_ppo2.py:185][0m |          -0.0138 |          25.3818 |          16.0162 |
[32m[20221213 22:48:24 @agent_ppo2.py:185][0m |          -0.0088 |          25.2431 |          16.0111 |
[32m[20221213 22:48:24 @agent_ppo2.py:185][0m |          -0.0158 |          24.9597 |          16.0098 |
[32m[20221213 22:48:24 @agent_ppo2.py:185][0m |          -0.0140 |          24.7545 |          15.9981 |
[32m[20221213 22:48:24 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 22:48:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.61
[32m[20221213 22:48:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.53
[32m[20221213 22:48:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.46
[32m[20221213 22:48:24 @agent_ppo2.py:143][0m Total time:      30.19 min
[32m[20221213 22:48:24 @agent_ppo2.py:145][0m 2947072 total steps have happened
[32m[20221213 22:48:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1439 --------------------------#
[32m[20221213 22:48:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:24 @agent_ppo2.py:185][0m |          -0.0020 |          28.8483 |          16.0147 |
[32m[20221213 22:48:24 @agent_ppo2.py:185][0m |          -0.0047 |          26.7269 |          16.0191 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0019 |          26.9284 |          16.0009 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0126 |          25.6092 |          15.9904 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0121 |          25.2790 |          16.0038 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0115 |          25.0421 |          15.9789 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0165 |          24.8671 |          15.9766 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0152 |          24.7053 |          15.9825 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0061 |          25.5143 |          15.9783 |
[32m[20221213 22:48:25 @agent_ppo2.py:185][0m |          -0.0165 |          24.3637 |          15.9675 |
[32m[20221213 22:48:25 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:48:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.66
[32m[20221213 22:48:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.53
[32m[20221213 22:48:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.87
[32m[20221213 22:48:25 @agent_ppo2.py:143][0m Total time:      30.21 min
[32m[20221213 22:48:25 @agent_ppo2.py:145][0m 2949120 total steps have happened
[32m[20221213 22:48:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1440 --------------------------#
[32m[20221213 22:48:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:48:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |           0.0029 |          36.1129 |          15.8072 |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |          -0.0043 |          32.6002 |          15.7901 |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |          -0.0086 |          31.6098 |          15.7761 |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |          -0.0102 |          30.9487 |          15.7862 |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |          -0.0120 |          30.5846 |          15.7702 |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |          -0.0114 |          30.3300 |          15.7710 |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |          -0.0137 |          29.9484 |          15.7695 |
[32m[20221213 22:48:26 @agent_ppo2.py:185][0m |          -0.0166 |          29.8632 |          15.7642 |
[32m[20221213 22:48:27 @agent_ppo2.py:185][0m |          -0.0131 |          29.4472 |          15.7788 |
[32m[20221213 22:48:27 @agent_ppo2.py:185][0m |          -0.0150 |          29.1866 |          15.7717 |
[32m[20221213 22:48:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.89
[32m[20221213 22:48:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.78
[32m[20221213 22:48:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.19
[32m[20221213 22:48:27 @agent_ppo2.py:143][0m Total time:      30.23 min
[32m[20221213 22:48:27 @agent_ppo2.py:145][0m 2951168 total steps have happened
[32m[20221213 22:48:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1441 --------------------------#
[32m[20221213 22:48:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:27 @agent_ppo2.py:185][0m |           0.0008 |          41.4946 |          15.6823 |
[32m[20221213 22:48:27 @agent_ppo2.py:185][0m |          -0.0035 |          38.9334 |          15.6780 |
[32m[20221213 22:48:27 @agent_ppo2.py:185][0m |          -0.0104 |          37.3640 |          15.6496 |
[32m[20221213 22:48:27 @agent_ppo2.py:185][0m |          -0.0075 |          36.5598 |          15.6430 |
[32m[20221213 22:48:28 @agent_ppo2.py:185][0m |          -0.0027 |          36.6356 |          15.6508 |
[32m[20221213 22:48:28 @agent_ppo2.py:185][0m |          -0.0111 |          35.5577 |          15.6222 |
[32m[20221213 22:48:28 @agent_ppo2.py:185][0m |          -0.0140 |          35.4126 |          15.6285 |
[32m[20221213 22:48:28 @agent_ppo2.py:185][0m |          -0.0137 |          34.9023 |          15.6116 |
[32m[20221213 22:48:28 @agent_ppo2.py:185][0m |          -0.0139 |          34.7288 |          15.6152 |
[32m[20221213 22:48:28 @agent_ppo2.py:185][0m |          -0.0140 |          34.6004 |          15.6176 |
[32m[20221213 22:48:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.43
[32m[20221213 22:48:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.15
[32m[20221213 22:48:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.39
[32m[20221213 22:48:28 @agent_ppo2.py:143][0m Total time:      30.25 min
[32m[20221213 22:48:28 @agent_ppo2.py:145][0m 2953216 total steps have happened
[32m[20221213 22:48:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1442 --------------------------#
[32m[20221213 22:48:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:28 @agent_ppo2.py:185][0m |           0.0008 |          48.3133 |          15.7012 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0056 |          43.4142 |          15.6705 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0057 |          41.5244 |          15.6830 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0056 |          40.4459 |          15.7031 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0100 |          39.6856 |          15.6929 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0091 |          39.2197 |          15.6933 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0077 |          38.6029 |          15.7028 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0117 |          38.1920 |          15.6957 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0030 |          39.8862 |          15.7069 |
[32m[20221213 22:48:29 @agent_ppo2.py:185][0m |          -0.0106 |          37.7958 |          15.6854 |
[32m[20221213 22:48:29 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 22:48:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.18
[32m[20221213 22:48:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.79
[32m[20221213 22:48:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.86
[32m[20221213 22:48:30 @agent_ppo2.py:143][0m Total time:      30.28 min
[32m[20221213 22:48:30 @agent_ppo2.py:145][0m 2955264 total steps have happened
[32m[20221213 22:48:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1443 --------------------------#
[32m[20221213 22:48:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:30 @agent_ppo2.py:185][0m |           0.0040 |          38.6695 |          15.7612 |
[32m[20221213 22:48:30 @agent_ppo2.py:185][0m |          -0.0034 |          34.4018 |          15.7499 |
[32m[20221213 22:48:30 @agent_ppo2.py:185][0m |          -0.0061 |          33.3604 |          15.7302 |
[32m[20221213 22:48:30 @agent_ppo2.py:185][0m |           0.0023 |          34.1909 |          15.7517 |
[32m[20221213 22:48:30 @agent_ppo2.py:185][0m |          -0.0056 |          32.2509 |          15.7407 |
[32m[20221213 22:48:30 @agent_ppo2.py:185][0m |           0.0031 |          33.4930 |          15.7487 |
[32m[20221213 22:48:30 @agent_ppo2.py:185][0m |          -0.0068 |          31.9206 |          15.7481 |
[32m[20221213 22:48:31 @agent_ppo2.py:185][0m |          -0.0123 |          31.4726 |          15.7484 |
[32m[20221213 22:48:31 @agent_ppo2.py:185][0m |          -0.0141 |          31.0785 |          15.7532 |
[32m[20221213 22:48:31 @agent_ppo2.py:185][0m |          -0.0159 |          31.1081 |          15.7400 |
[32m[20221213 22:48:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.24
[32m[20221213 22:48:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 352.97
[32m[20221213 22:48:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.94
[32m[20221213 22:48:31 @agent_ppo2.py:143][0m Total time:      30.30 min
[32m[20221213 22:48:31 @agent_ppo2.py:145][0m 2957312 total steps have happened
[32m[20221213 22:48:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1444 --------------------------#
[32m[20221213 22:48:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:31 @agent_ppo2.py:185][0m |           0.0019 |          39.4115 |          15.8090 |
[32m[20221213 22:48:31 @agent_ppo2.py:185][0m |          -0.0021 |          37.7505 |          15.7846 |
[32m[20221213 22:48:31 @agent_ppo2.py:185][0m |          -0.0052 |          36.3573 |          15.7876 |
[32m[20221213 22:48:32 @agent_ppo2.py:185][0m |          -0.0095 |          35.8209 |          15.7967 |
[32m[20221213 22:48:32 @agent_ppo2.py:185][0m |          -0.0099 |          35.7167 |          15.7881 |
[32m[20221213 22:48:32 @agent_ppo2.py:185][0m |          -0.0135 |          35.1660 |          15.7990 |
[32m[20221213 22:48:32 @agent_ppo2.py:185][0m |          -0.0095 |          34.9063 |          15.7822 |
[32m[20221213 22:48:32 @agent_ppo2.py:185][0m |          -0.0129 |          35.1701 |          15.8143 |
[32m[20221213 22:48:32 @agent_ppo2.py:185][0m |          -0.0149 |          34.6194 |          15.8247 |
[32m[20221213 22:48:32 @agent_ppo2.py:185][0m |          -0.0143 |          34.2719 |          15.8203 |
[32m[20221213 22:48:32 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.46
[32m[20221213 22:48:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.92
[32m[20221213 22:48:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.46
[32m[20221213 22:48:32 @agent_ppo2.py:143][0m Total time:      30.32 min
[32m[20221213 22:48:32 @agent_ppo2.py:145][0m 2959360 total steps have happened
[32m[20221213 22:48:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1445 --------------------------#
[32m[20221213 22:48:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0048 |          34.9938 |          15.9047 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0037 |          32.5650 |          15.9053 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0087 |          31.9462 |          15.8956 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0102 |          31.6159 |          15.8866 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0028 |          33.2117 |          15.8802 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0115 |          31.3147 |          15.8869 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0114 |          30.9021 |          15.8716 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0099 |          30.9091 |          15.8859 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0146 |          30.6848 |          15.8697 |
[32m[20221213 22:48:33 @agent_ppo2.py:185][0m |          -0.0080 |          30.8895 |          15.8787 |
[32m[20221213 22:48:33 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.59
[32m[20221213 22:48:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.88
[32m[20221213 22:48:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.94
[32m[20221213 22:48:34 @agent_ppo2.py:143][0m Total time:      30.35 min
[32m[20221213 22:48:34 @agent_ppo2.py:145][0m 2961408 total steps have happened
[32m[20221213 22:48:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1446 --------------------------#
[32m[20221213 22:48:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:34 @agent_ppo2.py:185][0m |          -0.0047 |          43.2150 |          15.6630 |
[32m[20221213 22:48:34 @agent_ppo2.py:185][0m |          -0.0037 |          41.9749 |          15.6565 |
[32m[20221213 22:48:34 @agent_ppo2.py:185][0m |          -0.0073 |          41.4837 |          15.6859 |
[32m[20221213 22:48:34 @agent_ppo2.py:185][0m |          -0.0013 |          42.1716 |          15.6868 |
[32m[20221213 22:48:34 @agent_ppo2.py:185][0m |          -0.0058 |          41.2547 |          15.6658 |
[32m[20221213 22:48:34 @agent_ppo2.py:185][0m |          -0.0115 |          40.6991 |          15.6794 |
[32m[20221213 22:48:35 @agent_ppo2.py:185][0m |          -0.0095 |          40.5088 |          15.6754 |
[32m[20221213 22:48:35 @agent_ppo2.py:185][0m |          -0.0096 |          40.2990 |          15.6724 |
[32m[20221213 22:48:35 @agent_ppo2.py:185][0m |          -0.0112 |          40.2440 |          15.6880 |
[32m[20221213 22:48:35 @agent_ppo2.py:185][0m |          -0.0112 |          40.0822 |          15.6960 |
[32m[20221213 22:48:35 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:48:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.02
[32m[20221213 22:48:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.75
[32m[20221213 22:48:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.20
[32m[20221213 22:48:35 @agent_ppo2.py:143][0m Total time:      30.37 min
[32m[20221213 22:48:35 @agent_ppo2.py:145][0m 2963456 total steps have happened
[32m[20221213 22:48:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1447 --------------------------#
[32m[20221213 22:48:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:35 @agent_ppo2.py:185][0m |           0.0050 |          48.7330 |          15.7995 |
[32m[20221213 22:48:35 @agent_ppo2.py:185][0m |          -0.0035 |          43.9272 |          15.7758 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0018 |          42.3095 |          15.7813 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0052 |          41.3725 |          15.7795 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0085 |          40.7021 |          15.7640 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0072 |          41.5989 |          15.7769 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0095 |          40.1756 |          15.7792 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0110 |          39.8293 |          15.7670 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0152 |          39.1112 |          15.7643 |
[32m[20221213 22:48:36 @agent_ppo2.py:185][0m |          -0.0081 |          39.6309 |          15.7626 |
[32m[20221213 22:48:36 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.08
[32m[20221213 22:48:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.97
[32m[20221213 22:48:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.47
[32m[20221213 22:48:36 @agent_ppo2.py:143][0m Total time:      30.39 min
[32m[20221213 22:48:36 @agent_ppo2.py:145][0m 2965504 total steps have happened
[32m[20221213 22:48:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1448 --------------------------#
[32m[20221213 22:48:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |           0.0014 |          36.3708 |          15.8179 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0061 |          32.7619 |          15.8061 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0049 |          31.7896 |          15.7877 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0040 |          31.0575 |          15.7735 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0071 |          30.5643 |          15.7808 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0090 |          30.3596 |          15.7454 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0092 |          29.9803 |          15.7612 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0105 |          29.7145 |          15.7420 |
[32m[20221213 22:48:37 @agent_ppo2.py:185][0m |          -0.0099 |          29.6053 |          15.7360 |
[32m[20221213 22:48:38 @agent_ppo2.py:185][0m |          -0.0123 |          29.2792 |          15.7512 |
[32m[20221213 22:48:38 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.14
[32m[20221213 22:48:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.01
[32m[20221213 22:48:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 184.78
[32m[20221213 22:48:38 @agent_ppo2.py:143][0m Total time:      30.41 min
[32m[20221213 22:48:38 @agent_ppo2.py:145][0m 2967552 total steps have happened
[32m[20221213 22:48:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1449 --------------------------#
[32m[20221213 22:48:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:38 @agent_ppo2.py:185][0m |           0.0020 |          40.4670 |          15.7125 |
[32m[20221213 22:48:38 @agent_ppo2.py:185][0m |          -0.0007 |          37.7866 |          15.7347 |
[32m[20221213 22:48:38 @agent_ppo2.py:185][0m |          -0.0070 |          36.5521 |          15.7386 |
[32m[20221213 22:48:38 @agent_ppo2.py:185][0m |          -0.0121 |          35.7773 |          15.7338 |
[32m[20221213 22:48:38 @agent_ppo2.py:185][0m |          -0.0091 |          35.5155 |          15.7269 |
[32m[20221213 22:48:39 @agent_ppo2.py:185][0m |          -0.0123 |          34.8712 |          15.7617 |
[32m[20221213 22:48:39 @agent_ppo2.py:185][0m |          -0.0186 |          34.6825 |          15.7292 |
[32m[20221213 22:48:39 @agent_ppo2.py:185][0m |          -0.0129 |          34.1303 |          15.7430 |
[32m[20221213 22:48:39 @agent_ppo2.py:185][0m |          -0.0150 |          34.1296 |          15.7517 |
[32m[20221213 22:48:39 @agent_ppo2.py:185][0m |          -0.0167 |          33.8631 |          15.7487 |
[32m[20221213 22:48:39 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.43
[32m[20221213 22:48:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.76
[32m[20221213 22:48:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.67
[32m[20221213 22:48:39 @agent_ppo2.py:143][0m Total time:      30.44 min
[32m[20221213 22:48:39 @agent_ppo2.py:145][0m 2969600 total steps have happened
[32m[20221213 22:48:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1450 --------------------------#
[32m[20221213 22:48:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:48:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:39 @agent_ppo2.py:185][0m |          -0.0027 |          41.4536 |          15.6494 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |           0.0004 |          40.0186 |          15.6471 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0043 |          39.0823 |          15.6545 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0082 |          38.6238 |          15.6561 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0105 |          38.2571 |          15.6460 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0087 |          37.9024 |          15.6464 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0143 |          37.6894 |          15.6569 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0105 |          37.4171 |          15.6557 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0127 |          37.3717 |          15.6447 |
[32m[20221213 22:48:40 @agent_ppo2.py:185][0m |          -0.0079 |          37.5288 |          15.6406 |
[32m[20221213 22:48:40 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:48:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.73
[32m[20221213 22:48:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 371.93
[32m[20221213 22:48:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.35
[32m[20221213 22:48:40 @agent_ppo2.py:143][0m Total time:      30.46 min
[32m[20221213 22:48:40 @agent_ppo2.py:145][0m 2971648 total steps have happened
[32m[20221213 22:48:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1451 --------------------------#
[32m[20221213 22:48:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:41 @agent_ppo2.py:185][0m |           0.0008 |          32.9054 |          15.8201 |
[32m[20221213 22:48:41 @agent_ppo2.py:185][0m |          -0.0050 |          30.6952 |          15.7995 |
[32m[20221213 22:48:41 @agent_ppo2.py:185][0m |          -0.0048 |          29.8983 |          15.8051 |
[32m[20221213 22:48:41 @agent_ppo2.py:185][0m |          -0.0070 |          29.3777 |          15.7853 |
[32m[20221213 22:48:41 @agent_ppo2.py:185][0m |          -0.0055 |          28.9838 |          15.7857 |
[32m[20221213 22:48:41 @agent_ppo2.py:185][0m |          -0.0146 |          28.7388 |          15.7768 |
[32m[20221213 22:48:41 @agent_ppo2.py:185][0m |          -0.0138 |          28.4946 |          15.7828 |
[32m[20221213 22:48:42 @agent_ppo2.py:185][0m |          -0.0115 |          28.2606 |          15.7745 |
[32m[20221213 22:48:42 @agent_ppo2.py:185][0m |          -0.0026 |          29.8840 |          15.7599 |
[32m[20221213 22:48:42 @agent_ppo2.py:185][0m |          -0.0070 |          28.3585 |          15.7590 |
[32m[20221213 22:48:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.99
[32m[20221213 22:48:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.62
[32m[20221213 22:48:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.28
[32m[20221213 22:48:42 @agent_ppo2.py:143][0m Total time:      30.48 min
[32m[20221213 22:48:42 @agent_ppo2.py:145][0m 2973696 total steps have happened
[32m[20221213 22:48:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1452 --------------------------#
[32m[20221213 22:48:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:42 @agent_ppo2.py:185][0m |           0.0035 |          39.5885 |          15.8303 |
[32m[20221213 22:48:42 @agent_ppo2.py:185][0m |           0.0111 |          38.3747 |          15.8090 |
[32m[20221213 22:48:42 @agent_ppo2.py:185][0m |          -0.0066 |          34.8310 |          15.8284 |
[32m[20221213 22:48:42 @agent_ppo2.py:185][0m |          -0.0067 |          34.2315 |          15.8449 |
[32m[20221213 22:48:43 @agent_ppo2.py:185][0m |          -0.0056 |          33.6617 |          15.8285 |
[32m[20221213 22:48:43 @agent_ppo2.py:185][0m |          -0.0081 |          33.2479 |          15.8237 |
[32m[20221213 22:48:43 @agent_ppo2.py:185][0m |          -0.0044 |          33.4618 |          15.8482 |
[32m[20221213 22:48:43 @agent_ppo2.py:185][0m |          -0.0066 |          32.6212 |          15.8421 |
[32m[20221213 22:48:43 @agent_ppo2.py:185][0m |          -0.0104 |          32.3194 |          15.8463 |
[32m[20221213 22:48:43 @agent_ppo2.py:185][0m |          -0.0114 |          32.0267 |          15.8652 |
[32m[20221213 22:48:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.57
[32m[20221213 22:48:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.90
[32m[20221213 22:48:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 25.65
[32m[20221213 22:48:43 @agent_ppo2.py:143][0m Total time:      30.51 min
[32m[20221213 22:48:43 @agent_ppo2.py:145][0m 2975744 total steps have happened
[32m[20221213 22:48:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1453 --------------------------#
[32m[20221213 22:48:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0018 |          45.1833 |          15.5853 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0018 |          43.2373 |          15.6026 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0024 |          43.3870 |          15.6012 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0076 |          41.8503 |          15.6064 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0091 |          41.4791 |          15.6000 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0087 |          41.0555 |          15.6175 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0153 |          40.9735 |          15.6069 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0068 |          40.7160 |          15.6059 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0181 |          40.4093 |          15.5967 |
[32m[20221213 22:48:44 @agent_ppo2.py:185][0m |          -0.0142 |          40.1398 |          15.6134 |
[32m[20221213 22:48:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 308.08
[32m[20221213 22:48:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 342.67
[32m[20221213 22:48:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.85
[32m[20221213 22:48:45 @agent_ppo2.py:143][0m Total time:      30.53 min
[32m[20221213 22:48:45 @agent_ppo2.py:145][0m 2977792 total steps have happened
[32m[20221213 22:48:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1454 --------------------------#
[32m[20221213 22:48:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:45 @agent_ppo2.py:185][0m |           0.0007 |          45.5343 |          15.7984 |
[32m[20221213 22:48:45 @agent_ppo2.py:185][0m |           0.0003 |          43.9494 |          15.8034 |
[32m[20221213 22:48:45 @agent_ppo2.py:185][0m |          -0.0043 |          42.9730 |          15.7927 |
[32m[20221213 22:48:45 @agent_ppo2.py:185][0m |          -0.0076 |          42.6044 |          15.7996 |
[32m[20221213 22:48:45 @agent_ppo2.py:185][0m |          -0.0061 |          42.4757 |          15.7927 |
[32m[20221213 22:48:45 @agent_ppo2.py:185][0m |          -0.0048 |          42.0267 |          15.8059 |
[32m[20221213 22:48:46 @agent_ppo2.py:185][0m |          -0.0066 |          41.6757 |          15.7959 |
[32m[20221213 22:48:46 @agent_ppo2.py:185][0m |          -0.0053 |          41.6053 |          15.8095 |
[32m[20221213 22:48:46 @agent_ppo2.py:185][0m |          -0.0095 |          41.0870 |          15.7967 |
[32m[20221213 22:48:46 @agent_ppo2.py:185][0m |          -0.0068 |          41.6094 |          15.8034 |
[32m[20221213 22:48:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.27
[32m[20221213 22:48:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.67
[32m[20221213 22:48:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.70
[32m[20221213 22:48:46 @agent_ppo2.py:143][0m Total time:      30.55 min
[32m[20221213 22:48:46 @agent_ppo2.py:145][0m 2979840 total steps have happened
[32m[20221213 22:48:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1455 --------------------------#
[32m[20221213 22:48:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:46 @agent_ppo2.py:185][0m |          -0.0037 |          34.1036 |          15.6097 |
[32m[20221213 22:48:46 @agent_ppo2.py:185][0m |          -0.0092 |          30.4507 |          15.6054 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0069 |          29.7392 |          15.5995 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0023 |          29.4221 |          15.6047 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0103 |          29.0982 |          15.5940 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0085 |          28.6539 |          15.5942 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0074 |          28.4583 |          15.5935 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0151 |          28.2852 |          15.6034 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0102 |          28.1225 |          15.6048 |
[32m[20221213 22:48:47 @agent_ppo2.py:185][0m |          -0.0108 |          28.0477 |          15.6018 |
[32m[20221213 22:48:47 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:48:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.82
[32m[20221213 22:48:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.81
[32m[20221213 22:48:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.78
[32m[20221213 22:48:47 @agent_ppo2.py:143][0m Total time:      30.57 min
[32m[20221213 22:48:47 @agent_ppo2.py:145][0m 2981888 total steps have happened
[32m[20221213 22:48:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1456 --------------------------#
[32m[20221213 22:48:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0026 |          36.6464 |          15.9066 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0016 |          34.5189 |          15.9194 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |           0.0018 |          37.5741 |          15.9271 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0128 |          32.4717 |          15.9081 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0139 |          31.8719 |          15.9256 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0076 |          32.0804 |          15.9249 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0144 |          31.3341 |          15.9297 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0111 |          31.0589 |          15.9085 |
[32m[20221213 22:48:48 @agent_ppo2.py:185][0m |          -0.0138 |          31.0149 |          15.9291 |
[32m[20221213 22:48:49 @agent_ppo2.py:185][0m |          -0.0157 |          30.7869 |          15.9355 |
[32m[20221213 22:48:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.96
[32m[20221213 22:48:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.51
[32m[20221213 22:48:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 48.48
[32m[20221213 22:48:49 @agent_ppo2.py:143][0m Total time:      30.60 min
[32m[20221213 22:48:49 @agent_ppo2.py:145][0m 2983936 total steps have happened
[32m[20221213 22:48:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1457 --------------------------#
[32m[20221213 22:48:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:49 @agent_ppo2.py:185][0m |          -0.0029 |          41.7403 |          15.7520 |
[32m[20221213 22:48:49 @agent_ppo2.py:185][0m |          -0.0092 |          38.4498 |          15.7164 |
[32m[20221213 22:48:49 @agent_ppo2.py:185][0m |          -0.0083 |          37.3128 |          15.7200 |
[32m[20221213 22:48:49 @agent_ppo2.py:185][0m |          -0.0111 |          36.5881 |          15.7301 |
[32m[20221213 22:48:49 @agent_ppo2.py:185][0m |          -0.0089 |          36.0144 |          15.7351 |
[32m[20221213 22:48:50 @agent_ppo2.py:185][0m |          -0.0119 |          35.4190 |          15.7182 |
[32m[20221213 22:48:50 @agent_ppo2.py:185][0m |          -0.0125 |          35.1489 |          15.7315 |
[32m[20221213 22:48:50 @agent_ppo2.py:185][0m |          -0.0076 |          35.5697 |          15.7273 |
[32m[20221213 22:48:50 @agent_ppo2.py:185][0m |          -0.0105 |          34.6688 |          15.7216 |
[32m[20221213 22:48:50 @agent_ppo2.py:185][0m |          -0.0132 |          34.3440 |          15.7445 |
[32m[20221213 22:48:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.78
[32m[20221213 22:48:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.28
[32m[20221213 22:48:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.79
[32m[20221213 22:48:50 @agent_ppo2.py:143][0m Total time:      30.62 min
[32m[20221213 22:48:50 @agent_ppo2.py:145][0m 2985984 total steps have happened
[32m[20221213 22:48:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1458 --------------------------#
[32m[20221213 22:48:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:50 @agent_ppo2.py:185][0m |          -0.0027 |          29.8082 |          15.9238 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0049 |          23.9093 |          15.9008 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0080 |          22.2293 |          15.8883 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0084 |          21.1074 |          15.8917 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0117 |          20.4135 |          15.8902 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0120 |          19.9038 |          15.8656 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0143 |          19.4938 |          15.8593 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0098 |          19.3821 |          15.8736 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0159 |          18.7844 |          15.8591 |
[32m[20221213 22:48:51 @agent_ppo2.py:185][0m |          -0.0129 |          18.7009 |          15.8578 |
[32m[20221213 22:48:51 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.22
[32m[20221213 22:48:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.43
[32m[20221213 22:48:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.91
[32m[20221213 22:48:51 @agent_ppo2.py:143][0m Total time:      30.64 min
[32m[20221213 22:48:51 @agent_ppo2.py:145][0m 2988032 total steps have happened
[32m[20221213 22:48:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1459 --------------------------#
[32m[20221213 22:48:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0035 |          39.2744 |          15.7390 |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0074 |          36.5293 |          15.7366 |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0087 |          35.7481 |          15.7281 |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0023 |          35.8082 |          15.7359 |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0069 |          34.9302 |          15.7180 |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0128 |          34.7259 |          15.7157 |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0117 |          34.4328 |          15.7246 |
[32m[20221213 22:48:52 @agent_ppo2.py:185][0m |          -0.0057 |          35.8130 |          15.7089 |
[32m[20221213 22:48:53 @agent_ppo2.py:185][0m |          -0.0131 |          34.2890 |          15.7186 |
[32m[20221213 22:48:53 @agent_ppo2.py:185][0m |          -0.0099 |          34.6642 |          15.7077 |
[32m[20221213 22:48:53 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:48:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.33
[32m[20221213 22:48:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 331.61
[32m[20221213 22:48:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.29
[32m[20221213 22:48:53 @agent_ppo2.py:143][0m Total time:      30.66 min
[32m[20221213 22:48:53 @agent_ppo2.py:145][0m 2990080 total steps have happened
[32m[20221213 22:48:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1460 --------------------------#
[32m[20221213 22:48:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:48:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:53 @agent_ppo2.py:185][0m |          -0.0013 |          46.3696 |          15.6448 |
[32m[20221213 22:48:53 @agent_ppo2.py:185][0m |          -0.0065 |          42.9417 |          15.6510 |
[32m[20221213 22:48:53 @agent_ppo2.py:185][0m |          -0.0081 |          41.1851 |          15.6485 |
[32m[20221213 22:48:53 @agent_ppo2.py:185][0m |          -0.0088 |          39.7757 |          15.6340 |
[32m[20221213 22:48:54 @agent_ppo2.py:185][0m |          -0.0072 |          38.7923 |          15.6431 |
[32m[20221213 22:48:54 @agent_ppo2.py:185][0m |          -0.0046 |          38.9159 |          15.6286 |
[32m[20221213 22:48:54 @agent_ppo2.py:185][0m |          -0.0154 |          37.5370 |          15.6192 |
[32m[20221213 22:48:54 @agent_ppo2.py:185][0m |          -0.0119 |          36.7884 |          15.6146 |
[32m[20221213 22:48:54 @agent_ppo2.py:185][0m |          -0.0129 |          36.3902 |          15.6258 |
[32m[20221213 22:48:54 @agent_ppo2.py:185][0m |          -0.0157 |          36.0616 |          15.6251 |
[32m[20221213 22:48:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.29
[32m[20221213 22:48:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.47
[32m[20221213 22:48:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.60
[32m[20221213 22:48:54 @agent_ppo2.py:143][0m Total time:      30.69 min
[32m[20221213 22:48:54 @agent_ppo2.py:145][0m 2992128 total steps have happened
[32m[20221213 22:48:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1461 --------------------------#
[32m[20221213 22:48:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:48:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |           0.0160 |          50.0850 |          15.8121 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |           0.0029 |          44.9912 |          15.7803 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0015 |          43.7264 |          15.7915 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0105 |          42.1570 |          15.7801 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0104 |          41.1750 |          15.7774 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0100 |          40.7858 |          15.7630 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0136 |          40.3750 |          15.7709 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0103 |          40.0601 |          15.7673 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0039 |          42.7307 |          15.7629 |
[32m[20221213 22:48:55 @agent_ppo2.py:185][0m |          -0.0121 |          39.3904 |          15.7631 |
[32m[20221213 22:48:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.55
[32m[20221213 22:48:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.36
[32m[20221213 22:48:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.05
[32m[20221213 22:48:56 @agent_ppo2.py:143][0m Total time:      30.71 min
[32m[20221213 22:48:56 @agent_ppo2.py:145][0m 2994176 total steps have happened
[32m[20221213 22:48:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1462 --------------------------#
[32m[20221213 22:48:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:56 @agent_ppo2.py:185][0m |          -0.0027 |          40.5668 |          15.8633 |
[32m[20221213 22:48:56 @agent_ppo2.py:185][0m |          -0.0053 |          38.8637 |          15.8313 |
[32m[20221213 22:48:56 @agent_ppo2.py:185][0m |          -0.0107 |          38.1510 |          15.8178 |
[32m[20221213 22:48:56 @agent_ppo2.py:185][0m |          -0.0097 |          37.9601 |          15.7958 |
[32m[20221213 22:48:56 @agent_ppo2.py:185][0m |          -0.0131 |          37.3118 |          15.7971 |
[32m[20221213 22:48:56 @agent_ppo2.py:185][0m |          -0.0132 |          37.0685 |          15.7959 |
[32m[20221213 22:48:56 @agent_ppo2.py:185][0m |          -0.0016 |          41.5108 |          15.7790 |
[32m[20221213 22:48:57 @agent_ppo2.py:185][0m |          -0.0067 |          36.6726 |          15.7577 |
[32m[20221213 22:48:57 @agent_ppo2.py:185][0m |          -0.0136 |          36.4862 |          15.7655 |
[32m[20221213 22:48:57 @agent_ppo2.py:185][0m |          -0.0198 |          36.3240 |          15.7731 |
[32m[20221213 22:48:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:48:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.61
[32m[20221213 22:48:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 342.00
[32m[20221213 22:48:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.31
[32m[20221213 22:48:57 @agent_ppo2.py:143][0m Total time:      30.73 min
[32m[20221213 22:48:57 @agent_ppo2.py:145][0m 2996224 total steps have happened
[32m[20221213 22:48:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1463 --------------------------#
[32m[20221213 22:48:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:57 @agent_ppo2.py:185][0m |          -0.0015 |          44.1174 |          15.7148 |
[32m[20221213 22:48:57 @agent_ppo2.py:185][0m |          -0.0080 |          42.2991 |          15.6969 |
[32m[20221213 22:48:57 @agent_ppo2.py:185][0m |          -0.0074 |          41.6369 |          15.6996 |
[32m[20221213 22:48:58 @agent_ppo2.py:185][0m |          -0.0078 |          41.1709 |          15.6905 |
[32m[20221213 22:48:58 @agent_ppo2.py:185][0m |          -0.0075 |          41.4595 |          15.6809 |
[32m[20221213 22:48:58 @agent_ppo2.py:185][0m |          -0.0114 |          40.4832 |          15.6866 |
[32m[20221213 22:48:58 @agent_ppo2.py:185][0m |          -0.0094 |          40.2246 |          15.6779 |
[32m[20221213 22:48:58 @agent_ppo2.py:185][0m |          -0.0127 |          40.0325 |          15.6763 |
[32m[20221213 22:48:58 @agent_ppo2.py:185][0m |          -0.0119 |          39.8130 |          15.6686 |
[32m[20221213 22:48:58 @agent_ppo2.py:185][0m |          -0.0092 |          39.8123 |          15.6723 |
[32m[20221213 22:48:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:48:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.70
[32m[20221213 22:48:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.03
[32m[20221213 22:48:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.19
[32m[20221213 22:48:58 @agent_ppo2.py:143][0m Total time:      30.76 min
[32m[20221213 22:48:58 @agent_ppo2.py:145][0m 2998272 total steps have happened
[32m[20221213 22:48:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1464 --------------------------#
[32m[20221213 22:48:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:48:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |           0.0042 |          27.8323 |          15.7070 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0002 |          21.8363 |          15.6797 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0091 |          20.2676 |          15.6462 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0144 |          19.3147 |          15.6571 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0073 |          18.5264 |          15.6424 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0148 |          18.0969 |          15.6352 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0173 |          17.6693 |          15.6291 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0234 |          17.2977 |          15.6202 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0181 |          17.0353 |          15.6009 |
[32m[20221213 22:48:59 @agent_ppo2.py:185][0m |          -0.0167 |          16.6539 |          15.6001 |
[32m[20221213 22:48:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:49:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 309.18
[32m[20221213 22:49:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.72
[32m[20221213 22:49:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.13
[32m[20221213 22:49:00 @agent_ppo2.py:143][0m Total time:      30.78 min
[32m[20221213 22:49:00 @agent_ppo2.py:145][0m 3000320 total steps have happened
[32m[20221213 22:49:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1465 --------------------------#
[32m[20221213 22:49:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:49:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:00 @agent_ppo2.py:185][0m |          -0.0021 |          44.3915 |          15.7064 |
[32m[20221213 22:49:00 @agent_ppo2.py:185][0m |          -0.0007 |          42.6338 |          15.6803 |
[32m[20221213 22:49:00 @agent_ppo2.py:185][0m |          -0.0113 |          41.3508 |          15.6820 |
[32m[20221213 22:49:00 @agent_ppo2.py:185][0m |          -0.0050 |          40.4543 |          15.6799 |
[32m[20221213 22:49:00 @agent_ppo2.py:185][0m |          -0.0102 |          39.9474 |          15.6547 |
[32m[20221213 22:49:00 @agent_ppo2.py:185][0m |          -0.0118 |          39.4113 |          15.6660 |
[32m[20221213 22:49:01 @agent_ppo2.py:185][0m |          -0.0116 |          38.9542 |          15.6539 |
[32m[20221213 22:49:01 @agent_ppo2.py:185][0m |          -0.0112 |          38.7127 |          15.6473 |
[32m[20221213 22:49:01 @agent_ppo2.py:185][0m |           0.0006 |          44.3919 |          15.6457 |
[32m[20221213 22:49:01 @agent_ppo2.py:185][0m |          -0.0100 |          38.4079 |          15.6495 |
[32m[20221213 22:49:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:49:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.11
[32m[20221213 22:49:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.48
[32m[20221213 22:49:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.99
[32m[20221213 22:49:01 @agent_ppo2.py:143][0m Total time:      30.80 min
[32m[20221213 22:49:01 @agent_ppo2.py:145][0m 3002368 total steps have happened
[32m[20221213 22:49:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1466 --------------------------#
[32m[20221213 22:49:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:49:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:01 @agent_ppo2.py:185][0m |           0.0014 |          48.5969 |          15.5634 |
[32m[20221213 22:49:01 @agent_ppo2.py:185][0m |           0.0032 |          44.0825 |          15.5692 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0068 |          42.4509 |          15.5687 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0127 |          41.7618 |          15.5713 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0086 |          41.4465 |          15.5803 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0105 |          41.1326 |          15.5759 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0107 |          40.9403 |          15.5649 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0139 |          40.7374 |          15.5787 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0101 |          40.4622 |          15.5714 |
[32m[20221213 22:49:02 @agent_ppo2.py:185][0m |          -0.0094 |          40.2571 |          15.5767 |
[32m[20221213 22:49:02 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:49:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.36
[32m[20221213 22:49:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.90
[32m[20221213 22:49:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.46
[32m[20221213 22:49:02 @agent_ppo2.py:143][0m Total time:      30.82 min
[32m[20221213 22:49:02 @agent_ppo2.py:145][0m 3004416 total steps have happened
[32m[20221213 22:49:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1467 --------------------------#
[32m[20221213 22:49:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:49:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |           0.0070 |          42.9024 |          15.7501 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |           0.0052 |          43.4978 |          15.7345 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |          -0.0075 |          39.7328 |          15.7310 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |           0.0053 |          41.1322 |          15.7323 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |          -0.0090 |          39.0424 |          15.7337 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |          -0.0096 |          38.6525 |          15.7500 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |          -0.0074 |          38.5015 |          15.7601 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |          -0.0069 |          38.3475 |          15.7631 |
[32m[20221213 22:49:03 @agent_ppo2.py:185][0m |          -0.0114 |          38.0956 |          15.7665 |
[32m[20221213 22:49:04 @agent_ppo2.py:185][0m |          -0.0128 |          37.9707 |          15.7483 |
[32m[20221213 22:49:04 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:49:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.64
[32m[20221213 22:49:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.70
[32m[20221213 22:49:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.09
[32m[20221213 22:49:04 @agent_ppo2.py:143][0m Total time:      30.85 min
[32m[20221213 22:49:04 @agent_ppo2.py:145][0m 3006464 total steps have happened
[32m[20221213 22:49:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1468 --------------------------#
[32m[20221213 22:49:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:49:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:04 @agent_ppo2.py:185][0m |          -0.0022 |          47.1285 |          15.7327 |
[32m[20221213 22:49:04 @agent_ppo2.py:185][0m |          -0.0087 |          44.2027 |          15.7062 |
[32m[20221213 22:49:04 @agent_ppo2.py:185][0m |          -0.0019 |          43.4101 |          15.7083 |
[32m[20221213 22:49:04 @agent_ppo2.py:185][0m |          -0.0110 |          42.9965 |          15.7051 |
[32m[20221213 22:49:04 @agent_ppo2.py:185][0m |           0.0022 |          45.9344 |          15.7099 |
[32m[20221213 22:49:05 @agent_ppo2.py:185][0m |          -0.0116 |          42.5400 |          15.7132 |
[32m[20221213 22:49:05 @agent_ppo2.py:185][0m |          -0.0095 |          42.2374 |          15.7172 |
[32m[20221213 22:49:05 @agent_ppo2.py:185][0m |          -0.0111 |          41.9481 |          15.6965 |
[32m[20221213 22:49:05 @agent_ppo2.py:185][0m |          -0.0155 |          41.8823 |          15.7284 |
[32m[20221213 22:49:05 @agent_ppo2.py:185][0m |          -0.0130 |          41.8834 |          15.7133 |
[32m[20221213 22:49:05 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 22:49:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.80
[32m[20221213 22:49:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.66
[32m[20221213 22:49:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.68
[32m[20221213 22:49:05 @agent_ppo2.py:143][0m Total time:      30.87 min
[32m[20221213 22:49:05 @agent_ppo2.py:145][0m 3008512 total steps have happened
[32m[20221213 22:49:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1469 --------------------------#
[32m[20221213 22:49:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:49:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:05 @agent_ppo2.py:185][0m |          -0.0011 |          39.4904 |          15.7125 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0048 |          37.4845 |          15.7034 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0078 |          36.9902 |          15.6941 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0077 |          36.8815 |          15.6954 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |           0.0090 |          42.0341 |          15.6911 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0088 |          36.4827 |          15.6509 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0118 |          36.2647 |          15.6793 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0097 |          36.1564 |          15.6964 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0105 |          35.9085 |          15.6791 |
[32m[20221213 22:49:06 @agent_ppo2.py:185][0m |          -0.0117 |          35.8894 |          15.6741 |
[32m[20221213 22:49:06 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:49:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.89
[32m[20221213 22:49:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.17
[32m[20221213 22:49:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 330.01
[32m[20221213 22:49:06 @agent_ppo2.py:143][0m Total time:      30.89 min
[32m[20221213 22:49:06 @agent_ppo2.py:145][0m 3010560 total steps have happened
[32m[20221213 22:49:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1470 --------------------------#
[32m[20221213 22:49:07 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:49:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |           0.0044 |          40.4531 |          15.8271 |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |          -0.0006 |          37.4315 |          15.7911 |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |          -0.0047 |          36.6956 |          15.7537 |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |          -0.0087 |          36.1599 |          15.7509 |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |          -0.0077 |          35.7326 |          15.7581 |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |          -0.0066 |          35.4640 |          15.7559 |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |          -0.0100 |          35.2997 |          15.7493 |
[32m[20221213 22:49:07 @agent_ppo2.py:185][0m |           0.0028 |          38.5010 |          15.7391 |
[32m[20221213 22:49:08 @agent_ppo2.py:185][0m |          -0.0077 |          34.8071 |          15.7384 |
[32m[20221213 22:49:08 @agent_ppo2.py:185][0m |          -0.0070 |          34.5841 |          15.7321 |
[32m[20221213 22:49:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:49:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 316.73
[32m[20221213 22:49:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.62
[32m[20221213 22:49:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.30
[32m[20221213 22:49:08 @agent_ppo2.py:143][0m Total time:      30.91 min
[32m[20221213 22:49:08 @agent_ppo2.py:145][0m 3012608 total steps have happened
[32m[20221213 22:49:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1471 --------------------------#
[32m[20221213 22:49:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:49:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:08 @agent_ppo2.py:185][0m |           0.0027 |          34.9516 |          15.8534 |
[32m[20221213 22:49:08 @agent_ppo2.py:185][0m |          -0.0029 |          31.9428 |          15.8334 |
[32m[20221213 22:49:08 @agent_ppo2.py:185][0m |          -0.0030 |          31.2230 |          15.8605 |
[32m[20221213 22:49:08 @agent_ppo2.py:185][0m |          -0.0072 |          30.4688 |          15.8445 |
[32m[20221213 22:49:08 @agent_ppo2.py:185][0m |          -0.0014 |          30.5251 |          15.8476 |
[32m[20221213 22:49:09 @agent_ppo2.py:185][0m |          -0.0150 |          29.6247 |          15.8605 |
[32m[20221213 22:49:09 @agent_ppo2.py:185][0m |          -0.0071 |          29.3111 |          15.8526 |
[32m[20221213 22:49:09 @agent_ppo2.py:185][0m |          -0.0116 |          28.8483 |          15.8564 |
[32m[20221213 22:49:09 @agent_ppo2.py:185][0m |          -0.0115 |          28.5843 |          15.8480 |
[32m[20221213 22:49:09 @agent_ppo2.py:185][0m |          -0.0079 |          28.5314 |          15.8501 |
[32m[20221213 22:49:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:49:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.60
[32m[20221213 22:49:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.52
[32m[20221213 22:49:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.45
[32m[20221213 22:49:09 @agent_ppo2.py:143][0m Total time:      30.94 min
[32m[20221213 22:49:09 @agent_ppo2.py:145][0m 3014656 total steps have happened
[32m[20221213 22:49:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1472 --------------------------#
[32m[20221213 22:49:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:09 @agent_ppo2.py:185][0m |           0.0033 |          35.8177 |          15.8704 |
[32m[20221213 22:49:09 @agent_ppo2.py:185][0m |           0.0013 |          34.4170 |          15.8514 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0045 |          33.1891 |          15.8741 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0062 |          32.5801 |          15.8685 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0064 |          32.1111 |          15.8906 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0093 |          31.7489 |          15.8957 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0093 |          31.4526 |          15.8875 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0100 |          31.2070 |          15.9141 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0007 |          32.5576 |          15.9240 |
[32m[20221213 22:49:10 @agent_ppo2.py:185][0m |          -0.0108 |          30.8921 |          15.9176 |
[32m[20221213 22:49:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:49:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.60
[32m[20221213 22:49:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.29
[32m[20221213 22:49:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.99
[32m[20221213 22:49:10 @agent_ppo2.py:143][0m Total time:      30.96 min
[32m[20221213 22:49:10 @agent_ppo2.py:145][0m 3016704 total steps have happened
[32m[20221213 22:49:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1473 --------------------------#
[32m[20221213 22:49:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |           0.0025 |          34.6012 |          15.6457 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0019 |          30.2307 |          15.6374 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0068 |          28.1627 |          15.6272 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0059 |          27.4155 |          15.6200 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0101 |          26.6899 |          15.6227 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0113 |          26.2743 |          15.6422 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0107 |          25.6943 |          15.6307 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0119 |          25.2838 |          15.6467 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0108 |          24.9299 |          15.6339 |
[32m[20221213 22:49:11 @agent_ppo2.py:185][0m |          -0.0118 |          24.8864 |          15.6452 |
[32m[20221213 22:49:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:49:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.34
[32m[20221213 22:49:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.66
[32m[20221213 22:49:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.54
[32m[20221213 22:49:12 @agent_ppo2.py:143][0m Total time:      30.98 min
[32m[20221213 22:49:12 @agent_ppo2.py:145][0m 3018752 total steps have happened
[32m[20221213 22:49:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1474 --------------------------#
[32m[20221213 22:49:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:49:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:12 @agent_ppo2.py:185][0m |           0.0016 |          41.1484 |          15.8067 |
[32m[20221213 22:49:12 @agent_ppo2.py:185][0m |           0.0036 |          36.2310 |          15.7927 |
[32m[20221213 22:49:12 @agent_ppo2.py:185][0m |          -0.0130 |          33.9890 |          15.7757 |
[32m[20221213 22:49:12 @agent_ppo2.py:185][0m |          -0.0053 |          33.2187 |          15.7736 |
[32m[20221213 22:49:12 @agent_ppo2.py:185][0m |          -0.0118 |          32.7231 |          15.7535 |
[32m[20221213 22:49:12 @agent_ppo2.py:185][0m |          -0.0085 |          32.5236 |          15.7510 |
[32m[20221213 22:49:13 @agent_ppo2.py:185][0m |          -0.0113 |          32.1273 |          15.7496 |
[32m[20221213 22:49:13 @agent_ppo2.py:185][0m |          -0.0119 |          31.9926 |          15.7461 |
[32m[20221213 22:49:13 @agent_ppo2.py:185][0m |          -0.0066 |          32.4022 |          15.7353 |
[32m[20221213 22:49:13 @agent_ppo2.py:185][0m |          -0.0136 |          31.7382 |          15.7377 |
[32m[20221213 22:49:13 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:49:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.06
[32m[20221213 22:49:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.72
[32m[20221213 22:49:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 175.75
[32m[20221213 22:49:13 @agent_ppo2.py:143][0m Total time:      31.00 min
[32m[20221213 22:49:13 @agent_ppo2.py:145][0m 3020800 total steps have happened
[32m[20221213 22:49:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1475 --------------------------#
[32m[20221213 22:49:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:13 @agent_ppo2.py:185][0m |          -0.0001 |          41.5876 |          15.7435 |
[32m[20221213 22:49:13 @agent_ppo2.py:185][0m |          -0.0008 |          39.6995 |          15.7442 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0002 |          39.4942 |          15.7328 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0060 |          37.6973 |          15.7286 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0031 |          37.2155 |          15.7288 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0047 |          36.8296 |          15.7353 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0121 |          36.5107 |          15.7323 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0119 |          36.2174 |          15.7319 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0065 |          36.0123 |          15.7352 |
[32m[20221213 22:49:14 @agent_ppo2.py:185][0m |          -0.0120 |          35.8456 |          15.7183 |
[32m[20221213 22:49:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:49:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.68
[32m[20221213 22:49:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.22
[32m[20221213 22:49:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.58
[32m[20221213 22:49:14 @agent_ppo2.py:143][0m Total time:      31.02 min
[32m[20221213 22:49:14 @agent_ppo2.py:145][0m 3022848 total steps have happened
[32m[20221213 22:49:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1476 --------------------------#
[32m[20221213 22:49:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |           0.0038 |          36.3326 |          15.8220 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |           0.0029 |          34.7284 |          15.7905 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0059 |          33.2638 |          15.7859 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0077 |          32.8726 |          15.7931 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0069 |          32.4741 |          15.7864 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0103 |          32.1049 |          15.7717 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0063 |          32.0291 |          15.7841 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0109 |          31.7610 |          15.7826 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0148 |          31.5442 |          15.7820 |
[32m[20221213 22:49:15 @agent_ppo2.py:185][0m |          -0.0069 |          31.5146 |          15.7741 |
[32m[20221213 22:49:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:49:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.22
[32m[20221213 22:49:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 335.40
[32m[20221213 22:49:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.95
[32m[20221213 22:49:16 @agent_ppo2.py:143][0m Total time:      31.05 min
[32m[20221213 22:49:16 @agent_ppo2.py:145][0m 3024896 total steps have happened
[32m[20221213 22:49:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1477 --------------------------#
[32m[20221213 22:49:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:16 @agent_ppo2.py:185][0m |          -0.0024 |          43.2753 |          15.8021 |
[32m[20221213 22:49:16 @agent_ppo2.py:185][0m |          -0.0058 |          38.8210 |          15.7860 |
[32m[20221213 22:49:16 @agent_ppo2.py:185][0m |          -0.0056 |          37.3920 |          15.7782 |
[32m[20221213 22:49:16 @agent_ppo2.py:185][0m |          -0.0128 |          36.5019 |          15.7789 |
[32m[20221213 22:49:16 @agent_ppo2.py:185][0m |          -0.0137 |          36.0189 |          15.7688 |
[32m[20221213 22:49:16 @agent_ppo2.py:185][0m |          -0.0111 |          35.3955 |          15.7468 |
[32m[20221213 22:49:16 @agent_ppo2.py:185][0m |          -0.0094 |          34.7552 |          15.7454 |
[32m[20221213 22:49:17 @agent_ppo2.py:185][0m |          -0.0159 |          34.4512 |          15.7406 |
[32m[20221213 22:49:17 @agent_ppo2.py:185][0m |          -0.0117 |          34.0006 |          15.7474 |
[32m[20221213 22:49:17 @agent_ppo2.py:185][0m |          -0.0044 |          35.9882 |          15.7273 |
[32m[20221213 22:49:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:49:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.30
[32m[20221213 22:49:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.13
[32m[20221213 22:49:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.29
[32m[20221213 22:49:17 @agent_ppo2.py:143][0m Total time:      31.07 min
[32m[20221213 22:49:17 @agent_ppo2.py:145][0m 3026944 total steps have happened
[32m[20221213 22:49:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1478 --------------------------#
[32m[20221213 22:49:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:17 @agent_ppo2.py:185][0m |           0.0083 |          33.1909 |          15.7235 |
[32m[20221213 22:49:17 @agent_ppo2.py:185][0m |          -0.0083 |          28.1980 |          15.7085 |
[32m[20221213 22:49:17 @agent_ppo2.py:185][0m |          -0.0099 |          27.1494 |          15.6770 |
[32m[20221213 22:49:17 @agent_ppo2.py:185][0m |          -0.0132 |          26.6849 |          15.6553 |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |          -0.0090 |          26.1642 |          15.6400 |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |          -0.0135 |          25.9130 |          15.6167 |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |          -0.0043 |          27.9031 |          15.6181 |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |          -0.0112 |          25.4191 |          15.6017 |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |          -0.0185 |          25.2197 |          15.5907 |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |          -0.0172 |          25.0873 |          15.5883 |
[32m[20221213 22:49:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.18
[32m[20221213 22:49:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.83
[32m[20221213 22:49:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.68
[32m[20221213 22:49:18 @agent_ppo2.py:143][0m Total time:      31.09 min
[32m[20221213 22:49:18 @agent_ppo2.py:145][0m 3028992 total steps have happened
[32m[20221213 22:49:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1479 --------------------------#
[32m[20221213 22:49:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |           0.0012 |          44.2100 |          15.6982 |
[32m[20221213 22:49:18 @agent_ppo2.py:185][0m |          -0.0074 |          41.6832 |          15.6897 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0066 |          40.8048 |          15.6677 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0094 |          40.4158 |          15.6605 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0133 |          39.7635 |          15.6562 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0070 |          39.4285 |          15.6439 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0137 |          38.9384 |          15.6403 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0096 |          38.7727 |          15.6245 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0107 |          38.4480 |          15.6240 |
[32m[20221213 22:49:19 @agent_ppo2.py:185][0m |          -0.0057 |          39.4276 |          15.6018 |
[32m[20221213 22:49:19 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:49:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.30
[32m[20221213 22:49:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.09
[32m[20221213 22:49:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.81
[32m[20221213 22:49:19 @agent_ppo2.py:143][0m Total time:      31.11 min
[32m[20221213 22:49:19 @agent_ppo2.py:145][0m 3031040 total steps have happened
[32m[20221213 22:49:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1480 --------------------------#
[32m[20221213 22:49:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:49:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |           0.0063 |          40.5356 |          15.6236 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0014 |          35.7866 |          15.6075 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0047 |          34.6297 |          15.6073 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0041 |          34.2438 |          15.5905 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0052 |          33.9332 |          15.5751 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0155 |          33.4738 |          15.5922 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |           0.0006 |          35.8732 |          15.5832 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0079 |          32.9482 |          15.5771 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0123 |          32.7383 |          15.5769 |
[32m[20221213 22:49:20 @agent_ppo2.py:185][0m |          -0.0184 |          32.6541 |          15.5586 |
[32m[20221213 22:49:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.30
[32m[20221213 22:49:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.51
[32m[20221213 22:49:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.98
[32m[20221213 22:49:20 @agent_ppo2.py:143][0m Total time:      31.13 min
[32m[20221213 22:49:20 @agent_ppo2.py:145][0m 3033088 total steps have happened
[32m[20221213 22:49:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1481 --------------------------#
[32m[20221213 22:49:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0034 |          41.9314 |          15.7734 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0040 |          37.5935 |          15.7697 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0046 |          36.1556 |          15.7608 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0051 |          35.3381 |          15.7660 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0090 |          34.8276 |          15.7506 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0067 |          34.0323 |          15.7620 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0084 |          33.4974 |          15.7749 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0071 |          32.7432 |          15.7714 |
[32m[20221213 22:49:21 @agent_ppo2.py:185][0m |          -0.0115 |          32.1470 |          15.7753 |
[32m[20221213 22:49:22 @agent_ppo2.py:185][0m |          -0.0112 |          32.1788 |          15.7785 |
[32m[20221213 22:49:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.13
[32m[20221213 22:49:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.80
[32m[20221213 22:49:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.66
[32m[20221213 22:49:22 @agent_ppo2.py:143][0m Total time:      31.15 min
[32m[20221213 22:49:22 @agent_ppo2.py:145][0m 3035136 total steps have happened
[32m[20221213 22:49:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1482 --------------------------#
[32m[20221213 22:49:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:22 @agent_ppo2.py:185][0m |          -0.0022 |          40.7981 |          15.7341 |
[32m[20221213 22:49:22 @agent_ppo2.py:185][0m |          -0.0065 |          37.8367 |          15.7150 |
[32m[20221213 22:49:22 @agent_ppo2.py:185][0m |          -0.0030 |          36.8381 |          15.7105 |
[32m[20221213 22:49:22 @agent_ppo2.py:185][0m |          -0.0093 |          36.3078 |          15.6917 |
[32m[20221213 22:49:22 @agent_ppo2.py:185][0m |          -0.0127 |          35.8731 |          15.6975 |
[32m[20221213 22:49:22 @agent_ppo2.py:185][0m |           0.0018 |          41.4126 |          15.6934 |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0127 |          35.2023 |          15.6571 |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0128 |          34.9288 |          15.6726 |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0050 |          35.0439 |          15.6842 |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0135 |          34.5839 |          15.6656 |
[32m[20221213 22:49:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:49:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.74
[32m[20221213 22:49:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.95
[32m[20221213 22:49:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.76
[32m[20221213 22:49:23 @agent_ppo2.py:143][0m Total time:      31.17 min
[32m[20221213 22:49:23 @agent_ppo2.py:145][0m 3037184 total steps have happened
[32m[20221213 22:49:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1483 --------------------------#
[32m[20221213 22:49:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0017 |          31.2321 |          15.5048 |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0099 |          25.4460 |          15.4810 |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0085 |          24.2429 |          15.4817 |
[32m[20221213 22:49:23 @agent_ppo2.py:185][0m |          -0.0087 |          23.6282 |          15.4424 |
[32m[20221213 22:49:24 @agent_ppo2.py:185][0m |          -0.0110 |          23.3156 |          15.4558 |
[32m[20221213 22:49:24 @agent_ppo2.py:185][0m |          -0.0130 |          23.0231 |          15.4510 |
[32m[20221213 22:49:24 @agent_ppo2.py:185][0m |          -0.0128 |          22.7935 |          15.4519 |
[32m[20221213 22:49:24 @agent_ppo2.py:185][0m |          -0.0101 |          22.7262 |          15.4420 |
[32m[20221213 22:49:24 @agent_ppo2.py:185][0m |          -0.0145 |          22.4492 |          15.4488 |
[32m[20221213 22:49:24 @agent_ppo2.py:185][0m |          -0.0141 |          22.2375 |          15.4380 |
[32m[20221213 22:49:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:49:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.06
[32m[20221213 22:49:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.90
[32m[20221213 22:49:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.16
[32m[20221213 22:49:24 @agent_ppo2.py:143][0m Total time:      31.19 min
[32m[20221213 22:49:24 @agent_ppo2.py:145][0m 3039232 total steps have happened
[32m[20221213 22:49:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1484 --------------------------#
[32m[20221213 22:49:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:24 @agent_ppo2.py:185][0m |           0.0061 |          27.1482 |          15.7120 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0002 |          22.9104 |          15.6671 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0081 |          21.4451 |          15.6639 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0001 |          23.8072 |          15.6529 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0110 |          20.3990 |          15.6523 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0109 |          19.8428 |          15.6682 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0129 |          19.4956 |          15.6576 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0134 |          19.2942 |          15.6634 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0190 |          19.1682 |          15.6540 |
[32m[20221213 22:49:25 @agent_ppo2.py:185][0m |          -0.0056 |          20.7444 |          15.6513 |
[32m[20221213 22:49:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.68
[32m[20221213 22:49:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.47
[32m[20221213 22:49:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.86
[32m[20221213 22:49:25 @agent_ppo2.py:143][0m Total time:      31.21 min
[32m[20221213 22:49:25 @agent_ppo2.py:145][0m 3041280 total steps have happened
[32m[20221213 22:49:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1485 --------------------------#
[32m[20221213 22:49:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |           0.0016 |          38.1004 |          15.6998 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0070 |          35.4238 |          15.6576 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |           0.0007 |          35.7519 |          15.6555 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0078 |          34.0410 |          15.6494 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0109 |          33.5733 |          15.6629 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0094 |          33.6140 |          15.6392 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0100 |          33.0745 |          15.6314 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0115 |          32.8985 |          15.6238 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0086 |          33.8354 |          15.6407 |
[32m[20221213 22:49:26 @agent_ppo2.py:185][0m |          -0.0169 |          32.5928 |          15.6272 |
[32m[20221213 22:49:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:49:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.88
[32m[20221213 22:49:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.08
[32m[20221213 22:49:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.96
[32m[20221213 22:49:27 @agent_ppo2.py:143][0m Total time:      31.23 min
[32m[20221213 22:49:27 @agent_ppo2.py:145][0m 3043328 total steps have happened
[32m[20221213 22:49:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1486 --------------------------#
[32m[20221213 22:49:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |           0.0008 |          38.3950 |          15.6426 |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |          -0.0032 |          36.0866 |          15.6240 |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |          -0.0108 |          35.5112 |          15.6104 |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |          -0.0070 |          35.0259 |          15.5995 |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |          -0.0023 |          35.4672 |          15.5925 |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |          -0.0104 |          34.5767 |          15.5988 |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |          -0.0105 |          34.3430 |          15.6026 |
[32m[20221213 22:49:27 @agent_ppo2.py:185][0m |          -0.0125 |          34.1925 |          15.5998 |
[32m[20221213 22:49:28 @agent_ppo2.py:185][0m |          -0.0094 |          34.0257 |          15.6058 |
[32m[20221213 22:49:28 @agent_ppo2.py:185][0m |          -0.0070 |          34.5991 |          15.6008 |
[32m[20221213 22:49:28 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.49
[32m[20221213 22:49:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.36
[32m[20221213 22:49:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.79
[32m[20221213 22:49:28 @agent_ppo2.py:143][0m Total time:      31.25 min
[32m[20221213 22:49:28 @agent_ppo2.py:145][0m 3045376 total steps have happened
[32m[20221213 22:49:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1487 --------------------------#
[32m[20221213 22:49:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:28 @agent_ppo2.py:185][0m |           0.0037 |          33.8316 |          15.8308 |
[32m[20221213 22:49:28 @agent_ppo2.py:185][0m |          -0.0045 |          30.9013 |          15.8162 |
[32m[20221213 22:49:28 @agent_ppo2.py:185][0m |          -0.0057 |          29.7314 |          15.8001 |
[32m[20221213 22:49:28 @agent_ppo2.py:185][0m |          -0.0047 |          29.0395 |          15.7935 |
[32m[20221213 22:49:28 @agent_ppo2.py:185][0m |          -0.0053 |          28.4876 |          15.7934 |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0103 |          28.0831 |          15.7857 |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0098 |          27.7551 |          15.7733 |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0124 |          27.4872 |          15.7726 |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0173 |          27.2148 |          15.7695 |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0110 |          27.0049 |          15.7568 |
[32m[20221213 22:49:29 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:49:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.91
[32m[20221213 22:49:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.31
[32m[20221213 22:49:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 180.04
[32m[20221213 22:49:29 @agent_ppo2.py:143][0m Total time:      31.27 min
[32m[20221213 22:49:29 @agent_ppo2.py:145][0m 3047424 total steps have happened
[32m[20221213 22:49:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1488 --------------------------#
[32m[20221213 22:49:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0001 |          41.6019 |          15.7197 |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0064 |          39.2572 |          15.7277 |
[32m[20221213 22:49:29 @agent_ppo2.py:185][0m |          -0.0025 |          38.7201 |          15.7010 |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |          -0.0031 |          38.5539 |          15.7151 |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |          -0.0104 |          37.7042 |          15.7013 |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |          -0.0143 |          37.5980 |          15.7152 |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |           0.0005 |          40.4060 |          15.6759 |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |          -0.0115 |          36.8798 |          15.6879 |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |          -0.0027 |          37.7707 |          15.6956 |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |          -0.0132 |          36.4727 |          15.6876 |
[32m[20221213 22:49:30 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.97
[32m[20221213 22:49:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.45
[32m[20221213 22:49:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.90
[32m[20221213 22:49:30 @agent_ppo2.py:143][0m Total time:      31.29 min
[32m[20221213 22:49:30 @agent_ppo2.py:145][0m 3049472 total steps have happened
[32m[20221213 22:49:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1489 --------------------------#
[32m[20221213 22:49:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:30 @agent_ppo2.py:185][0m |           0.0015 |          46.0333 |          15.5429 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0054 |          41.5306 |          15.4993 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0128 |          40.2069 |          15.5179 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0094 |          39.4619 |          15.5256 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0126 |          38.8424 |          15.5076 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0123 |          38.3965 |          15.5084 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0100 |          38.1817 |          15.5062 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0128 |          37.8765 |          15.5022 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0098 |          37.4105 |          15.4953 |
[32m[20221213 22:49:31 @agent_ppo2.py:185][0m |          -0.0019 |          39.3071 |          15.5034 |
[32m[20221213 22:49:31 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.74
[32m[20221213 22:49:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.99
[32m[20221213 22:49:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.85
[32m[20221213 22:49:31 @agent_ppo2.py:143][0m Total time:      31.31 min
[32m[20221213 22:49:31 @agent_ppo2.py:145][0m 3051520 total steps have happened
[32m[20221213 22:49:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1490 --------------------------#
[32m[20221213 22:49:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:49:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |           0.0025 |          31.3853 |          15.6097 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |           0.0015 |          27.0125 |          15.6152 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0077 |          25.7705 |          15.6148 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0075 |          25.3160 |          15.6067 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0025 |          25.7317 |          15.6290 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0112 |          24.8550 |          15.6190 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0022 |          24.9519 |          15.6205 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0080 |          24.4369 |          15.6234 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0076 |          24.3324 |          15.6284 |
[32m[20221213 22:49:32 @agent_ppo2.py:185][0m |          -0.0152 |          24.2644 |          15.6279 |
[32m[20221213 22:49:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:49:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.10
[32m[20221213 22:49:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.05
[32m[20221213 22:49:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.66
[32m[20221213 22:49:33 @agent_ppo2.py:143][0m Total time:      31.33 min
[32m[20221213 22:49:33 @agent_ppo2.py:145][0m 3053568 total steps have happened
[32m[20221213 22:49:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1491 --------------------------#
[32m[20221213 22:49:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |           0.0070 |          42.5252 |          15.6650 |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |           0.0024 |          39.7222 |          15.6748 |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |          -0.0068 |          38.9033 |          15.6655 |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |          -0.0043 |          38.6517 |          15.6507 |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |          -0.0053 |          38.1709 |          15.6567 |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |          -0.0022 |          38.2667 |          15.6637 |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |          -0.0091 |          37.9683 |          15.6487 |
[32m[20221213 22:49:33 @agent_ppo2.py:185][0m |          -0.0111 |          38.0257 |          15.6526 |
[32m[20221213 22:49:34 @agent_ppo2.py:185][0m |          -0.0063 |          37.9220 |          15.6478 |
[32m[20221213 22:49:34 @agent_ppo2.py:185][0m |          -0.0047 |          37.8157 |          15.6400 |
[32m[20221213 22:49:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:49:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.39
[32m[20221213 22:49:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.32
[32m[20221213 22:49:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.00
[32m[20221213 22:49:34 @agent_ppo2.py:143][0m Total time:      31.35 min
[32m[20221213 22:49:34 @agent_ppo2.py:145][0m 3055616 total steps have happened
[32m[20221213 22:49:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1492 --------------------------#
[32m[20221213 22:49:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:34 @agent_ppo2.py:185][0m |           0.0092 |          44.2490 |          15.5442 |
[32m[20221213 22:49:34 @agent_ppo2.py:185][0m |          -0.0041 |          39.6677 |          15.5514 |
[32m[20221213 22:49:34 @agent_ppo2.py:185][0m |          -0.0057 |          38.6896 |          15.5386 |
[32m[20221213 22:49:34 @agent_ppo2.py:185][0m |          -0.0102 |          38.2254 |          15.5437 |
[32m[20221213 22:49:34 @agent_ppo2.py:185][0m |          -0.0097 |          37.6982 |          15.5278 |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |          -0.0116 |          37.4789 |          15.5289 |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |          -0.0133 |          37.1926 |          15.5439 |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |          -0.0115 |          37.1588 |          15.5110 |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |           0.0021 |          41.4355 |          15.5336 |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |          -0.0089 |          37.0178 |          15.5168 |
[32m[20221213 22:49:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:49:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 307.13
[32m[20221213 22:49:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.21
[32m[20221213 22:49:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.85
[32m[20221213 22:49:35 @agent_ppo2.py:143][0m Total time:      31.37 min
[32m[20221213 22:49:35 @agent_ppo2.py:145][0m 3057664 total steps have happened
[32m[20221213 22:49:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1493 --------------------------#
[32m[20221213 22:49:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |           0.0016 |          30.9781 |          15.6583 |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |           0.0121 |          32.8090 |          15.6563 |
[32m[20221213 22:49:35 @agent_ppo2.py:185][0m |          -0.0028 |          29.2232 |          15.6575 |
[32m[20221213 22:49:36 @agent_ppo2.py:185][0m |          -0.0018 |          29.3476 |          15.6556 |
[32m[20221213 22:49:36 @agent_ppo2.py:185][0m |          -0.0083 |          28.9247 |          15.6586 |
[32m[20221213 22:49:36 @agent_ppo2.py:185][0m |          -0.0038 |          29.3257 |          15.6546 |
[32m[20221213 22:49:36 @agent_ppo2.py:185][0m |          -0.0072 |          28.8817 |          15.6582 |
[32m[20221213 22:49:36 @agent_ppo2.py:185][0m |          -0.0038 |          28.8930 |          15.6597 |
[32m[20221213 22:49:36 @agent_ppo2.py:185][0m |          -0.0088 |          28.8204 |          15.6776 |
[32m[20221213 22:49:36 @agent_ppo2.py:185][0m |          -0.0070 |          28.6661 |          15.6668 |
[32m[20221213 22:49:36 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.90
[32m[20221213 22:49:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.55
[32m[20221213 22:49:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.53
[32m[20221213 22:49:36 @agent_ppo2.py:143][0m Total time:      31.39 min
[32m[20221213 22:49:36 @agent_ppo2.py:145][0m 3059712 total steps have happened
[32m[20221213 22:49:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1494 --------------------------#
[32m[20221213 22:49:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0025 |          43.3958 |          15.6505 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0069 |          40.6335 |          15.6249 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0062 |          39.7894 |          15.6265 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0040 |          38.8347 |          15.6176 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0057 |          38.4608 |          15.6009 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0072 |          37.9999 |          15.5985 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0074 |          37.8036 |          15.5953 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0112 |          37.5658 |          15.5698 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |          -0.0068 |          37.9386 |          15.5888 |
[32m[20221213 22:49:37 @agent_ppo2.py:185][0m |           0.0004 |          39.0865 |          15.5747 |
[32m[20221213 22:49:37 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.95
[32m[20221213 22:49:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.32
[32m[20221213 22:49:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.04
[32m[20221213 22:49:37 @agent_ppo2.py:143][0m Total time:      31.41 min
[32m[20221213 22:49:37 @agent_ppo2.py:145][0m 3061760 total steps have happened
[32m[20221213 22:49:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1495 --------------------------#
[32m[20221213 22:49:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0003 |          38.6020 |          15.6835 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |           0.0128 |          39.0597 |          15.6841 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0046 |          34.0363 |          15.6676 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0036 |          33.3483 |          15.6919 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0098 |          32.8225 |          15.7083 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0101 |          32.4416 |          15.6987 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0010 |          32.1233 |          15.7104 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0014 |          31.6860 |          15.7054 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0090 |          31.1297 |          15.7157 |
[32m[20221213 22:49:38 @agent_ppo2.py:185][0m |          -0.0031 |          32.0242 |          15.7214 |
[32m[20221213 22:49:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:49:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.36
[32m[20221213 22:49:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.71
[32m[20221213 22:49:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.66
[32m[20221213 22:49:39 @agent_ppo2.py:143][0m Total time:      31.43 min
[32m[20221213 22:49:39 @agent_ppo2.py:145][0m 3063808 total steps have happened
[32m[20221213 22:49:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1496 --------------------------#
[32m[20221213 22:49:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:39 @agent_ppo2.py:185][0m |          -0.0006 |          36.2287 |          15.6187 |
[32m[20221213 22:49:39 @agent_ppo2.py:185][0m |           0.0002 |          33.0296 |          15.6104 |
[32m[20221213 22:49:39 @agent_ppo2.py:185][0m |           0.0020 |          33.4756 |          15.6174 |
[32m[20221213 22:49:39 @agent_ppo2.py:185][0m |          -0.0102 |          30.8392 |          15.6003 |
[32m[20221213 22:49:39 @agent_ppo2.py:185][0m |          -0.0087 |          30.2401 |          15.5918 |
[32m[20221213 22:49:39 @agent_ppo2.py:185][0m |          -0.0128 |          29.8587 |          15.5982 |
[32m[20221213 22:49:39 @agent_ppo2.py:185][0m |          -0.0125 |          29.4634 |          15.5887 |
[32m[20221213 22:49:40 @agent_ppo2.py:185][0m |          -0.0117 |          29.1935 |          15.5995 |
[32m[20221213 22:49:40 @agent_ppo2.py:185][0m |          -0.0157 |          29.0026 |          15.5932 |
[32m[20221213 22:49:40 @agent_ppo2.py:185][0m |          -0.0145 |          28.7487 |          15.5837 |
[32m[20221213 22:49:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.79
[32m[20221213 22:49:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.75
[32m[20221213 22:49:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.31
[32m[20221213 22:49:40 @agent_ppo2.py:143][0m Total time:      31.45 min
[32m[20221213 22:49:40 @agent_ppo2.py:145][0m 3065856 total steps have happened
[32m[20221213 22:49:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1497 --------------------------#
[32m[20221213 22:49:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:40 @agent_ppo2.py:185][0m |           0.0007 |          48.4111 |          15.7975 |
[32m[20221213 22:49:40 @agent_ppo2.py:185][0m |          -0.0081 |          47.0458 |          15.7809 |
[32m[20221213 22:49:40 @agent_ppo2.py:185][0m |          -0.0085 |          45.9388 |          15.7756 |
[32m[20221213 22:49:40 @agent_ppo2.py:185][0m |          -0.0042 |          45.7240 |          15.7960 |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |           0.0033 |          48.3056 |          15.7792 |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |          -0.0021 |          46.6488 |          15.7750 |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |          -0.0049 |          45.2042 |          15.7802 |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |          -0.0089 |          44.5423 |          15.7913 |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |          -0.0093 |          44.4056 |          15.7809 |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |          -0.0042 |          45.1916 |          15.7636 |
[32m[20221213 22:49:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:49:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.49
[32m[20221213 22:49:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.83
[32m[20221213 22:49:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 341.63
[32m[20221213 22:49:41 @agent_ppo2.py:143][0m Total time:      31.47 min
[32m[20221213 22:49:41 @agent_ppo2.py:145][0m 3067904 total steps have happened
[32m[20221213 22:49:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1498 --------------------------#
[32m[20221213 22:49:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |          -0.0002 |          34.6003 |          15.7064 |
[32m[20221213 22:49:41 @agent_ppo2.py:185][0m |           0.0009 |          29.9471 |          15.6872 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |          -0.0058 |          28.8885 |          15.6972 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |          -0.0064 |          28.3122 |          15.7050 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |          -0.0101 |          28.0080 |          15.6894 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |          -0.0102 |          27.6321 |          15.6815 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |           0.0023 |          29.7124 |          15.7003 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |          -0.0126 |          27.3910 |          15.6999 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |          -0.0102 |          26.9771 |          15.7004 |
[32m[20221213 22:49:42 @agent_ppo2.py:185][0m |          -0.0112 |          26.8103 |          15.6965 |
[32m[20221213 22:49:42 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.70
[32m[20221213 22:49:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.35
[32m[20221213 22:49:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.04
[32m[20221213 22:49:42 @agent_ppo2.py:143][0m Total time:      31.49 min
[32m[20221213 22:49:42 @agent_ppo2.py:145][0m 3069952 total steps have happened
[32m[20221213 22:49:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1499 --------------------------#
[32m[20221213 22:49:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |           0.0048 |          25.0039 |          15.6593 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |          -0.0052 |          21.5945 |          15.6576 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |           0.0001 |          21.1207 |          15.6647 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |          -0.0029 |          20.3807 |          15.6714 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |          -0.0020 |          20.1305 |          15.6579 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |           0.0013 |          22.2270 |          15.6726 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |          -0.0028 |          19.6827 |          15.6690 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |           0.0011 |          19.3437 |          15.6841 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |          -0.0124 |          19.0814 |          15.6921 |
[32m[20221213 22:49:43 @agent_ppo2.py:185][0m |          -0.0127 |          18.9573 |          15.6818 |
[32m[20221213 22:49:43 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.51
[32m[20221213 22:49:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 344.11
[32m[20221213 22:49:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.70
[32m[20221213 22:49:43 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 480.12
[32m[20221213 22:49:43 @agent_ppo2.py:143][0m Total time:      31.51 min
[32m[20221213 22:49:43 @agent_ppo2.py:145][0m 3072000 total steps have happened
[32m[20221213 22:49:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1500 --------------------------#
[32m[20221213 22:49:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:49:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0031 |          24.7420 |          15.6686 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0082 |          21.9200 |          15.6472 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0094 |          21.2671 |          15.6536 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0119 |          20.9116 |          15.6579 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0075 |          20.6570 |          15.6563 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0131 |          20.3847 |          15.6459 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0152 |          20.1975 |          15.6484 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0085 |          19.9843 |          15.6336 |
[32m[20221213 22:49:44 @agent_ppo2.py:185][0m |          -0.0172 |          19.9074 |          15.6299 |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |          -0.0175 |          19.9471 |          15.6415 |
[32m[20221213 22:49:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.26
[32m[20221213 22:49:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.72
[32m[20221213 22:49:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.50
[32m[20221213 22:49:45 @agent_ppo2.py:143][0m Total time:      31.53 min
[32m[20221213 22:49:45 @agent_ppo2.py:145][0m 3074048 total steps have happened
[32m[20221213 22:49:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1501 --------------------------#
[32m[20221213 22:49:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |          -0.0008 |          30.7281 |          15.6018 |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |          -0.0087 |          28.7049 |          15.5749 |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |          -0.0079 |          28.3500 |          15.5659 |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |           0.0075 |          31.0260 |          15.5671 |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |          -0.0093 |          27.9871 |          15.5555 |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |          -0.0081 |          28.0890 |          15.5489 |
[32m[20221213 22:49:45 @agent_ppo2.py:185][0m |          -0.0081 |          27.4765 |          15.5531 |
[32m[20221213 22:49:46 @agent_ppo2.py:185][0m |          -0.0092 |          27.2918 |          15.5544 |
[32m[20221213 22:49:46 @agent_ppo2.py:185][0m |          -0.0162 |          27.0589 |          15.5446 |
[32m[20221213 22:49:46 @agent_ppo2.py:185][0m |          -0.0140 |          26.9989 |          15.5716 |
[32m[20221213 22:49:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.12
[32m[20221213 22:49:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.42
[32m[20221213 22:49:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 370.80
[32m[20221213 22:49:46 @agent_ppo2.py:143][0m Total time:      31.55 min
[32m[20221213 22:49:46 @agent_ppo2.py:145][0m 3076096 total steps have happened
[32m[20221213 22:49:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1502 --------------------------#
[32m[20221213 22:49:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:46 @agent_ppo2.py:185][0m |           0.0035 |          33.1521 |          15.6346 |
[32m[20221213 22:49:46 @agent_ppo2.py:185][0m |          -0.0006 |          30.9821 |          15.5858 |
[32m[20221213 22:49:46 @agent_ppo2.py:185][0m |          -0.0075 |          30.3481 |          15.6158 |
[32m[20221213 22:49:46 @agent_ppo2.py:185][0m |          -0.0058 |          29.8791 |          15.6128 |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |          -0.0096 |          29.5398 |          15.6203 |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |          -0.0102 |          29.3655 |          15.6421 |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |          -0.0029 |          29.6210 |          15.6362 |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |          -0.0102 |          28.9905 |          15.6370 |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |          -0.0097 |          28.7592 |          15.6418 |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |          -0.0121 |          28.6080 |          15.6510 |
[32m[20221213 22:49:47 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:49:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.74
[32m[20221213 22:49:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.95
[32m[20221213 22:49:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.50
[32m[20221213 22:49:47 @agent_ppo2.py:143][0m Total time:      31.57 min
[32m[20221213 22:49:47 @agent_ppo2.py:145][0m 3078144 total steps have happened
[32m[20221213 22:49:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1503 --------------------------#
[32m[20221213 22:49:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |           0.0054 |          28.7291 |          15.4555 |
[32m[20221213 22:49:47 @agent_ppo2.py:185][0m |          -0.0016 |          25.5403 |          15.4533 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |          -0.0064 |          24.4621 |          15.4608 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |          -0.0083 |          23.9680 |          15.4512 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |          -0.0103 |          23.6844 |          15.4555 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |           0.0063 |          24.9142 |          15.4667 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |          -0.0105 |          23.2631 |          15.4625 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |          -0.0090 |          23.0214 |          15.4584 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |          -0.0128 |          22.9119 |          15.4633 |
[32m[20221213 22:49:48 @agent_ppo2.py:185][0m |          -0.0140 |          22.7457 |          15.4502 |
[32m[20221213 22:49:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:49:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.71
[32m[20221213 22:49:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.26
[32m[20221213 22:49:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.22
[32m[20221213 22:49:48 @agent_ppo2.py:143][0m Total time:      31.59 min
[32m[20221213 22:49:48 @agent_ppo2.py:145][0m 3080192 total steps have happened
[32m[20221213 22:49:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1504 --------------------------#
[32m[20221213 22:49:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:49:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0040 |          35.8874 |          15.7957 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0045 |          34.5882 |          15.7648 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0016 |          36.8130 |          15.7296 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0073 |          33.9691 |          15.7359 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0059 |          33.8126 |          15.7272 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0112 |          33.4473 |          15.7136 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0112 |          33.4917 |          15.7069 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0117 |          33.3207 |          15.7001 |
[32m[20221213 22:49:49 @agent_ppo2.py:185][0m |          -0.0114 |          33.1160 |          15.6891 |
[32m[20221213 22:49:50 @agent_ppo2.py:185][0m |          -0.0154 |          33.1676 |          15.6877 |
[32m[20221213 22:49:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:49:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.37
[32m[20221213 22:49:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.39
[32m[20221213 22:49:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.67
[32m[20221213 22:49:50 @agent_ppo2.py:143][0m Total time:      31.61 min
[32m[20221213 22:49:50 @agent_ppo2.py:145][0m 3082240 total steps have happened
[32m[20221213 22:49:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1505 --------------------------#
[32m[20221213 22:49:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:49:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:50 @agent_ppo2.py:185][0m |          -0.0019 |          39.1023 |          15.5479 |
[32m[20221213 22:49:50 @agent_ppo2.py:185][0m |          -0.0015 |          36.3294 |          15.5510 |
[32m[20221213 22:49:50 @agent_ppo2.py:185][0m |          -0.0078 |          35.4025 |          15.5067 |
[32m[20221213 22:49:50 @agent_ppo2.py:185][0m |          -0.0067 |          34.6458 |          15.5385 |
[32m[20221213 22:49:50 @agent_ppo2.py:185][0m |          -0.0099 |          34.5786 |          15.5216 |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |           0.0085 |          40.2045 |          15.5207 |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |          -0.0065 |          34.5446 |          15.5032 |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |          -0.0041 |          34.8407 |          15.4977 |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |          -0.0099 |          33.8251 |          15.5217 |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |          -0.0049 |          33.7815 |          15.5003 |
[32m[20221213 22:49:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:49:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.28
[32m[20221213 22:49:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.42
[32m[20221213 22:49:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.10
[32m[20221213 22:49:51 @agent_ppo2.py:143][0m Total time:      31.63 min
[32m[20221213 22:49:51 @agent_ppo2.py:145][0m 3084288 total steps have happened
[32m[20221213 22:49:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1506 --------------------------#
[32m[20221213 22:49:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |           0.0041 |          36.0579 |          15.6338 |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |          -0.0054 |          31.5200 |          15.6219 |
[32m[20221213 22:49:51 @agent_ppo2.py:185][0m |          -0.0076 |          30.5159 |          15.6177 |
[32m[20221213 22:49:52 @agent_ppo2.py:185][0m |          -0.0094 |          29.9106 |          15.6224 |
[32m[20221213 22:49:52 @agent_ppo2.py:185][0m |          -0.0119 |          29.5820 |          15.6253 |
[32m[20221213 22:49:52 @agent_ppo2.py:185][0m |          -0.0006 |          30.3736 |          15.6395 |
[32m[20221213 22:49:52 @agent_ppo2.py:185][0m |          -0.0067 |          29.0157 |          15.6292 |
[32m[20221213 22:49:52 @agent_ppo2.py:185][0m |          -0.0090 |          29.0946 |          15.6258 |
[32m[20221213 22:49:52 @agent_ppo2.py:185][0m |          -0.0129 |          28.4875 |          15.6384 |
[32m[20221213 22:49:52 @agent_ppo2.py:185][0m |          -0.0075 |          31.6988 |          15.6330 |
[32m[20221213 22:49:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:49:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.65
[32m[20221213 22:49:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.47
[32m[20221213 22:49:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.11
[32m[20221213 22:49:52 @agent_ppo2.py:143][0m Total time:      31.66 min
[32m[20221213 22:49:52 @agent_ppo2.py:145][0m 3086336 total steps have happened
[32m[20221213 22:49:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1507 --------------------------#
[32m[20221213 22:49:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0009 |          35.5384 |          15.7021 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0026 |          32.6240 |          15.6967 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0037 |          31.8131 |          15.6757 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0033 |          31.5085 |          15.6687 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0073 |          31.1815 |          15.6558 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0087 |          31.0405 |          15.6529 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0079 |          30.8347 |          15.6479 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0012 |          33.1019 |          15.6417 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0109 |          30.6299 |          15.6349 |
[32m[20221213 22:49:53 @agent_ppo2.py:185][0m |          -0.0107 |          30.4693 |          15.6383 |
[32m[20221213 22:49:53 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:49:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.47
[32m[20221213 22:49:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.03
[32m[20221213 22:49:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.21
[32m[20221213 22:49:54 @agent_ppo2.py:143][0m Total time:      31.68 min
[32m[20221213 22:49:54 @agent_ppo2.py:145][0m 3088384 total steps have happened
[32m[20221213 22:49:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1508 --------------------------#
[32m[20221213 22:49:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:49:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:54 @agent_ppo2.py:185][0m |          -0.0018 |          37.8016 |          15.2861 |
[32m[20221213 22:49:54 @agent_ppo2.py:185][0m |          -0.0059 |          35.4102 |          15.2976 |
[32m[20221213 22:49:54 @agent_ppo2.py:185][0m |          -0.0114 |          34.3159 |          15.2999 |
[32m[20221213 22:49:54 @agent_ppo2.py:185][0m |          -0.0091 |          33.5702 |          15.3007 |
[32m[20221213 22:49:54 @agent_ppo2.py:185][0m |          -0.0132 |          32.7082 |          15.2881 |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0102 |          32.1335 |          15.2985 |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0139 |          31.8082 |          15.3231 |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0139 |          31.3454 |          15.3079 |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0124 |          30.9825 |          15.3038 |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0136 |          30.8339 |          15.3069 |
[32m[20221213 22:49:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:49:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.65
[32m[20221213 22:49:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.69
[32m[20221213 22:49:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 209.08
[32m[20221213 22:49:55 @agent_ppo2.py:143][0m Total time:      31.70 min
[32m[20221213 22:49:55 @agent_ppo2.py:145][0m 3090432 total steps have happened
[32m[20221213 22:49:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1509 --------------------------#
[32m[20221213 22:49:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0019 |          32.3474 |          15.6311 |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0050 |          26.5848 |          15.6276 |
[32m[20221213 22:49:55 @agent_ppo2.py:185][0m |          -0.0003 |          25.7918 |          15.6176 |
[32m[20221213 22:49:56 @agent_ppo2.py:185][0m |          -0.0054 |          24.6783 |          15.6054 |
[32m[20221213 22:49:56 @agent_ppo2.py:185][0m |          -0.0096 |          24.3717 |          15.5886 |
[32m[20221213 22:49:56 @agent_ppo2.py:185][0m |          -0.0041 |          24.3410 |          15.5850 |
[32m[20221213 22:49:56 @agent_ppo2.py:185][0m |          -0.0074 |          24.0841 |          15.5776 |
[32m[20221213 22:49:56 @agent_ppo2.py:185][0m |          -0.0128 |          23.4371 |          15.5723 |
[32m[20221213 22:49:56 @agent_ppo2.py:185][0m |          -0.0140 |          23.1139 |          15.5897 |
[32m[20221213 22:49:56 @agent_ppo2.py:185][0m |          -0.0205 |          22.9704 |          15.5775 |
[32m[20221213 22:49:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.89
[32m[20221213 22:49:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.81
[32m[20221213 22:49:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.20
[32m[20221213 22:49:56 @agent_ppo2.py:143][0m Total time:      31.72 min
[32m[20221213 22:49:56 @agent_ppo2.py:145][0m 3092480 total steps have happened
[32m[20221213 22:49:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1510 --------------------------#
[32m[20221213 22:49:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:49:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |           0.0015 |          40.1536 |          15.6729 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0006 |          38.7594 |          15.6998 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0014 |          37.6490 |          15.6971 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0046 |          37.1996 |          15.6863 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0030 |          36.8413 |          15.6969 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |           0.0058 |          39.5000 |          15.6839 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0092 |          36.5697 |          15.6938 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0093 |          36.3303 |          15.6862 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0067 |          36.2177 |          15.6852 |
[32m[20221213 22:49:57 @agent_ppo2.py:185][0m |          -0.0030 |          36.2877 |          15.6785 |
[32m[20221213 22:49:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:49:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.24
[32m[20221213 22:49:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.32
[32m[20221213 22:49:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.23
[32m[20221213 22:49:57 @agent_ppo2.py:143][0m Total time:      31.74 min
[32m[20221213 22:49:57 @agent_ppo2.py:145][0m 3094528 total steps have happened
[32m[20221213 22:49:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1511 --------------------------#
[32m[20221213 22:49:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:49:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |           0.0153 |          31.8055 |          15.7055 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0094 |          23.8644 |          15.6955 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0050 |          22.7035 |          15.6943 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0100 |          22.1562 |          15.6946 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0057 |          21.8300 |          15.6989 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0115 |          21.6044 |          15.6999 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0098 |          21.5116 |          15.6924 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0142 |          21.2546 |          15.6998 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0134 |          21.1795 |          15.7125 |
[32m[20221213 22:49:58 @agent_ppo2.py:185][0m |          -0.0050 |          22.5269 |          15.6994 |
[32m[20221213 22:49:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:49:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.32
[32m[20221213 22:49:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.73
[32m[20221213 22:49:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.39
[32m[20221213 22:49:59 @agent_ppo2.py:143][0m Total time:      31.76 min
[32m[20221213 22:49:59 @agent_ppo2.py:145][0m 3096576 total steps have happened
[32m[20221213 22:49:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1512 --------------------------#
[32m[20221213 22:49:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:49:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:49:59 @agent_ppo2.py:185][0m |          -0.0016 |          41.8160 |          15.5771 |
[32m[20221213 22:49:59 @agent_ppo2.py:185][0m |          -0.0063 |          38.1109 |          15.5779 |
[32m[20221213 22:49:59 @agent_ppo2.py:185][0m |          -0.0080 |          37.0923 |          15.5995 |
[32m[20221213 22:49:59 @agent_ppo2.py:185][0m |          -0.0158 |          36.6177 |          15.6153 |
[32m[20221213 22:49:59 @agent_ppo2.py:185][0m |          -0.0065 |          36.2236 |          15.5944 |
[32m[20221213 22:49:59 @agent_ppo2.py:185][0m |          -0.0110 |          35.9143 |          15.6081 |
[32m[20221213 22:49:59 @agent_ppo2.py:185][0m |          -0.0122 |          35.4606 |          15.6203 |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |          -0.0120 |          35.3284 |          15.6316 |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |          -0.0112 |          34.9290 |          15.6428 |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |          -0.0138 |          34.7100 |          15.6267 |
[32m[20221213 22:50:00 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.67
[32m[20221213 22:50:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.20
[32m[20221213 22:50:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.26
[32m[20221213 22:50:00 @agent_ppo2.py:143][0m Total time:      31.78 min
[32m[20221213 22:50:00 @agent_ppo2.py:145][0m 3098624 total steps have happened
[32m[20221213 22:50:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1513 --------------------------#
[32m[20221213 22:50:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |           0.0018 |          41.7334 |          15.6014 |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |          -0.0026 |          39.9449 |          15.5851 |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |          -0.0048 |          39.0033 |          15.5603 |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |          -0.0098 |          38.5064 |          15.5779 |
[32m[20221213 22:50:00 @agent_ppo2.py:185][0m |          -0.0010 |          39.1616 |          15.5767 |
[32m[20221213 22:50:01 @agent_ppo2.py:185][0m |          -0.0089 |          37.7124 |          15.5630 |
[32m[20221213 22:50:01 @agent_ppo2.py:185][0m |          -0.0096 |          37.5534 |          15.5735 |
[32m[20221213 22:50:01 @agent_ppo2.py:185][0m |          -0.0064 |          37.7559 |          15.5682 |
[32m[20221213 22:50:01 @agent_ppo2.py:185][0m |          -0.0071 |          36.9856 |          15.5609 |
[32m[20221213 22:50:01 @agent_ppo2.py:185][0m |          -0.0069 |          38.1877 |          15.5670 |
[32m[20221213 22:50:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.14
[32m[20221213 22:50:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.57
[32m[20221213 22:50:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.45
[32m[20221213 22:50:01 @agent_ppo2.py:143][0m Total time:      31.80 min
[32m[20221213 22:50:01 @agent_ppo2.py:145][0m 3100672 total steps have happened
[32m[20221213 22:50:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1514 --------------------------#
[32m[20221213 22:50:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:01 @agent_ppo2.py:185][0m |           0.0037 |          33.6350 |          15.3916 |
[32m[20221213 22:50:01 @agent_ppo2.py:185][0m |          -0.0016 |          30.4433 |          15.3845 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0042 |          29.8753 |          15.4030 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0082 |          29.6372 |          15.4113 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0067 |          29.2503 |          15.4077 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0011 |          30.6281 |          15.4142 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0089 |          29.1755 |          15.4322 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0119 |          28.8821 |          15.4300 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0098 |          28.6893 |          15.4269 |
[32m[20221213 22:50:02 @agent_ppo2.py:185][0m |          -0.0043 |          29.1179 |          15.4303 |
[32m[20221213 22:50:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.72
[32m[20221213 22:50:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.38
[32m[20221213 22:50:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.15
[32m[20221213 22:50:02 @agent_ppo2.py:143][0m Total time:      31.82 min
[32m[20221213 22:50:02 @agent_ppo2.py:145][0m 3102720 total steps have happened
[32m[20221213 22:50:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1515 --------------------------#
[32m[20221213 22:50:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |           0.0024 |          53.6757 |          15.5919 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0050 |          49.7169 |          15.5733 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0027 |          48.7660 |          15.5755 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0025 |          48.3017 |          15.5605 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0055 |          47.7039 |          15.5794 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0084 |          47.5091 |          15.5751 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0082 |          47.2296 |          15.5783 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0130 |          46.8706 |          15.5711 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0132 |          46.6671 |          15.5788 |
[32m[20221213 22:50:03 @agent_ppo2.py:185][0m |          -0.0128 |          46.4170 |          15.5683 |
[32m[20221213 22:50:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.69
[32m[20221213 22:50:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.26
[32m[20221213 22:50:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.88
[32m[20221213 22:50:03 @agent_ppo2.py:143][0m Total time:      31.84 min
[32m[20221213 22:50:03 @agent_ppo2.py:145][0m 3104768 total steps have happened
[32m[20221213 22:50:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1516 --------------------------#
[32m[20221213 22:50:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0010 |          26.3060 |          15.8387 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0043 |          23.2653 |          15.8431 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0046 |          22.0297 |          15.8299 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0132 |          21.4325 |          15.8190 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0081 |          21.0204 |          15.8078 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0140 |          20.6281 |          15.7992 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0124 |          20.3822 |          15.7902 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0130 |          20.1151 |          15.7897 |
[32m[20221213 22:50:04 @agent_ppo2.py:185][0m |          -0.0063 |          20.7407 |          15.7761 |
[32m[20221213 22:50:05 @agent_ppo2.py:185][0m |          -0.0070 |          20.3716 |          15.7570 |
[32m[20221213 22:50:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:50:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.93
[32m[20221213 22:50:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 354.51
[32m[20221213 22:50:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.39
[32m[20221213 22:50:05 @agent_ppo2.py:143][0m Total time:      31.86 min
[32m[20221213 22:50:05 @agent_ppo2.py:145][0m 3106816 total steps have happened
[32m[20221213 22:50:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1517 --------------------------#
[32m[20221213 22:50:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:05 @agent_ppo2.py:185][0m |           0.0013 |          38.7553 |          15.5111 |
[32m[20221213 22:50:05 @agent_ppo2.py:185][0m |          -0.0048 |          36.2021 |          15.4938 |
[32m[20221213 22:50:05 @agent_ppo2.py:185][0m |          -0.0014 |          35.9711 |          15.4973 |
[32m[20221213 22:50:05 @agent_ppo2.py:185][0m |          -0.0065 |          34.8942 |          15.5003 |
[32m[20221213 22:50:05 @agent_ppo2.py:185][0m |          -0.0111 |          34.6856 |          15.4990 |
[32m[20221213 22:50:05 @agent_ppo2.py:185][0m |          -0.0101 |          34.2433 |          15.5163 |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |          -0.0101 |          34.1452 |          15.5310 |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |          -0.0106 |          33.8665 |          15.5216 |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |          -0.0119 |          33.9054 |          15.5263 |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |          -0.0115 |          33.8174 |          15.5271 |
[32m[20221213 22:50:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.60
[32m[20221213 22:50:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.66
[32m[20221213 22:50:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.49
[32m[20221213 22:50:06 @agent_ppo2.py:143][0m Total time:      31.88 min
[32m[20221213 22:50:06 @agent_ppo2.py:145][0m 3108864 total steps have happened
[32m[20221213 22:50:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1518 --------------------------#
[32m[20221213 22:50:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |           0.0059 |          45.8144 |          15.6900 |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |          -0.0050 |          40.6326 |          15.6505 |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |          -0.0019 |          40.5034 |          15.6615 |
[32m[20221213 22:50:06 @agent_ppo2.py:185][0m |          -0.0064 |          37.2880 |          15.6483 |
[32m[20221213 22:50:07 @agent_ppo2.py:185][0m |          -0.0070 |          36.8665 |          15.6344 |
[32m[20221213 22:50:07 @agent_ppo2.py:185][0m |          -0.0073 |          36.8850 |          15.6355 |
[32m[20221213 22:50:07 @agent_ppo2.py:185][0m |          -0.0140 |          35.6088 |          15.6291 |
[32m[20221213 22:50:07 @agent_ppo2.py:185][0m |          -0.0134 |          35.1116 |          15.6175 |
[32m[20221213 22:50:07 @agent_ppo2.py:185][0m |          -0.0134 |          34.5108 |          15.6185 |
[32m[20221213 22:50:07 @agent_ppo2.py:185][0m |          -0.0088 |          34.3439 |          15.6034 |
[32m[20221213 22:50:07 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.94
[32m[20221213 22:50:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.02
[32m[20221213 22:50:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 346.88
[32m[20221213 22:50:07 @agent_ppo2.py:143][0m Total time:      31.90 min
[32m[20221213 22:50:07 @agent_ppo2.py:145][0m 3110912 total steps have happened
[32m[20221213 22:50:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1519 --------------------------#
[32m[20221213 22:50:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:07 @agent_ppo2.py:185][0m |          -0.0057 |          55.3838 |          15.7365 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0046 |          51.5848 |          15.7090 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0079 |          50.6136 |          15.7125 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0087 |          50.3087 |          15.7282 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0096 |          49.9038 |          15.6921 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0089 |          49.5415 |          15.7130 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0113 |          49.3876 |          15.7007 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |           0.0012 |          53.1818 |          15.7047 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0083 |          49.3997 |          15.6702 |
[32m[20221213 22:50:08 @agent_ppo2.py:185][0m |          -0.0093 |          49.1445 |          15.6989 |
[32m[20221213 22:50:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.17
[32m[20221213 22:50:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.11
[32m[20221213 22:50:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.13
[32m[20221213 22:50:08 @agent_ppo2.py:143][0m Total time:      31.92 min
[32m[20221213 22:50:08 @agent_ppo2.py:145][0m 3112960 total steps have happened
[32m[20221213 22:50:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1520 --------------------------#
[32m[20221213 22:50:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:50:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |           0.0027 |          35.6469 |          15.4392 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0034 |          33.0232 |          15.4336 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0073 |          32.0014 |          15.4340 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0077 |          31.4865 |          15.4297 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0080 |          31.2236 |          15.4300 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0131 |          30.8048 |          15.4172 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0077 |          30.7993 |          15.4142 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0097 |          30.3739 |          15.4143 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0117 |          30.3020 |          15.4023 |
[32m[20221213 22:50:09 @agent_ppo2.py:185][0m |          -0.0113 |          29.9751 |          15.4134 |
[32m[20221213 22:50:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.14
[32m[20221213 22:50:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 362.01
[32m[20221213 22:50:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.55
[32m[20221213 22:50:10 @agent_ppo2.py:143][0m Total time:      31.94 min
[32m[20221213 22:50:10 @agent_ppo2.py:145][0m 3115008 total steps have happened
[32m[20221213 22:50:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1521 --------------------------#
[32m[20221213 22:50:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |          -0.0021 |          36.3090 |          15.5775 |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |          -0.0071 |          34.7296 |          15.5781 |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |          -0.0122 |          34.3053 |          15.5705 |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |          -0.0084 |          34.1624 |          15.5623 |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |          -0.0009 |          36.2599 |          15.5691 |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |          -0.0114 |          33.7762 |          15.5488 |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |           0.0015 |          36.6686 |          15.5771 |
[32m[20221213 22:50:10 @agent_ppo2.py:185][0m |           0.0023 |          37.0537 |          15.5619 |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |          -0.0095 |          33.4705 |          15.5440 |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |          -0.0102 |          33.3306 |          15.5673 |
[32m[20221213 22:50:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.39
[32m[20221213 22:50:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.65
[32m[20221213 22:50:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.27
[32m[20221213 22:50:11 @agent_ppo2.py:143][0m Total time:      31.96 min
[32m[20221213 22:50:11 @agent_ppo2.py:145][0m 3117056 total steps have happened
[32m[20221213 22:50:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1522 --------------------------#
[32m[20221213 22:50:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |           0.0136 |          51.9613 |          15.5094 |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |          -0.0028 |          42.0306 |          15.4800 |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |          -0.0081 |          41.2235 |          15.4928 |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |          -0.0090 |          40.9231 |          15.4986 |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |          -0.0074 |          40.6936 |          15.4942 |
[32m[20221213 22:50:11 @agent_ppo2.py:185][0m |          -0.0016 |          41.9176 |          15.4733 |
[32m[20221213 22:50:12 @agent_ppo2.py:185][0m |          -0.0141 |          40.6482 |          15.4650 |
[32m[20221213 22:50:12 @agent_ppo2.py:185][0m |          -0.0087 |          40.3579 |          15.4787 |
[32m[20221213 22:50:12 @agent_ppo2.py:185][0m |          -0.0092 |          40.2609 |          15.4900 |
[32m[20221213 22:50:12 @agent_ppo2.py:185][0m |          -0.0127 |          40.1801 |          15.4836 |
[32m[20221213 22:50:12 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.60
[32m[20221213 22:50:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.13
[32m[20221213 22:50:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.39
[32m[20221213 22:50:12 @agent_ppo2.py:143][0m Total time:      31.98 min
[32m[20221213 22:50:12 @agent_ppo2.py:145][0m 3119104 total steps have happened
[32m[20221213 22:50:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1523 --------------------------#
[32m[20221213 22:50:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:12 @agent_ppo2.py:185][0m |          -0.0024 |          33.5718 |          15.5062 |
[32m[20221213 22:50:12 @agent_ppo2.py:185][0m |          -0.0053 |          31.5159 |          15.4964 |
[32m[20221213 22:50:12 @agent_ppo2.py:185][0m |          -0.0049 |          30.4717 |          15.5257 |
[32m[20221213 22:50:13 @agent_ppo2.py:185][0m |          -0.0038 |          30.0647 |          15.5134 |
[32m[20221213 22:50:13 @agent_ppo2.py:185][0m |          -0.0108 |          29.6064 |          15.5020 |
[32m[20221213 22:50:13 @agent_ppo2.py:185][0m |          -0.0091 |          29.2837 |          15.5094 |
[32m[20221213 22:50:13 @agent_ppo2.py:185][0m |          -0.0062 |          28.9140 |          15.4954 |
[32m[20221213 22:50:13 @agent_ppo2.py:185][0m |          -0.0131 |          28.5935 |          15.5106 |
[32m[20221213 22:50:13 @agent_ppo2.py:185][0m |          -0.0091 |          28.4320 |          15.5056 |
[32m[20221213 22:50:13 @agent_ppo2.py:185][0m |          -0.0102 |          28.3957 |          15.5040 |
[32m[20221213 22:50:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.90
[32m[20221213 22:50:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 362.74
[32m[20221213 22:50:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.86
[32m[20221213 22:50:13 @agent_ppo2.py:143][0m Total time:      32.00 min
[32m[20221213 22:50:13 @agent_ppo2.py:145][0m 3121152 total steps have happened
[32m[20221213 22:50:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1524 --------------------------#
[32m[20221213 22:50:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0012 |          33.8873 |          15.5547 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0003 |          31.2008 |          15.5250 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0023 |          30.5602 |          15.5430 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0063 |          30.0894 |          15.5419 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0087 |          29.8096 |          15.5479 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0109 |          29.9013 |          15.5356 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0122 |          29.3293 |          15.5411 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0056 |          29.5241 |          15.5511 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0095 |          28.8853 |          15.5508 |
[32m[20221213 22:50:14 @agent_ppo2.py:185][0m |          -0.0124 |          28.7261 |          15.5412 |
[32m[20221213 22:50:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.58
[32m[20221213 22:50:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.04
[32m[20221213 22:50:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.18
[32m[20221213 22:50:14 @agent_ppo2.py:143][0m Total time:      32.02 min
[32m[20221213 22:50:14 @agent_ppo2.py:145][0m 3123200 total steps have happened
[32m[20221213 22:50:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1525 --------------------------#
[32m[20221213 22:50:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |           0.0026 |          48.5102 |          15.6659 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |           0.0064 |          49.9945 |          15.6551 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |          -0.0076 |          45.7496 |          15.6709 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |           0.0006 |          47.2393 |          15.6477 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |          -0.0080 |          45.0516 |          15.6466 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |          -0.0085 |          44.8858 |          15.6584 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |          -0.0085 |          44.9999 |          15.6591 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |          -0.0007 |          46.5718 |          15.6292 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |          -0.0050 |          45.1334 |          15.5955 |
[32m[20221213 22:50:15 @agent_ppo2.py:185][0m |          -0.0102 |          44.3975 |          15.6465 |
[32m[20221213 22:50:15 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.29
[32m[20221213 22:50:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.18
[32m[20221213 22:50:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.45
[32m[20221213 22:50:16 @agent_ppo2.py:143][0m Total time:      32.04 min
[32m[20221213 22:50:16 @agent_ppo2.py:145][0m 3125248 total steps have happened
[32m[20221213 22:50:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1526 --------------------------#
[32m[20221213 22:50:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:16 @agent_ppo2.py:185][0m |          -0.0015 |          44.3516 |          15.5616 |
[32m[20221213 22:50:16 @agent_ppo2.py:185][0m |           0.0068 |          46.6679 |          15.5266 |
[32m[20221213 22:50:16 @agent_ppo2.py:185][0m |          -0.0056 |          42.8454 |          15.5106 |
[32m[20221213 22:50:16 @agent_ppo2.py:185][0m |          -0.0094 |          42.6804 |          15.4993 |
[32m[20221213 22:50:16 @agent_ppo2.py:185][0m |          -0.0068 |          42.5322 |          15.5018 |
[32m[20221213 22:50:16 @agent_ppo2.py:185][0m |          -0.0021 |          42.6980 |          15.4848 |
[32m[20221213 22:50:16 @agent_ppo2.py:185][0m |          -0.0118 |          42.3670 |          15.4937 |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |          -0.0085 |          42.1784 |          15.4806 |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |           0.0026 |          48.2223 |          15.4656 |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |          -0.0051 |          42.3794 |          15.4475 |
[32m[20221213 22:50:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:50:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.48
[32m[20221213 22:50:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.54
[32m[20221213 22:50:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.08
[32m[20221213 22:50:17 @agent_ppo2.py:143][0m Total time:      32.07 min
[32m[20221213 22:50:17 @agent_ppo2.py:145][0m 3127296 total steps have happened
[32m[20221213 22:50:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1527 --------------------------#
[32m[20221213 22:50:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |          -0.0044 |          39.2769 |          15.6102 |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |           0.0001 |          36.2783 |          15.5962 |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |          -0.0068 |          35.1721 |          15.5899 |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |          -0.0003 |          35.4636 |          15.5995 |
[32m[20221213 22:50:17 @agent_ppo2.py:185][0m |          -0.0131 |          34.3324 |          15.6052 |
[32m[20221213 22:50:18 @agent_ppo2.py:185][0m |          -0.0133 |          33.7275 |          15.6046 |
[32m[20221213 22:50:18 @agent_ppo2.py:185][0m |          -0.0108 |          33.5138 |          15.6029 |
[32m[20221213 22:50:18 @agent_ppo2.py:185][0m |          -0.0081 |          33.6855 |          15.6058 |
[32m[20221213 22:50:18 @agent_ppo2.py:185][0m |          -0.0124 |          33.0883 |          15.5949 |
[32m[20221213 22:50:18 @agent_ppo2.py:185][0m |          -0.0123 |          32.8564 |          15.6009 |
[32m[20221213 22:50:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.12
[32m[20221213 22:50:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.74
[32m[20221213 22:50:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.94
[32m[20221213 22:50:18 @agent_ppo2.py:143][0m Total time:      32.09 min
[32m[20221213 22:50:18 @agent_ppo2.py:145][0m 3129344 total steps have happened
[32m[20221213 22:50:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1528 --------------------------#
[32m[20221213 22:50:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:18 @agent_ppo2.py:185][0m |           0.0055 |          50.4468 |          15.7220 |
[32m[20221213 22:50:18 @agent_ppo2.py:185][0m |           0.0006 |          46.9429 |          15.7126 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |          -0.0082 |          45.8091 |          15.7204 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |          -0.0006 |          45.6488 |          15.7477 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |          -0.0028 |          45.4779 |          15.7450 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |          -0.0079 |          43.7620 |          15.7266 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |          -0.0082 |          43.5102 |          15.7552 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |          -0.0074 |          43.1706 |          15.7456 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |           0.0011 |          46.2421 |          15.7556 |
[32m[20221213 22:50:19 @agent_ppo2.py:185][0m |          -0.0073 |          42.7477 |          15.7407 |
[32m[20221213 22:50:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.23
[32m[20221213 22:50:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.91
[32m[20221213 22:50:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.13
[32m[20221213 22:50:19 @agent_ppo2.py:143][0m Total time:      32.11 min
[32m[20221213 22:50:19 @agent_ppo2.py:145][0m 3131392 total steps have happened
[32m[20221213 22:50:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1529 --------------------------#
[32m[20221213 22:50:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0007 |          43.0524 |          15.6003 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0062 |          40.1964 |          15.5807 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0031 |          38.8647 |          15.5740 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0023 |          38.6018 |          15.5730 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0053 |          38.0522 |          15.5814 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0055 |          37.9171 |          15.5693 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0092 |          37.6130 |          15.5705 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0075 |          37.3174 |          15.5666 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |           0.0048 |          40.0077 |          15.5523 |
[32m[20221213 22:50:20 @agent_ppo2.py:185][0m |          -0.0088 |          37.0877 |          15.5558 |
[32m[20221213 22:50:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.81
[32m[20221213 22:50:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.91
[32m[20221213 22:50:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.69
[32m[20221213 22:50:20 @agent_ppo2.py:143][0m Total time:      32.13 min
[32m[20221213 22:50:20 @agent_ppo2.py:145][0m 3133440 total steps have happened
[32m[20221213 22:50:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1530 --------------------------#
[32m[20221213 22:50:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:50:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0002 |          44.1576 |          15.5619 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |           0.0111 |          39.6805 |          15.5579 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0043 |          34.0573 |          15.5490 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0043 |          31.9205 |          15.5446 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0017 |          31.5550 |          15.5367 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0043 |          29.6397 |          15.5309 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0128 |          29.0929 |          15.5090 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0157 |          28.3822 |          15.5345 |
[32m[20221213 22:50:21 @agent_ppo2.py:185][0m |          -0.0126 |          27.9017 |          15.5129 |
[32m[20221213 22:50:22 @agent_ppo2.py:185][0m |          -0.0049 |          30.0195 |          15.5092 |
[32m[20221213 22:50:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:50:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.73
[32m[20221213 22:50:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.13
[32m[20221213 22:50:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.95
[32m[20221213 22:50:22 @agent_ppo2.py:143][0m Total time:      32.15 min
[32m[20221213 22:50:22 @agent_ppo2.py:145][0m 3135488 total steps have happened
[32m[20221213 22:50:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1531 --------------------------#
[32m[20221213 22:50:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:22 @agent_ppo2.py:185][0m |          -0.0018 |          37.3216 |          15.6835 |
[32m[20221213 22:50:22 @agent_ppo2.py:185][0m |          -0.0044 |          33.3129 |          15.6858 |
[32m[20221213 22:50:22 @agent_ppo2.py:185][0m |          -0.0113 |          32.0511 |          15.6850 |
[32m[20221213 22:50:22 @agent_ppo2.py:185][0m |          -0.0031 |          31.0756 |          15.6578 |
[32m[20221213 22:50:22 @agent_ppo2.py:185][0m |          -0.0090 |          30.5722 |          15.6492 |
[32m[20221213 22:50:22 @agent_ppo2.py:185][0m |          -0.0059 |          30.1989 |          15.6579 |
[32m[20221213 22:50:23 @agent_ppo2.py:185][0m |          -0.0087 |          29.8434 |          15.6598 |
[32m[20221213 22:50:23 @agent_ppo2.py:185][0m |          -0.0105 |          29.6018 |          15.6525 |
[32m[20221213 22:50:23 @agent_ppo2.py:185][0m |          -0.0102 |          29.3445 |          15.6608 |
[32m[20221213 22:50:23 @agent_ppo2.py:185][0m |          -0.0079 |          29.0933 |          15.6443 |
[32m[20221213 22:50:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:50:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.47
[32m[20221213 22:50:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.71
[32m[20221213 22:50:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:50:23 @agent_ppo2.py:143][0m Total time:      32.17 min
[32m[20221213 22:50:23 @agent_ppo2.py:145][0m 3137536 total steps have happened
[32m[20221213 22:50:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1532 --------------------------#
[32m[20221213 22:50:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:50:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:23 @agent_ppo2.py:185][0m |           0.0002 |          42.3219 |          15.6200 |
[32m[20221213 22:50:23 @agent_ppo2.py:185][0m |          -0.0037 |          39.9936 |          15.6351 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |          -0.0055 |          39.3195 |          15.6304 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |          -0.0099 |          39.0014 |          15.6228 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |           0.0033 |          41.6679 |          15.6336 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |          -0.0107 |          38.6226 |          15.6450 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |          -0.0089 |          38.3823 |          15.6331 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |           0.0050 |          42.0288 |          15.6494 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |          -0.0108 |          38.2123 |          15.6548 |
[32m[20221213 22:50:24 @agent_ppo2.py:185][0m |          -0.0034 |          38.3938 |          15.6561 |
[32m[20221213 22:50:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:50:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.55
[32m[20221213 22:50:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.34
[32m[20221213 22:50:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.80
[32m[20221213 22:50:24 @agent_ppo2.py:143][0m Total time:      32.19 min
[32m[20221213 22:50:24 @agent_ppo2.py:145][0m 3139584 total steps have happened
[32m[20221213 22:50:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1533 --------------------------#
[32m[20221213 22:50:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0030 |          37.8727 |          15.4044 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0056 |          34.3152 |          15.3809 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0110 |          32.7081 |          15.3660 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0109 |          31.9259 |          15.3687 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0149 |          30.9574 |          15.3707 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0081 |          30.2381 |          15.3628 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0022 |          30.8544 |          15.3613 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0139 |          29.3723 |          15.3551 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0157 |          29.0889 |          15.3691 |
[32m[20221213 22:50:25 @agent_ppo2.py:185][0m |          -0.0113 |          28.9251 |          15.3523 |
[32m[20221213 22:50:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:50:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.89
[32m[20221213 22:50:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.05
[32m[20221213 22:50:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.40
[32m[20221213 22:50:26 @agent_ppo2.py:143][0m Total time:      32.21 min
[32m[20221213 22:50:26 @agent_ppo2.py:145][0m 3141632 total steps have happened
[32m[20221213 22:50:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1534 --------------------------#
[32m[20221213 22:50:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |           0.0011 |          38.3537 |          15.5937 |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |          -0.0071 |          36.0771 |          15.5735 |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |          -0.0089 |          35.4341 |          15.5861 |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |          -0.0015 |          35.7015 |          15.5888 |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |          -0.0072 |          34.7697 |          15.5701 |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |          -0.0111 |          34.6156 |          15.5605 |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |           0.0017 |          37.1144 |          15.5865 |
[32m[20221213 22:50:26 @agent_ppo2.py:185][0m |          -0.0061 |          34.8350 |          15.5618 |
[32m[20221213 22:50:27 @agent_ppo2.py:185][0m |          -0.0093 |          34.7321 |          15.5979 |
[32m[20221213 22:50:27 @agent_ppo2.py:185][0m |          -0.0104 |          34.3666 |          15.5374 |
[32m[20221213 22:50:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:50:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.05
[32m[20221213 22:50:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.39
[32m[20221213 22:50:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.67
[32m[20221213 22:50:27 @agent_ppo2.py:143][0m Total time:      32.23 min
[32m[20221213 22:50:27 @agent_ppo2.py:145][0m 3143680 total steps have happened
[32m[20221213 22:50:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1535 --------------------------#
[32m[20221213 22:50:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:27 @agent_ppo2.py:185][0m |           0.0032 |          52.8660 |          15.5468 |
[32m[20221213 22:50:27 @agent_ppo2.py:185][0m |           0.0001 |          51.0607 |          15.5428 |
[32m[20221213 22:50:27 @agent_ppo2.py:185][0m |           0.0078 |          55.4365 |          15.5166 |
[32m[20221213 22:50:27 @agent_ppo2.py:185][0m |          -0.0055 |          50.3838 |          15.5087 |
[32m[20221213 22:50:27 @agent_ppo2.py:185][0m |          -0.0086 |          49.9971 |          15.5010 |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0065 |          49.7004 |          15.4865 |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0096 |          49.6034 |          15.4687 |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0105 |          49.6273 |          15.4799 |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0078 |          49.4419 |          15.4641 |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0066 |          49.8534 |          15.4593 |
[32m[20221213 22:50:28 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.63
[32m[20221213 22:50:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.30
[32m[20221213 22:50:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.28
[32m[20221213 22:50:28 @agent_ppo2.py:143][0m Total time:      32.25 min
[32m[20221213 22:50:28 @agent_ppo2.py:145][0m 3145728 total steps have happened
[32m[20221213 22:50:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1536 --------------------------#
[32m[20221213 22:50:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0014 |          40.2930 |          15.6594 |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0103 |          37.9619 |          15.6541 |
[32m[20221213 22:50:28 @agent_ppo2.py:185][0m |          -0.0093 |          36.9806 |          15.6499 |
[32m[20221213 22:50:29 @agent_ppo2.py:185][0m |          -0.0076 |          36.4524 |          15.6462 |
[32m[20221213 22:50:29 @agent_ppo2.py:185][0m |          -0.0018 |          37.9231 |          15.6282 |
[32m[20221213 22:50:29 @agent_ppo2.py:185][0m |          -0.0073 |          36.0125 |          15.6384 |
[32m[20221213 22:50:29 @agent_ppo2.py:185][0m |          -0.0081 |          35.5634 |          15.6289 |
[32m[20221213 22:50:29 @agent_ppo2.py:185][0m |          -0.0136 |          35.2210 |          15.6231 |
[32m[20221213 22:50:29 @agent_ppo2.py:185][0m |          -0.0118 |          35.0131 |          15.6295 |
[32m[20221213 22:50:29 @agent_ppo2.py:185][0m |          -0.0154 |          34.9036 |          15.6368 |
[32m[20221213 22:50:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:50:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 307.10
[32m[20221213 22:50:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.49
[32m[20221213 22:50:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.79
[32m[20221213 22:50:29 @agent_ppo2.py:143][0m Total time:      32.27 min
[32m[20221213 22:50:29 @agent_ppo2.py:145][0m 3147776 total steps have happened
[32m[20221213 22:50:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1537 --------------------------#
[32m[20221213 22:50:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0001 |          37.0075 |          15.5117 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0041 |          35.1704 |          15.4857 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0070 |          34.2168 |          15.4942 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0058 |          33.6347 |          15.4994 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0045 |          33.2297 |          15.4682 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0085 |          32.8480 |          15.4968 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0093 |          32.4423 |          15.4787 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0121 |          32.1987 |          15.4972 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0083 |          32.1598 |          15.4831 |
[32m[20221213 22:50:30 @agent_ppo2.py:185][0m |          -0.0152 |          31.9138 |          15.4889 |
[32m[20221213 22:50:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.54
[32m[20221213 22:50:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.55
[32m[20221213 22:50:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.12
[32m[20221213 22:50:30 @agent_ppo2.py:143][0m Total time:      32.29 min
[32m[20221213 22:50:30 @agent_ppo2.py:145][0m 3149824 total steps have happened
[32m[20221213 22:50:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1538 --------------------------#
[32m[20221213 22:50:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |           0.0022 |          36.3674 |          15.3840 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |          -0.0029 |          34.9258 |          15.3573 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |          -0.0061 |          34.4126 |          15.3188 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |           0.0021 |          34.9176 |          15.3370 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |          -0.0064 |          33.9690 |          15.3222 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |          -0.0054 |          33.8512 |          15.2940 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |           0.0040 |          36.5150 |          15.3005 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |          -0.0033 |          33.5809 |          15.2924 |
[32m[20221213 22:50:31 @agent_ppo2.py:185][0m |          -0.0087 |          33.2436 |          15.2845 |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0099 |          33.2052 |          15.2736 |
[32m[20221213 22:50:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.56
[32m[20221213 22:50:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.36
[32m[20221213 22:50:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.40
[32m[20221213 22:50:32 @agent_ppo2.py:143][0m Total time:      32.31 min
[32m[20221213 22:50:32 @agent_ppo2.py:145][0m 3151872 total steps have happened
[32m[20221213 22:50:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1539 --------------------------#
[32m[20221213 22:50:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0015 |          33.7478 |          15.3669 |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0085 |          31.5619 |          15.3666 |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0067 |          30.7881 |          15.3759 |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0071 |          30.3936 |          15.3726 |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0120 |          30.0027 |          15.3759 |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0103 |          29.5410 |          15.3820 |
[32m[20221213 22:50:32 @agent_ppo2.py:185][0m |          -0.0068 |          30.5804 |          15.3701 |
[32m[20221213 22:50:33 @agent_ppo2.py:185][0m |          -0.0089 |          29.1339 |          15.3986 |
[32m[20221213 22:50:33 @agent_ppo2.py:185][0m |          -0.0087 |          29.1687 |          15.3963 |
[32m[20221213 22:50:33 @agent_ppo2.py:185][0m |          -0.0159 |          28.7871 |          15.3970 |
[32m[20221213 22:50:33 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.28
[32m[20221213 22:50:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.49
[32m[20221213 22:50:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.19
[32m[20221213 22:50:33 @agent_ppo2.py:143][0m Total time:      32.33 min
[32m[20221213 22:50:33 @agent_ppo2.py:145][0m 3153920 total steps have happened
[32m[20221213 22:50:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1540 --------------------------#
[32m[20221213 22:50:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:50:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:33 @agent_ppo2.py:185][0m |           0.0004 |          42.6869 |          15.4547 |
[32m[20221213 22:50:33 @agent_ppo2.py:185][0m |          -0.0040 |          39.0249 |          15.4257 |
[32m[20221213 22:50:33 @agent_ppo2.py:185][0m |          -0.0035 |          38.0131 |          15.4234 |
[32m[20221213 22:50:33 @agent_ppo2.py:185][0m |          -0.0032 |          38.8830 |          15.4227 |
[32m[20221213 22:50:34 @agent_ppo2.py:185][0m |          -0.0091 |          37.0212 |          15.4320 |
[32m[20221213 22:50:34 @agent_ppo2.py:185][0m |          -0.0086 |          36.6735 |          15.4123 |
[32m[20221213 22:50:34 @agent_ppo2.py:185][0m |          -0.0111 |          36.2973 |          15.3985 |
[32m[20221213 22:50:34 @agent_ppo2.py:185][0m |          -0.0100 |          36.0120 |          15.3949 |
[32m[20221213 22:50:34 @agent_ppo2.py:185][0m |          -0.0116 |          36.0271 |          15.3951 |
[32m[20221213 22:50:34 @agent_ppo2.py:185][0m |          -0.0138 |          35.7140 |          15.3944 |
[32m[20221213 22:50:34 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.56
[32m[20221213 22:50:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.30
[32m[20221213 22:50:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.76
[32m[20221213 22:50:34 @agent_ppo2.py:143][0m Total time:      32.35 min
[32m[20221213 22:50:34 @agent_ppo2.py:145][0m 3155968 total steps have happened
[32m[20221213 22:50:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1541 --------------------------#
[32m[20221213 22:50:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:34 @agent_ppo2.py:185][0m |           0.0126 |          49.5121 |          15.5581 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |           0.0092 |          47.2130 |          15.5354 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0083 |          42.2470 |          15.5282 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0135 |          41.9194 |          15.5441 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0131 |          41.8684 |          15.5138 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0079 |          41.5131 |          15.5149 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0070 |          41.4933 |          15.5075 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0087 |          41.2165 |          15.5033 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0098 |          41.1746 |          15.4917 |
[32m[20221213 22:50:35 @agent_ppo2.py:185][0m |          -0.0146 |          41.2082 |          15.4994 |
[32m[20221213 22:50:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:50:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.00
[32m[20221213 22:50:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.87
[32m[20221213 22:50:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.52
[32m[20221213 22:50:35 @agent_ppo2.py:143][0m Total time:      32.37 min
[32m[20221213 22:50:35 @agent_ppo2.py:145][0m 3158016 total steps have happened
[32m[20221213 22:50:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1542 --------------------------#
[32m[20221213 22:50:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |           0.0028 |          42.2759 |          15.4405 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |          -0.0065 |          39.8565 |          15.4320 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |          -0.0004 |          39.0802 |          15.4144 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |           0.0026 |          39.2580 |          15.4166 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |           0.0014 |          38.7299 |          15.4075 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |          -0.0018 |          38.1513 |          15.4278 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |          -0.0084 |          37.3877 |          15.4128 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |          -0.0134 |          37.1386 |          15.4154 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |          -0.0110 |          36.8349 |          15.4359 |
[32m[20221213 22:50:36 @agent_ppo2.py:185][0m |          -0.0118 |          36.6903 |          15.4185 |
[32m[20221213 22:50:36 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.24
[32m[20221213 22:50:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.98
[32m[20221213 22:50:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.82
[32m[20221213 22:50:37 @agent_ppo2.py:143][0m Total time:      32.39 min
[32m[20221213 22:50:37 @agent_ppo2.py:145][0m 3160064 total steps have happened
[32m[20221213 22:50:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1543 --------------------------#
[32m[20221213 22:50:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |          -0.0027 |          40.4414 |          15.4100 |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |           0.0039 |          42.6956 |          15.4043 |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |          -0.0092 |          38.6401 |          15.3911 |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |          -0.0055 |          38.7234 |          15.3944 |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |          -0.0098 |          38.1329 |          15.3970 |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |          -0.0111 |          37.8942 |          15.3900 |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |          -0.0086 |          37.7916 |          15.3759 |
[32m[20221213 22:50:37 @agent_ppo2.py:185][0m |          -0.0126 |          37.6813 |          15.3738 |
[32m[20221213 22:50:38 @agent_ppo2.py:185][0m |          -0.0127 |          37.9560 |          15.3661 |
[32m[20221213 22:50:38 @agent_ppo2.py:185][0m |          -0.0141 |          37.5467 |          15.3585 |
[32m[20221213 22:50:38 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.73
[32m[20221213 22:50:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 356.09
[32m[20221213 22:50:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.13
[32m[20221213 22:50:38 @agent_ppo2.py:143][0m Total time:      32.41 min
[32m[20221213 22:50:38 @agent_ppo2.py:145][0m 3162112 total steps have happened
[32m[20221213 22:50:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1544 --------------------------#
[32m[20221213 22:50:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:38 @agent_ppo2.py:185][0m |           0.0077 |          46.8514 |          15.4237 |
[32m[20221213 22:50:38 @agent_ppo2.py:185][0m |          -0.0034 |          41.8437 |          15.3945 |
[32m[20221213 22:50:38 @agent_ppo2.py:185][0m |          -0.0079 |          41.1765 |          15.4013 |
[32m[20221213 22:50:38 @agent_ppo2.py:185][0m |          -0.0028 |          40.8454 |          15.4020 |
[32m[20221213 22:50:38 @agent_ppo2.py:185][0m |          -0.0069 |          40.2265 |          15.4033 |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |          -0.0088 |          39.9245 |          15.4033 |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |          -0.0097 |          39.7250 |          15.4088 |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |          -0.0114 |          39.4800 |          15.4097 |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |          -0.0092 |          39.2992 |          15.4172 |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |          -0.0085 |          39.1127 |          15.4334 |
[32m[20221213 22:50:39 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.21
[32m[20221213 22:50:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.40
[32m[20221213 22:50:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.14
[32m[20221213 22:50:39 @agent_ppo2.py:143][0m Total time:      32.43 min
[32m[20221213 22:50:39 @agent_ppo2.py:145][0m 3164160 total steps have happened
[32m[20221213 22:50:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1545 --------------------------#
[32m[20221213 22:50:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |          -0.0002 |          47.5838 |          15.4767 |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |           0.0048 |          49.4527 |          15.4555 |
[32m[20221213 22:50:39 @agent_ppo2.py:185][0m |           0.0036 |          45.6967 |          15.4472 |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |          -0.0050 |          42.5693 |          15.4177 |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |          -0.0096 |          42.1668 |          15.4196 |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |          -0.0094 |          41.8532 |          15.4236 |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |          -0.0097 |          41.6873 |          15.3951 |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |           0.0060 |          48.2090 |          15.4058 |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |          -0.0079 |          41.6317 |          15.3863 |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |          -0.0124 |          41.2378 |          15.3888 |
[32m[20221213 22:50:40 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.98
[32m[20221213 22:50:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.73
[32m[20221213 22:50:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.24
[32m[20221213 22:50:40 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 501.24
[32m[20221213 22:50:40 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 501.24
[32m[20221213 22:50:40 @agent_ppo2.py:143][0m Total time:      32.45 min
[32m[20221213 22:50:40 @agent_ppo2.py:145][0m 3166208 total steps have happened
[32m[20221213 22:50:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1546 --------------------------#
[32m[20221213 22:50:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:40 @agent_ppo2.py:185][0m |           0.0011 |          44.3818 |          15.4229 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0027 |          42.2215 |          15.4393 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0020 |          41.0971 |          15.4297 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0078 |          39.8204 |          15.4384 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0084 |          39.0531 |          15.4219 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0085 |          38.6000 |          15.4380 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0066 |          38.2435 |          15.4292 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0073 |          38.2707 |          15.4545 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0054 |          37.9298 |          15.4583 |
[32m[20221213 22:50:41 @agent_ppo2.py:185][0m |          -0.0075 |          37.6597 |          15.4511 |
[32m[20221213 22:50:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.27
[32m[20221213 22:50:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.84
[32m[20221213 22:50:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.98
[32m[20221213 22:50:41 @agent_ppo2.py:143][0m Total time:      32.48 min
[32m[20221213 22:50:41 @agent_ppo2.py:145][0m 3168256 total steps have happened
[32m[20221213 22:50:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1547 --------------------------#
[32m[20221213 22:50:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |           0.0001 |          36.7522 |          15.3772 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0030 |          34.1665 |          15.3719 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0037 |          34.9062 |          15.3866 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0054 |          33.1547 |          15.3715 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |           0.0018 |          33.6283 |          15.3857 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0021 |          33.3319 |          15.3940 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0068 |          32.1431 |          15.3897 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0116 |          31.6953 |          15.3896 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0067 |          31.8250 |          15.3929 |
[32m[20221213 22:50:42 @agent_ppo2.py:185][0m |          -0.0037 |          31.7382 |          15.3894 |
[32m[20221213 22:50:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.70
[32m[20221213 22:50:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.48
[32m[20221213 22:50:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.53
[32m[20221213 22:50:43 @agent_ppo2.py:143][0m Total time:      32.50 min
[32m[20221213 22:50:43 @agent_ppo2.py:145][0m 3170304 total steps have happened
[32m[20221213 22:50:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1548 --------------------------#
[32m[20221213 22:50:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:43 @agent_ppo2.py:185][0m |          -0.0024 |          46.1647 |          15.2455 |
[32m[20221213 22:50:43 @agent_ppo2.py:185][0m |          -0.0052 |          44.6442 |          15.2399 |
[32m[20221213 22:50:43 @agent_ppo2.py:185][0m |          -0.0131 |          44.7401 |          15.2413 |
[32m[20221213 22:50:43 @agent_ppo2.py:185][0m |          -0.0096 |          44.2555 |          15.2371 |
[32m[20221213 22:50:43 @agent_ppo2.py:185][0m |          -0.0108 |          44.1119 |          15.2509 |
[32m[20221213 22:50:43 @agent_ppo2.py:185][0m |          -0.0004 |          45.5535 |          15.2561 |
[32m[20221213 22:50:43 @agent_ppo2.py:185][0m |          -0.0048 |          44.4066 |          15.2522 |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0104 |          43.9362 |          15.2565 |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0111 |          43.8114 |          15.2465 |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0114 |          43.9125 |          15.2467 |
[32m[20221213 22:50:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.60
[32m[20221213 22:50:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.11
[32m[20221213 22:50:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.40
[32m[20221213 22:50:44 @agent_ppo2.py:143][0m Total time:      32.52 min
[32m[20221213 22:50:44 @agent_ppo2.py:145][0m 3172352 total steps have happened
[32m[20221213 22:50:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1549 --------------------------#
[32m[20221213 22:50:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0011 |          29.3615 |          15.3950 |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0054 |          26.7539 |          15.3642 |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0016 |          26.0087 |          15.3382 |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0038 |          25.5346 |          15.3251 |
[32m[20221213 22:50:44 @agent_ppo2.py:185][0m |          -0.0097 |          25.2334 |          15.3283 |
[32m[20221213 22:50:45 @agent_ppo2.py:185][0m |          -0.0110 |          24.9906 |          15.3158 |
[32m[20221213 22:50:45 @agent_ppo2.py:185][0m |          -0.0071 |          24.7733 |          15.3002 |
[32m[20221213 22:50:45 @agent_ppo2.py:185][0m |          -0.0078 |          24.5921 |          15.2932 |
[32m[20221213 22:50:45 @agent_ppo2.py:185][0m |          -0.0130 |          24.5279 |          15.2777 |
[32m[20221213 22:50:45 @agent_ppo2.py:185][0m |          -0.0054 |          25.5037 |          15.2645 |
[32m[20221213 22:50:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:50:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.88
[32m[20221213 22:50:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.07
[32m[20221213 22:50:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.50
[32m[20221213 22:50:45 @agent_ppo2.py:143][0m Total time:      32.54 min
[32m[20221213 22:50:45 @agent_ppo2.py:145][0m 3174400 total steps have happened
[32m[20221213 22:50:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1550 --------------------------#
[32m[20221213 22:50:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:50:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:45 @agent_ppo2.py:185][0m |           0.0059 |          37.1298 |          15.3568 |
[32m[20221213 22:50:45 @agent_ppo2.py:185][0m |          -0.0074 |          34.2191 |          15.3577 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0071 |          34.6463 |          15.3462 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0116 |          33.4022 |          15.3485 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0095 |          33.2512 |          15.3449 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0089 |          32.7855 |          15.3472 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0067 |          33.7259 |          15.3436 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0191 |          32.6816 |          15.3356 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0144 |          32.4080 |          15.3347 |
[32m[20221213 22:50:46 @agent_ppo2.py:185][0m |          -0.0070 |          32.7389 |          15.3445 |
[32m[20221213 22:50:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.46
[32m[20221213 22:50:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.72
[32m[20221213 22:50:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.68
[32m[20221213 22:50:46 @agent_ppo2.py:143][0m Total time:      32.56 min
[32m[20221213 22:50:46 @agent_ppo2.py:145][0m 3176448 total steps have happened
[32m[20221213 22:50:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1551 --------------------------#
[32m[20221213 22:50:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0035 |          30.8293 |          15.3152 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0051 |          28.9334 |          15.3275 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0064 |          28.4835 |          15.2925 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0091 |          28.0063 |          15.3066 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0106 |          27.7926 |          15.3131 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |           0.0029 |          29.4098 |          15.2992 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0058 |          27.9212 |          15.3051 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0072 |          27.3875 |          15.3051 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0115 |          27.2413 |          15.3179 |
[32m[20221213 22:50:47 @agent_ppo2.py:185][0m |          -0.0111 |          27.1136 |          15.2889 |
[32m[20221213 22:50:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:50:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.09
[32m[20221213 22:50:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.90
[32m[20221213 22:50:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.82
[32m[20221213 22:50:47 @agent_ppo2.py:143][0m Total time:      32.58 min
[32m[20221213 22:50:47 @agent_ppo2.py:145][0m 3178496 total steps have happened
[32m[20221213 22:50:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1552 --------------------------#
[32m[20221213 22:50:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |           0.0082 |          48.1543 |          15.4146 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |          -0.0009 |          44.6023 |          15.4152 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |           0.0006 |          44.2853 |          15.4204 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |           0.0136 |          53.1553 |          15.4426 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |          -0.0077 |          42.7081 |          15.4335 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |          -0.0051 |          42.0074 |          15.4690 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |          -0.0104 |          41.8806 |          15.4557 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |          -0.0093 |          41.5193 |          15.4697 |
[32m[20221213 22:50:48 @agent_ppo2.py:185][0m |          -0.0119 |          41.4673 |          15.4797 |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |          -0.0101 |          41.5664 |          15.4705 |
[32m[20221213 22:50:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.69
[32m[20221213 22:50:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.63
[32m[20221213 22:50:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.92
[32m[20221213 22:50:49 @agent_ppo2.py:143][0m Total time:      32.60 min
[32m[20221213 22:50:49 @agent_ppo2.py:145][0m 3180544 total steps have happened
[32m[20221213 22:50:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1553 --------------------------#
[32m[20221213 22:50:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |           0.0091 |          32.3081 |          15.4659 |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |          -0.0009 |          29.0035 |          15.4565 |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |          -0.0019 |          27.7679 |          15.4639 |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |          -0.0100 |          27.5240 |          15.4824 |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |          -0.0092 |          27.0479 |          15.4722 |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |          -0.0116 |          26.8146 |          15.4937 |
[32m[20221213 22:50:49 @agent_ppo2.py:185][0m |          -0.0125 |          26.6378 |          15.4943 |
[32m[20221213 22:50:50 @agent_ppo2.py:185][0m |          -0.0032 |          27.7523 |          15.4933 |
[32m[20221213 22:50:50 @agent_ppo2.py:185][0m |          -0.0118 |          26.2886 |          15.5072 |
[32m[20221213 22:50:50 @agent_ppo2.py:185][0m |          -0.0092 |          26.3411 |          15.5040 |
[32m[20221213 22:50:50 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.90
[32m[20221213 22:50:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 321.71
[32m[20221213 22:50:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.39
[32m[20221213 22:50:50 @agent_ppo2.py:143][0m Total time:      32.62 min
[32m[20221213 22:50:50 @agent_ppo2.py:145][0m 3182592 total steps have happened
[32m[20221213 22:50:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1554 --------------------------#
[32m[20221213 22:50:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:50 @agent_ppo2.py:185][0m |           0.0003 |          44.0253 |          15.4818 |
[32m[20221213 22:50:50 @agent_ppo2.py:185][0m |          -0.0046 |          41.1330 |          15.4613 |
[32m[20221213 22:50:50 @agent_ppo2.py:185][0m |          -0.0064 |          40.3222 |          15.4399 |
[32m[20221213 22:50:50 @agent_ppo2.py:185][0m |          -0.0095 |          39.9389 |          15.4213 |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0048 |          41.8065 |          15.4414 |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0059 |          39.3912 |          15.4449 |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0082 |          38.8877 |          15.4361 |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0100 |          38.3693 |          15.4161 |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0124 |          38.1245 |          15.4180 |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0073 |          38.5630 |          15.4151 |
[32m[20221213 22:50:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.30
[32m[20221213 22:50:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.98
[32m[20221213 22:50:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.39
[32m[20221213 22:50:51 @agent_ppo2.py:143][0m Total time:      32.64 min
[32m[20221213 22:50:51 @agent_ppo2.py:145][0m 3184640 total steps have happened
[32m[20221213 22:50:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1555 --------------------------#
[32m[20221213 22:50:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0008 |          35.1014 |          15.6112 |
[32m[20221213 22:50:51 @agent_ppo2.py:185][0m |          -0.0067 |          32.0179 |          15.6231 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |           0.0023 |          33.6570 |          15.6129 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |          -0.0041 |          31.1734 |          15.6003 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |          -0.0125 |          30.5451 |          15.5944 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |          -0.0114 |          30.0960 |          15.6096 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |          -0.0128 |          29.9392 |          15.6157 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |          -0.0126 |          29.7561 |          15.6183 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |          -0.0128 |          29.8309 |          15.6034 |
[32m[20221213 22:50:52 @agent_ppo2.py:185][0m |          -0.0141 |          29.4477 |          15.6076 |
[32m[20221213 22:50:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.22
[32m[20221213 22:50:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.57
[32m[20221213 22:50:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.57
[32m[20221213 22:50:52 @agent_ppo2.py:143][0m Total time:      32.66 min
[32m[20221213 22:50:52 @agent_ppo2.py:145][0m 3186688 total steps have happened
[32m[20221213 22:50:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1556 --------------------------#
[32m[20221213 22:50:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0006 |          39.2478 |          15.4085 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0003 |          37.6782 |          15.3804 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0076 |          36.2984 |          15.3687 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0116 |          35.9723 |          15.3720 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0120 |          35.4527 |          15.3574 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0097 |          35.2289 |          15.3579 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0122 |          34.8349 |          15.3582 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0136 |          34.6142 |          15.3482 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0003 |          39.9152 |          15.3489 |
[32m[20221213 22:50:53 @agent_ppo2.py:185][0m |          -0.0150 |          34.2592 |          15.3529 |
[32m[20221213 22:50:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:50:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.77
[32m[20221213 22:50:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.60
[32m[20221213 22:50:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.12
[32m[20221213 22:50:54 @agent_ppo2.py:143][0m Total time:      32.68 min
[32m[20221213 22:50:54 @agent_ppo2.py:145][0m 3188736 total steps have happened
[32m[20221213 22:50:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1557 --------------------------#
[32m[20221213 22:50:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |           0.0020 |          44.8013 |          15.3971 |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |          -0.0054 |          41.6364 |          15.4024 |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |          -0.0004 |          40.8709 |          15.4121 |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |           0.0046 |          42.8485 |          15.4071 |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |           0.0157 |          48.5114 |          15.4038 |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |          -0.0068 |          40.3932 |          15.4181 |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |          -0.0073 |          40.0579 |          15.4146 |
[32m[20221213 22:50:54 @agent_ppo2.py:185][0m |           0.0033 |          44.5498 |          15.4191 |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |           0.0054 |          45.5866 |          15.4382 |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |          -0.0025 |          40.7716 |          15.4430 |
[32m[20221213 22:50:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.96
[32m[20221213 22:50:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.52
[32m[20221213 22:50:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.51
[32m[20221213 22:50:55 @agent_ppo2.py:143][0m Total time:      32.70 min
[32m[20221213 22:50:55 @agent_ppo2.py:145][0m 3190784 total steps have happened
[32m[20221213 22:50:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1558 --------------------------#
[32m[20221213 22:50:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |           0.0135 |          41.0993 |          15.5589 |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |          -0.0068 |          31.9669 |          15.5854 |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |          -0.0056 |          30.5724 |          15.5867 |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |          -0.0107 |          29.8063 |          15.5681 |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |          -0.0077 |          29.1801 |          15.5925 |
[32m[20221213 22:50:55 @agent_ppo2.py:185][0m |          -0.0085 |          28.8541 |          15.5783 |
[32m[20221213 22:50:56 @agent_ppo2.py:185][0m |          -0.0105 |          28.0952 |          15.5837 |
[32m[20221213 22:50:56 @agent_ppo2.py:185][0m |          -0.0123 |          27.7797 |          15.5938 |
[32m[20221213 22:50:56 @agent_ppo2.py:185][0m |          -0.0146 |          27.4406 |          15.6057 |
[32m[20221213 22:50:56 @agent_ppo2.py:185][0m |          -0.0166 |          27.3769 |          15.6230 |
[32m[20221213 22:50:56 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.61
[32m[20221213 22:50:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.88
[32m[20221213 22:50:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.31
[32m[20221213 22:50:56 @agent_ppo2.py:143][0m Total time:      32.72 min
[32m[20221213 22:50:56 @agent_ppo2.py:145][0m 3192832 total steps have happened
[32m[20221213 22:50:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1559 --------------------------#
[32m[20221213 22:50:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:56 @agent_ppo2.py:185][0m |           0.0145 |          52.8242 |          15.6087 |
[32m[20221213 22:50:56 @agent_ppo2.py:185][0m |          -0.0016 |          43.8808 |          15.6086 |
[32m[20221213 22:50:56 @agent_ppo2.py:185][0m |          -0.0084 |          42.4748 |          15.6063 |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |          -0.0103 |          41.5098 |          15.6035 |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |          -0.0064 |          41.0265 |          15.5914 |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |          -0.0074 |          40.5885 |          15.5895 |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |          -0.0093 |          40.3330 |          15.5879 |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |          -0.0101 |          39.9754 |          15.5895 |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |          -0.0087 |          39.6384 |          15.5881 |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |          -0.0112 |          39.2806 |          15.5777 |
[32m[20221213 22:50:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:50:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.74
[32m[20221213 22:50:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.86
[32m[20221213 22:50:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.69
[32m[20221213 22:50:57 @agent_ppo2.py:143][0m Total time:      32.74 min
[32m[20221213 22:50:57 @agent_ppo2.py:145][0m 3194880 total steps have happened
[32m[20221213 22:50:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1560 --------------------------#
[32m[20221213 22:50:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:50:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:57 @agent_ppo2.py:185][0m |           0.0040 |          39.7680 |          15.5531 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0062 |          30.4577 |          15.5350 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |           0.0009 |          29.1790 |          15.5012 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0070 |          28.1608 |          15.5063 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0026 |          27.8138 |          15.5027 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0005 |          27.7735 |          15.4896 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0097 |          27.1304 |          15.4893 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0066 |          26.6090 |          15.4844 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0086 |          26.3105 |          15.4740 |
[32m[20221213 22:50:58 @agent_ppo2.py:185][0m |          -0.0088 |          26.2121 |          15.4754 |
[32m[20221213 22:50:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:50:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.89
[32m[20221213 22:50:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.74
[32m[20221213 22:50:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.48
[32m[20221213 22:50:58 @agent_ppo2.py:143][0m Total time:      32.76 min
[32m[20221213 22:50:58 @agent_ppo2.py:145][0m 3196928 total steps have happened
[32m[20221213 22:50:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1561 --------------------------#
[32m[20221213 22:50:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:50:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |           0.0076 |          31.1486 |          15.4222 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0021 |          26.6365 |          15.3958 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0072 |          25.9553 |          15.3833 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0012 |          26.1417 |          15.3673 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0044 |          25.2116 |          15.3512 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0084 |          24.7370 |          15.3560 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0127 |          24.3277 |          15.3504 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0063 |          24.2220 |          15.3337 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0067 |          24.1493 |          15.3386 |
[32m[20221213 22:50:59 @agent_ppo2.py:185][0m |          -0.0094 |          23.7567 |          15.3231 |
[32m[20221213 22:50:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:51:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.93
[32m[20221213 22:51:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.93
[32m[20221213 22:51:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.28
[32m[20221213 22:51:00 @agent_ppo2.py:143][0m Total time:      32.78 min
[32m[20221213 22:51:00 @agent_ppo2.py:145][0m 3198976 total steps have happened
[32m[20221213 22:51:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1562 --------------------------#
[32m[20221213 22:51:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:00 @agent_ppo2.py:185][0m |          -0.0030 |          23.0151 |          15.4866 |
[32m[20221213 22:51:00 @agent_ppo2.py:185][0m |           0.0001 |          19.9559 |          15.4866 |
[32m[20221213 22:51:00 @agent_ppo2.py:185][0m |          -0.0040 |          18.9852 |          15.4736 |
[32m[20221213 22:51:00 @agent_ppo2.py:185][0m |          -0.0102 |          18.3046 |          15.4592 |
[32m[20221213 22:51:00 @agent_ppo2.py:185][0m |          -0.0072 |          17.8853 |          15.4785 |
[32m[20221213 22:51:00 @agent_ppo2.py:185][0m |          -0.0136 |          17.4780 |          15.4808 |
[32m[20221213 22:51:00 @agent_ppo2.py:185][0m |          -0.0079 |          17.2925 |          15.4778 |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0087 |          17.0384 |          15.4919 |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0098 |          16.7941 |          15.4932 |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0123 |          16.6224 |          15.5005 |
[32m[20221213 22:51:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.51
[32m[20221213 22:51:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.49
[32m[20221213 22:51:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.96
[32m[20221213 22:51:01 @agent_ppo2.py:143][0m Total time:      32.80 min
[32m[20221213 22:51:01 @agent_ppo2.py:145][0m 3201024 total steps have happened
[32m[20221213 22:51:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1563 --------------------------#
[32m[20221213 22:51:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0058 |          29.0436 |          15.6584 |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0095 |          25.7868 |          15.6377 |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0064 |          24.6929 |          15.6145 |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0076 |          24.0063 |          15.6279 |
[32m[20221213 22:51:01 @agent_ppo2.py:185][0m |          -0.0102 |          23.5029 |          15.6238 |
[32m[20221213 22:51:02 @agent_ppo2.py:185][0m |          -0.0107 |          23.2631 |          15.6194 |
[32m[20221213 22:51:02 @agent_ppo2.py:185][0m |          -0.0130 |          23.0296 |          15.6136 |
[32m[20221213 22:51:02 @agent_ppo2.py:185][0m |          -0.0115 |          22.7670 |          15.6163 |
[32m[20221213 22:51:02 @agent_ppo2.py:185][0m |          -0.0056 |          22.6620 |          15.6139 |
[32m[20221213 22:51:02 @agent_ppo2.py:185][0m |          -0.0112 |          22.3372 |          15.6188 |
[32m[20221213 22:51:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.88
[32m[20221213 22:51:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.10
[32m[20221213 22:51:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.05
[32m[20221213 22:51:02 @agent_ppo2.py:143][0m Total time:      32.82 min
[32m[20221213 22:51:02 @agent_ppo2.py:145][0m 3203072 total steps have happened
[32m[20221213 22:51:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1564 --------------------------#
[32m[20221213 22:51:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:02 @agent_ppo2.py:185][0m |           0.0000 |          44.5510 |          15.6641 |
[32m[20221213 22:51:02 @agent_ppo2.py:185][0m |           0.0013 |          43.1686 |          15.6535 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0071 |          41.5811 |          15.6822 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0082 |          40.9964 |          15.6995 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0031 |          40.7518 |          15.6863 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0101 |          40.3220 |          15.6860 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0138 |          39.9222 |          15.6957 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0085 |          39.9870 |          15.6878 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0068 |          40.0390 |          15.7083 |
[32m[20221213 22:51:03 @agent_ppo2.py:185][0m |          -0.0107 |          39.5132 |          15.7014 |
[32m[20221213 22:51:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.45
[32m[20221213 22:51:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.49
[32m[20221213 22:51:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.50
[32m[20221213 22:51:03 @agent_ppo2.py:143][0m Total time:      32.84 min
[32m[20221213 22:51:03 @agent_ppo2.py:145][0m 3205120 total steps have happened
[32m[20221213 22:51:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1565 --------------------------#
[32m[20221213 22:51:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |           0.0091 |          39.2838 |          15.5125 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0034 |          33.8179 |          15.5281 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0044 |          32.9913 |          15.5329 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0027 |          32.5350 |          15.5352 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0078 |          31.9876 |          15.5329 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0071 |          31.5688 |          15.5232 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0119 |          31.2970 |          15.5347 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0102 |          31.1073 |          15.5429 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0128 |          30.9715 |          15.5407 |
[32m[20221213 22:51:04 @agent_ppo2.py:185][0m |          -0.0126 |          30.7874 |          15.5432 |
[32m[20221213 22:51:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.18
[32m[20221213 22:51:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.75
[32m[20221213 22:51:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.92
[32m[20221213 22:51:04 @agent_ppo2.py:143][0m Total time:      32.86 min
[32m[20221213 22:51:04 @agent_ppo2.py:145][0m 3207168 total steps have happened
[32m[20221213 22:51:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1566 --------------------------#
[32m[20221213 22:51:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |           0.0033 |          29.8367 |          15.7378 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0056 |          25.5463 |          15.7280 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0002 |          24.5361 |          15.7090 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0087 |          23.5093 |          15.7144 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0064 |          23.1782 |          15.7143 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0061 |          22.4464 |          15.7201 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0135 |          22.0680 |          15.7217 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0138 |          21.7136 |          15.7084 |
[32m[20221213 22:51:05 @agent_ppo2.py:185][0m |          -0.0087 |          21.4045 |          15.7236 |
[32m[20221213 22:51:06 @agent_ppo2.py:185][0m |          -0.0000 |          22.8341 |          15.7199 |
[32m[20221213 22:51:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:51:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.64
[32m[20221213 22:51:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.94
[32m[20221213 22:51:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.59
[32m[20221213 22:51:06 @agent_ppo2.py:143][0m Total time:      32.88 min
[32m[20221213 22:51:06 @agent_ppo2.py:145][0m 3209216 total steps have happened
[32m[20221213 22:51:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1567 --------------------------#
[32m[20221213 22:51:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:06 @agent_ppo2.py:185][0m |          -0.0012 |          29.8357 |          15.5530 |
[32m[20221213 22:51:06 @agent_ppo2.py:185][0m |          -0.0078 |          25.7102 |          15.5498 |
[32m[20221213 22:51:06 @agent_ppo2.py:185][0m |          -0.0095 |          24.4795 |          15.5581 |
[32m[20221213 22:51:06 @agent_ppo2.py:185][0m |          -0.0004 |          24.1118 |          15.5539 |
[32m[20221213 22:51:06 @agent_ppo2.py:185][0m |          -0.0140 |          23.3295 |          15.5612 |
[32m[20221213 22:51:06 @agent_ppo2.py:185][0m |          -0.0086 |          22.9310 |          15.5658 |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |          -0.0119 |          22.6259 |          15.5791 |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |          -0.0130 |          22.4177 |          15.5714 |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |          -0.0136 |          22.2386 |          15.5853 |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |          -0.0141 |          22.1060 |          15.5759 |
[32m[20221213 22:51:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.06
[32m[20221213 22:51:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.90
[32m[20221213 22:51:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.29
[32m[20221213 22:51:07 @agent_ppo2.py:143][0m Total time:      32.90 min
[32m[20221213 22:51:07 @agent_ppo2.py:145][0m 3211264 total steps have happened
[32m[20221213 22:51:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1568 --------------------------#
[32m[20221213 22:51:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |           0.0002 |          40.6168 |          15.7079 |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |          -0.0003 |          36.8813 |          15.7198 |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |          -0.0059 |          35.7449 |          15.7365 |
[32m[20221213 22:51:07 @agent_ppo2.py:185][0m |          -0.0080 |          35.2438 |          15.7395 |
[32m[20221213 22:51:08 @agent_ppo2.py:185][0m |          -0.0058 |          34.7978 |          15.7164 |
[32m[20221213 22:51:08 @agent_ppo2.py:185][0m |          -0.0122 |          34.3108 |          15.7175 |
[32m[20221213 22:51:08 @agent_ppo2.py:185][0m |          -0.0089 |          34.3376 |          15.7484 |
[32m[20221213 22:51:08 @agent_ppo2.py:185][0m |          -0.0120 |          33.7675 |          15.7449 |
[32m[20221213 22:51:08 @agent_ppo2.py:185][0m |          -0.0064 |          33.7901 |          15.7396 |
[32m[20221213 22:51:08 @agent_ppo2.py:185][0m |          -0.0069 |          34.2798 |          15.7662 |
[32m[20221213 22:51:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.42
[32m[20221213 22:51:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.15
[32m[20221213 22:51:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.28
[32m[20221213 22:51:08 @agent_ppo2.py:143][0m Total time:      32.92 min
[32m[20221213 22:51:08 @agent_ppo2.py:145][0m 3213312 total steps have happened
[32m[20221213 22:51:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1569 --------------------------#
[32m[20221213 22:51:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:08 @agent_ppo2.py:185][0m |           0.0027 |          34.3880 |          15.6294 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0081 |          30.7831 |          15.6309 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0106 |          29.7859 |          15.6143 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0094 |          28.7738 |          15.6389 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0175 |          28.4665 |          15.6158 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0147 |          28.1091 |          15.6030 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0133 |          27.7792 |          15.6116 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0077 |          27.5466 |          15.5908 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0123 |          27.1612 |          15.5869 |
[32m[20221213 22:51:09 @agent_ppo2.py:185][0m |          -0.0159 |          27.0217 |          15.6017 |
[32m[20221213 22:51:09 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.06
[32m[20221213 22:51:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.10
[32m[20221213 22:51:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.99
[32m[20221213 22:51:09 @agent_ppo2.py:143][0m Total time:      32.94 min
[32m[20221213 22:51:09 @agent_ppo2.py:145][0m 3215360 total steps have happened
[32m[20221213 22:51:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1570 --------------------------#
[32m[20221213 22:51:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:51:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0057 |          32.6700 |          15.5322 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0113 |          25.5254 |          15.4821 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0049 |          24.5194 |          15.4870 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0057 |          23.8841 |          15.4911 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0063 |          24.1379 |          15.4972 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0057 |          22.8948 |          15.4762 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0104 |          22.6798 |          15.4820 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0168 |          22.3578 |          15.4998 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0137 |          22.3115 |          15.4885 |
[32m[20221213 22:51:10 @agent_ppo2.py:185][0m |          -0.0095 |          22.2004 |          15.4901 |
[32m[20221213 22:51:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.14
[32m[20221213 22:51:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.82
[32m[20221213 22:51:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.02
[32m[20221213 22:51:11 @agent_ppo2.py:143][0m Total time:      32.96 min
[32m[20221213 22:51:11 @agent_ppo2.py:145][0m 3217408 total steps have happened
[32m[20221213 22:51:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1571 --------------------------#
[32m[20221213 22:51:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0026 |          43.6366 |          15.4721 |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0057 |          41.0810 |          15.4658 |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0071 |          40.0577 |          15.4574 |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0081 |          39.6523 |          15.4924 |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0095 |          39.2074 |          15.4682 |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0043 |          39.8921 |          15.4770 |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0104 |          39.0153 |          15.4694 |
[32m[20221213 22:51:11 @agent_ppo2.py:185][0m |          -0.0119 |          38.5517 |          15.4885 |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |          -0.0128 |          38.3135 |          15.4829 |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |          -0.0072 |          38.3810 |          15.4917 |
[32m[20221213 22:51:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.26
[32m[20221213 22:51:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.01
[32m[20221213 22:51:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.15
[32m[20221213 22:51:12 @agent_ppo2.py:143][0m Total time:      32.98 min
[32m[20221213 22:51:12 @agent_ppo2.py:145][0m 3219456 total steps have happened
[32m[20221213 22:51:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1572 --------------------------#
[32m[20221213 22:51:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |          -0.0042 |          41.7653 |          15.5687 |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |          -0.0093 |          38.3152 |          15.5434 |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |          -0.0051 |          36.7248 |          15.5185 |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |           0.0002 |          36.0767 |          15.5145 |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |          -0.0091 |          35.5977 |          15.5051 |
[32m[20221213 22:51:12 @agent_ppo2.py:185][0m |          -0.0080 |          35.3284 |          15.4815 |
[32m[20221213 22:51:13 @agent_ppo2.py:185][0m |          -0.0060 |          36.4964 |          15.4614 |
[32m[20221213 22:51:13 @agent_ppo2.py:185][0m |           0.0018 |          36.7544 |          15.4271 |
[32m[20221213 22:51:13 @agent_ppo2.py:185][0m |          -0.0030 |          34.6892 |          15.4373 |
[32m[20221213 22:51:13 @agent_ppo2.py:185][0m |          -0.0106 |          34.0551 |          15.4284 |
[32m[20221213 22:51:13 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.12
[32m[20221213 22:51:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.10
[32m[20221213 22:51:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.94
[32m[20221213 22:51:13 @agent_ppo2.py:143][0m Total time:      33.00 min
[32m[20221213 22:51:13 @agent_ppo2.py:145][0m 3221504 total steps have happened
[32m[20221213 22:51:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1573 --------------------------#
[32m[20221213 22:51:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:13 @agent_ppo2.py:185][0m |           0.0025 |          35.8329 |          15.3644 |
[32m[20221213 22:51:13 @agent_ppo2.py:185][0m |          -0.0034 |          32.6735 |          15.3527 |
[32m[20221213 22:51:13 @agent_ppo2.py:185][0m |          -0.0107 |          31.6833 |          15.3329 |
[32m[20221213 22:51:14 @agent_ppo2.py:185][0m |          -0.0054 |          30.9860 |          15.3464 |
[32m[20221213 22:51:14 @agent_ppo2.py:185][0m |          -0.0073 |          30.8268 |          15.3316 |
[32m[20221213 22:51:14 @agent_ppo2.py:185][0m |          -0.0104 |          30.3510 |          15.3351 |
[32m[20221213 22:51:14 @agent_ppo2.py:185][0m |          -0.0101 |          29.7931 |          15.3316 |
[32m[20221213 22:51:14 @agent_ppo2.py:185][0m |          -0.0112 |          29.6475 |          15.3210 |
[32m[20221213 22:51:14 @agent_ppo2.py:185][0m |          -0.0107 |          29.3328 |          15.3097 |
[32m[20221213 22:51:14 @agent_ppo2.py:185][0m |          -0.0170 |          29.0957 |          15.3068 |
[32m[20221213 22:51:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.20
[32m[20221213 22:51:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.15
[32m[20221213 22:51:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.20
[32m[20221213 22:51:14 @agent_ppo2.py:143][0m Total time:      33.02 min
[32m[20221213 22:51:14 @agent_ppo2.py:145][0m 3223552 total steps have happened
[32m[20221213 22:51:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1574 --------------------------#
[32m[20221213 22:51:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |           0.0033 |          31.9893 |          15.7197 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0084 |          28.0196 |          15.7007 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0050 |          25.9957 |          15.7073 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0121 |          24.3944 |          15.6982 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0135 |          23.7564 |          15.6929 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0101 |          23.0900 |          15.7065 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0037 |          25.1539 |          15.6972 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0140 |          23.0125 |          15.7087 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0192 |          22.1625 |          15.7114 |
[32m[20221213 22:51:15 @agent_ppo2.py:185][0m |          -0.0186 |          21.8328 |          15.7081 |
[32m[20221213 22:51:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.89
[32m[20221213 22:51:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.68
[32m[20221213 22:51:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.86
[32m[20221213 22:51:15 @agent_ppo2.py:143][0m Total time:      33.04 min
[32m[20221213 22:51:15 @agent_ppo2.py:145][0m 3225600 total steps have happened
[32m[20221213 22:51:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1575 --------------------------#
[32m[20221213 22:51:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |           0.0015 |          36.4893 |          15.5467 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0033 |          34.6864 |          15.5344 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0065 |          34.1060 |          15.5424 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0085 |          33.5861 |          15.5172 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0076 |          33.3809 |          15.5148 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0094 |          33.2367 |          15.5193 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0108 |          33.0001 |          15.4968 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0116 |          32.8794 |          15.4791 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0078 |          33.2344 |          15.4999 |
[32m[20221213 22:51:16 @agent_ppo2.py:185][0m |          -0.0098 |          32.6816 |          15.4991 |
[32m[20221213 22:51:16 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.53
[32m[20221213 22:51:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.94
[32m[20221213 22:51:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 176.26
[32m[20221213 22:51:17 @agent_ppo2.py:143][0m Total time:      33.06 min
[32m[20221213 22:51:17 @agent_ppo2.py:145][0m 3227648 total steps have happened
[32m[20221213 22:51:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1576 --------------------------#
[32m[20221213 22:51:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:17 @agent_ppo2.py:185][0m |           0.0033 |          28.9224 |          15.6192 |
[32m[20221213 22:51:17 @agent_ppo2.py:185][0m |           0.0044 |          23.5806 |          15.5808 |
[32m[20221213 22:51:17 @agent_ppo2.py:185][0m |          -0.0031 |          21.5159 |          15.5830 |
[32m[20221213 22:51:17 @agent_ppo2.py:185][0m |          -0.0072 |          20.3606 |          15.5749 |
[32m[20221213 22:51:17 @agent_ppo2.py:185][0m |          -0.0105 |          19.6986 |          15.5849 |
[32m[20221213 22:51:17 @agent_ppo2.py:185][0m |          -0.0042 |          20.1707 |          15.5646 |
[32m[20221213 22:51:17 @agent_ppo2.py:185][0m |          -0.0105 |          18.7606 |          15.5623 |
[32m[20221213 22:51:18 @agent_ppo2.py:185][0m |          -0.0099 |          18.5694 |          15.5632 |
[32m[20221213 22:51:18 @agent_ppo2.py:185][0m |          -0.0058 |          18.6568 |          15.5485 |
[32m[20221213 22:51:18 @agent_ppo2.py:185][0m |          -0.0115 |          17.7968 |          15.5360 |
[32m[20221213 22:51:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.31
[32m[20221213 22:51:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.59
[32m[20221213 22:51:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.71
[32m[20221213 22:51:18 @agent_ppo2.py:143][0m Total time:      33.08 min
[32m[20221213 22:51:18 @agent_ppo2.py:145][0m 3229696 total steps have happened
[32m[20221213 22:51:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1577 --------------------------#
[32m[20221213 22:51:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:18 @agent_ppo2.py:185][0m |          -0.0014 |          34.6358 |          15.7080 |
[32m[20221213 22:51:18 @agent_ppo2.py:185][0m |          -0.0067 |          32.0968 |          15.6903 |
[32m[20221213 22:51:18 @agent_ppo2.py:185][0m |          -0.0064 |          31.2858 |          15.6834 |
[32m[20221213 22:51:18 @agent_ppo2.py:185][0m |          -0.0016 |          33.3063 |          15.6835 |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0127 |          31.0629 |          15.6662 |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0102 |          30.4475 |          15.6698 |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0128 |          30.2818 |          15.6627 |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0127 |          30.0705 |          15.6730 |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0137 |          30.0783 |          15.6680 |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0106 |          29.7879 |          15.6636 |
[32m[20221213 22:51:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.49
[32m[20221213 22:51:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.69
[32m[20221213 22:51:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.64
[32m[20221213 22:51:19 @agent_ppo2.py:143][0m Total time:      33.10 min
[32m[20221213 22:51:19 @agent_ppo2.py:145][0m 3231744 total steps have happened
[32m[20221213 22:51:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1578 --------------------------#
[32m[20221213 22:51:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0003 |          37.7220 |          15.7019 |
[32m[20221213 22:51:19 @agent_ppo2.py:185][0m |          -0.0054 |          35.7223 |          15.6777 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |           0.0011 |          37.0237 |          15.6876 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |          -0.0072 |          33.8696 |          15.6818 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |          -0.0117 |          33.2517 |          15.6680 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |          -0.0137 |          32.7492 |          15.6546 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |          -0.0107 |          32.6173 |          15.6636 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |          -0.0124 |          32.0806 |          15.6610 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |          -0.0154 |          31.7500 |          15.6590 |
[32m[20221213 22:51:20 @agent_ppo2.py:185][0m |          -0.0138 |          31.5462 |          15.6593 |
[32m[20221213 22:51:20 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 301.83
[32m[20221213 22:51:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.90
[32m[20221213 22:51:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.28
[32m[20221213 22:51:20 @agent_ppo2.py:143][0m Total time:      33.12 min
[32m[20221213 22:51:20 @agent_ppo2.py:145][0m 3233792 total steps have happened
[32m[20221213 22:51:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1579 --------------------------#
[32m[20221213 22:51:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |           0.0030 |          39.3820 |          15.6891 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0004 |          36.9102 |          15.6881 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0061 |          36.2674 |          15.7015 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0081 |          35.9864 |          15.6934 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |           0.0086 |          39.7508 |          15.6893 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0036 |          36.0709 |          15.7021 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0065 |          35.5046 |          15.7140 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0086 |          35.3894 |          15.7180 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0133 |          35.3198 |          15.7290 |
[32m[20221213 22:51:21 @agent_ppo2.py:185][0m |          -0.0123 |          35.2498 |          15.7251 |
[32m[20221213 22:51:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.52
[32m[20221213 22:51:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.42
[32m[20221213 22:51:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.66
[32m[20221213 22:51:21 @agent_ppo2.py:143][0m Total time:      33.14 min
[32m[20221213 22:51:21 @agent_ppo2.py:145][0m 3235840 total steps have happened
[32m[20221213 22:51:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1580 --------------------------#
[32m[20221213 22:51:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:51:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0012 |          49.1396 |          15.3482 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0055 |          47.2183 |          15.3083 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0090 |          46.6281 |          15.3362 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0122 |          46.4229 |          15.3518 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0128 |          46.1426 |          15.3513 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0092 |          45.9124 |          15.3602 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0062 |          47.3110 |          15.3710 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0123 |          45.5873 |          15.3646 |
[32m[20221213 22:51:22 @agent_ppo2.py:185][0m |          -0.0118 |          45.2535 |          15.3866 |
[32m[20221213 22:51:23 @agent_ppo2.py:185][0m |           0.0022 |          52.7502 |          15.3842 |
[32m[20221213 22:51:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.39
[32m[20221213 22:51:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.33
[32m[20221213 22:51:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.90
[32m[20221213 22:51:23 @agent_ppo2.py:143][0m Total time:      33.16 min
[32m[20221213 22:51:23 @agent_ppo2.py:145][0m 3237888 total steps have happened
[32m[20221213 22:51:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1581 --------------------------#
[32m[20221213 22:51:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:23 @agent_ppo2.py:185][0m |          -0.0024 |          43.6512 |          15.4992 |
[32m[20221213 22:51:23 @agent_ppo2.py:185][0m |          -0.0039 |          40.3780 |          15.4774 |
[32m[20221213 22:51:23 @agent_ppo2.py:185][0m |          -0.0058 |          39.4771 |          15.4812 |
[32m[20221213 22:51:23 @agent_ppo2.py:185][0m |          -0.0004 |          38.4797 |          15.4863 |
[32m[20221213 22:51:23 @agent_ppo2.py:185][0m |          -0.0021 |          38.4653 |          15.4723 |
[32m[20221213 22:51:23 @agent_ppo2.py:185][0m |          -0.0069 |          37.6136 |          15.4730 |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |           0.0024 |          42.6613 |          15.4860 |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |          -0.0052 |          36.9235 |          15.4774 |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |          -0.0033 |          36.0165 |          15.4866 |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |          -0.0085 |          36.5501 |          15.4904 |
[32m[20221213 22:51:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:51:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.82
[32m[20221213 22:51:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.08
[32m[20221213 22:51:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.21
[32m[20221213 22:51:24 @agent_ppo2.py:143][0m Total time:      33.18 min
[32m[20221213 22:51:24 @agent_ppo2.py:145][0m 3239936 total steps have happened
[32m[20221213 22:51:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1582 --------------------------#
[32m[20221213 22:51:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |           0.0036 |          29.4876 |          15.5839 |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |          -0.0033 |          24.8515 |          15.5736 |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |          -0.0034 |          23.9100 |          15.5612 |
[32m[20221213 22:51:24 @agent_ppo2.py:185][0m |          -0.0001 |          23.4033 |          15.5671 |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |          -0.0113 |          23.0779 |          15.5606 |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |          -0.0143 |          22.8685 |          15.5463 |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |          -0.0058 |          22.4557 |          15.5399 |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |          -0.0113 |          22.3838 |          15.5500 |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |          -0.0070 |          22.1629 |          15.5272 |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |          -0.0116 |          22.0438 |          15.5320 |
[32m[20221213 22:51:25 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.41
[32m[20221213 22:51:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.89
[32m[20221213 22:51:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.60
[32m[20221213 22:51:25 @agent_ppo2.py:143][0m Total time:      33.20 min
[32m[20221213 22:51:25 @agent_ppo2.py:145][0m 3241984 total steps have happened
[32m[20221213 22:51:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1583 --------------------------#
[32m[20221213 22:51:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |           0.0016 |          35.0590 |          15.7072 |
[32m[20221213 22:51:25 @agent_ppo2.py:185][0m |           0.0002 |          32.3669 |          15.6899 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0038 |          31.8502 |          15.7134 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0049 |          31.5509 |          15.7080 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0031 |          31.1989 |          15.7115 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0012 |          31.8379 |          15.7084 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0069 |          30.9193 |          15.7034 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0087 |          30.7993 |          15.7179 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0094 |          30.6085 |          15.6957 |
[32m[20221213 22:51:26 @agent_ppo2.py:185][0m |          -0.0063 |          30.5113 |          15.7158 |
[32m[20221213 22:51:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.19
[32m[20221213 22:51:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.78
[32m[20221213 22:51:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.96
[32m[20221213 22:51:26 @agent_ppo2.py:143][0m Total time:      33.22 min
[32m[20221213 22:51:26 @agent_ppo2.py:145][0m 3244032 total steps have happened
[32m[20221213 22:51:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1584 --------------------------#
[32m[20221213 22:51:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0034 |          48.7047 |          15.6376 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0015 |          47.5539 |          15.6171 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0054 |          47.1759 |          15.6264 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |           0.0053 |          48.5006 |          15.5933 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0008 |          47.9526 |          15.6263 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0075 |          46.9133 |          15.6130 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0096 |          46.6287 |          15.6302 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0085 |          46.4613 |          15.6182 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0116 |          46.6333 |          15.6138 |
[32m[20221213 22:51:27 @agent_ppo2.py:185][0m |          -0.0106 |          46.4002 |          15.6324 |
[32m[20221213 22:51:27 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.17
[32m[20221213 22:51:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.66
[32m[20221213 22:51:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.93
[32m[20221213 22:51:28 @agent_ppo2.py:143][0m Total time:      33.24 min
[32m[20221213 22:51:28 @agent_ppo2.py:145][0m 3246080 total steps have happened
[32m[20221213 22:51:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1585 --------------------------#
[32m[20221213 22:51:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:51:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:28 @agent_ppo2.py:185][0m |          -0.0006 |          38.6148 |          15.6091 |
[32m[20221213 22:51:28 @agent_ppo2.py:185][0m |          -0.0026 |          36.4507 |          15.5986 |
[32m[20221213 22:51:28 @agent_ppo2.py:185][0m |          -0.0006 |          36.1784 |          15.5986 |
[32m[20221213 22:51:28 @agent_ppo2.py:185][0m |          -0.0054 |          35.6494 |          15.5960 |
[32m[20221213 22:51:28 @agent_ppo2.py:185][0m |          -0.0034 |          35.3903 |          15.6046 |
[32m[20221213 22:51:28 @agent_ppo2.py:185][0m |          -0.0074 |          35.1022 |          15.5779 |
[32m[20221213 22:51:28 @agent_ppo2.py:185][0m |          -0.0085 |          34.8911 |          15.6094 |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |          -0.0006 |          36.3568 |          15.5945 |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |          -0.0108 |          34.7784 |          15.5863 |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |          -0.0064 |          34.4803 |          15.6048 |
[32m[20221213 22:51:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:51:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.68
[32m[20221213 22:51:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.33
[32m[20221213 22:51:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.78
[32m[20221213 22:51:29 @agent_ppo2.py:143][0m Total time:      33.27 min
[32m[20221213 22:51:29 @agent_ppo2.py:145][0m 3248128 total steps have happened
[32m[20221213 22:51:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1586 --------------------------#
[32m[20221213 22:51:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |          -0.0019 |          32.2815 |          15.6806 |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |           0.0136 |          34.3787 |          15.6806 |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |          -0.0034 |          29.2837 |          15.6812 |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |          -0.0084 |          28.5094 |          15.6696 |
[32m[20221213 22:51:29 @agent_ppo2.py:185][0m |          -0.0089 |          28.2759 |          15.6616 |
[32m[20221213 22:51:30 @agent_ppo2.py:185][0m |          -0.0086 |          28.1247 |          15.6714 |
[32m[20221213 22:51:30 @agent_ppo2.py:185][0m |          -0.0053 |          28.2977 |          15.6680 |
[32m[20221213 22:51:30 @agent_ppo2.py:185][0m |          -0.0098 |          27.9227 |          15.6593 |
[32m[20221213 22:51:30 @agent_ppo2.py:185][0m |          -0.0134 |          27.6856 |          15.6658 |
[32m[20221213 22:51:30 @agent_ppo2.py:185][0m |          -0.0115 |          27.8074 |          15.6698 |
[32m[20221213 22:51:30 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.15
[32m[20221213 22:51:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.98
[32m[20221213 22:51:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.61
[32m[20221213 22:51:30 @agent_ppo2.py:143][0m Total time:      33.29 min
[32m[20221213 22:51:30 @agent_ppo2.py:145][0m 3250176 total steps have happened
[32m[20221213 22:51:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1587 --------------------------#
[32m[20221213 22:51:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:30 @agent_ppo2.py:185][0m |           0.0030 |          32.1525 |          15.5050 |
[32m[20221213 22:51:30 @agent_ppo2.py:185][0m |          -0.0085 |          29.2054 |          15.4839 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0089 |          28.4797 |          15.4515 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0105 |          27.8393 |          15.4494 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0102 |          27.4334 |          15.4168 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0146 |          27.4163 |          15.4171 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0114 |          27.0473 |          15.4002 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0158 |          26.8836 |          15.3814 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0180 |          26.7960 |          15.3765 |
[32m[20221213 22:51:31 @agent_ppo2.py:185][0m |          -0.0159 |          26.6270 |          15.3677 |
[32m[20221213 22:51:31 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.26
[32m[20221213 22:51:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.56
[32m[20221213 22:51:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.14
[32m[20221213 22:51:31 @agent_ppo2.py:143][0m Total time:      33.31 min
[32m[20221213 22:51:31 @agent_ppo2.py:145][0m 3252224 total steps have happened
[32m[20221213 22:51:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1588 --------------------------#
[32m[20221213 22:51:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |           0.0027 |          29.9552 |          15.5644 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0056 |          27.0689 |          15.5399 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |           0.0019 |          25.7556 |          15.5384 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0106 |          25.1990 |          15.5538 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0036 |          24.6713 |          15.5491 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0087 |          24.1875 |          15.5407 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0107 |          23.8507 |          15.5483 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0098 |          23.6991 |          15.5635 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0138 |          23.4181 |          15.5708 |
[32m[20221213 22:51:32 @agent_ppo2.py:185][0m |          -0.0111 |          23.3267 |          15.5685 |
[32m[20221213 22:51:32 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.34
[32m[20221213 22:51:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.67
[32m[20221213 22:51:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.47
[32m[20221213 22:51:32 @agent_ppo2.py:143][0m Total time:      33.33 min
[32m[20221213 22:51:32 @agent_ppo2.py:145][0m 3254272 total steps have happened
[32m[20221213 22:51:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1589 --------------------------#
[32m[20221213 22:51:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |           0.0020 |          39.7207 |          15.4459 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |          -0.0012 |          37.5754 |          15.4284 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |           0.0075 |          37.5492 |          15.4509 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |          -0.0052 |          35.9084 |          15.4187 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |          -0.0072 |          35.5422 |          15.4722 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |          -0.0044 |          35.2117 |          15.4292 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |          -0.0076 |          35.0252 |          15.4048 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |          -0.0065 |          34.9578 |          15.4397 |
[32m[20221213 22:51:33 @agent_ppo2.py:185][0m |          -0.0069 |          34.9906 |          15.4322 |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |          -0.0066 |          34.6998 |          15.4376 |
[32m[20221213 22:51:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.20
[32m[20221213 22:51:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.80
[32m[20221213 22:51:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.69
[32m[20221213 22:51:34 @agent_ppo2.py:143][0m Total time:      33.35 min
[32m[20221213 22:51:34 @agent_ppo2.py:145][0m 3256320 total steps have happened
[32m[20221213 22:51:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1590 --------------------------#
[32m[20221213 22:51:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:51:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |           0.0114 |          47.0464 |          15.5682 |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |          -0.0031 |          42.7339 |          15.5554 |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |          -0.0051 |          41.5397 |          15.5672 |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |          -0.0066 |          40.8630 |          15.5755 |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |          -0.0088 |          40.3431 |          15.5781 |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |          -0.0076 |          40.0926 |          15.5888 |
[32m[20221213 22:51:34 @agent_ppo2.py:185][0m |          -0.0060 |          39.6731 |          15.5931 |
[32m[20221213 22:51:35 @agent_ppo2.py:185][0m |          -0.0063 |          39.4084 |          15.5843 |
[32m[20221213 22:51:35 @agent_ppo2.py:185][0m |          -0.0125 |          39.3340 |          15.6090 |
[32m[20221213 22:51:35 @agent_ppo2.py:185][0m |          -0.0102 |          38.9137 |          15.6097 |
[32m[20221213 22:51:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.93
[32m[20221213 22:51:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.34
[32m[20221213 22:51:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.57
[32m[20221213 22:51:35 @agent_ppo2.py:143][0m Total time:      33.37 min
[32m[20221213 22:51:35 @agent_ppo2.py:145][0m 3258368 total steps have happened
[32m[20221213 22:51:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1591 --------------------------#
[32m[20221213 22:51:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:35 @agent_ppo2.py:185][0m |          -0.0008 |          37.5356 |          15.6482 |
[32m[20221213 22:51:35 @agent_ppo2.py:185][0m |          -0.0063 |          33.9949 |          15.6313 |
[32m[20221213 22:51:35 @agent_ppo2.py:185][0m |          -0.0113 |          32.7921 |          15.6108 |
[32m[20221213 22:51:35 @agent_ppo2.py:185][0m |           0.0016 |          36.6868 |          15.6193 |
[32m[20221213 22:51:36 @agent_ppo2.py:185][0m |          -0.0058 |          32.0100 |          15.6044 |
[32m[20221213 22:51:36 @agent_ppo2.py:185][0m |          -0.0113 |          31.0453 |          15.6063 |
[32m[20221213 22:51:36 @agent_ppo2.py:185][0m |          -0.0104 |          30.7808 |          15.5927 |
[32m[20221213 22:51:36 @agent_ppo2.py:185][0m |          -0.0125 |          30.5393 |          15.5968 |
[32m[20221213 22:51:36 @agent_ppo2.py:185][0m |          -0.0133 |          30.3185 |          15.5863 |
[32m[20221213 22:51:36 @agent_ppo2.py:185][0m |          -0.0063 |          31.5543 |          15.5844 |
[32m[20221213 22:51:36 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.26
[32m[20221213 22:51:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.31
[32m[20221213 22:51:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.67
[32m[20221213 22:51:36 @agent_ppo2.py:143][0m Total time:      33.39 min
[32m[20221213 22:51:36 @agent_ppo2.py:145][0m 3260416 total steps have happened
[32m[20221213 22:51:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1592 --------------------------#
[32m[20221213 22:51:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:36 @agent_ppo2.py:185][0m |           0.0060 |          49.6479 |          15.3752 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0008 |          46.8096 |          15.3575 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0021 |          45.9411 |          15.3621 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0024 |          47.1026 |          15.3377 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0073 |          45.1984 |          15.3497 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0072 |          44.7362 |          15.3280 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0061 |          44.9689 |          15.3280 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0077 |          44.1846 |          15.3102 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |          -0.0101 |          43.9796 |          15.3001 |
[32m[20221213 22:51:37 @agent_ppo2.py:185][0m |           0.0044 |          45.4085 |          15.2994 |
[32m[20221213 22:51:37 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.38
[32m[20221213 22:51:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.07
[32m[20221213 22:51:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.74
[32m[20221213 22:51:37 @agent_ppo2.py:143][0m Total time:      33.41 min
[32m[20221213 22:51:37 @agent_ppo2.py:145][0m 3262464 total steps have happened
[32m[20221213 22:51:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1593 --------------------------#
[32m[20221213 22:51:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |           0.0069 |          34.4915 |          15.3803 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0053 |          29.1919 |          15.3408 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0056 |          28.1475 |          15.3324 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0066 |          27.6901 |          15.3287 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0093 |          27.1865 |          15.3179 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0144 |          26.8946 |          15.3005 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0086 |          26.4258 |          15.2974 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0065 |          26.3513 |          15.3073 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0101 |          26.0264 |          15.3021 |
[32m[20221213 22:51:38 @agent_ppo2.py:185][0m |          -0.0106 |          25.7952 |          15.2870 |
[32m[20221213 22:51:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.90
[32m[20221213 22:51:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.87
[32m[20221213 22:51:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.67
[32m[20221213 22:51:39 @agent_ppo2.py:143][0m Total time:      33.43 min
[32m[20221213 22:51:39 @agent_ppo2.py:145][0m 3264512 total steps have happened
[32m[20221213 22:51:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1594 --------------------------#
[32m[20221213 22:51:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |           0.0020 |          31.9350 |          15.4229 |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |          -0.0043 |          28.5007 |          15.4367 |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |          -0.0114 |          27.9033 |          15.4367 |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |           0.0051 |          30.4294 |          15.4174 |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |          -0.0074 |          27.2563 |          15.4407 |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |          -0.0117 |          26.8727 |          15.4410 |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |           0.0089 |          33.2127 |          15.4379 |
[32m[20221213 22:51:39 @agent_ppo2.py:185][0m |           0.0027 |          28.9507 |          15.4399 |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |          -0.0065 |          26.3914 |          15.4627 |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |          -0.0025 |          28.4787 |          15.4474 |
[32m[20221213 22:51:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.60
[32m[20221213 22:51:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.33
[32m[20221213 22:51:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.23
[32m[20221213 22:51:40 @agent_ppo2.py:143][0m Total time:      33.45 min
[32m[20221213 22:51:40 @agent_ppo2.py:145][0m 3266560 total steps have happened
[32m[20221213 22:51:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1595 --------------------------#
[32m[20221213 22:51:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |          -0.0001 |          42.2519 |          15.4214 |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |          -0.0032 |          40.0070 |          15.4140 |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |           0.0006 |          41.1489 |          15.3975 |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |          -0.0113 |          39.4378 |          15.4091 |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |          -0.0028 |          40.1275 |          15.4102 |
[32m[20221213 22:51:40 @agent_ppo2.py:185][0m |          -0.0080 |          38.7226 |          15.4053 |
[32m[20221213 22:51:41 @agent_ppo2.py:185][0m |          -0.0088 |          38.4931 |          15.4196 |
[32m[20221213 22:51:41 @agent_ppo2.py:185][0m |          -0.0124 |          38.2245 |          15.4184 |
[32m[20221213 22:51:41 @agent_ppo2.py:185][0m |           0.0023 |          40.4892 |          15.4244 |
[32m[20221213 22:51:41 @agent_ppo2.py:185][0m |          -0.0112 |          38.0395 |          15.3975 |
[32m[20221213 22:51:41 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.55
[32m[20221213 22:51:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.94
[32m[20221213 22:51:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.17
[32m[20221213 22:51:41 @agent_ppo2.py:143][0m Total time:      33.47 min
[32m[20221213 22:51:41 @agent_ppo2.py:145][0m 3268608 total steps have happened
[32m[20221213 22:51:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1596 --------------------------#
[32m[20221213 22:51:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:41 @agent_ppo2.py:185][0m |          -0.0009 |          41.3127 |          15.6013 |
[32m[20221213 22:51:41 @agent_ppo2.py:185][0m |          -0.0059 |          39.3218 |          15.6226 |
[32m[20221213 22:51:41 @agent_ppo2.py:185][0m |          -0.0100 |          39.0031 |          15.6117 |
[32m[20221213 22:51:42 @agent_ppo2.py:185][0m |          -0.0068 |          38.5750 |          15.6237 |
[32m[20221213 22:51:42 @agent_ppo2.py:185][0m |          -0.0088 |          38.3457 |          15.6344 |
[32m[20221213 22:51:42 @agent_ppo2.py:185][0m |           0.0003 |          40.0063 |          15.6317 |
[32m[20221213 22:51:42 @agent_ppo2.py:185][0m |          -0.0074 |          38.1601 |          15.6392 |
[32m[20221213 22:51:42 @agent_ppo2.py:185][0m |          -0.0115 |          38.0351 |          15.6436 |
[32m[20221213 22:51:42 @agent_ppo2.py:185][0m |          -0.0100 |          37.8531 |          15.6495 |
[32m[20221213 22:51:42 @agent_ppo2.py:185][0m |          -0.0122 |          37.8701 |          15.6600 |
[32m[20221213 22:51:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.88
[32m[20221213 22:51:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.71
[32m[20221213 22:51:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.79
[32m[20221213 22:51:42 @agent_ppo2.py:143][0m Total time:      33.49 min
[32m[20221213 22:51:42 @agent_ppo2.py:145][0m 3270656 total steps have happened
[32m[20221213 22:51:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1597 --------------------------#
[32m[20221213 22:51:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0016 |          32.9284 |          15.5969 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0044 |          29.3520 |          15.6090 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0060 |          27.8608 |          15.5931 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0099 |          26.8746 |          15.5992 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0064 |          26.2912 |          15.5880 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0111 |          25.8816 |          15.6064 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0093 |          25.5140 |          15.5888 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0107 |          25.3305 |          15.6063 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0187 |          25.1382 |          15.6012 |
[32m[20221213 22:51:43 @agent_ppo2.py:185][0m |          -0.0109 |          24.9383 |          15.6053 |
[32m[20221213 22:51:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.72
[32m[20221213 22:51:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.50
[32m[20221213 22:51:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.69
[32m[20221213 22:51:43 @agent_ppo2.py:143][0m Total time:      33.51 min
[32m[20221213 22:51:43 @agent_ppo2.py:145][0m 3272704 total steps have happened
[32m[20221213 22:51:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1598 --------------------------#
[32m[20221213 22:51:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0024 |          43.7781 |          15.5201 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0046 |          41.5452 |          15.5092 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |           0.0018 |          44.3901 |          15.5252 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0043 |          40.3371 |          15.4985 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0058 |          39.7431 |          15.5246 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0097 |          39.3875 |          15.5262 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0076 |          39.3550 |          15.5020 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0131 |          38.8365 |          15.5259 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0140 |          38.7712 |          15.5231 |
[32m[20221213 22:51:44 @agent_ppo2.py:185][0m |          -0.0121 |          38.5241 |          15.5326 |
[32m[20221213 22:51:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.28
[32m[20221213 22:51:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.33
[32m[20221213 22:51:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.84
[32m[20221213 22:51:45 @agent_ppo2.py:143][0m Total time:      33.53 min
[32m[20221213 22:51:45 @agent_ppo2.py:145][0m 3274752 total steps have happened
[32m[20221213 22:51:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1599 --------------------------#
[32m[20221213 22:51:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |           0.0052 |          50.0628 |          15.4708 |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |          -0.0024 |          47.2901 |          15.4783 |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |          -0.0019 |          46.3174 |          15.4619 |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |          -0.0058 |          45.9007 |          15.4437 |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |          -0.0059 |          45.5694 |          15.4185 |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |          -0.0086 |          45.2969 |          15.4487 |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |          -0.0101 |          44.9050 |          15.4265 |
[32m[20221213 22:51:45 @agent_ppo2.py:185][0m |          -0.0079 |          44.7400 |          15.4038 |
[32m[20221213 22:51:46 @agent_ppo2.py:185][0m |          -0.0050 |          44.4763 |          15.3997 |
[32m[20221213 22:51:46 @agent_ppo2.py:185][0m |           0.0100 |          50.2146 |          15.3961 |
[32m[20221213 22:51:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 307.02
[32m[20221213 22:51:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.64
[32m[20221213 22:51:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.97
[32m[20221213 22:51:46 @agent_ppo2.py:143][0m Total time:      33.55 min
[32m[20221213 22:51:46 @agent_ppo2.py:145][0m 3276800 total steps have happened
[32m[20221213 22:51:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1600 --------------------------#
[32m[20221213 22:51:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:51:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:46 @agent_ppo2.py:185][0m |           0.0018 |          36.9490 |          15.6561 |
[32m[20221213 22:51:46 @agent_ppo2.py:185][0m |           0.0012 |          34.2120 |          15.6305 |
[32m[20221213 22:51:46 @agent_ppo2.py:185][0m |           0.0003 |          33.2322 |          15.6451 |
[32m[20221213 22:51:46 @agent_ppo2.py:185][0m |          -0.0065 |          32.3842 |          15.6270 |
[32m[20221213 22:51:46 @agent_ppo2.py:185][0m |          -0.0056 |          32.5835 |          15.6217 |
[32m[20221213 22:51:47 @agent_ppo2.py:185][0m |          -0.0034 |          32.1430 |          15.6214 |
[32m[20221213 22:51:47 @agent_ppo2.py:185][0m |          -0.0122 |          31.7800 |          15.6294 |
[32m[20221213 22:51:47 @agent_ppo2.py:185][0m |           0.0003 |          34.2820 |          15.6182 |
[32m[20221213 22:51:47 @agent_ppo2.py:185][0m |          -0.0107 |          31.5952 |          15.5994 |
[32m[20221213 22:51:47 @agent_ppo2.py:185][0m |          -0.0126 |          31.2649 |          15.6163 |
[32m[20221213 22:51:47 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.75
[32m[20221213 22:51:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.95
[32m[20221213 22:51:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.97
[32m[20221213 22:51:47 @agent_ppo2.py:143][0m Total time:      33.57 min
[32m[20221213 22:51:47 @agent_ppo2.py:145][0m 3278848 total steps have happened
[32m[20221213 22:51:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1601 --------------------------#
[32m[20221213 22:51:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:47 @agent_ppo2.py:185][0m |          -0.0014 |          41.2903 |          15.5942 |
[32m[20221213 22:51:47 @agent_ppo2.py:185][0m |          -0.0032 |          38.9510 |          15.6033 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |          -0.0051 |          37.9786 |          15.5911 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |           0.0044 |          40.6147 |          15.6047 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |          -0.0061 |          37.2038 |          15.6066 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |          -0.0095 |          36.3840 |          15.6171 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |          -0.0125 |          36.0722 |          15.6024 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |          -0.0104 |          35.9673 |          15.6223 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |          -0.0122 |          35.7333 |          15.6149 |
[32m[20221213 22:51:48 @agent_ppo2.py:185][0m |          -0.0111 |          35.7023 |          15.6293 |
[32m[20221213 22:51:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.12
[32m[20221213 22:51:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.80
[32m[20221213 22:51:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.49
[32m[20221213 22:51:48 @agent_ppo2.py:143][0m Total time:      33.59 min
[32m[20221213 22:51:48 @agent_ppo2.py:145][0m 3280896 total steps have happened
[32m[20221213 22:51:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1602 --------------------------#
[32m[20221213 22:51:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |           0.0023 |          46.3189 |          15.6210 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0060 |          42.9398 |          15.5954 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0098 |          41.8763 |          15.5996 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0009 |          42.3998 |          15.6043 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0000 |          42.7760 |          15.6076 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |           0.0138 |          45.5540 |          15.6206 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0074 |          39.8058 |          15.6212 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0064 |          39.5513 |          15.6162 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0091 |          38.9760 |          15.6333 |
[32m[20221213 22:51:49 @agent_ppo2.py:185][0m |          -0.0132 |          38.6510 |          15.6164 |
[32m[20221213 22:51:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.17
[32m[20221213 22:51:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.10
[32m[20221213 22:51:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.63
[32m[20221213 22:51:49 @agent_ppo2.py:143][0m Total time:      33.61 min
[32m[20221213 22:51:49 @agent_ppo2.py:145][0m 3282944 total steps have happened
[32m[20221213 22:51:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1603 --------------------------#
[32m[20221213 22:51:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |           0.0034 |          48.3071 |          15.4183 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0003 |          43.0785 |          15.3937 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0058 |          41.8684 |          15.3792 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0101 |          41.2010 |          15.3934 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0094 |          40.6949 |          15.3886 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0059 |          40.2769 |          15.3820 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0069 |          40.4813 |          15.3820 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0082 |          39.5827 |          15.3807 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0109 |          39.3665 |          15.3543 |
[32m[20221213 22:51:50 @agent_ppo2.py:185][0m |          -0.0036 |          42.7044 |          15.3705 |
[32m[20221213 22:51:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.81
[32m[20221213 22:51:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.87
[32m[20221213 22:51:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.89
[32m[20221213 22:51:51 @agent_ppo2.py:143][0m Total time:      33.63 min
[32m[20221213 22:51:51 @agent_ppo2.py:145][0m 3284992 total steps have happened
[32m[20221213 22:51:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1604 --------------------------#
[32m[20221213 22:51:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:51 @agent_ppo2.py:185][0m |           0.0016 |          33.3055 |          15.4548 |
[32m[20221213 22:51:51 @agent_ppo2.py:185][0m |          -0.0154 |          28.1082 |          15.4455 |
[32m[20221213 22:51:51 @agent_ppo2.py:185][0m |          -0.0036 |          28.9075 |          15.4550 |
[32m[20221213 22:51:51 @agent_ppo2.py:185][0m |          -0.0082 |          25.5095 |          15.4404 |
[32m[20221213 22:51:51 @agent_ppo2.py:185][0m |          -0.0148 |          24.9567 |          15.4537 |
[32m[20221213 22:51:51 @agent_ppo2.py:185][0m |          -0.0155 |          24.3839 |          15.4505 |
[32m[20221213 22:51:51 @agent_ppo2.py:185][0m |          -0.0148 |          24.4961 |          15.4364 |
[32m[20221213 22:51:52 @agent_ppo2.py:185][0m |          -0.0139 |          23.9593 |          15.4456 |
[32m[20221213 22:51:52 @agent_ppo2.py:185][0m |          -0.0118 |          23.5694 |          15.4309 |
[32m[20221213 22:51:52 @agent_ppo2.py:185][0m |          -0.0127 |          23.4292 |          15.4359 |
[32m[20221213 22:51:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.29
[32m[20221213 22:51:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.12
[32m[20221213 22:51:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.32
[32m[20221213 22:51:52 @agent_ppo2.py:143][0m Total time:      33.65 min
[32m[20221213 22:51:52 @agent_ppo2.py:145][0m 3287040 total steps have happened
[32m[20221213 22:51:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1605 --------------------------#
[32m[20221213 22:51:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:52 @agent_ppo2.py:185][0m |           0.0003 |          38.3396 |          15.6299 |
[32m[20221213 22:51:52 @agent_ppo2.py:185][0m |          -0.0020 |          36.1137 |          15.5998 |
[32m[20221213 22:51:52 @agent_ppo2.py:185][0m |          -0.0096 |          35.4913 |          15.5957 |
[32m[20221213 22:51:52 @agent_ppo2.py:185][0m |          -0.0091 |          35.1515 |          15.5826 |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |          -0.0036 |          35.3455 |          15.5749 |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |          -0.0123 |          34.6544 |          15.5623 |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |          -0.0047 |          35.4459 |          15.5525 |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |          -0.0132 |          34.4848 |          15.5296 |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |          -0.0141 |          34.1859 |          15.5497 |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |          -0.0137 |          34.1116 |          15.5405 |
[32m[20221213 22:51:53 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.72
[32m[20221213 22:51:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.58
[32m[20221213 22:51:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.76
[32m[20221213 22:51:53 @agent_ppo2.py:143][0m Total time:      33.67 min
[32m[20221213 22:51:53 @agent_ppo2.py:145][0m 3289088 total steps have happened
[32m[20221213 22:51:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1606 --------------------------#
[32m[20221213 22:51:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |           0.0010 |          34.9883 |          15.5267 |
[32m[20221213 22:51:53 @agent_ppo2.py:185][0m |          -0.0070 |          32.7204 |          15.5241 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0105 |          31.8755 |          15.5178 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0094 |          31.4052 |          15.5197 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0158 |          31.0891 |          15.5158 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0129 |          30.6561 |          15.5088 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0127 |          30.3589 |          15.5141 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0178 |          30.1362 |          15.5241 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0168 |          29.9632 |          15.5236 |
[32m[20221213 22:51:54 @agent_ppo2.py:185][0m |          -0.0075 |          31.1565 |          15.5167 |
[32m[20221213 22:51:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.51
[32m[20221213 22:51:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.05
[32m[20221213 22:51:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.30
[32m[20221213 22:51:54 @agent_ppo2.py:143][0m Total time:      33.69 min
[32m[20221213 22:51:54 @agent_ppo2.py:145][0m 3291136 total steps have happened
[32m[20221213 22:51:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1607 --------------------------#
[32m[20221213 22:51:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |           0.0058 |          40.5414 |          15.4702 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |           0.0019 |          37.9934 |          15.4776 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0005 |          37.9529 |          15.4696 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0058 |          37.0063 |          15.4619 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0037 |          37.1049 |          15.4508 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0081 |          36.6124 |          15.4271 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0081 |          36.2156 |          15.4255 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0116 |          36.4135 |          15.4245 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0068 |          36.0917 |          15.4205 |
[32m[20221213 22:51:55 @agent_ppo2.py:185][0m |          -0.0116 |          35.8751 |          15.3834 |
[32m[20221213 22:51:55 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:51:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.69
[32m[20221213 22:51:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.56
[32m[20221213 22:51:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.67
[32m[20221213 22:51:56 @agent_ppo2.py:143][0m Total time:      33.71 min
[32m[20221213 22:51:56 @agent_ppo2.py:145][0m 3293184 total steps have happened
[32m[20221213 22:51:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1608 --------------------------#
[32m[20221213 22:51:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0016 |          44.7225 |          15.4857 |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0030 |          43.3923 |          15.4733 |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0069 |          42.7213 |          15.4762 |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0007 |          43.6189 |          15.4858 |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0087 |          41.8586 |          15.4799 |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0081 |          41.7114 |          15.4878 |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0091 |          41.4334 |          15.4862 |
[32m[20221213 22:51:56 @agent_ppo2.py:185][0m |          -0.0012 |          44.5113 |          15.4846 |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0119 |          41.5194 |          15.4942 |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0098 |          41.0293 |          15.4932 |
[32m[20221213 22:51:57 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:51:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.87
[32m[20221213 22:51:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.12
[32m[20221213 22:51:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.19
[32m[20221213 22:51:57 @agent_ppo2.py:143][0m Total time:      33.73 min
[32m[20221213 22:51:57 @agent_ppo2.py:145][0m 3295232 total steps have happened
[32m[20221213 22:51:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1609 --------------------------#
[32m[20221213 22:51:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0009 |          38.4721 |          15.5505 |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0050 |          34.5106 |          15.5324 |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0111 |          33.2927 |          15.5353 |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0099 |          32.4837 |          15.5243 |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0107 |          31.7123 |          15.5312 |
[32m[20221213 22:51:57 @agent_ppo2.py:185][0m |          -0.0127 |          31.1149 |          15.5251 |
[32m[20221213 22:51:58 @agent_ppo2.py:185][0m |          -0.0112 |          30.9065 |          15.5392 |
[32m[20221213 22:51:58 @agent_ppo2.py:185][0m |          -0.0114 |          30.4613 |          15.5093 |
[32m[20221213 22:51:58 @agent_ppo2.py:185][0m |          -0.0112 |          30.1049 |          15.5237 |
[32m[20221213 22:51:58 @agent_ppo2.py:185][0m |          -0.0166 |          29.8235 |          15.5243 |
[32m[20221213 22:51:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:51:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.70
[32m[20221213 22:51:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.69
[32m[20221213 22:51:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 326.09
[32m[20221213 22:51:58 @agent_ppo2.py:143][0m Total time:      33.75 min
[32m[20221213 22:51:58 @agent_ppo2.py:145][0m 3297280 total steps have happened
[32m[20221213 22:51:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1610 --------------------------#
[32m[20221213 22:51:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:51:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:51:58 @agent_ppo2.py:185][0m |           0.0017 |          49.1174 |          15.5484 |
[32m[20221213 22:51:58 @agent_ppo2.py:185][0m |          -0.0027 |          46.3004 |          15.5359 |
[32m[20221213 22:51:58 @agent_ppo2.py:185][0m |          -0.0093 |          45.4143 |          15.5219 |
[32m[20221213 22:51:59 @agent_ppo2.py:185][0m |          -0.0081 |          44.9333 |          15.5275 |
[32m[20221213 22:51:59 @agent_ppo2.py:185][0m |          -0.0086 |          44.6337 |          15.5251 |
[32m[20221213 22:51:59 @agent_ppo2.py:185][0m |          -0.0059 |          45.5735 |          15.5278 |
[32m[20221213 22:51:59 @agent_ppo2.py:185][0m |          -0.0061 |          44.4265 |          15.5245 |
[32m[20221213 22:51:59 @agent_ppo2.py:185][0m |          -0.0091 |          43.7909 |          15.5079 |
[32m[20221213 22:51:59 @agent_ppo2.py:185][0m |          -0.0121 |          43.6616 |          15.5097 |
[32m[20221213 22:51:59 @agent_ppo2.py:185][0m |          -0.0100 |          43.3802 |          15.5069 |
[32m[20221213 22:51:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:51:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.25
[32m[20221213 22:51:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.58
[32m[20221213 22:51:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.99
[32m[20221213 22:51:59 @agent_ppo2.py:143][0m Total time:      33.77 min
[32m[20221213 22:51:59 @agent_ppo2.py:145][0m 3299328 total steps have happened
[32m[20221213 22:51:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1611 --------------------------#
[32m[20221213 22:51:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:51:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |           0.0015 |          39.5358 |          15.3336 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0070 |          37.0669 |          15.3281 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0022 |          38.0621 |          15.2918 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0078 |          36.8647 |          15.2841 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0107 |          36.0957 |          15.2975 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0086 |          36.0125 |          15.2787 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0060 |          36.4078 |          15.2848 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0098 |          35.7667 |          15.2888 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0160 |          35.5852 |          15.2733 |
[32m[20221213 22:52:00 @agent_ppo2.py:185][0m |          -0.0142 |          35.5725 |          15.2653 |
[32m[20221213 22:52:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:52:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.53
[32m[20221213 22:52:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.35
[32m[20221213 22:52:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.34
[32m[20221213 22:52:00 @agent_ppo2.py:143][0m Total time:      33.79 min
[32m[20221213 22:52:00 @agent_ppo2.py:145][0m 3301376 total steps have happened
[32m[20221213 22:52:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1612 --------------------------#
[32m[20221213 22:52:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |          -0.0019 |          48.0952 |          15.5738 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |           0.0012 |          46.9378 |          15.5602 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |           0.0032 |          50.4598 |          15.5640 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |           0.0056 |          51.3659 |          15.5888 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |          -0.0101 |          45.1885 |          15.5659 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |          -0.0129 |          44.8217 |          15.5617 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |          -0.0132 |          44.6344 |          15.5693 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |          -0.0148 |          44.6441 |          15.5538 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |          -0.0044 |          46.9724 |          15.5673 |
[32m[20221213 22:52:01 @agent_ppo2.py:185][0m |          -0.0133 |          44.1457 |          15.5690 |
[32m[20221213 22:52:01 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:52:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.70
[32m[20221213 22:52:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.11
[32m[20221213 22:52:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.69
[32m[20221213 22:52:02 @agent_ppo2.py:143][0m Total time:      33.81 min
[32m[20221213 22:52:02 @agent_ppo2.py:145][0m 3303424 total steps have happened
[32m[20221213 22:52:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1613 --------------------------#
[32m[20221213 22:52:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:02 @agent_ppo2.py:185][0m |           0.0017 |          44.8804 |          15.3355 |
[32m[20221213 22:52:02 @agent_ppo2.py:185][0m |          -0.0059 |          42.1965 |          15.3336 |
[32m[20221213 22:52:02 @agent_ppo2.py:185][0m |          -0.0048 |          41.1012 |          15.3517 |
[32m[20221213 22:52:02 @agent_ppo2.py:185][0m |          -0.0016 |          41.8823 |          15.3388 |
[32m[20221213 22:52:02 @agent_ppo2.py:185][0m |          -0.0044 |          39.8978 |          15.3120 |
[32m[20221213 22:52:02 @agent_ppo2.py:185][0m |          -0.0056 |          39.9832 |          15.3464 |
[32m[20221213 22:52:02 @agent_ppo2.py:185][0m |          -0.0097 |          39.0939 |          15.3177 |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |          -0.0093 |          38.8297 |          15.3519 |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |          -0.0092 |          38.6230 |          15.3412 |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |          -0.0090 |          38.4166 |          15.3335 |
[32m[20221213 22:52:03 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.78
[32m[20221213 22:52:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.35
[32m[20221213 22:52:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.26
[32m[20221213 22:52:03 @agent_ppo2.py:143][0m Total time:      33.83 min
[32m[20221213 22:52:03 @agent_ppo2.py:145][0m 3305472 total steps have happened
[32m[20221213 22:52:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1614 --------------------------#
[32m[20221213 22:52:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |           0.0046 |          41.0586 |          15.5864 |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |           0.0005 |          38.2530 |          15.5532 |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |          -0.0122 |          36.7887 |          15.5526 |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |          -0.0067 |          36.4021 |          15.5541 |
[32m[20221213 22:52:03 @agent_ppo2.py:185][0m |          -0.0111 |          35.9382 |          15.5624 |
[32m[20221213 22:52:04 @agent_ppo2.py:185][0m |          -0.0079 |          35.6667 |          15.5553 |
[32m[20221213 22:52:04 @agent_ppo2.py:185][0m |          -0.0088 |          35.4707 |          15.5523 |
[32m[20221213 22:52:04 @agent_ppo2.py:185][0m |          -0.0073 |          35.2378 |          15.5617 |
[32m[20221213 22:52:04 @agent_ppo2.py:185][0m |          -0.0147 |          35.0485 |          15.5694 |
[32m[20221213 22:52:04 @agent_ppo2.py:185][0m |          -0.0043 |          35.3450 |          15.5652 |
[32m[20221213 22:52:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.30
[32m[20221213 22:52:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.03
[32m[20221213 22:52:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.91
[32m[20221213 22:52:04 @agent_ppo2.py:143][0m Total time:      33.85 min
[32m[20221213 22:52:04 @agent_ppo2.py:145][0m 3307520 total steps have happened
[32m[20221213 22:52:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1615 --------------------------#
[32m[20221213 22:52:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:04 @agent_ppo2.py:185][0m |          -0.0021 |          41.3988 |          15.4629 |
[32m[20221213 22:52:04 @agent_ppo2.py:185][0m |           0.0038 |          37.6726 |          15.4681 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |          -0.0060 |          35.9407 |          15.4774 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |          -0.0057 |          35.3523 |          15.4437 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |          -0.0046 |          36.1013 |          15.4525 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |           0.0012 |          34.2964 |          15.4490 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |          -0.0024 |          36.2779 |          15.4419 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |          -0.0117 |          33.5970 |          15.4448 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |          -0.0130 |          33.2788 |          15.4187 |
[32m[20221213 22:52:05 @agent_ppo2.py:185][0m |          -0.0126 |          33.2164 |          15.4252 |
[32m[20221213 22:52:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:52:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.30
[32m[20221213 22:52:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.65
[32m[20221213 22:52:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.49
[32m[20221213 22:52:05 @agent_ppo2.py:143][0m Total time:      33.87 min
[32m[20221213 22:52:05 @agent_ppo2.py:145][0m 3309568 total steps have happened
[32m[20221213 22:52:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1616 --------------------------#
[32m[20221213 22:52:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |           0.0021 |          38.3703 |          15.5973 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0024 |          33.5578 |          15.5941 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0067 |          32.1684 |          15.6073 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0021 |          31.6916 |          15.6133 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0112 |          31.3535 |          15.5843 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0058 |          31.2654 |          15.5961 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0092 |          30.3554 |          15.6217 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0115 |          30.1836 |          15.6248 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0134 |          29.9437 |          15.6108 |
[32m[20221213 22:52:06 @agent_ppo2.py:185][0m |          -0.0044 |          31.9617 |          15.6071 |
[32m[20221213 22:52:06 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.83
[32m[20221213 22:52:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.12
[32m[20221213 22:52:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.44
[32m[20221213 22:52:06 @agent_ppo2.py:143][0m Total time:      33.89 min
[32m[20221213 22:52:06 @agent_ppo2.py:145][0m 3311616 total steps have happened
[32m[20221213 22:52:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1617 --------------------------#
[32m[20221213 22:52:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0001 |          41.3202 |          15.2904 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0051 |          38.3878 |          15.2966 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0069 |          37.3161 |          15.3026 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0041 |          37.3026 |          15.2869 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0115 |          36.4038 |          15.2925 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0004 |          37.9956 |          15.2894 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0145 |          35.8108 |          15.2765 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0073 |          35.6863 |          15.2900 |
[32m[20221213 22:52:07 @agent_ppo2.py:185][0m |          -0.0101 |          35.4222 |          15.2872 |
[32m[20221213 22:52:08 @agent_ppo2.py:185][0m |          -0.0105 |          35.1003 |          15.2661 |
[32m[20221213 22:52:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 307.45
[32m[20221213 22:52:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.32
[32m[20221213 22:52:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.57
[32m[20221213 22:52:08 @agent_ppo2.py:143][0m Total time:      33.91 min
[32m[20221213 22:52:08 @agent_ppo2.py:145][0m 3313664 total steps have happened
[32m[20221213 22:52:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1618 --------------------------#
[32m[20221213 22:52:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:08 @agent_ppo2.py:185][0m |           0.0004 |          36.6163 |          15.3460 |
[32m[20221213 22:52:08 @agent_ppo2.py:185][0m |          -0.0039 |          34.2440 |          15.3500 |
[32m[20221213 22:52:08 @agent_ppo2.py:185][0m |           0.0045 |          34.1539 |          15.3251 |
[32m[20221213 22:52:08 @agent_ppo2.py:185][0m |          -0.0055 |          32.7040 |          15.3071 |
[32m[20221213 22:52:08 @agent_ppo2.py:185][0m |          -0.0094 |          32.1802 |          15.3010 |
[32m[20221213 22:52:08 @agent_ppo2.py:185][0m |          -0.0108 |          31.8640 |          15.2934 |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |          -0.0106 |          31.6611 |          15.2811 |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |          -0.0190 |          31.5349 |          15.2662 |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |          -0.0114 |          31.2268 |          15.2675 |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |          -0.0097 |          30.9502 |          15.2465 |
[32m[20221213 22:52:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.95
[32m[20221213 22:52:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.51
[32m[20221213 22:52:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.32
[32m[20221213 22:52:09 @agent_ppo2.py:143][0m Total time:      33.93 min
[32m[20221213 22:52:09 @agent_ppo2.py:145][0m 3315712 total steps have happened
[32m[20221213 22:52:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1619 --------------------------#
[32m[20221213 22:52:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |           0.0158 |          55.3462 |          15.3130 |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |          -0.0048 |          45.7701 |          15.2911 |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |          -0.0043 |          44.2752 |          15.2980 |
[32m[20221213 22:52:09 @agent_ppo2.py:185][0m |          -0.0070 |          43.3340 |          15.2809 |
[32m[20221213 22:52:10 @agent_ppo2.py:185][0m |          -0.0067 |          42.7721 |          15.2842 |
[32m[20221213 22:52:10 @agent_ppo2.py:185][0m |          -0.0109 |          42.2026 |          15.2719 |
[32m[20221213 22:52:10 @agent_ppo2.py:185][0m |          -0.0099 |          42.3612 |          15.2720 |
[32m[20221213 22:52:10 @agent_ppo2.py:185][0m |          -0.0086 |          41.5579 |          15.2648 |
[32m[20221213 22:52:10 @agent_ppo2.py:185][0m |          -0.0065 |          42.5942 |          15.2707 |
[32m[20221213 22:52:10 @agent_ppo2.py:185][0m |          -0.0105 |          41.0881 |          15.2554 |
[32m[20221213 22:52:10 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:52:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.60
[32m[20221213 22:52:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 317.47
[32m[20221213 22:52:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.67
[32m[20221213 22:52:10 @agent_ppo2.py:143][0m Total time:      33.95 min
[32m[20221213 22:52:10 @agent_ppo2.py:145][0m 3317760 total steps have happened
[32m[20221213 22:52:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1620 --------------------------#
[32m[20221213 22:52:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:52:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:10 @agent_ppo2.py:185][0m |          -0.0011 |          43.4440 |          15.3453 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0078 |          38.9536 |          15.3554 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0023 |          37.9270 |          15.3510 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0075 |          37.4150 |          15.3403 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0092 |          36.8655 |          15.3517 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |           0.0038 |          38.9637 |          15.3549 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0037 |          36.5838 |          15.3543 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0174 |          36.4149 |          15.3457 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0098 |          36.2886 |          15.3431 |
[32m[20221213 22:52:11 @agent_ppo2.py:185][0m |          -0.0119 |          36.2094 |          15.3481 |
[32m[20221213 22:52:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.32
[32m[20221213 22:52:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.59
[32m[20221213 22:52:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.19
[32m[20221213 22:52:11 @agent_ppo2.py:143][0m Total time:      33.97 min
[32m[20221213 22:52:11 @agent_ppo2.py:145][0m 3319808 total steps have happened
[32m[20221213 22:52:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1621 --------------------------#
[32m[20221213 22:52:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |           0.0015 |          35.3158 |          15.4313 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0018 |          31.6790 |          15.4165 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0032 |          30.6867 |          15.4297 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0090 |          30.1691 |          15.4074 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0091 |          29.7012 |          15.3967 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0053 |          29.4223 |          15.3793 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0054 |          29.1292 |          15.3852 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0083 |          29.0304 |          15.3639 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0120 |          28.7894 |          15.3534 |
[32m[20221213 22:52:12 @agent_ppo2.py:185][0m |          -0.0107 |          28.6838 |          15.3402 |
[32m[20221213 22:52:12 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.72
[32m[20221213 22:52:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.70
[32m[20221213 22:52:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.52
[32m[20221213 22:52:13 @agent_ppo2.py:143][0m Total time:      33.99 min
[32m[20221213 22:52:13 @agent_ppo2.py:145][0m 3321856 total steps have happened
[32m[20221213 22:52:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1622 --------------------------#
[32m[20221213 22:52:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0016 |          36.3099 |          15.3957 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0074 |          32.9858 |          15.3949 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0023 |          32.5877 |          15.3860 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0092 |          31.9231 |          15.3864 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0053 |          32.0283 |          15.4075 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0124 |          30.8868 |          15.3980 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0135 |          30.6679 |          15.4098 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0105 |          30.1892 |          15.4191 |
[32m[20221213 22:52:13 @agent_ppo2.py:185][0m |          -0.0090 |          30.1539 |          15.4191 |
[32m[20221213 22:52:14 @agent_ppo2.py:185][0m |          -0.0096 |          29.9217 |          15.4167 |
[32m[20221213 22:52:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.98
[32m[20221213 22:52:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 347.90
[32m[20221213 22:52:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.09
[32m[20221213 22:52:14 @agent_ppo2.py:143][0m Total time:      34.01 min
[32m[20221213 22:52:14 @agent_ppo2.py:145][0m 3323904 total steps have happened
[32m[20221213 22:52:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1623 --------------------------#
[32m[20221213 22:52:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:14 @agent_ppo2.py:185][0m |           0.0042 |          43.3209 |          15.4169 |
[32m[20221213 22:52:14 @agent_ppo2.py:185][0m |          -0.0048 |          41.1320 |          15.4045 |
[32m[20221213 22:52:14 @agent_ppo2.py:185][0m |          -0.0001 |          42.1312 |          15.3930 |
[32m[20221213 22:52:14 @agent_ppo2.py:185][0m |           0.0008 |          40.7206 |          15.3781 |
[32m[20221213 22:52:14 @agent_ppo2.py:185][0m |          -0.0043 |          39.9611 |          15.3766 |
[32m[20221213 22:52:14 @agent_ppo2.py:185][0m |           0.0071 |          43.5402 |          15.3575 |
[32m[20221213 22:52:15 @agent_ppo2.py:185][0m |          -0.0023 |          39.7542 |          15.3520 |
[32m[20221213 22:52:15 @agent_ppo2.py:185][0m |          -0.0081 |          39.3327 |          15.3533 |
[32m[20221213 22:52:15 @agent_ppo2.py:185][0m |          -0.0050 |          39.4246 |          15.3338 |
[32m[20221213 22:52:15 @agent_ppo2.py:185][0m |          -0.0104 |          39.0191 |          15.3382 |
[32m[20221213 22:52:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.00
[32m[20221213 22:52:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.09
[32m[20221213 22:52:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.24
[32m[20221213 22:52:15 @agent_ppo2.py:143][0m Total time:      34.03 min
[32m[20221213 22:52:15 @agent_ppo2.py:145][0m 3325952 total steps have happened
[32m[20221213 22:52:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1624 --------------------------#
[32m[20221213 22:52:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:15 @agent_ppo2.py:185][0m |           0.0016 |          45.1094 |          15.2928 |
[32m[20221213 22:52:15 @agent_ppo2.py:185][0m |          -0.0030 |          43.8647 |          15.2985 |
[32m[20221213 22:52:15 @agent_ppo2.py:185][0m |          -0.0054 |          43.0559 |          15.2946 |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0066 |          42.7071 |          15.2896 |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0097 |          42.2585 |          15.2841 |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0095 |          42.1220 |          15.2640 |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0014 |          43.7470 |          15.2649 |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0055 |          41.8384 |          15.2268 |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0115 |          41.5200 |          15.2283 |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0111 |          41.6221 |          15.2246 |
[32m[20221213 22:52:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:52:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.20
[32m[20221213 22:52:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.73
[32m[20221213 22:52:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.96
[32m[20221213 22:52:16 @agent_ppo2.py:143][0m Total time:      34.05 min
[32m[20221213 22:52:16 @agent_ppo2.py:145][0m 3328000 total steps have happened
[32m[20221213 22:52:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1625 --------------------------#
[32m[20221213 22:52:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:16 @agent_ppo2.py:185][0m |          -0.0001 |          48.6690 |          15.2735 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0046 |          44.1195 |          15.2773 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0067 |          42.9505 |          15.2649 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0050 |          41.5363 |          15.2190 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0036 |          41.2413 |          15.2383 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0061 |          40.4952 |          15.2356 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0097 |          39.6974 |          15.2184 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0101 |          39.4347 |          15.2090 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0119 |          38.8158 |          15.2011 |
[32m[20221213 22:52:17 @agent_ppo2.py:185][0m |          -0.0068 |          39.7649 |          15.1897 |
[32m[20221213 22:52:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:52:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.93
[32m[20221213 22:52:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.10
[32m[20221213 22:52:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.18
[32m[20221213 22:52:17 @agent_ppo2.py:143][0m Total time:      34.07 min
[32m[20221213 22:52:17 @agent_ppo2.py:145][0m 3330048 total steps have happened
[32m[20221213 22:52:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1626 --------------------------#
[32m[20221213 22:52:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0025 |          42.7184 |          15.2206 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0070 |          39.7056 |          15.2240 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0053 |          39.2649 |          15.2065 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0080 |          38.2509 |          15.2090 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0031 |          38.7077 |          15.1761 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0154 |          35.8694 |          15.1878 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0126 |          35.3698 |          15.1943 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0137 |          34.6940 |          15.1803 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0182 |          34.1654 |          15.1763 |
[32m[20221213 22:52:18 @agent_ppo2.py:185][0m |          -0.0155 |          33.7473 |          15.1619 |
[32m[20221213 22:52:18 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.03
[32m[20221213 22:52:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.97
[32m[20221213 22:52:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.79
[32m[20221213 22:52:19 @agent_ppo2.py:143][0m Total time:      34.09 min
[32m[20221213 22:52:19 @agent_ppo2.py:145][0m 3332096 total steps have happened
[32m[20221213 22:52:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1627 --------------------------#
[32m[20221213 22:52:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |           0.0040 |          45.8654 |          15.3167 |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |           0.0002 |          44.2587 |          15.3022 |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |          -0.0071 |          44.0644 |          15.3175 |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |           0.0064 |          46.6640 |          15.3413 |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |          -0.0018 |          44.1718 |          15.3143 |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |          -0.0051 |          43.4373 |          15.3396 |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |          -0.0115 |          43.2911 |          15.3542 |
[32m[20221213 22:52:19 @agent_ppo2.py:185][0m |          -0.0117 |          43.1116 |          15.3609 |
[32m[20221213 22:52:20 @agent_ppo2.py:185][0m |          -0.0112 |          43.1511 |          15.3711 |
[32m[20221213 22:52:20 @agent_ppo2.py:185][0m |          -0.0114 |          42.9278 |          15.3798 |
[32m[20221213 22:52:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.13
[32m[20221213 22:52:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.60
[32m[20221213 22:52:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.60
[32m[20221213 22:52:20 @agent_ppo2.py:143][0m Total time:      34.11 min
[32m[20221213 22:52:20 @agent_ppo2.py:145][0m 3334144 total steps have happened
[32m[20221213 22:52:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1628 --------------------------#
[32m[20221213 22:52:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:20 @agent_ppo2.py:185][0m |          -0.0032 |          39.3539 |          15.2409 |
[32m[20221213 22:52:20 @agent_ppo2.py:185][0m |          -0.0022 |          38.6553 |          15.2468 |
[32m[20221213 22:52:20 @agent_ppo2.py:185][0m |          -0.0031 |          37.6706 |          15.2402 |
[32m[20221213 22:52:20 @agent_ppo2.py:185][0m |          -0.0031 |          37.3284 |          15.2507 |
[32m[20221213 22:52:20 @agent_ppo2.py:185][0m |          -0.0068 |          36.9197 |          15.2468 |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0091 |          36.5227 |          15.2544 |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0041 |          36.4604 |          15.2496 |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0104 |          36.1994 |          15.2435 |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0097 |          36.0975 |          15.2657 |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0062 |          35.9599 |          15.2611 |
[32m[20221213 22:52:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.12
[32m[20221213 22:52:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.07
[32m[20221213 22:52:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.54
[32m[20221213 22:52:21 @agent_ppo2.py:143][0m Total time:      34.13 min
[32m[20221213 22:52:21 @agent_ppo2.py:145][0m 3336192 total steps have happened
[32m[20221213 22:52:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1629 --------------------------#
[32m[20221213 22:52:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0011 |          44.4784 |          15.3589 |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0085 |          42.1909 |          15.3351 |
[32m[20221213 22:52:21 @agent_ppo2.py:185][0m |          -0.0056 |          41.4691 |          15.3250 |
[32m[20221213 22:52:22 @agent_ppo2.py:185][0m |          -0.0083 |          41.2669 |          15.3357 |
[32m[20221213 22:52:22 @agent_ppo2.py:185][0m |          -0.0108 |          40.9985 |          15.3306 |
[32m[20221213 22:52:22 @agent_ppo2.py:185][0m |          -0.0124 |          40.7242 |          15.3297 |
[32m[20221213 22:52:22 @agent_ppo2.py:185][0m |          -0.0108 |          40.6394 |          15.3221 |
[32m[20221213 22:52:22 @agent_ppo2.py:185][0m |          -0.0107 |          40.5542 |          15.3270 |
[32m[20221213 22:52:22 @agent_ppo2.py:185][0m |          -0.0104 |          40.3721 |          15.3248 |
[32m[20221213 22:52:22 @agent_ppo2.py:185][0m |          -0.0149 |          40.2348 |          15.3277 |
[32m[20221213 22:52:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:52:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.34
[32m[20221213 22:52:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.90
[32m[20221213 22:52:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.09
[32m[20221213 22:52:22 @agent_ppo2.py:143][0m Total time:      34.15 min
[32m[20221213 22:52:22 @agent_ppo2.py:145][0m 3338240 total steps have happened
[32m[20221213 22:52:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1630 --------------------------#
[32m[20221213 22:52:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:52:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |           0.0007 |          39.1545 |          15.3389 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |          -0.0056 |          34.4849 |          15.3099 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |           0.0041 |          37.9975 |          15.2921 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |          -0.0099 |          31.1323 |          15.2742 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |          -0.0019 |          31.7016 |          15.2697 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |          -0.0068 |          29.5873 |          15.2649 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |          -0.0124 |          28.9270 |          15.2508 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |           0.0059 |          31.6263 |          15.2447 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |          -0.0122 |          28.4799 |          15.2395 |
[32m[20221213 22:52:23 @agent_ppo2.py:185][0m |          -0.0090 |          27.9560 |          15.2530 |
[32m[20221213 22:52:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:52:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.42
[32m[20221213 22:52:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.70
[32m[20221213 22:52:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.91
[32m[20221213 22:52:23 @agent_ppo2.py:143][0m Total time:      34.18 min
[32m[20221213 22:52:23 @agent_ppo2.py:145][0m 3340288 total steps have happened
[32m[20221213 22:52:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1631 --------------------------#
[32m[20221213 22:52:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0034 |          48.5507 |          15.2839 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0068 |          46.6412 |          15.2926 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0087 |          45.9155 |          15.3044 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0105 |          45.4166 |          15.3120 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |           0.0018 |          48.1627 |          15.2974 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0098 |          44.9760 |          15.3391 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0072 |          44.7237 |          15.3487 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0104 |          44.5581 |          15.3208 |
[32m[20221213 22:52:24 @agent_ppo2.py:185][0m |          -0.0062 |          44.9255 |          15.3346 |
[32m[20221213 22:52:25 @agent_ppo2.py:185][0m |          -0.0147 |          44.4500 |          15.3354 |
[32m[20221213 22:52:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:52:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.79
[32m[20221213 22:52:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.44
[32m[20221213 22:52:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.56
[32m[20221213 22:52:25 @agent_ppo2.py:143][0m Total time:      34.20 min
[32m[20221213 22:52:25 @agent_ppo2.py:145][0m 3342336 total steps have happened
[32m[20221213 22:52:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1632 --------------------------#
[32m[20221213 22:52:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:25 @agent_ppo2.py:185][0m |          -0.0044 |          35.0207 |          15.1264 |
[32m[20221213 22:52:25 @agent_ppo2.py:185][0m |          -0.0062 |          26.0336 |          15.0911 |
[32m[20221213 22:52:25 @agent_ppo2.py:185][0m |          -0.0038 |          26.1241 |          15.0898 |
[32m[20221213 22:52:25 @agent_ppo2.py:185][0m |          -0.0049 |          23.9108 |          15.0774 |
[32m[20221213 22:52:25 @agent_ppo2.py:185][0m |          -0.0148 |          23.3600 |          15.0789 |
[32m[20221213 22:52:25 @agent_ppo2.py:185][0m |          -0.0084 |          22.9412 |          15.0909 |
[32m[20221213 22:52:26 @agent_ppo2.py:185][0m |          -0.0098 |          22.6956 |          15.0897 |
[32m[20221213 22:52:26 @agent_ppo2.py:185][0m |          -0.0111 |          22.2816 |          15.1234 |
[32m[20221213 22:52:26 @agent_ppo2.py:185][0m |          -0.0109 |          22.7633 |          15.1060 |
[32m[20221213 22:52:26 @agent_ppo2.py:185][0m |          -0.0162 |          22.1273 |          15.1104 |
[32m[20221213 22:52:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:52:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.55
[32m[20221213 22:52:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.48
[32m[20221213 22:52:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.63
[32m[20221213 22:52:26 @agent_ppo2.py:143][0m Total time:      34.22 min
[32m[20221213 22:52:26 @agent_ppo2.py:145][0m 3344384 total steps have happened
[32m[20221213 22:52:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1633 --------------------------#
[32m[20221213 22:52:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:26 @agent_ppo2.py:185][0m |          -0.0011 |          28.9140 |          15.6642 |
[32m[20221213 22:52:26 @agent_ppo2.py:185][0m |          -0.0064 |          25.3688 |          15.6435 |
[32m[20221213 22:52:26 @agent_ppo2.py:185][0m |          -0.0056 |          24.5885 |          15.6476 |
[32m[20221213 22:52:27 @agent_ppo2.py:185][0m |          -0.0057 |          24.1331 |          15.6331 |
[32m[20221213 22:52:27 @agent_ppo2.py:185][0m |          -0.0081 |          23.8347 |          15.6312 |
[32m[20221213 22:52:27 @agent_ppo2.py:185][0m |          -0.0079 |          23.5417 |          15.6454 |
[32m[20221213 22:52:27 @agent_ppo2.py:185][0m |          -0.0055 |          23.3680 |          15.6409 |
[32m[20221213 22:52:27 @agent_ppo2.py:185][0m |          -0.0100 |          23.1784 |          15.6344 |
[32m[20221213 22:52:27 @agent_ppo2.py:185][0m |          -0.0095 |          23.1923 |          15.6380 |
[32m[20221213 22:52:27 @agent_ppo2.py:185][0m |          -0.0134 |          22.8549 |          15.6118 |
[32m[20221213 22:52:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:52:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.83
[32m[20221213 22:52:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.95
[32m[20221213 22:52:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.66
[32m[20221213 22:52:27 @agent_ppo2.py:143][0m Total time:      34.24 min
[32m[20221213 22:52:27 @agent_ppo2.py:145][0m 3346432 total steps have happened
[32m[20221213 22:52:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1634 --------------------------#
[32m[20221213 22:52:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |           0.0005 |          45.8619 |          15.2568 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |           0.0052 |          47.1964 |          15.2400 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0009 |          42.2958 |          15.2081 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0067 |          41.9387 |          15.2061 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0054 |          41.6844 |          15.2090 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0111 |          41.3860 |          15.2035 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0066 |          41.1721 |          15.1965 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0092 |          41.3877 |          15.1969 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0091 |          40.9737 |          15.1680 |
[32m[20221213 22:52:28 @agent_ppo2.py:185][0m |          -0.0128 |          40.7423 |          15.1712 |
[32m[20221213 22:52:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:52:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.53
[32m[20221213 22:52:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.97
[32m[20221213 22:52:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.84
[32m[20221213 22:52:29 @agent_ppo2.py:143][0m Total time:      34.26 min
[32m[20221213 22:52:29 @agent_ppo2.py:145][0m 3348480 total steps have happened
[32m[20221213 22:52:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1635 --------------------------#
[32m[20221213 22:52:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0008 |          39.4655 |          15.2394 |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0057 |          35.3907 |          15.2362 |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0062 |          33.7145 |          15.2541 |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0075 |          33.1579 |          15.2617 |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0086 |          32.6502 |          15.2425 |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0028 |          32.7150 |          15.2360 |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0072 |          32.2425 |          15.2454 |
[32m[20221213 22:52:29 @agent_ppo2.py:185][0m |          -0.0078 |          31.8604 |          15.2446 |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |          -0.0042 |          33.7207 |          15.2465 |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |          -0.0081 |          31.7701 |          15.2478 |
[32m[20221213 22:52:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:52:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.49
[32m[20221213 22:52:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.76
[32m[20221213 22:52:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.49
[32m[20221213 22:52:30 @agent_ppo2.py:143][0m Total time:      34.28 min
[32m[20221213 22:52:30 @agent_ppo2.py:145][0m 3350528 total steps have happened
[32m[20221213 22:52:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1636 --------------------------#
[32m[20221213 22:52:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |           0.0131 |          47.5718 |          15.3765 |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |           0.0090 |          42.3127 |          15.3529 |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |          -0.0046 |          36.7928 |          15.3492 |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |          -0.0093 |          35.9091 |          15.3487 |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |           0.0055 |          39.5296 |          15.3393 |
[32m[20221213 22:52:30 @agent_ppo2.py:185][0m |           0.0004 |          37.6095 |          15.3418 |
[32m[20221213 22:52:31 @agent_ppo2.py:185][0m |          -0.0096 |          34.9088 |          15.3321 |
[32m[20221213 22:52:31 @agent_ppo2.py:185][0m |          -0.0095 |          34.7536 |          15.3350 |
[32m[20221213 22:52:31 @agent_ppo2.py:185][0m |          -0.0107 |          34.5207 |          15.3305 |
[32m[20221213 22:52:31 @agent_ppo2.py:185][0m |          -0.0076 |          34.3880 |          15.3476 |
[32m[20221213 22:52:31 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:52:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.59
[32m[20221213 22:52:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.91
[32m[20221213 22:52:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.62
[32m[20221213 22:52:31 @agent_ppo2.py:143][0m Total time:      34.30 min
[32m[20221213 22:52:31 @agent_ppo2.py:145][0m 3352576 total steps have happened
[32m[20221213 22:52:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1637 --------------------------#
[32m[20221213 22:52:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:31 @agent_ppo2.py:185][0m |          -0.0036 |          43.5565 |          15.2344 |
[32m[20221213 22:52:31 @agent_ppo2.py:185][0m |           0.0100 |          42.1287 |          15.2183 |
[32m[20221213 22:52:31 @agent_ppo2.py:185][0m |          -0.0040 |          39.2598 |          15.2257 |
[32m[20221213 22:52:32 @agent_ppo2.py:185][0m |          -0.0063 |          38.7477 |          15.2198 |
[32m[20221213 22:52:32 @agent_ppo2.py:185][0m |          -0.0066 |          37.9538 |          15.1931 |
[32m[20221213 22:52:32 @agent_ppo2.py:185][0m |          -0.0032 |          39.1228 |          15.2075 |
[32m[20221213 22:52:32 @agent_ppo2.py:185][0m |          -0.0078 |          37.0551 |          15.2007 |
[32m[20221213 22:52:32 @agent_ppo2.py:185][0m |          -0.0045 |          37.3677 |          15.1930 |
[32m[20221213 22:52:32 @agent_ppo2.py:185][0m |          -0.0113 |          36.5972 |          15.2065 |
[32m[20221213 22:52:32 @agent_ppo2.py:185][0m |          -0.0123 |          36.3688 |          15.1946 |
[32m[20221213 22:52:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.92
[32m[20221213 22:52:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.35
[32m[20221213 22:52:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.42
[32m[20221213 22:52:32 @agent_ppo2.py:143][0m Total time:      34.32 min
[32m[20221213 22:52:32 @agent_ppo2.py:145][0m 3354624 total steps have happened
[32m[20221213 22:52:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1638 --------------------------#
[32m[20221213 22:52:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |           0.0017 |          41.3257 |          15.4143 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0058 |          39.1016 |          15.4196 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0017 |          37.8426 |          15.4161 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0118 |          36.5117 |          15.3918 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0121 |          36.1767 |          15.4215 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0010 |          36.4133 |          15.4171 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0134 |          35.0867 |          15.4136 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0121 |          34.8407 |          15.4129 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0181 |          34.3944 |          15.4218 |
[32m[20221213 22:52:33 @agent_ppo2.py:185][0m |          -0.0135 |          34.1758 |          15.4269 |
[32m[20221213 22:52:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.17
[32m[20221213 22:52:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.60
[32m[20221213 22:52:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.71
[32m[20221213 22:52:33 @agent_ppo2.py:143][0m Total time:      34.34 min
[32m[20221213 22:52:33 @agent_ppo2.py:145][0m 3356672 total steps have happened
[32m[20221213 22:52:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1639 --------------------------#
[32m[20221213 22:52:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |           0.0003 |          41.3198 |          15.3361 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0052 |          36.4998 |          15.3235 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0107 |          35.1276 |          15.3172 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0054 |          34.5537 |          15.3330 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0041 |          34.0201 |          15.3415 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0041 |          35.4992 |          15.3134 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0125 |          33.2787 |          15.3296 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0020 |          34.1945 |          15.3456 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0130 |          32.8657 |          15.3375 |
[32m[20221213 22:52:34 @agent_ppo2.py:185][0m |          -0.0160 |          32.5363 |          15.3409 |
[32m[20221213 22:52:34 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.44
[32m[20221213 22:52:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.28
[32m[20221213 22:52:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.98
[32m[20221213 22:52:35 @agent_ppo2.py:143][0m Total time:      34.36 min
[32m[20221213 22:52:35 @agent_ppo2.py:145][0m 3358720 total steps have happened
[32m[20221213 22:52:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1640 --------------------------#
[32m[20221213 22:52:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:52:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:35 @agent_ppo2.py:185][0m |          -0.0055 |          32.4628 |          15.4862 |
[32m[20221213 22:52:35 @agent_ppo2.py:185][0m |          -0.0060 |          29.0221 |          15.4767 |
[32m[20221213 22:52:35 @agent_ppo2.py:185][0m |          -0.0094 |          28.1119 |          15.4720 |
[32m[20221213 22:52:35 @agent_ppo2.py:185][0m |          -0.0107 |          27.2875 |          15.4678 |
[32m[20221213 22:52:35 @agent_ppo2.py:185][0m |          -0.0138 |          26.8555 |          15.4663 |
[32m[20221213 22:52:35 @agent_ppo2.py:185][0m |          -0.0097 |          26.4963 |          15.4510 |
[32m[20221213 22:52:35 @agent_ppo2.py:185][0m |          -0.0151 |          26.4112 |          15.4620 |
[32m[20221213 22:52:36 @agent_ppo2.py:185][0m |          -0.0143 |          25.8292 |          15.4753 |
[32m[20221213 22:52:36 @agent_ppo2.py:185][0m |          -0.0127 |          25.5191 |          15.4593 |
[32m[20221213 22:52:36 @agent_ppo2.py:185][0m |          -0.0169 |          25.2930 |          15.4587 |
[32m[20221213 22:52:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:52:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.79
[32m[20221213 22:52:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.44
[32m[20221213 22:52:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.90
[32m[20221213 22:52:36 @agent_ppo2.py:143][0m Total time:      34.38 min
[32m[20221213 22:52:36 @agent_ppo2.py:145][0m 3360768 total steps have happened
[32m[20221213 22:52:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1641 --------------------------#
[32m[20221213 22:52:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:52:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:36 @agent_ppo2.py:185][0m |          -0.0014 |          40.6201 |          15.3672 |
[32m[20221213 22:52:36 @agent_ppo2.py:185][0m |           0.0002 |          37.9872 |          15.3838 |
[32m[20221213 22:52:36 @agent_ppo2.py:185][0m |           0.0094 |          39.5292 |          15.3750 |
[32m[20221213 22:52:36 @agent_ppo2.py:185][0m |          -0.0087 |          37.0162 |          15.4238 |
[32m[20221213 22:52:37 @agent_ppo2.py:185][0m |          -0.0070 |          36.6174 |          15.4119 |
[32m[20221213 22:52:37 @agent_ppo2.py:185][0m |          -0.0054 |          36.5911 |          15.4198 |
[32m[20221213 22:52:37 @agent_ppo2.py:185][0m |          -0.0091 |          36.1003 |          15.4530 |
[32m[20221213 22:52:37 @agent_ppo2.py:185][0m |          -0.0103 |          35.9832 |          15.4253 |
[32m[20221213 22:52:37 @agent_ppo2.py:185][0m |           0.0065 |          39.7748 |          15.4510 |
[32m[20221213 22:52:37 @agent_ppo2.py:185][0m |          -0.0126 |          35.6314 |          15.4614 |
[32m[20221213 22:52:37 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:52:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.07
[32m[20221213 22:52:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.90
[32m[20221213 22:52:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.62
[32m[20221213 22:52:37 @agent_ppo2.py:143][0m Total time:      34.40 min
[32m[20221213 22:52:37 @agent_ppo2.py:145][0m 3362816 total steps have happened
[32m[20221213 22:52:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1642 --------------------------#
[32m[20221213 22:52:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:52:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0013 |          45.8996 |          15.2471 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0064 |          42.4187 |          15.2229 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0103 |          40.7936 |          15.2377 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |           0.0039 |          41.4378 |          15.2404 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0089 |          39.4889 |          15.2456 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0119 |          39.0143 |          15.2538 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0109 |          38.7068 |          15.2439 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0105 |          38.3082 |          15.2479 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0152 |          38.0584 |          15.2631 |
[32m[20221213 22:52:38 @agent_ppo2.py:185][0m |          -0.0151 |          38.0633 |          15.2614 |
[32m[20221213 22:52:38 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 22:52:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.68
[32m[20221213 22:52:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.61
[32m[20221213 22:52:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.70
[32m[20221213 22:52:39 @agent_ppo2.py:143][0m Total time:      34.43 min
[32m[20221213 22:52:39 @agent_ppo2.py:145][0m 3364864 total steps have happened
[32m[20221213 22:52:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1643 --------------------------#
[32m[20221213 22:52:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:52:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:39 @agent_ppo2.py:185][0m |          -0.0016 |          39.4592 |          15.2727 |
[32m[20221213 22:52:39 @agent_ppo2.py:185][0m |          -0.0065 |          34.4406 |          15.2883 |
[32m[20221213 22:52:39 @agent_ppo2.py:185][0m |           0.0037 |          36.3814 |          15.2729 |
[32m[20221213 22:52:39 @agent_ppo2.py:185][0m |          -0.0047 |          33.4773 |          15.2671 |
[32m[20221213 22:52:39 @agent_ppo2.py:185][0m |          -0.0084 |          33.1411 |          15.2871 |
[32m[20221213 22:52:39 @agent_ppo2.py:185][0m |          -0.0093 |          32.9942 |          15.2987 |
[32m[20221213 22:52:40 @agent_ppo2.py:185][0m |          -0.0053 |          32.8512 |          15.2865 |
[32m[20221213 22:52:40 @agent_ppo2.py:185][0m |          -0.0071 |          32.7273 |          15.2966 |
[32m[20221213 22:52:40 @agent_ppo2.py:185][0m |          -0.0070 |          32.6920 |          15.2858 |
[32m[20221213 22:52:40 @agent_ppo2.py:185][0m |          -0.0068 |          32.6532 |          15.2959 |
[32m[20221213 22:52:40 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 22:52:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.78
[32m[20221213 22:52:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.11
[32m[20221213 22:52:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.84
[32m[20221213 22:52:40 @agent_ppo2.py:143][0m Total time:      34.45 min
[32m[20221213 22:52:40 @agent_ppo2.py:145][0m 3366912 total steps have happened
[32m[20221213 22:52:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1644 --------------------------#
[32m[20221213 22:52:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:40 @agent_ppo2.py:185][0m |          -0.0014 |          43.5308 |          15.3778 |
[32m[20221213 22:52:40 @agent_ppo2.py:185][0m |          -0.0057 |          39.7914 |          15.3929 |
[32m[20221213 22:52:40 @agent_ppo2.py:185][0m |           0.0032 |          39.8011 |          15.3821 |
[32m[20221213 22:52:41 @agent_ppo2.py:185][0m |          -0.0098 |          37.4748 |          15.4181 |
[32m[20221213 22:52:41 @agent_ppo2.py:185][0m |          -0.0091 |          36.8672 |          15.4065 |
[32m[20221213 22:52:41 @agent_ppo2.py:185][0m |          -0.0186 |          36.5290 |          15.4407 |
[32m[20221213 22:52:41 @agent_ppo2.py:185][0m |          -0.0127 |          35.9568 |          15.4327 |
[32m[20221213 22:52:41 @agent_ppo2.py:185][0m |          -0.0087 |          35.7645 |          15.4529 |
[32m[20221213 22:52:41 @agent_ppo2.py:185][0m |          -0.0131 |          35.3423 |          15.4430 |
[32m[20221213 22:52:41 @agent_ppo2.py:185][0m |          -0.0160 |          35.1124 |          15.4497 |
[32m[20221213 22:52:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:52:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 300.54
[32m[20221213 22:52:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.76
[32m[20221213 22:52:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.91
[32m[20221213 22:52:41 @agent_ppo2.py:143][0m Total time:      34.47 min
[32m[20221213 22:52:41 @agent_ppo2.py:145][0m 3368960 total steps have happened
[32m[20221213 22:52:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1645 --------------------------#
[32m[20221213 22:52:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:52:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |           0.0023 |          40.4220 |          15.6138 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0048 |          38.1134 |          15.5865 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0117 |          37.0781 |          15.5681 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0093 |          36.5901 |          15.6042 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0091 |          36.1088 |          15.5746 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0060 |          35.7982 |          15.6007 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0089 |          35.5683 |          15.6074 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0090 |          35.5228 |          15.6189 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0136 |          35.2341 |          15.6000 |
[32m[20221213 22:52:42 @agent_ppo2.py:185][0m |          -0.0087 |          34.9807 |          15.6127 |
[32m[20221213 22:52:42 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:52:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.93
[32m[20221213 22:52:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.66
[32m[20221213 22:52:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.14
[32m[20221213 22:52:43 @agent_ppo2.py:143][0m Total time:      34.49 min
[32m[20221213 22:52:43 @agent_ppo2.py:145][0m 3371008 total steps have happened
[32m[20221213 22:52:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1646 --------------------------#
[32m[20221213 22:52:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |           0.0069 |          20.5585 |          15.5441 |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |          -0.0029 |          16.9110 |          15.5054 |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |          -0.0072 |          16.1675 |          15.5013 |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |          -0.0093 |          15.6602 |          15.5010 |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |          -0.0151 |          15.3455 |          15.4932 |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |          -0.0114 |          15.1001 |          15.4952 |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |          -0.0141 |          14.9265 |          15.4743 |
[32m[20221213 22:52:43 @agent_ppo2.py:185][0m |          -0.0107 |          14.7379 |          15.4748 |
[32m[20221213 22:52:44 @agent_ppo2.py:185][0m |          -0.0109 |          14.5855 |          15.4850 |
[32m[20221213 22:52:44 @agent_ppo2.py:185][0m |          -0.0178 |          14.4876 |          15.4677 |
[32m[20221213 22:52:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:52:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.03
[32m[20221213 22:52:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.87
[32m[20221213 22:52:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.88
[32m[20221213 22:52:44 @agent_ppo2.py:143][0m Total time:      34.52 min
[32m[20221213 22:52:44 @agent_ppo2.py:145][0m 3373056 total steps have happened
[32m[20221213 22:52:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1647 --------------------------#
[32m[20221213 22:52:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:44 @agent_ppo2.py:185][0m |           0.0008 |          40.2538 |          15.5083 |
[32m[20221213 22:52:44 @agent_ppo2.py:185][0m |          -0.0020 |          37.9963 |          15.5073 |
[32m[20221213 22:52:44 @agent_ppo2.py:185][0m |          -0.0105 |          37.0216 |          15.4916 |
[32m[20221213 22:52:44 @agent_ppo2.py:185][0m |          -0.0124 |          36.5424 |          15.4876 |
[32m[20221213 22:52:44 @agent_ppo2.py:185][0m |          -0.0045 |          36.5047 |          15.4670 |
[32m[20221213 22:52:45 @agent_ppo2.py:185][0m |          -0.0126 |          35.5803 |          15.4705 |
[32m[20221213 22:52:45 @agent_ppo2.py:185][0m |          -0.0135 |          35.2402 |          15.4720 |
[32m[20221213 22:52:45 @agent_ppo2.py:185][0m |          -0.0130 |          34.8755 |          15.4389 |
[32m[20221213 22:52:45 @agent_ppo2.py:185][0m |          -0.0122 |          34.8487 |          15.4266 |
[32m[20221213 22:52:45 @agent_ppo2.py:185][0m |          -0.0155 |          34.6203 |          15.4454 |
[32m[20221213 22:52:45 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:52:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.12
[32m[20221213 22:52:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.40
[32m[20221213 22:52:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.98
[32m[20221213 22:52:45 @agent_ppo2.py:143][0m Total time:      34.54 min
[32m[20221213 22:52:45 @agent_ppo2.py:145][0m 3375104 total steps have happened
[32m[20221213 22:52:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1648 --------------------------#
[32m[20221213 22:52:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:52:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:45 @agent_ppo2.py:185][0m |           0.0005 |          42.3411 |          15.4836 |
[32m[20221213 22:52:45 @agent_ppo2.py:185][0m |           0.0028 |          39.5708 |          15.4900 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0012 |          36.6455 |          15.4581 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0018 |          35.4052 |          15.4604 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0076 |          34.4367 |          15.4813 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0080 |          33.8669 |          15.4821 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0018 |          34.9381 |          15.4885 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0097 |          33.1731 |          15.4896 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0113 |          32.9371 |          15.4933 |
[32m[20221213 22:52:46 @agent_ppo2.py:185][0m |          -0.0045 |          32.5062 |          15.5049 |
[32m[20221213 22:52:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:52:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.74
[32m[20221213 22:52:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.37
[32m[20221213 22:52:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.93
[32m[20221213 22:52:46 @agent_ppo2.py:143][0m Total time:      34.56 min
[32m[20221213 22:52:46 @agent_ppo2.py:145][0m 3377152 total steps have happened
[32m[20221213 22:52:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1649 --------------------------#
[32m[20221213 22:52:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |           0.0015 |          24.2910 |          15.4467 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0084 |          16.8231 |          15.4542 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0055 |          15.7715 |          15.4403 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0057 |          15.8634 |          15.4530 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0087 |          14.8338 |          15.4323 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0094 |          14.6006 |          15.4147 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0026 |          15.2311 |          15.4121 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0117 |          14.0445 |          15.4005 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0131 |          13.8071 |          15.4053 |
[32m[20221213 22:52:47 @agent_ppo2.py:185][0m |          -0.0183 |          13.6015 |          15.3985 |
[32m[20221213 22:52:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:52:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.98
[32m[20221213 22:52:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.62
[32m[20221213 22:52:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.00
[32m[20221213 22:52:48 @agent_ppo2.py:143][0m Total time:      34.58 min
[32m[20221213 22:52:48 @agent_ppo2.py:145][0m 3379200 total steps have happened
[32m[20221213 22:52:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1650 --------------------------#
[32m[20221213 22:52:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:52:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:48 @agent_ppo2.py:185][0m |          -0.0017 |          31.2349 |          15.3218 |
[32m[20221213 22:52:48 @agent_ppo2.py:185][0m |          -0.0022 |          27.2888 |          15.3420 |
[32m[20221213 22:52:48 @agent_ppo2.py:185][0m |          -0.0030 |          26.9944 |          15.3331 |
[32m[20221213 22:52:48 @agent_ppo2.py:185][0m |          -0.0095 |          26.6174 |          15.3333 |
[32m[20221213 22:52:48 @agent_ppo2.py:185][0m |          -0.0045 |          26.4466 |          15.3405 |
[32m[20221213 22:52:48 @agent_ppo2.py:185][0m |          -0.0114 |          26.2755 |          15.3476 |
[32m[20221213 22:52:48 @agent_ppo2.py:185][0m |           0.0006 |          30.0816 |          15.3377 |
[32m[20221213 22:52:49 @agent_ppo2.py:185][0m |          -0.0082 |          26.7604 |          15.3421 |
[32m[20221213 22:52:49 @agent_ppo2.py:185][0m |          -0.0113 |          26.0916 |          15.3515 |
[32m[20221213 22:52:49 @agent_ppo2.py:185][0m |          -0.0115 |          26.0217 |          15.3648 |
[32m[20221213 22:52:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:52:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.45
[32m[20221213 22:52:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 387.47
[32m[20221213 22:52:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 226.86
[32m[20221213 22:52:49 @agent_ppo2.py:143][0m Total time:      34.60 min
[32m[20221213 22:52:49 @agent_ppo2.py:145][0m 3381248 total steps have happened
[32m[20221213 22:52:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1651 --------------------------#
[32m[20221213 22:52:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:52:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:49 @agent_ppo2.py:185][0m |          -0.0009 |          30.0632 |          15.5494 |
[32m[20221213 22:52:49 @agent_ppo2.py:185][0m |          -0.0089 |          25.8475 |          15.5460 |
[32m[20221213 22:52:49 @agent_ppo2.py:185][0m |          -0.0078 |          24.7691 |          15.5327 |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0096 |          23.9471 |          15.5551 |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0132 |          23.4955 |          15.5235 |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0106 |          23.1381 |          15.5381 |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0134 |          22.7572 |          15.5236 |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0067 |          23.9126 |          15.5274 |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0111 |          22.2743 |          15.5010 |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0134 |          21.8504 |          15.5023 |
[32m[20221213 22:52:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:52:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.75
[32m[20221213 22:52:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.47
[32m[20221213 22:52:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.38
[32m[20221213 22:52:50 @agent_ppo2.py:143][0m Total time:      34.62 min
[32m[20221213 22:52:50 @agent_ppo2.py:145][0m 3383296 total steps have happened
[32m[20221213 22:52:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1652 --------------------------#
[32m[20221213 22:52:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:50 @agent_ppo2.py:185][0m |          -0.0029 |          19.4469 |          15.3898 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0019 |          17.6015 |          15.3961 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0071 |          17.0491 |          15.3717 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0105 |          16.6831 |          15.3649 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0146 |          16.4315 |          15.3674 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0089 |          16.2264 |          15.3463 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0153 |          15.9977 |          15.3284 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0100 |          15.8675 |          15.3118 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0067 |          15.8316 |          15.3012 |
[32m[20221213 22:52:51 @agent_ppo2.py:185][0m |          -0.0120 |          15.6119 |          15.2967 |
[32m[20221213 22:52:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.53
[32m[20221213 22:52:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.22
[32m[20221213 22:52:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.74
[32m[20221213 22:52:51 @agent_ppo2.py:143][0m Total time:      34.64 min
[32m[20221213 22:52:51 @agent_ppo2.py:145][0m 3385344 total steps have happened
[32m[20221213 22:52:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1653 --------------------------#
[32m[20221213 22:52:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0020 |          24.9252 |          15.2526 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0096 |          22.8994 |          15.2290 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0062 |          22.1927 |          15.2539 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0118 |          21.7810 |          15.2387 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0100 |          21.7375 |          15.2401 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0111 |          21.4007 |          15.2502 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0091 |          21.2897 |          15.2447 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0122 |          21.0711 |          15.2472 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0137 |          20.8306 |          15.2473 |
[32m[20221213 22:52:52 @agent_ppo2.py:185][0m |          -0.0090 |          24.3180 |          15.2600 |
[32m[20221213 22:52:52 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.71
[32m[20221213 22:52:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.80
[32m[20221213 22:52:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 336.05
[32m[20221213 22:52:53 @agent_ppo2.py:143][0m Total time:      34.66 min
[32m[20221213 22:52:53 @agent_ppo2.py:145][0m 3387392 total steps have happened
[32m[20221213 22:52:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1654 --------------------------#
[32m[20221213 22:52:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |           0.0032 |          22.0958 |          15.4288 |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |          -0.0047 |          18.0839 |          15.4109 |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |          -0.0034 |          16.9591 |          15.4052 |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |          -0.0038 |          16.4529 |          15.3879 |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |          -0.0096 |          16.1565 |          15.3913 |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |          -0.0030 |          16.7971 |          15.3816 |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |          -0.0113 |          15.5016 |          15.3862 |
[32m[20221213 22:52:53 @agent_ppo2.py:185][0m |          -0.0110 |          15.3169 |          15.3693 |
[32m[20221213 22:52:54 @agent_ppo2.py:185][0m |          -0.0116 |          15.1190 |          15.3593 |
[32m[20221213 22:52:54 @agent_ppo2.py:185][0m |          -0.0120 |          15.0434 |          15.3409 |
[32m[20221213 22:52:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:52:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.99
[32m[20221213 22:52:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.16
[32m[20221213 22:52:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.07
[32m[20221213 22:52:54 @agent_ppo2.py:143][0m Total time:      34.68 min
[32m[20221213 22:52:54 @agent_ppo2.py:145][0m 3389440 total steps have happened
[32m[20221213 22:52:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1655 --------------------------#
[32m[20221213 22:52:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:54 @agent_ppo2.py:185][0m |           0.0097 |          25.3375 |          15.2058 |
[32m[20221213 22:52:54 @agent_ppo2.py:185][0m |          -0.0044 |          22.1947 |          15.1992 |
[32m[20221213 22:52:54 @agent_ppo2.py:185][0m |          -0.0086 |          21.5611 |          15.2133 |
[32m[20221213 22:52:54 @agent_ppo2.py:185][0m |          -0.0095 |          20.9438 |          15.1963 |
[32m[20221213 22:52:54 @agent_ppo2.py:185][0m |          -0.0105 |          20.8343 |          15.2036 |
[32m[20221213 22:52:55 @agent_ppo2.py:185][0m |          -0.0122 |          20.5392 |          15.2156 |
[32m[20221213 22:52:55 @agent_ppo2.py:185][0m |          -0.0116 |          20.5281 |          15.2156 |
[32m[20221213 22:52:55 @agent_ppo2.py:185][0m |          -0.0156 |          20.2721 |          15.1997 |
[32m[20221213 22:52:55 @agent_ppo2.py:185][0m |          -0.0156 |          20.2069 |          15.2181 |
[32m[20221213 22:52:55 @agent_ppo2.py:185][0m |          -0.0116 |          19.8277 |          15.2078 |
[32m[20221213 22:52:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:52:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.34
[32m[20221213 22:52:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.48
[32m[20221213 22:52:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.94
[32m[20221213 22:52:55 @agent_ppo2.py:143][0m Total time:      34.70 min
[32m[20221213 22:52:55 @agent_ppo2.py:145][0m 3391488 total steps have happened
[32m[20221213 22:52:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1656 --------------------------#
[32m[20221213 22:52:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:55 @agent_ppo2.py:185][0m |          -0.0025 |          15.7981 |          15.1678 |
[32m[20221213 22:52:55 @agent_ppo2.py:185][0m |          -0.0057 |          14.0084 |          15.1612 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |          -0.0112 |          13.3051 |          15.1672 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |          -0.0164 |          13.2650 |          15.1672 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |           0.0029 |          14.1239 |          15.1736 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |          -0.0114 |          12.2955 |          15.1742 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |          -0.0119 |          11.9941 |          15.1911 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |          -0.0125 |          11.9006 |          15.1913 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |          -0.0132 |          11.6828 |          15.1833 |
[32m[20221213 22:52:56 @agent_ppo2.py:185][0m |          -0.0110 |          11.6254 |          15.1959 |
[32m[20221213 22:52:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:52:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.78
[32m[20221213 22:52:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.51
[32m[20221213 22:52:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.15
[32m[20221213 22:52:56 @agent_ppo2.py:143][0m Total time:      34.72 min
[32m[20221213 22:52:56 @agent_ppo2.py:145][0m 3393536 total steps have happened
[32m[20221213 22:52:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1657 --------------------------#
[32m[20221213 22:52:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |           0.0009 |          45.5153 |          15.1062 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0050 |          41.0065 |          15.1030 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0042 |          39.2058 |          15.0965 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0049 |          37.8842 |          15.1101 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0097 |          36.9707 |          15.0867 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0095 |          36.6702 |          15.1004 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0052 |          36.1275 |          15.1016 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0087 |          35.6536 |          15.0964 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0069 |          35.3288 |          15.0863 |
[32m[20221213 22:52:57 @agent_ppo2.py:185][0m |          -0.0073 |          34.9057 |          15.0876 |
[32m[20221213 22:52:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:52:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.67
[32m[20221213 22:52:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.54
[32m[20221213 22:52:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.47
[32m[20221213 22:52:58 @agent_ppo2.py:143][0m Total time:      34.74 min
[32m[20221213 22:52:58 @agent_ppo2.py:145][0m 3395584 total steps have happened
[32m[20221213 22:52:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1658 --------------------------#
[32m[20221213 22:52:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:52:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0022 |           8.8585 |          15.4324 |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0039 |           6.5370 |          15.4450 |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0027 |           6.4260 |          15.4279 |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0050 |           6.3322 |          15.4493 |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0064 |           6.2863 |          15.4460 |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0076 |           6.2726 |          15.4713 |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0031 |           6.2635 |          15.4353 |
[32m[20221213 22:52:58 @agent_ppo2.py:185][0m |          -0.0024 |           6.3405 |          15.4262 |
[32m[20221213 22:52:59 @agent_ppo2.py:185][0m |          -0.0003 |           6.6403 |          15.4610 |
[32m[20221213 22:52:59 @agent_ppo2.py:185][0m |          -0.0059 |           6.2238 |          15.4460 |
[32m[20221213 22:52:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:52:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:52:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:52:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:52:59 @agent_ppo2.py:143][0m Total time:      34.76 min
[32m[20221213 22:52:59 @agent_ppo2.py:145][0m 3397632 total steps have happened
[32m[20221213 22:52:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1659 --------------------------#
[32m[20221213 22:52:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:52:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:52:59 @agent_ppo2.py:185][0m |          -0.0028 |          40.2630 |          15.3969 |
[32m[20221213 22:52:59 @agent_ppo2.py:185][0m |          -0.0003 |          35.5971 |          15.3852 |
[32m[20221213 22:52:59 @agent_ppo2.py:185][0m |          -0.0071 |          33.3340 |          15.3990 |
[32m[20221213 22:52:59 @agent_ppo2.py:185][0m |          -0.0048 |          31.9317 |          15.3998 |
[32m[20221213 22:52:59 @agent_ppo2.py:185][0m |          -0.0108 |          31.1832 |          15.3984 |
[32m[20221213 22:53:00 @agent_ppo2.py:185][0m |          -0.0121 |          30.6557 |          15.4029 |
[32m[20221213 22:53:00 @agent_ppo2.py:185][0m |          -0.0076 |          30.4316 |          15.4028 |
[32m[20221213 22:53:00 @agent_ppo2.py:185][0m |           0.0040 |          34.2936 |          15.4046 |
[32m[20221213 22:53:00 @agent_ppo2.py:185][0m |          -0.0145 |          29.3593 |          15.3807 |
[32m[20221213 22:53:00 @agent_ppo2.py:185][0m |          -0.0078 |          28.4905 |          15.3841 |
[32m[20221213 22:53:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:53:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.91
[32m[20221213 22:53:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.60
[32m[20221213 22:53:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.87
[32m[20221213 22:53:00 @agent_ppo2.py:143][0m Total time:      34.79 min
[32m[20221213 22:53:00 @agent_ppo2.py:145][0m 3399680 total steps have happened
[32m[20221213 22:53:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1660 --------------------------#
[32m[20221213 22:53:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:53:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:00 @agent_ppo2.py:185][0m |          -0.0016 |          41.8771 |          15.5323 |
[32m[20221213 22:53:00 @agent_ppo2.py:185][0m |          -0.0041 |          39.0165 |          15.5133 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0040 |          38.2249 |          15.5160 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0006 |          37.9956 |          15.5231 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0045 |          38.0835 |          15.5049 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0049 |          37.5659 |          15.5040 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0094 |          37.3309 |          15.5078 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0094 |          37.4847 |          15.5086 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0095 |          37.0896 |          15.5291 |
[32m[20221213 22:53:01 @agent_ppo2.py:185][0m |          -0.0090 |          37.3365 |          15.5121 |
[32m[20221213 22:53:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.58
[32m[20221213 22:53:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.90
[32m[20221213 22:53:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.48
[32m[20221213 22:53:01 @agent_ppo2.py:143][0m Total time:      34.81 min
[32m[20221213 22:53:01 @agent_ppo2.py:145][0m 3401728 total steps have happened
[32m[20221213 22:53:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1661 --------------------------#
[32m[20221213 22:53:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |           0.0008 |          41.7204 |          15.4425 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0018 |          35.7761 |          15.4276 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0051 |          33.6223 |          15.4137 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0079 |          32.2846 |          15.4056 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0075 |          31.4501 |          15.3988 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0114 |          30.8434 |          15.4111 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0110 |          30.6231 |          15.3871 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0092 |          30.1362 |          15.3798 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0115 |          29.9210 |          15.3798 |
[32m[20221213 22:53:02 @agent_ppo2.py:185][0m |          -0.0100 |          29.4605 |          15.3634 |
[32m[20221213 22:53:02 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.98
[32m[20221213 22:53:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.97
[32m[20221213 22:53:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.30
[32m[20221213 22:53:02 @agent_ppo2.py:143][0m Total time:      34.83 min
[32m[20221213 22:53:02 @agent_ppo2.py:145][0m 3403776 total steps have happened
[32m[20221213 22:53:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1662 --------------------------#
[32m[20221213 22:53:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |           0.0109 |          38.3248 |          15.4945 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |           0.0007 |          26.6903 |          15.4582 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |          -0.0041 |          25.9651 |          15.4405 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |          -0.0015 |          25.8484 |          15.4271 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |          -0.0013 |          25.8326 |          15.4050 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |          -0.0003 |          25.5914 |          15.3956 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |          -0.0071 |          25.4843 |          15.3980 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |          -0.0116 |          25.3937 |          15.3698 |
[32m[20221213 22:53:03 @agent_ppo2.py:185][0m |          -0.0093 |          25.7436 |          15.3625 |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |          -0.0069 |          25.3727 |          15.3394 |
[32m[20221213 22:53:04 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.05
[32m[20221213 22:53:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.27
[32m[20221213 22:53:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.28
[32m[20221213 22:53:04 @agent_ppo2.py:143][0m Total time:      34.85 min
[32m[20221213 22:53:04 @agent_ppo2.py:145][0m 3405824 total steps have happened
[32m[20221213 22:53:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1663 --------------------------#
[32m[20221213 22:53:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |          -0.0039 |          35.4517 |          15.1137 |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |           0.0016 |          34.1433 |          15.0972 |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |          -0.0080 |          30.8076 |          15.0710 |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |          -0.0058 |          30.3573 |          15.0799 |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |          -0.0004 |          30.5874 |          15.0573 |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |          -0.0089 |          29.7749 |          15.0434 |
[32m[20221213 22:53:04 @agent_ppo2.py:185][0m |          -0.0027 |          30.0549 |          15.0199 |
[32m[20221213 22:53:05 @agent_ppo2.py:185][0m |          -0.0170 |          29.6322 |          15.0248 |
[32m[20221213 22:53:05 @agent_ppo2.py:185][0m |          -0.0090 |          29.4636 |          15.0025 |
[32m[20221213 22:53:05 @agent_ppo2.py:185][0m |          -0.0060 |          29.6494 |          15.0127 |
[32m[20221213 22:53:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.81
[32m[20221213 22:53:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 401.16
[32m[20221213 22:53:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.30
[32m[20221213 22:53:05 @agent_ppo2.py:143][0m Total time:      34.87 min
[32m[20221213 22:53:05 @agent_ppo2.py:145][0m 3407872 total steps have happened
[32m[20221213 22:53:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1664 --------------------------#
[32m[20221213 22:53:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:05 @agent_ppo2.py:185][0m |          -0.0024 |          44.1096 |          15.3436 |
[32m[20221213 22:53:05 @agent_ppo2.py:185][0m |          -0.0073 |          41.8014 |          15.3414 |
[32m[20221213 22:53:05 @agent_ppo2.py:185][0m |          -0.0078 |          41.1534 |          15.3461 |
[32m[20221213 22:53:05 @agent_ppo2.py:185][0m |          -0.0076 |          40.8958 |          15.3210 |
[32m[20221213 22:53:06 @agent_ppo2.py:185][0m |           0.0001 |          42.8516 |          15.3218 |
[32m[20221213 22:53:06 @agent_ppo2.py:185][0m |          -0.0107 |          40.6065 |          15.3172 |
[32m[20221213 22:53:06 @agent_ppo2.py:185][0m |          -0.0103 |          40.4989 |          15.3080 |
[32m[20221213 22:53:06 @agent_ppo2.py:185][0m |          -0.0051 |          41.1916 |          15.3219 |
[32m[20221213 22:53:06 @agent_ppo2.py:185][0m |          -0.0111 |          40.2490 |          15.3224 |
[32m[20221213 22:53:06 @agent_ppo2.py:185][0m |          -0.0168 |          40.2425 |          15.3005 |
[32m[20221213 22:53:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:53:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.48
[32m[20221213 22:53:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.73
[32m[20221213 22:53:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.16
[32m[20221213 22:53:06 @agent_ppo2.py:143][0m Total time:      34.89 min
[32m[20221213 22:53:06 @agent_ppo2.py:145][0m 3409920 total steps have happened
[32m[20221213 22:53:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1665 --------------------------#
[32m[20221213 22:53:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:06 @agent_ppo2.py:185][0m |          -0.0020 |          45.6161 |          15.0810 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |          -0.0012 |          43.1387 |          15.0267 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |           0.0035 |          45.4237 |          15.0365 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |          -0.0007 |          43.0231 |          15.0277 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |          -0.0016 |          44.4375 |          15.0344 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |          -0.0077 |          41.7368 |          15.0240 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |          -0.0077 |          41.3814 |          15.0462 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |          -0.0048 |          42.0881 |          15.0144 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |          -0.0094 |          41.2314 |          15.0472 |
[32m[20221213 22:53:07 @agent_ppo2.py:185][0m |           0.0090 |          43.4660 |          15.0203 |
[32m[20221213 22:53:07 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.32
[32m[20221213 22:53:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.34
[32m[20221213 22:53:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.56
[32m[20221213 22:53:07 @agent_ppo2.py:143][0m Total time:      34.91 min
[32m[20221213 22:53:07 @agent_ppo2.py:145][0m 3411968 total steps have happened
[32m[20221213 22:53:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1666 --------------------------#
[32m[20221213 22:53:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |           0.0015 |          45.6780 |          15.4725 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0056 |          42.6950 |          15.4535 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0056 |          41.6836 |          15.4721 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0099 |          41.0594 |          15.4609 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0002 |          45.3247 |          15.4709 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0032 |          40.7272 |          15.4679 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0088 |          40.1501 |          15.5005 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0115 |          39.8716 |          15.4705 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0089 |          39.8748 |          15.4712 |
[32m[20221213 22:53:08 @agent_ppo2.py:185][0m |          -0.0081 |          40.8948 |          15.4705 |
[32m[20221213 22:53:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.64
[32m[20221213 22:53:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.26
[32m[20221213 22:53:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.27
[32m[20221213 22:53:09 @agent_ppo2.py:143][0m Total time:      34.93 min
[32m[20221213 22:53:09 @agent_ppo2.py:145][0m 3414016 total steps have happened
[32m[20221213 22:53:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1667 --------------------------#
[32m[20221213 22:53:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |           0.0013 |          31.5437 |          15.2727 |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |          -0.0033 |          25.5070 |          15.2723 |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |          -0.0004 |          24.8225 |          15.2637 |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |          -0.0028 |          25.6442 |          15.2594 |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |          -0.0066 |          23.8904 |          15.2593 |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |          -0.0076 |          26.4958 |          15.2604 |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |          -0.0081 |          23.4373 |          15.2637 |
[32m[20221213 22:53:09 @agent_ppo2.py:185][0m |          -0.0090 |          23.2747 |          15.2405 |
[32m[20221213 22:53:10 @agent_ppo2.py:185][0m |          -0.0107 |          23.1968 |          15.2513 |
[32m[20221213 22:53:10 @agent_ppo2.py:185][0m |          -0.0139 |          22.9443 |          15.2508 |
[32m[20221213 22:53:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.53
[32m[20221213 22:53:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.67
[32m[20221213 22:53:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.50
[32m[20221213 22:53:10 @agent_ppo2.py:143][0m Total time:      34.95 min
[32m[20221213 22:53:10 @agent_ppo2.py:145][0m 3416064 total steps have happened
[32m[20221213 22:53:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1668 --------------------------#
[32m[20221213 22:53:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:10 @agent_ppo2.py:185][0m |           0.0009 |          30.8827 |          15.1195 |
[32m[20221213 22:53:10 @agent_ppo2.py:185][0m |          -0.0049 |          26.9072 |          15.1126 |
[32m[20221213 22:53:10 @agent_ppo2.py:185][0m |          -0.0060 |          26.0480 |          15.0972 |
[32m[20221213 22:53:10 @agent_ppo2.py:185][0m |          -0.0042 |          25.8181 |          15.1032 |
[32m[20221213 22:53:10 @agent_ppo2.py:185][0m |          -0.0026 |          25.5974 |          15.0958 |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |          -0.0038 |          26.1628 |          15.0827 |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |          -0.0064 |          24.9376 |          15.0896 |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |          -0.0072 |          24.5869 |          15.0839 |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |          -0.0115 |          24.5600 |          15.0615 |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |          -0.0029 |          26.7669 |          15.0820 |
[32m[20221213 22:53:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.74
[32m[20221213 22:53:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.75
[32m[20221213 22:53:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.28
[32m[20221213 22:53:11 @agent_ppo2.py:143][0m Total time:      34.97 min
[32m[20221213 22:53:11 @agent_ppo2.py:145][0m 3418112 total steps have happened
[32m[20221213 22:53:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1669 --------------------------#
[32m[20221213 22:53:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |           0.0005 |          29.0340 |          15.3211 |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |          -0.0042 |          27.1231 |          15.2821 |
[32m[20221213 22:53:11 @agent_ppo2.py:185][0m |          -0.0058 |          26.3724 |          15.2963 |
[32m[20221213 22:53:12 @agent_ppo2.py:185][0m |          -0.0061 |          25.8342 |          15.2867 |
[32m[20221213 22:53:12 @agent_ppo2.py:185][0m |          -0.0058 |          25.9260 |          15.2835 |
[32m[20221213 22:53:12 @agent_ppo2.py:185][0m |          -0.0038 |          26.0867 |          15.2836 |
[32m[20221213 22:53:12 @agent_ppo2.py:185][0m |          -0.0099 |          24.8899 |          15.2867 |
[32m[20221213 22:53:12 @agent_ppo2.py:185][0m |          -0.0086 |          24.7591 |          15.2912 |
[32m[20221213 22:53:12 @agent_ppo2.py:185][0m |          -0.0104 |          24.5573 |          15.2867 |
[32m[20221213 22:53:12 @agent_ppo2.py:185][0m |          -0.0110 |          24.4055 |          15.3021 |
[32m[20221213 22:53:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:53:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.36
[32m[20221213 22:53:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.37
[32m[20221213 22:53:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.33
[32m[20221213 22:53:12 @agent_ppo2.py:143][0m Total time:      34.99 min
[32m[20221213 22:53:12 @agent_ppo2.py:145][0m 3420160 total steps have happened
[32m[20221213 22:53:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1670 --------------------------#
[32m[20221213 22:53:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:53:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0034 |          34.7034 |          15.1711 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0040 |          32.5974 |          15.1843 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0081 |          31.9781 |          15.1758 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0067 |          31.5582 |          15.1716 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0095 |          31.2292 |          15.1831 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0091 |          31.0739 |          15.1662 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0090 |          31.0087 |          15.1656 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0017 |          31.6531 |          15.1926 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0072 |          30.8086 |          15.1784 |
[32m[20221213 22:53:13 @agent_ppo2.py:185][0m |          -0.0047 |          31.8914 |          15.1874 |
[32m[20221213 22:53:13 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.31
[32m[20221213 22:53:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.24
[32m[20221213 22:53:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.75
[32m[20221213 22:53:13 @agent_ppo2.py:143][0m Total time:      35.01 min
[32m[20221213 22:53:13 @agent_ppo2.py:145][0m 3422208 total steps have happened
[32m[20221213 22:53:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1671 --------------------------#
[32m[20221213 22:53:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |           0.0013 |          48.3646 |          15.0843 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0024 |          44.7915 |          15.0857 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0019 |          43.9729 |          15.0791 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0067 |          43.4110 |          15.0641 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0023 |          43.0288 |          15.0522 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0057 |          42.7699 |          15.0541 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0013 |          44.3527 |          15.0513 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0066 |          42.3355 |          15.0270 |
[32m[20221213 22:53:14 @agent_ppo2.py:185][0m |          -0.0087 |          42.0084 |          15.0798 |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |          -0.0088 |          41.7046 |          15.0493 |
[32m[20221213 22:53:15 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.44
[32m[20221213 22:53:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.32
[32m[20221213 22:53:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.23
[32m[20221213 22:53:15 @agent_ppo2.py:143][0m Total time:      35.03 min
[32m[20221213 22:53:15 @agent_ppo2.py:145][0m 3424256 total steps have happened
[32m[20221213 22:53:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1672 --------------------------#
[32m[20221213 22:53:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |           0.0018 |          31.0691 |          15.2786 |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |          -0.0022 |          26.3553 |          15.2766 |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |          -0.0044 |          25.2599 |          15.2691 |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |          -0.0067 |          24.8174 |          15.2865 |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |          -0.0068 |          24.1044 |          15.2906 |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |          -0.0012 |          23.6681 |          15.2882 |
[32m[20221213 22:53:15 @agent_ppo2.py:185][0m |          -0.0086 |          23.4560 |          15.2974 |
[32m[20221213 22:53:16 @agent_ppo2.py:185][0m |          -0.0122 |          23.0233 |          15.3008 |
[32m[20221213 22:53:16 @agent_ppo2.py:185][0m |          -0.0139 |          22.8589 |          15.2947 |
[32m[20221213 22:53:16 @agent_ppo2.py:185][0m |          -0.0095 |          22.7299 |          15.2977 |
[32m[20221213 22:53:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 318.37
[32m[20221213 22:53:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 367.20
[32m[20221213 22:53:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.74
[32m[20221213 22:53:16 @agent_ppo2.py:143][0m Total time:      35.05 min
[32m[20221213 22:53:16 @agent_ppo2.py:145][0m 3426304 total steps have happened
[32m[20221213 22:53:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1673 --------------------------#
[32m[20221213 22:53:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:16 @agent_ppo2.py:185][0m |          -0.0020 |          30.2117 |          15.3344 |
[32m[20221213 22:53:16 @agent_ppo2.py:185][0m |          -0.0079 |          26.7707 |          15.3462 |
[32m[20221213 22:53:16 @agent_ppo2.py:185][0m |          -0.0113 |          25.2506 |          15.3304 |
[32m[20221213 22:53:16 @agent_ppo2.py:185][0m |          -0.0098 |          24.4270 |          15.3329 |
[32m[20221213 22:53:17 @agent_ppo2.py:185][0m |          -0.0048 |          25.9955 |          15.3502 |
[32m[20221213 22:53:17 @agent_ppo2.py:185][0m |          -0.0126 |          23.6039 |          15.3360 |
[32m[20221213 22:53:17 @agent_ppo2.py:185][0m |          -0.0123 |          23.2397 |          15.3388 |
[32m[20221213 22:53:17 @agent_ppo2.py:185][0m |          -0.0144 |          22.9054 |          15.3329 |
[32m[20221213 22:53:17 @agent_ppo2.py:185][0m |          -0.0162 |          22.7915 |          15.3351 |
[32m[20221213 22:53:17 @agent_ppo2.py:185][0m |          -0.0169 |          22.4979 |          15.3329 |
[32m[20221213 22:53:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.03
[32m[20221213 22:53:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.26
[32m[20221213 22:53:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.39
[32m[20221213 22:53:17 @agent_ppo2.py:143][0m Total time:      35.07 min
[32m[20221213 22:53:17 @agent_ppo2.py:145][0m 3428352 total steps have happened
[32m[20221213 22:53:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1674 --------------------------#
[32m[20221213 22:53:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:53:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:17 @agent_ppo2.py:185][0m |           0.0009 |          24.5335 |          15.0677 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0082 |          22.3254 |          15.0366 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0071 |          21.6473 |          15.0078 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0097 |          21.1839 |          15.0075 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0117 |          20.7084 |          14.9725 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0117 |          20.5170 |          14.9698 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0071 |          20.3099 |          14.9569 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0126 |          20.0804 |          14.9447 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0109 |          19.9679 |          14.9172 |
[32m[20221213 22:53:18 @agent_ppo2.py:185][0m |          -0.0130 |          19.8888 |          14.9088 |
[32m[20221213 22:53:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:53:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.10
[32m[20221213 22:53:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 341.85
[32m[20221213 22:53:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.74
[32m[20221213 22:53:18 @agent_ppo2.py:143][0m Total time:      35.09 min
[32m[20221213 22:53:18 @agent_ppo2.py:145][0m 3430400 total steps have happened
[32m[20221213 22:53:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1675 --------------------------#
[32m[20221213 22:53:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |           0.0083 |          42.0745 |          15.2780 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0013 |          37.4794 |          15.2454 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0052 |          36.0994 |          15.2150 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0047 |          35.3523 |          15.2266 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0058 |          34.7588 |          15.2339 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0105 |          34.1764 |          15.2189 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0069 |          33.7172 |          15.2226 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0062 |          34.6017 |          15.2232 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0105 |          32.9487 |          15.2039 |
[32m[20221213 22:53:19 @agent_ppo2.py:185][0m |          -0.0103 |          32.7527 |          15.1989 |
[32m[20221213 22:53:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.52
[32m[20221213 22:53:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.19
[32m[20221213 22:53:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.28
[32m[20221213 22:53:20 @agent_ppo2.py:143][0m Total time:      35.11 min
[32m[20221213 22:53:20 @agent_ppo2.py:145][0m 3432448 total steps have happened
[32m[20221213 22:53:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1676 --------------------------#
[32m[20221213 22:53:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0002 |          31.2720 |          15.0564 |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0036 |          21.6898 |          15.0525 |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0097 |          20.9500 |          15.0444 |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0107 |          20.7428 |          15.0711 |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0032 |          20.4801 |          15.0457 |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0074 |          20.4052 |          15.0676 |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0121 |          20.2356 |          15.0649 |
[32m[20221213 22:53:20 @agent_ppo2.py:185][0m |          -0.0086 |          20.1223 |          15.0757 |
[32m[20221213 22:53:21 @agent_ppo2.py:185][0m |          -0.0070 |          20.0846 |          15.0723 |
[32m[20221213 22:53:21 @agent_ppo2.py:185][0m |          -0.0201 |          20.0009 |          15.0871 |
[32m[20221213 22:53:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.74
[32m[20221213 22:53:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.61
[32m[20221213 22:53:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.85
[32m[20221213 22:53:21 @agent_ppo2.py:143][0m Total time:      35.13 min
[32m[20221213 22:53:21 @agent_ppo2.py:145][0m 3434496 total steps have happened
[32m[20221213 22:53:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1677 --------------------------#
[32m[20221213 22:53:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:21 @agent_ppo2.py:185][0m |           0.0019 |          39.1004 |          15.2196 |
[32m[20221213 22:53:21 @agent_ppo2.py:185][0m |          -0.0049 |          36.7262 |          15.1945 |
[32m[20221213 22:53:21 @agent_ppo2.py:185][0m |          -0.0003 |          36.1834 |          15.1989 |
[32m[20221213 22:53:21 @agent_ppo2.py:185][0m |          -0.0099 |          36.1155 |          15.1655 |
[32m[20221213 22:53:21 @agent_ppo2.py:185][0m |          -0.0057 |          35.7841 |          15.1758 |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |          -0.0072 |          35.6587 |          15.1506 |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |          -0.0098 |          35.4957 |          15.1572 |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |          -0.0106 |          35.4156 |          15.1494 |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |          -0.0089 |          35.2969 |          15.1355 |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |           0.0014 |          38.9114 |          15.1394 |
[32m[20221213 22:53:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.82
[32m[20221213 22:53:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.36
[32m[20221213 22:53:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.57
[32m[20221213 22:53:22 @agent_ppo2.py:143][0m Total time:      35.15 min
[32m[20221213 22:53:22 @agent_ppo2.py:145][0m 3436544 total steps have happened
[32m[20221213 22:53:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1678 --------------------------#
[32m[20221213 22:53:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |          -0.0017 |          41.6108 |          15.1266 |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |          -0.0082 |          39.8356 |          15.1322 |
[32m[20221213 22:53:22 @agent_ppo2.py:185][0m |           0.0022 |          45.0927 |          15.1219 |
[32m[20221213 22:53:23 @agent_ppo2.py:185][0m |          -0.0115 |          38.5964 |          15.1198 |
[32m[20221213 22:53:23 @agent_ppo2.py:185][0m |           0.0065 |          42.1272 |          15.1397 |
[32m[20221213 22:53:23 @agent_ppo2.py:185][0m |          -0.0120 |          37.7725 |          15.1382 |
[32m[20221213 22:53:23 @agent_ppo2.py:185][0m |          -0.0132 |          37.4655 |          15.1399 |
[32m[20221213 22:53:23 @agent_ppo2.py:185][0m |          -0.0120 |          37.2009 |          15.1598 |
[32m[20221213 22:53:23 @agent_ppo2.py:185][0m |          -0.0105 |          37.1992 |          15.1945 |
[32m[20221213 22:53:23 @agent_ppo2.py:185][0m |          -0.0141 |          36.9056 |          15.1536 |
[32m[20221213 22:53:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:53:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.18
[32m[20221213 22:53:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.35
[32m[20221213 22:53:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.77
[32m[20221213 22:53:23 @agent_ppo2.py:143][0m Total time:      35.17 min
[32m[20221213 22:53:23 @agent_ppo2.py:145][0m 3438592 total steps have happened
[32m[20221213 22:53:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1679 --------------------------#
[32m[20221213 22:53:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0020 |          28.6238 |          15.0988 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0007 |          24.6467 |          15.0440 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0058 |          23.8799 |          14.9805 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0051 |          23.4784 |          14.9931 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0034 |          23.8567 |          14.9838 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0053 |          23.0416 |          14.9698 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0110 |          22.9422 |          14.9678 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0006 |          25.0148 |          14.9411 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0039 |          24.5474 |          14.9059 |
[32m[20221213 22:53:24 @agent_ppo2.py:185][0m |          -0.0062 |          22.6365 |          14.9106 |
[32m[20221213 22:53:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:53:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 320.00
[32m[20221213 22:53:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.24
[32m[20221213 22:53:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.31
[32m[20221213 22:53:24 @agent_ppo2.py:143][0m Total time:      35.19 min
[32m[20221213 22:53:24 @agent_ppo2.py:145][0m 3440640 total steps have happened
[32m[20221213 22:53:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1680 --------------------------#
[32m[20221213 22:53:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:53:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:25 @agent_ppo2.py:185][0m |          -0.0001 |          28.5590 |          14.9739 |
[32m[20221213 22:53:25 @agent_ppo2.py:185][0m |           0.0021 |          25.8415 |          14.9467 |
[32m[20221213 22:53:25 @agent_ppo2.py:185][0m |          -0.0072 |          25.1670 |          14.9473 |
[32m[20221213 22:53:25 @agent_ppo2.py:185][0m |          -0.0072 |          24.7504 |          14.9550 |
[32m[20221213 22:53:25 @agent_ppo2.py:185][0m |          -0.0059 |          24.6278 |          14.9338 |
[32m[20221213 22:53:25 @agent_ppo2.py:185][0m |          -0.0062 |          24.5690 |          14.9200 |
[32m[20221213 22:53:25 @agent_ppo2.py:185][0m |          -0.0097 |          24.1235 |          14.9214 |
[32m[20221213 22:53:26 @agent_ppo2.py:185][0m |          -0.0129 |          23.7846 |          14.9134 |
[32m[20221213 22:53:26 @agent_ppo2.py:185][0m |          -0.0125 |          23.6030 |          14.9168 |
[32m[20221213 22:53:26 @agent_ppo2.py:185][0m |          -0.0092 |          23.4124 |          14.8963 |
[32m[20221213 22:53:26 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 22:53:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.09
[32m[20221213 22:53:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.90
[32m[20221213 22:53:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.65
[32m[20221213 22:53:26 @agent_ppo2.py:143][0m Total time:      35.22 min
[32m[20221213 22:53:26 @agent_ppo2.py:145][0m 3442688 total steps have happened
[32m[20221213 22:53:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1681 --------------------------#
[32m[20221213 22:53:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:53:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:26 @agent_ppo2.py:185][0m |           0.0014 |          36.2976 |          14.9728 |
[32m[20221213 22:53:26 @agent_ppo2.py:185][0m |          -0.0036 |          33.4384 |          14.9827 |
[32m[20221213 22:53:26 @agent_ppo2.py:185][0m |           0.0048 |          33.9583 |          15.0039 |
[32m[20221213 22:53:26 @agent_ppo2.py:185][0m |          -0.0096 |          32.0584 |          15.0148 |
[32m[20221213 22:53:27 @agent_ppo2.py:185][0m |          -0.0076 |          31.6653 |          15.0078 |
[32m[20221213 22:53:27 @agent_ppo2.py:185][0m |          -0.0059 |          32.7852 |          15.0196 |
[32m[20221213 22:53:27 @agent_ppo2.py:185][0m |          -0.0102 |          30.9611 |          15.0108 |
[32m[20221213 22:53:27 @agent_ppo2.py:185][0m |          -0.0074 |          31.1425 |          15.0173 |
[32m[20221213 22:53:27 @agent_ppo2.py:185][0m |           0.0002 |          33.4842 |          15.0235 |
[32m[20221213 22:53:27 @agent_ppo2.py:185][0m |          -0.0173 |          30.4351 |          15.0509 |
[32m[20221213 22:53:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:53:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.76
[32m[20221213 22:53:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.31
[32m[20221213 22:53:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.84
[32m[20221213 22:53:27 @agent_ppo2.py:143][0m Total time:      35.24 min
[32m[20221213 22:53:27 @agent_ppo2.py:145][0m 3444736 total steps have happened
[32m[20221213 22:53:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1682 --------------------------#
[32m[20221213 22:53:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:27 @agent_ppo2.py:185][0m |           0.0007 |          33.7173 |          14.6525 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0008 |          31.2263 |          14.6431 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0092 |          30.3219 |          14.6523 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0074 |          29.8094 |          14.6467 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0060 |          29.5195 |          14.6398 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0044 |          30.8250 |          14.6518 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0093 |          29.0423 |          14.6586 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0105 |          28.6727 |          14.6672 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0139 |          28.5400 |          14.6805 |
[32m[20221213 22:53:28 @agent_ppo2.py:185][0m |          -0.0132 |          28.2572 |          14.6858 |
[32m[20221213 22:53:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 22:53:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 307.30
[32m[20221213 22:53:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.45
[32m[20221213 22:53:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.54
[32m[20221213 22:53:28 @agent_ppo2.py:143][0m Total time:      35.26 min
[32m[20221213 22:53:28 @agent_ppo2.py:145][0m 3446784 total steps have happened
[32m[20221213 22:53:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1683 --------------------------#
[32m[20221213 22:53:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |           0.0057 |          40.4081 |          15.0222 |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |          -0.0085 |          36.9616 |          15.0146 |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |           0.0006 |          38.0956 |          15.0085 |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |          -0.0103 |          35.8243 |          15.0063 |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |          -0.0111 |          35.2472 |          15.0008 |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |          -0.0103 |          35.0140 |          14.9989 |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |          -0.0039 |          36.6975 |          14.9975 |
[32m[20221213 22:53:29 @agent_ppo2.py:185][0m |          -0.0099 |          34.6187 |          14.9824 |
[32m[20221213 22:53:30 @agent_ppo2.py:185][0m |          -0.0113 |          34.2786 |          14.9853 |
[32m[20221213 22:53:30 @agent_ppo2.py:185][0m |          -0.0139 |          34.1973 |          14.9941 |
[32m[20221213 22:53:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:53:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.38
[32m[20221213 22:53:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.58
[32m[20221213 22:53:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.95
[32m[20221213 22:53:30 @agent_ppo2.py:143][0m Total time:      35.28 min
[32m[20221213 22:53:30 @agent_ppo2.py:145][0m 3448832 total steps have happened
[32m[20221213 22:53:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1684 --------------------------#
[32m[20221213 22:53:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:30 @agent_ppo2.py:185][0m |           0.0016 |          34.0235 |          15.3157 |
[32m[20221213 22:53:30 @agent_ppo2.py:185][0m |          -0.0020 |          29.0176 |          15.2840 |
[32m[20221213 22:53:30 @agent_ppo2.py:185][0m |           0.0020 |          28.3431 |          15.3140 |
[32m[20221213 22:53:30 @agent_ppo2.py:185][0m |          -0.0057 |          27.2162 |          15.3148 |
[32m[20221213 22:53:30 @agent_ppo2.py:185][0m |          -0.0011 |          26.4776 |          15.3170 |
[32m[20221213 22:53:31 @agent_ppo2.py:185][0m |          -0.0059 |          26.1079 |          15.3274 |
[32m[20221213 22:53:31 @agent_ppo2.py:185][0m |          -0.0050 |          25.8011 |          15.2995 |
[32m[20221213 22:53:31 @agent_ppo2.py:185][0m |          -0.0068 |          25.6439 |          15.3450 |
[32m[20221213 22:53:31 @agent_ppo2.py:185][0m |          -0.0103 |          25.2811 |          15.3272 |
[32m[20221213 22:53:31 @agent_ppo2.py:185][0m |          -0.0093 |          25.0600 |          15.3498 |
[32m[20221213 22:53:31 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:53:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.47
[32m[20221213 22:53:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.20
[32m[20221213 22:53:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.48
[32m[20221213 22:53:31 @agent_ppo2.py:143][0m Total time:      35.30 min
[32m[20221213 22:53:31 @agent_ppo2.py:145][0m 3450880 total steps have happened
[32m[20221213 22:53:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1685 --------------------------#
[32m[20221213 22:53:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:31 @agent_ppo2.py:185][0m |           0.0097 |          33.3162 |          14.9770 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0022 |          29.1807 |          14.9570 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0003 |          28.4421 |          14.9822 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0090 |          28.3933 |          14.9977 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0101 |          27.9379 |          14.9689 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0044 |          27.8018 |          14.9806 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0040 |          29.1595 |          14.9902 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0085 |          27.3495 |          14.9610 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0110 |          27.2829 |          14.9753 |
[32m[20221213 22:53:32 @agent_ppo2.py:185][0m |          -0.0107 |          27.1782 |          14.9868 |
[32m[20221213 22:53:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:53:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.89
[32m[20221213 22:53:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.76
[32m[20221213 22:53:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 148.61
[32m[20221213 22:53:32 @agent_ppo2.py:143][0m Total time:      35.32 min
[32m[20221213 22:53:32 @agent_ppo2.py:145][0m 3452928 total steps have happened
[32m[20221213 22:53:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1686 --------------------------#
[32m[20221213 22:53:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0014 |          31.2266 |          15.1168 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0031 |          28.9040 |          15.0893 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0052 |          27.8948 |          15.1024 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0041 |          27.9027 |          15.0834 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0092 |          26.6896 |          15.1073 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0067 |          26.7986 |          15.0931 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0077 |          25.8169 |          15.1068 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0071 |          26.1177 |          15.0948 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0099 |          25.3172 |          15.1025 |
[32m[20221213 22:53:33 @agent_ppo2.py:185][0m |          -0.0155 |          25.0606 |          15.1005 |
[32m[20221213 22:53:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:53:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.28
[32m[20221213 22:53:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.35
[32m[20221213 22:53:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.31
[32m[20221213 22:53:34 @agent_ppo2.py:143][0m Total time:      35.35 min
[32m[20221213 22:53:34 @agent_ppo2.py:145][0m 3454976 total steps have happened
[32m[20221213 22:53:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1687 --------------------------#
[32m[20221213 22:53:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:53:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:34 @agent_ppo2.py:185][0m |           0.0031 |          34.7090 |          14.9819 |
[32m[20221213 22:53:34 @agent_ppo2.py:185][0m |          -0.0032 |          32.0059 |          14.9456 |
[32m[20221213 22:53:34 @agent_ppo2.py:185][0m |           0.0011 |          30.6702 |          14.9673 |
[32m[20221213 22:53:34 @agent_ppo2.py:185][0m |          -0.0074 |          30.1486 |          14.9068 |
[32m[20221213 22:53:34 @agent_ppo2.py:185][0m |          -0.0084 |          29.7941 |          14.9265 |
[32m[20221213 22:53:34 @agent_ppo2.py:185][0m |          -0.0085 |          29.4672 |          14.8976 |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |           0.0017 |          29.0857 |          14.8882 |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |           0.0021 |          31.5650 |          14.9033 |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |          -0.0046 |          28.5779 |          14.8740 |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |          -0.0107 |          28.3750 |          14.8680 |
[32m[20221213 22:53:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.67
[32m[20221213 22:53:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.64
[32m[20221213 22:53:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.51
[32m[20221213 22:53:35 @agent_ppo2.py:143][0m Total time:      35.37 min
[32m[20221213 22:53:35 @agent_ppo2.py:145][0m 3457024 total steps have happened
[32m[20221213 22:53:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1688 --------------------------#
[32m[20221213 22:53:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |           0.0036 |          24.8566 |          15.1023 |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |          -0.0035 |          20.2728 |          15.0768 |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |          -0.0109 |          19.1982 |          15.0901 |
[32m[20221213 22:53:35 @agent_ppo2.py:185][0m |          -0.0050 |          18.3257 |          15.0709 |
[32m[20221213 22:53:36 @agent_ppo2.py:185][0m |          -0.0093 |          17.9455 |          15.0774 |
[32m[20221213 22:53:36 @agent_ppo2.py:185][0m |          -0.0110 |          17.4366 |          15.0484 |
[32m[20221213 22:53:36 @agent_ppo2.py:185][0m |          -0.0111 |          17.1560 |          15.0422 |
[32m[20221213 22:53:36 @agent_ppo2.py:185][0m |          -0.0098 |          16.9926 |          15.0378 |
[32m[20221213 22:53:36 @agent_ppo2.py:185][0m |          -0.0130 |          16.6110 |          15.0477 |
[32m[20221213 22:53:36 @agent_ppo2.py:185][0m |          -0.0170 |          16.4692 |          15.0258 |
[32m[20221213 22:53:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:53:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.16
[32m[20221213 22:53:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.47
[32m[20221213 22:53:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.82
[32m[20221213 22:53:36 @agent_ppo2.py:143][0m Total time:      35.39 min
[32m[20221213 22:53:36 @agent_ppo2.py:145][0m 3459072 total steps have happened
[32m[20221213 22:53:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1689 --------------------------#
[32m[20221213 22:53:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:36 @agent_ppo2.py:185][0m |           0.0116 |          44.1957 |          14.8470 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0025 |          38.1695 |          14.8256 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0028 |          37.2515 |          14.8276 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0001 |          39.3808 |          14.8011 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0114 |          36.5384 |          14.8103 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0076 |          36.2645 |          14.8054 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0090 |          36.0459 |          14.7925 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0029 |          37.8894 |          14.7954 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0093 |          37.0680 |          14.8041 |
[32m[20221213 22:53:37 @agent_ppo2.py:185][0m |          -0.0132 |          35.4485 |          14.8017 |
[32m[20221213 22:53:37 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.16
[32m[20221213 22:53:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.51
[32m[20221213 22:53:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.52
[32m[20221213 22:53:37 @agent_ppo2.py:143][0m Total time:      35.41 min
[32m[20221213 22:53:37 @agent_ppo2.py:145][0m 3461120 total steps have happened
[32m[20221213 22:53:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1690 --------------------------#
[32m[20221213 22:53:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:53:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0012 |          28.7662 |          14.9045 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0039 |          26.4993 |          14.9057 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0108 |          25.9135 |          14.9071 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |           0.0121 |          28.2273 |          14.8969 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0118 |          25.3133 |          14.8978 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0120 |          25.0855 |          14.9464 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0084 |          24.7294 |          14.9035 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0109 |          24.6090 |          14.9488 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0121 |          24.4820 |          14.9358 |
[32m[20221213 22:53:38 @agent_ppo2.py:185][0m |          -0.0137 |          24.3557 |          14.9274 |
[32m[20221213 22:53:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.73
[32m[20221213 22:53:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.03
[32m[20221213 22:53:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.46
[32m[20221213 22:53:39 @agent_ppo2.py:143][0m Total time:      35.43 min
[32m[20221213 22:53:39 @agent_ppo2.py:145][0m 3463168 total steps have happened
[32m[20221213 22:53:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1691 --------------------------#
[32m[20221213 22:53:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |           0.0020 |          42.1567 |          15.0015 |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |          -0.0014 |          40.1396 |          15.0138 |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |          -0.0018 |          39.4618 |          15.0014 |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |          -0.0039 |          39.8048 |          15.0069 |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |          -0.0033 |          38.8433 |          14.9952 |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |          -0.0053 |          38.7250 |          15.0100 |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |          -0.0056 |          38.4133 |          14.9848 |
[32m[20221213 22:53:39 @agent_ppo2.py:185][0m |          -0.0037 |          38.3756 |          15.0004 |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |          -0.0018 |          38.3502 |          14.9841 |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |          -0.0060 |          38.1273 |          14.9826 |
[32m[20221213 22:53:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.17
[32m[20221213 22:53:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.53
[32m[20221213 22:53:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.21
[32m[20221213 22:53:40 @agent_ppo2.py:143][0m Total time:      35.45 min
[32m[20221213 22:53:40 @agent_ppo2.py:145][0m 3465216 total steps have happened
[32m[20221213 22:53:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1692 --------------------------#
[32m[20221213 22:53:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:53:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |           0.0025 |          42.1844 |          15.0149 |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |           0.0062 |          39.2409 |          14.9738 |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |          -0.0023 |          36.9751 |          15.0193 |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |          -0.0054 |          36.2666 |          15.0040 |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |          -0.0036 |          35.5810 |          15.0325 |
[32m[20221213 22:53:40 @agent_ppo2.py:185][0m |          -0.0085 |          35.2708 |          15.0168 |
[32m[20221213 22:53:41 @agent_ppo2.py:185][0m |          -0.0081 |          34.7655 |          15.0351 |
[32m[20221213 22:53:41 @agent_ppo2.py:185][0m |          -0.0074 |          34.1551 |          15.0457 |
[32m[20221213 22:53:41 @agent_ppo2.py:185][0m |          -0.0090 |          34.3913 |          15.0632 |
[32m[20221213 22:53:41 @agent_ppo2.py:185][0m |          -0.0093 |          34.0450 |          15.0636 |
[32m[20221213 22:53:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.72
[32m[20221213 22:53:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.75
[32m[20221213 22:53:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.49
[32m[20221213 22:53:41 @agent_ppo2.py:143][0m Total time:      35.47 min
[32m[20221213 22:53:41 @agent_ppo2.py:145][0m 3467264 total steps have happened
[32m[20221213 22:53:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1693 --------------------------#
[32m[20221213 22:53:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:41 @agent_ppo2.py:185][0m |           0.0068 |          42.3440 |          14.9039 |
[32m[20221213 22:53:41 @agent_ppo2.py:185][0m |          -0.0033 |          39.1306 |          14.9133 |
[32m[20221213 22:53:41 @agent_ppo2.py:185][0m |          -0.0064 |          38.7026 |          14.9001 |
[32m[20221213 22:53:42 @agent_ppo2.py:185][0m |          -0.0042 |          38.2241 |          14.8976 |
[32m[20221213 22:53:42 @agent_ppo2.py:185][0m |          -0.0074 |          37.5973 |          14.8788 |
[32m[20221213 22:53:42 @agent_ppo2.py:185][0m |          -0.0145 |          37.3293 |          14.8600 |
[32m[20221213 22:53:42 @agent_ppo2.py:185][0m |          -0.0080 |          37.0261 |          14.8613 |
[32m[20221213 22:53:42 @agent_ppo2.py:185][0m |          -0.0051 |          38.6524 |          14.8578 |
[32m[20221213 22:53:42 @agent_ppo2.py:185][0m |          -0.0117 |          36.6169 |          14.8531 |
[32m[20221213 22:53:42 @agent_ppo2.py:185][0m |          -0.0101 |          36.3040 |          14.8457 |
[32m[20221213 22:53:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:53:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.32
[32m[20221213 22:53:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.02
[32m[20221213 22:53:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.23
[32m[20221213 22:53:42 @agent_ppo2.py:143][0m Total time:      35.49 min
[32m[20221213 22:53:42 @agent_ppo2.py:145][0m 3469312 total steps have happened
[32m[20221213 22:53:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1694 --------------------------#
[32m[20221213 22:53:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |           0.0085 |          39.1370 |          14.9811 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0052 |          34.6942 |          14.9632 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0084 |          33.1809 |          14.9613 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0056 |          32.1543 |          14.9841 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0068 |          31.6946 |          15.0116 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0060 |          31.4391 |          15.0053 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0085 |          30.7366 |          15.0185 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0111 |          30.3204 |          15.0233 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0088 |          30.7114 |          15.0294 |
[32m[20221213 22:53:43 @agent_ppo2.py:185][0m |          -0.0116 |          29.5079 |          15.0429 |
[32m[20221213 22:53:43 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.49
[32m[20221213 22:53:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.45
[32m[20221213 22:53:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.24
[32m[20221213 22:53:43 @agent_ppo2.py:143][0m Total time:      35.51 min
[32m[20221213 22:53:43 @agent_ppo2.py:145][0m 3471360 total steps have happened
[32m[20221213 22:53:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1695 --------------------------#
[32m[20221213 22:53:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |           0.0038 |          41.2829 |          15.2663 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |           0.0006 |          35.4623 |          15.2496 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |          -0.0020 |          33.6054 |          15.2140 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |          -0.0075 |          32.4740 |          15.2199 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |          -0.0101 |          31.8866 |          15.1916 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |          -0.0080 |          31.4433 |          15.1873 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |          -0.0092 |          31.1415 |          15.1944 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |          -0.0118 |          31.0347 |          15.2023 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |           0.0009 |          34.0540 |          15.1493 |
[32m[20221213 22:53:44 @agent_ppo2.py:185][0m |          -0.0105 |          30.6113 |          15.1852 |
[32m[20221213 22:53:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.29
[32m[20221213 22:53:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.72
[32m[20221213 22:53:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.34
[32m[20221213 22:53:45 @agent_ppo2.py:143][0m Total time:      35.53 min
[32m[20221213 22:53:45 @agent_ppo2.py:145][0m 3473408 total steps have happened
[32m[20221213 22:53:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1696 --------------------------#
[32m[20221213 22:53:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |           0.0004 |          45.5097 |          14.8510 |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |          -0.0063 |          43.7780 |          14.8441 |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |           0.0071 |          48.2694 |          14.8425 |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |          -0.0083 |          43.0422 |          14.8351 |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |          -0.0108 |          42.5513 |          14.8239 |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |          -0.0071 |          42.3113 |          14.8266 |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |          -0.0103 |          42.2914 |          14.8185 |
[32m[20221213 22:53:45 @agent_ppo2.py:185][0m |          -0.0099 |          41.9373 |          14.8405 |
[32m[20221213 22:53:46 @agent_ppo2.py:185][0m |          -0.0103 |          41.7572 |          14.8338 |
[32m[20221213 22:53:46 @agent_ppo2.py:185][0m |          -0.0137 |          41.6047 |          14.8317 |
[32m[20221213 22:53:46 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.53
[32m[20221213 22:53:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.49
[32m[20221213 22:53:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.30
[32m[20221213 22:53:46 @agent_ppo2.py:143][0m Total time:      35.55 min
[32m[20221213 22:53:46 @agent_ppo2.py:145][0m 3475456 total steps have happened
[32m[20221213 22:53:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1697 --------------------------#
[32m[20221213 22:53:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:46 @agent_ppo2.py:185][0m |          -0.0004 |          47.5606 |          15.0279 |
[32m[20221213 22:53:46 @agent_ppo2.py:185][0m |          -0.0005 |          43.0718 |          14.9649 |
[32m[20221213 22:53:46 @agent_ppo2.py:185][0m |          -0.0024 |          42.9949 |          14.9568 |
[32m[20221213 22:53:46 @agent_ppo2.py:185][0m |          -0.0065 |          41.6847 |          14.9409 |
[32m[20221213 22:53:46 @agent_ppo2.py:185][0m |          -0.0119 |          41.3738 |          14.9196 |
[32m[20221213 22:53:47 @agent_ppo2.py:185][0m |          -0.0132 |          41.2418 |          14.9192 |
[32m[20221213 22:53:47 @agent_ppo2.py:185][0m |          -0.0071 |          40.9547 |          14.8909 |
[32m[20221213 22:53:47 @agent_ppo2.py:185][0m |          -0.0083 |          40.7296 |          14.8814 |
[32m[20221213 22:53:47 @agent_ppo2.py:185][0m |          -0.0145 |          40.5744 |          14.8724 |
[32m[20221213 22:53:47 @agent_ppo2.py:185][0m |          -0.0137 |          40.5991 |          14.8635 |
[32m[20221213 22:53:47 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.74
[32m[20221213 22:53:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.33
[32m[20221213 22:53:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.12
[32m[20221213 22:53:47 @agent_ppo2.py:143][0m Total time:      35.57 min
[32m[20221213 22:53:47 @agent_ppo2.py:145][0m 3477504 total steps have happened
[32m[20221213 22:53:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1698 --------------------------#
[32m[20221213 22:53:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:47 @agent_ppo2.py:185][0m |          -0.0019 |          44.1569 |          14.9046 |
[32m[20221213 22:53:47 @agent_ppo2.py:185][0m |          -0.0063 |          42.1319 |          14.8906 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |           0.0050 |          46.8018 |          14.8987 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |          -0.0070 |          40.5474 |          14.8837 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |          -0.0106 |          39.9442 |          14.8992 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |          -0.0072 |          39.8449 |          14.9040 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |          -0.0089 |          39.3938 |          14.9204 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |          -0.0120 |          39.1674 |          14.9103 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |          -0.0147 |          38.9700 |          14.9036 |
[32m[20221213 22:53:48 @agent_ppo2.py:185][0m |          -0.0080 |          38.9057 |          14.8888 |
[32m[20221213 22:53:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:53:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.76
[32m[20221213 22:53:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.18
[32m[20221213 22:53:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.22
[32m[20221213 22:53:48 @agent_ppo2.py:143][0m Total time:      35.59 min
[32m[20221213 22:53:48 @agent_ppo2.py:145][0m 3479552 total steps have happened
[32m[20221213 22:53:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1699 --------------------------#
[32m[20221213 22:53:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |           0.0001 |          53.6526 |          14.9987 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |          -0.0045 |          52.7916 |          14.9342 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |          -0.0065 |          52.4570 |          14.9777 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |           0.0031 |          57.9960 |          14.9862 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |          -0.0078 |          52.6490 |          14.9845 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |          -0.0103 |          52.0243 |          14.9935 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |          -0.0085 |          51.9772 |          14.9942 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |          -0.0077 |          51.9340 |          14.9983 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |           0.0000 |          53.3924 |          14.9917 |
[32m[20221213 22:53:49 @agent_ppo2.py:185][0m |          -0.0059 |          52.0540 |          14.9871 |
[32m[20221213 22:53:49 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.01
[32m[20221213 22:53:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.41
[32m[20221213 22:53:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.76
[32m[20221213 22:53:49 @agent_ppo2.py:143][0m Total time:      35.61 min
[32m[20221213 22:53:49 @agent_ppo2.py:145][0m 3481600 total steps have happened
[32m[20221213 22:53:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1700 --------------------------#
[32m[20221213 22:53:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:53:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0022 |          37.9108 |          14.8776 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0073 |          34.2459 |          14.8415 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |           0.0010 |          35.9223 |          14.8434 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0082 |          32.2394 |          14.8306 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0152 |          31.8239 |          14.8255 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0107 |          31.5634 |          14.8176 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0066 |          31.0437 |          14.7936 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0132 |          30.9436 |          14.8007 |
[32m[20221213 22:53:50 @agent_ppo2.py:185][0m |          -0.0135 |          30.7340 |          14.7905 |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |          -0.0130 |          30.6315 |          14.7757 |
[32m[20221213 22:53:51 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.14
[32m[20221213 22:53:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.51
[32m[20221213 22:53:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.03
[32m[20221213 22:53:51 @agent_ppo2.py:143][0m Total time:      35.63 min
[32m[20221213 22:53:51 @agent_ppo2.py:145][0m 3483648 total steps have happened
[32m[20221213 22:53:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1701 --------------------------#
[32m[20221213 22:53:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |          -0.0024 |          45.9175 |          14.8422 |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |           0.0014 |          43.9202 |          14.8078 |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |           0.0009 |          45.5463 |          14.8119 |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |           0.0010 |          44.9085 |          14.8222 |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |          -0.0056 |          42.4183 |          14.8213 |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |          -0.0087 |          42.0711 |          14.7973 |
[32m[20221213 22:53:51 @agent_ppo2.py:185][0m |          -0.0094 |          41.8853 |          14.7989 |
[32m[20221213 22:53:52 @agent_ppo2.py:185][0m |          -0.0085 |          41.5489 |          14.8139 |
[32m[20221213 22:53:52 @agent_ppo2.py:185][0m |          -0.0087 |          41.4023 |          14.7998 |
[32m[20221213 22:53:52 @agent_ppo2.py:185][0m |          -0.0117 |          41.1715 |          14.8052 |
[32m[20221213 22:53:52 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.23
[32m[20221213 22:53:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.51
[32m[20221213 22:53:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 177.31
[32m[20221213 22:53:52 @agent_ppo2.py:143][0m Total time:      35.65 min
[32m[20221213 22:53:52 @agent_ppo2.py:145][0m 3485696 total steps have happened
[32m[20221213 22:53:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1702 --------------------------#
[32m[20221213 22:53:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:52 @agent_ppo2.py:185][0m |          -0.0023 |          35.7764 |          15.1105 |
[32m[20221213 22:53:52 @agent_ppo2.py:185][0m |          -0.0051 |          31.4707 |          15.0930 |
[32m[20221213 22:53:52 @agent_ppo2.py:185][0m |          -0.0089 |          30.3037 |          15.1042 |
[32m[20221213 22:53:52 @agent_ppo2.py:185][0m |          -0.0071 |          29.5421 |          15.1038 |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |          -0.0070 |          28.9909 |          15.0945 |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |          -0.0005 |          29.6542 |          15.0940 |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |          -0.0111 |          28.3331 |          15.0915 |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |          -0.0105 |          28.1074 |          15.0807 |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |          -0.0152 |          27.8751 |          15.0845 |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |          -0.0107 |          27.7282 |          15.1063 |
[32m[20221213 22:53:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:53:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.72
[32m[20221213 22:53:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 349.67
[32m[20221213 22:53:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.91
[32m[20221213 22:53:53 @agent_ppo2.py:143][0m Total time:      35.67 min
[32m[20221213 22:53:53 @agent_ppo2.py:145][0m 3487744 total steps have happened
[32m[20221213 22:53:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1703 --------------------------#
[32m[20221213 22:53:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |           0.0026 |          37.1793 |          14.8301 |
[32m[20221213 22:53:53 @agent_ppo2.py:185][0m |          -0.0047 |          35.0485 |          14.7798 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |           0.0021 |          34.6745 |          14.7737 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |          -0.0013 |          34.4161 |          14.7587 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |          -0.0051 |          34.0070 |          14.7307 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |          -0.0059 |          33.8728 |          14.7329 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |          -0.0104 |          33.6496 |          14.7102 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |          -0.0069 |          33.4981 |          14.7213 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |          -0.0025 |          33.8341 |          14.7017 |
[32m[20221213 22:53:54 @agent_ppo2.py:185][0m |           0.0085 |          35.4903 |          14.7006 |
[32m[20221213 22:53:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:53:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.07
[32m[20221213 22:53:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.88
[32m[20221213 22:53:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.92
[32m[20221213 22:53:54 @agent_ppo2.py:143][0m Total time:      35.69 min
[32m[20221213 22:53:54 @agent_ppo2.py:145][0m 3489792 total steps have happened
[32m[20221213 22:53:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1704 --------------------------#
[32m[20221213 22:53:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0015 |          39.3409 |          14.8468 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0036 |          38.0631 |          14.8740 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0067 |          37.4637 |          14.8569 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0071 |          37.3666 |          14.8726 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0057 |          37.3144 |          14.8678 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0048 |          37.4691 |          14.8775 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0064 |          36.9822 |          14.8485 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0091 |          36.9856 |          14.8528 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0096 |          36.8793 |          14.8582 |
[32m[20221213 22:53:55 @agent_ppo2.py:185][0m |          -0.0113 |          36.7966 |          14.8505 |
[32m[20221213 22:53:55 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.75
[32m[20221213 22:53:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.80
[32m[20221213 22:53:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.57
[32m[20221213 22:53:56 @agent_ppo2.py:143][0m Total time:      35.71 min
[32m[20221213 22:53:56 @agent_ppo2.py:145][0m 3491840 total steps have happened
[32m[20221213 22:53:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1705 --------------------------#
[32m[20221213 22:53:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |           0.0002 |          36.0566 |          14.9519 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0084 |          34.2570 |          14.9672 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0123 |          33.9652 |          14.9679 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0094 |          33.7297 |          14.9588 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0131 |          33.4671 |          14.9843 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0113 |          33.4436 |          15.0006 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0114 |          33.2113 |          15.0129 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0115 |          33.0627 |          15.0158 |
[32m[20221213 22:53:56 @agent_ppo2.py:185][0m |          -0.0138 |          32.7766 |          15.0294 |
[32m[20221213 22:53:57 @agent_ppo2.py:185][0m |          -0.0145 |          32.7955 |          15.0415 |
[32m[20221213 22:53:57 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:53:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.36
[32m[20221213 22:53:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.66
[32m[20221213 22:53:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.09
[32m[20221213 22:53:57 @agent_ppo2.py:143][0m Total time:      35.73 min
[32m[20221213 22:53:57 @agent_ppo2.py:145][0m 3493888 total steps have happened
[32m[20221213 22:53:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1706 --------------------------#
[32m[20221213 22:53:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:57 @agent_ppo2.py:185][0m |          -0.0019 |          39.4040 |          14.9708 |
[32m[20221213 22:53:57 @agent_ppo2.py:185][0m |          -0.0017 |          36.8471 |          14.9689 |
[32m[20221213 22:53:57 @agent_ppo2.py:185][0m |           0.0054 |          39.7929 |          14.9934 |
[32m[20221213 22:53:57 @agent_ppo2.py:185][0m |           0.0024 |          36.1276 |          14.9903 |
[32m[20221213 22:53:57 @agent_ppo2.py:185][0m |          -0.0079 |          35.2371 |          14.9843 |
[32m[20221213 22:53:57 @agent_ppo2.py:185][0m |          -0.0072 |          34.9652 |          14.9810 |
[32m[20221213 22:53:58 @agent_ppo2.py:185][0m |          -0.0060 |          34.8660 |          14.9982 |
[32m[20221213 22:53:58 @agent_ppo2.py:185][0m |           0.0002 |          36.2265 |          15.0091 |
[32m[20221213 22:53:58 @agent_ppo2.py:185][0m |          -0.0097 |          34.4403 |          15.0190 |
[32m[20221213 22:53:58 @agent_ppo2.py:185][0m |          -0.0066 |          34.2291 |          15.0078 |
[32m[20221213 22:53:58 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:53:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.70
[32m[20221213 22:53:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.19
[32m[20221213 22:53:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.05
[32m[20221213 22:53:58 @agent_ppo2.py:143][0m Total time:      35.75 min
[32m[20221213 22:53:58 @agent_ppo2.py:145][0m 3495936 total steps have happened
[32m[20221213 22:53:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1707 --------------------------#
[32m[20221213 22:53:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:53:58 @agent_ppo2.py:185][0m |           0.0019 |          53.4464 |          14.9781 |
[32m[20221213 22:53:58 @agent_ppo2.py:185][0m |           0.0009 |          50.6888 |          14.9412 |
[32m[20221213 22:53:58 @agent_ppo2.py:185][0m |          -0.0094 |          49.3339 |          14.9131 |
[32m[20221213 22:53:59 @agent_ppo2.py:185][0m |          -0.0098 |          49.0157 |          14.9417 |
[32m[20221213 22:53:59 @agent_ppo2.py:185][0m |          -0.0107 |          48.7073 |          14.9576 |
[32m[20221213 22:53:59 @agent_ppo2.py:185][0m |          -0.0117 |          48.4081 |          14.9550 |
[32m[20221213 22:53:59 @agent_ppo2.py:185][0m |          -0.0074 |          48.5496 |          14.9671 |
[32m[20221213 22:53:59 @agent_ppo2.py:185][0m |          -0.0143 |          48.0647 |          14.9668 |
[32m[20221213 22:53:59 @agent_ppo2.py:185][0m |          -0.0003 |          49.9845 |          14.9672 |
[32m[20221213 22:53:59 @agent_ppo2.py:185][0m |          -0.0006 |          53.3365 |          14.9590 |
[32m[20221213 22:53:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:53:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.53
[32m[20221213 22:53:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.39
[32m[20221213 22:53:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.58
[32m[20221213 22:53:59 @agent_ppo2.py:143][0m Total time:      35.77 min
[32m[20221213 22:53:59 @agent_ppo2.py:145][0m 3497984 total steps have happened
[32m[20221213 22:53:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1708 --------------------------#
[32m[20221213 22:53:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:53:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |           0.0078 |          47.9766 |          14.8877 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |           0.0009 |          46.4242 |          14.9069 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0046 |          44.4006 |          14.9226 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0111 |          43.6965 |          14.9231 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0078 |          43.7185 |          14.9535 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0120 |          42.9209 |          14.9345 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0123 |          42.5521 |          14.9625 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0082 |          42.5875 |          14.9483 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0142 |          42.3388 |          14.9431 |
[32m[20221213 22:54:00 @agent_ppo2.py:185][0m |          -0.0096 |          42.1283 |          14.9779 |
[32m[20221213 22:54:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:54:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.41
[32m[20221213 22:54:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.01
[32m[20221213 22:54:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.32
[32m[20221213 22:54:00 @agent_ppo2.py:143][0m Total time:      35.79 min
[32m[20221213 22:54:00 @agent_ppo2.py:145][0m 3500032 total steps have happened
[32m[20221213 22:54:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1709 --------------------------#
[32m[20221213 22:54:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0005 |          37.3699 |          15.0778 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0080 |          34.7530 |          15.0873 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0033 |          34.4651 |          15.0728 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0006 |          34.1455 |          15.0537 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0078 |          33.2927 |          15.0792 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0133 |          33.1714 |          15.0439 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0087 |          32.7679 |          15.0772 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0135 |          32.6935 |          15.0563 |
[32m[20221213 22:54:01 @agent_ppo2.py:185][0m |          -0.0126 |          32.5037 |          15.0425 |
[32m[20221213 22:54:02 @agent_ppo2.py:185][0m |          -0.0141 |          32.3990 |          15.0427 |
[32m[20221213 22:54:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:54:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.37
[32m[20221213 22:54:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.49
[32m[20221213 22:54:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.77
[32m[20221213 22:54:02 @agent_ppo2.py:143][0m Total time:      35.81 min
[32m[20221213 22:54:02 @agent_ppo2.py:145][0m 3502080 total steps have happened
[32m[20221213 22:54:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1710 --------------------------#
[32m[20221213 22:54:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:54:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:02 @agent_ppo2.py:185][0m |          -0.0002 |          39.5917 |          15.0489 |
[32m[20221213 22:54:02 @agent_ppo2.py:185][0m |          -0.0049 |          37.1888 |          15.0470 |
[32m[20221213 22:54:02 @agent_ppo2.py:185][0m |          -0.0074 |          36.2407 |          15.0422 |
[32m[20221213 22:54:02 @agent_ppo2.py:185][0m |          -0.0098 |          36.0955 |          15.0166 |
[32m[20221213 22:54:02 @agent_ppo2.py:185][0m |          -0.0100 |          35.3093 |          15.0304 |
[32m[20221213 22:54:02 @agent_ppo2.py:185][0m |          -0.0128 |          34.9471 |          15.0228 |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0026 |          38.0231 |          15.0033 |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0099 |          34.9123 |          14.9742 |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0104 |          34.8878 |          15.0034 |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0144 |          34.4614 |          14.9900 |
[32m[20221213 22:54:03 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.69
[32m[20221213 22:54:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.65
[32m[20221213 22:54:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.18
[32m[20221213 22:54:03 @agent_ppo2.py:143][0m Total time:      35.83 min
[32m[20221213 22:54:03 @agent_ppo2.py:145][0m 3504128 total steps have happened
[32m[20221213 22:54:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1711 --------------------------#
[32m[20221213 22:54:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0018 |          40.6072 |          14.8674 |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0066 |          37.2581 |          14.9016 |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0050 |          36.0830 |          14.8869 |
[32m[20221213 22:54:03 @agent_ppo2.py:185][0m |          -0.0048 |          35.4853 |          14.9175 |
[32m[20221213 22:54:04 @agent_ppo2.py:185][0m |          -0.0065 |          35.1685 |          14.9229 |
[32m[20221213 22:54:04 @agent_ppo2.py:185][0m |          -0.0078 |          34.7424 |          14.9345 |
[32m[20221213 22:54:04 @agent_ppo2.py:185][0m |          -0.0073 |          34.7415 |          14.9464 |
[32m[20221213 22:54:04 @agent_ppo2.py:185][0m |          -0.0103 |          34.3476 |          14.9528 |
[32m[20221213 22:54:04 @agent_ppo2.py:185][0m |          -0.0101 |          34.2869 |          14.9673 |
[32m[20221213 22:54:04 @agent_ppo2.py:185][0m |          -0.0091 |          34.1751 |          14.9657 |
[32m[20221213 22:54:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:54:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.39
[32m[20221213 22:54:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.34
[32m[20221213 22:54:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.89
[32m[20221213 22:54:04 @agent_ppo2.py:143][0m Total time:      35.85 min
[32m[20221213 22:54:04 @agent_ppo2.py:145][0m 3506176 total steps have happened
[32m[20221213 22:54:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1712 --------------------------#
[32m[20221213 22:54:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:04 @agent_ppo2.py:185][0m |           0.0017 |          31.3350 |          15.2148 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0014 |          27.8778 |          15.2181 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0016 |          27.2868 |          15.2059 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0045 |          26.9607 |          15.2084 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0064 |          26.7065 |          15.2019 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0050 |          26.5956 |          15.2098 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0011 |          26.9402 |          15.2047 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0079 |          26.3040 |          15.1874 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0120 |          26.1865 |          15.2043 |
[32m[20221213 22:54:05 @agent_ppo2.py:185][0m |          -0.0086 |          26.1576 |          15.2055 |
[32m[20221213 22:54:05 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.49
[32m[20221213 22:54:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.14
[32m[20221213 22:54:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.07
[32m[20221213 22:54:05 @agent_ppo2.py:143][0m Total time:      35.87 min
[32m[20221213 22:54:05 @agent_ppo2.py:145][0m 3508224 total steps have happened
[32m[20221213 22:54:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1713 --------------------------#
[32m[20221213 22:54:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |           0.0008 |          44.7167 |          15.1716 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |          -0.0066 |          43.1551 |          15.1572 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |          -0.0031 |          42.7070 |          15.1484 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |           0.0070 |          46.2895 |          15.1432 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |          -0.0094 |          41.6996 |          15.1417 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |          -0.0100 |          41.3587 |          15.1485 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |          -0.0106 |          41.1611 |          15.1681 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |          -0.0113 |          41.1454 |          15.1577 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |          -0.0145 |          40.9241 |          15.1639 |
[32m[20221213 22:54:06 @agent_ppo2.py:185][0m |           0.0081 |          48.3249 |          15.1693 |
[32m[20221213 22:54:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.32
[32m[20221213 22:54:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.33
[32m[20221213 22:54:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.63
[32m[20221213 22:54:07 @agent_ppo2.py:143][0m Total time:      35.89 min
[32m[20221213 22:54:07 @agent_ppo2.py:145][0m 3510272 total steps have happened
[32m[20221213 22:54:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1714 --------------------------#
[32m[20221213 22:54:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |           0.0016 |          52.8449 |          15.2655 |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |          -0.0029 |          48.1020 |          15.2781 |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |          -0.0068 |          46.2584 |          15.2696 |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |          -0.0120 |          45.2066 |          15.2964 |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |          -0.0072 |          44.0124 |          15.2947 |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |          -0.0127 |          43.4923 |          15.3129 |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |          -0.0119 |          42.5423 |          15.3056 |
[32m[20221213 22:54:07 @agent_ppo2.py:185][0m |          -0.0143 |          42.1939 |          15.3367 |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |          -0.0113 |          41.6885 |          15.3459 |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |          -0.0016 |          46.7724 |          15.3393 |
[32m[20221213 22:54:08 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.76
[32m[20221213 22:54:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.05
[32m[20221213 22:54:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.49
[32m[20221213 22:54:08 @agent_ppo2.py:143][0m Total time:      35.91 min
[32m[20221213 22:54:08 @agent_ppo2.py:145][0m 3512320 total steps have happened
[32m[20221213 22:54:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1715 --------------------------#
[32m[20221213 22:54:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |           0.0076 |          50.8990 |          15.2144 |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |          -0.0079 |          47.9818 |          15.1710 |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |          -0.0040 |          47.5236 |          15.1559 |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |           0.0108 |          51.9216 |          15.1468 |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |          -0.0096 |          47.0474 |          15.1241 |
[32m[20221213 22:54:08 @agent_ppo2.py:185][0m |           0.0082 |          51.2104 |          15.1755 |
[32m[20221213 22:54:09 @agent_ppo2.py:185][0m |          -0.0089 |          46.9760 |          15.1888 |
[32m[20221213 22:54:09 @agent_ppo2.py:185][0m |          -0.0077 |          46.4184 |          15.1820 |
[32m[20221213 22:54:09 @agent_ppo2.py:185][0m |          -0.0102 |          46.4317 |          15.1490 |
[32m[20221213 22:54:09 @agent_ppo2.py:185][0m |          -0.0073 |          46.2977 |          15.1846 |
[32m[20221213 22:54:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.08
[32m[20221213 22:54:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.85
[32m[20221213 22:54:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.98
[32m[20221213 22:54:09 @agent_ppo2.py:143][0m Total time:      35.93 min
[32m[20221213 22:54:09 @agent_ppo2.py:145][0m 3514368 total steps have happened
[32m[20221213 22:54:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1716 --------------------------#
[32m[20221213 22:54:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:09 @agent_ppo2.py:185][0m |           0.0051 |          41.0506 |          15.1926 |
[32m[20221213 22:54:09 @agent_ppo2.py:185][0m |           0.0018 |          32.3691 |          15.1725 |
[32m[20221213 22:54:09 @agent_ppo2.py:185][0m |          -0.0005 |          31.3473 |          15.1835 |
[32m[20221213 22:54:10 @agent_ppo2.py:185][0m |          -0.0051 |          30.8986 |          15.1858 |
[32m[20221213 22:54:10 @agent_ppo2.py:185][0m |          -0.0042 |          30.3317 |          15.1701 |
[32m[20221213 22:54:10 @agent_ppo2.py:185][0m |          -0.0048 |          30.4007 |          15.1865 |
[32m[20221213 22:54:10 @agent_ppo2.py:185][0m |          -0.0132 |          29.9239 |          15.1856 |
[32m[20221213 22:54:10 @agent_ppo2.py:185][0m |          -0.0110 |          29.7932 |          15.1795 |
[32m[20221213 22:54:10 @agent_ppo2.py:185][0m |          -0.0048 |          29.5125 |          15.1675 |
[32m[20221213 22:54:10 @agent_ppo2.py:185][0m |          -0.0112 |          30.5919 |          15.1554 |
[32m[20221213 22:54:10 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.59
[32m[20221213 22:54:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 358.94
[32m[20221213 22:54:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.10
[32m[20221213 22:54:10 @agent_ppo2.py:143][0m Total time:      35.95 min
[32m[20221213 22:54:10 @agent_ppo2.py:145][0m 3516416 total steps have happened
[32m[20221213 22:54:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1717 --------------------------#
[32m[20221213 22:54:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0054 |          31.9027 |          15.3330 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0104 |          28.6537 |          15.3057 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0052 |          28.1251 |          15.2872 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0062 |          27.3710 |          15.2828 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0099 |          26.9643 |          15.2451 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0063 |          26.9348 |          15.2231 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0118 |          26.6559 |          15.2165 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0033 |          27.3890 |          15.2126 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0140 |          26.1168 |          15.2057 |
[32m[20221213 22:54:11 @agent_ppo2.py:185][0m |          -0.0153 |          25.9509 |          15.1798 |
[32m[20221213 22:54:11 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.55
[32m[20221213 22:54:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.68
[32m[20221213 22:54:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.51
[32m[20221213 22:54:11 @agent_ppo2.py:143][0m Total time:      35.97 min
[32m[20221213 22:54:11 @agent_ppo2.py:145][0m 3518464 total steps have happened
[32m[20221213 22:54:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1718 --------------------------#
[32m[20221213 22:54:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |           0.0071 |          48.4385 |          14.9417 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0017 |          45.1998 |          14.9304 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |           0.0051 |          47.1346 |          14.9279 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0042 |          44.4475 |          14.9251 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0094 |          43.8218 |          14.9397 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0074 |          43.4065 |          14.9286 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0064 |          43.3622 |          14.9455 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0073 |          43.2244 |          14.9483 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0114 |          42.9852 |          14.9260 |
[32m[20221213 22:54:12 @agent_ppo2.py:185][0m |          -0.0095 |          42.8697 |          14.9262 |
[32m[20221213 22:54:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.81
[32m[20221213 22:54:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.33
[32m[20221213 22:54:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.17
[32m[20221213 22:54:13 @agent_ppo2.py:143][0m Total time:      36.00 min
[32m[20221213 22:54:13 @agent_ppo2.py:145][0m 3520512 total steps have happened
[32m[20221213 22:54:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1719 --------------------------#
[32m[20221213 22:54:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:13 @agent_ppo2.py:185][0m |           0.0032 |          41.7741 |          14.9315 |
[32m[20221213 22:54:13 @agent_ppo2.py:185][0m |          -0.0009 |          38.7988 |          14.9635 |
[32m[20221213 22:54:13 @agent_ppo2.py:185][0m |          -0.0103 |          37.9591 |          14.9398 |
[32m[20221213 22:54:13 @agent_ppo2.py:185][0m |          -0.0136 |          37.0426 |          14.9579 |
[32m[20221213 22:54:13 @agent_ppo2.py:185][0m |          -0.0115 |          36.4802 |          14.9433 |
[32m[20221213 22:54:13 @agent_ppo2.py:185][0m |          -0.0124 |          36.1840 |          14.9507 |
[32m[20221213 22:54:13 @agent_ppo2.py:185][0m |          -0.0123 |          35.6929 |          14.9431 |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |          -0.0114 |          35.4247 |          14.9478 |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |          -0.0086 |          35.2152 |          14.9635 |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |          -0.0110 |          34.7391 |          14.9477 |
[32m[20221213 22:54:14 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:54:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.89
[32m[20221213 22:54:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.02
[32m[20221213 22:54:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.77
[32m[20221213 22:54:14 @agent_ppo2.py:143][0m Total time:      36.02 min
[32m[20221213 22:54:14 @agent_ppo2.py:145][0m 3522560 total steps have happened
[32m[20221213 22:54:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1720 --------------------------#
[32m[20221213 22:54:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:54:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |           0.0009 |          52.4415 |          15.1019 |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |          -0.0040 |          51.0813 |          15.1148 |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |          -0.0051 |          50.7157 |          15.1095 |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |          -0.0085 |          50.5167 |          15.1155 |
[32m[20221213 22:54:14 @agent_ppo2.py:185][0m |          -0.0056 |          50.6603 |          15.1333 |
[32m[20221213 22:54:15 @agent_ppo2.py:185][0m |          -0.0072 |          50.4578 |          15.1254 |
[32m[20221213 22:54:15 @agent_ppo2.py:185][0m |           0.0009 |          51.5938 |          15.1306 |
[32m[20221213 22:54:15 @agent_ppo2.py:185][0m |          -0.0094 |          50.1586 |          15.1450 |
[32m[20221213 22:54:15 @agent_ppo2.py:185][0m |          -0.0095 |          50.1838 |          15.1478 |
[32m[20221213 22:54:15 @agent_ppo2.py:185][0m |          -0.0104 |          50.1532 |          15.1651 |
[32m[20221213 22:54:15 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:54:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.53
[32m[20221213 22:54:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.01
[32m[20221213 22:54:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.70
[32m[20221213 22:54:15 @agent_ppo2.py:143][0m Total time:      36.04 min
[32m[20221213 22:54:15 @agent_ppo2.py:145][0m 3524608 total steps have happened
[32m[20221213 22:54:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1721 --------------------------#
[32m[20221213 22:54:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:15 @agent_ppo2.py:185][0m |          -0.0016 |          37.1398 |          15.0489 |
[32m[20221213 22:54:15 @agent_ppo2.py:185][0m |          -0.0061 |          35.3324 |          15.0390 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0098 |          34.8115 |          15.0740 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0110 |          34.6269 |          15.0775 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0112 |          34.2836 |          15.0949 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0098 |          34.1833 |          15.1008 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0121 |          34.0501 |          15.1292 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0138 |          33.8623 |          15.1208 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0146 |          33.7375 |          15.1339 |
[32m[20221213 22:54:16 @agent_ppo2.py:185][0m |          -0.0157 |          33.4841 |          15.1391 |
[32m[20221213 22:54:16 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.14
[32m[20221213 22:54:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.18
[32m[20221213 22:54:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.12
[32m[20221213 22:54:16 @agent_ppo2.py:143][0m Total time:      36.06 min
[32m[20221213 22:54:16 @agent_ppo2.py:145][0m 3526656 total steps have happened
[32m[20221213 22:54:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1722 --------------------------#
[32m[20221213 22:54:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |           0.0076 |          50.7775 |          15.5316 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0044 |          45.1873 |          15.5122 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0077 |          43.7276 |          15.4665 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0129 |          43.0899 |          15.4968 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0097 |          42.3663 |          15.4650 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0075 |          41.8940 |          15.4920 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0155 |          41.5148 |          15.4665 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0149 |          41.1936 |          15.4680 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0178 |          40.9588 |          15.4564 |
[32m[20221213 22:54:17 @agent_ppo2.py:185][0m |          -0.0171 |          40.5833 |          15.4598 |
[32m[20221213 22:54:17 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.70
[32m[20221213 22:54:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 368.83
[32m[20221213 22:54:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.57
[32m[20221213 22:54:17 @agent_ppo2.py:143][0m Total time:      36.08 min
[32m[20221213 22:54:17 @agent_ppo2.py:145][0m 3528704 total steps have happened
[32m[20221213 22:54:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1723 --------------------------#
[32m[20221213 22:54:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |           0.0113 |          47.0937 |          15.3334 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0036 |          43.1504 |          15.3114 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0064 |          42.5800 |          15.2861 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0093 |          42.1638 |          15.2899 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0021 |          42.4034 |          15.2688 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0084 |          41.8541 |          15.2511 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0133 |          41.6110 |          15.2546 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0085 |          41.3739 |          15.2611 |
[32m[20221213 22:54:18 @agent_ppo2.py:185][0m |          -0.0089 |          41.2722 |          15.2401 |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |          -0.0122 |          40.9890 |          15.2412 |
[32m[20221213 22:54:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:54:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.72
[32m[20221213 22:54:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.62
[32m[20221213 22:54:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.47
[32m[20221213 22:54:19 @agent_ppo2.py:143][0m Total time:      36.10 min
[32m[20221213 22:54:19 @agent_ppo2.py:145][0m 3530752 total steps have happened
[32m[20221213 22:54:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1724 --------------------------#
[32m[20221213 22:54:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |           0.0010 |          46.7324 |          15.2662 |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |          -0.0028 |          43.7164 |          15.2710 |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |          -0.0080 |          42.9459 |          15.2623 |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |          -0.0101 |          42.4986 |          15.2629 |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |          -0.0090 |          42.0644 |          15.2721 |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |          -0.0083 |          42.2563 |          15.2497 |
[32m[20221213 22:54:19 @agent_ppo2.py:185][0m |          -0.0087 |          41.8020 |          15.2679 |
[32m[20221213 22:54:20 @agent_ppo2.py:185][0m |          -0.0021 |          44.2781 |          15.2543 |
[32m[20221213 22:54:20 @agent_ppo2.py:185][0m |          -0.0137 |          41.5265 |          15.2827 |
[32m[20221213 22:54:20 @agent_ppo2.py:185][0m |          -0.0144 |          41.2976 |          15.2592 |
[32m[20221213 22:54:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.35
[32m[20221213 22:54:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.74
[32m[20221213 22:54:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.30
[32m[20221213 22:54:20 @agent_ppo2.py:143][0m Total time:      36.12 min
[32m[20221213 22:54:20 @agent_ppo2.py:145][0m 3532800 total steps have happened
[32m[20221213 22:54:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1725 --------------------------#
[32m[20221213 22:54:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:20 @agent_ppo2.py:185][0m |           0.0013 |          37.1138 |          15.0085 |
[32m[20221213 22:54:20 @agent_ppo2.py:185][0m |          -0.0058 |          33.8847 |          15.0177 |
[32m[20221213 22:54:20 @agent_ppo2.py:185][0m |          -0.0084 |          32.8361 |          15.0229 |
[32m[20221213 22:54:20 @agent_ppo2.py:185][0m |          -0.0127 |          32.3343 |          14.9949 |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0101 |          32.0757 |          15.0137 |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0112 |          31.5870 |          14.9976 |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0085 |          31.4448 |          15.0021 |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0132 |          31.1055 |          15.0182 |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0172 |          30.9977 |          15.0119 |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0113 |          30.9304 |          15.0057 |
[32m[20221213 22:54:21 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.39
[32m[20221213 22:54:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.94
[32m[20221213 22:54:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.97
[32m[20221213 22:54:21 @agent_ppo2.py:143][0m Total time:      36.14 min
[32m[20221213 22:54:21 @agent_ppo2.py:145][0m 3534848 total steps have happened
[32m[20221213 22:54:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1726 --------------------------#
[32m[20221213 22:54:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0011 |          33.4846 |          14.9562 |
[32m[20221213 22:54:21 @agent_ppo2.py:185][0m |          -0.0019 |          31.4323 |          14.9473 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0015 |          30.7786 |          14.9494 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0074 |          30.1776 |          14.9473 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0088 |          29.8779 |          14.9387 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0074 |          29.6641 |          14.9459 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0078 |          29.5444 |          14.9662 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0104 |          29.2939 |          14.9804 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0068 |          29.3308 |          14.9575 |
[32m[20221213 22:54:22 @agent_ppo2.py:185][0m |          -0.0095 |          29.1018 |          14.9811 |
[32m[20221213 22:54:22 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.57
[32m[20221213 22:54:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.53
[32m[20221213 22:54:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.52
[32m[20221213 22:54:22 @agent_ppo2.py:143][0m Total time:      36.16 min
[32m[20221213 22:54:22 @agent_ppo2.py:145][0m 3536896 total steps have happened
[32m[20221213 22:54:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1727 --------------------------#
[32m[20221213 22:54:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |           0.0007 |          37.9918 |          14.9566 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0026 |          36.7692 |          14.9720 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0070 |          36.5359 |          14.9538 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0064 |          36.1619 |          14.9617 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0060 |          35.8799 |          14.9538 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0086 |          35.8851 |          14.9812 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0065 |          35.7690 |          14.9863 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0118 |          35.5726 |          14.9831 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0084 |          35.2813 |          14.9877 |
[32m[20221213 22:54:23 @agent_ppo2.py:185][0m |          -0.0115 |          35.3793 |          15.0058 |
[32m[20221213 22:54:23 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.58
[32m[20221213 22:54:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.82
[32m[20221213 22:54:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.34
[32m[20221213 22:54:24 @agent_ppo2.py:143][0m Total time:      36.18 min
[32m[20221213 22:54:24 @agent_ppo2.py:145][0m 3538944 total steps have happened
[32m[20221213 22:54:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1728 --------------------------#
[32m[20221213 22:54:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0031 |          39.4531 |          15.3427 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0071 |          36.3931 |          15.3285 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0078 |          34.8812 |          15.3161 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0115 |          33.8178 |          15.3111 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0057 |          33.1659 |          15.3095 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0122 |          32.5873 |          15.3086 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0134 |          32.3497 |          15.3097 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0142 |          32.0554 |          15.2838 |
[32m[20221213 22:54:24 @agent_ppo2.py:185][0m |          -0.0140 |          31.7739 |          15.2735 |
[32m[20221213 22:54:25 @agent_ppo2.py:185][0m |          -0.0160 |          31.3845 |          15.2786 |
[32m[20221213 22:54:25 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.43
[32m[20221213 22:54:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.25
[32m[20221213 22:54:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.23
[32m[20221213 22:54:25 @agent_ppo2.py:143][0m Total time:      36.20 min
[32m[20221213 22:54:25 @agent_ppo2.py:145][0m 3540992 total steps have happened
[32m[20221213 22:54:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1729 --------------------------#
[32m[20221213 22:54:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:25 @agent_ppo2.py:185][0m |          -0.0031 |          45.9180 |          15.0444 |
[32m[20221213 22:54:25 @agent_ppo2.py:185][0m |          -0.0038 |          42.4525 |          15.0229 |
[32m[20221213 22:54:25 @agent_ppo2.py:185][0m |          -0.0030 |          41.3278 |          15.0484 |
[32m[20221213 22:54:25 @agent_ppo2.py:185][0m |          -0.0083 |          40.9165 |          15.0573 |
[32m[20221213 22:54:25 @agent_ppo2.py:185][0m |          -0.0070 |          40.6465 |          15.0522 |
[32m[20221213 22:54:25 @agent_ppo2.py:185][0m |          -0.0152 |          40.2420 |          15.0525 |
[32m[20221213 22:54:26 @agent_ppo2.py:185][0m |          -0.0092 |          40.0424 |          15.0584 |
[32m[20221213 22:54:26 @agent_ppo2.py:185][0m |          -0.0154 |          39.6933 |          15.0655 |
[32m[20221213 22:54:26 @agent_ppo2.py:185][0m |          -0.0113 |          39.7530 |          15.0622 |
[32m[20221213 22:54:26 @agent_ppo2.py:185][0m |          -0.0058 |          41.5537 |          15.0645 |
[32m[20221213 22:54:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:54:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.46
[32m[20221213 22:54:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.47
[32m[20221213 22:54:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.77
[32m[20221213 22:54:26 @agent_ppo2.py:143][0m Total time:      36.22 min
[32m[20221213 22:54:26 @agent_ppo2.py:145][0m 3543040 total steps have happened
[32m[20221213 22:54:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1730 --------------------------#
[32m[20221213 22:54:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:54:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:26 @agent_ppo2.py:185][0m |           0.0015 |          53.5568 |          15.3188 |
[32m[20221213 22:54:26 @agent_ppo2.py:185][0m |           0.0064 |          57.1471 |          15.2979 |
[32m[20221213 22:54:26 @agent_ppo2.py:185][0m |          -0.0030 |          51.1522 |          15.3209 |
[32m[20221213 22:54:27 @agent_ppo2.py:185][0m |          -0.0065 |          50.5736 |          15.3160 |
[32m[20221213 22:54:27 @agent_ppo2.py:185][0m |          -0.0093 |          50.3760 |          15.3097 |
[32m[20221213 22:54:27 @agent_ppo2.py:185][0m |          -0.0093 |          50.2323 |          15.2976 |
[32m[20221213 22:54:27 @agent_ppo2.py:185][0m |          -0.0100 |          49.8960 |          15.3101 |
[32m[20221213 22:54:27 @agent_ppo2.py:185][0m |          -0.0101 |          49.9917 |          15.3093 |
[32m[20221213 22:54:27 @agent_ppo2.py:185][0m |          -0.0092 |          49.9598 |          15.2935 |
[32m[20221213 22:54:27 @agent_ppo2.py:185][0m |          -0.0127 |          49.6931 |          15.3046 |
[32m[20221213 22:54:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:54:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.71
[32m[20221213 22:54:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.05
[32m[20221213 22:54:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.63
[32m[20221213 22:54:27 @agent_ppo2.py:143][0m Total time:      36.24 min
[32m[20221213 22:54:27 @agent_ppo2.py:145][0m 3545088 total steps have happened
[32m[20221213 22:54:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1731 --------------------------#
[32m[20221213 22:54:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:54:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |           0.0013 |          34.8474 |          15.1442 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |          -0.0033 |          31.4834 |          15.1554 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |           0.0035 |          34.5299 |          15.1687 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |           0.0000 |          29.7181 |          15.1481 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |          -0.0077 |          28.9714 |          15.1499 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |          -0.0075 |          28.6073 |          15.1489 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |          -0.0109 |          28.1474 |          15.1397 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |          -0.0115 |          28.1336 |          15.1448 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |          -0.0125 |          27.8172 |          15.1329 |
[32m[20221213 22:54:28 @agent_ppo2.py:185][0m |          -0.0126 |          27.5616 |          15.1357 |
[32m[20221213 22:54:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:54:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.55
[32m[20221213 22:54:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.60
[32m[20221213 22:54:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.92
[32m[20221213 22:54:29 @agent_ppo2.py:143][0m Total time:      36.26 min
[32m[20221213 22:54:29 @agent_ppo2.py:145][0m 3547136 total steps have happened
[32m[20221213 22:54:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1732 --------------------------#
[32m[20221213 22:54:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |           0.0026 |          45.5199 |          15.1528 |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |          -0.0051 |          43.6149 |          15.1691 |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |          -0.0076 |          43.0907 |          15.1437 |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |          -0.0072 |          42.6578 |          15.1524 |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |          -0.0051 |          42.6312 |          15.1454 |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |          -0.0123 |          42.2761 |          15.1402 |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |          -0.0067 |          42.8864 |          15.1456 |
[32m[20221213 22:54:29 @agent_ppo2.py:185][0m |          -0.0099 |          42.1102 |          15.1256 |
[32m[20221213 22:54:30 @agent_ppo2.py:185][0m |          -0.0113 |          41.9658 |          15.1165 |
[32m[20221213 22:54:30 @agent_ppo2.py:185][0m |          -0.0133 |          41.8491 |          15.1204 |
[32m[20221213 22:54:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:54:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.68
[32m[20221213 22:54:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.39
[32m[20221213 22:54:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.03
[32m[20221213 22:54:30 @agent_ppo2.py:143][0m Total time:      36.28 min
[32m[20221213 22:54:30 @agent_ppo2.py:145][0m 3549184 total steps have happened
[32m[20221213 22:54:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1733 --------------------------#
[32m[20221213 22:54:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:30 @agent_ppo2.py:185][0m |           0.0111 |          47.2122 |          14.8455 |
[32m[20221213 22:54:30 @agent_ppo2.py:185][0m |          -0.0001 |          43.8305 |          14.8182 |
[32m[20221213 22:54:30 @agent_ppo2.py:185][0m |          -0.0019 |          43.2456 |          14.8341 |
[32m[20221213 22:54:30 @agent_ppo2.py:185][0m |          -0.0053 |          42.8874 |          14.8137 |
[32m[20221213 22:54:30 @agent_ppo2.py:185][0m |          -0.0050 |          43.2630 |          14.8251 |
[32m[20221213 22:54:31 @agent_ppo2.py:185][0m |          -0.0083 |          42.2025 |          14.8190 |
[32m[20221213 22:54:31 @agent_ppo2.py:185][0m |          -0.0062 |          42.0558 |          14.8066 |
[32m[20221213 22:54:31 @agent_ppo2.py:185][0m |          -0.0085 |          41.8111 |          14.8288 |
[32m[20221213 22:54:31 @agent_ppo2.py:185][0m |          -0.0075 |          41.8347 |          14.8267 |
[32m[20221213 22:54:31 @agent_ppo2.py:185][0m |          -0.0101 |          41.5159 |          14.8404 |
[32m[20221213 22:54:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:54:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.10
[32m[20221213 22:54:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.77
[32m[20221213 22:54:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.05
[32m[20221213 22:54:31 @agent_ppo2.py:143][0m Total time:      36.30 min
[32m[20221213 22:54:31 @agent_ppo2.py:145][0m 3551232 total steps have happened
[32m[20221213 22:54:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1734 --------------------------#
[32m[20221213 22:54:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:31 @agent_ppo2.py:185][0m |          -0.0067 |          51.1559 |          15.3416 |
[32m[20221213 22:54:31 @agent_ppo2.py:185][0m |          -0.0056 |          48.7813 |          15.3717 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0101 |          47.9940 |          15.3739 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0039 |          47.7973 |          15.3965 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0094 |          46.9059 |          15.3978 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0137 |          46.6657 |          15.3857 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0100 |          46.2863 |          15.4166 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0082 |          46.2591 |          15.4187 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0095 |          45.8692 |          15.4183 |
[32m[20221213 22:54:32 @agent_ppo2.py:185][0m |          -0.0145 |          45.5947 |          15.4497 |
[32m[20221213 22:54:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.46
[32m[20221213 22:54:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.25
[32m[20221213 22:54:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.31
[32m[20221213 22:54:32 @agent_ppo2.py:143][0m Total time:      36.32 min
[32m[20221213 22:54:32 @agent_ppo2.py:145][0m 3553280 total steps have happened
[32m[20221213 22:54:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1735 --------------------------#
[32m[20221213 22:54:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |           0.0007 |          47.4884 |          15.2813 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0019 |          45.7917 |          15.2698 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0025 |          45.1815 |          15.2365 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |           0.0025 |          45.4936 |          15.2743 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0068 |          44.3409 |          15.2686 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0090 |          44.1939 |          15.2414 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0099 |          44.2152 |          15.2433 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0081 |          43.7812 |          15.2467 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0106 |          43.8298 |          15.2392 |
[32m[20221213 22:54:33 @agent_ppo2.py:185][0m |          -0.0122 |          43.7328 |          15.2292 |
[32m[20221213 22:54:33 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:54:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.98
[32m[20221213 22:54:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.22
[32m[20221213 22:54:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.82
[32m[20221213 22:54:33 @agent_ppo2.py:143][0m Total time:      36.34 min
[32m[20221213 22:54:33 @agent_ppo2.py:145][0m 3555328 total steps have happened
[32m[20221213 22:54:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1736 --------------------------#
[32m[20221213 22:54:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0003 |          54.5581 |          15.2893 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0036 |          51.0719 |          15.2887 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0079 |          50.8183 |          15.2928 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0030 |          50.8097 |          15.3003 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0137 |          49.9423 |          15.3077 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0047 |          49.9052 |          15.3095 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0086 |          50.0179 |          15.3111 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0033 |          49.5913 |          15.3060 |
[32m[20221213 22:54:34 @agent_ppo2.py:185][0m |          -0.0117 |          49.5323 |          15.3054 |
[32m[20221213 22:54:35 @agent_ppo2.py:185][0m |          -0.0162 |          49.3812 |          15.3175 |
[32m[20221213 22:54:35 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.54
[32m[20221213 22:54:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.49
[32m[20221213 22:54:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.30
[32m[20221213 22:54:35 @agent_ppo2.py:143][0m Total time:      36.36 min
[32m[20221213 22:54:35 @agent_ppo2.py:145][0m 3557376 total steps have happened
[32m[20221213 22:54:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1737 --------------------------#
[32m[20221213 22:54:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:35 @agent_ppo2.py:185][0m |           0.0034 |          41.7069 |          15.1352 |
[32m[20221213 22:54:35 @agent_ppo2.py:185][0m |          -0.0073 |          38.0066 |          15.1651 |
[32m[20221213 22:54:35 @agent_ppo2.py:185][0m |          -0.0093 |          36.8887 |          15.1479 |
[32m[20221213 22:54:35 @agent_ppo2.py:185][0m |          -0.0120 |          36.1828 |          15.1579 |
[32m[20221213 22:54:35 @agent_ppo2.py:185][0m |          -0.0094 |          35.5728 |          15.1709 |
[32m[20221213 22:54:35 @agent_ppo2.py:185][0m |          -0.0084 |          35.2070 |          15.1709 |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0120 |          35.0105 |          15.1772 |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0152 |          34.7593 |          15.2050 |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0144 |          34.5261 |          15.2004 |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0188 |          34.5080 |          15.2105 |
[32m[20221213 22:54:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:54:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 311.34
[32m[20221213 22:54:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 345.63
[32m[20221213 22:54:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.26
[32m[20221213 22:54:36 @agent_ppo2.py:143][0m Total time:      36.38 min
[32m[20221213 22:54:36 @agent_ppo2.py:145][0m 3559424 total steps have happened
[32m[20221213 22:54:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1738 --------------------------#
[32m[20221213 22:54:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0046 |          39.5907 |          15.2929 |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0072 |          37.5613 |          15.2766 |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0135 |          36.5760 |          15.2873 |
[32m[20221213 22:54:36 @agent_ppo2.py:185][0m |          -0.0123 |          35.9225 |          15.2860 |
[32m[20221213 22:54:37 @agent_ppo2.py:185][0m |          -0.0129 |          35.4834 |          15.2810 |
[32m[20221213 22:54:37 @agent_ppo2.py:185][0m |          -0.0113 |          35.0167 |          15.2754 |
[32m[20221213 22:54:37 @agent_ppo2.py:185][0m |          -0.0158 |          34.5244 |          15.2798 |
[32m[20221213 22:54:37 @agent_ppo2.py:185][0m |          -0.0149 |          34.3511 |          15.2941 |
[32m[20221213 22:54:37 @agent_ppo2.py:185][0m |          -0.0153 |          34.0387 |          15.2895 |
[32m[20221213 22:54:37 @agent_ppo2.py:185][0m |          -0.0206 |          33.8795 |          15.2812 |
[32m[20221213 22:54:37 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.60
[32m[20221213 22:54:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.70
[32m[20221213 22:54:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.43
[32m[20221213 22:54:37 @agent_ppo2.py:143][0m Total time:      36.40 min
[32m[20221213 22:54:37 @agent_ppo2.py:145][0m 3561472 total steps have happened
[32m[20221213 22:54:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1739 --------------------------#
[32m[20221213 22:54:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:37 @agent_ppo2.py:185][0m |          -0.0005 |          41.6891 |          15.3682 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0034 |          38.5855 |          15.3724 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0026 |          37.7976 |          15.3699 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0059 |          36.7542 |          15.3785 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0047 |          36.3773 |          15.3832 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0087 |          36.0332 |          15.3984 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0091 |          35.8543 |          15.3952 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0104 |          35.6346 |          15.3850 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0111 |          35.3814 |          15.3911 |
[32m[20221213 22:54:38 @agent_ppo2.py:185][0m |          -0.0087 |          35.0720 |          15.4055 |
[32m[20221213 22:54:38 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.07
[32m[20221213 22:54:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.06
[32m[20221213 22:54:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 322.93
[32m[20221213 22:54:38 @agent_ppo2.py:143][0m Total time:      36.42 min
[32m[20221213 22:54:38 @agent_ppo2.py:145][0m 3563520 total steps have happened
[32m[20221213 22:54:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1740 --------------------------#
[32m[20221213 22:54:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:54:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |           0.0105 |          49.6603 |          15.3813 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0026 |          41.6867 |          15.3792 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |           0.0048 |          46.7282 |          15.3749 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0078 |          39.9628 |          15.3784 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0047 |          39.3991 |          15.3548 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0001 |          40.8078 |          15.3805 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0067 |          38.6462 |          15.3633 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0028 |          41.7460 |          15.3662 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0155 |          38.3835 |          15.3721 |
[32m[20221213 22:54:39 @agent_ppo2.py:185][0m |          -0.0134 |          38.0199 |          15.3674 |
[32m[20221213 22:54:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.53
[32m[20221213 22:54:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.38
[32m[20221213 22:54:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.31
[32m[20221213 22:54:40 @agent_ppo2.py:143][0m Total time:      36.44 min
[32m[20221213 22:54:40 @agent_ppo2.py:145][0m 3565568 total steps have happened
[32m[20221213 22:54:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1741 --------------------------#
[32m[20221213 22:54:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0017 |          53.8974 |          15.3633 |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0073 |          51.3650 |          15.3639 |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0031 |          51.3062 |          15.3553 |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0077 |          50.3344 |          15.3623 |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0015 |          51.0502 |          15.3640 |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0081 |          50.0517 |          15.3766 |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0024 |          52.2102 |          15.3697 |
[32m[20221213 22:54:40 @agent_ppo2.py:185][0m |          -0.0083 |          49.8246 |          15.3735 |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |          -0.0148 |          49.2472 |          15.3542 |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |          -0.0106 |          49.1818 |          15.3645 |
[32m[20221213 22:54:41 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.10
[32m[20221213 22:54:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 382.40
[32m[20221213 22:54:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.05
[32m[20221213 22:54:41 @agent_ppo2.py:143][0m Total time:      36.46 min
[32m[20221213 22:54:41 @agent_ppo2.py:145][0m 3567616 total steps have happened
[32m[20221213 22:54:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1742 --------------------------#
[32m[20221213 22:54:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |           0.0018 |          50.1477 |          15.3656 |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |          -0.0032 |          48.9489 |          15.3656 |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |          -0.0047 |          48.7175 |          15.3804 |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |          -0.0052 |          48.4307 |          15.3638 |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |          -0.0072 |          48.3750 |          15.3702 |
[32m[20221213 22:54:41 @agent_ppo2.py:185][0m |          -0.0074 |          48.3135 |          15.3747 |
[32m[20221213 22:54:42 @agent_ppo2.py:185][0m |          -0.0012 |          48.8807 |          15.3613 |
[32m[20221213 22:54:42 @agent_ppo2.py:185][0m |          -0.0072 |          48.2473 |          15.3794 |
[32m[20221213 22:54:42 @agent_ppo2.py:185][0m |          -0.0084 |          48.1244 |          15.3537 |
[32m[20221213 22:54:42 @agent_ppo2.py:185][0m |          -0.0083 |          47.9480 |          15.3723 |
[32m[20221213 22:54:42 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.92
[32m[20221213 22:54:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.86
[32m[20221213 22:54:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.27
[32m[20221213 22:54:42 @agent_ppo2.py:143][0m Total time:      36.48 min
[32m[20221213 22:54:42 @agent_ppo2.py:145][0m 3569664 total steps have happened
[32m[20221213 22:54:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1743 --------------------------#
[32m[20221213 22:54:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:42 @agent_ppo2.py:185][0m |          -0.0016 |          49.4764 |          15.4622 |
[32m[20221213 22:54:42 @agent_ppo2.py:185][0m |          -0.0064 |          47.4378 |          15.4562 |
[32m[20221213 22:54:42 @agent_ppo2.py:185][0m |          -0.0050 |          46.3796 |          15.4389 |
[32m[20221213 22:54:43 @agent_ppo2.py:185][0m |          -0.0111 |          45.6164 |          15.4286 |
[32m[20221213 22:54:43 @agent_ppo2.py:185][0m |          -0.0084 |          44.8609 |          15.4402 |
[32m[20221213 22:54:43 @agent_ppo2.py:185][0m |          -0.0111 |          44.6419 |          15.4421 |
[32m[20221213 22:54:43 @agent_ppo2.py:185][0m |          -0.0137 |          44.0147 |          15.4350 |
[32m[20221213 22:54:43 @agent_ppo2.py:185][0m |          -0.0113 |          43.7671 |          15.4682 |
[32m[20221213 22:54:43 @agent_ppo2.py:185][0m |          -0.0128 |          43.5256 |          15.4485 |
[32m[20221213 22:54:43 @agent_ppo2.py:185][0m |           0.0009 |          47.4657 |          15.4380 |
[32m[20221213 22:54:43 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.95
[32m[20221213 22:54:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.67
[32m[20221213 22:54:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.67
[32m[20221213 22:54:43 @agent_ppo2.py:143][0m Total time:      36.51 min
[32m[20221213 22:54:43 @agent_ppo2.py:145][0m 3571712 total steps have happened
[32m[20221213 22:54:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1744 --------------------------#
[32m[20221213 22:54:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0027 |          55.2003 |          15.5297 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0081 |          52.2976 |          15.5469 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0063 |          51.1999 |          15.5554 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0070 |          50.8172 |          15.5543 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0151 |          50.3269 |          15.5498 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0115 |          49.9589 |          15.5546 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0091 |          49.9595 |          15.5582 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0125 |          49.6523 |          15.5852 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0126 |          49.2625 |          15.5737 |
[32m[20221213 22:54:44 @agent_ppo2.py:185][0m |          -0.0154 |          49.2904 |          15.5672 |
[32m[20221213 22:54:44 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:54:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.14
[32m[20221213 22:54:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.87
[32m[20221213 22:54:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.83
[32m[20221213 22:54:44 @agent_ppo2.py:143][0m Total time:      36.53 min
[32m[20221213 22:54:44 @agent_ppo2.py:145][0m 3573760 total steps have happened
[32m[20221213 22:54:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1745 --------------------------#
[32m[20221213 22:54:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0002 |          48.8078 |          15.3443 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |           0.0007 |          46.1217 |          15.3129 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0051 |          45.2245 |          15.3352 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0060 |          44.3589 |          15.3406 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0073 |          43.9646 |          15.3238 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0040 |          43.5508 |          15.3380 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0129 |          43.4280 |          15.3556 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0074 |          42.8944 |          15.3602 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |          -0.0119 |          42.7065 |          15.3626 |
[32m[20221213 22:54:45 @agent_ppo2.py:185][0m |           0.0035 |          44.9807 |          15.3735 |
[32m[20221213 22:54:45 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.91
[32m[20221213 22:54:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 327.18
[32m[20221213 22:54:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.36
[32m[20221213 22:54:46 @agent_ppo2.py:143][0m Total time:      36.55 min
[32m[20221213 22:54:46 @agent_ppo2.py:145][0m 3575808 total steps have happened
[32m[20221213 22:54:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1746 --------------------------#
[32m[20221213 22:54:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:46 @agent_ppo2.py:185][0m |           0.0013 |          42.5471 |          15.4254 |
[32m[20221213 22:54:46 @agent_ppo2.py:185][0m |           0.0015 |          42.4010 |          15.4082 |
[32m[20221213 22:54:46 @agent_ppo2.py:185][0m |          -0.0069 |          37.4225 |          15.4101 |
[32m[20221213 22:54:46 @agent_ppo2.py:185][0m |          -0.0147 |          36.5802 |          15.4207 |
[32m[20221213 22:54:46 @agent_ppo2.py:185][0m |          -0.0122 |          36.1615 |          15.4176 |
[32m[20221213 22:54:46 @agent_ppo2.py:185][0m |          -0.0118 |          35.8032 |          15.4156 |
[32m[20221213 22:54:46 @agent_ppo2.py:185][0m |          -0.0136 |          35.5589 |          15.4254 |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |          -0.0079 |          36.4440 |          15.4157 |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |          -0.0140 |          35.3060 |          15.4127 |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |          -0.0132 |          35.0277 |          15.4234 |
[32m[20221213 22:54:47 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.84
[32m[20221213 22:54:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.62
[32m[20221213 22:54:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.91
[32m[20221213 22:54:47 @agent_ppo2.py:143][0m Total time:      36.57 min
[32m[20221213 22:54:47 @agent_ppo2.py:145][0m 3577856 total steps have happened
[32m[20221213 22:54:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1747 --------------------------#
[32m[20221213 22:54:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |           0.0069 |          44.3169 |          15.3348 |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |          -0.0033 |          40.2764 |          15.3633 |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |          -0.0044 |          39.2425 |          15.3637 |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |          -0.0063 |          38.3878 |          15.3702 |
[32m[20221213 22:54:47 @agent_ppo2.py:185][0m |          -0.0063 |          37.7624 |          15.3732 |
[32m[20221213 22:54:48 @agent_ppo2.py:185][0m |           0.0017 |          38.3387 |          15.3663 |
[32m[20221213 22:54:48 @agent_ppo2.py:185][0m |          -0.0083 |          37.0939 |          15.3828 |
[32m[20221213 22:54:48 @agent_ppo2.py:185][0m |          -0.0099 |          36.9962 |          15.3816 |
[32m[20221213 22:54:48 @agent_ppo2.py:185][0m |          -0.0117 |          36.9892 |          15.4003 |
[32m[20221213 22:54:48 @agent_ppo2.py:185][0m |          -0.0082 |          36.5315 |          15.3955 |
[32m[20221213 22:54:48 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.49
[32m[20221213 22:54:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.96
[32m[20221213 22:54:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.72
[32m[20221213 22:54:48 @agent_ppo2.py:143][0m Total time:      36.59 min
[32m[20221213 22:54:48 @agent_ppo2.py:145][0m 3579904 total steps have happened
[32m[20221213 22:54:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1748 --------------------------#
[32m[20221213 22:54:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:48 @agent_ppo2.py:185][0m |          -0.0022 |          43.2681 |          15.4874 |
[32m[20221213 22:54:48 @agent_ppo2.py:185][0m |          -0.0054 |          40.3768 |          15.4866 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0104 |          39.8682 |          15.4903 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0110 |          39.6626 |          15.4877 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0055 |          41.3893 |          15.4914 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0058 |          39.7002 |          15.4705 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0122 |          39.1315 |          15.4837 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0068 |          40.0841 |          15.4805 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0137 |          39.0002 |          15.4850 |
[32m[20221213 22:54:49 @agent_ppo2.py:185][0m |          -0.0129 |          38.9046 |          15.4960 |
[32m[20221213 22:54:49 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.90
[32m[20221213 22:54:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.88
[32m[20221213 22:54:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.57
[32m[20221213 22:54:49 @agent_ppo2.py:143][0m Total time:      36.61 min
[32m[20221213 22:54:49 @agent_ppo2.py:145][0m 3581952 total steps have happened
[32m[20221213 22:54:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1749 --------------------------#
[32m[20221213 22:54:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |           0.0002 |          51.7415 |          15.5539 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0012 |          49.5027 |          15.5416 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0066 |          48.4313 |          15.5503 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0086 |          47.7486 |          15.5060 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0003 |          51.1912 |          15.5366 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0085 |          47.0950 |          15.5067 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0130 |          46.9665 |          15.4937 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0101 |          46.5593 |          15.5034 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0069 |          48.0199 |          15.5052 |
[32m[20221213 22:54:50 @agent_ppo2.py:185][0m |          -0.0093 |          46.3085 |          15.4867 |
[32m[20221213 22:54:50 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.85
[32m[20221213 22:54:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.29
[32m[20221213 22:54:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.50
[32m[20221213 22:54:50 @agent_ppo2.py:143][0m Total time:      36.63 min
[32m[20221213 22:54:50 @agent_ppo2.py:145][0m 3584000 total steps have happened
[32m[20221213 22:54:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1750 --------------------------#
[32m[20221213 22:54:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:54:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |          -0.0033 |          47.6810 |          15.3880 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |          -0.0004 |          48.3291 |          15.3591 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |          -0.0058 |          45.8571 |          15.3642 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |           0.0057 |          50.0004 |          15.3621 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |          -0.0071 |          45.7734 |          15.3382 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |          -0.0091 |          45.1449 |          15.3492 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |           0.0003 |          50.1187 |          15.3560 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |          -0.0131 |          45.1051 |          15.3506 |
[32m[20221213 22:54:51 @agent_ppo2.py:185][0m |          -0.0060 |          44.8864 |          15.3711 |
[32m[20221213 22:54:52 @agent_ppo2.py:185][0m |          -0.0100 |          45.0600 |          15.3506 |
[32m[20221213 22:54:52 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:54:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.56
[32m[20221213 22:54:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.42
[32m[20221213 22:54:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.70
[32m[20221213 22:54:52 @agent_ppo2.py:143][0m Total time:      36.65 min
[32m[20221213 22:54:52 @agent_ppo2.py:145][0m 3586048 total steps have happened
[32m[20221213 22:54:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1751 --------------------------#
[32m[20221213 22:54:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:52 @agent_ppo2.py:185][0m |          -0.0004 |          44.3918 |          15.3718 |
[32m[20221213 22:54:52 @agent_ppo2.py:185][0m |          -0.0032 |          41.9932 |          15.3457 |
[32m[20221213 22:54:52 @agent_ppo2.py:185][0m |          -0.0065 |          41.0961 |          15.3720 |
[32m[20221213 22:54:52 @agent_ppo2.py:185][0m |          -0.0043 |          40.2286 |          15.3761 |
[32m[20221213 22:54:52 @agent_ppo2.py:185][0m |          -0.0091 |          39.4796 |          15.3845 |
[32m[20221213 22:54:52 @agent_ppo2.py:185][0m |          -0.0064 |          38.9762 |          15.3832 |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |          -0.0090 |          38.5197 |          15.4070 |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |          -0.0095 |          38.4978 |          15.4047 |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |          -0.0094 |          37.9609 |          15.3968 |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |          -0.0091 |          37.7852 |          15.4012 |
[32m[20221213 22:54:53 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.39
[32m[20221213 22:54:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.69
[32m[20221213 22:54:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 74.27
[32m[20221213 22:54:53 @agent_ppo2.py:143][0m Total time:      36.67 min
[32m[20221213 22:54:53 @agent_ppo2.py:145][0m 3588096 total steps have happened
[32m[20221213 22:54:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1752 --------------------------#
[32m[20221213 22:54:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |           0.0001 |          53.1031 |          15.4236 |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |          -0.0036 |          50.1322 |          15.3967 |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |          -0.0072 |          49.4303 |          15.4044 |
[32m[20221213 22:54:53 @agent_ppo2.py:185][0m |          -0.0078 |          49.1122 |          15.4028 |
[32m[20221213 22:54:54 @agent_ppo2.py:185][0m |          -0.0060 |          49.1979 |          15.4140 |
[32m[20221213 22:54:54 @agent_ppo2.py:185][0m |          -0.0030 |          50.9952 |          15.4147 |
[32m[20221213 22:54:54 @agent_ppo2.py:185][0m |          -0.0062 |          48.3927 |          15.3855 |
[32m[20221213 22:54:54 @agent_ppo2.py:185][0m |          -0.0031 |          50.2518 |          15.3898 |
[32m[20221213 22:54:54 @agent_ppo2.py:185][0m |          -0.0093 |          47.8102 |          15.3692 |
[32m[20221213 22:54:54 @agent_ppo2.py:185][0m |          -0.0112 |          47.5924 |          15.3901 |
[32m[20221213 22:54:54 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.49
[32m[20221213 22:54:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.28
[32m[20221213 22:54:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.52
[32m[20221213 22:54:54 @agent_ppo2.py:143][0m Total time:      36.69 min
[32m[20221213 22:54:54 @agent_ppo2.py:145][0m 3590144 total steps have happened
[32m[20221213 22:54:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1753 --------------------------#
[32m[20221213 22:54:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:54 @agent_ppo2.py:185][0m |           0.0178 |          58.4978 |          15.7172 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0055 |          51.2649 |          15.7051 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0063 |          49.1894 |          15.7185 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0067 |          47.8116 |          15.7097 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0080 |          47.2735 |          15.7219 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0106 |          46.7128 |          15.7087 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0114 |          46.7779 |          15.6992 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0132 |          46.1660 |          15.7153 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0142 |          45.8945 |          15.6789 |
[32m[20221213 22:54:55 @agent_ppo2.py:185][0m |          -0.0130 |          45.6797 |          15.7034 |
[32m[20221213 22:54:55 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.04
[32m[20221213 22:54:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.18
[32m[20221213 22:54:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.93
[32m[20221213 22:54:55 @agent_ppo2.py:143][0m Total time:      36.71 min
[32m[20221213 22:54:55 @agent_ppo2.py:145][0m 3592192 total steps have happened
[32m[20221213 22:54:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1754 --------------------------#
[32m[20221213 22:54:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0012 |          44.8820 |          15.3827 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0104 |          41.0550 |          15.3639 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0086 |          39.8454 |          15.3633 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0075 |          39.2076 |          15.3662 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0097 |          38.7306 |          15.3793 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0094 |          38.3854 |          15.3555 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0118 |          38.3056 |          15.3497 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0119 |          38.0076 |          15.3548 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0138 |          37.7376 |          15.3505 |
[32m[20221213 22:54:56 @agent_ppo2.py:185][0m |          -0.0147 |          37.7162 |          15.3251 |
[32m[20221213 22:54:56 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.69
[32m[20221213 22:54:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 350.42
[32m[20221213 22:54:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.04
[32m[20221213 22:54:57 @agent_ppo2.py:143][0m Total time:      36.73 min
[32m[20221213 22:54:57 @agent_ppo2.py:145][0m 3594240 total steps have happened
[32m[20221213 22:54:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1755 --------------------------#
[32m[20221213 22:54:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |           0.0081 |          44.1603 |          15.3913 |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |           0.0033 |          40.9380 |          15.3505 |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |          -0.0079 |          39.6770 |          15.3457 |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |          -0.0061 |          38.6178 |          15.3417 |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |          -0.0051 |          37.7522 |          15.3352 |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |          -0.0070 |          37.1850 |          15.3091 |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |           0.0024 |          38.2036 |          15.3340 |
[32m[20221213 22:54:57 @agent_ppo2.py:185][0m |          -0.0043 |          36.7771 |          15.2986 |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |          -0.0061 |          37.3825 |          15.3111 |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |          -0.0056 |          36.8933 |          15.3069 |
[32m[20221213 22:54:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:54:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.89
[32m[20221213 22:54:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.22
[32m[20221213 22:54:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.40
[32m[20221213 22:54:58 @agent_ppo2.py:143][0m Total time:      36.75 min
[32m[20221213 22:54:58 @agent_ppo2.py:145][0m 3596288 total steps have happened
[32m[20221213 22:54:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1756 --------------------------#
[32m[20221213 22:54:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:54:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |           0.0017 |          39.9475 |          15.4477 |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |           0.0046 |          38.6056 |          15.4528 |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |          -0.0073 |          35.9384 |          15.4483 |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |          -0.0103 |          35.4504 |          15.4364 |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |          -0.0072 |          35.0024 |          15.4084 |
[32m[20221213 22:54:58 @agent_ppo2.py:185][0m |          -0.0095 |          34.8055 |          15.4467 |
[32m[20221213 22:54:59 @agent_ppo2.py:185][0m |          -0.0123 |          34.4625 |          15.4344 |
[32m[20221213 22:54:59 @agent_ppo2.py:185][0m |          -0.0098 |          34.2311 |          15.4152 |
[32m[20221213 22:54:59 @agent_ppo2.py:185][0m |          -0.0104 |          33.9636 |          15.4211 |
[32m[20221213 22:54:59 @agent_ppo2.py:185][0m |          -0.0124 |          33.8625 |          15.4166 |
[32m[20221213 22:54:59 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:54:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.66
[32m[20221213 22:54:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.81
[32m[20221213 22:54:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.85
[32m[20221213 22:54:59 @agent_ppo2.py:143][0m Total time:      36.77 min
[32m[20221213 22:54:59 @agent_ppo2.py:145][0m 3598336 total steps have happened
[32m[20221213 22:54:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1757 --------------------------#
[32m[20221213 22:54:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:54:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:54:59 @agent_ppo2.py:185][0m |          -0.0024 |           9.4798 |          15.6407 |
[32m[20221213 22:54:59 @agent_ppo2.py:185][0m |          -0.0062 |           7.4516 |          15.6765 |
[32m[20221213 22:54:59 @agent_ppo2.py:185][0m |          -0.0028 |           7.8157 |          15.6487 |
[32m[20221213 22:55:00 @agent_ppo2.py:185][0m |          -0.0043 |           7.3411 |          15.6641 |
[32m[20221213 22:55:00 @agent_ppo2.py:185][0m |          -0.0058 |           7.2734 |          15.6617 |
[32m[20221213 22:55:00 @agent_ppo2.py:185][0m |          -0.0085 |           7.1418 |          15.6632 |
[32m[20221213 22:55:00 @agent_ppo2.py:185][0m |          -0.0107 |           7.1364 |          15.6666 |
[32m[20221213 22:55:00 @agent_ppo2.py:185][0m |          -0.0018 |           7.3784 |          15.6715 |
[32m[20221213 22:55:00 @agent_ppo2.py:185][0m |          -0.0102 |           7.1061 |          15.6800 |
[32m[20221213 22:55:00 @agent_ppo2.py:185][0m |          -0.0118 |           7.0760 |          15.6579 |
[32m[20221213 22:55:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:55:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:55:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.78
[32m[20221213 22:55:00 @agent_ppo2.py:143][0m Total time:      36.79 min
[32m[20221213 22:55:00 @agent_ppo2.py:145][0m 3600384 total steps have happened
[32m[20221213 22:55:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1758 --------------------------#
[32m[20221213 22:55:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |           0.0038 |          48.1063 |          15.2729 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |           0.0023 |          44.9010 |          15.2939 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |          -0.0033 |          42.0471 |          15.2545 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |          -0.0086 |          41.3001 |          15.2637 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |          -0.0066 |          40.7930 |          15.2681 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |           0.0036 |          43.4878 |          15.2670 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |          -0.0133 |          40.5387 |          15.2390 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |          -0.0106 |          40.1966 |          15.2852 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |          -0.0114 |          39.8702 |          15.2640 |
[32m[20221213 22:55:01 @agent_ppo2.py:185][0m |          -0.0066 |          41.8879 |          15.2603 |
[32m[20221213 22:55:01 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.70
[32m[20221213 22:55:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.95
[32m[20221213 22:55:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.49
[32m[20221213 22:55:01 @agent_ppo2.py:143][0m Total time:      36.81 min
[32m[20221213 22:55:01 @agent_ppo2.py:145][0m 3602432 total steps have happened
[32m[20221213 22:55:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1759 --------------------------#
[32m[20221213 22:55:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |           0.0088 |          42.4050 |          15.4726 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0053 |          38.9588 |          15.4549 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0059 |          37.7619 |          15.4853 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0018 |          37.2922 |          15.4898 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0095 |          36.7858 |          15.4952 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0084 |          36.3654 |          15.5014 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0083 |          35.9032 |          15.5098 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0106 |          35.6115 |          15.5190 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0113 |          35.2416 |          15.5323 |
[32m[20221213 22:55:02 @agent_ppo2.py:185][0m |          -0.0140 |          35.1930 |          15.5500 |
[32m[20221213 22:55:02 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.85
[32m[20221213 22:55:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.30
[32m[20221213 22:55:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.49
[32m[20221213 22:55:03 @agent_ppo2.py:143][0m Total time:      36.83 min
[32m[20221213 22:55:03 @agent_ppo2.py:145][0m 3604480 total steps have happened
[32m[20221213 22:55:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1760 --------------------------#
[32m[20221213 22:55:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:55:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:03 @agent_ppo2.py:185][0m |           0.0023 |          43.7305 |          15.5036 |
[32m[20221213 22:55:03 @agent_ppo2.py:185][0m |          -0.0042 |          43.1463 |          15.5113 |
[32m[20221213 22:55:03 @agent_ppo2.py:185][0m |          -0.0072 |          42.5872 |          15.4912 |
[32m[20221213 22:55:03 @agent_ppo2.py:185][0m |          -0.0073 |          42.3425 |          15.4891 |
[32m[20221213 22:55:03 @agent_ppo2.py:185][0m |           0.0026 |          48.2838 |          15.5046 |
[32m[20221213 22:55:03 @agent_ppo2.py:185][0m |          -0.0097 |          42.1196 |          15.5073 |
[32m[20221213 22:55:03 @agent_ppo2.py:185][0m |          -0.0074 |          41.7885 |          15.5073 |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |          -0.0032 |          41.7505 |          15.5221 |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |          -0.0104 |          41.4533 |          15.5187 |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |          -0.0108 |          41.4751 |          15.5231 |
[32m[20221213 22:55:04 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.28
[32m[20221213 22:55:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.74
[32m[20221213 22:55:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.97
[32m[20221213 22:55:04 @agent_ppo2.py:143][0m Total time:      36.85 min
[32m[20221213 22:55:04 @agent_ppo2.py:145][0m 3606528 total steps have happened
[32m[20221213 22:55:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1761 --------------------------#
[32m[20221213 22:55:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |           0.0020 |          37.0324 |          15.3962 |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |          -0.0014 |          34.3673 |          15.4064 |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |          -0.0035 |          34.4656 |          15.4087 |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |           0.0017 |          35.3912 |          15.3945 |
[32m[20221213 22:55:04 @agent_ppo2.py:185][0m |          -0.0068 |          33.7330 |          15.3963 |
[32m[20221213 22:55:05 @agent_ppo2.py:185][0m |          -0.0062 |          33.4787 |          15.4054 |
[32m[20221213 22:55:05 @agent_ppo2.py:185][0m |          -0.0015 |          36.1104 |          15.4008 |
[32m[20221213 22:55:05 @agent_ppo2.py:185][0m |          -0.0091 |          33.4286 |          15.3917 |
[32m[20221213 22:55:05 @agent_ppo2.py:185][0m |           0.0043 |          36.9595 |          15.4007 |
[32m[20221213 22:55:05 @agent_ppo2.py:185][0m |          -0.0075 |          33.3828 |          15.4112 |
[32m[20221213 22:55:05 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.84
[32m[20221213 22:55:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.79
[32m[20221213 22:55:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.19
[32m[20221213 22:55:05 @agent_ppo2.py:143][0m Total time:      36.87 min
[32m[20221213 22:55:05 @agent_ppo2.py:145][0m 3608576 total steps have happened
[32m[20221213 22:55:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1762 --------------------------#
[32m[20221213 22:55:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:05 @agent_ppo2.py:185][0m |          -0.0037 |          50.8791 |          15.4272 |
[32m[20221213 22:55:05 @agent_ppo2.py:185][0m |          -0.0069 |          48.4648 |          15.4056 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |          -0.0058 |          47.6263 |          15.3957 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |           0.0022 |          48.5518 |          15.3955 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |          -0.0087 |          46.0147 |          15.3968 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |          -0.0051 |          45.6198 |          15.4060 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |          -0.0032 |          45.2771 |          15.4008 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |          -0.0095 |          44.3960 |          15.4154 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |          -0.0080 |          43.7978 |          15.4085 |
[32m[20221213 22:55:06 @agent_ppo2.py:185][0m |          -0.0089 |          44.0535 |          15.4138 |
[32m[20221213 22:55:06 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.67
[32m[20221213 22:55:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.10
[32m[20221213 22:55:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.98
[32m[20221213 22:55:06 @agent_ppo2.py:143][0m Total time:      36.89 min
[32m[20221213 22:55:06 @agent_ppo2.py:145][0m 3610624 total steps have happened
[32m[20221213 22:55:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1763 --------------------------#
[32m[20221213 22:55:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0038 |          41.3820 |          15.6294 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0026 |          37.5452 |          15.5923 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0079 |          38.1643 |          15.6014 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0062 |          36.9022 |          15.5737 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0112 |          36.6455 |          15.5789 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0080 |          36.4423 |          15.5784 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0059 |          36.0776 |          15.5810 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0155 |          35.9847 |          15.5719 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0039 |          36.4555 |          15.5802 |
[32m[20221213 22:55:07 @agent_ppo2.py:185][0m |          -0.0079 |          35.7515 |          15.5758 |
[32m[20221213 22:55:07 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.02
[32m[20221213 22:55:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.84
[32m[20221213 22:55:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.80
[32m[20221213 22:55:07 @agent_ppo2.py:143][0m Total time:      36.91 min
[32m[20221213 22:55:07 @agent_ppo2.py:145][0m 3612672 total steps have happened
[32m[20221213 22:55:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1764 --------------------------#
[32m[20221213 22:55:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |          -0.0024 |          50.1727 |          15.4691 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |          -0.0053 |          48.5023 |          15.4581 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |          -0.0081 |          48.2542 |          15.4615 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |           0.0004 |          48.6135 |          15.4270 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |          -0.0071 |          47.1938 |          15.4264 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |          -0.0096 |          46.7915 |          15.4220 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |          -0.0011 |          51.2237 |          15.4269 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |           0.0016 |          48.2653 |          15.4100 |
[32m[20221213 22:55:08 @agent_ppo2.py:185][0m |          -0.0101 |          46.3409 |          15.3995 |
[32m[20221213 22:55:09 @agent_ppo2.py:185][0m |          -0.0087 |          46.1031 |          15.4091 |
[32m[20221213 22:55:09 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.78
[32m[20221213 22:55:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.44
[32m[20221213 22:55:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.98
[32m[20221213 22:55:09 @agent_ppo2.py:143][0m Total time:      36.93 min
[32m[20221213 22:55:09 @agent_ppo2.py:145][0m 3614720 total steps have happened
[32m[20221213 22:55:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1765 --------------------------#
[32m[20221213 22:55:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:09 @agent_ppo2.py:185][0m |          -0.0024 |          48.6773 |          15.4576 |
[32m[20221213 22:55:09 @agent_ppo2.py:185][0m |          -0.0059 |          46.1948 |          15.4637 |
[32m[20221213 22:55:09 @agent_ppo2.py:185][0m |          -0.0064 |          44.7974 |          15.4707 |
[32m[20221213 22:55:09 @agent_ppo2.py:185][0m |          -0.0042 |          44.6285 |          15.4699 |
[32m[20221213 22:55:09 @agent_ppo2.py:185][0m |          -0.0062 |          44.5195 |          15.4681 |
[32m[20221213 22:55:09 @agent_ppo2.py:185][0m |          -0.0135 |          43.7095 |          15.4732 |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0129 |          43.3704 |          15.4949 |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0117 |          43.1970 |          15.4748 |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0046 |          43.3248 |          15.4945 |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0113 |          42.9339 |          15.4971 |
[32m[20221213 22:55:10 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.82
[32m[20221213 22:55:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.52
[32m[20221213 22:55:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.73
[32m[20221213 22:55:10 @agent_ppo2.py:143][0m Total time:      36.95 min
[32m[20221213 22:55:10 @agent_ppo2.py:145][0m 3616768 total steps have happened
[32m[20221213 22:55:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1766 --------------------------#
[32m[20221213 22:55:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0012 |          35.7987 |          15.6659 |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0007 |          33.9855 |          15.6493 |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0050 |          33.6270 |          15.6553 |
[32m[20221213 22:55:10 @agent_ppo2.py:185][0m |          -0.0045 |          33.4096 |          15.6227 |
[32m[20221213 22:55:11 @agent_ppo2.py:185][0m |          -0.0069 |          33.2911 |          15.6492 |
[32m[20221213 22:55:11 @agent_ppo2.py:185][0m |          -0.0066 |          33.0029 |          15.6532 |
[32m[20221213 22:55:11 @agent_ppo2.py:185][0m |          -0.0048 |          33.0878 |          15.6560 |
[32m[20221213 22:55:11 @agent_ppo2.py:185][0m |           0.0066 |          36.8677 |          15.6684 |
[32m[20221213 22:55:11 @agent_ppo2.py:185][0m |          -0.0082 |          32.7627 |          15.6507 |
[32m[20221213 22:55:11 @agent_ppo2.py:185][0m |          -0.0102 |          32.5281 |          15.6564 |
[32m[20221213 22:55:11 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.88
[32m[20221213 22:55:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.84
[32m[20221213 22:55:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.53
[32m[20221213 22:55:11 @agent_ppo2.py:143][0m Total time:      36.97 min
[32m[20221213 22:55:11 @agent_ppo2.py:145][0m 3618816 total steps have happened
[32m[20221213 22:55:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1767 --------------------------#
[32m[20221213 22:55:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:11 @agent_ppo2.py:185][0m |           0.0006 |          47.9420 |          15.6300 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |          -0.0072 |          46.5701 |          15.5978 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |           0.0017 |          48.6556 |          15.6099 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |          -0.0062 |          45.6500 |          15.6026 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |          -0.0109 |          45.2584 |          15.6033 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |          -0.0111 |          44.9157 |          15.5846 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |           0.0010 |          50.0271 |          15.5886 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |          -0.0084 |          44.7505 |          15.6024 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |          -0.0097 |          44.2698 |          15.5699 |
[32m[20221213 22:55:12 @agent_ppo2.py:185][0m |          -0.0115 |          44.3044 |          15.6006 |
[32m[20221213 22:55:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:55:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.11
[32m[20221213 22:55:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.54
[32m[20221213 22:55:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.81
[32m[20221213 22:55:12 @agent_ppo2.py:143][0m Total time:      36.99 min
[32m[20221213 22:55:12 @agent_ppo2.py:145][0m 3620864 total steps have happened
[32m[20221213 22:55:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1768 --------------------------#
[32m[20221213 22:55:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |           0.0027 |          39.9538 |          15.7270 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0046 |          37.6771 |          15.7184 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0053 |          37.1695 |          15.7111 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0114 |          36.6168 |          15.7152 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0072 |          36.5475 |          15.7263 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0134 |          36.0636 |          15.7095 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0142 |          35.8670 |          15.7046 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0068 |          37.8963 |          15.7066 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0144 |          35.5574 |          15.7065 |
[32m[20221213 22:55:13 @agent_ppo2.py:185][0m |          -0.0120 |          35.4986 |          15.6966 |
[32m[20221213 22:55:13 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.84
[32m[20221213 22:55:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.21
[32m[20221213 22:55:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 354.51
[32m[20221213 22:55:14 @agent_ppo2.py:143][0m Total time:      37.01 min
[32m[20221213 22:55:14 @agent_ppo2.py:145][0m 3622912 total steps have happened
[32m[20221213 22:55:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1769 --------------------------#
[32m[20221213 22:55:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |           0.0047 |          44.9180 |          15.5613 |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |          -0.0023 |          43.8849 |          15.5378 |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |          -0.0001 |          43.6412 |          15.5449 |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |          -0.0027 |          43.7460 |          15.5566 |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |           0.0030 |          49.0335 |          15.5560 |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |          -0.0041 |          43.2820 |          15.5411 |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |          -0.0094 |          42.9939 |          15.5311 |
[32m[20221213 22:55:14 @agent_ppo2.py:185][0m |           0.0012 |          44.8179 |          15.5237 |
[32m[20221213 22:55:15 @agent_ppo2.py:185][0m |          -0.0076 |          42.8388 |          15.5334 |
[32m[20221213 22:55:15 @agent_ppo2.py:185][0m |          -0.0074 |          42.7185 |          15.5051 |
[32m[20221213 22:55:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.44
[32m[20221213 22:55:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.75
[32m[20221213 22:55:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.42
[32m[20221213 22:55:15 @agent_ppo2.py:143][0m Total time:      37.03 min
[32m[20221213 22:55:15 @agent_ppo2.py:145][0m 3624960 total steps have happened
[32m[20221213 22:55:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1770 --------------------------#
[32m[20221213 22:55:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:55:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:15 @agent_ppo2.py:185][0m |          -0.0017 |          42.1756 |          15.5230 |
[32m[20221213 22:55:15 @agent_ppo2.py:185][0m |          -0.0064 |          40.2287 |          15.5212 |
[32m[20221213 22:55:15 @agent_ppo2.py:185][0m |          -0.0068 |          39.4018 |          15.5037 |
[32m[20221213 22:55:15 @agent_ppo2.py:185][0m |          -0.0035 |          40.8125 |          15.5102 |
[32m[20221213 22:55:15 @agent_ppo2.py:185][0m |          -0.0074 |          38.8731 |          15.5293 |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |          -0.0133 |          38.5411 |          15.5149 |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |          -0.0157 |          38.5273 |          15.5071 |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |          -0.0095 |          38.3711 |          15.5130 |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |          -0.0142 |          38.0017 |          15.5255 |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |           0.0002 |          43.0124 |          15.5225 |
[32m[20221213 22:55:16 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.99
[32m[20221213 22:55:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 390.73
[32m[20221213 22:55:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.93
[32m[20221213 22:55:16 @agent_ppo2.py:143][0m Total time:      37.05 min
[32m[20221213 22:55:16 @agent_ppo2.py:145][0m 3627008 total steps have happened
[32m[20221213 22:55:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1771 --------------------------#
[32m[20221213 22:55:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |          -0.0014 |          43.7823 |          15.4414 |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |          -0.0072 |          38.5363 |          15.4654 |
[32m[20221213 22:55:16 @agent_ppo2.py:185][0m |          -0.0028 |          36.8678 |          15.4620 |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |          -0.0005 |          37.9047 |          15.4687 |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |          -0.0078 |          35.5760 |          15.4623 |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |          -0.0107 |          35.0284 |          15.4896 |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |          -0.0131 |          34.7462 |          15.4954 |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |          -0.0101 |          34.6111 |          15.5220 |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |          -0.0124 |          34.3945 |          15.5211 |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |          -0.0132 |          34.3919 |          15.5100 |
[32m[20221213 22:55:17 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.96
[32m[20221213 22:55:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.77
[32m[20221213 22:55:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.58
[32m[20221213 22:55:17 @agent_ppo2.py:143][0m Total time:      37.07 min
[32m[20221213 22:55:17 @agent_ppo2.py:145][0m 3629056 total steps have happened
[32m[20221213 22:55:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1772 --------------------------#
[32m[20221213 22:55:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:17 @agent_ppo2.py:185][0m |           0.0054 |          49.1151 |          15.4599 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0101 |          37.3033 |          15.4563 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0008 |          36.0096 |          15.4457 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |           0.0011 |          35.4652 |          15.4514 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0074 |          35.0223 |          15.4326 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0051 |          34.8136 |          15.4448 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0029 |          34.7713 |          15.4297 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0059 |          34.3313 |          15.4413 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0157 |          34.1686 |          15.4163 |
[32m[20221213 22:55:18 @agent_ppo2.py:185][0m |          -0.0124 |          34.0974 |          15.4161 |
[32m[20221213 22:55:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.46
[32m[20221213 22:55:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 377.97
[32m[20221213 22:55:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.10
[32m[20221213 22:55:18 @agent_ppo2.py:143][0m Total time:      37.09 min
[32m[20221213 22:55:18 @agent_ppo2.py:145][0m 3631104 total steps have happened
[32m[20221213 22:55:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1773 --------------------------#
[32m[20221213 22:55:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |           0.0027 |          50.3223 |          15.7972 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |           0.0000 |          48.7473 |          15.7814 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0031 |          48.0562 |          15.7781 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0073 |          47.8454 |          15.7954 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0036 |          48.1018 |          15.7853 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0094 |          47.4167 |          15.7980 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0102 |          47.3636 |          15.8017 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0100 |          47.1695 |          15.8022 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0085 |          47.0241 |          15.8073 |
[32m[20221213 22:55:19 @agent_ppo2.py:185][0m |          -0.0119 |          46.9844 |          15.7972 |
[32m[20221213 22:55:19 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.29
[32m[20221213 22:55:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.40
[32m[20221213 22:55:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.32
[32m[20221213 22:55:20 @agent_ppo2.py:143][0m Total time:      37.11 min
[32m[20221213 22:55:20 @agent_ppo2.py:145][0m 3633152 total steps have happened
[32m[20221213 22:55:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1774 --------------------------#
[32m[20221213 22:55:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:20 @agent_ppo2.py:185][0m |          -0.0018 |          40.4222 |          15.4725 |
[32m[20221213 22:55:20 @agent_ppo2.py:185][0m |          -0.0078 |          39.3498 |          15.4703 |
[32m[20221213 22:55:20 @agent_ppo2.py:185][0m |          -0.0069 |          39.2660 |          15.4495 |
[32m[20221213 22:55:20 @agent_ppo2.py:185][0m |          -0.0090 |          38.9169 |          15.4362 |
[32m[20221213 22:55:20 @agent_ppo2.py:185][0m |          -0.0064 |          38.9092 |          15.4259 |
[32m[20221213 22:55:20 @agent_ppo2.py:185][0m |          -0.0055 |          38.7326 |          15.4484 |
[32m[20221213 22:55:20 @agent_ppo2.py:185][0m |           0.0004 |          39.6316 |          15.4376 |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |          -0.0006 |          40.6295 |          15.4241 |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |          -0.0072 |          38.4215 |          15.4292 |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |          -0.0110 |          38.2324 |          15.4197 |
[32m[20221213 22:55:21 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.60
[32m[20221213 22:55:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.03
[32m[20221213 22:55:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.97
[32m[20221213 22:55:21 @agent_ppo2.py:143][0m Total time:      37.13 min
[32m[20221213 22:55:21 @agent_ppo2.py:145][0m 3635200 total steps have happened
[32m[20221213 22:55:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1775 --------------------------#
[32m[20221213 22:55:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |           0.0009 |          37.4049 |          15.5796 |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |          -0.0040 |          32.8404 |          15.5739 |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |          -0.0096 |          31.8424 |          15.5645 |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |          -0.0059 |          31.0171 |          15.5673 |
[32m[20221213 22:55:21 @agent_ppo2.py:185][0m |          -0.0101 |          30.7271 |          15.5547 |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |          -0.0099 |          30.3092 |          15.5566 |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |          -0.0138 |          30.1007 |          15.5547 |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |          -0.0049 |          31.5534 |          15.5404 |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |          -0.0146 |          29.6629 |          15.5232 |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |          -0.0136 |          29.4347 |          15.5231 |
[32m[20221213 22:55:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 327.24
[32m[20221213 22:55:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.79
[32m[20221213 22:55:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.02
[32m[20221213 22:55:22 @agent_ppo2.py:143][0m Total time:      37.15 min
[32m[20221213 22:55:22 @agent_ppo2.py:145][0m 3637248 total steps have happened
[32m[20221213 22:55:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1776 --------------------------#
[32m[20221213 22:55:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |           0.0172 |          55.9575 |          15.4980 |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |           0.0006 |          48.0738 |          15.4971 |
[32m[20221213 22:55:22 @agent_ppo2.py:185][0m |          -0.0054 |          46.8020 |          15.5128 |
[32m[20221213 22:55:23 @agent_ppo2.py:185][0m |          -0.0054 |          46.1440 |          15.5087 |
[32m[20221213 22:55:23 @agent_ppo2.py:185][0m |          -0.0072 |          45.7698 |          15.5009 |
[32m[20221213 22:55:23 @agent_ppo2.py:185][0m |          -0.0077 |          45.4868 |          15.5155 |
[32m[20221213 22:55:23 @agent_ppo2.py:185][0m |          -0.0100 |          45.4305 |          15.5156 |
[32m[20221213 22:55:23 @agent_ppo2.py:185][0m |          -0.0086 |          45.4161 |          15.5183 |
[32m[20221213 22:55:23 @agent_ppo2.py:185][0m |          -0.0115 |          45.0524 |          15.5379 |
[32m[20221213 22:55:23 @agent_ppo2.py:185][0m |          -0.0053 |          45.5569 |          15.5196 |
[32m[20221213 22:55:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.84
[32m[20221213 22:55:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.89
[32m[20221213 22:55:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.56
[32m[20221213 22:55:23 @agent_ppo2.py:143][0m Total time:      37.17 min
[32m[20221213 22:55:23 @agent_ppo2.py:145][0m 3639296 total steps have happened
[32m[20221213 22:55:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1777 --------------------------#
[32m[20221213 22:55:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0003 |          38.5921 |          15.5741 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0030 |          34.9424 |          15.5282 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0096 |          33.7903 |          15.5535 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |           0.0016 |          36.7616 |          15.5460 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0094 |          32.7404 |          15.5503 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0096 |          32.2754 |          15.5219 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0095 |          32.1202 |          15.5384 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0117 |          31.6333 |          15.5448 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0030 |          36.4152 |          15.5328 |
[32m[20221213 22:55:24 @agent_ppo2.py:185][0m |          -0.0150 |          31.0962 |          15.5260 |
[32m[20221213 22:55:24 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.67
[32m[20221213 22:55:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.08
[32m[20221213 22:55:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.96
[32m[20221213 22:55:24 @agent_ppo2.py:143][0m Total time:      37.19 min
[32m[20221213 22:55:24 @agent_ppo2.py:145][0m 3641344 total steps have happened
[32m[20221213 22:55:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1778 --------------------------#
[32m[20221213 22:55:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |           0.0124 |          40.0037 |          15.7529 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0030 |          34.5662 |          15.7422 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0079 |          33.4911 |          15.7422 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0011 |          33.9660 |          15.7278 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0082 |          32.5809 |          15.7344 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0115 |          32.1644 |          15.7449 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0116 |          31.8817 |          15.7412 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0128 |          31.8374 |          15.7511 |
[32m[20221213 22:55:25 @agent_ppo2.py:185][0m |          -0.0120 |          31.6431 |          15.7285 |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |          -0.0138 |          31.8396 |          15.7609 |
[32m[20221213 22:55:26 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.61
[32m[20221213 22:55:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.31
[32m[20221213 22:55:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.87
[32m[20221213 22:55:26 @agent_ppo2.py:143][0m Total time:      37.21 min
[32m[20221213 22:55:26 @agent_ppo2.py:145][0m 3643392 total steps have happened
[32m[20221213 22:55:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1779 --------------------------#
[32m[20221213 22:55:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |           0.0003 |          34.7188 |          15.6926 |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |          -0.0073 |          31.0946 |          15.6647 |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |          -0.0050 |          30.3892 |          15.6756 |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |          -0.0114 |          30.0583 |          15.6779 |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |          -0.0115 |          29.6674 |          15.6832 |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |          -0.0041 |          30.7446 |          15.6741 |
[32m[20221213 22:55:26 @agent_ppo2.py:185][0m |          -0.0101 |          29.3812 |          15.6911 |
[32m[20221213 22:55:27 @agent_ppo2.py:185][0m |          -0.0124 |          29.2209 |          15.7018 |
[32m[20221213 22:55:27 @agent_ppo2.py:185][0m |          -0.0177 |          29.0804 |          15.7096 |
[32m[20221213 22:55:27 @agent_ppo2.py:185][0m |          -0.0145 |          28.9150 |          15.6969 |
[32m[20221213 22:55:27 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.47
[32m[20221213 22:55:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.16
[32m[20221213 22:55:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.75
[32m[20221213 22:55:27 @agent_ppo2.py:143][0m Total time:      37.23 min
[32m[20221213 22:55:27 @agent_ppo2.py:145][0m 3645440 total steps have happened
[32m[20221213 22:55:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1780 --------------------------#
[32m[20221213 22:55:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:55:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:27 @agent_ppo2.py:185][0m |          -0.0030 |          45.0477 |          15.7070 |
[32m[20221213 22:55:27 @agent_ppo2.py:185][0m |          -0.0079 |          42.0279 |          15.7011 |
[32m[20221213 22:55:27 @agent_ppo2.py:185][0m |          -0.0068 |          41.2850 |          15.6940 |
[32m[20221213 22:55:27 @agent_ppo2.py:185][0m |          -0.0103 |          40.5939 |          15.6998 |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0118 |          40.5461 |          15.6768 |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0114 |          40.1351 |          15.6945 |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0136 |          40.2457 |          15.6829 |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0129 |          39.9549 |          15.6847 |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0093 |          39.6812 |          15.6879 |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0142 |          39.4350 |          15.6923 |
[32m[20221213 22:55:28 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.18
[32m[20221213 22:55:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.00
[32m[20221213 22:55:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.20
[32m[20221213 22:55:28 @agent_ppo2.py:143][0m Total time:      37.25 min
[32m[20221213 22:55:28 @agent_ppo2.py:145][0m 3647488 total steps have happened
[32m[20221213 22:55:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1781 --------------------------#
[32m[20221213 22:55:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0051 |          45.9923 |          15.3605 |
[32m[20221213 22:55:28 @agent_ppo2.py:185][0m |          -0.0082 |          44.9269 |          15.3714 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0078 |          44.8595 |          15.3741 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0081 |          44.5106 |          15.3645 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0089 |          44.4718 |          15.3932 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0102 |          44.1926 |          15.3832 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0104 |          44.0539 |          15.3854 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0134 |          44.1351 |          15.3959 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0101 |          43.9596 |          15.4129 |
[32m[20221213 22:55:29 @agent_ppo2.py:185][0m |          -0.0148 |          43.9097 |          15.4203 |
[32m[20221213 22:55:29 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.80
[32m[20221213 22:55:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.62
[32m[20221213 22:55:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.12
[32m[20221213 22:55:29 @agent_ppo2.py:143][0m Total time:      37.27 min
[32m[20221213 22:55:29 @agent_ppo2.py:145][0m 3649536 total steps have happened
[32m[20221213 22:55:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1782 --------------------------#
[32m[20221213 22:55:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |          -0.0008 |          43.8501 |          15.7440 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |           0.0010 |          41.4678 |          15.7020 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |          -0.0035 |          40.2990 |          15.6872 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |          -0.0064 |          39.4998 |          15.6901 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |          -0.0095 |          38.8403 |          15.6812 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |          -0.0006 |          38.4835 |          15.6735 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |           0.0063 |          42.2442 |          15.6551 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |           0.0034 |          42.7118 |          15.6615 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |          -0.0084 |          37.1015 |          15.6429 |
[32m[20221213 22:55:30 @agent_ppo2.py:185][0m |          -0.0124 |          36.5904 |          15.6502 |
[32m[20221213 22:55:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:55:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.47
[32m[20221213 22:55:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.04
[32m[20221213 22:55:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.33
[32m[20221213 22:55:31 @agent_ppo2.py:143][0m Total time:      37.29 min
[32m[20221213 22:55:31 @agent_ppo2.py:145][0m 3651584 total steps have happened
[32m[20221213 22:55:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1783 --------------------------#
[32m[20221213 22:55:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0000 |          31.4669 |          15.5757 |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0080 |          28.3909 |          15.5681 |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0110 |          27.5000 |          15.5706 |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0104 |          27.0615 |          15.5784 |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0084 |          26.7466 |          15.5803 |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0075 |          26.3662 |          15.5866 |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0147 |          26.1561 |          15.5856 |
[32m[20221213 22:55:31 @agent_ppo2.py:185][0m |          -0.0157 |          25.9453 |          15.5736 |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |          -0.0144 |          25.7699 |          15.5698 |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |          -0.0088 |          25.5916 |          15.5776 |
[32m[20221213 22:55:32 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.03
[32m[20221213 22:55:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.30
[32m[20221213 22:55:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.71
[32m[20221213 22:55:32 @agent_ppo2.py:143][0m Total time:      37.31 min
[32m[20221213 22:55:32 @agent_ppo2.py:145][0m 3653632 total steps have happened
[32m[20221213 22:55:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1784 --------------------------#
[32m[20221213 22:55:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |           0.0025 |          37.4018 |          15.5109 |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |          -0.0030 |          35.1989 |          15.4890 |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |          -0.0069 |          34.8751 |          15.5015 |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |          -0.0076 |          34.4818 |          15.5008 |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |          -0.0127 |          34.4377 |          15.5022 |
[32m[20221213 22:55:32 @agent_ppo2.py:185][0m |          -0.0121 |          34.1549 |          15.4975 |
[32m[20221213 22:55:33 @agent_ppo2.py:185][0m |          -0.0103 |          34.0809 |          15.5004 |
[32m[20221213 22:55:33 @agent_ppo2.py:185][0m |          -0.0123 |          33.9998 |          15.5013 |
[32m[20221213 22:55:33 @agent_ppo2.py:185][0m |          -0.0084 |          33.8754 |          15.4934 |
[32m[20221213 22:55:33 @agent_ppo2.py:185][0m |          -0.0150 |          34.7670 |          15.4982 |
[32m[20221213 22:55:33 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.27
[32m[20221213 22:55:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.36
[32m[20221213 22:55:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.61
[32m[20221213 22:55:33 @agent_ppo2.py:143][0m Total time:      37.33 min
[32m[20221213 22:55:33 @agent_ppo2.py:145][0m 3655680 total steps have happened
[32m[20221213 22:55:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1785 --------------------------#
[32m[20221213 22:55:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:33 @agent_ppo2.py:185][0m |          -0.0057 |          37.0553 |          15.6760 |
[32m[20221213 22:55:33 @agent_ppo2.py:185][0m |          -0.0034 |          33.3797 |          15.6794 |
[32m[20221213 22:55:33 @agent_ppo2.py:185][0m |          -0.0101 |          32.3367 |          15.6861 |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |          -0.0099 |          31.6527 |          15.6926 |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |          -0.0133 |          31.0197 |          15.6850 |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |          -0.0119 |          30.6447 |          15.7001 |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |          -0.0121 |          30.1164 |          15.7011 |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |          -0.0138 |          29.8836 |          15.7191 |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |          -0.0147 |          29.5784 |          15.7275 |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |          -0.0135 |          29.4204 |          15.7181 |
[32m[20221213 22:55:34 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.15
[32m[20221213 22:55:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.12
[32m[20221213 22:55:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.81
[32m[20221213 22:55:34 @agent_ppo2.py:143][0m Total time:      37.35 min
[32m[20221213 22:55:34 @agent_ppo2.py:145][0m 3657728 total steps have happened
[32m[20221213 22:55:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1786 --------------------------#
[32m[20221213 22:55:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:34 @agent_ppo2.py:185][0m |           0.0033 |          38.9786 |          15.5874 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |           0.0033 |          38.4719 |          15.5736 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0067 |          36.2466 |          15.5388 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0095 |          35.5097 |          15.5646 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0090 |          35.0432 |          15.5486 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0124 |          34.8444 |          15.5534 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0074 |          34.7001 |          15.5455 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0076 |          34.6351 |          15.5357 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0149 |          34.1250 |          15.5396 |
[32m[20221213 22:55:35 @agent_ppo2.py:185][0m |          -0.0128 |          34.0253 |          15.5444 |
[32m[20221213 22:55:35 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 321.13
[32m[20221213 22:55:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.50
[32m[20221213 22:55:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 386.73
[32m[20221213 22:55:35 @agent_ppo2.py:143][0m Total time:      37.37 min
[32m[20221213 22:55:35 @agent_ppo2.py:145][0m 3659776 total steps have happened
[32m[20221213 22:55:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1787 --------------------------#
[32m[20221213 22:55:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0001 |          43.3777 |          15.7763 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |           0.0002 |          40.3524 |          15.7493 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0017 |          39.7806 |          15.7508 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0079 |          39.2097 |          15.7640 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0094 |          38.4894 |          15.7550 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0012 |          39.0540 |          15.7687 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0066 |          37.8972 |          15.7528 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0076 |          37.7408 |          15.7509 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0038 |          38.7951 |          15.7484 |
[32m[20221213 22:55:36 @agent_ppo2.py:185][0m |          -0.0072 |          37.4288 |          15.7379 |
[32m[20221213 22:55:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:55:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.87
[32m[20221213 22:55:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.89
[32m[20221213 22:55:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.12
[32m[20221213 22:55:37 @agent_ppo2.py:143][0m Total time:      37.39 min
[32m[20221213 22:55:37 @agent_ppo2.py:145][0m 3661824 total steps have happened
[32m[20221213 22:55:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1788 --------------------------#
[32m[20221213 22:55:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |           0.0000 |          31.8230 |          15.6143 |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |          -0.0029 |          28.6273 |          15.5958 |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |          -0.0023 |          27.5796 |          15.6009 |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |          -0.0068 |          27.2076 |          15.5999 |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |          -0.0100 |          26.5904 |          15.5860 |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |          -0.0076 |          26.5791 |          15.5851 |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |          -0.0090 |          26.2989 |          15.5973 |
[32m[20221213 22:55:37 @agent_ppo2.py:185][0m |          -0.0114 |          25.5062 |          15.5802 |
[32m[20221213 22:55:38 @agent_ppo2.py:185][0m |          -0.0057 |          25.3852 |          15.5845 |
[32m[20221213 22:55:38 @agent_ppo2.py:185][0m |          -0.0141 |          25.0431 |          15.5708 |
[32m[20221213 22:55:38 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.17
[32m[20221213 22:55:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.95
[32m[20221213 22:55:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.29
[32m[20221213 22:55:38 @agent_ppo2.py:143][0m Total time:      37.41 min
[32m[20221213 22:55:38 @agent_ppo2.py:145][0m 3663872 total steps have happened
[32m[20221213 22:55:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1789 --------------------------#
[32m[20221213 22:55:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:38 @agent_ppo2.py:185][0m |          -0.0010 |          36.9030 |          15.3685 |
[32m[20221213 22:55:38 @agent_ppo2.py:185][0m |          -0.0056 |          33.5992 |          15.3798 |
[32m[20221213 22:55:38 @agent_ppo2.py:185][0m |          -0.0055 |          32.4840 |          15.3587 |
[32m[20221213 22:55:38 @agent_ppo2.py:185][0m |           0.0012 |          33.2135 |          15.3621 |
[32m[20221213 22:55:38 @agent_ppo2.py:185][0m |          -0.0146 |          31.5396 |          15.3683 |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0128 |          30.8756 |          15.3709 |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0116 |          30.7328 |          15.3667 |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0064 |          31.3021 |          15.3676 |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0044 |          30.9830 |          15.3666 |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0154 |          29.8228 |          15.3735 |
[32m[20221213 22:55:39 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.46
[32m[20221213 22:55:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 361.78
[32m[20221213 22:55:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.55
[32m[20221213 22:55:39 @agent_ppo2.py:143][0m Total time:      37.43 min
[32m[20221213 22:55:39 @agent_ppo2.py:145][0m 3665920 total steps have happened
[32m[20221213 22:55:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1790 --------------------------#
[32m[20221213 22:55:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:55:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0035 |          33.9349 |          15.7062 |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0050 |          30.3869 |          15.7097 |
[32m[20221213 22:55:39 @agent_ppo2.py:185][0m |          -0.0071 |          29.3894 |          15.7038 |
[32m[20221213 22:55:40 @agent_ppo2.py:185][0m |          -0.0059 |          28.6519 |          15.7158 |
[32m[20221213 22:55:40 @agent_ppo2.py:185][0m |          -0.0005 |          28.3368 |          15.7141 |
[32m[20221213 22:55:40 @agent_ppo2.py:185][0m |          -0.0086 |          27.6488 |          15.7136 |
[32m[20221213 22:55:40 @agent_ppo2.py:185][0m |          -0.0159 |          27.1640 |          15.7098 |
[32m[20221213 22:55:40 @agent_ppo2.py:185][0m |          -0.0111 |          26.8899 |          15.7122 |
[32m[20221213 22:55:40 @agent_ppo2.py:185][0m |          -0.0196 |          26.9489 |          15.7092 |
[32m[20221213 22:55:40 @agent_ppo2.py:185][0m |          -0.0038 |          28.8986 |          15.7287 |
[32m[20221213 22:55:40 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.72
[32m[20221213 22:55:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 313.45
[32m[20221213 22:55:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.34
[32m[20221213 22:55:40 @agent_ppo2.py:143][0m Total time:      37.46 min
[32m[20221213 22:55:40 @agent_ppo2.py:145][0m 3667968 total steps have happened
[32m[20221213 22:55:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1791 --------------------------#
[32m[20221213 22:55:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |           0.0003 |          53.8163 |          15.6795 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0046 |          51.9476 |          15.6860 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0089 |          51.7452 |          15.6844 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0103 |          51.3320 |          15.6502 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0122 |          51.1303 |          15.6621 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0025 |          52.5278 |          15.6554 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |           0.0007 |          55.3597 |          15.6479 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0130 |          50.7587 |          15.6533 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0074 |          51.4327 |          15.6428 |
[32m[20221213 22:55:41 @agent_ppo2.py:185][0m |          -0.0130 |          50.6028 |          15.6205 |
[32m[20221213 22:55:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:55:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.36
[32m[20221213 22:55:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.75
[32m[20221213 22:55:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.33
[32m[20221213 22:55:42 @agent_ppo2.py:143][0m Total time:      37.48 min
[32m[20221213 22:55:42 @agent_ppo2.py:145][0m 3670016 total steps have happened
[32m[20221213 22:55:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1792 --------------------------#
[32m[20221213 22:55:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0019 |          42.5067 |          15.8561 |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0037 |          40.5762 |          15.8763 |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0058 |          40.0902 |          15.8493 |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0090 |          39.7778 |          15.8634 |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0105 |          39.5219 |          15.8716 |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0024 |          40.9286 |          15.8480 |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0019 |          39.7315 |          15.8480 |
[32m[20221213 22:55:42 @agent_ppo2.py:185][0m |          -0.0060 |          40.4176 |          15.8527 |
[32m[20221213 22:55:43 @agent_ppo2.py:185][0m |          -0.0103 |          39.2435 |          15.8667 |
[32m[20221213 22:55:43 @agent_ppo2.py:185][0m |          -0.0113 |          38.6709 |          15.8535 |
[32m[20221213 22:55:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:55:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.34
[32m[20221213 22:55:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.94
[32m[20221213 22:55:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.52
[32m[20221213 22:55:43 @agent_ppo2.py:143][0m Total time:      37.50 min
[32m[20221213 22:55:43 @agent_ppo2.py:145][0m 3672064 total steps have happened
[32m[20221213 22:55:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1793 --------------------------#
[32m[20221213 22:55:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:43 @agent_ppo2.py:185][0m |           0.0001 |          38.0129 |          15.6021 |
[32m[20221213 22:55:43 @agent_ppo2.py:185][0m |          -0.0088 |          34.3648 |          15.5688 |
[32m[20221213 22:55:43 @agent_ppo2.py:185][0m |          -0.0088 |          33.0733 |          15.5540 |
[32m[20221213 22:55:43 @agent_ppo2.py:185][0m |          -0.0126 |          32.3541 |          15.5546 |
[32m[20221213 22:55:43 @agent_ppo2.py:185][0m |          -0.0114 |          31.7810 |          15.5554 |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0158 |          31.4908 |          15.5656 |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0151 |          31.0687 |          15.5606 |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0082 |          32.2704 |          15.5642 |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0091 |          30.5395 |          15.5553 |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0211 |          30.2347 |          15.5520 |
[32m[20221213 22:55:44 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.80
[32m[20221213 22:55:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 311.22
[32m[20221213 22:55:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.37
[32m[20221213 22:55:44 @agent_ppo2.py:143][0m Total time:      37.52 min
[32m[20221213 22:55:44 @agent_ppo2.py:145][0m 3674112 total steps have happened
[32m[20221213 22:55:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1794 --------------------------#
[32m[20221213 22:55:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0029 |          45.3992 |          15.6846 |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0020 |          43.4145 |          15.6952 |
[32m[20221213 22:55:44 @agent_ppo2.py:185][0m |          -0.0053 |          43.0084 |          15.6827 |
[32m[20221213 22:55:45 @agent_ppo2.py:185][0m |          -0.0047 |          42.5001 |          15.6838 |
[32m[20221213 22:55:45 @agent_ppo2.py:185][0m |          -0.0109 |          42.2163 |          15.6976 |
[32m[20221213 22:55:45 @agent_ppo2.py:185][0m |          -0.0013 |          42.4230 |          15.6926 |
[32m[20221213 22:55:45 @agent_ppo2.py:185][0m |          -0.0094 |          41.8429 |          15.7175 |
[32m[20221213 22:55:45 @agent_ppo2.py:185][0m |          -0.0063 |          42.3100 |          15.7096 |
[32m[20221213 22:55:45 @agent_ppo2.py:185][0m |          -0.0072 |          41.7026 |          15.7394 |
[32m[20221213 22:55:45 @agent_ppo2.py:185][0m |          -0.0107 |          41.5124 |          15.7410 |
[32m[20221213 22:55:45 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.66
[32m[20221213 22:55:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.21
[32m[20221213 22:55:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.90
[32m[20221213 22:55:45 @agent_ppo2.py:143][0m Total time:      37.54 min
[32m[20221213 22:55:45 @agent_ppo2.py:145][0m 3676160 total steps have happened
[32m[20221213 22:55:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1795 --------------------------#
[32m[20221213 22:55:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |           0.0034 |          38.3649 |          15.6896 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0060 |          34.3323 |          15.6820 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0034 |          33.6708 |          15.6882 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0083 |          32.3330 |          15.6829 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0094 |          31.4631 |          15.6803 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0087 |          30.8679 |          15.6849 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0104 |          30.3777 |          15.6751 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0076 |          30.1211 |          15.6751 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0073 |          29.7340 |          15.6820 |
[32m[20221213 22:55:46 @agent_ppo2.py:185][0m |          -0.0134 |          29.5060 |          15.6776 |
[32m[20221213 22:55:46 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.46
[32m[20221213 22:55:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 409.65
[32m[20221213 22:55:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.74
[32m[20221213 22:55:46 @agent_ppo2.py:143][0m Total time:      37.56 min
[32m[20221213 22:55:46 @agent_ppo2.py:145][0m 3678208 total steps have happened
[32m[20221213 22:55:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1796 --------------------------#
[32m[20221213 22:55:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0034 |          45.6899 |          15.5629 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0048 |          43.5156 |          15.5646 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0095 |          42.3352 |          15.5733 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0075 |          41.5264 |          15.5642 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0127 |          41.0808 |          15.5647 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0114 |          40.7981 |          15.5558 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0128 |          39.9806 |          15.5693 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |           0.0004 |          43.6784 |          15.5454 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0143 |          39.5564 |          15.5529 |
[32m[20221213 22:55:47 @agent_ppo2.py:185][0m |          -0.0140 |          39.1311 |          15.5457 |
[32m[20221213 22:55:47 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.37
[32m[20221213 22:55:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.42
[32m[20221213 22:55:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:55:48 @agent_ppo2.py:143][0m Total time:      37.58 min
[32m[20221213 22:55:48 @agent_ppo2.py:145][0m 3680256 total steps have happened
[32m[20221213 22:55:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1797 --------------------------#
[32m[20221213 22:55:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:48 @agent_ppo2.py:185][0m |           0.0011 |          48.6994 |          15.7438 |
[32m[20221213 22:55:48 @agent_ppo2.py:185][0m |           0.0058 |          50.0448 |          15.7276 |
[32m[20221213 22:55:48 @agent_ppo2.py:185][0m |          -0.0069 |          46.7576 |          15.7357 |
[32m[20221213 22:55:48 @agent_ppo2.py:185][0m |          -0.0078 |          46.3709 |          15.7382 |
[32m[20221213 22:55:48 @agent_ppo2.py:185][0m |           0.0087 |          50.0656 |          15.7439 |
[32m[20221213 22:55:48 @agent_ppo2.py:185][0m |          -0.0005 |          47.5751 |          15.7353 |
[32m[20221213 22:55:48 @agent_ppo2.py:185][0m |          -0.0039 |          46.6956 |          15.7380 |
[32m[20221213 22:55:49 @agent_ppo2.py:185][0m |          -0.0097 |          45.9539 |          15.7499 |
[32m[20221213 22:55:49 @agent_ppo2.py:185][0m |          -0.0069 |          45.8494 |          15.7588 |
[32m[20221213 22:55:49 @agent_ppo2.py:185][0m |          -0.0110 |          45.7409 |          15.7578 |
[32m[20221213 22:55:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:55:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.66
[32m[20221213 22:55:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.96
[32m[20221213 22:55:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.94
[32m[20221213 22:55:49 @agent_ppo2.py:143][0m Total time:      37.60 min
[32m[20221213 22:55:49 @agent_ppo2.py:145][0m 3682304 total steps have happened
[32m[20221213 22:55:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1798 --------------------------#
[32m[20221213 22:55:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:49 @agent_ppo2.py:185][0m |          -0.0030 |          48.7235 |          15.7216 |
[32m[20221213 22:55:49 @agent_ppo2.py:185][0m |          -0.0069 |          45.2927 |          15.7080 |
[32m[20221213 22:55:49 @agent_ppo2.py:185][0m |           0.0002 |          46.5098 |          15.7156 |
[32m[20221213 22:55:49 @agent_ppo2.py:185][0m |          -0.0095 |          43.5703 |          15.7149 |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |          -0.0120 |          42.6975 |          15.7049 |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |          -0.0130 |          42.3686 |          15.7084 |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |          -0.0142 |          42.0477 |          15.7011 |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |          -0.0114 |          41.8487 |          15.7078 |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |          -0.0129 |          41.6198 |          15.6910 |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |          -0.0138 |          41.3993 |          15.7171 |
[32m[20221213 22:55:50 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.11
[32m[20221213 22:55:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.78
[32m[20221213 22:55:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.56
[32m[20221213 22:55:50 @agent_ppo2.py:143][0m Total time:      37.62 min
[32m[20221213 22:55:50 @agent_ppo2.py:145][0m 3684352 total steps have happened
[32m[20221213 22:55:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1799 --------------------------#
[32m[20221213 22:55:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |           0.0078 |          50.8049 |          15.7448 |
[32m[20221213 22:55:50 @agent_ppo2.py:185][0m |           0.0023 |          41.2153 |          15.7507 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |           0.0003 |          45.6247 |          15.7316 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |          -0.0005 |          39.9138 |          15.7166 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |          -0.0037 |          39.4777 |          15.7088 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |          -0.0086 |          39.3571 |          15.7056 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |          -0.0058 |          39.1948 |          15.7033 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |          -0.0127 |          38.8159 |          15.6787 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |          -0.0082 |          38.7430 |          15.6866 |
[32m[20221213 22:55:51 @agent_ppo2.py:185][0m |          -0.0153 |          38.4851 |          15.6700 |
[32m[20221213 22:55:51 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.92
[32m[20221213 22:55:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.85
[32m[20221213 22:55:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.28
[32m[20221213 22:55:51 @agent_ppo2.py:143][0m Total time:      37.64 min
[32m[20221213 22:55:51 @agent_ppo2.py:145][0m 3686400 total steps have happened
[32m[20221213 22:55:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1800 --------------------------#
[32m[20221213 22:55:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:55:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0002 |          51.6840 |          15.6310 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |           0.0038 |          51.2304 |          15.6049 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0018 |          48.2060 |          15.5988 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0054 |          47.6895 |          15.5949 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |           0.0021 |          51.7035 |          15.5828 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0074 |          46.9055 |          15.5668 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0060 |          46.8546 |          15.5712 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0083 |          46.5297 |          15.5653 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0123 |          46.5156 |          15.5499 |
[32m[20221213 22:55:52 @agent_ppo2.py:185][0m |          -0.0101 |          46.3652 |          15.5410 |
[32m[20221213 22:55:52 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.22
[32m[20221213 22:55:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.99
[32m[20221213 22:55:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.69
[32m[20221213 22:55:53 @agent_ppo2.py:143][0m Total time:      37.66 min
[32m[20221213 22:55:53 @agent_ppo2.py:145][0m 3688448 total steps have happened
[32m[20221213 22:55:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1801 --------------------------#
[32m[20221213 22:55:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |           0.0019 |          48.6135 |          15.4804 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |          -0.0038 |          47.4884 |          15.5099 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |           0.0003 |          47.7817 |          15.5132 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |          -0.0021 |          47.0614 |          15.5151 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |          -0.0039 |          46.7983 |          15.5164 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |          -0.0056 |          46.7023 |          15.5270 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |          -0.0075 |          46.7961 |          15.5199 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |          -0.0087 |          46.6155 |          15.5272 |
[32m[20221213 22:55:53 @agent_ppo2.py:185][0m |          -0.0039 |          46.7136 |          15.5334 |
[32m[20221213 22:55:54 @agent_ppo2.py:185][0m |          -0.0058 |          46.3808 |          15.5174 |
[32m[20221213 22:55:54 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:55:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.58
[32m[20221213 22:55:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.60
[32m[20221213 22:55:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.02
[32m[20221213 22:55:54 @agent_ppo2.py:143][0m Total time:      37.68 min
[32m[20221213 22:55:54 @agent_ppo2.py:145][0m 3690496 total steps have happened
[32m[20221213 22:55:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1802 --------------------------#
[32m[20221213 22:55:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:54 @agent_ppo2.py:185][0m |          -0.0004 |          47.1430 |          15.5713 |
[32m[20221213 22:55:54 @agent_ppo2.py:185][0m |          -0.0069 |          43.0391 |          15.5535 |
[32m[20221213 22:55:54 @agent_ppo2.py:185][0m |          -0.0042 |          41.7835 |          15.5496 |
[32m[20221213 22:55:54 @agent_ppo2.py:185][0m |          -0.0105 |          40.8335 |          15.5503 |
[32m[20221213 22:55:54 @agent_ppo2.py:185][0m |           0.0033 |          43.9190 |          15.5457 |
[32m[20221213 22:55:54 @agent_ppo2.py:185][0m |          -0.0094 |          40.2817 |          15.5519 |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |          -0.0124 |          39.8549 |          15.5305 |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |          -0.0131 |          39.6568 |          15.5497 |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |          -0.0130 |          39.4591 |          15.5392 |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |          -0.0103 |          39.3014 |          15.5546 |
[32m[20221213 22:55:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:55:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.07
[32m[20221213 22:55:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.27
[32m[20221213 22:55:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.96
[32m[20221213 22:55:55 @agent_ppo2.py:143][0m Total time:      37.70 min
[32m[20221213 22:55:55 @agent_ppo2.py:145][0m 3692544 total steps have happened
[32m[20221213 22:55:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1803 --------------------------#
[32m[20221213 22:55:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |          -0.0037 |          36.5121 |          15.6172 |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |           0.0040 |          37.8585 |          15.5874 |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |          -0.0018 |          32.4506 |          15.5901 |
[32m[20221213 22:55:55 @agent_ppo2.py:185][0m |          -0.0039 |          31.8885 |          15.5958 |
[32m[20221213 22:55:56 @agent_ppo2.py:185][0m |          -0.0060 |          31.6233 |          15.5725 |
[32m[20221213 22:55:56 @agent_ppo2.py:185][0m |          -0.0057 |          31.3182 |          15.5839 |
[32m[20221213 22:55:56 @agent_ppo2.py:185][0m |          -0.0069 |          31.0863 |          15.5710 |
[32m[20221213 22:55:56 @agent_ppo2.py:185][0m |          -0.0070 |          30.9327 |          15.5799 |
[32m[20221213 22:55:56 @agent_ppo2.py:185][0m |          -0.0104 |          30.8164 |          15.5761 |
[32m[20221213 22:55:56 @agent_ppo2.py:185][0m |          -0.0160 |          30.7302 |          15.5701 |
[32m[20221213 22:55:56 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.78
[32m[20221213 22:55:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.53
[32m[20221213 22:55:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.80
[32m[20221213 22:55:56 @agent_ppo2.py:143][0m Total time:      37.72 min
[32m[20221213 22:55:56 @agent_ppo2.py:145][0m 3694592 total steps have happened
[32m[20221213 22:55:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1804 --------------------------#
[32m[20221213 22:55:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:56 @agent_ppo2.py:185][0m |          -0.0007 |          39.6877 |          15.8167 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0017 |          32.8316 |          15.8044 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0056 |          31.8872 |          15.7668 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0010 |          32.0841 |          15.7791 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0056 |          31.9163 |          15.7875 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |           0.0031 |          34.8653 |          15.7815 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0038 |          30.8230 |          15.7893 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0160 |          30.3540 |          15.8091 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0073 |          30.0392 |          15.8006 |
[32m[20221213 22:55:57 @agent_ppo2.py:185][0m |          -0.0072 |          29.9360 |          15.8034 |
[32m[20221213 22:55:57 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:55:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 315.32
[32m[20221213 22:55:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.70
[32m[20221213 22:55:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.03
[32m[20221213 22:55:57 @agent_ppo2.py:143][0m Total time:      37.74 min
[32m[20221213 22:55:57 @agent_ppo2.py:145][0m 3696640 total steps have happened
[32m[20221213 22:55:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1805 --------------------------#
[32m[20221213 22:55:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |           0.0031 |          45.7130 |          15.7560 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |          -0.0074 |          42.4994 |          15.7340 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |          -0.0045 |          41.8230 |          15.7453 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |           0.0019 |          44.5808 |          15.7417 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |          -0.0109 |          40.9126 |          15.7205 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |           0.0007 |          40.5464 |          15.7451 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |          -0.0115 |          40.2984 |          15.7220 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |          -0.0084 |          40.1014 |          15.7436 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |          -0.0078 |          42.0484 |          15.7518 |
[32m[20221213 22:55:58 @agent_ppo2.py:185][0m |          -0.0113 |          39.6057 |          15.7360 |
[32m[20221213 22:55:58 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:55:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.70
[32m[20221213 22:55:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.90
[32m[20221213 22:55:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.03
[32m[20221213 22:55:59 @agent_ppo2.py:143][0m Total time:      37.76 min
[32m[20221213 22:55:59 @agent_ppo2.py:145][0m 3698688 total steps have happened
[32m[20221213 22:55:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1806 --------------------------#
[32m[20221213 22:55:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:55:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |           0.0038 |          54.4355 |          15.3480 |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |          -0.0062 |          50.5601 |          15.3551 |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |          -0.0058 |          49.0719 |          15.3617 |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |          -0.0083 |          48.1667 |          15.3403 |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |          -0.0116 |          47.6226 |          15.3557 |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |          -0.0090 |          47.5840 |          15.3565 |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |          -0.0072 |          46.9475 |          15.3615 |
[32m[20221213 22:55:59 @agent_ppo2.py:185][0m |          -0.0109 |          46.7572 |          15.3662 |
[32m[20221213 22:56:00 @agent_ppo2.py:185][0m |          -0.0110 |          46.5489 |          15.3529 |
[32m[20221213 22:56:00 @agent_ppo2.py:185][0m |          -0.0115 |          46.1662 |          15.3446 |
[32m[20221213 22:56:00 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:56:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.02
[32m[20221213 22:56:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.71
[32m[20221213 22:56:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.38
[32m[20221213 22:56:00 @agent_ppo2.py:143][0m Total time:      37.78 min
[32m[20221213 22:56:00 @agent_ppo2.py:145][0m 3700736 total steps have happened
[32m[20221213 22:56:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1807 --------------------------#
[32m[20221213 22:56:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:00 @agent_ppo2.py:185][0m |          -0.0074 |          37.7086 |          15.7968 |
[32m[20221213 22:56:00 @agent_ppo2.py:185][0m |          -0.0004 |          33.9909 |          15.7934 |
[32m[20221213 22:56:00 @agent_ppo2.py:185][0m |          -0.0032 |          33.4544 |          15.7737 |
[32m[20221213 22:56:00 @agent_ppo2.py:185][0m |           0.0030 |          35.5561 |          15.7664 |
[32m[20221213 22:56:00 @agent_ppo2.py:185][0m |          -0.0035 |          33.1025 |          15.7538 |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0118 |          32.0492 |          15.7675 |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0106 |          31.9000 |          15.7588 |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0128 |          31.5289 |          15.7681 |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0107 |          31.4831 |          15.7538 |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0129 |          31.3455 |          15.7664 |
[32m[20221213 22:56:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:56:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.42
[32m[20221213 22:56:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.29
[32m[20221213 22:56:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.74
[32m[20221213 22:56:01 @agent_ppo2.py:143][0m Total time:      37.80 min
[32m[20221213 22:56:01 @agent_ppo2.py:145][0m 3702784 total steps have happened
[32m[20221213 22:56:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1808 --------------------------#
[32m[20221213 22:56:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0022 |          39.7078 |          15.7265 |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0044 |          36.9130 |          15.6930 |
[32m[20221213 22:56:01 @agent_ppo2.py:185][0m |          -0.0078 |          35.5961 |          15.7134 |
[32m[20221213 22:56:02 @agent_ppo2.py:185][0m |          -0.0069 |          34.7663 |          15.6839 |
[32m[20221213 22:56:02 @agent_ppo2.py:185][0m |          -0.0094 |          34.4272 |          15.6884 |
[32m[20221213 22:56:02 @agent_ppo2.py:185][0m |           0.0002 |          34.6672 |          15.6803 |
[32m[20221213 22:56:02 @agent_ppo2.py:185][0m |          -0.0134 |          33.6774 |          15.6818 |
[32m[20221213 22:56:02 @agent_ppo2.py:185][0m |          -0.0038 |          34.3778 |          15.6955 |
[32m[20221213 22:56:02 @agent_ppo2.py:185][0m |          -0.0156 |          33.2274 |          15.6952 |
[32m[20221213 22:56:02 @agent_ppo2.py:185][0m |          -0.0130 |          32.9744 |          15.6818 |
[32m[20221213 22:56:02 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:56:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.88
[32m[20221213 22:56:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.43
[32m[20221213 22:56:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.97
[32m[20221213 22:56:02 @agent_ppo2.py:143][0m Total time:      37.82 min
[32m[20221213 22:56:02 @agent_ppo2.py:145][0m 3704832 total steps have happened
[32m[20221213 22:56:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1809 --------------------------#
[32m[20221213 22:56:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |           0.0106 |          50.3356 |          15.6659 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |           0.0134 |          48.2014 |          15.6589 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |          -0.0072 |          41.6283 |          15.6726 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |          -0.0031 |          40.9659 |          15.6855 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |          -0.0058 |          40.4203 |          15.6767 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |          -0.0052 |          39.9257 |          15.6748 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |          -0.0071 |          39.6740 |          15.6662 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |           0.0039 |          42.3872 |          15.6659 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |          -0.0076 |          39.3028 |          15.6857 |
[32m[20221213 22:56:03 @agent_ppo2.py:185][0m |          -0.0115 |          39.0070 |          15.6766 |
[32m[20221213 22:56:03 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.22
[32m[20221213 22:56:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.19
[32m[20221213 22:56:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.52
[32m[20221213 22:56:03 @agent_ppo2.py:143][0m Total time:      37.84 min
[32m[20221213 22:56:03 @agent_ppo2.py:145][0m 3706880 total steps have happened
[32m[20221213 22:56:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1810 --------------------------#
[32m[20221213 22:56:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:56:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0057 |          27.3055 |          15.6897 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0013 |          19.6060 |          15.6558 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0097 |          18.7335 |          15.6507 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0134 |          18.0069 |          15.6342 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0057 |          17.4960 |          15.6379 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0128 |          17.0187 |          15.6457 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0016 |          17.0201 |          15.6331 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0150 |          16.4920 |          15.6321 |
[32m[20221213 22:56:04 @agent_ppo2.py:185][0m |          -0.0148 |          16.3352 |          15.6143 |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |          -0.0117 |          15.9789 |          15.6207 |
[32m[20221213 22:56:05 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:56:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.85
[32m[20221213 22:56:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 348.18
[32m[20221213 22:56:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.94
[32m[20221213 22:56:05 @agent_ppo2.py:143][0m Total time:      37.86 min
[32m[20221213 22:56:05 @agent_ppo2.py:145][0m 3708928 total steps have happened
[32m[20221213 22:56:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1811 --------------------------#
[32m[20221213 22:56:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |          -0.0041 |          40.9577 |          15.6529 |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |          -0.0054 |          38.3963 |          15.6353 |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |           0.0025 |          38.2877 |          15.6534 |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |           0.0074 |          38.7810 |          15.6219 |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |          -0.0093 |          37.0401 |          15.6287 |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |          -0.0030 |          37.1514 |          15.6351 |
[32m[20221213 22:56:05 @agent_ppo2.py:185][0m |          -0.0093 |          36.6355 |          15.6187 |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |          -0.0101 |          36.3269 |          15.6125 |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |          -0.0104 |          36.7839 |          15.6269 |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |          -0.0097 |          36.0980 |          15.6134 |
[32m[20221213 22:56:06 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.12
[32m[20221213 22:56:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.48
[32m[20221213 22:56:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.27
[32m[20221213 22:56:06 @agent_ppo2.py:143][0m Total time:      37.88 min
[32m[20221213 22:56:06 @agent_ppo2.py:145][0m 3710976 total steps have happened
[32m[20221213 22:56:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1812 --------------------------#
[32m[20221213 22:56:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |           0.0081 |          25.6528 |          15.8197 |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |          -0.0065 |          22.0854 |          15.8185 |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |          -0.0041 |          22.4768 |          15.8203 |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |          -0.0111 |          20.9755 |          15.7929 |
[32m[20221213 22:56:06 @agent_ppo2.py:185][0m |          -0.0103 |          20.6827 |          15.8091 |
[32m[20221213 22:56:07 @agent_ppo2.py:185][0m |          -0.0137 |          20.5900 |          15.8004 |
[32m[20221213 22:56:07 @agent_ppo2.py:185][0m |          -0.0140 |          20.3726 |          15.7829 |
[32m[20221213 22:56:07 @agent_ppo2.py:185][0m |          -0.0121 |          20.2043 |          15.8000 |
[32m[20221213 22:56:07 @agent_ppo2.py:185][0m |          -0.0110 |          20.2130 |          15.7873 |
[32m[20221213 22:56:07 @agent_ppo2.py:185][0m |          -0.0135 |          19.9914 |          15.7821 |
[32m[20221213 22:56:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:56:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.25
[32m[20221213 22:56:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 324.41
[32m[20221213 22:56:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.70
[32m[20221213 22:56:07 @agent_ppo2.py:143][0m Total time:      37.90 min
[32m[20221213 22:56:07 @agent_ppo2.py:145][0m 3713024 total steps have happened
[32m[20221213 22:56:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1813 --------------------------#
[32m[20221213 22:56:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:07 @agent_ppo2.py:185][0m |           0.0015 |          29.8265 |          15.5313 |
[32m[20221213 22:56:07 @agent_ppo2.py:185][0m |          -0.0080 |          27.4738 |          15.5031 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0082 |          26.7314 |          15.4851 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0131 |          26.3320 |          15.5167 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0097 |          25.9799 |          15.5007 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0146 |          25.7206 |          15.4991 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0135 |          25.5254 |          15.5010 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0109 |          25.2380 |          15.5098 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0119 |          25.1013 |          15.5153 |
[32m[20221213 22:56:08 @agent_ppo2.py:185][0m |          -0.0164 |          24.8894 |          15.5464 |
[32m[20221213 22:56:08 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:56:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.72
[32m[20221213 22:56:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.16
[32m[20221213 22:56:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.97
[32m[20221213 22:56:08 @agent_ppo2.py:143][0m Total time:      37.92 min
[32m[20221213 22:56:08 @agent_ppo2.py:145][0m 3715072 total steps have happened
[32m[20221213 22:56:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1814 --------------------------#
[32m[20221213 22:56:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |           0.0007 |          46.0793 |          15.5909 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0006 |          44.9102 |          15.5869 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0071 |          44.5940 |          15.5681 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0084 |          44.3853 |          15.5697 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0079 |          44.1555 |          15.5757 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0104 |          44.0230 |          15.5697 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0106 |          43.9262 |          15.5812 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0099 |          43.8577 |          15.5685 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0099 |          43.7197 |          15.5677 |
[32m[20221213 22:56:09 @agent_ppo2.py:185][0m |          -0.0145 |          43.5998 |          15.5699 |
[32m[20221213 22:56:09 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:56:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.41
[32m[20221213 22:56:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.13
[32m[20221213 22:56:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.13
[32m[20221213 22:56:10 @agent_ppo2.py:143][0m Total time:      37.94 min
[32m[20221213 22:56:10 @agent_ppo2.py:145][0m 3717120 total steps have happened
[32m[20221213 22:56:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1815 --------------------------#
[32m[20221213 22:56:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |           0.0027 |          42.3600 |          15.8078 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |           0.0033 |          42.6224 |          15.7748 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |          -0.0105 |          39.6583 |          15.7745 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |          -0.0038 |          39.0360 |          15.7670 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |          -0.0119 |          38.4899 |          15.7761 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |          -0.0103 |          38.2338 |          15.7830 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |          -0.0052 |          38.1183 |          15.7800 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |          -0.0081 |          37.9124 |          15.7742 |
[32m[20221213 22:56:10 @agent_ppo2.py:185][0m |          -0.0134 |          37.6862 |          15.7913 |
[32m[20221213 22:56:11 @agent_ppo2.py:185][0m |          -0.0126 |          37.4838 |          15.7882 |
[32m[20221213 22:56:11 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.53
[32m[20221213 22:56:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 336.28
[32m[20221213 22:56:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.67
[32m[20221213 22:56:11 @agent_ppo2.py:143][0m Total time:      37.96 min
[32m[20221213 22:56:11 @agent_ppo2.py:145][0m 3719168 total steps have happened
[32m[20221213 22:56:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1816 --------------------------#
[32m[20221213 22:56:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:11 @agent_ppo2.py:185][0m |          -0.0025 |          38.9699 |          15.6621 |
[32m[20221213 22:56:11 @agent_ppo2.py:185][0m |          -0.0074 |          37.7621 |          15.6501 |
[32m[20221213 22:56:11 @agent_ppo2.py:185][0m |          -0.0089 |          37.3559 |          15.6385 |
[32m[20221213 22:56:11 @agent_ppo2.py:185][0m |          -0.0070 |          37.0808 |          15.6432 |
[32m[20221213 22:56:11 @agent_ppo2.py:185][0m |          -0.0109 |          36.8208 |          15.6404 |
[32m[20221213 22:56:11 @agent_ppo2.py:185][0m |          -0.0077 |          36.5805 |          15.6410 |
[32m[20221213 22:56:12 @agent_ppo2.py:185][0m |          -0.0080 |          36.5003 |          15.6318 |
[32m[20221213 22:56:12 @agent_ppo2.py:185][0m |          -0.0012 |          39.3375 |          15.6477 |
[32m[20221213 22:56:12 @agent_ppo2.py:185][0m |          -0.0120 |          36.5517 |          15.6146 |
[32m[20221213 22:56:12 @agent_ppo2.py:185][0m |          -0.0108 |          36.1740 |          15.6272 |
[32m[20221213 22:56:12 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:56:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.47
[32m[20221213 22:56:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.82
[32m[20221213 22:56:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.41
[32m[20221213 22:56:12 @agent_ppo2.py:143][0m Total time:      37.98 min
[32m[20221213 22:56:12 @agent_ppo2.py:145][0m 3721216 total steps have happened
[32m[20221213 22:56:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1817 --------------------------#
[32m[20221213 22:56:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:12 @agent_ppo2.py:185][0m |           0.0112 |          29.3540 |          15.7647 |
[32m[20221213 22:56:12 @agent_ppo2.py:185][0m |          -0.0028 |          26.4340 |          15.7429 |
[32m[20221213 22:56:12 @agent_ppo2.py:185][0m |          -0.0038 |          25.7313 |          15.7575 |
[32m[20221213 22:56:13 @agent_ppo2.py:185][0m |          -0.0051 |          25.3411 |          15.7450 |
[32m[20221213 22:56:13 @agent_ppo2.py:185][0m |          -0.0025 |          25.3748 |          15.7488 |
[32m[20221213 22:56:13 @agent_ppo2.py:185][0m |           0.0004 |          25.3356 |          15.7379 |
[32m[20221213 22:56:13 @agent_ppo2.py:185][0m |          -0.0100 |          24.4299 |          15.7330 |
[32m[20221213 22:56:13 @agent_ppo2.py:185][0m |          -0.0125 |          24.3058 |          15.7160 |
[32m[20221213 22:56:13 @agent_ppo2.py:185][0m |          -0.0122 |          24.1527 |          15.7224 |
[32m[20221213 22:56:13 @agent_ppo2.py:185][0m |          -0.0043 |          24.7302 |          15.7368 |
[32m[20221213 22:56:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:56:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.15
[32m[20221213 22:56:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.12
[32m[20221213 22:56:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.72
[32m[20221213 22:56:13 @agent_ppo2.py:143][0m Total time:      38.00 min
[32m[20221213 22:56:13 @agent_ppo2.py:145][0m 3723264 total steps have happened
[32m[20221213 22:56:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1818 --------------------------#
[32m[20221213 22:56:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |           0.0017 |          30.8720 |          15.8089 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0033 |          27.7107 |          15.8015 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0081 |          26.6427 |          15.7971 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0048 |          25.9267 |          15.7822 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0069 |          25.4698 |          15.7737 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0037 |          25.2361 |          15.7707 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0085 |          25.1259 |          15.7714 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0123 |          24.5576 |          15.7754 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0145 |          24.3458 |          15.7613 |
[32m[20221213 22:56:14 @agent_ppo2.py:185][0m |          -0.0124 |          24.0900 |          15.7485 |
[32m[20221213 22:56:14 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:56:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.83
[32m[20221213 22:56:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 412.31
[32m[20221213 22:56:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.35
[32m[20221213 22:56:14 @agent_ppo2.py:143][0m Total time:      38.03 min
[32m[20221213 22:56:14 @agent_ppo2.py:145][0m 3725312 total steps have happened
[32m[20221213 22:56:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1819 --------------------------#
[32m[20221213 22:56:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |           0.0030 |          43.7651 |          15.6679 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |           0.0003 |          41.1018 |          15.6625 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |           0.0018 |          40.5684 |          15.6729 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |          -0.0104 |          39.7062 |          15.6793 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |          -0.0086 |          39.1594 |          15.6855 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |          -0.0075 |          38.7298 |          15.6909 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |          -0.0091 |          38.4279 |          15.6927 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |          -0.0103 |          38.2025 |          15.6968 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |          -0.0101 |          37.9499 |          15.6980 |
[32m[20221213 22:56:15 @agent_ppo2.py:185][0m |          -0.0050 |          37.7182 |          15.7007 |
[32m[20221213 22:56:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:56:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.36
[32m[20221213 22:56:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.50
[32m[20221213 22:56:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.29
[32m[20221213 22:56:16 @agent_ppo2.py:143][0m Total time:      38.05 min
[32m[20221213 22:56:16 @agent_ppo2.py:145][0m 3727360 total steps have happened
[32m[20221213 22:56:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1820 --------------------------#
[32m[20221213 22:56:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:56:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:16 @agent_ppo2.py:185][0m |           0.0005 |          42.6090 |          15.7316 |
[32m[20221213 22:56:16 @agent_ppo2.py:185][0m |           0.0086 |          41.2251 |          15.7328 |
[32m[20221213 22:56:16 @agent_ppo2.py:185][0m |          -0.0067 |          38.8974 |          15.7189 |
[32m[20221213 22:56:16 @agent_ppo2.py:185][0m |          -0.0084 |          38.3417 |          15.7296 |
[32m[20221213 22:56:16 @agent_ppo2.py:185][0m |          -0.0084 |          37.8968 |          15.7250 |
[32m[20221213 22:56:16 @agent_ppo2.py:185][0m |          -0.0098 |          37.6047 |          15.7186 |
[32m[20221213 22:56:16 @agent_ppo2.py:185][0m |          -0.0084 |          37.4889 |          15.7345 |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |          -0.0094 |          37.2777 |          15.7441 |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |          -0.0073 |          36.9925 |          15.7528 |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |          -0.0045 |          37.8755 |          15.7367 |
[32m[20221213 22:56:17 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.07
[32m[20221213 22:56:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.71
[32m[20221213 22:56:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.35
[32m[20221213 22:56:17 @agent_ppo2.py:143][0m Total time:      38.07 min
[32m[20221213 22:56:17 @agent_ppo2.py:145][0m 3729408 total steps have happened
[32m[20221213 22:56:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1821 --------------------------#
[32m[20221213 22:56:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |           0.0025 |          42.5507 |          15.5877 |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |           0.0013 |          38.9266 |          15.5785 |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |           0.0009 |          40.7791 |          15.5800 |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |          -0.0011 |          39.0717 |          15.5689 |
[32m[20221213 22:56:17 @agent_ppo2.py:185][0m |          -0.0102 |          37.3689 |          15.5544 |
[32m[20221213 22:56:18 @agent_ppo2.py:185][0m |          -0.0110 |          36.9385 |          15.5676 |
[32m[20221213 22:56:18 @agent_ppo2.py:185][0m |          -0.0050 |          36.9572 |          15.5574 |
[32m[20221213 22:56:18 @agent_ppo2.py:185][0m |          -0.0109 |          36.7283 |          15.5305 |
[32m[20221213 22:56:18 @agent_ppo2.py:185][0m |          -0.0096 |          36.5231 |          15.5468 |
[32m[20221213 22:56:18 @agent_ppo2.py:185][0m |          -0.0113 |          36.4916 |          15.5335 |
[32m[20221213 22:56:18 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.17
[32m[20221213 22:56:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.72
[32m[20221213 22:56:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.69
[32m[20221213 22:56:18 @agent_ppo2.py:143][0m Total time:      38.09 min
[32m[20221213 22:56:18 @agent_ppo2.py:145][0m 3731456 total steps have happened
[32m[20221213 22:56:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1822 --------------------------#
[32m[20221213 22:56:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:18 @agent_ppo2.py:185][0m |          -0.0011 |          43.6660 |          15.5008 |
[32m[20221213 22:56:18 @agent_ppo2.py:185][0m |          -0.0063 |          41.9209 |          15.4938 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0007 |          43.9084 |          15.5050 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0117 |          41.3025 |          15.5071 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0128 |          41.0724 |          15.5191 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0137 |          40.7522 |          15.5303 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0118 |          40.6800 |          15.5383 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0123 |          40.4437 |          15.5316 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0129 |          40.4064 |          15.5434 |
[32m[20221213 22:56:19 @agent_ppo2.py:185][0m |          -0.0114 |          40.1626 |          15.5422 |
[32m[20221213 22:56:19 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:56:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.18
[32m[20221213 22:56:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.91
[32m[20221213 22:56:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.69
[32m[20221213 22:56:19 @agent_ppo2.py:143][0m Total time:      38.11 min
[32m[20221213 22:56:19 @agent_ppo2.py:145][0m 3733504 total steps have happened
[32m[20221213 22:56:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1823 --------------------------#
[32m[20221213 22:56:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |           0.0020 |          49.0566 |          15.6627 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |          -0.0014 |          44.6102 |          15.6667 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |           0.0017 |          45.0846 |          15.6697 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |          -0.0101 |          41.9824 |          15.6823 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |          -0.0059 |          41.1049 |          15.6960 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |          -0.0045 |          40.4624 |          15.6745 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |          -0.0084 |          40.1778 |          15.6975 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |           0.0132 |          51.4489 |          15.7112 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |          -0.0057 |          40.0509 |          15.7351 |
[32m[20221213 22:56:20 @agent_ppo2.py:185][0m |          -0.0099 |          39.4631 |          15.7196 |
[32m[20221213 22:56:20 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:56:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.88
[32m[20221213 22:56:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.47
[32m[20221213 22:56:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.59
[32m[20221213 22:56:20 @agent_ppo2.py:143][0m Total time:      38.13 min
[32m[20221213 22:56:20 @agent_ppo2.py:145][0m 3735552 total steps have happened
[32m[20221213 22:56:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1824 --------------------------#
[32m[20221213 22:56:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |           0.0027 |          43.3883 |          15.6324 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0091 |          39.8676 |          15.6365 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0090 |          38.6858 |          15.6463 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0102 |          37.9205 |          15.6502 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0141 |          37.5697 |          15.6434 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0108 |          37.2790 |          15.6388 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0095 |          36.9900 |          15.6447 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0108 |          36.7536 |          15.6532 |
[32m[20221213 22:56:21 @agent_ppo2.py:185][0m |          -0.0060 |          36.7485 |          15.6500 |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |          -0.0120 |          36.6455 |          15.6422 |
[32m[20221213 22:56:22 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.11
[32m[20221213 22:56:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.14
[32m[20221213 22:56:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.52
[32m[20221213 22:56:22 @agent_ppo2.py:143][0m Total time:      38.15 min
[32m[20221213 22:56:22 @agent_ppo2.py:145][0m 3737600 total steps have happened
[32m[20221213 22:56:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1825 --------------------------#
[32m[20221213 22:56:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |           0.0124 |          54.0318 |          15.6732 |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |          -0.0053 |          48.0721 |          15.6318 |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |          -0.0069 |          47.0012 |          15.6223 |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |          -0.0089 |          46.5173 |          15.6164 |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |          -0.0071 |          45.8363 |          15.6055 |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |          -0.0101 |          45.4263 |          15.6089 |
[32m[20221213 22:56:22 @agent_ppo2.py:185][0m |          -0.0101 |          45.1539 |          15.6008 |
[32m[20221213 22:56:23 @agent_ppo2.py:185][0m |          -0.0121 |          44.6628 |          15.5923 |
[32m[20221213 22:56:23 @agent_ppo2.py:185][0m |           0.0052 |          50.3570 |          15.5799 |
[32m[20221213 22:56:23 @agent_ppo2.py:185][0m |          -0.0123 |          44.1084 |          15.5855 |
[32m[20221213 22:56:23 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.28
[32m[20221213 22:56:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.48
[32m[20221213 22:56:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.53
[32m[20221213 22:56:23 @agent_ppo2.py:143][0m Total time:      38.17 min
[32m[20221213 22:56:23 @agent_ppo2.py:145][0m 3739648 total steps have happened
[32m[20221213 22:56:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1826 --------------------------#
[32m[20221213 22:56:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:23 @agent_ppo2.py:185][0m |           0.0009 |          49.3862 |          15.7641 |
[32m[20221213 22:56:23 @agent_ppo2.py:185][0m |          -0.0031 |          47.7792 |          15.7913 |
[32m[20221213 22:56:23 @agent_ppo2.py:185][0m |          -0.0035 |          47.2924 |          15.7850 |
[32m[20221213 22:56:23 @agent_ppo2.py:185][0m |          -0.0040 |          46.9156 |          15.7736 |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |          -0.0059 |          46.6607 |          15.7780 |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |          -0.0053 |          46.5939 |          15.7665 |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |          -0.0056 |          46.4629 |          15.7610 |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |          -0.0062 |          46.2712 |          15.7576 |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |           0.0020 |          49.6702 |          15.7487 |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |          -0.0080 |          46.2676 |          15.7456 |
[32m[20221213 22:56:24 @agent_ppo2.py:130][0m Policy update time: 0.90 s
[32m[20221213 22:56:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.19
[32m[20221213 22:56:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.23
[32m[20221213 22:56:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.00
[32m[20221213 22:56:24 @agent_ppo2.py:143][0m Total time:      38.19 min
[32m[20221213 22:56:24 @agent_ppo2.py:145][0m 3741696 total steps have happened
[32m[20221213 22:56:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1827 --------------------------#
[32m[20221213 22:56:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |          -0.0017 |          41.6338 |          15.6967 |
[32m[20221213 22:56:24 @agent_ppo2.py:185][0m |           0.0014 |          41.7972 |          15.6844 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0070 |          38.0404 |          15.6729 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0092 |          37.1873 |          15.6756 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0113 |          36.8033 |          15.6756 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0027 |          37.5212 |          15.6557 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0111 |          36.2934 |          15.6450 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0150 |          36.0296 |          15.6547 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0093 |          35.8884 |          15.6564 |
[32m[20221213 22:56:25 @agent_ppo2.py:185][0m |          -0.0142 |          35.7221 |          15.6652 |
[32m[20221213 22:56:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:56:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 303.73
[32m[20221213 22:56:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.84
[32m[20221213 22:56:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.40
[32m[20221213 22:56:25 @agent_ppo2.py:143][0m Total time:      38.21 min
[32m[20221213 22:56:25 @agent_ppo2.py:145][0m 3743744 total steps have happened
[32m[20221213 22:56:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1828 --------------------------#
[32m[20221213 22:56:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0007 |          41.0970 |          15.8256 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0031 |          39.6618 |          15.7970 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0048 |          39.2205 |          15.8028 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0104 |          38.9626 |          15.7761 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0092 |          38.8640 |          15.7761 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0099 |          38.6922 |          15.7677 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0126 |          38.4519 |          15.7617 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0018 |          40.1599 |          15.7592 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0086 |          38.3834 |          15.7664 |
[32m[20221213 22:56:26 @agent_ppo2.py:185][0m |          -0.0129 |          38.2110 |          15.7473 |
[32m[20221213 22:56:26 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:56:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.83
[32m[20221213 22:56:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.18
[32m[20221213 22:56:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.03
[32m[20221213 22:56:27 @agent_ppo2.py:143][0m Total time:      38.23 min
[32m[20221213 22:56:27 @agent_ppo2.py:145][0m 3745792 total steps have happened
[32m[20221213 22:56:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1829 --------------------------#
[32m[20221213 22:56:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |          -0.0012 |          43.1669 |          15.5408 |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |          -0.0055 |          41.9005 |          15.5412 |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |          -0.0062 |          41.6137 |          15.5186 |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |          -0.0020 |          41.9929 |          15.5210 |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |           0.0069 |          45.6739 |          15.5154 |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |           0.0121 |          48.5702 |          15.5105 |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |           0.0028 |          44.5902 |          15.4726 |
[32m[20221213 22:56:27 @agent_ppo2.py:185][0m |          -0.0075 |          41.2252 |          15.4991 |
[32m[20221213 22:56:28 @agent_ppo2.py:185][0m |          -0.0081 |          41.0847 |          15.4965 |
[32m[20221213 22:56:28 @agent_ppo2.py:185][0m |          -0.0085 |          41.0244 |          15.5026 |
[32m[20221213 22:56:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:56:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.59
[32m[20221213 22:56:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.99
[32m[20221213 22:56:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.15
[32m[20221213 22:56:28 @agent_ppo2.py:143][0m Total time:      38.25 min
[32m[20221213 22:56:28 @agent_ppo2.py:145][0m 3747840 total steps have happened
[32m[20221213 22:56:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1830 --------------------------#
[32m[20221213 22:56:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:56:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:28 @agent_ppo2.py:185][0m |          -0.0010 |          45.3847 |          15.7045 |
[32m[20221213 22:56:28 @agent_ppo2.py:185][0m |          -0.0084 |          44.8092 |          15.6821 |
[32m[20221213 22:56:28 @agent_ppo2.py:185][0m |          -0.0079 |          44.6288 |          15.6852 |
[32m[20221213 22:56:28 @agent_ppo2.py:185][0m |          -0.0077 |          44.5484 |          15.6952 |
[32m[20221213 22:56:28 @agent_ppo2.py:185][0m |          -0.0088 |          44.4109 |          15.7041 |
[32m[20221213 22:56:29 @agent_ppo2.py:185][0m |          -0.0137 |          44.4376 |          15.7162 |
[32m[20221213 22:56:29 @agent_ppo2.py:185][0m |          -0.0107 |          44.2863 |          15.7107 |
[32m[20221213 22:56:29 @agent_ppo2.py:185][0m |          -0.0125 |          44.3843 |          15.7053 |
[32m[20221213 22:56:29 @agent_ppo2.py:185][0m |          -0.0097 |          44.1112 |          15.7278 |
[32m[20221213 22:56:29 @agent_ppo2.py:185][0m |          -0.0057 |          45.4916 |          15.7343 |
[32m[20221213 22:56:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:56:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.64
[32m[20221213 22:56:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.47
[32m[20221213 22:56:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.20
[32m[20221213 22:56:29 @agent_ppo2.py:143][0m Total time:      38.27 min
[32m[20221213 22:56:29 @agent_ppo2.py:145][0m 3749888 total steps have happened
[32m[20221213 22:56:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1831 --------------------------#
[32m[20221213 22:56:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:29 @agent_ppo2.py:185][0m |           0.0003 |          52.4550 |          15.7647 |
[32m[20221213 22:56:29 @agent_ppo2.py:185][0m |           0.0086 |          54.7756 |          15.7625 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |          -0.0089 |          46.4248 |          15.7572 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |          -0.0017 |          45.9520 |          15.7544 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |          -0.0038 |          44.7355 |          15.7379 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |          -0.0070 |          43.9879 |          15.7434 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |          -0.0125 |          43.9580 |          15.7431 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |          -0.0057 |          46.3455 |          15.7333 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |           0.0018 |          44.2470 |          15.7285 |
[32m[20221213 22:56:30 @agent_ppo2.py:185][0m |          -0.0009 |          43.7307 |          15.7309 |
[32m[20221213 22:56:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:56:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 299.41
[32m[20221213 22:56:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 370.88
[32m[20221213 22:56:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.15
[32m[20221213 22:56:30 @agent_ppo2.py:143][0m Total time:      38.29 min
[32m[20221213 22:56:30 @agent_ppo2.py:145][0m 3751936 total steps have happened
[32m[20221213 22:56:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1832 --------------------------#
[32m[20221213 22:56:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |           0.0262 |          56.7996 |          15.6355 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0073 |          46.0411 |          15.6352 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0086 |          44.9731 |          15.6349 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0111 |          44.4378 |          15.6317 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |           0.0012 |          49.7144 |          15.6316 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0099 |          43.8307 |          15.6259 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0060 |          44.6205 |          15.6321 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0110 |          43.3264 |          15.6565 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0134 |          43.1068 |          15.6530 |
[32m[20221213 22:56:31 @agent_ppo2.py:185][0m |          -0.0122 |          42.9652 |          15.6430 |
[32m[20221213 22:56:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:56:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.24
[32m[20221213 22:56:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.60
[32m[20221213 22:56:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.95
[32m[20221213 22:56:32 @agent_ppo2.py:143][0m Total time:      38.31 min
[32m[20221213 22:56:32 @agent_ppo2.py:145][0m 3753984 total steps have happened
[32m[20221213 22:56:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1833 --------------------------#
[32m[20221213 22:56:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |           0.0006 |          36.0672 |          15.7347 |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |           0.0020 |          35.2032 |          15.7234 |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |           0.0089 |          35.7370 |          15.7209 |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |          -0.0009 |          33.8754 |          15.7166 |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |           0.0087 |          38.9068 |          15.7334 |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |          -0.0062 |          33.7394 |          15.7295 |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |          -0.0049 |          33.3836 |          15.7458 |
[32m[20221213 22:56:32 @agent_ppo2.py:185][0m |          -0.0084 |          33.3900 |          15.7499 |
[32m[20221213 22:56:33 @agent_ppo2.py:185][0m |          -0.0156 |          33.2969 |          15.7322 |
[32m[20221213 22:56:33 @agent_ppo2.py:185][0m |          -0.0032 |          34.1099 |          15.7436 |
[32m[20221213 22:56:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:56:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.80
[32m[20221213 22:56:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.07
[32m[20221213 22:56:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.59
[32m[20221213 22:56:33 @agent_ppo2.py:143][0m Total time:      38.33 min
[32m[20221213 22:56:33 @agent_ppo2.py:145][0m 3756032 total steps have happened
[32m[20221213 22:56:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1834 --------------------------#
[32m[20221213 22:56:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:33 @agent_ppo2.py:185][0m |          -0.0005 |          46.0678 |          15.7496 |
[32m[20221213 22:56:33 @agent_ppo2.py:185][0m |          -0.0079 |          43.7431 |          15.7585 |
[32m[20221213 22:56:33 @agent_ppo2.py:185][0m |          -0.0005 |          44.6255 |          15.7564 |
[32m[20221213 22:56:33 @agent_ppo2.py:185][0m |          -0.0042 |          43.6326 |          15.7561 |
[32m[20221213 22:56:34 @agent_ppo2.py:185][0m |          -0.0066 |          43.0806 |          15.7698 |
[32m[20221213 22:56:34 @agent_ppo2.py:185][0m |          -0.0132 |          42.4571 |          15.7548 |
[32m[20221213 22:56:34 @agent_ppo2.py:185][0m |           0.0030 |          45.5270 |          15.7727 |
[32m[20221213 22:56:34 @agent_ppo2.py:185][0m |          -0.0101 |          42.2429 |          15.7714 |
[32m[20221213 22:56:34 @agent_ppo2.py:185][0m |          -0.0097 |          41.9296 |          15.7798 |
[32m[20221213 22:56:34 @agent_ppo2.py:185][0m |          -0.0065 |          42.0717 |          15.7773 |
[32m[20221213 22:56:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:56:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.19
[32m[20221213 22:56:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.69
[32m[20221213 22:56:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.22
[32m[20221213 22:56:34 @agent_ppo2.py:143][0m Total time:      38.35 min
[32m[20221213 22:56:34 @agent_ppo2.py:145][0m 3758080 total steps have happened
[32m[20221213 22:56:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1835 --------------------------#
[32m[20221213 22:56:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:34 @agent_ppo2.py:185][0m |           0.0034 |          34.6323 |          15.8929 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0070 |          31.5675 |          15.8913 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0128 |          31.0493 |          15.8822 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0105 |          31.0917 |          15.8960 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0086 |          30.3500 |          15.8939 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0134 |          30.0658 |          15.8905 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0123 |          29.8717 |          15.8884 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0140 |          29.7037 |          15.9029 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0100 |          29.9754 |          15.9087 |
[32m[20221213 22:56:35 @agent_ppo2.py:185][0m |          -0.0104 |          29.7063 |          15.9171 |
[32m[20221213 22:56:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:56:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.68
[32m[20221213 22:56:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.09
[32m[20221213 22:56:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.66
[32m[20221213 22:56:35 @agent_ppo2.py:143][0m Total time:      38.37 min
[32m[20221213 22:56:35 @agent_ppo2.py:145][0m 3760128 total steps have happened
[32m[20221213 22:56:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1836 --------------------------#
[32m[20221213 22:56:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |           0.0029 |          49.2263 |          15.7152 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0041 |          48.0180 |          15.6822 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0035 |          47.6919 |          15.6674 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0044 |          47.7167 |          15.6861 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0028 |          47.7727 |          15.6514 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0009 |          47.6035 |          15.6735 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0063 |          47.3762 |          15.6590 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0093 |          47.1029 |          15.6559 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0061 |          47.0257 |          15.6695 |
[32m[20221213 22:56:36 @agent_ppo2.py:185][0m |          -0.0077 |          46.9902 |          15.6455 |
[32m[20221213 22:56:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:56:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.22
[32m[20221213 22:56:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.82
[32m[20221213 22:56:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.40
[32m[20221213 22:56:37 @agent_ppo2.py:143][0m Total time:      38.40 min
[32m[20221213 22:56:37 @agent_ppo2.py:145][0m 3762176 total steps have happened
[32m[20221213 22:56:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1837 --------------------------#
[32m[20221213 22:56:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:37 @agent_ppo2.py:185][0m |          -0.0006 |          54.7518 |          15.6691 |
[32m[20221213 22:56:37 @agent_ppo2.py:185][0m |          -0.0003 |          49.9673 |          15.6493 |
[32m[20221213 22:56:37 @agent_ppo2.py:185][0m |          -0.0040 |          47.9190 |          15.6281 |
[32m[20221213 22:56:37 @agent_ppo2.py:185][0m |          -0.0074 |          46.7265 |          15.6207 |
[32m[20221213 22:56:37 @agent_ppo2.py:185][0m |          -0.0099 |          46.0947 |          15.6180 |
[32m[20221213 22:56:37 @agent_ppo2.py:185][0m |          -0.0097 |          45.5033 |          15.6133 |
[32m[20221213 22:56:37 @agent_ppo2.py:185][0m |          -0.0086 |          45.1884 |          15.5954 |
[32m[20221213 22:56:38 @agent_ppo2.py:185][0m |          -0.0082 |          44.8017 |          15.5939 |
[32m[20221213 22:56:38 @agent_ppo2.py:185][0m |          -0.0022 |          46.5334 |          15.6031 |
[32m[20221213 22:56:38 @agent_ppo2.py:185][0m |           0.0000 |          47.1072 |          15.5886 |
[32m[20221213 22:56:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:56:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.44
[32m[20221213 22:56:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.98
[32m[20221213 22:56:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.83
[32m[20221213 22:56:38 @agent_ppo2.py:143][0m Total time:      38.42 min
[32m[20221213 22:56:38 @agent_ppo2.py:145][0m 3764224 total steps have happened
[32m[20221213 22:56:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1838 --------------------------#
[32m[20221213 22:56:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:38 @agent_ppo2.py:185][0m |           0.0083 |          52.6652 |          15.8540 |
[32m[20221213 22:56:38 @agent_ppo2.py:185][0m |           0.0011 |          43.0268 |          15.8425 |
[32m[20221213 22:56:38 @agent_ppo2.py:185][0m |           0.0059 |          43.3836 |          15.8240 |
[32m[20221213 22:56:38 @agent_ppo2.py:185][0m |          -0.0021 |          40.5669 |          15.8075 |
[32m[20221213 22:56:39 @agent_ppo2.py:185][0m |          -0.0072 |          39.9240 |          15.7996 |
[32m[20221213 22:56:39 @agent_ppo2.py:185][0m |          -0.0060 |          39.6694 |          15.7988 |
[32m[20221213 22:56:39 @agent_ppo2.py:185][0m |          -0.0098 |          39.3388 |          15.7890 |
[32m[20221213 22:56:39 @agent_ppo2.py:185][0m |          -0.0084 |          39.3526 |          15.7730 |
[32m[20221213 22:56:39 @agent_ppo2.py:185][0m |          -0.0105 |          38.7551 |          15.7661 |
[32m[20221213 22:56:39 @agent_ppo2.py:185][0m |          -0.0088 |          38.8848 |          15.7582 |
[32m[20221213 22:56:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:56:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.28
[32m[20221213 22:56:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 340.30
[32m[20221213 22:56:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.87
[32m[20221213 22:56:39 @agent_ppo2.py:143][0m Total time:      38.44 min
[32m[20221213 22:56:39 @agent_ppo2.py:145][0m 3766272 total steps have happened
[32m[20221213 22:56:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1839 --------------------------#
[32m[20221213 22:56:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:56:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:39 @agent_ppo2.py:185][0m |          -0.0000 |          41.8984 |          15.5963 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0057 |          39.0562 |          15.5830 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0059 |          38.2270 |          15.5948 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0083 |          37.7029 |          15.5891 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0070 |          37.1004 |          15.5732 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0126 |          36.8318 |          15.5634 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0084 |          36.4967 |          15.5550 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0003 |          38.0992 |          15.5615 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0092 |          36.1528 |          15.5591 |
[32m[20221213 22:56:40 @agent_ppo2.py:185][0m |          -0.0127 |          35.9595 |          15.5338 |
[32m[20221213 22:56:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:56:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.52
[32m[20221213 22:56:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.66
[32m[20221213 22:56:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.68
[32m[20221213 22:56:40 @agent_ppo2.py:143][0m Total time:      38.46 min
[32m[20221213 22:56:40 @agent_ppo2.py:145][0m 3768320 total steps have happened
[32m[20221213 22:56:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1840 --------------------------#
[32m[20221213 22:56:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:56:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0010 |          53.7803 |          15.7001 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0026 |          52.8501 |          15.6980 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |           0.0078 |          54.8062 |          15.6899 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0069 |          52.1338 |          15.6903 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0107 |          51.8628 |          15.6863 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0089 |          51.6434 |          15.7036 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0102 |          51.3438 |          15.7041 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0005 |          56.5270 |          15.6917 |
[32m[20221213 22:56:41 @agent_ppo2.py:185][0m |          -0.0090 |          51.1494 |          15.6896 |
[32m[20221213 22:56:42 @agent_ppo2.py:185][0m |          -0.0043 |          53.7209 |          15.6925 |
[32m[20221213 22:56:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:56:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.80
[32m[20221213 22:56:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.55
[32m[20221213 22:56:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.17
[32m[20221213 22:56:42 @agent_ppo2.py:143][0m Total time:      38.48 min
[32m[20221213 22:56:42 @agent_ppo2.py:145][0m 3770368 total steps have happened
[32m[20221213 22:56:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1841 --------------------------#
[32m[20221213 22:56:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:42 @agent_ppo2.py:185][0m |           0.0039 |          38.3591 |          15.6754 |
[32m[20221213 22:56:42 @agent_ppo2.py:185][0m |          -0.0064 |          35.1085 |          15.6629 |
[32m[20221213 22:56:42 @agent_ppo2.py:185][0m |          -0.0100 |          34.2655 |          15.6413 |
[32m[20221213 22:56:42 @agent_ppo2.py:185][0m |          -0.0110 |          33.7459 |          15.6440 |
[32m[20221213 22:56:42 @agent_ppo2.py:185][0m |          -0.0107 |          33.4534 |          15.6306 |
[32m[20221213 22:56:42 @agent_ppo2.py:185][0m |          -0.0140 |          33.2012 |          15.6439 |
[32m[20221213 22:56:43 @agent_ppo2.py:185][0m |          -0.0156 |          32.9468 |          15.6173 |
[32m[20221213 22:56:43 @agent_ppo2.py:185][0m |          -0.0134 |          32.6631 |          15.6304 |
[32m[20221213 22:56:43 @agent_ppo2.py:185][0m |          -0.0122 |          32.4852 |          15.6230 |
[32m[20221213 22:56:43 @agent_ppo2.py:185][0m |          -0.0137 |          32.2752 |          15.6211 |
[32m[20221213 22:56:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:56:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.24
[32m[20221213 22:56:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.15
[32m[20221213 22:56:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.02
[32m[20221213 22:56:43 @agent_ppo2.py:143][0m Total time:      38.50 min
[32m[20221213 22:56:43 @agent_ppo2.py:145][0m 3772416 total steps have happened
[32m[20221213 22:56:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1842 --------------------------#
[32m[20221213 22:56:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:56:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:43 @agent_ppo2.py:185][0m |           0.0032 |          37.2345 |          15.6527 |
[32m[20221213 22:56:43 @agent_ppo2.py:185][0m |          -0.0107 |          33.3556 |          15.6136 |
[32m[20221213 22:56:43 @agent_ppo2.py:185][0m |          -0.0084 |          32.2291 |          15.6234 |
[32m[20221213 22:56:44 @agent_ppo2.py:185][0m |          -0.0134 |          31.7416 |          15.6179 |
[32m[20221213 22:56:44 @agent_ppo2.py:185][0m |          -0.0127 |          31.3305 |          15.5969 |
[32m[20221213 22:56:44 @agent_ppo2.py:185][0m |          -0.0147 |          31.0309 |          15.5809 |
[32m[20221213 22:56:44 @agent_ppo2.py:185][0m |          -0.0104 |          30.7055 |          15.5780 |
[32m[20221213 22:56:44 @agent_ppo2.py:185][0m |          -0.0182 |          30.4875 |          15.5660 |
[32m[20221213 22:56:44 @agent_ppo2.py:185][0m |          -0.0210 |          30.3497 |          15.5669 |
[32m[20221213 22:56:44 @agent_ppo2.py:185][0m |          -0.0143 |          30.1509 |          15.5655 |
[32m[20221213 22:56:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:56:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.68
[32m[20221213 22:56:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 386.63
[32m[20221213 22:56:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.25
[32m[20221213 22:56:44 @agent_ppo2.py:143][0m Total time:      38.52 min
[32m[20221213 22:56:44 @agent_ppo2.py:145][0m 3774464 total steps have happened
[32m[20221213 22:56:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1843 --------------------------#
[32m[20221213 22:56:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |           0.0008 |          45.0964 |          15.7257 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0026 |          42.6905 |          15.7282 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0061 |          41.4230 |          15.7223 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0068 |          40.8342 |          15.7238 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0086 |          40.2763 |          15.7204 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0073 |          40.0509 |          15.7183 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0064 |          40.5586 |          15.7162 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0125 |          39.6253 |          15.7166 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0032 |          41.3525 |          15.7145 |
[32m[20221213 22:56:45 @agent_ppo2.py:185][0m |          -0.0135 |          38.9571 |          15.7102 |
[32m[20221213 22:56:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:56:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.70
[32m[20221213 22:56:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 371.76
[32m[20221213 22:56:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.19
[32m[20221213 22:56:45 @agent_ppo2.py:143][0m Total time:      38.54 min
[32m[20221213 22:56:45 @agent_ppo2.py:145][0m 3776512 total steps have happened
[32m[20221213 22:56:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1844 --------------------------#
[32m[20221213 22:56:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0025 |          50.0857 |          15.5508 |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0062 |          47.5402 |          15.5441 |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0062 |          46.3831 |          15.5387 |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0091 |          45.9752 |          15.5454 |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0083 |          45.5287 |          15.5025 |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0105 |          45.2858 |          15.5227 |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0114 |          45.2903 |          15.5492 |
[32m[20221213 22:56:46 @agent_ppo2.py:185][0m |          -0.0136 |          44.7879 |          15.5193 |
[32m[20221213 22:56:47 @agent_ppo2.py:185][0m |          -0.0115 |          44.7889 |          15.5317 |
[32m[20221213 22:56:47 @agent_ppo2.py:185][0m |          -0.0128 |          44.4914 |          15.5406 |
[32m[20221213 22:56:47 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:56:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.52
[32m[20221213 22:56:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.80
[32m[20221213 22:56:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.35
[32m[20221213 22:56:47 @agent_ppo2.py:143][0m Total time:      38.56 min
[32m[20221213 22:56:47 @agent_ppo2.py:145][0m 3778560 total steps have happened
[32m[20221213 22:56:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1845 --------------------------#
[32m[20221213 22:56:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:47 @agent_ppo2.py:185][0m |          -0.0030 |          43.9767 |          15.7144 |
[32m[20221213 22:56:47 @agent_ppo2.py:185][0m |          -0.0042 |          43.1354 |          15.6978 |
[32m[20221213 22:56:47 @agent_ppo2.py:185][0m |           0.0016 |          43.8997 |          15.6870 |
[32m[20221213 22:56:47 @agent_ppo2.py:185][0m |          -0.0103 |          42.4521 |          15.6511 |
[32m[20221213 22:56:47 @agent_ppo2.py:185][0m |          -0.0059 |          42.3775 |          15.6627 |
[32m[20221213 22:56:48 @agent_ppo2.py:185][0m |          -0.0070 |          42.2485 |          15.6432 |
[32m[20221213 22:56:48 @agent_ppo2.py:185][0m |          -0.0151 |          42.0190 |          15.6462 |
[32m[20221213 22:56:48 @agent_ppo2.py:185][0m |          -0.0094 |          41.8793 |          15.6279 |
[32m[20221213 22:56:48 @agent_ppo2.py:185][0m |          -0.0121 |          41.7549 |          15.6248 |
[32m[20221213 22:56:48 @agent_ppo2.py:185][0m |          -0.0113 |          41.9242 |          15.6083 |
[32m[20221213 22:56:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:56:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.10
[32m[20221213 22:56:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.08
[32m[20221213 22:56:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.92
[32m[20221213 22:56:48 @agent_ppo2.py:143][0m Total time:      38.58 min
[32m[20221213 22:56:48 @agent_ppo2.py:145][0m 3780608 total steps have happened
[32m[20221213 22:56:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1846 --------------------------#
[32m[20221213 22:56:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:48 @agent_ppo2.py:185][0m |          -0.0005 |          47.7383 |          15.6636 |
[32m[20221213 22:56:48 @agent_ppo2.py:185][0m |           0.0053 |          49.2568 |          15.6602 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |          -0.0036 |          46.2501 |          15.6471 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |          -0.0085 |          45.7770 |          15.6371 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |          -0.0050 |          45.7668 |          15.6485 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |           0.0025 |          50.0710 |          15.6405 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |          -0.0066 |          45.3469 |          15.6128 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |           0.0000 |          48.1802 |          15.6428 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |          -0.0118 |          45.0625 |          15.6305 |
[32m[20221213 22:56:49 @agent_ppo2.py:185][0m |          -0.0096 |          44.9134 |          15.6354 |
[32m[20221213 22:56:49 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:56:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.04
[32m[20221213 22:56:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.66
[32m[20221213 22:56:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.96
[32m[20221213 22:56:49 @agent_ppo2.py:143][0m Total time:      38.61 min
[32m[20221213 22:56:49 @agent_ppo2.py:145][0m 3782656 total steps have happened
[32m[20221213 22:56:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1847 --------------------------#
[32m[20221213 22:56:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |           0.0027 |          51.1837 |          15.6267 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0025 |          49.1096 |          15.5980 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0040 |          48.3686 |          15.6015 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0060 |          48.0347 |          15.6002 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0017 |          48.3137 |          15.5950 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0051 |          47.4846 |          15.5873 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0107 |          47.5369 |          15.5963 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0091 |          47.2585 |          15.5978 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0100 |          47.1319 |          15.5977 |
[32m[20221213 22:56:50 @agent_ppo2.py:185][0m |          -0.0091 |          47.0469 |          15.6087 |
[32m[20221213 22:56:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:56:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.47
[32m[20221213 22:56:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 414.03
[32m[20221213 22:56:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.98
[32m[20221213 22:56:51 @agent_ppo2.py:143][0m Total time:      38.63 min
[32m[20221213 22:56:51 @agent_ppo2.py:145][0m 3784704 total steps have happened
[32m[20221213 22:56:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1848 --------------------------#
[32m[20221213 22:56:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |           0.0082 |          37.4714 |          15.8039 |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |          -0.0087 |          32.7533 |          15.7752 |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |          -0.0109 |          31.6918 |          15.7692 |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |          -0.0101 |          31.0262 |          15.7592 |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |          -0.0120 |          30.7256 |          15.7611 |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |          -0.0103 |          30.7270 |          15.7579 |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |          -0.0155 |          29.9365 |          15.7399 |
[32m[20221213 22:56:51 @agent_ppo2.py:185][0m |          -0.0053 |          29.7778 |          15.7263 |
[32m[20221213 22:56:52 @agent_ppo2.py:185][0m |          -0.0066 |          30.9873 |          15.7372 |
[32m[20221213 22:56:52 @agent_ppo2.py:185][0m |          -0.0171 |          29.2103 |          15.7339 |
[32m[20221213 22:56:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:56:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.70
[32m[20221213 22:56:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.11
[32m[20221213 22:56:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.86
[32m[20221213 22:56:52 @agent_ppo2.py:143][0m Total time:      38.65 min
[32m[20221213 22:56:52 @agent_ppo2.py:145][0m 3786752 total steps have happened
[32m[20221213 22:56:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1849 --------------------------#
[32m[20221213 22:56:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:56:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:52 @agent_ppo2.py:185][0m |          -0.0057 |          34.4529 |          15.8336 |
[32m[20221213 22:56:52 @agent_ppo2.py:185][0m |          -0.0083 |          30.6773 |          15.8425 |
[32m[20221213 22:56:52 @agent_ppo2.py:185][0m |          -0.0100 |          29.5809 |          15.8423 |
[32m[20221213 22:56:52 @agent_ppo2.py:185][0m |          -0.0178 |          28.9756 |          15.8406 |
[32m[20221213 22:56:52 @agent_ppo2.py:185][0m |          -0.0081 |          28.3090 |          15.8576 |
[32m[20221213 22:56:53 @agent_ppo2.py:185][0m |          -0.0118 |          27.8873 |          15.8431 |
[32m[20221213 22:56:53 @agent_ppo2.py:185][0m |          -0.0147 |          27.7451 |          15.8488 |
[32m[20221213 22:56:53 @agent_ppo2.py:185][0m |          -0.0161 |          27.3810 |          15.8551 |
[32m[20221213 22:56:53 @agent_ppo2.py:185][0m |          -0.0162 |          27.4181 |          15.8579 |
[32m[20221213 22:56:53 @agent_ppo2.py:185][0m |          -0.0174 |          26.9953 |          15.8617 |
[32m[20221213 22:56:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:56:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.90
[32m[20221213 22:56:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 402.81
[32m[20221213 22:56:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.51
[32m[20221213 22:56:53 @agent_ppo2.py:143][0m Total time:      38.67 min
[32m[20221213 22:56:53 @agent_ppo2.py:145][0m 3788800 total steps have happened
[32m[20221213 22:56:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1850 --------------------------#
[32m[20221213 22:56:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:56:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:53 @agent_ppo2.py:185][0m |           0.0026 |          40.1314 |          15.7174 |
[32m[20221213 22:56:53 @agent_ppo2.py:185][0m |          -0.0026 |          34.5852 |          15.7178 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |          -0.0085 |          33.0889 |          15.7208 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |          -0.0065 |          32.5087 |          15.7257 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |           0.0025 |          34.8593 |          15.7297 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |           0.0017 |          32.4175 |          15.7028 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |          -0.0082 |          31.1468 |          15.7159 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |          -0.0158 |          30.7801 |          15.7266 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |          -0.0047 |          33.6652 |          15.7314 |
[32m[20221213 22:56:54 @agent_ppo2.py:185][0m |          -0.0091 |          30.3070 |          15.7241 |
[32m[20221213 22:56:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:56:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.02
[32m[20221213 22:56:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 326.36
[32m[20221213 22:56:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.09
[32m[20221213 22:56:54 @agent_ppo2.py:143][0m Total time:      38.69 min
[32m[20221213 22:56:54 @agent_ppo2.py:145][0m 3790848 total steps have happened
[32m[20221213 22:56:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1851 --------------------------#
[32m[20221213 22:56:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:56:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |           0.0068 |          36.2586 |          15.7274 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0039 |          32.9849 |          15.7166 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0076 |          31.8409 |          15.7310 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |           0.0026 |          31.3355 |          15.7195 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0104 |          30.3751 |          15.7163 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0058 |          30.0168 |          15.7118 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0103 |          29.6713 |          15.7004 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0072 |          29.7266 |          15.7224 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0134 |          29.0391 |          15.7088 |
[32m[20221213 22:56:55 @agent_ppo2.py:185][0m |          -0.0140 |          28.8321 |          15.7115 |
[32m[20221213 22:56:55 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:56:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.69
[32m[20221213 22:56:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.91
[32m[20221213 22:56:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.30
[32m[20221213 22:56:56 @agent_ppo2.py:143][0m Total time:      38.71 min
[32m[20221213 22:56:56 @agent_ppo2.py:145][0m 3792896 total steps have happened
[32m[20221213 22:56:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1852 --------------------------#
[32m[20221213 22:56:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:56 @agent_ppo2.py:185][0m |           0.0008 |          45.0123 |          15.7065 |
[32m[20221213 22:56:56 @agent_ppo2.py:185][0m |          -0.0040 |          42.1988 |          15.6703 |
[32m[20221213 22:56:56 @agent_ppo2.py:185][0m |           0.0017 |          41.9526 |          15.6738 |
[32m[20221213 22:56:56 @agent_ppo2.py:185][0m |          -0.0060 |          41.4654 |          15.6364 |
[32m[20221213 22:56:56 @agent_ppo2.py:185][0m |          -0.0103 |          41.1126 |          15.6569 |
[32m[20221213 22:56:56 @agent_ppo2.py:185][0m |          -0.0057 |          41.0123 |          15.6571 |
[32m[20221213 22:56:56 @agent_ppo2.py:185][0m |          -0.0058 |          40.7473 |          15.6624 |
[32m[20221213 22:56:57 @agent_ppo2.py:185][0m |          -0.0139 |          40.6079 |          15.6563 |
[32m[20221213 22:56:57 @agent_ppo2.py:185][0m |          -0.0107 |          40.5380 |          15.6480 |
[32m[20221213 22:56:57 @agent_ppo2.py:185][0m |          -0.0106 |          40.3819 |          15.6460 |
[32m[20221213 22:56:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:56:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.84
[32m[20221213 22:56:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.80
[32m[20221213 22:56:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.99
[32m[20221213 22:56:57 @agent_ppo2.py:143][0m Total time:      38.73 min
[32m[20221213 22:56:57 @agent_ppo2.py:145][0m 3794944 total steps have happened
[32m[20221213 22:56:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1853 --------------------------#
[32m[20221213 22:56:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:57 @agent_ppo2.py:185][0m |          -0.0003 |          33.3225 |          15.7040 |
[32m[20221213 22:56:57 @agent_ppo2.py:185][0m |          -0.0075 |          30.6967 |          15.7016 |
[32m[20221213 22:56:57 @agent_ppo2.py:185][0m |          -0.0061 |          29.6426 |          15.7017 |
[32m[20221213 22:56:57 @agent_ppo2.py:185][0m |           0.0062 |          34.3937 |          15.6903 |
[32m[20221213 22:56:58 @agent_ppo2.py:185][0m |          -0.0029 |          30.5015 |          15.6748 |
[32m[20221213 22:56:58 @agent_ppo2.py:185][0m |          -0.0114 |          28.0600 |          15.6793 |
[32m[20221213 22:56:58 @agent_ppo2.py:185][0m |          -0.0035 |          28.2444 |          15.6880 |
[32m[20221213 22:56:58 @agent_ppo2.py:185][0m |          -0.0140 |          27.5588 |          15.6811 |
[32m[20221213 22:56:58 @agent_ppo2.py:185][0m |          -0.0139 |          27.2117 |          15.6960 |
[32m[20221213 22:56:58 @agent_ppo2.py:185][0m |           0.0008 |          32.4113 |          15.6699 |
[32m[20221213 22:56:58 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:56:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.54
[32m[20221213 22:56:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.11
[32m[20221213 22:56:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.60
[32m[20221213 22:56:58 @agent_ppo2.py:143][0m Total time:      38.75 min
[32m[20221213 22:56:58 @agent_ppo2.py:145][0m 3796992 total steps have happened
[32m[20221213 22:56:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1854 --------------------------#
[32m[20221213 22:56:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:56:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0002 |          35.3558 |          15.6734 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0058 |          31.2116 |          15.6979 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0053 |          30.2816 |          15.7001 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0087 |          29.4464 |          15.7081 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0094 |          28.9248 |          15.7203 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0074 |          28.4871 |          15.7245 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0130 |          28.1964 |          15.7145 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0124 |          27.9556 |          15.7113 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0107 |          27.7235 |          15.7235 |
[32m[20221213 22:56:59 @agent_ppo2.py:185][0m |          -0.0137 |          27.4515 |          15.7290 |
[32m[20221213 22:56:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:56:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.42
[32m[20221213 22:56:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.35
[32m[20221213 22:56:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.99
[32m[20221213 22:56:59 @agent_ppo2.py:143][0m Total time:      38.78 min
[32m[20221213 22:56:59 @agent_ppo2.py:145][0m 3799040 total steps have happened
[32m[20221213 22:56:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1855 --------------------------#
[32m[20221213 22:57:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |           0.0014 |          46.8867 |          15.6916 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |           0.0045 |          49.0004 |          15.6601 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |          -0.0051 |          43.3942 |          15.6424 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |          -0.0043 |          43.0068 |          15.6510 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |          -0.0077 |          42.6575 |          15.6465 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |          -0.0106 |          42.2970 |          15.6263 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |          -0.0141 |          42.4186 |          15.6269 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |          -0.0095 |          42.0594 |          15.6195 |
[32m[20221213 22:57:00 @agent_ppo2.py:185][0m |          -0.0056 |          42.7752 |          15.6027 |
[32m[20221213 22:57:01 @agent_ppo2.py:185][0m |          -0.0149 |          41.8253 |          15.5963 |
[32m[20221213 22:57:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:57:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.50
[32m[20221213 22:57:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.69
[32m[20221213 22:57:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.09
[32m[20221213 22:57:01 @agent_ppo2.py:143][0m Total time:      38.80 min
[32m[20221213 22:57:01 @agent_ppo2.py:145][0m 3801088 total steps have happened
[32m[20221213 22:57:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1856 --------------------------#
[32m[20221213 22:57:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:01 @agent_ppo2.py:185][0m |          -0.0003 |          32.1458 |          15.6117 |
[32m[20221213 22:57:01 @agent_ppo2.py:185][0m |          -0.0030 |          29.0290 |          15.6081 |
[32m[20221213 22:57:01 @agent_ppo2.py:185][0m |          -0.0118 |          28.0142 |          15.6027 |
[32m[20221213 22:57:01 @agent_ppo2.py:185][0m |          -0.0080 |          27.2882 |          15.5997 |
[32m[20221213 22:57:01 @agent_ppo2.py:185][0m |          -0.0157 |          26.8596 |          15.6115 |
[32m[20221213 22:57:01 @agent_ppo2.py:185][0m |          -0.0112 |          26.5611 |          15.5833 |
[32m[20221213 22:57:02 @agent_ppo2.py:185][0m |          -0.0127 |          26.2913 |          15.5866 |
[32m[20221213 22:57:02 @agent_ppo2.py:185][0m |          -0.0177 |          26.1698 |          15.6137 |
[32m[20221213 22:57:02 @agent_ppo2.py:185][0m |          -0.0159 |          25.8303 |          15.5706 |
[32m[20221213 22:57:02 @agent_ppo2.py:185][0m |          -0.0123 |          25.6361 |          15.5998 |
[32m[20221213 22:57:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:57:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.50
[32m[20221213 22:57:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 360.56
[32m[20221213 22:57:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.88
[32m[20221213 22:57:02 @agent_ppo2.py:143][0m Total time:      38.82 min
[32m[20221213 22:57:02 @agent_ppo2.py:145][0m 3803136 total steps have happened
[32m[20221213 22:57:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1857 --------------------------#
[32m[20221213 22:57:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:02 @agent_ppo2.py:185][0m |           0.0036 |          45.7572 |          15.4882 |
[32m[20221213 22:57:02 @agent_ppo2.py:185][0m |           0.0007 |          42.5234 |          15.4968 |
[32m[20221213 22:57:02 @agent_ppo2.py:185][0m |          -0.0098 |          40.7824 |          15.5031 |
[32m[20221213 22:57:03 @agent_ppo2.py:185][0m |          -0.0054 |          40.4415 |          15.5083 |
[32m[20221213 22:57:03 @agent_ppo2.py:185][0m |          -0.0101 |          39.6569 |          15.5220 |
[32m[20221213 22:57:03 @agent_ppo2.py:185][0m |          -0.0084 |          39.6428 |          15.5255 |
[32m[20221213 22:57:03 @agent_ppo2.py:185][0m |          -0.0070 |          40.2592 |          15.5298 |
[32m[20221213 22:57:03 @agent_ppo2.py:185][0m |          -0.0149 |          38.8299 |          15.5101 |
[32m[20221213 22:57:03 @agent_ppo2.py:185][0m |          -0.0107 |          39.5535 |          15.5373 |
[32m[20221213 22:57:03 @agent_ppo2.py:185][0m |          -0.0167 |          38.3278 |          15.5144 |
[32m[20221213 22:57:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:57:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.40
[32m[20221213 22:57:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.93
[32m[20221213 22:57:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 382.74
[32m[20221213 22:57:03 @agent_ppo2.py:143][0m Total time:      38.84 min
[32m[20221213 22:57:03 @agent_ppo2.py:145][0m 3805184 total steps have happened
[32m[20221213 22:57:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1858 --------------------------#
[32m[20221213 22:57:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0004 |          46.3360 |          15.4723 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0044 |          43.0079 |          15.4687 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0044 |          42.3236 |          15.4740 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0090 |          41.2400 |          15.4497 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0088 |          40.6666 |          15.4549 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0094 |          40.2290 |          15.4680 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0107 |          39.8302 |          15.4570 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0093 |          39.4799 |          15.4498 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0157 |          39.2233 |          15.4323 |
[32m[20221213 22:57:04 @agent_ppo2.py:185][0m |          -0.0116 |          39.0472 |          15.4369 |
[32m[20221213 22:57:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.60
[32m[20221213 22:57:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 359.01
[32m[20221213 22:57:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.32
[32m[20221213 22:57:05 @agent_ppo2.py:143][0m Total time:      38.86 min
[32m[20221213 22:57:05 @agent_ppo2.py:145][0m 3807232 total steps have happened
[32m[20221213 22:57:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1859 --------------------------#
[32m[20221213 22:57:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0057 |          48.3551 |          15.5414 |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0103 |          45.3534 |          15.5215 |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0102 |          44.1496 |          15.5306 |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0092 |          43.5437 |          15.5188 |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0122 |          42.7532 |          15.5249 |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0146 |          42.4681 |          15.5314 |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0113 |          42.0046 |          15.5319 |
[32m[20221213 22:57:05 @agent_ppo2.py:185][0m |          -0.0160 |          41.9905 |          15.5131 |
[32m[20221213 22:57:06 @agent_ppo2.py:185][0m |          -0.0161 |          41.6365 |          15.5081 |
[32m[20221213 22:57:06 @agent_ppo2.py:185][0m |          -0.0163 |          41.4609 |          15.5263 |
[32m[20221213 22:57:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:57:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.83
[32m[20221213 22:57:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.51
[32m[20221213 22:57:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.27
[32m[20221213 22:57:06 @agent_ppo2.py:143][0m Total time:      38.88 min
[32m[20221213 22:57:06 @agent_ppo2.py:145][0m 3809280 total steps have happened
[32m[20221213 22:57:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1860 --------------------------#
[32m[20221213 22:57:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:57:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:06 @agent_ppo2.py:185][0m |          -0.0035 |          48.2658 |          15.5690 |
[32m[20221213 22:57:06 @agent_ppo2.py:185][0m |           0.0026 |          50.0962 |          15.5764 |
[32m[20221213 22:57:06 @agent_ppo2.py:185][0m |          -0.0084 |          46.9736 |          15.5929 |
[32m[20221213 22:57:06 @agent_ppo2.py:185][0m |          -0.0096 |          46.2822 |          15.5811 |
[32m[20221213 22:57:06 @agent_ppo2.py:185][0m |          -0.0103 |          46.1894 |          15.6014 |
[32m[20221213 22:57:07 @agent_ppo2.py:185][0m |          -0.0124 |          46.0946 |          15.5750 |
[32m[20221213 22:57:07 @agent_ppo2.py:185][0m |          -0.0132 |          45.8846 |          15.5888 |
[32m[20221213 22:57:07 @agent_ppo2.py:185][0m |          -0.0137 |          45.7325 |          15.5826 |
[32m[20221213 22:57:07 @agent_ppo2.py:185][0m |          -0.0153 |          45.6556 |          15.5836 |
[32m[20221213 22:57:07 @agent_ppo2.py:185][0m |          -0.0128 |          45.7559 |          15.5831 |
[32m[20221213 22:57:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:57:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.86
[32m[20221213 22:57:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.20
[32m[20221213 22:57:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.98
[32m[20221213 22:57:07 @agent_ppo2.py:143][0m Total time:      38.90 min
[32m[20221213 22:57:07 @agent_ppo2.py:145][0m 3811328 total steps have happened
[32m[20221213 22:57:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1861 --------------------------#
[32m[20221213 22:57:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:57:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:07 @agent_ppo2.py:185][0m |          -0.0023 |          51.3139 |          15.7113 |
[32m[20221213 22:57:07 @agent_ppo2.py:185][0m |          -0.0071 |          49.4221 |          15.6965 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0088 |          48.9705 |          15.6921 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0114 |          48.4944 |          15.6749 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0093 |          48.2355 |          15.6528 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0122 |          48.0223 |          15.6362 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0036 |          52.8619 |          15.6251 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0090 |          48.2759 |          15.5841 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0105 |          47.5267 |          15.5973 |
[32m[20221213 22:57:08 @agent_ppo2.py:185][0m |          -0.0135 |          47.4113 |          15.6015 |
[32m[20221213 22:57:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:57:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.35
[32m[20221213 22:57:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.10
[32m[20221213 22:57:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.05
[32m[20221213 22:57:08 @agent_ppo2.py:143][0m Total time:      38.92 min
[32m[20221213 22:57:08 @agent_ppo2.py:145][0m 3813376 total steps have happened
[32m[20221213 22:57:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1862 --------------------------#
[32m[20221213 22:57:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0001 |          52.4945 |          15.5254 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0037 |          49.7170 |          15.5154 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0071 |          49.0023 |          15.5231 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0079 |          48.4295 |          15.5121 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0063 |          48.1785 |          15.5312 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0100 |          47.8482 |          15.5268 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0126 |          47.5353 |          15.5216 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0102 |          47.2731 |          15.5244 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0148 |          47.1573 |          15.5344 |
[32m[20221213 22:57:09 @agent_ppo2.py:185][0m |          -0.0143 |          46.8550 |          15.5448 |
[32m[20221213 22:57:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:57:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.09
[32m[20221213 22:57:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.11
[32m[20221213 22:57:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.76
[32m[20221213 22:57:10 @agent_ppo2.py:143][0m Total time:      38.94 min
[32m[20221213 22:57:10 @agent_ppo2.py:145][0m 3815424 total steps have happened
[32m[20221213 22:57:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1863 --------------------------#
[32m[20221213 22:57:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0004 |          36.3472 |          15.4778 |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0047 |          32.6679 |          15.4833 |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0049 |          31.3592 |          15.4626 |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0017 |          30.5183 |          15.4563 |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0075 |          29.8017 |          15.4740 |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0036 |          29.1342 |          15.4539 |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0110 |          28.6345 |          15.4582 |
[32m[20221213 22:57:10 @agent_ppo2.py:185][0m |          -0.0126 |          28.2924 |          15.4657 |
[32m[20221213 22:57:11 @agent_ppo2.py:185][0m |          -0.0037 |          28.0308 |          15.4515 |
[32m[20221213 22:57:11 @agent_ppo2.py:185][0m |          -0.0059 |          27.6792 |          15.4636 |
[32m[20221213 22:57:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:57:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.80
[32m[20221213 22:57:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 374.74
[32m[20221213 22:57:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.79
[32m[20221213 22:57:11 @agent_ppo2.py:143][0m Total time:      38.97 min
[32m[20221213 22:57:11 @agent_ppo2.py:145][0m 3817472 total steps have happened
[32m[20221213 22:57:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1864 --------------------------#
[32m[20221213 22:57:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:11 @agent_ppo2.py:185][0m |          -0.0006 |          40.8966 |          15.6961 |
[32m[20221213 22:57:11 @agent_ppo2.py:185][0m |          -0.0014 |          39.7063 |          15.6929 |
[32m[20221213 22:57:11 @agent_ppo2.py:185][0m |          -0.0075 |          38.4679 |          15.6824 |
[32m[20221213 22:57:11 @agent_ppo2.py:185][0m |          -0.0119 |          38.0706 |          15.6718 |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |          -0.0083 |          38.8004 |          15.6628 |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |          -0.0074 |          37.6188 |          15.6437 |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |          -0.0130 |          37.3754 |          15.6703 |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |          -0.0103 |          37.5770 |          15.6714 |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |          -0.0145 |          37.2559 |          15.6508 |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |          -0.0101 |          37.0587 |          15.6736 |
[32m[20221213 22:57:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:57:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.92
[32m[20221213 22:57:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.04
[32m[20221213 22:57:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.44
[32m[20221213 22:57:12 @agent_ppo2.py:143][0m Total time:      38.99 min
[32m[20221213 22:57:12 @agent_ppo2.py:145][0m 3819520 total steps have happened
[32m[20221213 22:57:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1865 --------------------------#
[32m[20221213 22:57:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |           0.0025 |          40.2545 |          15.6583 |
[32m[20221213 22:57:12 @agent_ppo2.py:185][0m |          -0.0035 |          36.4750 |          15.6600 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |          -0.0111 |          35.7063 |          15.6388 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |           0.0140 |          39.1425 |          15.6369 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |          -0.0119 |          34.3348 |          15.6391 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |           0.0027 |          40.6187 |          15.6484 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |          -0.0109 |          33.6977 |          15.6447 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |          -0.0130 |          33.2853 |          15.6423 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |          -0.0090 |          33.0143 |          15.6563 |
[32m[20221213 22:57:13 @agent_ppo2.py:185][0m |          -0.0122 |          32.7547 |          15.6611 |
[32m[20221213 22:57:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:57:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.33
[32m[20221213 22:57:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.67
[32m[20221213 22:57:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.75
[32m[20221213 22:57:13 @agent_ppo2.py:143][0m Total time:      39.01 min
[32m[20221213 22:57:13 @agent_ppo2.py:145][0m 3821568 total steps have happened
[32m[20221213 22:57:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1866 --------------------------#
[32m[20221213 22:57:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:57:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0019 |          46.9905 |          15.6707 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0028 |          45.4050 |          15.6624 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0028 |          45.7076 |          15.6759 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0068 |          44.7761 |          15.6875 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0101 |          44.6044 |          15.6621 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0076 |          44.5657 |          15.6840 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0059 |          44.3558 |          15.6825 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0121 |          44.2894 |          15.6721 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0092 |          44.1162 |          15.6899 |
[32m[20221213 22:57:14 @agent_ppo2.py:185][0m |          -0.0075 |          44.1188 |          15.6808 |
[32m[20221213 22:57:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:57:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.76
[32m[20221213 22:57:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.46
[32m[20221213 22:57:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.59
[32m[20221213 22:57:15 @agent_ppo2.py:143][0m Total time:      39.03 min
[32m[20221213 22:57:15 @agent_ppo2.py:145][0m 3823616 total steps have happened
[32m[20221213 22:57:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1867 --------------------------#
[32m[20221213 22:57:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:15 @agent_ppo2.py:185][0m |           0.0056 |          52.5549 |          15.7871 |
[32m[20221213 22:57:15 @agent_ppo2.py:185][0m |          -0.0049 |          47.5827 |          15.7600 |
[32m[20221213 22:57:15 @agent_ppo2.py:185][0m |          -0.0065 |          46.5214 |          15.7631 |
[32m[20221213 22:57:15 @agent_ppo2.py:185][0m |          -0.0090 |          45.7316 |          15.7651 |
[32m[20221213 22:57:15 @agent_ppo2.py:185][0m |          -0.0086 |          45.3650 |          15.7561 |
[32m[20221213 22:57:15 @agent_ppo2.py:185][0m |          -0.0026 |          44.9221 |          15.7544 |
[32m[20221213 22:57:15 @agent_ppo2.py:185][0m |          -0.0144 |          43.9672 |          15.7607 |
[32m[20221213 22:57:16 @agent_ppo2.py:185][0m |          -0.0120 |          43.6039 |          15.7602 |
[32m[20221213 22:57:16 @agent_ppo2.py:185][0m |          -0.0112 |          43.2797 |          15.7608 |
[32m[20221213 22:57:16 @agent_ppo2.py:185][0m |          -0.0127 |          43.0518 |          15.7573 |
[32m[20221213 22:57:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:57:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.15
[32m[20221213 22:57:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.34
[32m[20221213 22:57:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.02
[32m[20221213 22:57:16 @agent_ppo2.py:143][0m Total time:      39.05 min
[32m[20221213 22:57:16 @agent_ppo2.py:145][0m 3825664 total steps have happened
[32m[20221213 22:57:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1868 --------------------------#
[32m[20221213 22:57:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:16 @agent_ppo2.py:185][0m |           0.0051 |          31.9748 |          15.6734 |
[32m[20221213 22:57:16 @agent_ppo2.py:185][0m |          -0.0062 |          28.5666 |          15.6609 |
[32m[20221213 22:57:16 @agent_ppo2.py:185][0m |          -0.0084 |          27.6228 |          15.6431 |
[32m[20221213 22:57:16 @agent_ppo2.py:185][0m |          -0.0127 |          26.9520 |          15.6374 |
[32m[20221213 22:57:17 @agent_ppo2.py:185][0m |          -0.0128 |          26.8381 |          15.6287 |
[32m[20221213 22:57:17 @agent_ppo2.py:185][0m |          -0.0090 |          26.7764 |          15.6283 |
[32m[20221213 22:57:17 @agent_ppo2.py:185][0m |          -0.0078 |          26.8349 |          15.6156 |
[32m[20221213 22:57:17 @agent_ppo2.py:185][0m |          -0.0119 |          25.9276 |          15.6141 |
[32m[20221213 22:57:17 @agent_ppo2.py:185][0m |          -0.0158 |          25.8533 |          15.6356 |
[32m[20221213 22:57:17 @agent_ppo2.py:185][0m |          -0.0135 |          25.7056 |          15.6057 |
[32m[20221213 22:57:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:57:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.90
[32m[20221213 22:57:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.30
[32m[20221213 22:57:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.16
[32m[20221213 22:57:17 @agent_ppo2.py:143][0m Total time:      39.07 min
[32m[20221213 22:57:17 @agent_ppo2.py:145][0m 3827712 total steps have happened
[32m[20221213 22:57:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1869 --------------------------#
[32m[20221213 22:57:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:17 @agent_ppo2.py:185][0m |           0.0043 |          37.0235 |          15.6769 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0055 |          31.4025 |          15.6577 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0054 |          30.2153 |          15.6478 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0065 |          29.5581 |          15.6527 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0047 |          28.9448 |          15.6636 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0093 |          28.6800 |          15.6606 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0104 |          28.4478 |          15.6567 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0095 |          28.0352 |          15.6438 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0079 |          28.9740 |          15.6720 |
[32m[20221213 22:57:18 @agent_ppo2.py:185][0m |          -0.0051 |          30.7145 |          15.6666 |
[32m[20221213 22:57:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:57:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.75
[32m[20221213 22:57:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.60
[32m[20221213 22:57:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 422.28
[32m[20221213 22:57:18 @agent_ppo2.py:143][0m Total time:      39.09 min
[32m[20221213 22:57:18 @agent_ppo2.py:145][0m 3829760 total steps have happened
[32m[20221213 22:57:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1870 --------------------------#
[32m[20221213 22:57:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:57:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:19 @agent_ppo2.py:185][0m |           0.0024 |          42.7500 |          15.6340 |
[32m[20221213 22:57:19 @agent_ppo2.py:185][0m |          -0.0062 |          41.6444 |          15.6341 |
[32m[20221213 22:57:19 @agent_ppo2.py:185][0m |          -0.0045 |          41.1533 |          15.6259 |
[32m[20221213 22:57:19 @agent_ppo2.py:185][0m |          -0.0063 |          40.4427 |          15.6187 |
[32m[20221213 22:57:19 @agent_ppo2.py:185][0m |          -0.0078 |          40.5005 |          15.6094 |
[32m[20221213 22:57:19 @agent_ppo2.py:185][0m |          -0.0078 |          40.1938 |          15.6066 |
[32m[20221213 22:57:19 @agent_ppo2.py:185][0m |          -0.0097 |          40.0377 |          15.6100 |
[32m[20221213 22:57:20 @agent_ppo2.py:185][0m |          -0.0125 |          39.9593 |          15.5965 |
[32m[20221213 22:57:20 @agent_ppo2.py:185][0m |          -0.0003 |          43.4613 |          15.5930 |
[32m[20221213 22:57:20 @agent_ppo2.py:185][0m |          -0.0103 |          39.9307 |          15.5796 |
[32m[20221213 22:57:20 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 22:57:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.86
[32m[20221213 22:57:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.80
[32m[20221213 22:57:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.03
[32m[20221213 22:57:20 @agent_ppo2.py:143][0m Total time:      39.12 min
[32m[20221213 22:57:20 @agent_ppo2.py:145][0m 3831808 total steps have happened
[32m[20221213 22:57:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1871 --------------------------#
[32m[20221213 22:57:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:20 @agent_ppo2.py:185][0m |          -0.0039 |          33.0275 |          15.5976 |
[32m[20221213 22:57:20 @agent_ppo2.py:185][0m |          -0.0051 |          28.5746 |          15.5948 |
[32m[20221213 22:57:20 @agent_ppo2.py:185][0m |          -0.0041 |          27.7267 |          15.5899 |
[32m[20221213 22:57:21 @agent_ppo2.py:185][0m |          -0.0110 |          27.2753 |          15.5640 |
[32m[20221213 22:57:21 @agent_ppo2.py:185][0m |          -0.0170 |          26.6759 |          15.5562 |
[32m[20221213 22:57:21 @agent_ppo2.py:185][0m |          -0.0086 |          26.4151 |          15.5633 |
[32m[20221213 22:57:21 @agent_ppo2.py:185][0m |          -0.0141 |          26.0780 |          15.5526 |
[32m[20221213 22:57:21 @agent_ppo2.py:185][0m |          -0.0112 |          25.8165 |          15.5344 |
[32m[20221213 22:57:21 @agent_ppo2.py:185][0m |          -0.0149 |          25.6009 |          15.5319 |
[32m[20221213 22:57:21 @agent_ppo2.py:185][0m |          -0.0215 |          25.4509 |          15.5237 |
[32m[20221213 22:57:21 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 22:57:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.25
[32m[20221213 22:57:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 388.38
[32m[20221213 22:57:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.98
[32m[20221213 22:57:21 @agent_ppo2.py:143][0m Total time:      39.14 min
[32m[20221213 22:57:21 @agent_ppo2.py:145][0m 3833856 total steps have happened
[32m[20221213 22:57:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1872 --------------------------#
[32m[20221213 22:57:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |           0.0026 |          46.8882 |          15.7541 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0051 |          43.7816 |          15.6943 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0059 |          42.8860 |          15.7104 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0086 |          42.2825 |          15.6939 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0105 |          42.1911 |          15.6877 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0087 |          41.4919 |          15.6809 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0100 |          41.3377 |          15.6885 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0002 |          43.7329 |          15.6716 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |          -0.0108 |          40.7021 |          15.6627 |
[32m[20221213 22:57:22 @agent_ppo2.py:185][0m |           0.0048 |          44.2982 |          15.6592 |
[32m[20221213 22:57:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.19
[32m[20221213 22:57:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.94
[32m[20221213 22:57:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.84
[32m[20221213 22:57:23 @agent_ppo2.py:143][0m Total time:      39.16 min
[32m[20221213 22:57:23 @agent_ppo2.py:145][0m 3835904 total steps have happened
[32m[20221213 22:57:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1873 --------------------------#
[32m[20221213 22:57:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:23 @agent_ppo2.py:185][0m |          -0.0044 |          35.7635 |          15.6448 |
[32m[20221213 22:57:23 @agent_ppo2.py:185][0m |          -0.0090 |          32.6661 |          15.6430 |
[32m[20221213 22:57:23 @agent_ppo2.py:185][0m |          -0.0012 |          33.1970 |          15.6582 |
[32m[20221213 22:57:23 @agent_ppo2.py:185][0m |          -0.0110 |          31.3007 |          15.6186 |
[32m[20221213 22:57:23 @agent_ppo2.py:185][0m |          -0.0127 |          30.8655 |          15.6452 |
[32m[20221213 22:57:23 @agent_ppo2.py:185][0m |           0.0022 |          33.6131 |          15.6567 |
[32m[20221213 22:57:23 @agent_ppo2.py:185][0m |          -0.0107 |          30.3424 |          15.6566 |
[32m[20221213 22:57:24 @agent_ppo2.py:185][0m |          -0.0104 |          30.0337 |          15.6667 |
[32m[20221213 22:57:24 @agent_ppo2.py:185][0m |          -0.0143 |          29.7829 |          15.6548 |
[32m[20221213 22:57:24 @agent_ppo2.py:185][0m |          -0.0169 |          29.4158 |          15.6570 |
[32m[20221213 22:57:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:57:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.71
[32m[20221213 22:57:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.00
[32m[20221213 22:57:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.22
[32m[20221213 22:57:24 @agent_ppo2.py:143][0m Total time:      39.18 min
[32m[20221213 22:57:24 @agent_ppo2.py:145][0m 3837952 total steps have happened
[32m[20221213 22:57:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1874 --------------------------#
[32m[20221213 22:57:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:24 @agent_ppo2.py:185][0m |          -0.0006 |          44.7036 |          15.6446 |
[32m[20221213 22:57:24 @agent_ppo2.py:185][0m |          -0.0062 |          43.5695 |          15.6203 |
[32m[20221213 22:57:24 @agent_ppo2.py:185][0m |          -0.0057 |          43.0719 |          15.6062 |
[32m[20221213 22:57:24 @agent_ppo2.py:185][0m |           0.0038 |          44.4372 |          15.6143 |
[32m[20221213 22:57:25 @agent_ppo2.py:185][0m |          -0.0076 |          42.7307 |          15.6043 |
[32m[20221213 22:57:25 @agent_ppo2.py:185][0m |          -0.0078 |          42.3644 |          15.6131 |
[32m[20221213 22:57:25 @agent_ppo2.py:185][0m |          -0.0034 |          42.5014 |          15.6061 |
[32m[20221213 22:57:25 @agent_ppo2.py:185][0m |          -0.0094 |          42.2052 |          15.6235 |
[32m[20221213 22:57:25 @agent_ppo2.py:185][0m |          -0.0010 |          43.5487 |          15.6104 |
[32m[20221213 22:57:25 @agent_ppo2.py:185][0m |          -0.0065 |          42.2499 |          15.6049 |
[32m[20221213 22:57:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.45
[32m[20221213 22:57:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.09
[32m[20221213 22:57:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.93
[32m[20221213 22:57:25 @agent_ppo2.py:143][0m Total time:      39.20 min
[32m[20221213 22:57:25 @agent_ppo2.py:145][0m 3840000 total steps have happened
[32m[20221213 22:57:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1875 --------------------------#
[32m[20221213 22:57:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0033 |          39.4133 |          15.6346 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0043 |          34.6546 |          15.6229 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0083 |          32.9474 |          15.6076 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0135 |          31.8420 |          15.6328 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0037 |          31.2143 |          15.6107 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0111 |          30.6760 |          15.6072 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0105 |          30.1666 |          15.5952 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0169 |          29.9058 |          15.5951 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0091 |          30.1378 |          15.5840 |
[32m[20221213 22:57:26 @agent_ppo2.py:185][0m |          -0.0175 |          29.2531 |          15.5980 |
[32m[20221213 22:57:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:57:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.50
[32m[20221213 22:57:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.37
[32m[20221213 22:57:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.94
[32m[20221213 22:57:26 @agent_ppo2.py:143][0m Total time:      39.23 min
[32m[20221213 22:57:26 @agent_ppo2.py:145][0m 3842048 total steps have happened
[32m[20221213 22:57:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1876 --------------------------#
[32m[20221213 22:57:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0024 |          42.7890 |          15.5541 |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0041 |          39.9385 |          15.5801 |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0079 |          38.7781 |          15.5873 |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0059 |          37.9100 |          15.5982 |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0076 |          37.1842 |          15.6093 |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0078 |          36.8292 |          15.6015 |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0054 |          37.0767 |          15.6225 |
[32m[20221213 22:57:27 @agent_ppo2.py:185][0m |          -0.0055 |          37.1300 |          15.6168 |
[32m[20221213 22:57:28 @agent_ppo2.py:185][0m |          -0.0115 |          36.3069 |          15.6430 |
[32m[20221213 22:57:28 @agent_ppo2.py:185][0m |          -0.0109 |          36.1046 |          15.6572 |
[32m[20221213 22:57:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:57:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.07
[32m[20221213 22:57:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.11
[32m[20221213 22:57:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.93
[32m[20221213 22:57:28 @agent_ppo2.py:143][0m Total time:      39.25 min
[32m[20221213 22:57:28 @agent_ppo2.py:145][0m 3844096 total steps have happened
[32m[20221213 22:57:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1877 --------------------------#
[32m[20221213 22:57:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:28 @agent_ppo2.py:185][0m |           0.0014 |          26.8570 |          15.8021 |
[32m[20221213 22:57:28 @agent_ppo2.py:185][0m |          -0.0007 |          23.0656 |          15.7832 |
[32m[20221213 22:57:28 @agent_ppo2.py:185][0m |          -0.0049 |          22.1249 |          15.7979 |
[32m[20221213 22:57:28 @agent_ppo2.py:185][0m |          -0.0085 |          21.4657 |          15.8021 |
[32m[20221213 22:57:28 @agent_ppo2.py:185][0m |          -0.0039 |          20.9877 |          15.7963 |
[32m[20221213 22:57:29 @agent_ppo2.py:185][0m |          -0.0104 |          20.6777 |          15.8123 |
[32m[20221213 22:57:29 @agent_ppo2.py:185][0m |          -0.0136 |          20.3328 |          15.8091 |
[32m[20221213 22:57:29 @agent_ppo2.py:185][0m |          -0.0146 |          20.1507 |          15.7914 |
[32m[20221213 22:57:29 @agent_ppo2.py:185][0m |          -0.0106 |          19.9331 |          15.7958 |
[32m[20221213 22:57:29 @agent_ppo2.py:185][0m |          -0.0105 |          19.6194 |          15.7863 |
[32m[20221213 22:57:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:57:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.81
[32m[20221213 22:57:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 355.89
[32m[20221213 22:57:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 345.90
[32m[20221213 22:57:29 @agent_ppo2.py:143][0m Total time:      39.27 min
[32m[20221213 22:57:29 @agent_ppo2.py:145][0m 3846144 total steps have happened
[32m[20221213 22:57:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1878 --------------------------#
[32m[20221213 22:57:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:57:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:29 @agent_ppo2.py:185][0m |          -0.0019 |          38.6496 |          15.7166 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0058 |          33.9420 |          15.6871 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0098 |          32.6104 |          15.6849 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0031 |          32.8978 |          15.7034 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0125 |          31.4174 |          15.7030 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0111 |          31.0439 |          15.7042 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0096 |          31.2459 |          15.7025 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0140 |          30.5387 |          15.7142 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0153 |          30.1746 |          15.7079 |
[32m[20221213 22:57:30 @agent_ppo2.py:185][0m |          -0.0192 |          30.1400 |          15.7128 |
[32m[20221213 22:57:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:57:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.89
[32m[20221213 22:57:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 420.55
[32m[20221213 22:57:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.74
[32m[20221213 22:57:30 @agent_ppo2.py:143][0m Total time:      39.29 min
[32m[20221213 22:57:30 @agent_ppo2.py:145][0m 3848192 total steps have happened
[32m[20221213 22:57:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1879 --------------------------#
[32m[20221213 22:57:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |           0.0068 |          41.5203 |          15.8130 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |          -0.0040 |          38.6635 |          15.8101 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |          -0.0024 |          38.3997 |          15.8060 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |          -0.0077 |          37.4426 |          15.7950 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |          -0.0060 |          37.1128 |          15.7965 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |           0.0030 |          41.2622 |          15.8009 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |          -0.0081 |          36.6720 |          15.7564 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |          -0.0098 |          36.3601 |          15.7872 |
[32m[20221213 22:57:31 @agent_ppo2.py:185][0m |          -0.0113 |          36.2371 |          15.7949 |
[32m[20221213 22:57:32 @agent_ppo2.py:185][0m |          -0.0090 |          36.0658 |          15.8044 |
[32m[20221213 22:57:32 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.42
[32m[20221213 22:57:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.77
[32m[20221213 22:57:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.70
[32m[20221213 22:57:32 @agent_ppo2.py:143][0m Total time:      39.31 min
[32m[20221213 22:57:32 @agent_ppo2.py:145][0m 3850240 total steps have happened
[32m[20221213 22:57:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1880 --------------------------#
[32m[20221213 22:57:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:57:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:32 @agent_ppo2.py:185][0m |           0.0002 |          45.4855 |          15.7267 |
[32m[20221213 22:57:32 @agent_ppo2.py:185][0m |           0.0029 |          45.1784 |          15.7026 |
[32m[20221213 22:57:32 @agent_ppo2.py:185][0m |          -0.0057 |          42.2879 |          15.6912 |
[32m[20221213 22:57:32 @agent_ppo2.py:185][0m |          -0.0055 |          41.8132 |          15.6980 |
[32m[20221213 22:57:32 @agent_ppo2.py:185][0m |          -0.0065 |          41.4577 |          15.6991 |
[32m[20221213 22:57:33 @agent_ppo2.py:185][0m |          -0.0091 |          41.2557 |          15.6798 |
[32m[20221213 22:57:33 @agent_ppo2.py:185][0m |          -0.0130 |          40.9839 |          15.6976 |
[32m[20221213 22:57:33 @agent_ppo2.py:185][0m |          -0.0027 |          40.8952 |          15.6944 |
[32m[20221213 22:57:33 @agent_ppo2.py:185][0m |          -0.0013 |          42.5522 |          15.6937 |
[32m[20221213 22:57:33 @agent_ppo2.py:185][0m |          -0.0109 |          40.3893 |          15.7142 |
[32m[20221213 22:57:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:57:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.02
[32m[20221213 22:57:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.33
[32m[20221213 22:57:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.53
[32m[20221213 22:57:33 @agent_ppo2.py:143][0m Total time:      39.34 min
[32m[20221213 22:57:33 @agent_ppo2.py:145][0m 3852288 total steps have happened
[32m[20221213 22:57:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1881 --------------------------#
[32m[20221213 22:57:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:33 @agent_ppo2.py:185][0m |          -0.0008 |          42.3565 |          15.8591 |
[32m[20221213 22:57:33 @agent_ppo2.py:185][0m |          -0.0029 |          37.4369 |          15.8456 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0040 |          36.3076 |          15.8469 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0049 |          35.6980 |          15.8408 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0101 |          35.4449 |          15.8444 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0079 |          35.0850 |          15.8409 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0056 |          35.6690 |          15.8420 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0090 |          34.9118 |          15.8544 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0093 |          34.6964 |          15.8449 |
[32m[20221213 22:57:34 @agent_ppo2.py:185][0m |          -0.0125 |          34.5227 |          15.8551 |
[32m[20221213 22:57:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:57:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.14
[32m[20221213 22:57:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 365.23
[32m[20221213 22:57:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.41
[32m[20221213 22:57:34 @agent_ppo2.py:143][0m Total time:      39.36 min
[32m[20221213 22:57:34 @agent_ppo2.py:145][0m 3854336 total steps have happened
[32m[20221213 22:57:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1882 --------------------------#
[32m[20221213 22:57:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:57:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |           0.0003 |          45.1232 |          15.6504 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0076 |          42.9408 |          15.6333 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0068 |          42.0980 |          15.6183 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0050 |          42.1212 |          15.6252 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0107 |          41.2754 |          15.6295 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0105 |          41.0636 |          15.6225 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0132 |          40.7315 |          15.6107 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |           0.0066 |          44.5831 |          15.6242 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0137 |          40.4277 |          15.6071 |
[32m[20221213 22:57:35 @agent_ppo2.py:185][0m |          -0.0159 |          40.1652 |          15.6113 |
[32m[20221213 22:57:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:57:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.15
[32m[20221213 22:57:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.58
[32m[20221213 22:57:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.97
[32m[20221213 22:57:36 @agent_ppo2.py:143][0m Total time:      39.38 min
[32m[20221213 22:57:36 @agent_ppo2.py:145][0m 3856384 total steps have happened
[32m[20221213 22:57:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1883 --------------------------#
[32m[20221213 22:57:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:36 @agent_ppo2.py:185][0m |          -0.0026 |          38.9297 |          15.5778 |
[32m[20221213 22:57:36 @agent_ppo2.py:185][0m |          -0.0047 |          36.2449 |          15.5566 |
[32m[20221213 22:57:36 @agent_ppo2.py:185][0m |          -0.0101 |          35.5792 |          15.5459 |
[32m[20221213 22:57:36 @agent_ppo2.py:185][0m |          -0.0097 |          35.2972 |          15.5534 |
[32m[20221213 22:57:36 @agent_ppo2.py:185][0m |          -0.0088 |          35.0717 |          15.5654 |
[32m[20221213 22:57:36 @agent_ppo2.py:185][0m |          -0.0117 |          34.8483 |          15.5476 |
[32m[20221213 22:57:36 @agent_ppo2.py:185][0m |          -0.0009 |          39.2465 |          15.5583 |
[32m[20221213 22:57:37 @agent_ppo2.py:185][0m |          -0.0173 |          34.6140 |          15.5568 |
[32m[20221213 22:57:37 @agent_ppo2.py:185][0m |          -0.0117 |          34.3294 |          15.5468 |
[32m[20221213 22:57:37 @agent_ppo2.py:185][0m |          -0.0106 |          34.2873 |          15.5602 |
[32m[20221213 22:57:37 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.81
[32m[20221213 22:57:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.40
[32m[20221213 22:57:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.96
[32m[20221213 22:57:37 @agent_ppo2.py:143][0m Total time:      39.40 min
[32m[20221213 22:57:37 @agent_ppo2.py:145][0m 3858432 total steps have happened
[32m[20221213 22:57:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1884 --------------------------#
[32m[20221213 22:57:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:37 @agent_ppo2.py:185][0m |           0.0018 |          46.9076 |          15.8459 |
[32m[20221213 22:57:37 @agent_ppo2.py:185][0m |          -0.0067 |          45.2564 |          15.8495 |
[32m[20221213 22:57:37 @agent_ppo2.py:185][0m |          -0.0049 |          44.8001 |          15.8520 |
[32m[20221213 22:57:38 @agent_ppo2.py:185][0m |          -0.0047 |          44.5334 |          15.8416 |
[32m[20221213 22:57:38 @agent_ppo2.py:185][0m |          -0.0035 |          44.2405 |          15.8431 |
[32m[20221213 22:57:38 @agent_ppo2.py:185][0m |          -0.0036 |          45.4137 |          15.8570 |
[32m[20221213 22:57:38 @agent_ppo2.py:185][0m |          -0.0096 |          44.2227 |          15.8533 |
[32m[20221213 22:57:38 @agent_ppo2.py:185][0m |          -0.0056 |          44.6455 |          15.8373 |
[32m[20221213 22:57:38 @agent_ppo2.py:185][0m |          -0.0069 |          44.2464 |          15.8363 |
[32m[20221213 22:57:38 @agent_ppo2.py:185][0m |          -0.0095 |          43.9346 |          15.8453 |
[32m[20221213 22:57:38 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:57:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.07
[32m[20221213 22:57:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.85
[32m[20221213 22:57:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.96
[32m[20221213 22:57:38 @agent_ppo2.py:143][0m Total time:      39.42 min
[32m[20221213 22:57:38 @agent_ppo2.py:145][0m 3860480 total steps have happened
[32m[20221213 22:57:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1885 --------------------------#
[32m[20221213 22:57:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |           0.0016 |          42.4724 |          15.8834 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0034 |          39.8701 |          15.8783 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0051 |          39.0114 |          15.8616 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0119 |          38.4698 |          15.8771 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0110 |          38.2005 |          15.8852 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0127 |          37.8590 |          15.9076 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0109 |          37.6099 |          15.9061 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0155 |          37.4758 |          15.9121 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0145 |          37.4336 |          15.9029 |
[32m[20221213 22:57:39 @agent_ppo2.py:185][0m |          -0.0102 |          37.1492 |          15.9237 |
[32m[20221213 22:57:39 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.40
[32m[20221213 22:57:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.43
[32m[20221213 22:57:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.17
[32m[20221213 22:57:40 @agent_ppo2.py:143][0m Total time:      39.44 min
[32m[20221213 22:57:40 @agent_ppo2.py:145][0m 3862528 total steps have happened
[32m[20221213 22:57:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1886 --------------------------#
[32m[20221213 22:57:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |           0.0003 |          57.7691 |          15.7936 |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |           0.0071 |          60.9017 |          15.7574 |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |           0.0061 |          60.4022 |          15.7671 |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |          -0.0005 |          61.3953 |          15.7678 |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |          -0.0100 |          55.4768 |          15.7541 |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |          -0.0110 |          55.1239 |          15.7510 |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |          -0.0128 |          54.9958 |          15.7712 |
[32m[20221213 22:57:40 @agent_ppo2.py:185][0m |          -0.0092 |          55.1121 |          15.7870 |
[32m[20221213 22:57:41 @agent_ppo2.py:185][0m |          -0.0121 |          54.7200 |          15.7957 |
[32m[20221213 22:57:41 @agent_ppo2.py:185][0m |          -0.0126 |          54.6422 |          15.7979 |
[32m[20221213 22:57:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:57:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.39
[32m[20221213 22:57:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.73
[32m[20221213 22:57:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.99
[32m[20221213 22:57:41 @agent_ppo2.py:143][0m Total time:      39.47 min
[32m[20221213 22:57:41 @agent_ppo2.py:145][0m 3864576 total steps have happened
[32m[20221213 22:57:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1887 --------------------------#
[32m[20221213 22:57:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:41 @agent_ppo2.py:185][0m |           0.0013 |          33.6620 |          15.7194 |
[32m[20221213 22:57:41 @agent_ppo2.py:185][0m |          -0.0048 |          30.9947 |          15.7207 |
[32m[20221213 22:57:41 @agent_ppo2.py:185][0m |           0.0027 |          31.5479 |          15.7008 |
[32m[20221213 22:57:41 @agent_ppo2.py:185][0m |          -0.0053 |          30.0289 |          15.7072 |
[32m[20221213 22:57:42 @agent_ppo2.py:185][0m |          -0.0075 |          29.7491 |          15.6966 |
[32m[20221213 22:57:42 @agent_ppo2.py:185][0m |          -0.0080 |          29.6447 |          15.6907 |
[32m[20221213 22:57:42 @agent_ppo2.py:185][0m |           0.0040 |          33.8362 |          15.6722 |
[32m[20221213 22:57:42 @agent_ppo2.py:185][0m |          -0.0096 |          29.3468 |          15.6586 |
[32m[20221213 22:57:42 @agent_ppo2.py:185][0m |          -0.0094 |          29.4508 |          15.6525 |
[32m[20221213 22:57:42 @agent_ppo2.py:185][0m |          -0.0095 |          29.2096 |          15.6483 |
[32m[20221213 22:57:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:57:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.48
[32m[20221213 22:57:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.93
[32m[20221213 22:57:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.21
[32m[20221213 22:57:42 @agent_ppo2.py:143][0m Total time:      39.49 min
[32m[20221213 22:57:42 @agent_ppo2.py:145][0m 3866624 total steps have happened
[32m[20221213 22:57:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1888 --------------------------#
[32m[20221213 22:57:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:42 @agent_ppo2.py:185][0m |           0.0032 |          39.4520 |          15.6476 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0052 |          37.8488 |          15.6275 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0082 |          37.4025 |          15.5844 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0090 |          37.1555 |          15.5944 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0081 |          36.6942 |          15.5795 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0075 |          36.3900 |          15.5765 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0118 |          36.2262 |          15.5579 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0163 |          36.0417 |          15.5496 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0128 |          35.9014 |          15.5515 |
[32m[20221213 22:57:43 @agent_ppo2.py:185][0m |          -0.0123 |          35.7718 |          15.5412 |
[32m[20221213 22:57:43 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.69
[32m[20221213 22:57:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.54
[32m[20221213 22:57:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.66
[32m[20221213 22:57:43 @agent_ppo2.py:143][0m Total time:      39.51 min
[32m[20221213 22:57:43 @agent_ppo2.py:145][0m 3868672 total steps have happened
[32m[20221213 22:57:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1889 --------------------------#
[32m[20221213 22:57:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:57:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |           0.0023 |          31.3031 |          15.6107 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |          -0.0069 |          27.8051 |          15.5931 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |          -0.0073 |          26.5265 |          15.5772 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |          -0.0052 |          25.9246 |          15.5633 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |          -0.0109 |          25.4038 |          15.5576 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |           0.0077 |          29.4854 |          15.5715 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |          -0.0053 |          24.7311 |          15.5653 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |          -0.0076 |          24.3827 |          15.5596 |
[32m[20221213 22:57:44 @agent_ppo2.py:185][0m |          -0.0145 |          24.1964 |          15.5580 |
[32m[20221213 22:57:45 @agent_ppo2.py:185][0m |          -0.0117 |          23.7765 |          15.5559 |
[32m[20221213 22:57:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:57:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.06
[32m[20221213 22:57:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.25
[32m[20221213 22:57:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.12
[32m[20221213 22:57:45 @agent_ppo2.py:143][0m Total time:      39.53 min
[32m[20221213 22:57:45 @agent_ppo2.py:145][0m 3870720 total steps have happened
[32m[20221213 22:57:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1890 --------------------------#
[32m[20221213 22:57:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:57:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:45 @agent_ppo2.py:185][0m |           0.0008 |          47.5051 |          15.6331 |
[32m[20221213 22:57:45 @agent_ppo2.py:185][0m |           0.0045 |          45.8808 |          15.5790 |
[32m[20221213 22:57:45 @agent_ppo2.py:185][0m |           0.0063 |          48.2724 |          15.5592 |
[32m[20221213 22:57:45 @agent_ppo2.py:185][0m |          -0.0018 |          43.6221 |          15.5685 |
[32m[20221213 22:57:45 @agent_ppo2.py:185][0m |          -0.0041 |          43.4869 |          15.5505 |
[32m[20221213 22:57:46 @agent_ppo2.py:185][0m |          -0.0090 |          42.8286 |          15.5485 |
[32m[20221213 22:57:46 @agent_ppo2.py:185][0m |           0.0135 |          47.2145 |          15.5253 |
[32m[20221213 22:57:46 @agent_ppo2.py:185][0m |          -0.0072 |          42.5904 |          15.5113 |
[32m[20221213 22:57:46 @agent_ppo2.py:185][0m |          -0.0035 |          42.1757 |          15.4954 |
[32m[20221213 22:57:46 @agent_ppo2.py:185][0m |          -0.0122 |          42.0178 |          15.4890 |
[32m[20221213 22:57:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:57:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.40
[32m[20221213 22:57:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.94
[32m[20221213 22:57:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.65
[32m[20221213 22:57:46 @agent_ppo2.py:143][0m Total time:      39.55 min
[32m[20221213 22:57:46 @agent_ppo2.py:145][0m 3872768 total steps have happened
[32m[20221213 22:57:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1891 --------------------------#
[32m[20221213 22:57:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:46 @agent_ppo2.py:185][0m |          -0.0053 |          40.1984 |          15.7477 |
[32m[20221213 22:57:46 @agent_ppo2.py:185][0m |          -0.0096 |          35.4824 |          15.7581 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0077 |          34.4067 |          15.7434 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0107 |          33.6005 |          15.7512 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0098 |          33.0441 |          15.7415 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0131 |          32.7713 |          15.7431 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0112 |          32.4406 |          15.7461 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0132 |          32.1140 |          15.7507 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0045 |          34.5455 |          15.7501 |
[32m[20221213 22:57:47 @agent_ppo2.py:185][0m |          -0.0126 |          31.7620 |          15.7501 |
[32m[20221213 22:57:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.58
[32m[20221213 22:57:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.20
[32m[20221213 22:57:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.22
[32m[20221213 22:57:47 @agent_ppo2.py:143][0m Total time:      39.57 min
[32m[20221213 22:57:47 @agent_ppo2.py:145][0m 3874816 total steps have happened
[32m[20221213 22:57:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1892 --------------------------#
[32m[20221213 22:57:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0030 |          41.1638 |          15.5033 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0056 |          38.9822 |          15.4888 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0068 |          38.2237 |          15.4865 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0074 |          38.2603 |          15.4847 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |           0.0031 |          40.8938 |          15.4753 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0017 |          38.9967 |          15.4832 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0055 |          38.2506 |          15.4829 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0104 |          36.8408 |          15.4814 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0117 |          36.9089 |          15.4832 |
[32m[20221213 22:57:48 @agent_ppo2.py:185][0m |          -0.0087 |          36.7811 |          15.4997 |
[32m[20221213 22:57:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:57:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.52
[32m[20221213 22:57:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 383.61
[32m[20221213 22:57:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 394.81
[32m[20221213 22:57:49 @agent_ppo2.py:143][0m Total time:      39.60 min
[32m[20221213 22:57:49 @agent_ppo2.py:145][0m 3876864 total steps have happened
[32m[20221213 22:57:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1893 --------------------------#
[32m[20221213 22:57:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:49 @agent_ppo2.py:185][0m |           0.0040 |          55.1712 |          15.6524 |
[32m[20221213 22:57:49 @agent_ppo2.py:185][0m |          -0.0012 |          53.1453 |          15.6361 |
[32m[20221213 22:57:49 @agent_ppo2.py:185][0m |          -0.0068 |          52.3773 |          15.6036 |
[32m[20221213 22:57:49 @agent_ppo2.py:185][0m |          -0.0061 |          51.8683 |          15.5921 |
[32m[20221213 22:57:49 @agent_ppo2.py:185][0m |          -0.0095 |          51.4822 |          15.5936 |
[32m[20221213 22:57:49 @agent_ppo2.py:185][0m |          -0.0075 |          51.3630 |          15.5901 |
[32m[20221213 22:57:49 @agent_ppo2.py:185][0m |          -0.0065 |          51.1495 |          15.5887 |
[32m[20221213 22:57:50 @agent_ppo2.py:185][0m |          -0.0065 |          50.9643 |          15.5815 |
[32m[20221213 22:57:50 @agent_ppo2.py:185][0m |          -0.0097 |          50.9863 |          15.5684 |
[32m[20221213 22:57:50 @agent_ppo2.py:185][0m |          -0.0003 |          52.5605 |          15.5666 |
[32m[20221213 22:57:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:57:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.91
[32m[20221213 22:57:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.47
[32m[20221213 22:57:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.72
[32m[20221213 22:57:50 @agent_ppo2.py:143][0m Total time:      39.62 min
[32m[20221213 22:57:50 @agent_ppo2.py:145][0m 3878912 total steps have happened
[32m[20221213 22:57:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1894 --------------------------#
[32m[20221213 22:57:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:57:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:50 @agent_ppo2.py:185][0m |          -0.0040 |          54.8365 |          15.5189 |
[32m[20221213 22:57:50 @agent_ppo2.py:185][0m |          -0.0026 |          52.1081 |          15.5205 |
[32m[20221213 22:57:50 @agent_ppo2.py:185][0m |          -0.0051 |          51.1129 |          15.4999 |
[32m[20221213 22:57:51 @agent_ppo2.py:185][0m |          -0.0043 |          50.7964 |          15.4917 |
[32m[20221213 22:57:51 @agent_ppo2.py:185][0m |          -0.0061 |          50.0981 |          15.4865 |
[32m[20221213 22:57:51 @agent_ppo2.py:185][0m |           0.0012 |          54.4720 |          15.4843 |
[32m[20221213 22:57:51 @agent_ppo2.py:185][0m |          -0.0053 |          49.4092 |          15.4925 |
[32m[20221213 22:57:51 @agent_ppo2.py:185][0m |          -0.0055 |          49.0546 |          15.4815 |
[32m[20221213 22:57:51 @agent_ppo2.py:185][0m |          -0.0082 |          48.7724 |          15.4868 |
[32m[20221213 22:57:51 @agent_ppo2.py:185][0m |          -0.0072 |          48.4709 |          15.4709 |
[32m[20221213 22:57:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:57:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.73
[32m[20221213 22:57:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 403.08
[32m[20221213 22:57:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.21
[32m[20221213 22:57:51 @agent_ppo2.py:143][0m Total time:      39.64 min
[32m[20221213 22:57:51 @agent_ppo2.py:145][0m 3880960 total steps have happened
[32m[20221213 22:57:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1895 --------------------------#
[32m[20221213 22:57:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |           0.0019 |          38.6609 |          15.5749 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0030 |          30.2566 |          15.5626 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0080 |          29.0806 |          15.5564 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0064 |          28.4148 |          15.5541 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0055 |          28.0263 |          15.5627 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0110 |          27.7156 |          15.5543 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0041 |          29.3555 |          15.5338 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0098 |          27.5038 |          15.5481 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0091 |          27.1423 |          15.5493 |
[32m[20221213 22:57:52 @agent_ppo2.py:185][0m |          -0.0022 |          30.9896 |          15.5336 |
[32m[20221213 22:57:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:57:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.06
[32m[20221213 22:57:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.29
[32m[20221213 22:57:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.71
[32m[20221213 22:57:53 @agent_ppo2.py:143][0m Total time:      39.66 min
[32m[20221213 22:57:53 @agent_ppo2.py:145][0m 3883008 total steps have happened
[32m[20221213 22:57:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1896 --------------------------#
[32m[20221213 22:57:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:53 @agent_ppo2.py:185][0m |           0.0008 |          35.8860 |          15.7465 |
[32m[20221213 22:57:53 @agent_ppo2.py:185][0m |           0.0015 |          30.0584 |          15.7324 |
[32m[20221213 22:57:53 @agent_ppo2.py:185][0m |          -0.0013 |          29.0340 |          15.7376 |
[32m[20221213 22:57:53 @agent_ppo2.py:185][0m |          -0.0029 |          28.1009 |          15.7370 |
[32m[20221213 22:57:53 @agent_ppo2.py:185][0m |          -0.0050 |          27.7158 |          15.7317 |
[32m[20221213 22:57:53 @agent_ppo2.py:185][0m |          -0.0126 |          27.1390 |          15.7124 |
[32m[20221213 22:57:53 @agent_ppo2.py:185][0m |          -0.0118 |          26.9966 |          15.7262 |
[32m[20221213 22:57:54 @agent_ppo2.py:185][0m |          -0.0031 |          29.1106 |          15.7037 |
[32m[20221213 22:57:54 @agent_ppo2.py:185][0m |          -0.0117 |          26.4805 |          15.7367 |
[32m[20221213 22:57:54 @agent_ppo2.py:185][0m |          -0.0058 |          26.2888 |          15.7070 |
[32m[20221213 22:57:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:57:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.58
[32m[20221213 22:57:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.50
[32m[20221213 22:57:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.59
[32m[20221213 22:57:54 @agent_ppo2.py:143][0m Total time:      39.68 min
[32m[20221213 22:57:54 @agent_ppo2.py:145][0m 3885056 total steps have happened
[32m[20221213 22:57:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1897 --------------------------#
[32m[20221213 22:57:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:54 @agent_ppo2.py:185][0m |           0.0028 |          36.7872 |          15.7819 |
[32m[20221213 22:57:54 @agent_ppo2.py:185][0m |          -0.0054 |          34.9721 |          15.7610 |
[32m[20221213 22:57:54 @agent_ppo2.py:185][0m |          -0.0050 |          34.1378 |          15.7445 |
[32m[20221213 22:57:54 @agent_ppo2.py:185][0m |          -0.0101 |          33.6394 |          15.7236 |
[32m[20221213 22:57:55 @agent_ppo2.py:185][0m |          -0.0091 |          33.0621 |          15.7227 |
[32m[20221213 22:57:55 @agent_ppo2.py:185][0m |          -0.0151 |          32.7806 |          15.7209 |
[32m[20221213 22:57:55 @agent_ppo2.py:185][0m |          -0.0151 |          32.3957 |          15.7157 |
[32m[20221213 22:57:55 @agent_ppo2.py:185][0m |          -0.0131 |          32.1659 |          15.7138 |
[32m[20221213 22:57:55 @agent_ppo2.py:185][0m |          -0.0132 |          31.7965 |          15.7052 |
[32m[20221213 22:57:55 @agent_ppo2.py:185][0m |          -0.0128 |          31.6074 |          15.6945 |
[32m[20221213 22:57:55 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.80
[32m[20221213 22:57:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.78
[32m[20221213 22:57:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.28
[32m[20221213 22:57:55 @agent_ppo2.py:143][0m Total time:      39.70 min
[32m[20221213 22:57:55 @agent_ppo2.py:145][0m 3887104 total steps have happened
[32m[20221213 22:57:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1898 --------------------------#
[32m[20221213 22:57:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:57:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:55 @agent_ppo2.py:185][0m |           0.0002 |          39.0098 |          15.7218 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0081 |          36.4388 |          15.7195 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0097 |          35.5637 |          15.7294 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0084 |          35.0938 |          15.7446 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0082 |          34.5302 |          15.7276 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0090 |          34.1913 |          15.7451 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0081 |          35.5740 |          15.7362 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0100 |          34.0867 |          15.7403 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0146 |          33.5236 |          15.7374 |
[32m[20221213 22:57:56 @agent_ppo2.py:185][0m |          -0.0167 |          33.3747 |          15.7418 |
[32m[20221213 22:57:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.47
[32m[20221213 22:57:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.91
[32m[20221213 22:57:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.44
[32m[20221213 22:57:56 @agent_ppo2.py:143][0m Total time:      39.73 min
[32m[20221213 22:57:56 @agent_ppo2.py:145][0m 3889152 total steps have happened
[32m[20221213 22:57:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1899 --------------------------#
[32m[20221213 22:57:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:57:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0019 |          39.9544 |          15.6317 |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0002 |          37.6353 |          15.6306 |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0118 |          37.0352 |          15.6068 |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0024 |          37.9117 |          15.6172 |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0030 |          36.3711 |          15.6098 |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0133 |          35.9692 |          15.6198 |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0108 |          35.7951 |          15.6069 |
[32m[20221213 22:57:57 @agent_ppo2.py:185][0m |          -0.0134 |          35.5286 |          15.6112 |
[32m[20221213 22:57:58 @agent_ppo2.py:185][0m |          -0.0099 |          35.4088 |          15.6124 |
[32m[20221213 22:57:58 @agent_ppo2.py:185][0m |          -0.0159 |          35.2124 |          15.6103 |
[32m[20221213 22:57:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.27
[32m[20221213 22:57:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.35
[32m[20221213 22:57:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.18
[32m[20221213 22:57:58 @agent_ppo2.py:143][0m Total time:      39.75 min
[32m[20221213 22:57:58 @agent_ppo2.py:145][0m 3891200 total steps have happened
[32m[20221213 22:57:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1900 --------------------------#
[32m[20221213 22:57:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:57:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:58 @agent_ppo2.py:185][0m |           0.0055 |          33.6754 |          15.5562 |
[32m[20221213 22:57:58 @agent_ppo2.py:185][0m |          -0.0041 |          30.2006 |          15.4931 |
[32m[20221213 22:57:58 @agent_ppo2.py:185][0m |          -0.0042 |          29.1572 |          15.4846 |
[32m[20221213 22:57:58 @agent_ppo2.py:185][0m |          -0.0123 |          28.3906 |          15.4964 |
[32m[20221213 22:57:58 @agent_ppo2.py:185][0m |          -0.0108 |          27.8885 |          15.4757 |
[32m[20221213 22:57:59 @agent_ppo2.py:185][0m |          -0.0082 |          27.6459 |          15.4717 |
[32m[20221213 22:57:59 @agent_ppo2.py:185][0m |          -0.0138 |          26.9897 |          15.4766 |
[32m[20221213 22:57:59 @agent_ppo2.py:185][0m |          -0.0169 |          26.9193 |          15.4675 |
[32m[20221213 22:57:59 @agent_ppo2.py:185][0m |          -0.0160 |          26.5355 |          15.4751 |
[32m[20221213 22:57:59 @agent_ppo2.py:185][0m |          -0.0125 |          26.2023 |          15.4614 |
[32m[20221213 22:57:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:57:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.97
[32m[20221213 22:57:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.23
[32m[20221213 22:57:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.60
[32m[20221213 22:57:59 @agent_ppo2.py:143][0m Total time:      39.77 min
[32m[20221213 22:57:59 @agent_ppo2.py:145][0m 3893248 total steps have happened
[32m[20221213 22:57:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1901 --------------------------#
[32m[20221213 22:57:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:57:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:57:59 @agent_ppo2.py:185][0m |           0.0019 |          41.7595 |          15.3763 |
[32m[20221213 22:57:59 @agent_ppo2.py:185][0m |          -0.0024 |          39.6800 |          15.3596 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0081 |          38.7532 |          15.3552 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0032 |          38.8828 |          15.3349 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0093 |          37.1401 |          15.3514 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0139 |          36.9400 |          15.3142 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0130 |          36.5184 |          15.3282 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0110 |          36.1633 |          15.3232 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0140 |          35.9607 |          15.3294 |
[32m[20221213 22:58:00 @agent_ppo2.py:185][0m |          -0.0156 |          35.9116 |          15.3128 |
[32m[20221213 22:58:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:58:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.89
[32m[20221213 22:58:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 376.42
[32m[20221213 22:58:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.75
[32m[20221213 22:58:00 @agent_ppo2.py:143][0m Total time:      39.79 min
[32m[20221213 22:58:00 @agent_ppo2.py:145][0m 3895296 total steps have happened
[32m[20221213 22:58:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1902 --------------------------#
[32m[20221213 22:58:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |           0.0024 |          45.3086 |          15.6195 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0080 |          42.6394 |          15.6199 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0062 |          41.3341 |          15.6093 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0089 |          40.9269 |          15.5994 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0096 |          40.3300 |          15.5930 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0101 |          40.1837 |          15.5902 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0060 |          40.8499 |          15.6249 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0041 |          39.9007 |          15.6410 |
[32m[20221213 22:58:01 @agent_ppo2.py:185][0m |          -0.0061 |          39.9606 |          15.6298 |
[32m[20221213 22:58:02 @agent_ppo2.py:185][0m |          -0.0078 |          41.3184 |          15.6284 |
[32m[20221213 22:58:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:58:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.88
[32m[20221213 22:58:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.86
[32m[20221213 22:58:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.45
[32m[20221213 22:58:02 @agent_ppo2.py:143][0m Total time:      39.81 min
[32m[20221213 22:58:02 @agent_ppo2.py:145][0m 3897344 total steps have happened
[32m[20221213 22:58:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1903 --------------------------#
[32m[20221213 22:58:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:02 @agent_ppo2.py:185][0m |           0.0116 |          52.8442 |          15.4642 |
[32m[20221213 22:58:02 @agent_ppo2.py:185][0m |          -0.0031 |          48.8422 |          15.4480 |
[32m[20221213 22:58:02 @agent_ppo2.py:185][0m |          -0.0034 |          48.1109 |          15.4336 |
[32m[20221213 22:58:02 @agent_ppo2.py:185][0m |          -0.0075 |          47.7419 |          15.4199 |
[32m[20221213 22:58:02 @agent_ppo2.py:185][0m |          -0.0053 |          47.8808 |          15.4136 |
[32m[20221213 22:58:02 @agent_ppo2.py:185][0m |          -0.0056 |          47.2660 |          15.4004 |
[32m[20221213 22:58:03 @agent_ppo2.py:185][0m |          -0.0018 |          47.5250 |          15.3813 |
[32m[20221213 22:58:03 @agent_ppo2.py:185][0m |          -0.0049 |          46.8890 |          15.3897 |
[32m[20221213 22:58:03 @agent_ppo2.py:185][0m |          -0.0084 |          46.9143 |          15.3705 |
[32m[20221213 22:58:03 @agent_ppo2.py:185][0m |          -0.0113 |          46.7126 |          15.3631 |
[32m[20221213 22:58:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:58:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.27
[32m[20221213 22:58:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.40
[32m[20221213 22:58:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.69
[32m[20221213 22:58:03 @agent_ppo2.py:143][0m Total time:      39.84 min
[32m[20221213 22:58:03 @agent_ppo2.py:145][0m 3899392 total steps have happened
[32m[20221213 22:58:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1904 --------------------------#
[32m[20221213 22:58:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:58:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:03 @agent_ppo2.py:185][0m |           0.0049 |          45.9869 |          15.6222 |
[32m[20221213 22:58:03 @agent_ppo2.py:185][0m |          -0.0034 |          44.0294 |          15.5997 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0015 |          43.5151 |          15.5814 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0051 |          43.1539 |          15.5798 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0074 |          43.1267 |          15.5625 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0044 |          43.0382 |          15.5745 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0057 |          42.6976 |          15.5702 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0063 |          42.5072 |          15.5571 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0069 |          42.2802 |          15.5514 |
[32m[20221213 22:58:04 @agent_ppo2.py:185][0m |          -0.0021 |          43.4153 |          15.5323 |
[32m[20221213 22:58:04 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:58:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.47
[32m[20221213 22:58:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.81
[32m[20221213 22:58:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.60
[32m[20221213 22:58:04 @agent_ppo2.py:143][0m Total time:      39.86 min
[32m[20221213 22:58:04 @agent_ppo2.py:145][0m 3901440 total steps have happened
[32m[20221213 22:58:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1905 --------------------------#
[32m[20221213 22:58:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |           0.0043 |          46.4913 |          15.5584 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0047 |          43.7083 |          15.5467 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0022 |          43.5875 |          15.5464 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0113 |          42.4447 |          15.5360 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0045 |          43.1785 |          15.5205 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0099 |          41.8756 |          15.5192 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0072 |          41.6040 |          15.5157 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0146 |          41.4976 |          15.5009 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0116 |          41.3099 |          15.5048 |
[32m[20221213 22:58:05 @agent_ppo2.py:185][0m |          -0.0128 |          41.3102 |          15.4880 |
[32m[20221213 22:58:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:58:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.68
[32m[20221213 22:58:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 369.30
[32m[20221213 22:58:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.51
[32m[20221213 22:58:06 @agent_ppo2.py:143][0m Total time:      39.88 min
[32m[20221213 22:58:06 @agent_ppo2.py:145][0m 3903488 total steps have happened
[32m[20221213 22:58:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1906 --------------------------#
[32m[20221213 22:58:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:58:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:06 @agent_ppo2.py:185][0m |           0.0035 |          44.7123 |          15.3120 |
[32m[20221213 22:58:06 @agent_ppo2.py:185][0m |          -0.0046 |          41.7479 |          15.3169 |
[32m[20221213 22:58:06 @agent_ppo2.py:185][0m |          -0.0077 |          40.7647 |          15.3118 |
[32m[20221213 22:58:06 @agent_ppo2.py:185][0m |          -0.0078 |          39.9570 |          15.3284 |
[32m[20221213 22:58:06 @agent_ppo2.py:185][0m |          -0.0145 |          39.5921 |          15.3229 |
[32m[20221213 22:58:06 @agent_ppo2.py:185][0m |          -0.0132 |          39.3131 |          15.3149 |
[32m[20221213 22:58:06 @agent_ppo2.py:185][0m |          -0.0116 |          38.7492 |          15.3094 |
[32m[20221213 22:58:07 @agent_ppo2.py:185][0m |          -0.0011 |          40.8095 |          15.3215 |
[32m[20221213 22:58:07 @agent_ppo2.py:185][0m |          -0.0139 |          38.4343 |          15.3172 |
[32m[20221213 22:58:07 @agent_ppo2.py:185][0m |          -0.0150 |          38.1096 |          15.3056 |
[32m[20221213 22:58:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:58:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.90
[32m[20221213 22:58:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.53
[32m[20221213 22:58:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.06
[32m[20221213 22:58:07 @agent_ppo2.py:143][0m Total time:      39.90 min
[32m[20221213 22:58:07 @agent_ppo2.py:145][0m 3905536 total steps have happened
[32m[20221213 22:58:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1907 --------------------------#
[32m[20221213 22:58:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:58:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:07 @agent_ppo2.py:185][0m |          -0.0010 |          47.1351 |          15.4393 |
[32m[20221213 22:58:07 @agent_ppo2.py:185][0m |           0.0018 |          45.8881 |          15.4171 |
[32m[20221213 22:58:07 @agent_ppo2.py:185][0m |          -0.0071 |          43.1113 |          15.4125 |
[32m[20221213 22:58:07 @agent_ppo2.py:185][0m |          -0.0073 |          42.4777 |          15.4141 |
[32m[20221213 22:58:08 @agent_ppo2.py:185][0m |          -0.0006 |          43.0971 |          15.4038 |
[32m[20221213 22:58:08 @agent_ppo2.py:185][0m |          -0.0122 |          41.9080 |          15.4010 |
[32m[20221213 22:58:08 @agent_ppo2.py:185][0m |          -0.0120 |          41.5176 |          15.3997 |
[32m[20221213 22:58:08 @agent_ppo2.py:185][0m |          -0.0131 |          41.3939 |          15.3953 |
[32m[20221213 22:58:08 @agent_ppo2.py:185][0m |          -0.0139 |          41.1646 |          15.4016 |
[32m[20221213 22:58:08 @agent_ppo2.py:185][0m |          -0.0018 |          46.0361 |          15.3739 |
[32m[20221213 22:58:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.97
[32m[20221213 22:58:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.45
[32m[20221213 22:58:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.11
[32m[20221213 22:58:08 @agent_ppo2.py:143][0m Total time:      39.92 min
[32m[20221213 22:58:08 @agent_ppo2.py:145][0m 3907584 total steps have happened
[32m[20221213 22:58:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1908 --------------------------#
[32m[20221213 22:58:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:08 @agent_ppo2.py:185][0m |           0.0088 |          45.2036 |          15.3654 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |          -0.0040 |          42.0877 |          15.3877 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |           0.0035 |          43.0470 |          15.3690 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |          -0.0012 |          41.6062 |          15.3926 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |           0.0001 |          44.5554 |          15.3810 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |          -0.0059 |          41.3037 |          15.3888 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |          -0.0091 |          41.0937 |          15.3995 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |          -0.0003 |          44.2440 |          15.3859 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |          -0.0068 |          41.0497 |          15.3984 |
[32m[20221213 22:58:09 @agent_ppo2.py:185][0m |          -0.0123 |          40.8449 |          15.3964 |
[32m[20221213 22:58:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.65
[32m[20221213 22:58:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.66
[32m[20221213 22:58:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.16
[32m[20221213 22:58:09 @agent_ppo2.py:143][0m Total time:      39.94 min
[32m[20221213 22:58:09 @agent_ppo2.py:145][0m 3909632 total steps have happened
[32m[20221213 22:58:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1909 --------------------------#
[32m[20221213 22:58:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0004 |          51.0196 |          15.5814 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0029 |          45.6909 |          15.5738 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0057 |          43.9247 |          15.5917 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0022 |          43.9041 |          15.5469 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |           0.0017 |          47.7064 |          15.5532 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |           0.0076 |          44.5661 |          15.5501 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0080 |          41.6965 |          15.5589 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0086 |          40.9086 |          15.5693 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0067 |          40.6322 |          15.5691 |
[32m[20221213 22:58:10 @agent_ppo2.py:185][0m |          -0.0075 |          40.5174 |          15.5537 |
[32m[20221213 22:58:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.95
[32m[20221213 22:58:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 358.21
[32m[20221213 22:58:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.80
[32m[20221213 22:58:11 @agent_ppo2.py:143][0m Total time:      39.96 min
[32m[20221213 22:58:11 @agent_ppo2.py:145][0m 3911680 total steps have happened
[32m[20221213 22:58:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1910 --------------------------#
[32m[20221213 22:58:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:58:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:11 @agent_ppo2.py:185][0m |           0.0022 |          46.8196 |          15.3141 |
[32m[20221213 22:58:11 @agent_ppo2.py:185][0m |          -0.0058 |          43.2333 |          15.3144 |
[32m[20221213 22:58:11 @agent_ppo2.py:185][0m |          -0.0066 |          42.7409 |          15.3075 |
[32m[20221213 22:58:11 @agent_ppo2.py:185][0m |          -0.0047 |          42.2180 |          15.3020 |
[32m[20221213 22:58:11 @agent_ppo2.py:185][0m |          -0.0085 |          41.8747 |          15.3082 |
[32m[20221213 22:58:11 @agent_ppo2.py:185][0m |           0.0007 |          44.9386 |          15.3111 |
[32m[20221213 22:58:11 @agent_ppo2.py:185][0m |          -0.0080 |          41.5751 |          15.2917 |
[32m[20221213 22:58:12 @agent_ppo2.py:185][0m |          -0.0079 |          41.6026 |          15.3171 |
[32m[20221213 22:58:12 @agent_ppo2.py:185][0m |          -0.0133 |          41.2972 |          15.2950 |
[32m[20221213 22:58:12 @agent_ppo2.py:185][0m |          -0.0103 |          41.1917 |          15.3040 |
[32m[20221213 22:58:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.51
[32m[20221213 22:58:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.11
[32m[20221213 22:58:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.40
[32m[20221213 22:58:12 @agent_ppo2.py:143][0m Total time:      39.98 min
[32m[20221213 22:58:12 @agent_ppo2.py:145][0m 3913728 total steps have happened
[32m[20221213 22:58:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1911 --------------------------#
[32m[20221213 22:58:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:12 @agent_ppo2.py:185][0m |           0.0027 |          44.1389 |          15.4161 |
[32m[20221213 22:58:12 @agent_ppo2.py:185][0m |          -0.0030 |          42.1667 |          15.3716 |
[32m[20221213 22:58:12 @agent_ppo2.py:185][0m |          -0.0057 |          41.2733 |          15.3877 |
[32m[20221213 22:58:12 @agent_ppo2.py:185][0m |          -0.0051 |          40.7728 |          15.3581 |
[32m[20221213 22:58:13 @agent_ppo2.py:185][0m |          -0.0007 |          40.6498 |          15.3624 |
[32m[20221213 22:58:13 @agent_ppo2.py:185][0m |          -0.0050 |          40.2234 |          15.3520 |
[32m[20221213 22:58:13 @agent_ppo2.py:185][0m |          -0.0072 |          39.7891 |          15.3482 |
[32m[20221213 22:58:13 @agent_ppo2.py:185][0m |          -0.0086 |          39.7035 |          15.3292 |
[32m[20221213 22:58:13 @agent_ppo2.py:185][0m |          -0.0124 |          39.5278 |          15.3238 |
[32m[20221213 22:58:13 @agent_ppo2.py:185][0m |          -0.0098 |          39.2570 |          15.3273 |
[32m[20221213 22:58:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:58:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.27
[32m[20221213 22:58:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.24
[32m[20221213 22:58:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.81
[32m[20221213 22:58:13 @agent_ppo2.py:143][0m Total time:      40.00 min
[32m[20221213 22:58:13 @agent_ppo2.py:145][0m 3915776 total steps have happened
[32m[20221213 22:58:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1912 --------------------------#
[32m[20221213 22:58:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:13 @agent_ppo2.py:185][0m |          -0.0055 |          34.5925 |          15.3320 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |           0.0061 |          33.0680 |          15.3341 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0048 |          30.4482 |          15.3268 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0063 |          29.8992 |          15.3191 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0106 |          29.5862 |          15.3045 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0072 |          29.3549 |          15.3095 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0123 |          29.0391 |          15.3085 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0101 |          28.8981 |          15.3032 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0158 |          28.8262 |          15.2893 |
[32m[20221213 22:58:14 @agent_ppo2.py:185][0m |          -0.0078 |          28.6783 |          15.2990 |
[32m[20221213 22:58:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.24
[32m[20221213 22:58:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.48
[32m[20221213 22:58:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.48
[32m[20221213 22:58:14 @agent_ppo2.py:143][0m Total time:      40.02 min
[32m[20221213 22:58:14 @agent_ppo2.py:145][0m 3917824 total steps have happened
[32m[20221213 22:58:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1913 --------------------------#
[32m[20221213 22:58:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |           0.0100 |          36.2979 |          15.5476 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0038 |          31.6386 |          15.5026 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0116 |          28.6002 |          15.5125 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0156 |          27.5753 |          15.5041 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0077 |          26.8563 |          15.4987 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0149 |          26.2666 |          15.4839 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0129 |          25.6261 |          15.4794 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0121 |          25.3653 |          15.4577 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0131 |          24.9673 |          15.4553 |
[32m[20221213 22:58:15 @agent_ppo2.py:185][0m |          -0.0178 |          24.5172 |          15.4599 |
[32m[20221213 22:58:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.85
[32m[20221213 22:58:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 391.59
[32m[20221213 22:58:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.06
[32m[20221213 22:58:16 @agent_ppo2.py:143][0m Total time:      40.05 min
[32m[20221213 22:58:16 @agent_ppo2.py:145][0m 3919872 total steps have happened
[32m[20221213 22:58:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1914 --------------------------#
[32m[20221213 22:58:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:58:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:16 @agent_ppo2.py:185][0m |          -0.0016 |          43.0817 |          15.5077 |
[32m[20221213 22:58:16 @agent_ppo2.py:185][0m |          -0.0071 |          40.0353 |          15.5019 |
[32m[20221213 22:58:16 @agent_ppo2.py:185][0m |          -0.0096 |          39.0187 |          15.4879 |
[32m[20221213 22:58:16 @agent_ppo2.py:185][0m |          -0.0087 |          37.9314 |          15.4750 |
[32m[20221213 22:58:16 @agent_ppo2.py:185][0m |          -0.0085 |          37.6097 |          15.4828 |
[32m[20221213 22:58:16 @agent_ppo2.py:185][0m |          -0.0076 |          36.7464 |          15.4751 |
[32m[20221213 22:58:16 @agent_ppo2.py:185][0m |          -0.0105 |          36.1504 |          15.4733 |
[32m[20221213 22:58:17 @agent_ppo2.py:185][0m |          -0.0103 |          36.1566 |          15.4646 |
[32m[20221213 22:58:17 @agent_ppo2.py:185][0m |          -0.0019 |          36.4609 |          15.4584 |
[32m[20221213 22:58:17 @agent_ppo2.py:185][0m |          -0.0124 |          35.2131 |          15.4898 |
[32m[20221213 22:58:17 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.93
[32m[20221213 22:58:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.03
[32m[20221213 22:58:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.28
[32m[20221213 22:58:17 @agent_ppo2.py:143][0m Total time:      40.07 min
[32m[20221213 22:58:17 @agent_ppo2.py:145][0m 3921920 total steps have happened
[32m[20221213 22:58:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1915 --------------------------#
[32m[20221213 22:58:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:17 @agent_ppo2.py:185][0m |           0.0020 |          50.1676 |          15.3505 |
[32m[20221213 22:58:17 @agent_ppo2.py:185][0m |          -0.0051 |          46.7233 |          15.3570 |
[32m[20221213 22:58:17 @agent_ppo2.py:185][0m |          -0.0064 |          45.3385 |          15.3385 |
[32m[20221213 22:58:17 @agent_ppo2.py:185][0m |          -0.0079 |          44.1853 |          15.3323 |
[32m[20221213 22:58:18 @agent_ppo2.py:185][0m |          -0.0116 |          43.9066 |          15.3411 |
[32m[20221213 22:58:18 @agent_ppo2.py:185][0m |          -0.0000 |          45.0811 |          15.3199 |
[32m[20221213 22:58:18 @agent_ppo2.py:185][0m |          -0.0119 |          42.7566 |          15.3294 |
[32m[20221213 22:58:18 @agent_ppo2.py:185][0m |          -0.0121 |          42.6018 |          15.3400 |
[32m[20221213 22:58:18 @agent_ppo2.py:185][0m |          -0.0111 |          42.5786 |          15.3463 |
[32m[20221213 22:58:18 @agent_ppo2.py:185][0m |          -0.0139 |          42.1353 |          15.3327 |
[32m[20221213 22:58:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.33
[32m[20221213 22:58:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.31
[32m[20221213 22:58:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.86
[32m[20221213 22:58:18 @agent_ppo2.py:143][0m Total time:      40.09 min
[32m[20221213 22:58:18 @agent_ppo2.py:145][0m 3923968 total steps have happened
[32m[20221213 22:58:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1916 --------------------------#
[32m[20221213 22:58:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:58:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:18 @agent_ppo2.py:185][0m |           0.0044 |          36.9346 |          15.3246 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0081 |          30.6161 |          15.3430 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0098 |          29.7437 |          15.3336 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0085 |          29.2308 |          15.3055 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0084 |          28.6673 |          15.3248 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0107 |          28.3750 |          15.3200 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0031 |          29.3335 |          15.3192 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0072 |          27.9373 |          15.3257 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0082 |          28.0409 |          15.3319 |
[32m[20221213 22:58:19 @agent_ppo2.py:185][0m |          -0.0156 |          27.7649 |          15.3213 |
[32m[20221213 22:58:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:58:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.69
[32m[20221213 22:58:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.28
[32m[20221213 22:58:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.61
[32m[20221213 22:58:19 @agent_ppo2.py:143][0m Total time:      40.11 min
[32m[20221213 22:58:19 @agent_ppo2.py:145][0m 3926016 total steps have happened
[32m[20221213 22:58:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1917 --------------------------#
[32m[20221213 22:58:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:58:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0032 |          42.8241 |          15.5598 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0061 |          39.3681 |          15.5266 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0052 |          38.0278 |          15.5282 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0105 |          37.0749 |          15.5099 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0087 |          36.4249 |          15.5134 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0116 |          35.8590 |          15.5081 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0120 |          35.4272 |          15.5037 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0118 |          35.0335 |          15.4918 |
[32m[20221213 22:58:20 @agent_ppo2.py:185][0m |          -0.0125 |          34.6932 |          15.4757 |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |          -0.0122 |          34.3538 |          15.4758 |
[32m[20221213 22:58:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.59
[32m[20221213 22:58:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.41
[32m[20221213 22:58:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.56
[32m[20221213 22:58:21 @agent_ppo2.py:143][0m Total time:      40.13 min
[32m[20221213 22:58:21 @agent_ppo2.py:145][0m 3928064 total steps have happened
[32m[20221213 22:58:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1918 --------------------------#
[32m[20221213 22:58:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |           0.0002 |          43.8723 |          15.4915 |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |          -0.0036 |          42.4983 |          15.4946 |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |           0.0011 |          43.9315 |          15.4922 |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |          -0.0098 |          41.8509 |          15.4575 |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |          -0.0083 |          41.6918 |          15.4680 |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |          -0.0106 |          41.6009 |          15.4563 |
[32m[20221213 22:58:21 @agent_ppo2.py:185][0m |          -0.0041 |          42.2535 |          15.4547 |
[32m[20221213 22:58:22 @agent_ppo2.py:185][0m |          -0.0096 |          41.3288 |          15.4699 |
[32m[20221213 22:58:22 @agent_ppo2.py:185][0m |          -0.0104 |          41.2027 |          15.4709 |
[32m[20221213 22:58:22 @agent_ppo2.py:185][0m |          -0.0123 |          41.1345 |          15.4586 |
[32m[20221213 22:58:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.10
[32m[20221213 22:58:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.88
[32m[20221213 22:58:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.07
[32m[20221213 22:58:22 @agent_ppo2.py:143][0m Total time:      40.15 min
[32m[20221213 22:58:22 @agent_ppo2.py:145][0m 3930112 total steps have happened
[32m[20221213 22:58:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1919 --------------------------#
[32m[20221213 22:58:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:22 @agent_ppo2.py:185][0m |           0.0008 |          47.3949 |          15.2907 |
[32m[20221213 22:58:22 @agent_ppo2.py:185][0m |           0.0052 |          50.7643 |          15.2566 |
[32m[20221213 22:58:22 @agent_ppo2.py:185][0m |          -0.0026 |          46.4901 |          15.2875 |
[32m[20221213 22:58:22 @agent_ppo2.py:185][0m |          -0.0102 |          45.5932 |          15.2866 |
[32m[20221213 22:58:23 @agent_ppo2.py:185][0m |          -0.0057 |          46.3478 |          15.2773 |
[32m[20221213 22:58:23 @agent_ppo2.py:185][0m |          -0.0040 |          46.9166 |          15.2409 |
[32m[20221213 22:58:23 @agent_ppo2.py:185][0m |          -0.0099 |          45.1635 |          15.2795 |
[32m[20221213 22:58:23 @agent_ppo2.py:185][0m |           0.0041 |          50.1790 |          15.2779 |
[32m[20221213 22:58:23 @agent_ppo2.py:185][0m |          -0.0077 |          45.5853 |          15.2419 |
[32m[20221213 22:58:23 @agent_ppo2.py:185][0m |          -0.0115 |          44.9995 |          15.2633 |
[32m[20221213 22:58:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.00
[32m[20221213 22:58:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.40
[32m[20221213 22:58:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.71
[32m[20221213 22:58:23 @agent_ppo2.py:143][0m Total time:      40.17 min
[32m[20221213 22:58:23 @agent_ppo2.py:145][0m 3932160 total steps have happened
[32m[20221213 22:58:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1920 --------------------------#
[32m[20221213 22:58:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:58:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:23 @agent_ppo2.py:185][0m |           0.0005 |          26.6528 |          15.3201 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0119 |          22.0254 |          15.3176 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0046 |          21.2960 |          15.3344 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0079 |          20.8410 |          15.3388 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0043 |          20.9711 |          15.3429 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0088 |          20.5789 |          15.3350 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0119 |          20.3127 |          15.3305 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0033 |          20.1020 |          15.3490 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0174 |          19.9484 |          15.3465 |
[32m[20221213 22:58:24 @agent_ppo2.py:185][0m |          -0.0173 |          19.7929 |          15.3657 |
[32m[20221213 22:58:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.15
[32m[20221213 22:58:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.75
[32m[20221213 22:58:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.53
[32m[20221213 22:58:24 @agent_ppo2.py:143][0m Total time:      40.19 min
[32m[20221213 22:58:24 @agent_ppo2.py:145][0m 3934208 total steps have happened
[32m[20221213 22:58:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1921 --------------------------#
[32m[20221213 22:58:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:58:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0007 |          31.8105 |          15.4109 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0039 |          29.0096 |          15.4099 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0080 |          28.4385 |          15.3892 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0125 |          28.0812 |          15.3853 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0092 |          27.9049 |          15.3793 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0074 |          27.7215 |          15.3773 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0073 |          28.8559 |          15.3757 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0135 |          27.4977 |          15.3613 |
[32m[20221213 22:58:25 @agent_ppo2.py:185][0m |          -0.0107 |          27.7611 |          15.3710 |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |          -0.0114 |          27.3479 |          15.3774 |
[32m[20221213 22:58:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:58:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.08
[32m[20221213 22:58:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.83
[32m[20221213 22:58:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.68
[32m[20221213 22:58:26 @agent_ppo2.py:143][0m Total time:      40.21 min
[32m[20221213 22:58:26 @agent_ppo2.py:145][0m 3936256 total steps have happened
[32m[20221213 22:58:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1922 --------------------------#
[32m[20221213 22:58:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |          -0.0018 |          27.2418 |          15.3454 |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |          -0.0078 |          25.3378 |          15.3299 |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |           0.0092 |          29.5505 |          15.3425 |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |          -0.0077 |          24.5794 |          15.3460 |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |          -0.0061 |          24.2177 |          15.3394 |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |          -0.0050 |          24.1010 |          15.3222 |
[32m[20221213 22:58:26 @agent_ppo2.py:185][0m |          -0.0007 |          25.3744 |          15.3269 |
[32m[20221213 22:58:27 @agent_ppo2.py:185][0m |          -0.0054 |          23.8775 |          15.3334 |
[32m[20221213 22:58:27 @agent_ppo2.py:185][0m |          -0.0117 |          23.5382 |          15.3420 |
[32m[20221213 22:58:27 @agent_ppo2.py:185][0m |          -0.0135 |          23.4259 |          15.3486 |
[32m[20221213 22:58:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.43
[32m[20221213 22:58:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.76
[32m[20221213 22:58:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.02
[32m[20221213 22:58:27 @agent_ppo2.py:143][0m Total time:      40.23 min
[32m[20221213 22:58:27 @agent_ppo2.py:145][0m 3938304 total steps have happened
[32m[20221213 22:58:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1923 --------------------------#
[32m[20221213 22:58:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:27 @agent_ppo2.py:185][0m |          -0.0026 |          44.9956 |          15.1962 |
[32m[20221213 22:58:27 @agent_ppo2.py:185][0m |          -0.0079 |          43.7693 |          15.1908 |
[32m[20221213 22:58:27 @agent_ppo2.py:185][0m |          -0.0090 |          42.8957 |          15.2159 |
[32m[20221213 22:58:27 @agent_ppo2.py:185][0m |          -0.0087 |          42.4768 |          15.2056 |
[32m[20221213 22:58:28 @agent_ppo2.py:185][0m |          -0.0125 |          42.0628 |          15.2249 |
[32m[20221213 22:58:28 @agent_ppo2.py:185][0m |          -0.0084 |          41.7264 |          15.2223 |
[32m[20221213 22:58:28 @agent_ppo2.py:185][0m |          -0.0113 |          41.5476 |          15.2324 |
[32m[20221213 22:58:28 @agent_ppo2.py:185][0m |          -0.0120 |          41.2586 |          15.2488 |
[32m[20221213 22:58:28 @agent_ppo2.py:185][0m |          -0.0129 |          41.3220 |          15.2529 |
[32m[20221213 22:58:28 @agent_ppo2.py:185][0m |          -0.0068 |          43.9151 |          15.2770 |
[32m[20221213 22:58:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:58:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.69
[32m[20221213 22:58:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.97
[32m[20221213 22:58:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.85
[32m[20221213 22:58:28 @agent_ppo2.py:143][0m Total time:      40.25 min
[32m[20221213 22:58:28 @agent_ppo2.py:145][0m 3940352 total steps have happened
[32m[20221213 22:58:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1924 --------------------------#
[32m[20221213 22:58:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:58:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:28 @agent_ppo2.py:185][0m |          -0.0012 |          13.9282 |          15.4374 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |           0.0012 |          10.4840 |          15.4377 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |          -0.0045 |          10.0176 |          15.4361 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |          -0.0031 |           9.9988 |          15.4344 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |          -0.0054 |           9.9033 |          15.4482 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |          -0.0054 |           9.8857 |          15.4309 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |          -0.0023 |           9.8765 |          15.4426 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |          -0.0049 |           9.9312 |          15.4406 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |          -0.0024 |           9.8078 |          15.4474 |
[32m[20221213 22:58:29 @agent_ppo2.py:185][0m |           0.0022 |           9.8896 |          15.4478 |
[32m[20221213 22:58:29 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:58:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:58:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:58:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.66
[32m[20221213 22:58:29 @agent_ppo2.py:143][0m Total time:      40.27 min
[32m[20221213 22:58:29 @agent_ppo2.py:145][0m 3942400 total steps have happened
[32m[20221213 22:58:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1925 --------------------------#
[32m[20221213 22:58:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0004 |          50.1922 |          15.4028 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0005 |          48.8089 |          15.3955 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0052 |          47.9680 |          15.3752 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0034 |          49.1095 |          15.3979 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0089 |          47.2841 |          15.3926 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0052 |          48.6429 |          15.3735 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0051 |          46.9999 |          15.3974 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0115 |          46.8324 |          15.3878 |
[32m[20221213 22:58:30 @agent_ppo2.py:185][0m |          -0.0139 |          46.8199 |          15.3950 |
[32m[20221213 22:58:31 @agent_ppo2.py:185][0m |          -0.0123 |          46.6711 |          15.3762 |
[32m[20221213 22:58:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:58:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.14
[32m[20221213 22:58:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.07
[32m[20221213 22:58:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.06
[32m[20221213 22:58:31 @agent_ppo2.py:143][0m Total time:      40.30 min
[32m[20221213 22:58:31 @agent_ppo2.py:145][0m 3944448 total steps have happened
[32m[20221213 22:58:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1926 --------------------------#
[32m[20221213 22:58:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:31 @agent_ppo2.py:185][0m |           0.0073 |          22.5857 |          15.2886 |
[32m[20221213 22:58:31 @agent_ppo2.py:185][0m |          -0.0046 |          20.1972 |          15.3007 |
[32m[20221213 22:58:31 @agent_ppo2.py:185][0m |          -0.0035 |          19.7015 |          15.2822 |
[32m[20221213 22:58:31 @agent_ppo2.py:185][0m |          -0.0069 |          19.3145 |          15.2861 |
[32m[20221213 22:58:31 @agent_ppo2.py:185][0m |          -0.0040 |          19.1517 |          15.2590 |
[32m[20221213 22:58:31 @agent_ppo2.py:185][0m |          -0.0113 |          18.9534 |          15.2581 |
[32m[20221213 22:58:32 @agent_ppo2.py:185][0m |          -0.0095 |          18.8857 |          15.2585 |
[32m[20221213 22:58:32 @agent_ppo2.py:185][0m |          -0.0095 |          18.7816 |          15.2465 |
[32m[20221213 22:58:32 @agent_ppo2.py:185][0m |          -0.0018 |          19.9903 |          15.2314 |
[32m[20221213 22:58:32 @agent_ppo2.py:185][0m |          -0.0039 |          18.5306 |          15.2364 |
[32m[20221213 22:58:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:58:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.36
[32m[20221213 22:58:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.87
[32m[20221213 22:58:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 408.44
[32m[20221213 22:58:32 @agent_ppo2.py:143][0m Total time:      40.32 min
[32m[20221213 22:58:32 @agent_ppo2.py:145][0m 3946496 total steps have happened
[32m[20221213 22:58:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1927 --------------------------#
[32m[20221213 22:58:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:32 @agent_ppo2.py:185][0m |          -0.0001 |          46.4580 |          15.2680 |
[32m[20221213 22:58:32 @agent_ppo2.py:185][0m |          -0.0006 |          46.5010 |          15.2805 |
[32m[20221213 22:58:32 @agent_ppo2.py:185][0m |          -0.0036 |          45.2497 |          15.2502 |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |          -0.0054 |          45.1656 |          15.2765 |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |          -0.0078 |          44.9755 |          15.2737 |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |           0.0007 |          46.3634 |          15.2697 |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |          -0.0068 |          44.8873 |          15.2435 |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |          -0.0040 |          44.5152 |          15.2681 |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |          -0.0087 |          44.2777 |          15.2493 |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |          -0.0093 |          44.2558 |          15.2541 |
[32m[20221213 22:58:33 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.36
[32m[20221213 22:58:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.46
[32m[20221213 22:58:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.17
[32m[20221213 22:58:33 @agent_ppo2.py:143][0m Total time:      40.34 min
[32m[20221213 22:58:33 @agent_ppo2.py:145][0m 3948544 total steps have happened
[32m[20221213 22:58:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1928 --------------------------#
[32m[20221213 22:58:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:33 @agent_ppo2.py:185][0m |          -0.0031 |          40.0819 |          15.3925 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0010 |          39.5204 |          15.3623 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |           0.0031 |          41.0442 |          15.3741 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0098 |          36.6965 |          15.3409 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0127 |          36.4106 |          15.3524 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0108 |          36.2203 |          15.3331 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0126 |          36.0895 |          15.3297 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0120 |          36.2163 |          15.3242 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0021 |          35.8293 |          15.3073 |
[32m[20221213 22:58:34 @agent_ppo2.py:185][0m |          -0.0117 |          35.8605 |          15.3067 |
[32m[20221213 22:58:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.59
[32m[20221213 22:58:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.51
[32m[20221213 22:58:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 22:58:34 @agent_ppo2.py:143][0m Total time:      40.36 min
[32m[20221213 22:58:34 @agent_ppo2.py:145][0m 3950592 total steps have happened
[32m[20221213 22:58:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1929 --------------------------#
[32m[20221213 22:58:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |          -0.0003 |          47.1870 |          15.2521 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |          -0.0066 |          45.5051 |          15.2584 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |           0.0001 |          46.4531 |          15.2389 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |          -0.0077 |          44.7419 |          15.2424 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |          -0.0091 |          44.4997 |          15.2558 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |           0.0048 |          50.1102 |          15.2515 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |          -0.0071 |          44.2601 |          15.2453 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |          -0.0141 |          43.9102 |          15.2385 |
[32m[20221213 22:58:35 @agent_ppo2.py:185][0m |          -0.0114 |          43.7952 |          15.2510 |
[32m[20221213 22:58:36 @agent_ppo2.py:185][0m |          -0.0149 |          43.7094 |          15.2524 |
[32m[20221213 22:58:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.46
[32m[20221213 22:58:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.70
[32m[20221213 22:58:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.38
[32m[20221213 22:58:36 @agent_ppo2.py:143][0m Total time:      40.38 min
[32m[20221213 22:58:36 @agent_ppo2.py:145][0m 3952640 total steps have happened
[32m[20221213 22:58:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1930 --------------------------#
[32m[20221213 22:58:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:58:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:36 @agent_ppo2.py:185][0m |          -0.0006 |          56.0073 |          15.4305 |
[32m[20221213 22:58:36 @agent_ppo2.py:185][0m |          -0.0040 |          53.9380 |          15.4568 |
[32m[20221213 22:58:36 @agent_ppo2.py:185][0m |          -0.0031 |          54.3782 |          15.4369 |
[32m[20221213 22:58:36 @agent_ppo2.py:185][0m |          -0.0075 |          53.1879 |          15.4339 |
[32m[20221213 22:58:36 @agent_ppo2.py:185][0m |          -0.0027 |          52.9360 |          15.4487 |
[32m[20221213 22:58:36 @agent_ppo2.py:185][0m |          -0.0074 |          52.2563 |          15.4504 |
[32m[20221213 22:58:37 @agent_ppo2.py:185][0m |          -0.0071 |          52.1163 |          15.4443 |
[32m[20221213 22:58:37 @agent_ppo2.py:185][0m |          -0.0103 |          52.0378 |          15.4667 |
[32m[20221213 22:58:37 @agent_ppo2.py:185][0m |          -0.0105 |          51.7794 |          15.4389 |
[32m[20221213 22:58:37 @agent_ppo2.py:185][0m |          -0.0114 |          52.2134 |          15.4585 |
[32m[20221213 22:58:37 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:58:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.22
[32m[20221213 22:58:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 366.37
[32m[20221213 22:58:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.05
[32m[20221213 22:58:37 @agent_ppo2.py:143][0m Total time:      40.40 min
[32m[20221213 22:58:37 @agent_ppo2.py:145][0m 3954688 total steps have happened
[32m[20221213 22:58:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1931 --------------------------#
[32m[20221213 22:58:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:37 @agent_ppo2.py:185][0m |          -0.0032 |          45.2087 |          15.2135 |
[32m[20221213 22:58:37 @agent_ppo2.py:185][0m |          -0.0061 |          43.6464 |          15.2056 |
[32m[20221213 22:58:37 @agent_ppo2.py:185][0m |          -0.0087 |          43.0660 |          15.1812 |
[32m[20221213 22:58:38 @agent_ppo2.py:185][0m |          -0.0073 |          42.8419 |          15.1800 |
[32m[20221213 22:58:38 @agent_ppo2.py:185][0m |          -0.0114 |          42.5344 |          15.1846 |
[32m[20221213 22:58:38 @agent_ppo2.py:185][0m |          -0.0089 |          42.3041 |          15.1698 |
[32m[20221213 22:58:38 @agent_ppo2.py:185][0m |          -0.0107 |          42.2281 |          15.1823 |
[32m[20221213 22:58:38 @agent_ppo2.py:185][0m |          -0.0093 |          42.2014 |          15.1750 |
[32m[20221213 22:58:38 @agent_ppo2.py:185][0m |          -0.0142 |          41.9767 |          15.1774 |
[32m[20221213 22:58:38 @agent_ppo2.py:185][0m |          -0.0081 |          42.2667 |          15.1697 |
[32m[20221213 22:58:38 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.72
[32m[20221213 22:58:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.70
[32m[20221213 22:58:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 442.15
[32m[20221213 22:58:38 @agent_ppo2.py:143][0m Total time:      40.42 min
[32m[20221213 22:58:38 @agent_ppo2.py:145][0m 3956736 total steps have happened
[32m[20221213 22:58:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1932 --------------------------#
[32m[20221213 22:58:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0009 |          43.5715 |          15.3760 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0022 |          42.6038 |          15.3819 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |           0.0001 |          42.9861 |          15.4050 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0034 |          42.1657 |          15.3712 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0084 |          42.2591 |          15.3538 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0071 |          42.0454 |          15.3909 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0102 |          41.9363 |          15.3853 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0075 |          42.0373 |          15.3454 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0007 |          42.7915 |          15.3914 |
[32m[20221213 22:58:39 @agent_ppo2.py:185][0m |          -0.0036 |          42.3487 |          15.4051 |
[32m[20221213 22:58:39 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.30
[32m[20221213 22:58:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.89
[32m[20221213 22:58:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.86
[32m[20221213 22:58:39 @agent_ppo2.py:143][0m Total time:      40.44 min
[32m[20221213 22:58:39 @agent_ppo2.py:145][0m 3958784 total steps have happened
[32m[20221213 22:58:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1933 --------------------------#
[32m[20221213 22:58:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0015 |          47.7527 |          15.2873 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0063 |          45.9319 |          15.2650 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0089 |          45.1753 |          15.2580 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0107 |          44.6934 |          15.2708 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0107 |          44.3589 |          15.2751 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0034 |          45.4051 |          15.2933 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0132 |          43.8594 |          15.2832 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |           0.0096 |          48.7663 |          15.2945 |
[32m[20221213 22:58:40 @agent_ppo2.py:185][0m |          -0.0118 |          43.6733 |          15.2805 |
[32m[20221213 22:58:41 @agent_ppo2.py:185][0m |          -0.0064 |          43.5362 |          15.3070 |
[32m[20221213 22:58:41 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.50
[32m[20221213 22:58:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.12
[32m[20221213 22:58:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.65
[32m[20221213 22:58:41 @agent_ppo2.py:143][0m Total time:      40.46 min
[32m[20221213 22:58:41 @agent_ppo2.py:145][0m 3960832 total steps have happened
[32m[20221213 22:58:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1934 --------------------------#
[32m[20221213 22:58:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:41 @agent_ppo2.py:185][0m |          -0.0006 |          30.8234 |          15.5001 |
[32m[20221213 22:58:41 @agent_ppo2.py:185][0m |           0.0001 |          26.4814 |          15.4979 |
[32m[20221213 22:58:41 @agent_ppo2.py:185][0m |          -0.0038 |          25.0344 |          15.4945 |
[32m[20221213 22:58:41 @agent_ppo2.py:185][0m |          -0.0047 |          24.6195 |          15.4906 |
[32m[20221213 22:58:41 @agent_ppo2.py:185][0m |          -0.0053 |          23.9954 |          15.4703 |
[32m[20221213 22:58:41 @agent_ppo2.py:185][0m |          -0.0099 |          23.6356 |          15.4627 |
[32m[20221213 22:58:42 @agent_ppo2.py:185][0m |          -0.0077 |          23.2741 |          15.4577 |
[32m[20221213 22:58:42 @agent_ppo2.py:185][0m |          -0.0140 |          23.3335 |          15.4485 |
[32m[20221213 22:58:42 @agent_ppo2.py:185][0m |          -0.0082 |          23.0120 |          15.4503 |
[32m[20221213 22:58:42 @agent_ppo2.py:185][0m |          -0.0124 |          22.6562 |          15.4222 |
[32m[20221213 22:58:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.31
[32m[20221213 22:58:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.78
[32m[20221213 22:58:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 347.29
[32m[20221213 22:58:42 @agent_ppo2.py:143][0m Total time:      40.48 min
[32m[20221213 22:58:42 @agent_ppo2.py:145][0m 3962880 total steps have happened
[32m[20221213 22:58:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1935 --------------------------#
[32m[20221213 22:58:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:42 @agent_ppo2.py:185][0m |           0.0051 |          46.3651 |          15.4073 |
[32m[20221213 22:58:42 @agent_ppo2.py:185][0m |          -0.0030 |          42.2654 |          15.3837 |
[32m[20221213 22:58:42 @agent_ppo2.py:185][0m |          -0.0028 |          41.2635 |          15.4169 |
[32m[20221213 22:58:43 @agent_ppo2.py:185][0m |          -0.0066 |          40.5029 |          15.3898 |
[32m[20221213 22:58:43 @agent_ppo2.py:185][0m |          -0.0055 |          40.0130 |          15.3973 |
[32m[20221213 22:58:43 @agent_ppo2.py:185][0m |          -0.0059 |          39.5508 |          15.3741 |
[32m[20221213 22:58:43 @agent_ppo2.py:185][0m |          -0.0064 |          39.2206 |          15.3955 |
[32m[20221213 22:58:43 @agent_ppo2.py:185][0m |          -0.0059 |          38.9111 |          15.3772 |
[32m[20221213 22:58:43 @agent_ppo2.py:185][0m |          -0.0052 |          38.8533 |          15.3802 |
[32m[20221213 22:58:43 @agent_ppo2.py:185][0m |          -0.0092 |          38.4727 |          15.3760 |
[32m[20221213 22:58:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:58:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.06
[32m[20221213 22:58:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.88
[32m[20221213 22:58:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.76
[32m[20221213 22:58:43 @agent_ppo2.py:143][0m Total time:      40.50 min
[32m[20221213 22:58:43 @agent_ppo2.py:145][0m 3964928 total steps have happened
[32m[20221213 22:58:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1936 --------------------------#
[32m[20221213 22:58:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |           0.0037 |          26.6811 |          15.3689 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |           0.0045 |          21.9086 |          15.3593 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |           0.0016 |          20.8978 |          15.3667 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |          -0.0030 |          20.4185 |          15.3552 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |          -0.0087 |          20.0281 |          15.3453 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |          -0.0058 |          19.6462 |          15.3505 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |          -0.0101 |          19.3610 |          15.3597 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |          -0.0139 |          19.3020 |          15.3515 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |          -0.0058 |          19.3038 |          15.3379 |
[32m[20221213 22:58:44 @agent_ppo2.py:185][0m |          -0.0079 |          19.0291 |          15.3369 |
[32m[20221213 22:58:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.45
[32m[20221213 22:58:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.49
[32m[20221213 22:58:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.31
[32m[20221213 22:58:44 @agent_ppo2.py:143][0m Total time:      40.53 min
[32m[20221213 22:58:44 @agent_ppo2.py:145][0m 3966976 total steps have happened
[32m[20221213 22:58:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1937 --------------------------#
[32m[20221213 22:58:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |           0.0048 |          28.1476 |          15.2391 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |           0.0011 |          25.7284 |          15.2436 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |          -0.0056 |          24.1514 |          15.2213 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |          -0.0129 |          23.7659 |          15.2245 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |          -0.0119 |          23.2472 |          15.2373 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |          -0.0106 |          23.0685 |          15.2256 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |          -0.0090 |          22.7402 |          15.2235 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |          -0.0142 |          22.5266 |          15.2104 |
[32m[20221213 22:58:45 @agent_ppo2.py:185][0m |          -0.0130 |          22.4598 |          15.2251 |
[32m[20221213 22:58:46 @agent_ppo2.py:185][0m |          -0.0141 |          22.2551 |          15.2220 |
[32m[20221213 22:58:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.25
[32m[20221213 22:58:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 396.28
[32m[20221213 22:58:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 356.35
[32m[20221213 22:58:46 @agent_ppo2.py:143][0m Total time:      40.55 min
[32m[20221213 22:58:46 @agent_ppo2.py:145][0m 3969024 total steps have happened
[32m[20221213 22:58:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1938 --------------------------#
[32m[20221213 22:58:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:46 @agent_ppo2.py:185][0m |           0.0008 |          44.1354 |          15.5277 |
[32m[20221213 22:58:46 @agent_ppo2.py:185][0m |          -0.0036 |          40.5588 |          15.5040 |
[32m[20221213 22:58:46 @agent_ppo2.py:185][0m |          -0.0044 |          39.5690 |          15.5061 |
[32m[20221213 22:58:46 @agent_ppo2.py:185][0m |          -0.0060 |          38.9748 |          15.4979 |
[32m[20221213 22:58:46 @agent_ppo2.py:185][0m |          -0.0081 |          38.2413 |          15.5128 |
[32m[20221213 22:58:46 @agent_ppo2.py:185][0m |          -0.0079 |          37.9917 |          15.5179 |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |          -0.0095 |          37.7608 |          15.5211 |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |          -0.0104 |          37.2572 |          15.5252 |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |          -0.0045 |          37.5603 |          15.5305 |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |          -0.0085 |          36.7114 |          15.5349 |
[32m[20221213 22:58:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.28
[32m[20221213 22:58:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.46
[32m[20221213 22:58:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 234.53
[32m[20221213 22:58:47 @agent_ppo2.py:143][0m Total time:      40.57 min
[32m[20221213 22:58:47 @agent_ppo2.py:145][0m 3971072 total steps have happened
[32m[20221213 22:58:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1939 --------------------------#
[32m[20221213 22:58:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |           0.0025 |          37.7804 |          15.4512 |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |          -0.0070 |          33.1825 |          15.4349 |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |          -0.0067 |          32.1957 |          15.4363 |
[32m[20221213 22:58:47 @agent_ppo2.py:185][0m |          -0.0140 |          31.7460 |          15.4275 |
[32m[20221213 22:58:48 @agent_ppo2.py:185][0m |          -0.0076 |          31.6405 |          15.4379 |
[32m[20221213 22:58:48 @agent_ppo2.py:185][0m |          -0.0132 |          31.2785 |          15.4302 |
[32m[20221213 22:58:48 @agent_ppo2.py:185][0m |          -0.0140 |          30.9685 |          15.4423 |
[32m[20221213 22:58:48 @agent_ppo2.py:185][0m |          -0.0144 |          30.8217 |          15.4374 |
[32m[20221213 22:58:48 @agent_ppo2.py:185][0m |          -0.0111 |          30.6914 |          15.4184 |
[32m[20221213 22:58:48 @agent_ppo2.py:185][0m |          -0.0139 |          30.5157 |          15.4226 |
[32m[20221213 22:58:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.21
[32m[20221213 22:58:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 351.07
[32m[20221213 22:58:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.59
[32m[20221213 22:58:48 @agent_ppo2.py:143][0m Total time:      40.59 min
[32m[20221213 22:58:48 @agent_ppo2.py:145][0m 3973120 total steps have happened
[32m[20221213 22:58:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1940 --------------------------#
[32m[20221213 22:58:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:58:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:48 @agent_ppo2.py:185][0m |           0.0039 |          23.7897 |          15.1507 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0053 |          20.7815 |          15.1236 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0059 |          19.8347 |          15.0977 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0063 |          19.2474 |          15.1059 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0104 |          18.8023 |          15.0967 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0091 |          18.4554 |          15.0797 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0135 |          18.0973 |          15.0736 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0152 |          17.9065 |          15.0756 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0115 |          17.8353 |          15.0698 |
[32m[20221213 22:58:49 @agent_ppo2.py:185][0m |          -0.0128 |          17.7899 |          15.0713 |
[32m[20221213 22:58:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:58:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.62
[32m[20221213 22:58:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 395.22
[32m[20221213 22:58:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.46
[32m[20221213 22:58:49 @agent_ppo2.py:143][0m Total time:      40.61 min
[32m[20221213 22:58:49 @agent_ppo2.py:145][0m 3975168 total steps have happened
[32m[20221213 22:58:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1941 --------------------------#
[32m[20221213 22:58:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |           0.0024 |          36.2366 |          15.0815 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |          -0.0072 |          34.0297 |          15.0846 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |          -0.0046 |          32.9106 |          15.0886 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |           0.0035 |          37.0858 |          15.0772 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |          -0.0110 |          32.0982 |          15.0720 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |          -0.0050 |          31.9541 |          15.0726 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |          -0.0075 |          31.2152 |          15.0914 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |          -0.0016 |          31.3605 |          15.0658 |
[32m[20221213 22:58:50 @agent_ppo2.py:185][0m |          -0.0125 |          30.7944 |          15.0774 |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |          -0.0096 |          30.6250 |          15.0720 |
[32m[20221213 22:58:51 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.66
[32m[20221213 22:58:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 397.54
[32m[20221213 22:58:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.34
[32m[20221213 22:58:51 @agent_ppo2.py:143][0m Total time:      40.63 min
[32m[20221213 22:58:51 @agent_ppo2.py:145][0m 3977216 total steps have happened
[32m[20221213 22:58:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1942 --------------------------#
[32m[20221213 22:58:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |           0.0000 |          37.0926 |          15.1073 |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |          -0.0060 |          33.1279 |          15.0987 |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |          -0.0058 |          31.8307 |          15.0900 |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |          -0.0104 |          31.0449 |          15.0882 |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |          -0.0111 |          30.5936 |          15.0880 |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |          -0.0063 |          31.3724 |          15.0948 |
[32m[20221213 22:58:51 @agent_ppo2.py:185][0m |          -0.0103 |          29.7088 |          15.0652 |
[32m[20221213 22:58:52 @agent_ppo2.py:185][0m |          -0.0178 |          29.2808 |          15.0677 |
[32m[20221213 22:58:52 @agent_ppo2.py:185][0m |          -0.0134 |          28.9322 |          15.0693 |
[32m[20221213 22:58:52 @agent_ppo2.py:185][0m |          -0.0175 |          28.7557 |          15.0639 |
[32m[20221213 22:58:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 324.29
[32m[20221213 22:58:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.65
[32m[20221213 22:58:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.14
[32m[20221213 22:58:52 @agent_ppo2.py:143][0m Total time:      40.65 min
[32m[20221213 22:58:52 @agent_ppo2.py:145][0m 3979264 total steps have happened
[32m[20221213 22:58:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1943 --------------------------#
[32m[20221213 22:58:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:52 @agent_ppo2.py:185][0m |           0.0028 |          25.4536 |          15.4050 |
[32m[20221213 22:58:52 @agent_ppo2.py:185][0m |          -0.0010 |          23.3856 |          15.3908 |
[32m[20221213 22:58:52 @agent_ppo2.py:185][0m |          -0.0091 |          22.5884 |          15.3740 |
[32m[20221213 22:58:52 @agent_ppo2.py:185][0m |          -0.0062 |          21.9425 |          15.3803 |
[32m[20221213 22:58:53 @agent_ppo2.py:185][0m |          -0.0103 |          21.5679 |          15.3685 |
[32m[20221213 22:58:53 @agent_ppo2.py:185][0m |          -0.0130 |          21.4149 |          15.3690 |
[32m[20221213 22:58:53 @agent_ppo2.py:185][0m |          -0.0130 |          20.9791 |          15.3612 |
[32m[20221213 22:58:53 @agent_ppo2.py:185][0m |          -0.0112 |          20.8480 |          15.3490 |
[32m[20221213 22:58:53 @agent_ppo2.py:185][0m |          -0.0020 |          21.3312 |          15.3461 |
[32m[20221213 22:58:53 @agent_ppo2.py:185][0m |          -0.0123 |          20.5735 |          15.3557 |
[32m[20221213 22:58:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.69
[32m[20221213 22:58:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.43
[32m[20221213 22:58:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.20
[32m[20221213 22:58:53 @agent_ppo2.py:143][0m Total time:      40.67 min
[32m[20221213 22:58:53 @agent_ppo2.py:145][0m 3981312 total steps have happened
[32m[20221213 22:58:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1944 --------------------------#
[32m[20221213 22:58:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:53 @agent_ppo2.py:185][0m |           0.0013 |          38.0434 |          15.2260 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0046 |          33.0526 |          15.2481 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0039 |          31.9853 |          15.2257 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0076 |          31.0755 |          15.2332 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |           0.0012 |          32.9531 |          15.2490 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0106 |          30.8216 |          15.2183 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0042 |          32.0719 |          15.2671 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0124 |          30.0971 |          15.2482 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0135 |          29.8885 |          15.2584 |
[32m[20221213 22:58:54 @agent_ppo2.py:185][0m |          -0.0169 |          29.6902 |          15.2553 |
[32m[20221213 22:58:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.35
[32m[20221213 22:58:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.64
[32m[20221213 22:58:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.83
[32m[20221213 22:58:54 @agent_ppo2.py:143][0m Total time:      40.69 min
[32m[20221213 22:58:54 @agent_ppo2.py:145][0m 3983360 total steps have happened
[32m[20221213 22:58:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1945 --------------------------#
[32m[20221213 22:58:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |           0.0020 |          44.3220 |          15.0634 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |          -0.0077 |          42.9190 |          15.0505 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |           0.0003 |          43.0630 |          15.0572 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |          -0.0082 |          42.1036 |          15.0647 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |          -0.0096 |          41.7566 |          15.0628 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |          -0.0097 |          41.5623 |          15.0924 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |          -0.0141 |          41.2621 |          15.0891 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |          -0.0100 |          40.9900 |          15.0858 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |          -0.0102 |          43.0265 |          15.0918 |
[32m[20221213 22:58:55 @agent_ppo2.py:185][0m |           0.0022 |          45.4870 |          15.0745 |
[32m[20221213 22:58:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:58:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.74
[32m[20221213 22:58:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.52
[32m[20221213 22:58:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.88
[32m[20221213 22:58:56 @agent_ppo2.py:143][0m Total time:      40.71 min
[32m[20221213 22:58:56 @agent_ppo2.py:145][0m 3985408 total steps have happened
[32m[20221213 22:58:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1946 --------------------------#
[32m[20221213 22:58:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:56 @agent_ppo2.py:185][0m |          -0.0039 |          32.5671 |          15.2409 |
[32m[20221213 22:58:56 @agent_ppo2.py:185][0m |          -0.0070 |          26.6196 |          15.2109 |
[32m[20221213 22:58:56 @agent_ppo2.py:185][0m |          -0.0010 |          28.6208 |          15.1925 |
[32m[20221213 22:58:56 @agent_ppo2.py:185][0m |          -0.0024 |          26.8622 |          15.2075 |
[32m[20221213 22:58:56 @agent_ppo2.py:185][0m |          -0.0066 |          25.1057 |          15.1875 |
[32m[20221213 22:58:56 @agent_ppo2.py:185][0m |          -0.0135 |          24.7477 |          15.2182 |
[32m[20221213 22:58:56 @agent_ppo2.py:185][0m |          -0.0031 |          24.6251 |          15.2106 |
[32m[20221213 22:58:57 @agent_ppo2.py:185][0m |          -0.0125 |          24.4579 |          15.2073 |
[32m[20221213 22:58:57 @agent_ppo2.py:185][0m |          -0.0175 |          24.2904 |          15.1993 |
[32m[20221213 22:58:57 @agent_ppo2.py:185][0m |          -0.0171 |          24.0569 |          15.2074 |
[32m[20221213 22:58:57 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.95
[32m[20221213 22:58:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.19
[32m[20221213 22:58:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.34
[32m[20221213 22:58:57 @agent_ppo2.py:143][0m Total time:      40.73 min
[32m[20221213 22:58:57 @agent_ppo2.py:145][0m 3987456 total steps have happened
[32m[20221213 22:58:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1947 --------------------------#
[32m[20221213 22:58:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:57 @agent_ppo2.py:185][0m |           0.0030 |          42.6044 |          15.3911 |
[32m[20221213 22:58:57 @agent_ppo2.py:185][0m |          -0.0072 |          37.8402 |          15.3801 |
[32m[20221213 22:58:57 @agent_ppo2.py:185][0m |          -0.0066 |          36.1606 |          15.3722 |
[32m[20221213 22:58:57 @agent_ppo2.py:185][0m |          -0.0087 |          36.1424 |          15.3857 |
[32m[20221213 22:58:58 @agent_ppo2.py:185][0m |          -0.0101 |          34.9269 |          15.3840 |
[32m[20221213 22:58:58 @agent_ppo2.py:185][0m |          -0.0107 |          34.5436 |          15.3894 |
[32m[20221213 22:58:58 @agent_ppo2.py:185][0m |          -0.0120 |          34.1974 |          15.3811 |
[32m[20221213 22:58:58 @agent_ppo2.py:185][0m |          -0.0108 |          33.9773 |          15.3883 |
[32m[20221213 22:58:58 @agent_ppo2.py:185][0m |          -0.0143 |          33.7403 |          15.3725 |
[32m[20221213 22:58:58 @agent_ppo2.py:185][0m |          -0.0022 |          35.1286 |          15.3669 |
[32m[20221213 22:58:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.54
[32m[20221213 22:58:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.31
[32m[20221213 22:58:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.00
[32m[20221213 22:58:58 @agent_ppo2.py:143][0m Total time:      40.75 min
[32m[20221213 22:58:58 @agent_ppo2.py:145][0m 3989504 total steps have happened
[32m[20221213 22:58:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1948 --------------------------#
[32m[20221213 22:58:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:58:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:58:58 @agent_ppo2.py:185][0m |           0.0012 |          49.4389 |          15.3816 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0007 |          47.1568 |          15.3681 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0058 |          45.6484 |          15.3772 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0063 |          44.6542 |          15.3558 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0089 |          43.9683 |          15.3700 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0108 |          43.3411 |          15.3459 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0088 |          42.9538 |          15.3493 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0117 |          42.4650 |          15.3478 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0113 |          42.2060 |          15.3444 |
[32m[20221213 22:58:59 @agent_ppo2.py:185][0m |          -0.0120 |          41.9501 |          15.3463 |
[32m[20221213 22:58:59 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:58:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.27
[32m[20221213 22:58:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.75
[32m[20221213 22:58:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.04
[32m[20221213 22:58:59 @agent_ppo2.py:143][0m Total time:      40.77 min
[32m[20221213 22:58:59 @agent_ppo2.py:145][0m 3991552 total steps have happened
[32m[20221213 22:58:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1949 --------------------------#
[32m[20221213 22:59:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |           0.0017 |          24.7524 |          15.1147 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0026 |          21.7140 |          15.0850 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0078 |          20.4168 |          15.0765 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0102 |          19.5257 |          15.0588 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0137 |          19.1775 |          15.0528 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0177 |          18.9656 |          15.0450 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0180 |          18.6639 |          15.0168 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0148 |          18.2919 |          15.0350 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0002 |          19.4519 |          14.9923 |
[32m[20221213 22:59:00 @agent_ppo2.py:185][0m |          -0.0147 |          17.8307 |          15.0057 |
[32m[20221213 22:59:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:59:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.34
[32m[20221213 22:59:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.35
[32m[20221213 22:59:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.29
[32m[20221213 22:59:01 @agent_ppo2.py:143][0m Total time:      40.79 min
[32m[20221213 22:59:01 @agent_ppo2.py:145][0m 3993600 total steps have happened
[32m[20221213 22:59:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1950 --------------------------#
[32m[20221213 22:59:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:59:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:01 @agent_ppo2.py:185][0m |           0.0005 |          31.3339 |          14.8757 |
[32m[20221213 22:59:01 @agent_ppo2.py:185][0m |           0.0045 |          30.7375 |          14.8679 |
[32m[20221213 22:59:01 @agent_ppo2.py:185][0m |          -0.0035 |          27.4427 |          14.8674 |
[32m[20221213 22:59:01 @agent_ppo2.py:185][0m |          -0.0105 |          26.4647 |          14.8548 |
[32m[20221213 22:59:01 @agent_ppo2.py:185][0m |          -0.0080 |          25.8009 |          14.8600 |
[32m[20221213 22:59:01 @agent_ppo2.py:185][0m |          -0.0116 |          25.4050 |          14.8852 |
[32m[20221213 22:59:01 @agent_ppo2.py:185][0m |          -0.0128 |          25.1789 |          14.8736 |
[32m[20221213 22:59:02 @agent_ppo2.py:185][0m |          -0.0080 |          24.7147 |          14.8742 |
[32m[20221213 22:59:02 @agent_ppo2.py:185][0m |          -0.0121 |          24.5507 |          14.8527 |
[32m[20221213 22:59:02 @agent_ppo2.py:185][0m |          -0.0114 |          24.2391 |          14.8378 |
[32m[20221213 22:59:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:59:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.33
[32m[20221213 22:59:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 393.80
[32m[20221213 22:59:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 360.74
[32m[20221213 22:59:02 @agent_ppo2.py:143][0m Total time:      40.82 min
[32m[20221213 22:59:02 @agent_ppo2.py:145][0m 3995648 total steps have happened
[32m[20221213 22:59:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1951 --------------------------#
[32m[20221213 22:59:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:02 @agent_ppo2.py:185][0m |          -0.0001 |          43.4738 |          15.2832 |
[32m[20221213 22:59:02 @agent_ppo2.py:185][0m |           0.0074 |          44.3261 |          15.2689 |
[32m[20221213 22:59:02 @agent_ppo2.py:185][0m |          -0.0010 |          42.1372 |          15.2708 |
[32m[20221213 22:59:02 @agent_ppo2.py:185][0m |          -0.0068 |          41.1731 |          15.2430 |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |          -0.0021 |          42.1121 |          15.2300 |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |          -0.0101 |          40.3417 |          15.2326 |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |          -0.0074 |          40.3072 |          15.2284 |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |          -0.0092 |          40.1700 |          15.2060 |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |          -0.0124 |          39.8184 |          15.1983 |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |          -0.0079 |          39.6591 |          15.1979 |
[32m[20221213 22:59:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.11
[32m[20221213 22:59:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.11
[32m[20221213 22:59:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 413.23
[32m[20221213 22:59:03 @agent_ppo2.py:143][0m Total time:      40.84 min
[32m[20221213 22:59:03 @agent_ppo2.py:145][0m 3997696 total steps have happened
[32m[20221213 22:59:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1952 --------------------------#
[32m[20221213 22:59:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |           0.0026 |          29.1611 |          15.1679 |
[32m[20221213 22:59:03 @agent_ppo2.py:185][0m |          -0.0083 |          25.9521 |          15.1840 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0061 |          25.4920 |          15.1974 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0099 |          24.7069 |          15.1809 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0046 |          24.5060 |          15.1923 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0109 |          24.1665 |          15.2009 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0097 |          24.2472 |          15.2118 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0152 |          23.6110 |          15.1913 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0106 |          23.4140 |          15.1965 |
[32m[20221213 22:59:04 @agent_ppo2.py:185][0m |          -0.0138 |          23.2308 |          15.2056 |
[32m[20221213 22:59:04 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.73
[32m[20221213 22:59:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.14
[32m[20221213 22:59:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.82
[32m[20221213 22:59:04 @agent_ppo2.py:143][0m Total time:      40.86 min
[32m[20221213 22:59:04 @agent_ppo2.py:145][0m 3999744 total steps have happened
[32m[20221213 22:59:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1953 --------------------------#
[32m[20221213 22:59:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0004 |          40.8869 |          15.1762 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0074 |          36.6195 |          15.1911 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0013 |          40.3329 |          15.1764 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0128 |          34.7664 |          15.1550 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0110 |          34.3157 |          15.1535 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0127 |          33.9798 |          15.1565 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0163 |          33.7525 |          15.1569 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0162 |          33.5044 |          15.1483 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0169 |          33.2494 |          15.1548 |
[32m[20221213 22:59:05 @agent_ppo2.py:185][0m |          -0.0164 |          33.1746 |          15.1540 |
[32m[20221213 22:59:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.30
[32m[20221213 22:59:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 406.36
[32m[20221213 22:59:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.42
[32m[20221213 22:59:06 @agent_ppo2.py:143][0m Total time:      40.88 min
[32m[20221213 22:59:06 @agent_ppo2.py:145][0m 4001792 total steps have happened
[32m[20221213 22:59:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1954 --------------------------#
[32m[20221213 22:59:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:06 @agent_ppo2.py:185][0m |           0.0025 |          36.1136 |          15.1177 |
[32m[20221213 22:59:06 @agent_ppo2.py:185][0m |          -0.0035 |          32.6132 |          15.0726 |
[32m[20221213 22:59:06 @agent_ppo2.py:185][0m |          -0.0080 |          31.4581 |          15.0427 |
[32m[20221213 22:59:06 @agent_ppo2.py:185][0m |          -0.0086 |          30.7627 |          15.0319 |
[32m[20221213 22:59:06 @agent_ppo2.py:185][0m |          -0.0084 |          30.4203 |          14.9979 |
[32m[20221213 22:59:06 @agent_ppo2.py:185][0m |          -0.0107 |          29.9137 |          15.0026 |
[32m[20221213 22:59:06 @agent_ppo2.py:185][0m |          -0.0112 |          29.7159 |          14.9912 |
[32m[20221213 22:59:07 @agent_ppo2.py:185][0m |          -0.0126 |          29.3619 |          14.9890 |
[32m[20221213 22:59:07 @agent_ppo2.py:185][0m |          -0.0130 |          28.9462 |          14.9675 |
[32m[20221213 22:59:07 @agent_ppo2.py:185][0m |          -0.0126 |          28.9400 |          14.9539 |
[32m[20221213 22:59:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:59:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.08
[32m[20221213 22:59:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.06
[32m[20221213 22:59:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.92
[32m[20221213 22:59:07 @agent_ppo2.py:143][0m Total time:      40.90 min
[32m[20221213 22:59:07 @agent_ppo2.py:145][0m 4003840 total steps have happened
[32m[20221213 22:59:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1955 --------------------------#
[32m[20221213 22:59:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:07 @agent_ppo2.py:185][0m |           0.0036 |          55.0775 |          15.1393 |
[32m[20221213 22:59:07 @agent_ppo2.py:185][0m |          -0.0038 |          48.8718 |          15.1310 |
[32m[20221213 22:59:07 @agent_ppo2.py:185][0m |          -0.0056 |          48.1193 |          15.1170 |
[32m[20221213 22:59:07 @agent_ppo2.py:185][0m |          -0.0045 |          47.4131 |          15.1269 |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |          -0.0000 |          51.1474 |          15.1200 |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |          -0.0095 |          46.4826 |          15.0943 |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |          -0.0069 |          46.2785 |          15.1117 |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |          -0.0102 |          46.0314 |          15.1000 |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |          -0.0136 |          45.8312 |          15.1053 |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |          -0.0038 |          46.3336 |          15.0981 |
[32m[20221213 22:59:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:59:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.27
[32m[20221213 22:59:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 405.40
[32m[20221213 22:59:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.86
[32m[20221213 22:59:08 @agent_ppo2.py:143][0m Total time:      40.92 min
[32m[20221213 22:59:08 @agent_ppo2.py:145][0m 4005888 total steps have happened
[32m[20221213 22:59:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1956 --------------------------#
[32m[20221213 22:59:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |           0.0011 |          35.7507 |          15.0230 |
[32m[20221213 22:59:08 @agent_ppo2.py:185][0m |          -0.0027 |          32.8474 |          15.0134 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |          -0.0083 |          31.9426 |          15.0054 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |           0.0033 |          31.6051 |          14.9859 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |          -0.0085 |          30.4862 |          14.9734 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |          -0.0098 |          30.0247 |          14.9519 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |          -0.0074 |          29.7662 |          14.9333 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |          -0.0087 |          29.4170 |          14.9252 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |           0.0014 |          34.1146 |          14.9039 |
[32m[20221213 22:59:09 @agent_ppo2.py:185][0m |          -0.0139 |          29.0319 |          14.8762 |
[32m[20221213 22:59:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.85
[32m[20221213 22:59:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 373.25
[32m[20221213 22:59:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.98
[32m[20221213 22:59:09 @agent_ppo2.py:143][0m Total time:      40.94 min
[32m[20221213 22:59:09 @agent_ppo2.py:145][0m 4007936 total steps have happened
[32m[20221213 22:59:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1957 --------------------------#
[32m[20221213 22:59:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |           0.0046 |          43.5507 |          15.0077 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |           0.0000 |          39.9156 |          15.0388 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0033 |          38.6577 |          15.0078 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0033 |          38.1385 |          15.0368 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0058 |          37.4084 |          15.0237 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0076 |          37.0116 |          15.0555 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0096 |          36.6608 |          15.0540 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0122 |          36.5300 |          15.0732 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0107 |          36.2156 |          15.0603 |
[32m[20221213 22:59:10 @agent_ppo2.py:185][0m |          -0.0099 |          36.0991 |          15.0431 |
[32m[20221213 22:59:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 342.14
[32m[20221213 22:59:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 421.51
[32m[20221213 22:59:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.65
[32m[20221213 22:59:11 @agent_ppo2.py:143][0m Total time:      40.96 min
[32m[20221213 22:59:11 @agent_ppo2.py:145][0m 4009984 total steps have happened
[32m[20221213 22:59:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1958 --------------------------#
[32m[20221213 22:59:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |           0.0037 |          33.2849 |          14.8286 |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |          -0.0017 |          29.1840 |          14.8110 |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |          -0.0117 |          26.4491 |          14.8442 |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |          -0.0093 |          25.3660 |          14.8338 |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |          -0.0120 |          24.7558 |          14.8188 |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |          -0.0039 |          24.5482 |          14.8433 |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |          -0.0111 |          23.9858 |          14.8514 |
[32m[20221213 22:59:11 @agent_ppo2.py:185][0m |          -0.0103 |          23.7656 |          14.8330 |
[32m[20221213 22:59:12 @agent_ppo2.py:185][0m |          -0.0101 |          23.6434 |          14.8323 |
[32m[20221213 22:59:12 @agent_ppo2.py:185][0m |          -0.0070 |          23.4293 |          14.8551 |
[32m[20221213 22:59:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.06
[32m[20221213 22:59:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.45
[32m[20221213 22:59:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.38
[32m[20221213 22:59:12 @agent_ppo2.py:143][0m Total time:      40.98 min
[32m[20221213 22:59:12 @agent_ppo2.py:145][0m 4012032 total steps have happened
[32m[20221213 22:59:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1959 --------------------------#
[32m[20221213 22:59:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:12 @agent_ppo2.py:185][0m |           0.0029 |          43.7286 |          15.1106 |
[32m[20221213 22:59:12 @agent_ppo2.py:185][0m |           0.0076 |          40.6958 |          15.0860 |
[32m[20221213 22:59:12 @agent_ppo2.py:185][0m |          -0.0047 |          35.7737 |          15.0971 |
[32m[20221213 22:59:12 @agent_ppo2.py:185][0m |          -0.0092 |          34.4634 |          15.0850 |
[32m[20221213 22:59:12 @agent_ppo2.py:185][0m |          -0.0073 |          33.9353 |          15.0949 |
[32m[20221213 22:59:13 @agent_ppo2.py:185][0m |          -0.0103 |          33.4681 |          15.0800 |
[32m[20221213 22:59:13 @agent_ppo2.py:185][0m |          -0.0103 |          33.1239 |          15.0850 |
[32m[20221213 22:59:13 @agent_ppo2.py:185][0m |          -0.0123 |          32.8343 |          15.0806 |
[32m[20221213 22:59:13 @agent_ppo2.py:185][0m |          -0.0115 |          32.5303 |          15.0839 |
[32m[20221213 22:59:13 @agent_ppo2.py:185][0m |          -0.0150 |          32.0972 |          15.0719 |
[32m[20221213 22:59:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:59:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.36
[32m[20221213 22:59:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 415.79
[32m[20221213 22:59:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.81
[32m[20221213 22:59:13 @agent_ppo2.py:143][0m Total time:      41.00 min
[32m[20221213 22:59:13 @agent_ppo2.py:145][0m 4014080 total steps have happened
[32m[20221213 22:59:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1960 --------------------------#
[32m[20221213 22:59:13 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:59:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:13 @agent_ppo2.py:185][0m |          -0.0026 |          47.5704 |          14.8595 |
[32m[20221213 22:59:13 @agent_ppo2.py:185][0m |          -0.0060 |          44.1147 |          14.8398 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0105 |          42.6157 |          14.8539 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0124 |          41.4742 |          14.8308 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0098 |          41.2210 |          14.8237 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0101 |          40.2907 |          14.8183 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0107 |          40.1486 |          14.8144 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0141 |          39.7071 |          14.8085 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0173 |          39.4150 |          14.8153 |
[32m[20221213 22:59:14 @agent_ppo2.py:185][0m |          -0.0126 |          39.1707 |          14.7920 |
[32m[20221213 22:59:14 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:59:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.15
[32m[20221213 22:59:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.47
[32m[20221213 22:59:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 426.19
[32m[20221213 22:59:14 @agent_ppo2.py:143][0m Total time:      41.02 min
[32m[20221213 22:59:14 @agent_ppo2.py:145][0m 4016128 total steps have happened
[32m[20221213 22:59:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1961 --------------------------#
[32m[20221213 22:59:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:59:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0026 |          15.3497 |          15.0546 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0007 |          12.3362 |          15.0118 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0018 |          12.1663 |          14.9768 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0045 |          12.0876 |          15.0041 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0027 |          12.0086 |          14.9999 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |           0.0068 |          12.0039 |          14.9904 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0009 |          12.0302 |          15.0131 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0034 |          12.0105 |          14.9683 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |           0.0008 |          11.9761 |          14.9967 |
[32m[20221213 22:59:15 @agent_ppo2.py:185][0m |          -0.0002 |          12.1657 |          14.9814 |
[32m[20221213 22:59:15 @agent_ppo2.py:130][0m Policy update time: 0.91 s
[32m[20221213 22:59:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 22:59:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 22:59:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 162.32
[32m[20221213 22:59:16 @agent_ppo2.py:143][0m Total time:      41.04 min
[32m[20221213 22:59:16 @agent_ppo2.py:145][0m 4018176 total steps have happened
[32m[20221213 22:59:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1962 --------------------------#
[32m[20221213 22:59:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |           0.0063 |          27.6892 |          14.7667 |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |          -0.0030 |          24.5636 |          14.7537 |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |          -0.0054 |          23.5307 |          14.7663 |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |           0.0014 |          27.2288 |          14.7735 |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |          -0.0107 |          22.5351 |          14.7537 |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |          -0.0069 |          22.2265 |          14.7582 |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |          -0.0069 |          21.9022 |          14.7786 |
[32m[20221213 22:59:16 @agent_ppo2.py:185][0m |          -0.0109 |          21.5708 |          14.7547 |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |          -0.0103 |          21.3296 |          14.7676 |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |          -0.0077 |          21.4580 |          14.7619 |
[32m[20221213 22:59:17 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:59:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 296.88
[32m[20221213 22:59:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 363.97
[32m[20221213 22:59:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.65
[32m[20221213 22:59:17 @agent_ppo2.py:143][0m Total time:      41.06 min
[32m[20221213 22:59:17 @agent_ppo2.py:145][0m 4020224 total steps have happened
[32m[20221213 22:59:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1963 --------------------------#
[32m[20221213 22:59:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |          -0.0031 |          38.5719 |          14.8080 |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |           0.0064 |          38.0011 |          14.8257 |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |           0.0011 |          33.5704 |          14.8156 |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |          -0.0094 |          32.6855 |          14.8255 |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |          -0.0074 |          32.2954 |          14.8163 |
[32m[20221213 22:59:17 @agent_ppo2.py:185][0m |          -0.0099 |          32.1701 |          14.8081 |
[32m[20221213 22:59:18 @agent_ppo2.py:185][0m |          -0.0081 |          31.9700 |          14.8120 |
[32m[20221213 22:59:18 @agent_ppo2.py:185][0m |          -0.0138 |          31.8463 |          14.8173 |
[32m[20221213 22:59:18 @agent_ppo2.py:185][0m |          -0.0102 |          31.7465 |          14.8037 |
[32m[20221213 22:59:18 @agent_ppo2.py:185][0m |          -0.0090 |          31.5557 |          14.8152 |
[32m[20221213 22:59:18 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 22:59:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.99
[32m[20221213 22:59:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.92
[32m[20221213 22:59:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.94
[32m[20221213 22:59:18 @agent_ppo2.py:143][0m Total time:      41.08 min
[32m[20221213 22:59:18 @agent_ppo2.py:145][0m 4022272 total steps have happened
[32m[20221213 22:59:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1964 --------------------------#
[32m[20221213 22:59:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:18 @agent_ppo2.py:185][0m |           0.0017 |          42.8327 |          14.8872 |
[32m[20221213 22:59:18 @agent_ppo2.py:185][0m |          -0.0030 |          41.5885 |          14.8718 |
[32m[20221213 22:59:18 @agent_ppo2.py:185][0m |          -0.0075 |          41.3012 |          14.8808 |
[32m[20221213 22:59:19 @agent_ppo2.py:185][0m |          -0.0058 |          41.4340 |          14.8930 |
[32m[20221213 22:59:19 @agent_ppo2.py:185][0m |          -0.0079 |          41.1873 |          14.9037 |
[32m[20221213 22:59:19 @agent_ppo2.py:185][0m |          -0.0077 |          41.1672 |          14.8690 |
[32m[20221213 22:59:19 @agent_ppo2.py:185][0m |          -0.0079 |          41.1501 |          14.8818 |
[32m[20221213 22:59:19 @agent_ppo2.py:185][0m |          -0.0064 |          41.1101 |          14.8815 |
[32m[20221213 22:59:19 @agent_ppo2.py:185][0m |          -0.0087 |          41.0143 |          14.8718 |
[32m[20221213 22:59:19 @agent_ppo2.py:185][0m |          -0.0095 |          41.0292 |          14.8788 |
[32m[20221213 22:59:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:59:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.11
[32m[20221213 22:59:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.01
[32m[20221213 22:59:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.77
[32m[20221213 22:59:19 @agent_ppo2.py:143][0m Total time:      41.11 min
[32m[20221213 22:59:19 @agent_ppo2.py:145][0m 4024320 total steps have happened
[32m[20221213 22:59:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1965 --------------------------#
[32m[20221213 22:59:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |           0.0003 |          50.1303 |          14.9550 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |           0.0026 |          49.9434 |          14.9513 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0066 |          46.8095 |          14.9325 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0051 |          46.4992 |          15.0010 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0089 |          46.0829 |          14.9800 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0099 |          45.8694 |          14.9878 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0054 |          45.9411 |          14.9662 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0126 |          45.4658 |          15.0250 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0110 |          45.3928 |          14.9927 |
[32m[20221213 22:59:20 @agent_ppo2.py:185][0m |          -0.0103 |          45.1843 |          14.9980 |
[32m[20221213 22:59:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.59
[32m[20221213 22:59:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.84
[32m[20221213 22:59:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.83
[32m[20221213 22:59:20 @agent_ppo2.py:143][0m Total time:      41.13 min
[32m[20221213 22:59:20 @agent_ppo2.py:145][0m 4026368 total steps have happened
[32m[20221213 22:59:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1966 --------------------------#
[32m[20221213 22:59:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |           0.0001 |          46.9631 |          14.8006 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0016 |          42.7214 |          14.8050 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0090 |          41.6232 |          14.8036 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0067 |          41.0031 |          14.7623 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0045 |          40.9033 |          14.7736 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0090 |          40.1943 |          14.7602 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0135 |          39.7716 |          14.7518 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0108 |          39.5902 |          14.7379 |
[32m[20221213 22:59:21 @agent_ppo2.py:185][0m |          -0.0075 |          39.5534 |          14.7362 |
[32m[20221213 22:59:22 @agent_ppo2.py:185][0m |          -0.0107 |          39.1930 |          14.7405 |
[32m[20221213 22:59:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 310.33
[32m[20221213 22:59:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 381.16
[32m[20221213 22:59:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.41
[32m[20221213 22:59:22 @agent_ppo2.py:143][0m Total time:      41.15 min
[32m[20221213 22:59:22 @agent_ppo2.py:145][0m 4028416 total steps have happened
[32m[20221213 22:59:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1967 --------------------------#
[32m[20221213 22:59:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:22 @agent_ppo2.py:185][0m |           0.0058 |          49.7207 |          14.6546 |
[32m[20221213 22:59:22 @agent_ppo2.py:185][0m |          -0.0035 |          45.3143 |          14.6594 |
[32m[20221213 22:59:22 @agent_ppo2.py:185][0m |          -0.0066 |          45.0189 |          14.6528 |
[32m[20221213 22:59:22 @agent_ppo2.py:185][0m |          -0.0084 |          44.6225 |          14.6569 |
[32m[20221213 22:59:22 @agent_ppo2.py:185][0m |          -0.0022 |          45.0538 |          14.6511 |
[32m[20221213 22:59:22 @agent_ppo2.py:185][0m |          -0.0072 |          44.4406 |          14.6513 |
[32m[20221213 22:59:23 @agent_ppo2.py:185][0m |          -0.0090 |          44.2766 |          14.6539 |
[32m[20221213 22:59:23 @agent_ppo2.py:185][0m |           0.0026 |          50.0498 |          14.6489 |
[32m[20221213 22:59:23 @agent_ppo2.py:185][0m |           0.0121 |          48.5239 |          14.6472 |
[32m[20221213 22:59:23 @agent_ppo2.py:185][0m |          -0.0094 |          43.7829 |          14.6638 |
[32m[20221213 22:59:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.13
[32m[20221213 22:59:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.64
[32m[20221213 22:59:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.52
[32m[20221213 22:59:23 @agent_ppo2.py:143][0m Total time:      41.17 min
[32m[20221213 22:59:23 @agent_ppo2.py:145][0m 4030464 total steps have happened
[32m[20221213 22:59:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1968 --------------------------#
[32m[20221213 22:59:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:23 @agent_ppo2.py:185][0m |          -0.0018 |          47.6325 |          15.2929 |
[32m[20221213 22:59:23 @agent_ppo2.py:185][0m |           0.0004 |          46.8914 |          15.2706 |
[32m[20221213 22:59:23 @agent_ppo2.py:185][0m |          -0.0067 |          46.4938 |          15.2740 |
[32m[20221213 22:59:24 @agent_ppo2.py:185][0m |          -0.0043 |          46.2031 |          15.2828 |
[32m[20221213 22:59:24 @agent_ppo2.py:185][0m |          -0.0040 |          46.4302 |          15.3135 |
[32m[20221213 22:59:24 @agent_ppo2.py:185][0m |          -0.0068 |          45.9089 |          15.3051 |
[32m[20221213 22:59:24 @agent_ppo2.py:185][0m |          -0.0004 |          48.5931 |          15.3124 |
[32m[20221213 22:59:24 @agent_ppo2.py:185][0m |          -0.0063 |          45.6302 |          15.2692 |
[32m[20221213 22:59:24 @agent_ppo2.py:185][0m |          -0.0109 |          45.4462 |          15.3273 |
[32m[20221213 22:59:24 @agent_ppo2.py:185][0m |          -0.0089 |          45.2755 |          15.3352 |
[32m[20221213 22:59:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.74
[32m[20221213 22:59:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.43
[32m[20221213 22:59:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.78
[32m[20221213 22:59:24 @agent_ppo2.py:143][0m Total time:      41.19 min
[32m[20221213 22:59:24 @agent_ppo2.py:145][0m 4032512 total steps have happened
[32m[20221213 22:59:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1969 --------------------------#
[32m[20221213 22:59:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0008 |          39.0050 |          15.0250 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0028 |          37.8760 |          15.0009 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0047 |          37.6351 |          15.0044 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0051 |          37.4869 |          15.0281 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0089 |          37.3724 |          15.0249 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0098 |          37.2844 |          15.0091 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0057 |          37.4517 |          15.0212 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0074 |          37.2211 |          15.0049 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0092 |          37.2074 |          15.0043 |
[32m[20221213 22:59:25 @agent_ppo2.py:185][0m |          -0.0066 |          37.2250 |          15.0197 |
[32m[20221213 22:59:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:59:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.80
[32m[20221213 22:59:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.64
[32m[20221213 22:59:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.78
[32m[20221213 22:59:25 @agent_ppo2.py:143][0m Total time:      41.21 min
[32m[20221213 22:59:25 @agent_ppo2.py:145][0m 4034560 total steps have happened
[32m[20221213 22:59:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1970 --------------------------#
[32m[20221213 22:59:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:59:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |           0.0033 |          44.2590 |          14.8994 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |          -0.0044 |          42.4947 |          14.8910 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |          -0.0050 |          41.8144 |          14.8693 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |          -0.0089 |          41.5859 |          14.8595 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |           0.0015 |          42.5013 |          14.8597 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |          -0.0094 |          40.9661 |          14.8581 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |          -0.0021 |          42.9331 |          14.8670 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |          -0.0019 |          41.5048 |          14.8727 |
[32m[20221213 22:59:26 @agent_ppo2.py:185][0m |          -0.0083 |          40.3675 |          14.8601 |
[32m[20221213 22:59:27 @agent_ppo2.py:185][0m |          -0.0113 |          40.3915 |          14.8637 |
[32m[20221213 22:59:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.21
[32m[20221213 22:59:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.79
[32m[20221213 22:59:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.94
[32m[20221213 22:59:27 @agent_ppo2.py:143][0m Total time:      41.23 min
[32m[20221213 22:59:27 @agent_ppo2.py:145][0m 4036608 total steps have happened
[32m[20221213 22:59:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1971 --------------------------#
[32m[20221213 22:59:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:27 @agent_ppo2.py:185][0m |           0.0046 |          45.1377 |          14.9717 |
[32m[20221213 22:59:27 @agent_ppo2.py:185][0m |          -0.0022 |          41.2422 |          14.9833 |
[32m[20221213 22:59:27 @agent_ppo2.py:185][0m |          -0.0031 |          40.5367 |          14.9656 |
[32m[20221213 22:59:27 @agent_ppo2.py:185][0m |           0.0009 |          40.9836 |          14.9501 |
[32m[20221213 22:59:27 @agent_ppo2.py:185][0m |          -0.0080 |          39.2767 |          14.9493 |
[32m[20221213 22:59:27 @agent_ppo2.py:185][0m |          -0.0117 |          39.0291 |          14.9320 |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0060 |          39.5749 |          14.9284 |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0086 |          38.4441 |          14.9234 |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0125 |          38.4724 |          14.9164 |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0126 |          38.0260 |          14.9155 |
[32m[20221213 22:59:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.95
[32m[20221213 22:59:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.86
[32m[20221213 22:59:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.15
[32m[20221213 22:59:28 @agent_ppo2.py:143][0m Total time:      41.25 min
[32m[20221213 22:59:28 @agent_ppo2.py:145][0m 4038656 total steps have happened
[32m[20221213 22:59:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1972 --------------------------#
[32m[20221213 22:59:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0033 |          37.9934 |          14.8391 |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0028 |          37.2960 |          14.8543 |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0060 |          36.9152 |          14.8631 |
[32m[20221213 22:59:28 @agent_ppo2.py:185][0m |          -0.0058 |          36.5845 |          14.8628 |
[32m[20221213 22:59:29 @agent_ppo2.py:185][0m |          -0.0092 |          36.2924 |          14.8791 |
[32m[20221213 22:59:29 @agent_ppo2.py:185][0m |           0.0038 |          41.7218 |          14.9058 |
[32m[20221213 22:59:29 @agent_ppo2.py:185][0m |          -0.0109 |          36.2713 |          14.9236 |
[32m[20221213 22:59:29 @agent_ppo2.py:185][0m |          -0.0075 |          35.9616 |          14.9262 |
[32m[20221213 22:59:29 @agent_ppo2.py:185][0m |          -0.0093 |          35.9109 |          14.9448 |
[32m[20221213 22:59:29 @agent_ppo2.py:185][0m |          -0.0062 |          36.0203 |          14.9537 |
[32m[20221213 22:59:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:59:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.14
[32m[20221213 22:59:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.93
[32m[20221213 22:59:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.10
[32m[20221213 22:59:29 @agent_ppo2.py:143][0m Total time:      41.27 min
[32m[20221213 22:59:29 @agent_ppo2.py:145][0m 4040704 total steps have happened
[32m[20221213 22:59:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1973 --------------------------#
[32m[20221213 22:59:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:29 @agent_ppo2.py:185][0m |           0.0051 |          58.9214 |          14.8813 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0018 |          52.9676 |          14.8663 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0074 |          51.7373 |          14.8819 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0059 |          51.0161 |          14.8757 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0082 |          50.6044 |          14.8704 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0041 |          50.3979 |          14.8750 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0169 |          49.9466 |          14.8706 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0061 |          50.0477 |          14.8641 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0102 |          49.4780 |          14.8669 |
[32m[20221213 22:59:30 @agent_ppo2.py:185][0m |          -0.0065 |          49.1990 |          14.8825 |
[32m[20221213 22:59:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.80
[32m[20221213 22:59:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 353.16
[32m[20221213 22:59:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.01
[32m[20221213 22:59:30 @agent_ppo2.py:143][0m Total time:      41.29 min
[32m[20221213 22:59:30 @agent_ppo2.py:145][0m 4042752 total steps have happened
[32m[20221213 22:59:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1974 --------------------------#
[32m[20221213 22:59:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0006 |          38.6056 |          15.0949 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |           0.0027 |          38.2998 |          15.1153 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0033 |          37.9454 |          15.0779 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0046 |          37.7631 |          15.0974 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0078 |          37.7054 |          15.0806 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0048 |          37.5786 |          15.0803 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0072 |          37.4843 |          15.0732 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0066 |          37.4255 |          15.0816 |
[32m[20221213 22:59:31 @agent_ppo2.py:185][0m |          -0.0086 |          37.3140 |          15.0799 |
[32m[20221213 22:59:32 @agent_ppo2.py:185][0m |          -0.0081 |          37.2421 |          15.0655 |
[32m[20221213 22:59:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 22:59:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.89
[32m[20221213 22:59:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.70
[32m[20221213 22:59:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.00
[32m[20221213 22:59:32 @agent_ppo2.py:143][0m Total time:      41.31 min
[32m[20221213 22:59:32 @agent_ppo2.py:145][0m 4044800 total steps have happened
[32m[20221213 22:59:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1975 --------------------------#
[32m[20221213 22:59:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:32 @agent_ppo2.py:185][0m |           0.0063 |          34.6731 |          15.1964 |
[32m[20221213 22:59:32 @agent_ppo2.py:185][0m |          -0.0006 |          30.5050 |          15.1866 |
[32m[20221213 22:59:32 @agent_ppo2.py:185][0m |          -0.0051 |          29.2386 |          15.2312 |
[32m[20221213 22:59:32 @agent_ppo2.py:185][0m |          -0.0052 |          28.9937 |          15.2212 |
[32m[20221213 22:59:32 @agent_ppo2.py:185][0m |          -0.0115 |          28.4885 |          15.2139 |
[32m[20221213 22:59:32 @agent_ppo2.py:185][0m |          -0.0117 |          28.2965 |          15.2353 |
[32m[20221213 22:59:33 @agent_ppo2.py:185][0m |          -0.0128 |          28.2618 |          15.2444 |
[32m[20221213 22:59:33 @agent_ppo2.py:185][0m |          -0.0121 |          27.7899 |          15.2448 |
[32m[20221213 22:59:33 @agent_ppo2.py:185][0m |          -0.0012 |          29.8341 |          15.2447 |
[32m[20221213 22:59:33 @agent_ppo2.py:185][0m |          -0.0119 |          27.6548 |          15.2362 |
[32m[20221213 22:59:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:59:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.20
[32m[20221213 22:59:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.71
[32m[20221213 22:59:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.27
[32m[20221213 22:59:33 @agent_ppo2.py:143][0m Total time:      41.33 min
[32m[20221213 22:59:33 @agent_ppo2.py:145][0m 4046848 total steps have happened
[32m[20221213 22:59:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1976 --------------------------#
[32m[20221213 22:59:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:33 @agent_ppo2.py:185][0m |          -0.0007 |          42.9118 |          15.0731 |
[32m[20221213 22:59:33 @agent_ppo2.py:185][0m |          -0.0107 |          40.1365 |          15.0687 |
[32m[20221213 22:59:33 @agent_ppo2.py:185][0m |          -0.0079 |          38.2361 |          15.0778 |
[32m[20221213 22:59:34 @agent_ppo2.py:185][0m |          -0.0152 |          37.5611 |          15.0935 |
[32m[20221213 22:59:34 @agent_ppo2.py:185][0m |          -0.0138 |          36.7599 |          15.1138 |
[32m[20221213 22:59:34 @agent_ppo2.py:185][0m |          -0.0147 |          36.4325 |          15.1060 |
[32m[20221213 22:59:34 @agent_ppo2.py:185][0m |          -0.0112 |          35.8163 |          15.1251 |
[32m[20221213 22:59:34 @agent_ppo2.py:185][0m |          -0.0160 |          35.5314 |          15.1521 |
[32m[20221213 22:59:34 @agent_ppo2.py:185][0m |          -0.0182 |          35.3007 |          15.1431 |
[32m[20221213 22:59:34 @agent_ppo2.py:185][0m |          -0.0119 |          35.0994 |          15.1489 |
[32m[20221213 22:59:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 22:59:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.39
[32m[20221213 22:59:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.47
[32m[20221213 22:59:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.45
[32m[20221213 22:59:34 @agent_ppo2.py:143][0m Total time:      41.36 min
[32m[20221213 22:59:34 @agent_ppo2.py:145][0m 4048896 total steps have happened
[32m[20221213 22:59:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1977 --------------------------#
[32m[20221213 22:59:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |           0.0038 |          47.5558 |          15.0965 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0052 |          46.8439 |          15.0944 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0041 |          46.4102 |          15.0621 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |           0.0088 |          51.8056 |          15.0619 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0056 |          46.1531 |          15.0524 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0074 |          45.6974 |          15.0569 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0067 |          45.7911 |          15.0364 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0115 |          45.3418 |          15.0126 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0101 |          45.0615 |          15.0182 |
[32m[20221213 22:59:35 @agent_ppo2.py:185][0m |          -0.0115 |          44.9786 |          15.0177 |
[32m[20221213 22:59:35 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.90
[32m[20221213 22:59:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.86
[32m[20221213 22:59:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.35
[32m[20221213 22:59:36 @agent_ppo2.py:143][0m Total time:      41.38 min
[32m[20221213 22:59:36 @agent_ppo2.py:145][0m 4050944 total steps have happened
[32m[20221213 22:59:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1978 --------------------------#
[32m[20221213 22:59:36 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 22:59:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:36 @agent_ppo2.py:185][0m |          -0.0005 |          47.6700 |          15.0099 |
[32m[20221213 22:59:36 @agent_ppo2.py:185][0m |          -0.0045 |          41.9280 |          14.9961 |
[32m[20221213 22:59:36 @agent_ppo2.py:185][0m |          -0.0075 |          40.8965 |          14.9876 |
[32m[20221213 22:59:36 @agent_ppo2.py:185][0m |           0.0021 |          41.1130 |          14.9831 |
[32m[20221213 22:59:37 @agent_ppo2.py:185][0m |          -0.0050 |          39.9655 |          14.9699 |
[32m[20221213 22:59:37 @agent_ppo2.py:185][0m |          -0.0080 |          39.8428 |          14.9877 |
[32m[20221213 22:59:37 @agent_ppo2.py:185][0m |          -0.0095 |          39.4314 |          14.9876 |
[32m[20221213 22:59:37 @agent_ppo2.py:185][0m |          -0.0122 |          39.3319 |          14.9733 |
[32m[20221213 22:59:37 @agent_ppo2.py:185][0m |          -0.0092 |          38.9935 |          14.9836 |
[32m[20221213 22:59:37 @agent_ppo2.py:185][0m |          -0.0086 |          38.9209 |          14.9923 |
[32m[20221213 22:59:37 @agent_ppo2.py:130][0m Policy update time: 1.34 s
[32m[20221213 22:59:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.95
[32m[20221213 22:59:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.91
[32m[20221213 22:59:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 210.52
[32m[20221213 22:59:37 @agent_ppo2.py:143][0m Total time:      41.41 min
[32m[20221213 22:59:37 @agent_ppo2.py:145][0m 4052992 total steps have happened
[32m[20221213 22:59:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1979 --------------------------#
[32m[20221213 22:59:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 22:59:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0025 |          44.1306 |          15.0476 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |           0.0036 |          43.8803 |          15.0476 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0060 |          37.3133 |          15.0466 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0037 |          37.4510 |          15.0410 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0074 |          35.7246 |          15.0326 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0088 |          34.9069 |          15.0552 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0126 |          34.4388 |          15.0615 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0102 |          33.9491 |          15.0656 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0149 |          33.6083 |          15.0760 |
[32m[20221213 22:59:38 @agent_ppo2.py:185][0m |          -0.0187 |          33.2140 |          15.0782 |
[32m[20221213 22:59:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:59:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.87
[32m[20221213 22:59:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 407.68
[32m[20221213 22:59:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.20
[32m[20221213 22:59:39 @agent_ppo2.py:143][0m Total time:      41.43 min
[32m[20221213 22:59:39 @agent_ppo2.py:145][0m 4055040 total steps have happened
[32m[20221213 22:59:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1980 --------------------------#
[32m[20221213 22:59:39 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 22:59:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:39 @agent_ppo2.py:185][0m |           0.0038 |          41.8472 |          14.9760 |
[32m[20221213 22:59:39 @agent_ppo2.py:185][0m |          -0.0060 |          40.1178 |          15.0019 |
[32m[20221213 22:59:39 @agent_ppo2.py:185][0m |          -0.0088 |          39.8777 |          14.9995 |
[32m[20221213 22:59:39 @agent_ppo2.py:185][0m |          -0.0105 |          39.5183 |          14.9867 |
[32m[20221213 22:59:39 @agent_ppo2.py:185][0m |          -0.0100 |          39.2909 |          14.9981 |
[32m[20221213 22:59:40 @agent_ppo2.py:185][0m |          -0.0120 |          39.2427 |          15.0048 |
[32m[20221213 22:59:40 @agent_ppo2.py:185][0m |          -0.0094 |          38.9514 |          15.0017 |
[32m[20221213 22:59:40 @agent_ppo2.py:185][0m |          -0.0126 |          39.0670 |          14.9882 |
[32m[20221213 22:59:40 @agent_ppo2.py:185][0m |          -0.0080 |          39.3456 |          14.9982 |
[32m[20221213 22:59:40 @agent_ppo2.py:185][0m |          -0.0134 |          38.6817 |          15.0052 |
[32m[20221213 22:59:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:59:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.37
[32m[20221213 22:59:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.48
[32m[20221213 22:59:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.09
[32m[20221213 22:59:40 @agent_ppo2.py:143][0m Total time:      41.45 min
[32m[20221213 22:59:40 @agent_ppo2.py:145][0m 4057088 total steps have happened
[32m[20221213 22:59:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1981 --------------------------#
[32m[20221213 22:59:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:40 @agent_ppo2.py:185][0m |           0.0013 |          49.9230 |          15.2510 |
[32m[20221213 22:59:40 @agent_ppo2.py:185][0m |          -0.0057 |          47.0381 |          15.2389 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0063 |          46.1444 |          15.2408 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0079 |          45.4032 |          15.2614 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0053 |          45.1348 |          15.2491 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0110 |          44.7869 |          15.2675 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0089 |          44.5077 |          15.2632 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0095 |          44.5489 |          15.2376 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0071 |          44.8352 |          15.2490 |
[32m[20221213 22:59:41 @agent_ppo2.py:185][0m |          -0.0146 |          44.2851 |          15.2672 |
[32m[20221213 22:59:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 22:59:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.50
[32m[20221213 22:59:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.13
[32m[20221213 22:59:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.07
[32m[20221213 22:59:41 @agent_ppo2.py:143][0m Total time:      41.47 min
[32m[20221213 22:59:41 @agent_ppo2.py:145][0m 4059136 total steps have happened
[32m[20221213 22:59:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1982 --------------------------#
[32m[20221213 22:59:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |           0.0054 |          37.3844 |          15.0856 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0048 |          32.9231 |          15.0491 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0029 |          31.8029 |          15.0191 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0131 |          30.9955 |          15.0297 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0099 |          30.4035 |          15.0340 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0105 |          30.0780 |          15.0248 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0051 |          29.6719 |          15.0092 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0093 |          29.1954 |          15.0085 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0199 |          29.0226 |          14.9991 |
[32m[20221213 22:59:42 @agent_ppo2.py:185][0m |          -0.0189 |          28.8732 |          15.0158 |
[32m[20221213 22:59:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:59:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.64
[32m[20221213 22:59:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 385.11
[32m[20221213 22:59:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.96
[32m[20221213 22:59:43 @agent_ppo2.py:143][0m Total time:      41.49 min
[32m[20221213 22:59:43 @agent_ppo2.py:145][0m 4061184 total steps have happened
[32m[20221213 22:59:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1983 --------------------------#
[32m[20221213 22:59:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:59:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:43 @agent_ppo2.py:185][0m |           0.0053 |          42.6179 |          15.0255 |
[32m[20221213 22:59:43 @agent_ppo2.py:185][0m |          -0.0026 |          39.4533 |          15.0288 |
[32m[20221213 22:59:43 @agent_ppo2.py:185][0m |          -0.0059 |          38.6785 |          15.0031 |
[32m[20221213 22:59:43 @agent_ppo2.py:185][0m |          -0.0067 |          38.5355 |          15.0406 |
[32m[20221213 22:59:43 @agent_ppo2.py:185][0m |          -0.0068 |          39.2295 |          15.0064 |
[32m[20221213 22:59:43 @agent_ppo2.py:185][0m |          -0.0012 |          38.5353 |          15.0192 |
[32m[20221213 22:59:43 @agent_ppo2.py:185][0m |          -0.0134 |          37.3593 |          14.9996 |
[32m[20221213 22:59:44 @agent_ppo2.py:185][0m |          -0.0061 |          37.4771 |          15.0337 |
[32m[20221213 22:59:44 @agent_ppo2.py:185][0m |          -0.0089 |          36.9178 |          15.0031 |
[32m[20221213 22:59:44 @agent_ppo2.py:185][0m |           0.0066 |          42.9855 |          15.0276 |
[32m[20221213 22:59:44 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:59:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.44
[32m[20221213 22:59:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.77
[32m[20221213 22:59:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.92
[32m[20221213 22:59:44 @agent_ppo2.py:143][0m Total time:      41.52 min
[32m[20221213 22:59:44 @agent_ppo2.py:145][0m 4063232 total steps have happened
[32m[20221213 22:59:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1984 --------------------------#
[32m[20221213 22:59:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:44 @agent_ppo2.py:185][0m |          -0.0012 |          34.3914 |          15.2766 |
[32m[20221213 22:59:44 @agent_ppo2.py:185][0m |           0.0028 |          30.5212 |          15.2871 |
[32m[20221213 22:59:44 @agent_ppo2.py:185][0m |          -0.0078 |          28.3927 |          15.2870 |
[32m[20221213 22:59:45 @agent_ppo2.py:185][0m |          -0.0114 |          27.2879 |          15.2754 |
[32m[20221213 22:59:45 @agent_ppo2.py:185][0m |          -0.0080 |          27.1822 |          15.2595 |
[32m[20221213 22:59:45 @agent_ppo2.py:185][0m |          -0.0025 |          26.5833 |          15.2791 |
[32m[20221213 22:59:45 @agent_ppo2.py:185][0m |          -0.0189 |          25.7416 |          15.2780 |
[32m[20221213 22:59:45 @agent_ppo2.py:185][0m |          -0.0106 |          25.2502 |          15.2757 |
[32m[20221213 22:59:45 @agent_ppo2.py:185][0m |          -0.0130 |          25.0231 |          15.2705 |
[32m[20221213 22:59:45 @agent_ppo2.py:185][0m |          -0.0202 |          24.5885 |          15.2876 |
[32m[20221213 22:59:45 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:59:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.49
[32m[20221213 22:59:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 394.19
[32m[20221213 22:59:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 340.97
[32m[20221213 22:59:45 @agent_ppo2.py:143][0m Total time:      41.54 min
[32m[20221213 22:59:45 @agent_ppo2.py:145][0m 4065280 total steps have happened
[32m[20221213 22:59:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1985 --------------------------#
[32m[20221213 22:59:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0007 |          52.4167 |          15.3306 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0082 |          49.0582 |          15.3364 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0072 |          47.9460 |          15.3465 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0108 |          47.2721 |          15.3530 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0086 |          46.6633 |          15.3439 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0110 |          46.2030 |          15.3473 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0093 |          46.0081 |          15.3548 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0117 |          45.5404 |          15.3481 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0146 |          45.2512 |          15.3553 |
[32m[20221213 22:59:46 @agent_ppo2.py:185][0m |          -0.0163 |          45.3529 |          15.3438 |
[32m[20221213 22:59:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 22:59:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.14
[32m[20221213 22:59:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 400.33
[32m[20221213 22:59:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.17
[32m[20221213 22:59:47 @agent_ppo2.py:143][0m Total time:      41.56 min
[32m[20221213 22:59:47 @agent_ppo2.py:145][0m 4067328 total steps have happened
[32m[20221213 22:59:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1986 --------------------------#
[32m[20221213 22:59:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |          -0.0019 |          46.2312 |          15.2873 |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |          -0.0063 |          42.3463 |          15.2582 |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |          -0.0107 |          41.1185 |          15.2424 |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |          -0.0077 |          40.3550 |          15.2332 |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |          -0.0114 |          39.7402 |          15.2330 |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |          -0.0150 |          39.4138 |          15.2484 |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |          -0.0110 |          38.8477 |          15.2145 |
[32m[20221213 22:59:47 @agent_ppo2.py:185][0m |           0.0060 |          41.2682 |          15.2267 |
[32m[20221213 22:59:48 @agent_ppo2.py:185][0m |          -0.0085 |          38.8395 |          15.2126 |
[32m[20221213 22:59:48 @agent_ppo2.py:185][0m |          -0.0151 |          38.2379 |          15.2211 |
[32m[20221213 22:59:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 22:59:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.82
[32m[20221213 22:59:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 379.67
[32m[20221213 22:59:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.19
[32m[20221213 22:59:48 @agent_ppo2.py:143][0m Total time:      41.58 min
[32m[20221213 22:59:48 @agent_ppo2.py:145][0m 4069376 total steps have happened
[32m[20221213 22:59:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1987 --------------------------#
[32m[20221213 22:59:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:48 @agent_ppo2.py:185][0m |           0.0002 |          62.6159 |          15.1427 |
[32m[20221213 22:59:48 @agent_ppo2.py:185][0m |          -0.0027 |          58.5845 |          15.1299 |
[32m[20221213 22:59:48 @agent_ppo2.py:185][0m |          -0.0090 |          57.5411 |          15.1161 |
[32m[20221213 22:59:48 @agent_ppo2.py:185][0m |          -0.0080 |          56.6991 |          15.1033 |
[32m[20221213 22:59:49 @agent_ppo2.py:185][0m |          -0.0132 |          56.2862 |          15.1157 |
[32m[20221213 22:59:49 @agent_ppo2.py:185][0m |          -0.0124 |          56.1386 |          15.0946 |
[32m[20221213 22:59:49 @agent_ppo2.py:185][0m |          -0.0078 |          55.5838 |          15.0828 |
[32m[20221213 22:59:49 @agent_ppo2.py:185][0m |          -0.0119 |          55.2754 |          15.0977 |
[32m[20221213 22:59:49 @agent_ppo2.py:185][0m |          -0.0115 |          55.2023 |          15.0955 |
[32m[20221213 22:59:49 @agent_ppo2.py:185][0m |          -0.0118 |          54.7431 |          15.1002 |
[32m[20221213 22:59:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 22:59:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.01
[32m[20221213 22:59:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 384.28
[32m[20221213 22:59:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.50
[32m[20221213 22:59:49 @agent_ppo2.py:143][0m Total time:      41.60 min
[32m[20221213 22:59:49 @agent_ppo2.py:145][0m 4071424 total steps have happened
[32m[20221213 22:59:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1988 --------------------------#
[32m[20221213 22:59:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 22:59:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |           0.0081 |          45.3967 |          14.9423 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0028 |          43.4939 |          14.9503 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0056 |          43.2303 |          14.9648 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0010 |          43.1674 |          14.9684 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0045 |          43.1576 |          15.0043 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0061 |          42.9694 |          14.9871 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0065 |          42.9293 |          14.9745 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0051 |          42.9805 |          15.0041 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0090 |          42.8244 |          14.9651 |
[32m[20221213 22:59:50 @agent_ppo2.py:185][0m |          -0.0053 |          42.8398 |          15.0062 |
[32m[20221213 22:59:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 22:59:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.09
[32m[20221213 22:59:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.56
[32m[20221213 22:59:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.22
[32m[20221213 22:59:51 @agent_ppo2.py:143][0m Total time:      41.63 min
[32m[20221213 22:59:51 @agent_ppo2.py:145][0m 4073472 total steps have happened
[32m[20221213 22:59:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1989 --------------------------#
[32m[20221213 22:59:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 22:59:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |          -0.0009 |          39.7836 |          15.1012 |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |          -0.0035 |          34.3937 |          15.0963 |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |          -0.0069 |          34.0510 |          15.0975 |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |          -0.0139 |          32.7719 |          15.0821 |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |          -0.0057 |          32.9253 |          15.0675 |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |           0.0014 |          34.3927 |          15.0791 |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |          -0.0187 |          31.2226 |          15.0522 |
[32m[20221213 22:59:51 @agent_ppo2.py:185][0m |          -0.0100 |          30.8492 |          15.0643 |
[32m[20221213 22:59:52 @agent_ppo2.py:185][0m |          -0.0163 |          30.6281 |          15.0530 |
[32m[20221213 22:59:52 @agent_ppo2.py:185][0m |          -0.0160 |          30.6939 |          15.0553 |
[32m[20221213 22:59:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:59:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.74
[32m[20221213 22:59:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.14
[32m[20221213 22:59:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.32
[32m[20221213 22:59:52 @agent_ppo2.py:143][0m Total time:      41.65 min
[32m[20221213 22:59:52 @agent_ppo2.py:145][0m 4075520 total steps have happened
[32m[20221213 22:59:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1990 --------------------------#
[32m[20221213 22:59:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 22:59:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:52 @agent_ppo2.py:185][0m |           0.0018 |          46.7784 |          15.0110 |
[32m[20221213 22:59:52 @agent_ppo2.py:185][0m |           0.0029 |          46.4847 |          15.0020 |
[32m[20221213 22:59:52 @agent_ppo2.py:185][0m |           0.0078 |          47.2324 |          14.9738 |
[32m[20221213 22:59:52 @agent_ppo2.py:185][0m |          -0.0050 |          44.7989 |          14.9754 |
[32m[20221213 22:59:52 @agent_ppo2.py:185][0m |          -0.0034 |          44.6036 |          14.9684 |
[32m[20221213 22:59:53 @agent_ppo2.py:185][0m |          -0.0071 |          44.3435 |          14.9577 |
[32m[20221213 22:59:53 @agent_ppo2.py:185][0m |          -0.0054 |          44.7917 |          14.9484 |
[32m[20221213 22:59:53 @agent_ppo2.py:185][0m |          -0.0078 |          44.2348 |          14.9444 |
[32m[20221213 22:59:53 @agent_ppo2.py:185][0m |          -0.0016 |          45.7345 |          14.9273 |
[32m[20221213 22:59:53 @agent_ppo2.py:185][0m |          -0.0089 |          44.0375 |          14.8949 |
[32m[20221213 22:59:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.46
[32m[20221213 22:59:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 427.78
[32m[20221213 22:59:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.44
[32m[20221213 22:59:53 @agent_ppo2.py:143][0m Total time:      41.67 min
[32m[20221213 22:59:53 @agent_ppo2.py:145][0m 4077568 total steps have happened
[32m[20221213 22:59:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1991 --------------------------#
[32m[20221213 22:59:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:53 @agent_ppo2.py:185][0m |           0.0001 |          31.3120 |          15.1270 |
[32m[20221213 22:59:53 @agent_ppo2.py:185][0m |          -0.0065 |          27.9106 |          15.1290 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0091 |          26.7368 |          15.1335 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0054 |          26.0963 |          15.1522 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0077 |          25.8314 |          15.1551 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0115 |          25.1732 |          15.1691 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0135 |          24.7046 |          15.1325 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0140 |          24.4046 |          15.1657 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0124 |          24.2009 |          15.1768 |
[32m[20221213 22:59:54 @agent_ppo2.py:185][0m |          -0.0135 |          23.8679 |          15.1570 |
[32m[20221213 22:59:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:59:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 334.13
[32m[20221213 22:59:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.38
[32m[20221213 22:59:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.58
[32m[20221213 22:59:54 @agent_ppo2.py:143][0m Total time:      41.69 min
[32m[20221213 22:59:54 @agent_ppo2.py:145][0m 4079616 total steps have happened
[32m[20221213 22:59:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1992 --------------------------#
[32m[20221213 22:59:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0002 |          56.8755 |          15.0990 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0034 |          56.2073 |          15.0943 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0018 |          56.4275 |          15.0683 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0080 |          55.2000 |          15.0802 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |           0.0031 |          63.9961 |          15.0781 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0052 |          55.3370 |          15.0652 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0101 |          54.6155 |          15.0757 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0095 |          54.6693 |          15.0631 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0123 |          54.4857 |          15.1052 |
[32m[20221213 22:59:55 @agent_ppo2.py:185][0m |          -0.0117 |          54.4725 |          15.0696 |
[32m[20221213 22:59:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 22:59:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.20
[32m[20221213 22:59:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.95
[32m[20221213 22:59:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.43
[32m[20221213 22:59:56 @agent_ppo2.py:143][0m Total time:      41.71 min
[32m[20221213 22:59:56 @agent_ppo2.py:145][0m 4081664 total steps have happened
[32m[20221213 22:59:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1993 --------------------------#
[32m[20221213 22:59:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0006 |          48.1528 |          15.1306 |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0012 |          44.6864 |          15.1246 |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0064 |          43.2946 |          15.0838 |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0106 |          42.6968 |          15.0869 |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0070 |          42.2328 |          15.0940 |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0085 |          42.0722 |          15.0727 |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0123 |          41.5971 |          15.0781 |
[32m[20221213 22:59:56 @agent_ppo2.py:185][0m |          -0.0101 |          41.5335 |          15.0761 |
[32m[20221213 22:59:57 @agent_ppo2.py:185][0m |          -0.0099 |          41.2228 |          15.0513 |
[32m[20221213 22:59:57 @agent_ppo2.py:185][0m |          -0.0126 |          40.8357 |          15.0546 |
[32m[20221213 22:59:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:59:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 319.24
[32m[20221213 22:59:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.99
[32m[20221213 22:59:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.41
[32m[20221213 22:59:57 @agent_ppo2.py:143][0m Total time:      41.73 min
[32m[20221213 22:59:57 @agent_ppo2.py:145][0m 4083712 total steps have happened
[32m[20221213 22:59:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1994 --------------------------#
[32m[20221213 22:59:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:57 @agent_ppo2.py:185][0m |           0.0011 |          51.7142 |          15.1478 |
[32m[20221213 22:59:57 @agent_ppo2.py:185][0m |           0.0046 |          54.3420 |          15.1456 |
[32m[20221213 22:59:57 @agent_ppo2.py:185][0m |          -0.0046 |          48.5388 |          15.1550 |
[32m[20221213 22:59:57 @agent_ppo2.py:185][0m |          -0.0029 |          48.0820 |          15.1581 |
[32m[20221213 22:59:57 @agent_ppo2.py:185][0m |          -0.0039 |          48.1175 |          15.1612 |
[32m[20221213 22:59:58 @agent_ppo2.py:185][0m |          -0.0061 |          47.5407 |          15.1683 |
[32m[20221213 22:59:58 @agent_ppo2.py:185][0m |          -0.0080 |          47.1370 |          15.1701 |
[32m[20221213 22:59:58 @agent_ppo2.py:185][0m |          -0.0086 |          47.1704 |          15.1462 |
[32m[20221213 22:59:58 @agent_ppo2.py:185][0m |          -0.0057 |          46.7849 |          15.1741 |
[32m[20221213 22:59:58 @agent_ppo2.py:185][0m |          -0.0092 |          46.6454 |          15.1823 |
[32m[20221213 22:59:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 22:59:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.48
[32m[20221213 22:59:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.01
[32m[20221213 22:59:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.29
[32m[20221213 22:59:58 @agent_ppo2.py:143][0m Total time:      41.75 min
[32m[20221213 22:59:58 @agent_ppo2.py:145][0m 4085760 total steps have happened
[32m[20221213 22:59:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1995 --------------------------#
[32m[20221213 22:59:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 22:59:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 22:59:58 @agent_ppo2.py:185][0m |           0.0021 |          47.5076 |          15.0575 |
[32m[20221213 22:59:58 @agent_ppo2.py:185][0m |          -0.0036 |          43.4137 |          15.0429 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |          -0.0056 |          41.9832 |          15.0299 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |          -0.0085 |          41.1054 |          15.0095 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |          -0.0082 |          40.5326 |          15.0081 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |          -0.0123 |          40.0416 |          14.9906 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |           0.0003 |          41.6699 |          15.0012 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |          -0.0132 |          39.5678 |          14.9804 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |          -0.0110 |          39.1461 |          14.9571 |
[32m[20221213 22:59:59 @agent_ppo2.py:185][0m |          -0.0137 |          38.9361 |          14.9568 |
[32m[20221213 22:59:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 22:59:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.91
[32m[20221213 22:59:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.24
[32m[20221213 22:59:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 363.02
[32m[20221213 22:59:59 @agent_ppo2.py:143][0m Total time:      41.77 min
[32m[20221213 22:59:59 @agent_ppo2.py:145][0m 4087808 total steps have happened
[32m[20221213 22:59:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1996 --------------------------#
[32m[20221213 22:59:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:00:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0002 |          53.8324 |          15.0801 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0060 |          51.5910 |          15.0661 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0061 |          51.0286 |          15.0637 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0097 |          50.4899 |          15.0998 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0098 |          50.3065 |          15.0981 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0111 |          49.9523 |          15.1055 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0097 |          49.8880 |          15.0948 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0120 |          49.6201 |          15.1052 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0089 |          49.5782 |          15.1163 |
[32m[20221213 23:00:00 @agent_ppo2.py:185][0m |          -0.0121 |          49.3782 |          15.1216 |
[32m[20221213 23:00:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:00:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.15
[32m[20221213 23:00:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.40
[32m[20221213 23:00:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.43
[32m[20221213 23:00:01 @agent_ppo2.py:143][0m Total time:      41.79 min
[32m[20221213 23:00:01 @agent_ppo2.py:145][0m 4089856 total steps have happened
[32m[20221213 23:00:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1997 --------------------------#
[32m[20221213 23:00:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:00:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:00:01 @agent_ppo2.py:185][0m |           0.0027 |          52.3176 |          15.0694 |
[32m[20221213 23:00:01 @agent_ppo2.py:185][0m |          -0.0069 |          49.6310 |          15.0481 |
[32m[20221213 23:00:01 @agent_ppo2.py:185][0m |          -0.0048 |          48.9427 |          15.0859 |
[32m[20221213 23:00:01 @agent_ppo2.py:185][0m |           0.0080 |          53.1830 |          15.0651 |
[32m[20221213 23:00:01 @agent_ppo2.py:185][0m |          -0.0066 |          48.5727 |          15.0552 |
[32m[20221213 23:00:01 @agent_ppo2.py:185][0m |          -0.0017 |          51.9249 |          15.0656 |
[32m[20221213 23:00:01 @agent_ppo2.py:185][0m |          -0.0066 |          48.0753 |          15.0236 |
[32m[20221213 23:00:02 @agent_ppo2.py:185][0m |          -0.0097 |          47.9531 |          15.0536 |
[32m[20221213 23:00:02 @agent_ppo2.py:185][0m |          -0.0138 |          47.9421 |          15.0497 |
[32m[20221213 23:00:02 @agent_ppo2.py:185][0m |          -0.0099 |          47.6527 |          15.0465 |
[32m[20221213 23:00:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:00:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.52
[32m[20221213 23:00:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.22
[32m[20221213 23:00:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.75
[32m[20221213 23:00:02 @agent_ppo2.py:143][0m Total time:      41.82 min
[32m[20221213 23:00:02 @agent_ppo2.py:145][0m 4091904 total steps have happened
[32m[20221213 23:00:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1998 --------------------------#
[32m[20221213 23:00:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:00:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:00:02 @agent_ppo2.py:185][0m |          -0.0041 |          50.9255 |          15.1340 |
[32m[20221213 23:00:02 @agent_ppo2.py:185][0m |          -0.0061 |          48.8436 |          15.1470 |
[32m[20221213 23:00:02 @agent_ppo2.py:185][0m |          -0.0037 |          48.7851 |          15.1438 |
[32m[20221213 23:00:02 @agent_ppo2.py:185][0m |          -0.0053 |          48.1900 |          15.1508 |
[32m[20221213 23:00:03 @agent_ppo2.py:185][0m |          -0.0081 |          47.8486 |          15.1449 |
[32m[20221213 23:00:03 @agent_ppo2.py:185][0m |          -0.0047 |          47.3317 |          15.1376 |
[32m[20221213 23:00:03 @agent_ppo2.py:185][0m |          -0.0064 |          47.1000 |          15.1590 |
[32m[20221213 23:00:03 @agent_ppo2.py:185][0m |          -0.0031 |          48.6157 |          15.1509 |
[32m[20221213 23:00:03 @agent_ppo2.py:185][0m |          -0.0074 |          46.6503 |          15.1378 |
[32m[20221213 23:00:03 @agent_ppo2.py:185][0m |          -0.0100 |          46.5178 |          15.1480 |
[32m[20221213 23:00:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:00:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.06
[32m[20221213 23:00:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.48
[32m[20221213 23:00:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.25
[32m[20221213 23:00:03 @agent_ppo2.py:143][0m Total time:      41.84 min
[32m[20221213 23:00:03 @agent_ppo2.py:145][0m 4093952 total steps have happened
[32m[20221213 23:00:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1999 --------------------------#
[32m[20221213 23:00:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:00:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:00:03 @agent_ppo2.py:185][0m |           0.0004 |          45.1145 |          15.3773 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0056 |          41.1428 |          15.3585 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0088 |          39.2785 |          15.3820 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0082 |          38.1564 |          15.3726 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0120 |          37.1537 |          15.3691 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0096 |          36.8125 |          15.3630 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0041 |          37.8377 |          15.3711 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0082 |          36.8061 |          15.3823 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0144 |          35.4896 |          15.3532 |
[32m[20221213 23:00:04 @agent_ppo2.py:185][0m |          -0.0079 |          35.4353 |          15.3540 |
[32m[20221213 23:00:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:00:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.03
[32m[20221213 23:00:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.19
[32m[20221213 23:00:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.40
[32m[20221213 23:00:04 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 501.24
[32m[20221213 23:00:04 @agent_ppo2.py:143][0m Total time:      41.86 min
[32m[20221213 23:00:04 @agent_ppo2.py:145][0m 4096000 total steps have happened
[32m[20221213 23:00:04 @train.py:55][0m [4m[34mCRITICAL[0m Training completed!
